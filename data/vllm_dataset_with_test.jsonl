{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-8d75fe4", "created_at": "2024-06-07T08:42:35+00:00", "base_commit": "388596c91437a51d428a447594e9faec340c29b2", "head_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769", "patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a75..cae682216 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -179,7 +179,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n \n # cutlass\n def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n-                         a_scales: torch.Tensor, b_scales: torch.Tensor,\n+                         scale_a: torch.Tensor, scale_b: torch.Tensor,\n                          out_dtype: Type[torch.dtype]) -> torch.Tensor:\n     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\n     assert (out_dtype is torch.bfloat16 or out_dtype is torch.float16)\n@@ -188,7 +188,7 @@ def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n     n = b.shape[1]\n     out = torch.empty((m, n), dtype=out_dtype, device=a.device)\n \n-    vllm_ops.cutlass_scaled_mm_dq(out, a, b, a_scales, b_scales)\n+    vllm_ops.cutlass_scaled_mm_dq(out, a, b, scale_a, scale_b)\n \n     return out\n \ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e3d..136a64623 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -17,6 +17,24 @@ ACTIVATION_SCHEMES = [\"static\", \"dynamic\"]\n logger = init_logger(__name__)\n \n \n+def cutlass_fp8_supported() -> bool:\n+    capability = torch.cuda.get_device_capability()\n+    capability = capability[0] * 10 + capability[1]\n+    version = torch.version.cuda\n+    version = version[0] * 10 + version[1]\n+\n+    # CUTLASS FP8 kernels need at least\n+    #   CUDA 12.0 on SM90 systems (Hopper)\n+    #   CUDA 12.4 on SM89 systems (Lovelace)\n+    gpu_is_supported = False\n+    if capability >= 900:\n+        gpu_is_supported = version > 120\n+    elif capability >= 890:\n+        gpu_is_supported = version > 124\n+\n+    return gpu_is_supported\n+\n+\n class Fp8Config(QuantizationConfig):\n     \"\"\"Config class for FP8.\"\"\"\n \n@@ -92,6 +110,7 @@ class Fp8LinearMethod(LinearMethodBase):\n \n     def __init__(self, quant_config: Fp8Config):\n         self.quant_config = quant_config\n+        self.cutlass_fp8_supported = cutlass_fp8_supported()\n \n     def _create_scale_param(\n         self,\n@@ -233,25 +252,40 @@ class Fp8LinearMethod(LinearMethodBase):\n               layer: torch.nn.Module,\n               x: torch.Tensor,\n               bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n+\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x,\n-                                               layer.act_scale,\n-                                               batch_dim_padding=17)\n-\n-        # Fused GEMM_DQ -- note we padded the input above because\n-        # torch._scaled_mm is more performant for matrices with\n-        # batch dimension > 16. Note that this could change\n-        # in the future.\n-        output, _ = torch._scaled_mm(\n-            qinput,\n-            layer.weight,\n-            out_dtype=x.dtype,\n-            scale_a=x_scale,\n-            scale_b=layer.weight_scale,\n-            bias=bias,\n-        )\n+\n+        if bias is None and self.cutlass_fp8_supported:\n+            qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n+\n+            # Fused GEMM_DQ\n+            output = ops.cutlass_scaled_mm_dq(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+            )\n+\n+        else:\n+            qinput, x_scale = ops.scaled_fp8_quant(x,\n+                                                   layer.act_scale,\n+                                                   batch_dim_padding=17)\n+\n+            # Fused GEMM_DQ -- note we padded the input above because\n+            # torch._scaled_mm is more performant for matrices with\n+            # batch dimension > 16. Note that this could change\n+            # in the future.\n+            output, _ = torch._scaled_mm(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+                bias=bias,\n+            )\n \n         return torch.narrow(output, 0, 0, x.shape[0])", "test_patch": "", "efficiency_test": ["import os\nimport json\nimport time\nimport importlib\nimport types\nfrom typing import Dict, Any, Tuple, Callable, Optional\n\nimport numpy as np\nimport torch\n\n# -------------------------------\n# Error taxonomy (deterministic)\n# -------------------------------\nE_IMPORT_MISSING = \"IMPORT_MISSING_SYMBOL\"\nE_OPT_PATH_NOT_TRIGGERED = \"OPT_PATH_NOT_TRIGGERED\"\nE_CAPABILITY = \"CAPABILITY_UNSUPPORTED\"\nE_EQFAIL = \"EQUIVALENCE_FAILED\"\n\n# -------------------------------\n# Determinism & policy helpers\n# -------------------------------\ndef ensure_determinism() -> None:\n    torch.manual_seed(1234)\n    np.random.seed(1234)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(1234)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    # Be strict by default unless commit requires TF32\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\ndef pick_device() -> torch.device:\n    want = os.getenv(\"PROB_DEVICE\", \"auto\").lower()\n    if want == \"cuda\" and torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if want == \"mps\" and getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    if want == \"cpu\":\n        return torch.device(\"cpu\")\n    # auto\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef pick_dtype() -> torch.dtype:\n    key = os.getenv(\"PROB_FORCE_DTYPE\", \"auto\").lower()\n    map_ = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16, \"auto\": torch.float32, \"\": torch.float32}\n    return map_.get(key, torch.float32)\n\ndef parse_opt_gates() -> Dict[str, Any]:\n    raw = os.getenv(\"PROB_OPT_GATES\", '{}')\n    if not raw:\n        return {}\n    try:\n        return json.loads(raw)\n    except Exception:\n        gates = {}\n        for kv in raw.split(\",\"):\n            if \"=\" in kv:\n                k, v = kv.split(\"=\", 1)\n                gates[k.strip()] = v.strip()\n        return gates\n\n# -------------------------------\n# Diff-aware import resolution\n# -------------------------------\ndef _infer_candidates_from_diff() -> Tuple[Optional[str], Optional[str], list]:\n    \"\"\"\n    Deterministically infer (module, symbol) from the provided diff and commit message.\n    Returns: (module_path or None, symbol_name or None, candidate_list_for_errors)\n    \"\"\"\n    module_hint = os.getenv(\"PROB_MODULE\", \"\").strip() or \"\".strip()\n    symbol_hint = os.getenv(\"PROB_SYMBOL\", \"\").strip() or \"\".strip()\n    candidates = []\n\n    # Highest priority: explicit hints\n    if module_hint and symbol_hint:\n        return module_hint, symbol_hint, [(module_hint, symbol_hint)]\n\n    # Parse inputs (static text replacement already done upstream)\n    diff_text = '''diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a75..cae682216 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -179,7 +179,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n \n # cutlass\n def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n-                         a_scales: torch.Tensor, b_scales: torch.Tensor,\n+                         scale_a: torch.Tensor, scale_b: torch.Tensor,\n                          out_dtype: Type[torch.dtype]) -> torch.Tensor:\n     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\n     assert (out_dtype is torch.bfloat16 or out_dtype is torch.float16)\n@@ -188,7 +188,7 @@ def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n     n = b.shape[1]\n     out = torch.empty((m, n), dtype=out_dtype, device=a.device)\n \n-    vllm_ops.cutlass_scaled_mm_dq(out, a, b, a_scales, b_scales)\n+    vllm_ops.cutlass_scaled_mm_dq(out, a, b, scale_a, scale_b)\n \n     return out\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e3d..136a64623 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -17,6 +17,24 @@ ACTIVATION_SCHEMES = [\"static\", \"dynamic\"]\n logger = init_logger(__name__)\n \n \n+def cutlass_fp8_supported() -> bool:\n+    capability = torch.cuda.get_device_capability()\n+    capability = capability[0] * 10 + capability[1]\n+    version = torch.version.cuda\n+    version = version[0] * 10 + version[1]\n+\n+    # CUTLASS FP8 kernels need at least\n+    #   CUDA 12.0 on SM90 systems (Hopper)\n+    #   CUDA 12.4 on SM89 systems (Lovelace)\n+    gpu_is_supported = False\n+    if capability >= 900:\n+        gpu_is_supported = version > 120\n+    elif capability >= 890:\n+        gpu_is_supported = version > 124\n+\n+    return gpu_is_supported\n+\n+\n class Fp8Config(QuantizationConfig):\n     \"\"\"Config class for FP8.\"\"\"\n \n@@ -92,6 +110,7 @@ class Fp8LinearMethod(LinearMethodBase):\n \n     def __init__(self, quant_config: Fp8Config):\n         self.quant_config = quant_config\n+        self.cutlass_fp8_supported = cutlass_fp8_supported()\n \n     def _create_scale_param(\n         self,\n@@ -233,25 +252,40 @@ class Fp8LinearMethod(LinearMethodBase):\n               layer: torch.nn.Module,\n               x: torch.Tensor,\n               bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n+\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x,\n-                                               layer.act_scale,\n-                                               batch_dim_padding=17)\n-\n-        # Fused GEMM_DQ -- note we padded the input above because\n-        # torch._scaled_mm is more performant for matrices with\n-        # batch dimension > 16. Note that this could change\n-        # in the future.\n-        output, _ = torch._scaled_mm(\n-            qinput,\n-            layer.weight,\n-            out_dtype=x.dtype,\n-            scale_a=x_scale,\n-            scale_b=layer.weight_scale,\n-            bias=bias,\n-        )\n+\n+        if bias is None and self.cutlass_fp8_supported:\n+            qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n+\n+            # Fused GEMM_DQ\n+            output = ops.cutlass_scaled_mm_dq(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+            )\n+\n+        else:\n+            qinput, x_scale = ops.scaled_fp8_quant(x,\n+                                                   layer.act_scale,\n+                                                   batch_dim_padding=17)\n+\n+            # Fused GEMM_DQ -- note we padded the input above because\n+            # torch._scaled_mm is more performant for matrices with\n+            # batch dimension > 16. Note that this could change\n+            # in the future.\n+            output, _ = torch._scaled_mm(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+                bias=bias,\n+            )\n \n         return torch.narrow(output, 0, 0, x.shape[0])'''\n    commit_msg = \"\"\"[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)\n\nSwitching from torch._scaled_mm to vLLM's cutlass fp8 kernels when supported as we are seeing 5-15% improvement in e2e performance on neuralmagic/Meta-Llama-3-8B-Instruct-FP8\n\nsee https://docs.google.com/spreadsheets/d/1GiAnmzyGHgZ6zL_LDSTm35Bdrt4A8AaFEurDlISYYA4/ for some quick e2e benchmarks and #5144 for comparisons across different GEMM sizes.\"\"\"\n    changed_files_json = \"\"\"[\"vllm/_custom_ops.py\", \"vllm/model_executor/layers/quantization/fp8.py\"]\"\"\"\n    changed_symbols_json = \"\"\"[]\"\"\"\n\n    # 1) Prefer items listed in changed_symbols_json if present.\n    try:\n        import json as _json\n        syms = _json.loads(changed_symbols_json) if changed_symbols_json.strip() else []\n    except Exception:\n        syms = []\n    if syms:\n        syms_sorted = sorted(syms, key=lambda s: (-int(s.get(\"changed_loc\", 0)), s.get(\"qualified\", \"\")))\n        top = syms_sorted[0]\n        mod = top.get(\"module\", \"\")\n        sym = top.get(\"qualified\", \"\") or top.get(\"name\", \"\")\n        if mod and sym:\n            candidates.append((mod, sym))\n\n    # 2) Fallback: scan diff headers and known tokens\n    for line in diff_text.splitlines():\n        if line.startswith(\"+++ \") or line.startswith(\"--- \"):\n            path = line.split(\"\\t\")[0].split()[-1]\n            if path.endswith(\".py\") and \"/tests/\" not in path:\n                mod = path.replace(\"/\", \".\").rstrip(\".py\").rstrip(\".\")\n                candidates.append((mod, None))\n        elif line.startswith(\"+class \") or line.startswith(\"+def \"):\n            token = line.split()[0][1:]\n            name = line.split()[1].split(\"(\")[0].strip()\n            if candidates and candidates[-1][1] is None:\n                mod = candidates[-1][0]\n                candidates[-1] = (mod, name)\n            else:\n                candidates.append((None, name))\n\n    # Token-based deterministic hints for this commit\n    if \"cutlass_scaled_mm_dq\" in diff_text:\n        candidates.append((\"vllm._custom_ops\", \"cutlass_scaled_mm_dq\"))\n    if \"class Fp8LinearMethod\" in diff_text:\n        candidates.append((\"vllm.model_executor.layers.quantization.fp8\", \"Fp8LinearMethod.apply\"))\n\n    # 3) Boost candidates mentioned in commit message\n    boosted = []\n    for mod, sym in candidates:\n        score = 0\n        if mod and \"fp8\" in mod:\n            score += 1\n        if mod and \"cutlass\" in mod:\n            score += 1\n        if \"CUTLASS\" in commit_msg or \"cutlass\" in commit_msg.lower():\n            score += 1\n        if sym and \"cutlass_scaled_mm_dq\" in (sym or \"\"):\n            score += 2\n        if sym and \"Fp8LinearMethod.apply\" in (sym or \"\"):\n            score += 3  # Favor method with larger LOC change\n        boosted.append((score, mod, sym))\n    boosted.sort(key=lambda t: (-t[0], t[1] or \"\", t[2] or \"\"))\n    if boosted:\n        _, mod, sym = boosted[0]\n        return mod, sym, [(m, s) for _, m, s in boosted]\n\n    return None, None, candidates\n\ndef resolve_target() -> Tuple[Callable, Dict[str, Any], str]:\n    \"\"\"\n    Returns (callable_or_bound_method, call_kwargs, fq_name_string).\n    Must resolve to the EXACT code path described by the commit diff, honoring env hints first.\n    \"\"\"\n    mod_hint = os.getenv(\"PROB_MODULE\", \"\").strip()\n    sym_hint = os.getenv(\"PROB_SYMBOL\", \"\").strip()\n\n    if mod_hint and sym_hint:\n        mod_path, sym_name = mod_hint, sym_hint\n        candidates = [(mod_path, sym_name)]\n    else:\n        mod_path, sym_name, candidates = _infer_candidates_from_diff()\n\n    if not mod_path or not sym_name:\n        raise ImportError(f\"{E_IMPORT_MISSING}: Unable to infer target. Candidates={candidates}\")\n\n    m = importlib.import_module(mod_path)\n    target = m\n    for part in sym_name.split(\".\"):\n        if not hasattr(target, part):\n            raise ImportError(f\"{E_IMPORT_MISSING}: {mod_path}.{sym_name} not found; nearest candidates={candidates}\")\n        target = getattr(target, part)\n\n    fq = f\"{mod_path}.{sym_name}\"\n    return target, {}, fq\n\n# -------------------------------\n# Setup: workload reflecting commit\n# -------------------------------\ndef _cap_by_memory(nelms: int, bytes_per: int, frac: float = 0.7) -> int:\n    try:\n        if torch.cuda.is_available():\n            free, total = torch.cuda.mem_get_info()\n            cap = int((total * frac) // max(bytes_per, 1))\n            return min(nelms, cap)\n    except Exception:\n        pass\n    return nelms\n\ndef _require_cuda_for_fp8() -> None:\n    if not torch.cuda.is_available():\n        raise RuntimeError(f\"{E_CAPABILITY}: CUDA device is required for FP8 CUTLASS kernels.\")\n\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload that exercises the optimization.\"\"\"\n    ensure_determinism()\n    device = pick_device()\n    _require_cuda_for_fp8()\n    dtype_pref = pick_dtype()\n    # For FP8 path, out dtype must be fp16 or bf16; choose fp16 by default if not provided.\n    out_dtype = dtype_pref if dtype_pref in (torch.float16, torch.bfloat16) else torch.float16\n\n    # GEMM sizes (ensure multiples of 16 for B matrix dims as required by kernel)\n    M = 2048  # batch (first dim)\n    K = 4096  # shared dim\n    N = 4096  # output dim\n\n    # Memory cap heuristic (no-op but keeps determinism hooks)\n    _ = _cap_by_memory(M * K + K * N + M * N, bytes_per=2 if out_dtype in (torch.float16, torch.bfloat16) else 4)\n\n    # Pre-allocate inputs on device\n    x = torch.randn((M, K), dtype=out_dtype, device=device)\n    w_full = torch.randn((K, N), dtype=out_dtype, device=device)\n\n    # Import ops dynamically\n    try:\n        ops_mod = importlib.import_module(\"vllm._custom_ops\")\n    except Exception as e:\n        raise ImportError(f\"{E_IMPORT_MISSING}: failed to import vllm._custom_ops: {e}\")\n\n    # Quantize weight once (dynamic per-tensor scale)\n    qweight, w_scale = ops_mod.scaled_fp8_quant(w_full, scale=None)\n\n    # Store gates/environment flags\n    opt_gates = parse_opt_gates()\n    for k_env, v_env in opt_gates.items():\n        os.environ[str(k_env)] = str(v_env)\n\n    data = {\n        \"device\": device,\n        \"dtype\": out_dtype,\n        \"M\": M, \"K\": K, \"N\": N,\n        \"x\": x,\n        \"qweight\": qweight,\n        \"w_scale\": w_scale,\n        \"opt_gates\": opt_gates,\n        \"ops_mod\": ops_mod,\n        # Lazy-initialized for Fp8LinearMethod.apply path\n        \"method\": None,\n        \"layer\": None,\n        \"opt_path_hit\": False,\n    }\n    return data\n\n# -------------------------------\n# Experiment: EXACT optimized path\n# -------------------------------\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"\n    Execute ONLY the performance-critical code path being optimized.\n    \"\"\"\n    target, call_kwargs, fqname = resolve_target()\n\n    device = data[\"device\"]\n    x = data[\"x\"]\n    qweight = data[\"qweight\"]\n    w_scale = data[\"w_scale\"]\n    ops_mod = data[\"ops_mod\"]\n    out_dtype = data[\"dtype\"]\n\n    with torch.no_grad():\n        # Branch based on resolved target\n        if fqname.endswith(\"vllm._custom_ops.cutlass_scaled_mm_dq\") or fqname.endswith(\"._custom_ops.cutlass_scaled_mm_dq\"):\n            # Quantize activation dynamically each call (matches Fp8LinearMethod path)\n            qinput, x_scale = ops_mod.scaled_fp8_quant(x, scale=None)\n            # Call CUTLASS fused GEMM_DQ\n            result = target(qinput,\n                            qweight,\n                            out_dtype=out_dtype,\n                            scale_a=x_scale,\n                            scale_b=w_scale,\n                            **call_kwargs)\n            data[\"opt_path_hit\"] = True\n            return result\n\n        if fqname.endswith(\"vllm.model_executor.layers.quantization.fp8.Fp8LinearMethod.apply\") or fqname.endswith(\".quantization.fp8.Fp8LinearMethod.apply\"):\n            # Lazily construct method and minimal layer with required attributes\n            if data[\"method\"] is None or data[\"layer\"] is None:\n                mod = importlib.import_module(\"vllm.model_executor.layers.quantization.fp8\")\n                Fp8Config = getattr(mod, \"Fp8Config\")\n                Fp8LinearMethod = getattr(mod, \"Fp8LinearMethod\")\n                method = Fp8LinearMethod(Fp8Config())\n                # Minimal layer-like object\n                layer = types.SimpleNamespace()\n                layer.weight = qweight\n                layer.weight_scale = w_scale\n                layer.act_scale = None  # dynamic activation scaling\n                data[\"method\"] = method\n                data[\"layer\"] = layer\n            method = data[\"method\"]\n            layer = data[\"layer\"]\n            # Bias must be None to trigger CUTLASS path inside apply\n            result = target(method, layer, x, None)\n            # We mark opt path as hit if method indicates support; apply handles fallback internally otherwise.\n            data[\"opt_path_hit\"] = bool(getattr(method, \"cutlass_fp8_supported\", False))\n            return result\n\n        # Fallback: this should not be reached for this commit; raise deterministically.\n        raise ImportError(f\"{E_IMPORT_MISSING}: Resolved unexpected target {fqname}\")\n\n# -------------------------------\n# Result I/O for equivalence\n# -------------------------------\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for future equivalence checking.\"\"\"\n    if isinstance(result, torch.Tensor):\n        payload = {\n            \"type\": \"torch_tensor\",\n            \"shape\": tuple(result.shape),\n            \"dtype\": str(result.dtype),\n            \"device\": \"cpu\",\n            \"sample\": result.flatten()[:4096].detach().cpu(),\n        }\n        torch.save(payload, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"value\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    return torch.load(filepath)\n\n# -------------------------------\n# Equivalence with dtype-aware tolerances\n# -------------------------------\ndef _eq_tolerances(dtype: torch.dtype, level: str) -> Tuple[float, float]:\n    if level == \"exact\":\n        return (0.0, 0.0)\n    if dtype in (torch.float16, torch.bfloat16):\n        return (1e-3, 1e-4)\n    if dtype == torch.float32:\n        return (1e-5, 1e-7)\n    return (1e-5, 1e-7)\n\ndef check_equivalence(current_result: Any, reference_payload: Any) -> None:\n    level = os.getenv(\"PROB_EQ_LEVEL\", \"numeric\").lower()\n    if isinstance(current_result, torch.Tensor) and reference_payload.get(\"type\") == \"torch_tensor\":\n        ref_sample = reference_payload[\"sample\"]\n        assert tuple(current_result.shape) == tuple(reference_payload[\"shape\"]), \\\n            f\"Shape mismatch: {tuple(current_result.shape)} vs {tuple(reference_payload['shape'])}\"\n        assert str(current_result.dtype) == reference_payload[\"dtype\"], \\\n            f\"Dtype mismatch: {current_result.dtype} vs {reference_payload['dtype']}\"\n        rtol, atol = _eq_tolerances(current_result.dtype, level)\n        torch.testing.assert_close(\n            current_result.flatten()[: ref_sample.numel()].cpu(),\n            ref_sample,\n            rtol=rtol,\n            atol=atol,\n            msg=f\"{E_EQFAIL}: deviation beyond tolerances (level={level})\"\n        )\n    else:\n        assert current_result == reference_payload.get(\"value\"), f\"{E_EQFAIL}: non-tensor results not equal\"\n\n# -------------------------------\n# Timing utilities\n# -------------------------------\ndef _time_gpu(run: Callable, iters: int) -> Tuple[float, float, float]:\n    torch.cuda.synchronize()\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    times = []\n    for _ in range(iters):\n        start.record()\n        _ = run()\n        end.record()\n        torch.cuda.synchronize()\n        times.append(start.elapsed_time(end))  # ms\n    times.sort()\n    avg = sum(times) / len(times)\n    p50 = times[len(times) // 2]\n    p95 = times[int(len(times) * 0.95) - 1]\n    return avg, p50, p95\n\ndef _time_cpu(run: Callable, iters: int) -> Tuple[float, float, float]:\n    times = []\n    for _ in range(iters):\n        t0 = time.perf_counter()\n        _ = run()\n        t1 = time.perf_counter()\n        times.append((t1 - t0) * 1000.0)\n    times.sort()\n    avg = sum(times) / len(times)\n    p50 = times[len(times) // 2]\n    p95 = times[int(len(times) * 0.95) - 1]\n    return avg, p50, p95\n\n# -------------------------------\n# Main entry: run_test\n# -------------------------------\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"\n    Run the performance test and return average execution time in milliseconds.\n\n    Args:\n        eqcheck: Compare current result vs stored reference.\n        reference: Store current result as reference.\n        prefix: Prefix for reference filenames.\n\n    Returns:\n        Average execution time (ms).\n    \"\"\"\n    data = setup()\n    impl_tag = os.getenv(\"PROB_IMPL_TAG\", \"parent\")\n    commit_hash = os.getenv(\"PROB_COMMIT_HASH\", \"8d75fe48ca5f46b7af0f5201d8500b9604eed769\")\n\n    # Warmup\n    warmup = 5 if torch.cuda.is_available() else 3\n    for _ in range(warmup):\n        _ = experiment(data)\n\n    # Timing iterations\n    iters = 50 if torch.cuda.is_available() else 10\n    if torch.cuda.is_available():\n        avg_ms, p50_ms, p95_ms = _time_gpu(lambda: experiment(data), iters)\n    else:\n        avg_ms, p50_ms, p95_ms = _time_cpu(lambda: experiment(data), iters)\n\n    # Equivalence/reference I/O\n    result = experiment(data)\n    ref_path = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    if reference:\n        store_result(result, ref_path)\n    if eqcheck:\n        reference_payload = load_result(ref_path)\n        check_equivalence(result, reference_payload)\n\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(data[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": round(avg_ms, 6),\n        \"p50_ms\": round(p50_ms, 6),\n        \"p95_ms\": round(p95_ms, 6),\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": bool(data.get(\"opt_path_hit\", False)),\n    }\n    print(json.dumps(summary, sort_keys=True))\n    return avg_ms\n\n# End of script"], "duration_changes": [{"base": [1124.2557373046875], "head": [1058.966552734375], "main": [1046.79833984375]}], "human_performance": 1.0616536796197464, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)\n\nSwitching from torch._scaled_mm to vLLM's cutlass fp8 kernels when supported as we are seeing 5-15% improvement in e2e performance on neuralmagic/Meta-Llama-3-8B-Instruct-FP8\n\nsee https://docs.google.com/spreadsheets/d/1GiAnmzyGHgZ6zL_LDSTm35Bdrt4A8AaFEurDlISYYA4/ for some quick e2e benchmarks and #5144 for comparisons across different GEMM sizes.", "setup_commands": [], "install_commands": [], "notes": null}
