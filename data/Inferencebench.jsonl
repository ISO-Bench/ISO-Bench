{"repo": "coding-mess/vllm", "instance_id": "coding-mess__vllm-PR-498", "created_at": "2023-04-08T00:45:07+00:00", "base_commit": "0f40557af6141ced118b81f2a04e651a0c6c9dbd^", "head_commit": "0f40557af6141ced118b81f2a04e651a0c6c9dbd", "patch": "diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\nindex aa611ac0d..24727713f 100644\n--- a/benchmark/benchmark_latency.py\n+++ b/benchmark/benchmark_latency.py\n@@ -50,14 +50,15 @@ def main(args: argparse.Namespace):\n         block_size=args.block_size,\n     )\n     sampling_params_dict = {\n-        'n': 1,\n-        'temperature': 0.0,\n+        'n': args.n,\n+        'temperature': 0.0 if args.use_beam_search else 1.0,\n         'top_p': 1.0,\n-        'use_beam_search': False,\n+        'use_beam_search': args.use_beam_search,\n         'stop_token_ids': set(),\n         'max_num_steps': args.output_len,\n     }\n     sampling_params = SamplingParams.from_dict(sampling_params_dict)\n+    print(sampling_params)\n     input_token_ids = [0] * args.input_len\n \n     def profile_step(profile=False):\n@@ -93,6 +94,8 @@ if __name__ == '__main__':\n     parser.add_argument('--input-len', type=int, default=32)\n     parser.add_argument('--output-len', type=int, default=128)\n     parser.add_argument('--batch-size', type=int, default=8)\n+    parser.add_argument('--n', type=int, default=1)\n+    parser.add_argument('--use-beam-search', action='store_true')\n     args = parser.parse_args()\n     args.max_num_batched_tokens = max(\n         args.max_num_batched_tokens, args.batch_size * args.input_len)\ndiff --git a/cacheflow/models/sample.py b/cacheflow/models/sample.py\nindex 3b53f34f4..1e358c7e5 100644\n--- a/cacheflow/models/sample.py\n+++ b/cacheflow/models/sample.py\n@@ -185,9 +185,10 @@ def _sample_from_generation_tokens(\n         vocab_size = logprobs.size(-1)\n         beam_width = len(seq_ids)\n         _, topk_ids = torch.topk(logprobs.flatten(), beam_width)\n-        seq_idx = torch.div(topk_ids, vocab_size, rounding_mode='floor').tolist()\n+        topk_ids = topk_ids.tolist()\n+        seq_idx = [i // vocab_size for i in topk_ids]\n         beam_seq_ids = [seq_ids[i] for i in seq_idx]\n-        token_ids = (topk_ids % vocab_size).tolist()\n+        token_ids = [i % vocab_size for i in topk_ids]\n \n         beam_outputs: Dict[int, Tuple[int, int]] = {}\n         outstanding_beams: List[Tuple[int, int]] = []\ndiff --git a/cacheflow/worker/cache_engine.py b/cacheflow/worker/cache_engine.py\nindex 164b2a2a6..addde3883 100644\n--- a/cacheflow/worker/cache_engine.py\n+++ b/cacheflow/worker/cache_engine.py\n@@ -120,24 +120,8 @@ class CacheEngine:\n     def swap_out(self, src_to_dst: Dict[int, int]) -> None:\n         self._swap(self.gpu_cache, self.cpu_cache, src_to_dst)\n \n-    def _copy(\n-        self,\n-        src: List[KVCache],\n-        dst: List[KVCache],\n-        src_to_dsts: Dict[int, List[int]],\n-    ) -> None:\n-        with torch.cuda.stream(self.cache_stream):\n-            for i in range(self.num_layers):\n-                src_key_cache, src_value_cache = src[i]\n-                dst_key_cache, dst_value_cache = dst[i]\n-                # Copy the key blocks.\n-                cache_ops.copy_blocks(\n-                    src_key_cache, dst_key_cache, src_to_dsts)\n-                # Copy the value blocks.\n-                cache_ops.copy_blocks(\n-                    src_value_cache, dst_value_cache, src_to_dsts)\n-                event = self.events[i]\n-                event.record(stream=self.cache_stream)\n-\n     def copy(self, src_to_dsts: Dict[int, List[int]]) -> None:\n-        self._copy(self.gpu_cache, self.gpu_cache, src_to_dsts)\n+        key_caches = [key_cache for key_cache, _ in self.gpu_cache]\n+        value_caches = [value_cache for _, value_cache in self.gpu_cache]\n+        # NOTE(woosuk): This operation implicitly synchronizes the CPU and GPU.\n+        cache_ops.copy_blocks(key_caches, value_caches, src_to_dsts)\ndiff --git a/csrc/cache.cpp b/csrc/cache.cpp\nindex fcf8b69fe..907736a98 100644\n--- a/csrc/cache.cpp\n+++ b/csrc/cache.cpp\n@@ -9,8 +9,8 @@ void swap_blocks(\n   const std::map<int64_t, int64_t>& block_mapping);\n \n void copy_blocks(\n-  torch::Tensor& src,\n-  torch::Tensor& dst,\n+  std::vector<torch::Tensor>& key_caches,\n+  std::vector<torch::Tensor>& value_caches,\n   const std::map<int64_t, std::vector<int64_t>>& block_mapping);\n \n void reshape_and_cache(\ndiff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 8b5537c47..3a34ba578 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -43,33 +43,93 @@ void swap_blocks(\n   }\n }\n \n-void copy_blocks(\n-  torch::Tensor& src,\n-  torch::Tensor& dst,\n-  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n-  torch::Device src_device = src.device();\n-  torch::Device dst_device = dst.device();\n-  assert(src_device.is_cuda() && dst_device.is_cuda());\n-  cudaMemcpyKind memcpy_type = cudaMemcpyDeviceToDevice;\n+namespace cacheflow {\n \n-  void *src_ptr = src.data_ptr();\n-  void *dst_ptr = dst.data_ptr();\n+// Grid: (num_layers, num_pairs)\n+template<typename scalar_t>\n+__global__ void copy_blocks_kernel(\n+  int64_t* key_cache_ptrs,\n+  int64_t* value_cache_ptrs,\n+  const int* __restrict__ block_mapping,\n+  const int numel_per_block) {\n+  const int layer_idx = blockIdx.x;\n+  const int pair_idx = blockIdx.y;\n+\n+  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);\n+  scalar_t* value_cache = reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);\n+  int src_block_number = block_mapping[2 * pair_idx];\n+  int dst_block_number = block_mapping[2 * pair_idx + 1];\n+\n+  const int src_block_offset = src_block_number * numel_per_block;\n+  const int dst_block_offset = dst_block_number * numel_per_block;\n+  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n+    int src_offset = src_block_offset + i;\n+    int dst_offset = dst_block_offset + i;\n+    key_cache[dst_offset] = key_cache[src_offset];\n+  }\n+  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n+    int src_offset = src_block_offset + i;\n+    int dst_offset = dst_block_offset + i;\n+    value_cache[dst_offset] = value_cache[src_offset];\n+  }\n+}\n \n-  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+} // namespace cacheflow\n+\n+void copy_blocks(\n+  std::vector<torch::Tensor>& key_caches,\n+  std::vector<torch::Tensor>& value_caches,\n+  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n+  int num_layers = key_caches.size();\n+  TORCH_CHECK(num_layers == value_caches.size());\n+  if (num_layers == 0) {\n+    return;\n+  }\n+  torch::Device cache_device = key_caches[0].device();\n+  TORCH_CHECK(cache_device.is_cuda());\n+\n+  // Create data structures for the kernel.\n+  // Create an array of pointers to the key and value caches.\n+  int64_t key_cache_ptrs[num_layers];\n+  int64_t value_cache_ptrs[num_layers];\n+  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {\n+    key_cache_ptrs[layer_idx] = reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());\n+    value_cache_ptrs[layer_idx] = reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());\n+  }\n+  // Create block mapping array.\n+  std::vector<int> block_mapping_vec;\n   for (const auto& pair : block_mapping) {\n-    int64_t src_block_number = pair.first;\n-    for (int64_t dst_block_number : pair.second) {\n-      int64_t src_offset = src_block_number * block_size_in_bytes;\n-      int64_t dst_offset = dst_block_number * block_size_in_bytes;\n-      cudaMemcpyAsync(\n-        dst_ptr + dst_offset,\n-        src_ptr + src_offset,\n-        block_size_in_bytes,\n-        memcpy_type,\n-        stream);\n+    int src_block_number = pair.first;\n+    for (int dst_block_number : pair.second) {\n+      block_mapping_vec.push_back(src_block_number);\n+      block_mapping_vec.push_back(dst_block_number);\n     }\n   }\n+  int* block_mapping_array = block_mapping_vec.data();\n+  int num_pairs = block_mapping_vec.size() / 2;\n+\n+  // Move the data structures to the GPU.\n+  // NOTE: This synchronizes the CPU and GPU.\n+  torch::Tensor key_cache_ptrs_tensor = torch::from_blob(\n+    key_cache_ptrs, {num_layers}, torch::kInt64).to(cache_device);\n+  torch::Tensor value_cache_ptrs_tensor = torch::from_blob(\n+    value_cache_ptrs, {num_layers}, torch::kInt64).to(cache_device);\n+  torch::Tensor block_mapping_tensor = torch::from_blob(\n+    block_mapping_array, {2 * num_pairs}, torch::kInt).to(cache_device);\n+\n+  // Launch the kernel.\n+  const int numel_per_block = key_caches[0][0].numel();\n+  dim3 grid(num_layers, num_pairs);\n+  dim3 block(std::min(1024, numel_per_block));\n+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n+    key_caches[0].scalar_type(), \"copy_blocks_kernel\", ([&] {\n+      cacheflow::copy_blocks_kernel<scalar_t><<<grid, block, 0, stream>>>(\n+        key_cache_ptrs_tensor.data_ptr<int64_t>(),\n+        value_cache_ptrs_tensor.data_ptr<int64_t>(),\n+        block_mapping_tensor.data_ptr<int>(),\n+        numel_per_block);\n+    }));\n }\n \n namespace cacheflow {\ndiff --git a/tests/kernels/cache.py b/tests/kernels/cache.py\nindex d6b1c3d2d..89f14cca8 100644\n--- a/tests/kernels/cache.py\n+++ b/tests/kernels/cache.py\n@@ -5,6 +5,61 @@ import torch\n from cacheflow import cache_ops\n \n \n+def test_copy_blocks(\n+    num_mappings: int,\n+    num_layers: int,\n+    num_heads: int,\n+    head_size: int,\n+    block_size: int,\n+    num_blocks: int,\n+    dtype: torch.dtype,\n+) -> None:\n+    # Generate random block mappings.\n+    src_blocks = random.sample(range(num_blocks), num_mappings)\n+    remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\n+    dst_blocks = random.sample(remainig_blocks, num_mappings)\n+    block_mapping = {src: [dst] for src, dst in zip(src_blocks, dst_blocks)}\n+\n+    # Create the KV cache.\n+    x = 16 // torch.tensor([], dtype=dtype).element_size()\n+    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n+    key_caches = []\n+    for _ in range(num_layers):\n+        key_cache = torch.randn(\n+            size=key_cache_shape, dtype=dtype, device='cuda')\n+        key_caches.append(key_cache)\n+    cloned_key_caches = []\n+    for key_cache in key_caches:\n+        cloned_key_caches.append(key_cache.clone())\n+\n+    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n+    value_caches = []\n+    for _ in range(num_layers):\n+        value_cache = torch.randn(\n+            size=value_cache_shape, dtype=dtype, device='cuda')\n+        value_caches.append(value_cache)\n+    cloned_value_caches = []\n+    for value_cache in value_caches:\n+        cloned_value_caches.append(value_cache.clone())\n+\n+    # Call the copy blocks kernel.\n+    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n+\n+    # Reference implementation.\n+    for src, dsts in block_mapping.items():\n+        for dst in dsts:\n+            for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n+                cloned_key_cache[dst] = cloned_key_cache[src]\n+            for value_cache, cloned_value_cache in zip(value_caches, cloned_value_caches):\n+                cloned_value_cache[dst] = cloned_value_cache[src]\n+\n+    # Compare the results.\n+    for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n+        assert torch.allclose(key_cache, cloned_key_cache)\n+    for value_cache, cloned_value_cache in zip(value_caches, cloned_value_caches):\n+        assert torch.allclose(value_cache, cloned_value_cache)\n+\n+\n def test_reshape_and_cache(\n     num_tokens: int,\n     num_heads: int,\n@@ -46,6 +101,9 @@ def test_reshape_and_cache(\n \n @torch.inference_mode()\n def test_cache() -> None:\n+    test_copy_blocks(\n+        num_mappings=23, num_layers=7, num_heads=17, head_size=16,\n+        block_size=8, num_blocks=1024, dtype=torch.half)\n     test_reshape_and_cache(\n         num_tokens=3, num_heads=2, head_size=16, block_size=8, num_blocks=2,\n         dtype=torch.half)", "test_patch": "", "efficiency_test": ["import argparse\nimport timeit\nimport random\nfrom typing import Dict, Any, List\nfrom unittest.mock import patch, MagicMock\n\nimport pytest\nimport torch\nimport numpy as np\n\n# Import actual modules being tested\ntry:\n    from cacheflow import cache_ops\n    from cacheflow.worker import cache_engine\n    from cacheflow.models import sample as sample_module\n    from cacheflow.sampling_params import SamplingParams\n    from cacheflow.models.sample import _sample_from_generation_tokens\nexcept Exception:\n    # If the repository modules are not importable in the test environment,\n    # skip the whole test module.\n    pytest.skip(\"Required cacheflow modules not available\", allow_module_level=True)\n\n\ndef setup_workload(seed: int = 42) -> Dict[str, Any]:\n    \"\"\"\n    Create a challenging workload for cache copy kernel and sampler tests.\n\n    NOTE: The true production workloads are very large (e.g., head_size and\n    num_blocks can be huge). For CI friendliness we scale down while keeping\n    structural complexity (multiple layers, multiple blocks, multiple heads).\n    Set a random seed to ensure reproducibility.\n    \"\"\"\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    # Scaled-down but structurally realistic parameters:\n    # (In production, these might be num_layers=24, num_heads=32, head_size=1024, num_blocks=1024, etc.)\n    num_layers = 4\n    num_heads = 8\n    head_size = 64  # must be multiple of 16 per CacheEngine requirement\n    block_size = 16\n    num_blocks = 128\n    dtype = torch.half  # half precision used in kernels\n    num_mappings = min(32, num_blocks // 2)\n\n    # Create random block mappings for testing copy (src -> [dsts]).\n    # Ensure dsts are distinct and do not equal src to exercise copying logic.\n    src_blocks = random.sample(range(num_blocks), num_mappings)\n    remaining = list(set(range(num_blocks)) - set(src_blocks))\n    if len(remaining) < num_mappings:\n        # fallback: allow some overlaps if range small\n        remaining = list(range(num_blocks))\n    dst_blocks = random.sample(remaining, num_mappings)\n    block_mapping = {int(s): [int(d)] for s, d in zip(src_blocks, dst_blocks)}\n\n    return {\n        \"num_layers\": num_layers,\n        \"num_heads\": num_heads,\n        \"head_size\": head_size,\n        \"block_size\": block_size,\n        \"num_blocks\": num_blocks,\n        \"dtype\": dtype,\n        \"block_mapping\": block_mapping,\n        \"num_mappings\": num_mappings,\n    }\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA is required for kernel tests\")\ndef test_copy_blocks_kernel_functionality():\n    \"\"\"\n    Test that cache_ops.copy_blocks performs the same in-place copy as a\n    reference implementation across multiple layers and caches.\n\n    This covers the changed API: copy_blocks now accepts vectors of key/value caches.\n    \"\"\"\n    workload = setup_workload()\n    num_layers = workload[\"num_layers\"]\n    num_heads = workload[\"num_heads\"]\n    head_size = workload[\"head_size\"]\n    block_size = workload[\"block_size\"]\n    num_blocks = workload[\"num_blocks\"]\n    dtype = workload[\"dtype\"]\n    block_mapping = workload[\"block_mapping\"]\n\n    # Build key/value caches on CUDA as lists (as expected by the new API).\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n\n    key_caches = []\n    value_caches = []\n    cloned_key_caches = []\n    cloned_value_caches = []\n\n    for _ in range(num_layers):\n        key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device=\"cuda\")\n        value_cache = torch.randn(size=value_cache_shape, dtype=dtype, device=\"cuda\")\n        key_caches.append(key_cache)\n        value_caches.append(value_cache)\n        cloned_key_caches.append(key_cache.clone())\n        cloned_value_caches.append(value_cache.clone())\n\n    # Call the new kernel API.\n    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n\n    # Reference implementation: for each mapping src -> [dsts], copy the entire block at src to dst.\n    for src, dsts in block_mapping.items():\n        for dst in dsts:\n            for kc, ckc in zip(cloned_key_caches, cloned_key_caches):\n                # We apply copy on the clones (simulate the reference copy)\n                pass\n            # Apply reference copy on clones to reflect the intended behavior.\n            for i_layer in range(num_layers):\n                cloned_key_caches[i_layer][dst].copy_(cloned_key_caches[i_layer][src])\n                cloned_value_caches[i_layer][dst].copy_(cloned_value_caches[i_layer][src])\n\n    # Compare results\n    for kc, ckc in zip(key_caches, cloned_key_caches):\n        assert kc.shape == ckc.shape, \"Key cache shape mismatch after copy\"\n        assert torch.allclose(kc, ckc, rtol=1e-3, atol=1e-6), \"Key cache content mismatch\"\n\n    for vc, cvc in zip(value_caches, cloned_value_caches):\n        assert vc.shape == cvc.shape, \"Value cache shape mismatch after copy\"\n        assert torch.allclose(vc, cvc, rtol=1e-3, atol=1e-6), \"Value cache content mismatch\"\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA is required for kernel tests\")\ndef test_copy_blocks_performance_vs_baseline():\n    \"\"\"\n    Simple performance test: ensure the new kernel is not worse than a na\u00efve\n    PyTorch-level block-copy baseline by more than 20%.\n\n    The commit message indicates the implementation is an optimization. While no\n    numeric speedup is given, we assert the optimized kernel should not be\n    significantly slower than a direct tensor-assignment baseline.\n    \"\"\"\n    workload = setup_workload()\n    num_layers = workload[\"num_layers\"]\n    num_heads = workload[\"num_heads\"]\n    head_size = workload[\"head_size\"]\n    block_size = workload[\"block_size\"]\n    num_blocks = workload[\"num_blocks\"]\n    dtype = workload[\"dtype\"]\n    block_mapping = workload[\"block_mapping\"]\n\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n\n    # Prepare caches for kernel run\n    key_caches_kernel = [torch.randn(size=key_cache_shape, dtype=dtype, device=\"cuda\") for _ in range(num_layers)]\n    value_caches_kernel = [torch.randn(size=value_cache_shape, dtype=dtype, device=\"cuda\") for _ in range(num_layers)]\n\n    # Prepare clones for baseline run (use identical initial data to be fair)\n    key_caches_baseline = [kc.clone() for kc in key_caches_kernel]\n    value_caches_baseline = [vc.clone() for vc in value_caches_kernel]\n\n    # Warm up\n    cache_ops.copy_blocks(key_caches_kernel, value_caches_kernel, block_mapping)\n    torch.cuda.synchronize()\n\n    def run_kernel():\n        cache_ops.copy_blocks(key_caches_kernel, value_caches_kernel, block_mapping)\n        torch.cuda.synchronize()\n\n    def run_baseline():\n        # Baseline: for each layer, for each src->dst copy via tensor assignment.\n        for layer_idx in range(num_layers):\n            for src, dsts in block_mapping.items():\n                for dst in dsts:\n                    # Direct index assignment on CUDA tensors\n                    key_caches_baseline[layer_idx][dst].copy_(key_caches_baseline[layer_idx][src])\n                    value_caches_baseline[layer_idx][dst].copy_(value_caches_baseline[layer_idx][src])\n        torch.cuda.synchronize()\n\n    kernel_time = timeit.timeit(run_kernel, number=1)\n    baseline_time = timeit.timeit(run_baseline, number=1)\n\n    # Accept up to 20% slower due to runtime variance, otherwise expect kernel to be comparable or faster.\n    assert kernel_time <= baseline_time * 1.2, (\n        f\"Optimized kernel slower than baseline: kernel={kernel_time:.6f}s, \"\n        f\"baseline={baseline_time:.6f}s (allowed 20% tolerance)\"\n    )\n\n\ndef test_cache_engine_copy_invokes_copy_blocks(monkeypatch):\n    \"\"\"\n    Test that CacheEngine.copy extracts key/value caches and calls cache_ops.copy_blocks\n    with two separate lists (key_caches, value_caches). This checks the changed\n    CacheEngine.copy behavior without invoking GPU allocations.\n    \"\"\"\n    # Create a minimal fake CacheEngine instance without running __init__\n    CE = cache_engine.CacheEngine  # actual class\n    instance = object.__new__(CE)\n\n    # Create fake gpu_cache: list of (key_cache, value_cache) pairs.\n    # These can be CPU tensors because we only verify the arguments passed to cache_ops.copy_blocks.\n    key_cache0 = torch.randn(2, 3, dtype=torch.float32)  # shapes arbitrary\n    key_cache1 = torch.randn(2, 3, dtype=torch.float32)\n    value_cache0 = torch.randn(2, 3, dtype=torch.float32)\n    value_cache1 = torch.randn(2, 3, dtype=torch.float32)\n    instance.gpu_cache = [(key_cache0, value_cache0), (key_cache1, value_cache1)]\n\n    called = {\"args\": None, \"kwargs\": None}\n\n    def fake_copy_blocks(key_caches, value_caches, src_to_dsts):\n        # record the call for assertions\n        called[\"args\"] = (key_caches, value_caches, src_to_dsts)\n        called[\"kwargs\"] = {}\n        # Do nothing else\n\n    monkeypatch.setattr(cache_ops, \"copy_blocks\", fake_copy_blocks)\n\n    # Provide a mapping to copy\n    src_to_dsts = {0: [1]}\n\n    # Call the method under test\n    CE.copy(instance, src_to_dsts)\n\n    assert called[\"args\"] is not None, \"cache_ops.copy_blocks was not called\"\n    key_caches_arg, value_caches_arg, mapping_arg = called[\"args\"]\n\n    # Check that keys and values lists extracted properly and maintain order\n    assert isinstance(key_caches_arg, list) and isinstance(value_caches_arg, list)\n    assert key_caches_arg == [key_cache0, key_cache1]\n    assert value_caches_arg == [value_cache0, value_cache1]\n    assert mapping_arg == src_to_dsts\n\n\ndef _reference_sample_from_generation_tokens(\n    seq_ids: List[int],\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    seq_logprobs: List[float],\n    sampling_params: Any,\n):\n    \"\"\"\n    Reference implementation (pure PyTorch + Python) of the beam-search branch\n    of _sample_from_generation_tokens to validate the corrected indexing.\n    This mirrors the intent of the function in the module.\n    \"\"\"\n    # Convert inputs to torch tensors where needed\n    seq_logprobs_t = torch.tensor(seq_logprobs, dtype=logprobs.dtype, device=logprobs.device)\n    combined = logprobs + seq_logprobs_t.unsqueeze(dim=1)  # shape [len(seq_ids), vocab]\n    vocab_size = combined.size(-1)\n    beam_width = len(seq_ids)\n\n    flat = combined.flatten()\n    topk_vals, topk_idx = torch.topk(flat, beam_width)\n    topk_idx = topk_idx.tolist()\n    seq_idx = [i // vocab_size for i in topk_idx]\n    beam_seq_ids = [seq_ids[i] for i in seq_idx]\n    token_ids = [i % vocab_size for i in topk_idx]\n\n    beam_outputs = {}\n    outstanding_beams = []\n    for seq_id, token_id in zip(beam_seq_ids, token_ids):\n        if seq_id not in beam_outputs:\n            beam_outputs[seq_id] = (seq_id, token_id)\n        else:\n            outstanding_beams.append((seq_id, token_id))\n\n    for seq_id in seq_ids:\n        if seq_id not in beam_outputs:\n            # take replacement from outstanding\n            beam_outputs[seq_id] = outstanding_beams.pop()\n    assert not outstanding_beams\n\n    parent_seq_ids = [beam_outputs[seq_id][0] for seq_id in seq_ids]\n    next_token_ids = [beam_outputs[seq_id][1] for seq_id in seq_ids]\n    return parent_seq_ids, next_token_ids\n\n\ndef test_sample_from_generation_tokens_beam_indexing():\n    \"\"\"\n    Test that _sample_from_generation_tokens returns identical results to a\n    reference implementation for beam search. This verifies the bug fix in\n    indexing (conversion from topk indices to seq/token ids).\n    \"\"\"\n    # Build a small but non-trivial scenario\n    vocab_size = 50\n    seq_ids = [10, 11, 12, 13]  # four sequences in group\n    num_seqs = len(seq_ids)\n\n    # Create logprobs so that multiple topk picks come from same sequence to force\n    # outstanding_beams behavior (i.e., repeated seq indices among topk).\n    # We'll construct combined scores where seq 10 has highest values concentrated.\n    rng = np.random.RandomState(1234)\n    base = torch.tensor(rng.randn(num_seqs, vocab_size), dtype=torch.float32)\n    # Make seq 0 have very large values in several top positions\n    base[0, :10] += 50.0\n    # Convert to \"logprobs\" by applying log softmax to make values comparable\n    logprobs = torch.log_softmax(base, dim=-1)\n\n    # For probs (softmax probabilities) convert logprobs to probabilities\n    probs = torch.softmax(logprobs, dim=-1)\n\n    seq_logprobs = [0.0 for _ in seq_ids]\n\n    class DummySamplingParams:\n        use_beam_search = True\n        n = num_seqs\n        temperature = 0.0\n        num_logprobs = 0\n\n    parent_ref, tokens_ref = _reference_sample_from_generation_tokens(\n        seq_ids, probs, logprobs, seq_logprobs, DummySamplingParams()\n    )\n\n    parent_new, tokens_new = _sample_from_generation_tokens(\n        seq_ids, probs, logprobs, seq_logprobs, DummySamplingParams()\n    )\n\n    # Structural comparisons\n    assert isinstance(parent_new, list)\n    assert isinstance(tokens_new, list)\n    assert parent_new == parent_ref, f\"parent_seq_ids mismatch: {parent_new} vs {parent_ref}\"\n    assert tokens_new == tokens_ref, f\"token ids mismatch: {tokens_new} vs {tokens_ref}\"\n\n\ndef test_sample_from_generation_tokens_greedy_assertion():\n    \"\"\"\n    Edge case: when temperature == 0.0 (greedy) the function expects len(seq_ids) == 1.\n    Ensure assertion triggers when violating this precondition.\n    \"\"\"\n    vocab_size = 16\n    seq_ids = [1, 2]  # >1 sequences\n    num_seqs = len(seq_ids)\n    # simple uniform probabilities/logprobs\n    probs = torch.full((num_seqs, vocab_size), 1.0 / vocab_size, dtype=torch.float32)\n    logprobs = torch.log(probs)\n    seq_logprobs = [0.0 for _ in seq_ids]\n\n    class DummySamplingParams:\n        use_beam_search = False\n        n = 1\n        temperature = 0.0\n        num_logprobs = 0\n\n    with pytest.raises(AssertionError):\n        _sample_from_generation_tokens(seq_ids, probs, logprobs, seq_logprobs, DummySamplingParams())\n\n\ndef test_benchmark_sampling_params_integration():\n    \"\"\"\n    Ensure the benchmark changes (new CLI arguments --n and --use-beam-search)\n    would result in SamplingParams constructed with the expected fields.\n\n    We cannot (and should not) invoke the whole benchmark script in tests;\n    instead test SamplingParams.from_dict behavior for the fields that the\n    benchmark populates.\n    \"\"\"\n    # Example where use_beam_search is True: temperature must be 0.0 by the benchmark logic\n    params_dict = {\n        \"n\": 5,\n        \"temperature\": 0.0 if True else 1.0,\n        \"top_p\": 0.9,\n        \"use_beam_search\": True,\n        \"stop_token_ids\": set(),\n        \"max_num_steps\": 10,\n    }\n    params = SamplingParams.from_dict(params_dict)\n    assert params.n == 5\n    assert params.use_beam_search is True\n    # When use_beam_search is True the benchmark sets temperature 0.0 to indicate greedy/beam\n    assert params.temperature == pytest.approx(0.0, rel=1e-6)\n\n    # Example where use_beam_search is False: temperature should be 1.0 as per changed logic\n    params_dict2 = {\n        \"n\": 3,\n        \"temperature\": 0.0 if False else 1.0,\n        \"top_p\": 1.0,\n        \"use_beam_search\": False,\n        \"stop_token_ids\": set(),\n        \"max_num_steps\": 5,\n    }\n    params2 = SamplingParams.from_dict(params_dict2)\n    assert params2.n == 3\n    assert params2.use_beam_search is False\n    assert params2.temperature == pytest.approx(1.0, rel=1e-6)\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-q\"])"], "duration_changes": [{"base": [0.0017805099487304688], "head": [0.0015251636505126953], "main": [0.0008852481842041016]}], "human_performance": 1.1674222291699234, "version": "python==unknown;arch=x86_64;image=ayushnangia16/nvidia-vllm-docker:latest;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "Implement block copy kernel to optimize beam search (#32)", "setup_commands": [], "install_commands": [], "notes": null}
