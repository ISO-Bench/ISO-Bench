{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-ad8d696", "created_at": "2024-04-22T21:11:06+00:00", "base_commit": "3d925165f2b18379640a63fbb42de95440d63b64", "head_commit": "ad8d696a99ca1eee19f1404e16e8e82df592ff85", "patch": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 9588a1bea..a25112385 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -540,7 +540,7 @@ def test_decode_schedule_preempted():\n     curr_loras = None\n     for i in range(3):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         running.append(seq_group)\n     scheduler.block_manager.can_append_slots = MagicMock()\n@@ -581,7 +581,7 @@ def test_decode_swap_beam_search():\n     budget = create_token_budget()\n     for i in range(3):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         running.append(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         budget.add_num_seqs(seq_group.request_id,\n@@ -629,7 +629,7 @@ def test_schedule_decode_blocks_to_copy_update():\n     running = deque()\n     policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n     curr_loras = None\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     running.append(seq_group)\n \n@@ -659,7 +659,7 @@ def test_schedule_swapped_simple():\n     curr_loras = None\n     blocks_to_swap_out = {}\n     _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     scheduler._swap_out(seq_group, blocks_to_swap_out)\n     swapped.append(seq_group)\n@@ -687,7 +687,7 @@ def test_schedule_swapped_max_token_budget():\n     blocks_to_swap_out = {}\n     for _ in range(2):\n         _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -721,7 +721,7 @@ def test_schedule_swapped_max_seqs():\n     blocks_to_swap_out = {}\n     for i in range(4):\n         _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -759,7 +759,7 @@ def test_schedule_swapped_max_loras():\n                                                lora_name=str(i),\n                                                lora_int_id=i + 1,\n                                                lora_local_path=\"abc\"))\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -783,7 +783,7 @@ def test_schedule_swapped_cannot_swap_in():\n     blocks_to_swap_out = {}\n     for _ in range(2):\n         _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-        scheduler._allocate_and_set_running(seq_group, 60)\n+        scheduler._allocate_and_set_running(seq_group)\n         append_new_token_seq_group(60, seq_group, 1)\n         scheduler._swap_out(seq_group, blocks_to_swap_out)\n         swapped.append(seq_group)\n@@ -808,7 +808,7 @@ def test_schedule_swapped_blocks_to_copy():\n     policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n     curr_loras = None\n     _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n-    scheduler._allocate_and_set_running(seq_group, 60)\n+    scheduler._allocate_and_set_running(seq_group)\n     append_new_token_seq_group(60, seq_group, 1)\n     blocks_to_swap_out = {}\n     scheduler._swap_out(seq_group, blocks_to_swap_out)\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 419855062..8d7db09bb 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -297,7 +297,6 @@ class Scheduler:\n \n     def add_seq_group(self, seq_group: SequenceGroup) -> None:\n         # Add sequence groups to the waiting queue.\n-        logger.debug(f\"add_seq_group {seq_group.request_id}\")\n         self.waiting.append(seq_group)\n \n     def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n@@ -427,7 +426,6 @@ class Scheduler:\n                         swapped_out.append(seq_group)\n                     break\n             else:\n-                logger.debug(f\"append slot for {seq_group}\")\n                 self._append_slots(seq_group, blocks_to_copy)\n                 is_prefill = seq_group.is_prefill()\n                 if is_prefill:\n@@ -659,7 +657,7 @@ class Scheduler:\n             if curr_loras is not None and lora_int_id > 0:\n                 curr_loras.add(lora_int_id)\n             waiting_queue.popleft()\n-            self._allocate_and_set_running(seq_group, num_new_tokens)\n+            self._allocate_and_set_running(seq_group)\n             seq_groups.append(\n                 ScheduledSequenceGroup(seq_group=seq_group,\n                                        token_chunk_size=num_new_tokens))\n@@ -952,8 +950,7 @@ class Scheduler:\n         self.running = deque(seq_group for seq_group in self.running\n                              if not seq_group.is_finished())\n \n-    def _allocate_and_set_running(self, seq_group: SequenceGroup,\n-                                  num_new_tokens: int) -> None:\n+    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:\n         self.block_manager.allocate(seq_group)\n         for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n             seq.status = SequenceStatus.RUNNING", "test_patch": "", "efficiency_test": ["#!/usr/bin/env python3\n\"\"\"\nPerformance test for commit: ad8d696a99ca1eee19f1404e16e8e82df592ff85\nMessage: [Core] Scheduler perf fix (#4270)\n\nThis script measures the performance impact of removing the unused num_new_tokens\nparameter from _allocate_and_set_running method in the vLLM scheduler.\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport time\nimport importlib\nfrom typing import Dict, Any, Tuple, Optional, List\nfrom collections import deque\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport torch\n\n# =======================\n# Determinism Setup\n# =======================\ndef ensure_determinism():\n    torch.manual_seed(42)\n    np.random.seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(42)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.backends.cuda.matmul.allow_tf32 = False\n        torch.backends.cudnn.allow_tf32 = False\n\n# =======================\n# Hardware Detection\n# =======================\ndef detect_hardware() -> Dict[str, Any]:\n    hw_info = {}\n    if torch.cuda.is_available():\n        hw_info[\"device\"] = \"cuda\"\n        hw_info[\"device_name\"] = torch.cuda.get_device_name()\n        hw_info[\"capability\"] = torch.cuda.get_device_capability()\n        hw_info[\"memory_gb\"] = torch.cuda.get_device_properties(0).total_memory / 1e9\n    else:\n        hw_info[\"device\"] = \"cpu\"\n        hw_info[\"device_name\"] = \"CPU\"\n        hw_info[\"memory_gb\"] = 0\n    return hw_info\n\n# =======================\n# Import Resolution\n# =======================\ndef resolve_target() -> Tuple[Any, str]:\n    \"\"\"Resolve the optimization target from environment or metadata.\"\"\"\n    \n    # Priority 1: Environment variables\n    module_path = os.getenv(\"PROB_MODULE\", \"vllm.core.scheduler\")\n    symbol_name = os.getenv(\"PROB_SYMBOL\", \"Scheduler\")\n    \n    # Import with error handling\n    try:\n        module = importlib.import_module(module_path)\n        target = getattr(module, symbol_name)\n        \n        fq_name = f\"{module_path}.{symbol_name}\"\n        return target, fq_name\n        \n    except (ImportError, AttributeError) as e:\n        error_data = {\n            \"target_resolved\": False,\n            \"error\": str(e),\n            \"attempted_module\": module_path,\n            \"attempted_symbol\": symbol_name\n        }\n        print(json.dumps(error_data))\n        sys.exit(1)\n\n# =======================\n# Workload Setup\n# =======================\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload for the scheduler optimization.\"\"\"\n    ensure_determinism()\n    hw_info = detect_hardware()\n    \n    # Import required vLLM components\n    try:\n        from vllm.config import CacheConfig, SchedulerConfig\n        from vllm.core.scheduler import Scheduler\n        from vllm.sequence import SequenceGroup, Sequence, SequenceData, SequenceStatus\n        from vllm.sampling_params import SamplingParams\n        from vllm.block import LogicalTokenBlock\n    except ImportError as e:\n        print(json.dumps({\"target_resolved\": False, \"error\": f\"Failed to import vLLM components: {e}\"}))\n        sys.exit(1)\n    \n    device = torch.device(hw_info[\"device\"])\n    dtype = torch.float16 if hw_info[\"device\"] == \"cuda\" else torch.float32\n    \n    # Create scheduler configuration\n    block_size = 16\n    num_gpu_blocks = 1024\n    num_cpu_blocks = 512\n    max_num_seqs = 256\n    max_model_len = 2048\n    max_num_batched_tokens = 2048\n    \n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens=max_num_batched_tokens,\n        max_num_seqs=max_num_seqs,\n        max_model_len=max_model_len\n    )\n    \n    cache_config = CacheConfig(\n        block_size=block_size,\n        gpu_memory_utilization=0.9,\n        swap_space_bytes=0,\n        cache_dtype=\"auto\"\n    )\n    cache_config.num_gpu_blocks = num_gpu_blocks\n    cache_config.num_cpu_blocks = num_cpu_blocks\n    \n    # Create scheduler instance\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    \n    # Create sequence groups to simulate realistic workload\n    seq_groups = []\n    num_requests = 64  # Simulate many concurrent requests\n    \n    for i in range(num_requests):\n        # Create sequence with varying prompt lengths\n        prompt_length = 128 + (i % 8) * 64  # Vary from 128 to 640 tokens\n        seq_id = i\n        \n        # Create sequence data\n        prompt_token_ids = list(range(prompt_length))\n        seq_data = SequenceData(prompt_token_ids)\n        \n        # Create sequence\n        seq = Sequence(\n            seq_id=seq_id,\n            prompt=prompt_token_ids,\n            prompt_token_ids=prompt_token_ids,\n            block_size=block_size\n        )\n        seq.data = seq_data\n        seq.status = SequenceStatus.WAITING\n        \n        # Create sampling params\n        sampling_params = SamplingParams(\n            temperature=0.7,\n            top_p=0.9,\n            max_tokens=128\n        )\n        \n        # Create sequence group\n        seq_group = SequenceGroup(\n            request_id=str(i),\n            seqs=[seq],\n            sampling_params=sampling_params,\n            arrival_time=time.time() - (num_requests - i) * 0.01  # Stagger arrival times\n        )\n        \n        seq_groups.append(seq_group)\n    \n    data = {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"hw_info\": hw_info,\n        \"scheduler\": scheduler,\n        \"seq_groups\": seq_groups,\n        \"scheduler_config\": scheduler_config,\n        \"cache_config\": cache_config\n    }\n    \n    return data\n\n# =======================\n# Experiment Execution\n# =======================\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"Execute the optimized _allocate_and_set_running operation.\"\"\"\n    scheduler = data[\"scheduler\"]\n    seq_groups = data[\"seq_groups\"]\n    \n    # Clear scheduler state\n    scheduler.waiting.clear()\n    scheduler.running.clear()\n    scheduler.swapped.clear()\n    \n    # Add sequence groups to waiting queue\n    for seq_group in seq_groups:\n        # Reset sequence status\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.WAITING\n        scheduler.waiting.append(seq_group)\n    \n    # Measure the performance of _allocate_and_set_running calls\n    # This happens during scheduling when moving sequences from waiting to running\n    allocated_groups = []\n    \n    while scheduler.waiting and len(allocated_groups) < 32:  # Process batch of requests\n        seq_group = scheduler.waiting.popleft()\n        \n        # This is the optimized method - no longer takes num_new_tokens parameter\n        scheduler._allocate_and_set_running(seq_group)\n        \n        allocated_groups.append(seq_group)\n        scheduler.running.append(seq_group)\n    \n    return {\n        \"num_allocated\": len(allocated_groups),\n        \"num_remaining\": len(scheduler.waiting),\n        \"num_running\": len(scheduler.running)\n    }\n\n# =======================\n# Result I/O\n# =======================\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for reference comparison.\"\"\"\n    torch.save({\"type\": \"dict\", \"data\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result.\"\"\"\n    data = torch.load(filepath)\n    return data.get(\"data\", data)\n\n# =======================\n# Equivalence Checking\n# =======================\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Verify functional equivalence.\"\"\"\n    assert isinstance(current_result, dict)\n    assert isinstance(reference_result, dict)\n    \n    # Check that the same number of sequences were allocated\n    assert current_result[\"num_allocated\"] == reference_result[\"num_allocated\"], \\\n        f\"Allocated count mismatch: {current_result['num_allocated']} vs {reference_result['num_allocated']}\"\n    \n    assert current_result[\"num_remaining\"] == reference_result[\"num_remaining\"], \\\n        f\"Remaining count mismatch: {current_result['num_remaining']} vs {reference_result['num_remaining']}\"\n    \n    assert current_result[\"num_running\"] == reference_result[\"num_running\"], \\\n        f\"Running count mismatch: {current_result['num_running']} vs {reference_result['num_running']}\"\n\n# =======================\n# Timing Implementation\n# =======================\ndef time_cpu(func, warmup=3, iterations=10) -> Tuple[Any, Dict[str, float]]:\n    \"\"\"Time CPU operations.\"\"\"\n    # Warmup\n    for _ in range(warmup):\n        _ = func()\n    \n    # Timing\n    times_ms = []\n    for _ in range(iterations):\n        start = time.perf_counter()\n        result = func()\n        end = time.perf_counter()\n        times_ms.append((end - start) * 1000)\n    \n    # Statistics\n    times_ms.sort()\n    stats = {\n        \"avg_ms\": sum(times_ms) / len(times_ms),\n        \"p50_ms\": times_ms[len(times_ms) // 2],\n        \"p95_ms\": times_ms[int(len(times_ms) * 0.95) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"p99_ms\": times_ms[int(len(times_ms) * 0.99) - 1] if len(times_ms) > 1 else times_ms[0],\n        \"min_ms\": times_ms[0],\n        \"max_ms\": times_ms[-1],\n        \"std_ms\": np.std(times_ms) if len(times_ms) > 1 else 0.0\n    }\n    \n    return result, stats\n\n# =======================\n# Main Test Function\n# =======================\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"Main test entry point.\"\"\"\n    \n    # Setup\n    data = setup()\n    hw_info = data[\"hw_info\"]\n    \n    # Since this is a CPU-bound scheduler operation, we always use CPU timing\n    warmup = 5\n    iters = 20\n    \n    result, timing_stats = time_cpu(lambda: experiment(data), warmup=warmup, iterations=iters)\n    avg_ms = timing_stats[\"avg_ms\"]\n    p50_ms = timing_stats[\"p50_ms\"]\n    p95_ms = timing_stats[\"p95_ms\"]\n    \n    # Reference handling\n    commit_hash = os.getenv(\"COMMIT_HASH\", \"ad8d696a99ca1eee19f1404e16e8e82df592ff85\")\n    impl_tag = os.getenv(\"IMPL_TAG\", \"child\")\n    ref_file = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    \n    if reference:\n        store_result(result, ref_file)\n    \n    if eqcheck and os.path.exists(ref_file):\n        ref_result = load_result(ref_file)\n        check_equivalence(result, ref_result)\n    \n    # Output compact JSON schema\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": \"cpu\",  # Scheduler operations are CPU-bound\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": avg_ms,\n        \"p50_ms\": p50_ms,\n        \"p95_ms\": p95_ms,\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"exact\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary))\n    \n    return avg_ms / 1000.0\n\n# =======================\n# Entry Point\n# =======================\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--eqcheck\", action=\"store_true\")\n    parser.add_argument(\"--reference\", action=\"store_true\")\n    parser.add_argument(\"--prefix\", type=str, default=\"\")\n    args = parser.parse_args()\n    \n    run_test(args.eqcheck, args.reference, args.prefix)"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Scheduler perf fix (#4270)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/3d925165_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/ad8d696a_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-25ebed2", "created_at": "2024-12-15T21:33:00+00:00", "base_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310", "head_commit": "25ebed2f8ca6d747d63f2be9ede023c561851ac8", "patch": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex abcd4b007..67166fb05 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -118,6 +118,12 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n+        self.arange_np = np.arange(max(self.max_num_reqs, self.max_model_len),\n+                                   dtype=np.int32)\n+        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n+        # a faster version of creating a new tensor every time. Thus, we should\n+        # not make any assumptions about the values in these tensors.\n         self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n@@ -269,11 +275,13 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n+        req_indices = np.repeat(self.arange_np[:num_reqs],\n+                                num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n+        arange = np.concatenate(\n+            [self.arange_np[:n] for n in num_scheduled_tokens])\n \n         # Get positions.\n         positions_np = self.positions_np[:total_num_scheduled_tokens]", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport random\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom vllm.attention import Attention\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                         SchedulerConfig, VllmConfig, set_current_vllm_config)\nfrom vllm.distributed.parallel_state import (init_distributed_environment,\n                                             initialize_model_parallel)\nfrom vllm.model_executor.layers.mamba.mamba_mixer2 import MambaMixer2\nfrom vllm.platforms import current_platform\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import GiB_bytes, update_environment_variables\nfrom vllm.v1.core.kv_cache_utils import (estimate_max_model_len,\n                                         get_kv_cache_config)\nfrom vllm.v1.core.sched.output import (CachedRequestData, NewRequestData,\n                                       SchedulerOutput)\nfrom vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n                                        KVCacheGroupSpec, KVCacheTensor)\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.gpu_input_batch import InputBatch\nfrom vllm.v1.worker.gpu_model_runner import GPUModelRunner\n\nBLOCK_SIZE = 16\nNUM_BLOCKS = 10\nDEVICE = current_platform.device_type\n\n\ndef initialize_kv_cache(runner: GPUModelRunner):\n    \"\"\"\n    Only perform necessary steps in GPUModelRunner.initialize_kv_cache()\n    \"\"\"\n    attn_spec = FullAttentionSpec(\n        block_size=BLOCK_SIZE,\n        num_kv_heads=runner.model_config.get_num_kv_heads(\n            runner.parallel_config),\n        head_size=runner.model_config.get_head_size(),\n        dtype=runner.kv_cache_dtype,\n        use_mla=False,\n    )\n    tensor_size = attn_spec.page_size_bytes * NUM_BLOCKS\n    kv_cache_config = KVCacheConfig(\n        num_blocks=NUM_BLOCKS,\n        kv_cache_tensors=[\n            KVCacheTensor(size=tensor_size, shared_by=[\"layer.0\"]),\n        ],\n        kv_cache_groups=[\n            KVCacheGroupSpec(layer_names=[\"layer.0\"], kv_cache_spec=attn_spec)\n        ],\n    )\n    runner.kv_cache_config = kv_cache_config\n    runner.input_batch = InputBatch(\n        max_num_reqs=runner.max_num_reqs,\n        max_model_len=runner.max_model_len,\n        max_num_batched_tokens=runner.max_num_tokens,\n        device=runner.device,\n        pin_memory=runner.pin_memory,\n        vocab_size=runner.model_config.get_vocab_size(),\n        block_sizes=[\n            kv_cache_config.kv_cache_groups[0].kv_cache_spec.block_size\n        ],\n    )\n    runner.initialize_attn_backend(kv_cache_config)\n\n\ndef get_vllm_config():\n    scheduler_config = SchedulerConfig(\n        max_num_seqs=10,\n        max_num_batched_tokens=512,\n        max_model_len=512,\n    )\n    model_config = ModelConfig(\n        model=\"facebook/opt-125m\",\n        dtype=\"float16\",\n        seed=42,\n    )\n    cache_config = CacheConfig(\n        block_size=BLOCK_SIZE,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=\"auto\",\n    )\n    parallel_config = ParallelConfig()\n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        scheduler_config=scheduler_config,\n        parallel_config=parallel_config,\n    )\n    return vllm_config\n\n\n@pytest.fixture\ndef model_runner():\n    vllm_config = get_vllm_config()\n    model_config = vllm_config.model_config\n    num_heads = model_config.get_num_kv_heads(vllm_config.parallel_config)\n    head_size = model_config.get_head_size()\n    vllm_config.compilation_config.static_forward_context[\n        \"layer.0\"] = Attention(num_heads, head_size, 0.1)\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    initialize_kv_cache(runner)\n    return runner\n\n\nmodel_runner_2 = model_runner\n\n\ndef _schedule_new_request(*req_ids: str) -> SchedulerOutput:\n    new_reqs = []\n    num_scheduled_tokens = {}\n    total_num_scheduled_tokens = 0\n    for req_id in req_ids:\n        new_reqs.append(\n            NewRequestData(\n                req_id=req_id,\n                prompt_token_ids=[1, 2, 3],\n                mm_kwargs=[],\n                mm_hashes=[],\n                mm_positions=[],\n                sampling_params=SamplingParams(),\n                pooling_params=None,\n                block_ids=([0], ),\n                num_computed_tokens=0,\n                lora_request=None,\n            ))\n        num_scheduled_tokens[req_id] = 3\n        total_num_scheduled_tokens += num_scheduled_tokens[req_id]\n\n    return SchedulerOutput(\n        scheduled_new_reqs=new_reqs,\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens=num_scheduled_tokens,\n        total_num_scheduled_tokens=total_num_scheduled_tokens,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n\ndef _is_req_scheduled(model_runner, req_id: str) -> bool:\n    return req_id in model_runner.input_batch.req_id_to_index\n\n\ndef _is_req_added(model_runner, req_id: str) -> bool:\n    return req_id in model_runner.requests\n\n\ndef _is_sampling_metadata_changed(model_runner,\n                                  sampling_metadata_before: SamplingMetadata):\n    return model_runner.input_batch.sampling_metadata is not (\n        sampling_metadata_before)\n\n\ndef _is_req_state_block_table_match(model_runner, req_id: str) -> bool:\n    req_index = model_runner.input_batch.req_id_to_index[req_id]\n    block_table = model_runner.input_batch.block_table[0]\n    req_state = model_runner.requests[req_id]\n    if block_table.num_blocks_per_row[req_index] != len(\n            req_state.block_ids[0]):\n        return False\n    num_blocks = block_table.num_blocks_per_row[req_index]\n    return (block_table.block_table_np[req_index, :num_blocks] ==\n            req_state.block_ids[0]).all()\n\n\ndef test_update_states_new_request(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_update_states_request_finished(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # finish req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={},\n        total_num_scheduled_tokens=0,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids={req_id},\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert not _is_req_added(model_runner, req_id)\n    assert not _is_req_scheduled(model_runner, req_id)\n\n\ndef test_update_states_request_resumed(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # unschedule req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={},\n        total_num_scheduled_tokens=0,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert not _is_req_scheduled(model_runner, req_id)\n\n    # resume req\n    cached_req_data = CachedRequestData(\n        req_ids=[req_id],\n        resumed_from_preemption=[False],\n        new_token_ids=[[]],\n        new_block_ids=([[0]], ),\n        num_computed_tokens=[0],\n    )\n\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=cached_req_data,\n        num_scheduled_tokens={req_id: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_get_nans_in_logits(model_runner, dist_init):\n    req_ids = (\"req_0\", \"req_1\")\n\n    scheduler_output = _schedule_new_request(*req_ids)\n    model_runner._update_states(scheduler_output)\n\n    logits = torch.tensor([\n        [1.0, 2.0, 3.0],\n        [3.0, 2.0, 1.0],\n    ], device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 0, \"req_1\": 0}\n\n    logits = torch.tensor([\n        [1.0, float('nan'), 3.0],\n        [4.0, float('nan'), float('nan')],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 1, \"req_1\": 2}\n\n    logits = torch.tensor([\n        [1.0, 2.0, 3.0],\n        [4.0, float('nan'), float('nan')],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 0, \"req_1\": 2}\n\n    result = model_runner._get_nans_in_logits(logits=None)\n    assert result == {\"req_0\": 0, \"req_1\": 0}\n\n    logits = torch.tensor([\n        [1.0, float('nan'), 3.0],\n    ], device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {'req_0': 1, 'req_1': 0}\n\n    logits = torch.tensor([\n        [float('nan'), float('nan'), 2.0],\n        [1.0, 2.0, 3.0],\n        [float('nan'), 2.0, 3.0],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {'req_0': 2, 'req_1': 0}\n\n\ndef test_update_states_no_changes(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # schedule req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={req_id: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert not _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_update_states_request_unscheduled(model_runner, dist_init):\n    req_ids = (\"req_0\", \"req_1\")\n\n    # new reqs\n    scheduler_output = _schedule_new_request(*req_ids)\n\n    model_runner._update_states(scheduler_output)\n\n    assert _is_req_added(model_runner, req_ids[0])\n    assert _is_req_scheduled(model_runner, req_ids[0])\n\n    assert _is_req_added(model_runner, req_ids[1])\n    assert _is_req_scheduled(model_runner, req_ids[1])\n\n    # unschedule req_1\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={req_ids[0]: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n\n    assert _is_req_added(model_runner, req_ids[0])\n    assert _is_req_scheduled(model_runner, req_ids[0])\n\n    assert _is_req_added(model_runner, req_ids[1])\n    assert not _is_req_scheduled(model_runner, req_ids[1])\n\n\ndef test_kv_cache_stride_order(monkeypatch, model_runner):\n    # This test checks if GPUModelRunner initializes correctly when an attention\n    # backend enforces a non-default KV cache stride order.\n    n_heads = model_runner.model_config.get_num_kv_heads(\n        model_runner.parallel_config)\n    expected_kv_cache_shape = [\n        2, NUM_BLOCKS, BLOCK_SIZE, n_heads,\n        model_runner.model_config.get_head_size()\n    ]\n    # TODO mla test\n    default_stride = list(range(5))\n    # Permutation that gets you back to expected kv shape\n    rnd_stride = tuple(random.sample(default_stride, len(default_stride)))\n\n    def rnd_stride_order():\n        return rnd_stride\n\n    # Patch the attention backend class and re-trigger the KV cache creation.\n    for attn_group in model_runner._attn_group_iterator():\n        attn_backend = attn_group.backend\n        monkeypatch.setattr(attn_backend, \"get_kv_cache_stride_order\",\n                            rnd_stride_order)\n\n    model_runner.attn_groups = []\n    model_runner.initialize_kv_cache(model_runner.kv_cache_config)\n\n    # Shape is unchanged, but layout may differ\n    kv_cache_shape = model_runner.kv_caches[0].shape\n    assert list(kv_cache_shape) == expected_kv_cache_shape\n    if default_stride == rnd_stride:\n        assert all(kv.is_contiguous() for kv in model_runner.kv_caches)\n    else:\n        assert all(not kv.is_contiguous() for kv in model_runner.kv_caches)\n\n\ndef test_update_config(model_runner):\n    # Simple update\n    model_runner.update_config({\"load_config\": {\"load_format\": \"dummy\"}})\n    assert model_runner.load_config.load_format == \"dummy\"\n    # Raise error on non-existing config\n    with pytest.raises(AssertionError):\n        model_runner.update_config({\"do_not_exist_config\": \"dummy\"})\n\n\ndef test_load_model_weights_inplace(dist_init, model_runner, model_runner_2):\n    # In this test, model_runner loads model + weights in one go, while\n    # model_runner_2 loads dummy weights first then load real weights inplace\n    model_runner.load_model()\n    original_load_format = model_runner_2.load_config.load_format\n    model_runner_2.update_config({\"load_config\": {\"load_format\": \"dummy\"}})\n    model_runner_2.load_model()  # Initial model loading with dummy weights\n    assert str(model_runner.get_model().state_dict()) != str(\n        model_runner_2.get_model().state_dict())\n    model_runner_2.update_config(\n        {\"load_config\": {\n            \"load_format\": original_load_format\n        }})\n    model_runner_2.reload_weights()  # Load real weights inplace\n    assert str(model_runner.get_model().state_dict()) == str(\n        model_runner_2.get_model().state_dict())\n\n\ndef test_reload_weights_before_load_model(model_runner):\n    with pytest.raises(AssertionError):\n        model_runner.reload_weights()\n\n\ndef test_init_kv_cache_with_kv_sharing_invalid_target_layer_order():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    error_msg = f\"{layer_1} must come before the current layer\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            # initialization below will fail because target layer is invalid;\n            # the target layer needs to come before layer 1\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n                kv_sharing_target_layer_name=layer_1,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_with_kv_sharing_target_layer_not_exist():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    invalid_layer = \"model.layers.0.cross_attn.attn\"\n    error_msg = f\"{invalid_layer} is not a valid Attention layer in the model\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                # invalid layer: cross_attn.atn doesn't exist!\n                kv_sharing_target_layer_name=invalid_layer,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_with_kv_sharing_target_same_as_current():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    error_msg = f\"{layer_1} cannot be the same as the current layer\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            # initialization below will fail because target layer is invalid;\n            # the target layer needs to come before layer 1\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                kv_sharing_target_layer_name=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_without_kv_sharing():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    vllm_config = get_vllm_config()\n    with set_current_vllm_config(vllm_config):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n    # Set high context length to test max context length estimation\n    vllm_config.model_config.max_model_len = 3_000_000\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    kv_cache_spec = runner.get_kv_cache_spec()\n    assert len(kv_cache_spec) == 2\n    assert len(runner.shared_kv_cache_layers) == 0\n\n    available_memory = 20 * GiB_bytes\n    # page size for layer 0's kv_cache_spec is 32KB\n    num_expected_blocks = 327680  # 20GB / 32KB / 2 (num layers)\n    kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                          available_memory)\n    assert kv_cache_config.num_blocks == num_expected_blocks\n    assert len(kv_cache_config.kv_cache_tensors) == 2\n    assert kv_cache_config.kv_cache_tensors[0].size == available_memory // 2\n    assert kv_cache_config.kv_cache_tensors[1].size == available_memory // 2\n\n    max_context_len =\\\n        estimate_max_model_len(vllm_config, kv_cache_spec, 5 * GiB_bytes)\n    # max context len with KV sharing should be 2x as large as without\n    assert max_context_len == 1310720\n\n    # important: override tensor size to prevent large mem alloc during test\n    # this will only allocate 2 block worth of memory (2 * 32kb)\n    kv_cache_config.num_blocks = 1\n    for kv_cache_tensor in kv_cache_config.kv_cache_tensors:\n        kv_cache_tensor.size = (\n            kv_cache_spec[kv_cache_tensor.shared_by[0]].page_size_bytes)\n\n    runner.initialize_kv_cache(kv_cache_config)\n\n    layer_0_kv = vllm_ctx[layer_0].kv_cache[0]\n    layer_1_kv = vllm_ctx[layer_1].kv_cache[0]\n    # check layer 1 kv cache does NOT share memory with layer 0\n    assert id(layer_1_kv) != id(layer_0_kv)\n\n    # check layer 1 added to kv cache group's layer names\n    assert len(kv_cache_config.kv_cache_groups) == 1\n    assert len(kv_cache_config.kv_cache_groups[0].layer_names) == 2\n    assert kv_cache_config.kv_cache_groups[0].layer_names[0] == layer_0\n    assert kv_cache_config.kv_cache_groups[0].layer_names[1] == layer_1\n\n\ndef test_init_kv_cache_with_kv_sharing_valid():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    vllm_config = get_vllm_config()\n    with set_current_vllm_config(vllm_config):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                kv_sharing_target_layer_name=\"model.layers.0.self_attn.attn\",\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n    # Set high context length to test max context length estimation\n    vllm_config.model_config.max_model_len = 3_000_000\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    kv_cache_spec = runner.get_kv_cache_spec()\n    assert len(kv_cache_spec) == 1\n    assert layer_0 in kv_cache_spec\n    assert runner.shared_kv_cache_layers[layer_1] == layer_0\n\n    available_memory = 20 * GiB_bytes\n    # page size for layer 0's kv_cache_spec is 32KB\n    # with KV sharing, we can allocate (available_mem//page_size//1) blocks\n    # which is twice as many as without KV sharing\n    num_expected_blocks = 655360  # 20GB / 32KB\n    kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                          available_memory)\n    assert kv_cache_config.num_blocks == num_expected_blocks\n    assert len(kv_cache_config.kv_cache_tensors) == 1\n    # Each layer now has twice the available memory for KV cache\n    # compared to no KV sharing\n    assert kv_cache_config.kv_cache_tensors[0].size == available_memory\n\n    max_context_len =\\\n        estimate_max_model_len(vllm_config, kv_cache_spec, 5 * GiB_bytes)\n    # max context len with KV sharing should be 2x as large as without\n    assert max_context_len == 2 * 1310720\n\n    # important: override tensor size to prevent large mem alloc during test\n    # this will only allocate 1 block worth of memory (32kb)\n    kv_cache_config.num_blocks = 1\n    kv_cache_config.kv_cache_tensors[0].size =\\\n        kv_cache_spec[layer_0].page_size_bytes\n\n    runner.initialize_kv_cache(kv_cache_config)\n    kv_cache_config_after_init = runner.kv_cache_config\n\n    layer_0_kv = vllm_ctx[layer_0].kv_cache[0]\n    layer_1_kv = vllm_ctx[layer_1].kv_cache[0]\n    # check layer 1 kv cache shares memory with layer 0\n    assert id(layer_1_kv) == id(layer_0_kv)\n\n    # check layer 1 added to kv cache group's layer names\n    assert len(kv_cache_config_after_init.kv_cache_groups) == 1\n    assert len(kv_cache_config_after_init.kv_cache_groups[0].layer_names) == 2\n    assert kv_cache_config_after_init.kv_cache_groups[0].layer_names[\n        0] == layer_0\n    assert kv_cache_config_after_init.kv_cache_groups[0].layer_names[\n        1] == layer_1\n\n\ndef test_hybrid_attention_mamba_tensor_shapes(monkeypatch):\n    '''\n    The GPU model runner creates different views into the\n    KVCacheTensors for the attention and mamba layers\n    (via _reshape_kv_cache_tensors function). This test verifies\n    that the views are compatible: writing a mamba block\n    will not corrupt an attention block and vice-versa\n    '''\n\n    current_platform.seed_everything(42)\n\n    update_environment_variables({\n        'RANK': \"0\",\n        'LOCAL_RANK': \"0\",\n        'WORLD_SIZE': \"1\",\n        'MASTER_ADDR': 'localhost',\n        'MASTER_PORT': '12345',\n    })\n    init_distributed_environment()\n    initialize_model_parallel(tensor_model_parallel_size=1)\n    torch.set_default_dtype(torch.float16)\n\n    scheduler_config = SchedulerConfig(\n        max_num_seqs=10,\n        max_num_batched_tokens=512,\n        max_model_len=512,\n    )\n    model_config = ModelConfig(\n        model=\"ibm-granite/granite-4.0-tiny-preview\",\n        dtype=\"float16\",\n    )\n    cache_config = CacheConfig(\n        block_size=BLOCK_SIZE,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=\"auto\",\n    )\n    parallel_config = ParallelConfig()\n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        scheduler_config=scheduler_config,\n        parallel_config=parallel_config,\n    )\n\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    layer_2 = \"model.layers.2.mixer\"\n    layer_3 = \"model.layers.3.mixer\"\n    layer_4 = \"model.layers.4.mixer\"\n    layer_5 = \"model.layers.5.mixer\"\n\n    with set_current_vllm_config(vllm_config), monkeypatch.context() as m:\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLASHINFER\")\n        hf_config = vllm_config.model_config.hf_config\n        fwd_context = {}\n        for key in [layer_0, layer_1]:\n            fwd_context[key] = Attention(\n                num_heads=model_config.get_num_attention_heads(\n                    parallel_config),\n                num_kv_heads=model_config.get_num_kv_heads(parallel_config),\n                head_size=model_config.get_head_size(),\n                scale=1.0,\n                prefix=key,\n            )\n        for key in [layer_2, layer_3, layer_4, layer_5]:\n            fwd_context[key] = MambaMixer2(\n                hidden_size = hf_config.hidden_size,\n                ssm_state_size = hf_config.mamba_d_state,\n                conv_kernel_size = hf_config.mamba_d_conv,\n                intermediate_size = hf_config.mamba_expand *\\\n                                    hf_config.hidden_size,\n                use_conv_bias = hf_config.mamba_conv_bias,\n                use_bias = hf_config.mamba_proj_bias,\n                n_groups=hf_config.mamba_n_groups,\n                num_heads=hf_config.mamba_n_heads,\n                head_dim=hf_config.mamba_d_head,\n                rms_norm_eps=hf_config.rms_norm_eps,\n                activation=hf_config.hidden_act,\n                cache_config=cache_config,\n                model_config=model_config,\n                prefix=key,\n            )\n        # suppress var not used error\n        assert fwd_context is not None\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n\n    with monkeypatch.context() as m:\n\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLASHINFER\")\n\n        runner = GPUModelRunner(vllm_config, DEVICE)\n        kv_cache_spec = runner.get_kv_cache_spec()\n\n        available_memory = 5 * GiB_bytes\n        kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                              available_memory)\n        runner.initialize_kv_cache(kv_cache_config)\n\n        # random partition of blocks\n        # blocks0 will be assigned to attention layers\n        # blocks1 will be assigned to mamba layers\n        num_blocks = kv_cache_config.num_blocks\n        ind = np.arange(num_blocks)\n        np.random.shuffle(ind)\n        blocks0, blocks1 = ind[:(num_blocks // 2)], ind[(num_blocks // 2):]\n\n        attn_shape = vllm_ctx[layer_0].kv_cache[0].shape\n        conv_shape = vllm_ctx[layer_2].kv_cache[0][0].shape\n        ssm_shape = vllm_ctx[layer_2].kv_cache[0][1].shape\n\n        # assert we are using FlashInfer\n        assert attn_shape[0] == num_blocks\n\n        attn_blocks_constant = torch.full((len(blocks0), *attn_shape[1:]),\n                                          device=DEVICE,\n                                          fill_value=3.33)\n        conv_blocks_constant = torch.full((len(blocks1), *conv_shape[1:]),\n                                          device=DEVICE,\n                                          fill_value=6.66)\n        ssm_blocks_constant = torch.full((len(blocks1), *ssm_shape[1:]),\n                                         device=DEVICE,\n                                         fill_value=9.99)\n\n        # fill all attention blocks with constant\n        for layer in [layer_0, layer_1]:\n            vllm_ctx[layer].kv_cache[0][\n                blocks0, :] = attn_blocks_constant.detach().clone()\n\n        # fill all mamba blocks with constant\n        for layer in [layer_2, layer_3, layer_4, layer_5]:\n            vllm_ctx[layer].kv_cache[0][0][\n                blocks1, :] = conv_blocks_constant.detach().clone()\n            vllm_ctx[layer].kv_cache[0][1][\n                blocks1, :] = ssm_blocks_constant.detach().clone()\n\n        # verify attention and mamba contents are correct\n        for layer in [layer_0, layer_1]:\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][blocks0, :],\n                               attn_blocks_constant)\n        for layer in [layer_2, layer_3, layer_4, layer_5]:\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][0][blocks1, :],\n                               conv_blocks_constant)\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][1][blocks1, :],\n                               ssm_blocks_constant)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/d263bd9d_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/25ebed2f_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-2a05201", "created_at": "2024-05-04T18:45:16+00:00", "base_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4", "head_commit": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3", "patch": "diff --git a/tests/kernels/test_moe.py b/tests/kernels/test_moe.py\nindex 046f11d95..2356b9ec1 100644\n--- a/tests/kernels/test_moe.py\n+++ b/tests/kernels/test_moe.py\n@@ -77,8 +77,8 @@ def test_mixtral_moe(dtype: torch.dtype):\n     for i in range(config.num_local_experts):\n         weights = (hf_moe.experts[i].w1.weight.data,\n                    hf_moe.experts[i].w3.weight.data)\n-        vllm_moe.ws[i][:] = torch.cat(weights, dim=0)\n-        vllm_moe.w2s[i][:] = hf_moe.experts[i].w2.weight.data\n+        vllm_moe.w13_weight[i][:] = torch.cat(weights, dim=0)\n+        vllm_moe.w2_weight[i][:] = hf_moe.experts[i].w2.weight.data\n \n     # Generate input batch of dimensions [batch_size, seq_len, hidden_dim]\n     hf_inputs = torch.randn((1, 64, config.hidden_size)).to(dtype).to(\"cuda\")\ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 9ff9ba298..efa4de751 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -78,6 +78,8 @@ class MixtralMoE(nn.Module):\n         self.top_k = top_k\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size // self.tp_size\n+        self.quant_config = quant_config\n+\n         # FIXME(pcmoritz): Make this more general to support different\n         # quantization schemes\n         self.use_fp8 = isinstance(quant_config, Fp8Config)\n@@ -86,55 +88,79 @@ class MixtralMoE(nn.Module):\n             params_dtype = torch.get_default_dtype()\n         self.params_dtype = params_dtype\n \n+        # Gate always runs at half / full precision for now.\n         self.gate = ReplicatedLinear(self.hidden_size,\n                                      self.num_total_experts,\n                                      bias=False,\n                                      params_dtype=self.params_dtype,\n                                      quant_config=None)\n \n-        self.ws = nn.Parameter(\n+        if self.use_fp8:\n+            params_dtype = torch.float8_e4m3fn\n+\n+        self.w13_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         2 * self.intermediate_size,\n                         self.hidden_size,\n-                        dtype=self.params_dtype))\n-        self.w2s = nn.Parameter(\n+                        dtype=params_dtype))\n+        self.w2_weight = nn.Parameter(\n             torch.empty(self.num_total_experts,\n                         self.hidden_size,\n                         self.intermediate_size,\n-                        dtype=self.params_dtype))\n+                        dtype=params_dtype))\n \n-        set_weight_attrs(self.ws, {\n+        set_weight_attrs(self.w13_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n-        set_weight_attrs(self.w2s, {\n+        set_weight_attrs(self.w2_weight, {\n             \"weight_loader\": self.weight_loader,\n         })\n \n-        # Scaling factors for FP8 weights\n-        self.ws_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n-            requires_grad=False) if self.use_fp8 else None\n-        self.w2s_scale = nn.Parameter(\n-            torch.ones(self.num_total_experts, dtype=torch.float32),\n-            requires_grad=False) if self.use_fp8 else None\n-\n-        # Scaling factors for FP8 activations\n-        need_act_scales = (self.use_fp8\n-                           and quant_config.activation_scheme == \"static\")\n-        self.as_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n-            requires_grad=False) if need_act_scales else None\n-        self.a2s_scale = nn.Parameter(\n-            torch.zeros(1, dtype=torch.float32),\n-            requires_grad=False) if need_act_scales else None\n-\n-        if need_act_scales:\n-            set_weight_attrs(self.as_scale, {\n-                \"weight_loader\": self.weight_loader,\n-            })\n-            set_weight_attrs(self.a2s_scale, {\n-                \"weight_loader\": self.weight_loader,\n-            })\n+        # Used for fp8.\n+        self.w13_scale = None\n+        self.w2_scale = None\n+        self.a13_scale = None\n+        self.a2_scale = None\n+\n+        if self.use_fp8:\n+            # WEIGHT_SCALE (for fp8)\n+            self.w13_scale = nn.Parameter(torch.ones(self.num_total_experts,\n+                                                     dtype=torch.float32),\n+                                          requires_grad=False)\n+            self.w2_scale = nn.Parameter(torch.ones(self.num_total_experts,\n+                                                    dtype=torch.float32),\n+                                         requires_grad=False)\n+\n+            # If loading fp8 checkpoint, pass the weight loaders.\n+            # If loading an fp16 checkpoint, do not (we will quantize in\n+            #   process_weights_after_loading()\n+            if quant_config.is_checkpoint_fp8_serialized:\n+                set_weight_attrs(self.w13_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+                set_weight_attrs(self.w2_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+\n+            # ACT_SCALE (for fp8)\n+            if quant_config.activation_scheme == \"static\":\n+                if not quant_config.is_checkpoint_fp8_serialized:\n+                    raise ValueError(\n+                        \"Found static activation scheme for checkpoint that \"\n+                        \"was not serialized fp8.\")\n+                self.a13_scale = nn.Parameter(torch.zeros(\n+                    self.num_total_experts, dtype=torch.float32),\n+                                              requires_grad=False)\n+                self.a2_scale = nn.Parameter(torch.zeros(\n+                    self.num_total_experts, dtype=torch.float32),\n+                                             requires_grad=False)\n+\n+                set_weight_attrs(self.a13_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n+                set_weight_attrs(self.a2_scale, {\n+                    \"weight_loader\": self.weight_loader,\n+                })\n \n     def weight_loader(self, param: nn.Parameter, loaded_weight: torch.Tensor,\n                       weight_name: str, expert_id: int):\n@@ -149,20 +175,49 @@ class MixtralMoE(nn.Module):\n                        shard_size:2 * shard_size, :] = loaded_weight[shard, :]\n         if weight_name.endswith(\"w2.weight\"):\n             param_data[expert_id, :, :] = loaded_weight[:, shard]\n-        if \"act_scale\" in weight_name:\n-            param_data[:] = param_data[:].max(loaded_weight)\n+        if \"act_scale\" in weight_name or \"weight_scale\" in weight_name:\n+            param_data[expert_id] = loaded_weight\n \n     def process_weights_after_loading(self):\n-        if self.use_fp8:\n-            ws = torch.empty_like(self.ws.data, dtype=torch.float8_e4m3fn)\n-            w2s = torch.empty_like(self.w2s.data, dtype=torch.float8_e4m3fn)\n+        # Fp8 is the only case where we need to process after loading.\n+        if not self.use_fp8:\n+            return\n+\n+        # If checkpoint is fp16, quantize here.\n+        if not self.quant_config.is_checkpoint_fp8_serialized:\n+            w13_weight = torch.empty_like(self.w13_weight.data,\n+                                          dtype=torch.float8_e4m3fn)\n+            w2_weight = torch.empty_like(self.w2_weight.data,\n+                                         dtype=torch.float8_e4m3fn)\n             for expert in range(self.num_total_experts):\n-                ws[expert, :, :], self.ws_scale[expert] = ops.scaled_fp8_quant(\n-                    self.ws.data[expert, :, :])\n-                w2s[expert, :, :], self.w2s_scale[\n-                    expert] = ops.scaled_fp8_quant(self.w2s.data[expert, :, :])\n-            self.ws = nn.Parameter(ws, requires_grad=False)\n-            self.w2s = nn.Parameter(w2s, requires_grad=False)\n+                w13_weight[expert, :, :], self.w13_scale[\n+                    expert] = ops.scaled_fp8_quant(\n+                        self.w13_weight.data[expert, :, :])\n+                w2_weight[expert, :, :], self.w2_scale[\n+                    expert] = ops.scaled_fp8_quant(\n+                        self.w2_weight.data[expert, :, :])\n+            self.w13_weight = nn.Parameter(w13_weight, requires_grad=False)\n+            self.w2_weight = nn.Parameter(w2_weight, requires_grad=False)\n+\n+        # If checkpoint is fp8 + static, cleanup act_scales.\n+        #   Since state_dict has an act_scale per expert but our kernels\n+        #   are passed one act_scale shared across all experts.\n+        elif self.quant_config.activation_scheme == \"static\":\n+            if self.a13_scale is None or self.a2_scale is None:\n+                raise ValueError(\n+                    \"QuantConfig has static quantization, but found \"\n+                    \"activation scales are None.\")\n+\n+            if (not all_close_1d(self.a13_scale)\n+                    or not all_close_1d(self.a2_scale)):\n+                print_warning_once(\n+                    \"Found act_scales that are not equal for fp8 MoE layer. \"\n+                    \"Using the maximum across experts for each layer. \")\n+\n+            self.a13_scale = nn.Parameter(self.a13_scale.max(),\n+                                          requires_grad=False)\n+            self.a2_scale = nn.Parameter(self.a2_scale.max(),\n+                                         requires_grad=False)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         num_tokens, hidden_size = hidden_states.shape\n@@ -170,17 +225,17 @@ class MixtralMoE(nn.Module):\n         # router_logits: (num_tokens, n_experts)\n         router_logits, _ = self.gate(hidden_states)\n         final_hidden_states = fused_moe(hidden_states,\n-                                        self.ws,\n-                                        self.w2s,\n+                                        self.w13_weight,\n+                                        self.w2_weight,\n                                         router_logits,\n                                         self.top_k,\n                                         renormalize=True,\n                                         inplace=True,\n                                         use_fp8=self.use_fp8,\n-                                        w1_scale=self.ws_scale,\n-                                        w2_scale=self.w2s_scale,\n-                                        a1_scale=self.as_scale,\n-                                        a2_scale=self.a2s_scale)\n+                                        w1_scale=self.w13_scale,\n+                                        w2_scale=self.w2_scale,\n+                                        a1_scale=self.a13_scale,\n+                                        a2_scale=self.a2_scale)\n \n         if self.tp_size > 1:\n             final_hidden_states = tensor_model_parallel_all_reduce(\n@@ -222,7 +277,9 @@ class MixtralAttention(nn.Module):\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n \n-        if isinstance(quant_config, Fp8Config):\n+        if isinstance(\n+                quant_config,\n+                Fp8Config) and not quant_config.is_checkpoint_fp8_serialized:\n             print_warning_once(\n                 \"For Mixtral FP8 quantization, we currently do not quantize \"\n                 \"the attention layers until their FP8 performance is improved.\"\n@@ -461,16 +518,23 @@ class MixtralForCausalLM(nn.Module):\n         ]\n \n         expert_params_mapping = [\n+            # These are the weight scales for the experts\n+            # (param_name, weight_name, expert_id)\n+            (\"w13_scale\" if weight_name in [\"w1\", \"w3\"] else \"w2_scale\",\n+             f\"experts.{expert_id}.{weight_name}.weight_scale\", expert_id)\n+            for expert_id in range(self.config.num_local_experts)\n+            for weight_name in [\"w1\", \"w2\", \"w3\"]\n+        ] + [\n             # These are the weights for the experts\n             # (param_name, weight_name, expert_id)\n-            (\"ws\" if weight_name in [\"w1\", \"w3\"] else \"w2s\",\n+            (\"w13_weight\" if weight_name in [\"w1\", \"w3\"] else \"w2_weight\",\n              f\"experts.{expert_id}.{weight_name}.weight\", expert_id)\n             for expert_id in range(self.config.num_local_experts)\n             for weight_name in [\"w1\", \"w2\", \"w3\"]\n         ] + [\n             # These are the activation scales for the experts\n             # (param_name, weight_name, expert_id)\n-            (\"as_scale\" if weight_name in [\"w1\", \"w3\"] else \"a2s_scale\",\n+            (\"a13_scale\" if weight_name in [\"w1\", \"w3\"] else \"a2_scale\",\n              f\"experts.{expert_id}.{weight_name}.act_scale\", expert_id)\n             for expert_id in range(self.config.num_local_experts)\n             for weight_name in [\"w1\", \"w2\", \"w3\"]\n@@ -512,3 +576,8 @@ class MixtralForCausalLM(nn.Module):\n                     weight_loader = getattr(param, \"weight_loader\",\n                                             default_weight_loader)\n                     weight_loader(param, loaded_weight)\n+\n+\n+def all_close_1d(x: torch.Tensor) -> bool:\n+    assert len(x.shape) == 1\n+    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\nimport torch\n\nimport vllm\nfrom vllm.lora.request import LoRARequest\nfrom vllm.platforms import current_platform\n\nMODEL_PATH = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n\n\ndef do_sample(llm: vllm.LLM, lora_path: str, lora_id: int,\n              prompts: list[str]) -> list[str]:\n\n    sampling_params = vllm.SamplingParams(temperature=0, max_tokens=256)\n    outputs = llm.generate(\n        prompts,\n        sampling_params,\n        lora_request=LoRARequest(str(lora_id), lora_id, lora_path)\n        if lora_id else None)\n    # Print the outputs.\n    generated_texts: list[str] = []\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text.strip()\n        generated_texts.append(generated_text)\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n    return generated_texts\n\n\n@pytest.mark.parametrize(\"tp_size\", [4])\ndef test_mixtral_lora(mixtral_lora_files, tp_size):\n    \"\"\"Original test, the LoRA model has the common target modules, not all\"\"\"\n    if torch.cuda.device_count(\n    ) < tp_size and tp_size > 1 and current_platform.is_cuda_alike():\n        pytest.skip(f\"Not enough GPUs for tensor parallelism {tp_size}\")\n\n    prompts = [\n        \"[system] Given a target sentence construct the underlying meaning representation\\nof the input sentence as a single function with attributes and attribute\\nvalues. This function should describe the target string accurately and the\\nfunction must be one of the following ['inform', 'request', 'give_opinion',\\n'confirm', 'verify_attribute', 'suggest', 'request_explanation',\\n'recommend', 'request_attribute'].\\n\\nThe attributes must be one of the following:\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating',\\n'genres', 'player_perspective', 'has_multiplayer', 'platforms',\\n'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier'] [/system] [user] Here is the target sentence:\\nSpellForce 3 is a pretty bad game. The developer Grimlore Games is clearly a bunch of no-talent hacks, and 2017 was a terrible year for games anyway. [/user] [assistant]\",  # noqa: E501\n        \"[system] Given a target sentence construct the underlying meaning representation\\nof the input sentence as a single function with attributes and attribute\\nvalues. This function should describe the target string accurately and the\\nfunction must be one of the following ['inform', 'request', 'give_opinion',\\n'confirm', 'verify_attribute', 'suggest', 'request_explanation',\\n'recommend', 'request_attribute'].\\n\\nThe attributes must be one of the following:\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating',\\n'genres', 'player_perspective', 'has_multiplayer', 'platforms',\\n'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier'] [/system] [user] Here is the target sentence:\\nI wanted to like Grimlore Games' 2017 entry, but in SpellForce 3 they just didn't get anything right. [/user] [assistant]\",  # noqa: E501\n        \"[system] Given a target sentence construct the underlying meaning representation\\nof the input sentence as a single function with attributes and attribute\\nvalues. This function should describe the target string accurately and the\\nfunction must be one of the following ['inform', 'request', 'give_opinion',\\n'confirm', 'verify_attribute', 'suggest', 'request_explanation',\\n'recommend', 'request_attribute'].\\n\\nThe attributes must be one of the following:\\n['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating',\\n'genres', 'player_perspective', 'has_multiplayer', 'platforms',\\n'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier'] [/system] [user] Here is the target sentence:\\nBioShock is a good role-playing, action-adventure, shooter that released for PlayStation, Xbox, and PC in 2007. It is available on Steam, and it has a Mac release but not a Linux release. [/user] [assistant]\",  # noqa: E501\n    ]\n\n    llm = vllm.LLM(\n        MODEL_PATH,\n        enable_lora=True,\n        max_num_seqs=16,\n        max_loras=4,\n        distributed_executor_backend=\"ray\",\n        tensor_parallel_size=tp_size,\n    )\n\n    expected_lora_output = [\n        \"give_opinion(name[SpellForce 3], release_year[2017], developer[Grimlore Games], rating[poor])\",  # noqa: E501\n        \"give_opinion(name[SpellForce 3], developer[Grimlore Games], release_year[2017], rating[poor])\",  # noqa: E501\n        \"inform(name[BioShock], release_year[2007], rating[good], genres[action-adventure, role-playing, shooter], platforms[PlayStation, Xbox, PC], available_on_steam[yes], has_linux_release[no], has_mac_release[yes])\",  # noqa: E501\n    ]\n    assert do_sample(llm, mixtral_lora_files, lora_id=1,\n                     prompts=prompts) == expected_lora_output\n    assert do_sample(llm, mixtral_lora_files, lora_id=2,\n                     prompts=prompts) == expected_lora_output\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)\n\nFollow on to #4332 to enable FP8 checkpoint loading for Mixtral and supersedes #4436.\n\nThis PR enables the following checkpoint loading features for Mixtral:\n\nSupports loading fp8 checkpoints for Mixtral, such as this \"nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8\" test model\nSupports static or dynamic activation quantization with static weight quantization (all per tensor)\nSupports different scales for each expert weight\nSupports Fp8 in QKV layer\nNotes:\n\nThe Expert Gate/Router always runs at half / full precision for now.\nIf there are different weight scales between QKV layer (for separate QKV weights), they are re-quantized using layer.weight_scale.max() so we can have a single gemm for performance.", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/36fb68f9_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/2a052011_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-2deb029", "created_at": "2024-08-26T18:24:53+00:00", "base_commit": "029c71de11bc3bcf84a1b3cf9d91e79ab6949799", "head_commit": "2deb029d115dadd012ce5ea70487a207cb025493", "patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..25be2dd13 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -708,6 +708,37 @@ class TestPrefixCachingBlockAllocator:\n                                                token_ids=token_ids)\n         assert allocator.get_prefix_cache_hit_rate() > 0.99\n \n+    # Test case for marking cache hit blocks as computed right after\n+    # a batch of prefill sequences are scheduled.\n+    @staticmethod\n+    def test_touch_block():\n+        block_size = 16\n+        common_blocks = 4\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+\n+        common_token_ids = list(range(block_size * common_blocks))\n+\n+        # Mimic the behavior of allocating the same block chain\n+        # (i.e., common prefix) for a batch of 3 different prefill sequences.\n+        for _ in range(3):\n+            blocks = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+                block_size=block_size,\n+                token_ids=common_token_ids,\n+                allocator=allocator,\n+            )\n+            block_ids = [block.block_id for block in blocks]\n+            # The allocated blocks should  be marked as touched\n+            # but not computed.\n+            computed_block_ids = allocator.get_computed_block_ids(\n+                [], block_ids, skip_last_block_id=False)\n+            assert len(computed_block_ids) == 0\n+\n+        allocator.mark_blocks_as_computed([])\n+        computed_block_ids = allocator.get_computed_block_ids(\n+            [], block_ids, skip_last_block_id=False)\n+        assert len(computed_block_ids) == common_blocks\n+\n     @staticmethod\n     def create_immutable_chain(\n         block_size: int,\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..a87e814cf 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -1,6 +1,6 @@\n \"\"\"Token blocks.\"\"\"\n from os.path import commonprefix\n-from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n+from typing import Dict, FrozenSet, Iterable, List, Optional, Set, Tuple\n \n from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,\n                                     get_all_blocks_recursively)\n@@ -73,6 +73,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # prefix hash will be in this dict, even if they have refcount 0.\n         self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n \n+        # A list of immutable block IDs that have been touched by scheduler\n+        # and should be marked as computed after an entire batch of sequences\n+        # are scheduled.\n+        self._touched_blocks: Set[BlockId] = set()\n+\n         # Used to track status of each physical block id\n         self._block_tracker: Dict[BlockId, BlockTracker] = {}\n         for block_id in block_ids:\n@@ -438,10 +443,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert self._refcounter.get(block.block_id) > 0\n \n         if block.content_hash not in self._cached_blocks:\n-            # No cached content hash => Set this block as cached\n-            # (Note that this block is not computed yet =>\n-            #  Will be computed after free())\n+            # No cached content hash => Set this block as cached.\n+            # Note that this block cannot be marked as computed yet\n+            # because other sequences in the same batch cannot reuse\n+            # this block.\n             self._cached_blocks[block.content_hash] = block.block_id\n+            # Mark this block as touched so that it can be marked as\n+            # computed after the entire batch of sequences are scheduled.\n+            self._touched_blocks.add(block.block_id)\n             return block.block_id\n \n         # Reuse the cached content hash\n@@ -507,7 +516,10 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        # Mark all touched blocks as computed.\n+        for block_id in self._touched_blocks:\n+            self._block_tracker[block_id].computed = True\n+        self._touched_blocks.clear()\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..7d4919a0d 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,11 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        # If prefix caching is enabled, mark immutable blocks as computed\n+        # right after they have been scheduled (for prefill). This assumes\n+        # the scheduler is synchronous so blocks are actually computed when\n+        # scheduling the next batch.\n+        self.block_allocator.mark_blocks_as_computed([])\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\n\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.sampling_params import SamplingParams\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\n@pytest.mark.parametrize(\"block_size\", [16])\ndef test_computed_prefix_blocks(model: str, block_size: int):\n    # This test checks if we are able to run the engine to completion\n    # without triggering asserts.\n    # We are in a scenario where all blocks from the second request's prompt\n    # are full and already computed when the second request arrives.\n    prompt = (\n        \"You are a helpful assistant. How do I build a car from cardboard and \"\n        \"paper clips? Is there an easy to follow video tutorial available \"\n        \"online for free?\")\n    prompt2 = (\n        \" Please recommend to me some resources where I can learn not only to \"\n        \"handle technical difficulties of building a car, but also \"\n        \"decoration.\")\n\n    engine_args = EngineArgs(model=model,\n                             block_size=block_size,\n                             enable_prefix_caching=True)\n\n    engine = LLMEngine.from_engine_args(engine_args)\n    sampling_params = SamplingParams()\n\n    engine.add_request(\"0\", prompt + prompt2, sampling_params)\n    engine.step()\n    engine.add_request(\"1\", prompt, sampling_params)\n    engine.step()\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/029c71de_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/2deb029d_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-2f19283", "created_at": "2024-04-07T02:14:06+00:00", "base_commit": "95baec828f3ee046074dace1d88202a920b7dc15", "head_commit": "2f1928354903ae0c6edfe76cc90081eb513ead2c", "patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex b2aaeb33c..e7e3b4dc1 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -328,7 +328,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         self,\n         seq: Sequence,\n     ) -> bool:\n-        token_ids_len = len(seq.data.get_token_ids())\n+        token_ids_len = seq.data.get_len()\n         return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n \n     def _maybe_promote_last_block(", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"Test hashing of cache blocks.\n\nRun `pytest tests/test_cache_block_hashing.py`.\n\"\"\"\nfrom typing import Optional\n\nimport pytest\n\nfrom vllm.inputs import token_inputs\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import Sequence\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\n\n# Make two prefixes with different first blocks.\nprefix_start = [(\"You are an expert\"), (\"You are a\")]\nprefix_common = (\n    \" school principal, skilled in effectively managing \"\n    \"faculty and staff. Draft 10-15 questions for a potential first grade \"\n    \"Head Teacher for my K-12, all-girls', independent school that emphasizes \"\n    \"community, joyful discovery, and life-long learning. The candidate is \"\n    \"coming in for a first-round panel interview for a 8th grade Math \"\n    \"teaching role. They have 5 years of previous teaching experience \"\n    \"as an assistant teacher at a co-ed, public school with experience \"\n    \"in middle school math teaching. Based on this, fulfill \"\n    \"the following: \")\nprefixes = [start + prefix_common for start in prefix_start]\n\n# Sample prompts.\nsample_prompts = [\n    \"Hello, my name is\", \"The president of the United States is\",\n    \"The capital of France is\", \"The future of AI is\"\n]\n\n\n# Helper function.\ndef flatten_2d(li):\n    return [lss for ls in li for lss in ls]\n\n\n@pytest.mark.parametrize(\"model\", [\"facebook/opt-125m\"])\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"max_num_seqs\", [256])\n@pytest.mark.parametrize(\"concurrent_lora_int_ids\",\n                         [[None], [1], [None, 1], [None, 1, 2], [1, 2]])\ndef test_auto_prefix_caching(model: str, block_size: int, max_num_seqs: int,\n                             concurrent_lora_int_ids: list[Optional[int]]):\n\n    tokenizer = TokenizerGroup(\n        tokenizer_id=\"facebook/opt-125m\",\n        enable_lora=False,\n        max_num_seqs=max_num_seqs,\n        max_input_length=None,\n    )\n\n    hashes: list[list[list[int]]] = []\n\n    for prefix in prefixes:\n        for lora_int_id in concurrent_lora_int_ids:\n            lora_request = None\n\n            if lora_int_id is not None:\n                lora_request = LoRARequest(\n                    f\"example_lora_{lora_int_id}\",\n                    lora_int_id,\n                    f\"example/path/to/lora_{lora_int_id}\",\n                )\n\n            hashes.append([])\n            prompts = [prefix + prompt for prompt in sample_prompts]\n            for seq_id, prompt in enumerate(prompts):\n                hashes[-1].append([])\n                prompt_token_ids = tokenizer.encode(prompt)\n                seq = Sequence(seq_id,\n                               inputs=token_inputs(prompt_token_ids,\n                                                   prompt=prompt),\n                               block_size=block_size,\n                               eos_token_id=tokenizer.tokenizer.eos_token_id,\n                               lora_request=lora_request)\n\n                num_blocks = len(prompt_token_ids) // block_size\n                for idx in range(num_blocks):\n                    hashes[-1][-1].append(seq.hash_of_block(idx))\n\n    # Check that hashes made with two prefixes with different first blocks are\n    # different everywhere.\n    for hash0, hash1 in zip(flatten_2d(hashes[0]), flatten_2d(hashes[1])):\n        assert (hash0 != hash1)\n\n    # Check that hashes of different prompts made with the same prefix are the\n    # same until the hashes that contain the prompt.\n    for hash_pref in hashes:\n        same_hashes = [tuple(h[:-1]) for h in hash_pref]\n        different_hashes = [h[-1] for h in hash_pref]\n        assert (len(set(same_hashes)) == 1)\n        assert (len(set(different_hashes)) == len(different_hashes))\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] latency optimization (#3890)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/95baec82_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/2f192835_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-310aca8", "created_at": "2025-01-09T07:18:21+00:00", "base_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834", "head_commit": "310aca88c984983189a57f1b72e3b1dde89fb92f", "patch": "diff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py\nindex fda4d007c..efc599871 100644\n--- a/vllm/distributed/device_communicators/pynccl.py\n+++ b/vllm/distributed/device_communicators/pynccl.py\n@@ -10,6 +10,7 @@ from vllm.distributed.device_communicators.pynccl_wrapper import (\n     ncclRedOpTypeEnum, ncclUniqueId)\n from vllm.distributed.utils import StatelessProcessGroup\n from vllm.logger import init_logger\n+from vllm.utils import current_stream\n \n logger = init_logger(__name__)\n \n@@ -96,7 +97,7 @@ class PyNcclCommunicator:\n             self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                 self.world_size, self.unique_id, self.rank)\n \n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n             # A small all_reduce for warmup.\n             data = torch.zeros(1, device=device)\n             self.all_reduce(data)\n@@ -119,7 +120,7 @@ class PyNcclCommunicator:\n         out_tensor = torch.empty_like(in_tensor)\n \n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllReduce(buffer_type(in_tensor.data_ptr()),\n                                 buffer_type(out_tensor.data_ptr()),\n                                 in_tensor.numel(),\n@@ -141,7 +142,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllGather(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), input_tensor.numel(),\n@@ -162,7 +163,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclReduceScatter(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), output_tensor.numel(),\n@@ -177,7 +178,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclSend(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), dst,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -189,7 +190,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclRecv(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), src,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -201,7 +202,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         if src == self.rank:\n             sendbuff = buffer_type(tensor.data_ptr())\n             # NCCL requires the sender also to have a receive buffer\ndiff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py\nindex a837c1dc5..be7f16ef5 100644\n--- a/vllm/distributed/parallel_state.py\n+++ b/vllm/distributed/parallel_state.py\n@@ -357,10 +357,7 @@ class GroupCoordinator:\n             return out\n         pynccl_comm = self.pynccl_comm\n         assert pynccl_comm is not None\n-        # TODO: pynccl should not use `stream=`\n-        # it can just always use the current stream.\n-        out = pynccl_comm.all_reduce(input_,\n-                                     stream=torch.cuda.current_stream())\n+        out = pynccl_comm.all_reduce(input_)\n         if out is None:\n             # fall back to the default all-reduce using PyTorch.\n             # this usually happens during testing.\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex a92b77efd..0b0905e67 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -944,6 +944,39 @@ def find_nccl_library() -> str:\n     return so_file\n \n \n+prev_set_stream = torch.cuda.set_stream\n+\n+_current_stream = None\n+\n+\n+def _patched_set_stream(stream: torch.cuda.Stream) -> None:\n+    global _current_stream\n+    _current_stream = stream\n+    prev_set_stream(stream)\n+\n+\n+torch.cuda.set_stream = _patched_set_stream\n+\n+\n+def current_stream() -> torch.cuda.Stream:\n+    \"\"\"\n+    replace `torch.cuda.current_stream()` with `vllm.utils.current_stream()`.\n+    it turns out that `torch.cuda.current_stream()` is quite expensive,\n+    as it will construct a new stream object at each call.\n+    here we patch `torch.cuda.set_stream` to keep track of the current stream\n+    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n+\n+    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n+    from C/C++ code.\n+    \"\"\"\n+    global _current_stream\n+    if _current_stream is None:\n+        # when this function is called before any stream is set,\n+        # we return the default stream.\n+        _current_stream = torch.cuda.current_stream()\n+    return _current_stream\n+\n+\n def enable_trace_function_call_for_thread(vllm_config: \"VllmConfig\") -> None:\n     \"\"\"Set up function tracing for the current thread,\n     if enabled via the VLLM_TRACE_FUNCTION environment variable\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex a2c2cebf8..acce92349 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -14,7 +14,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n-from vllm.utils import PyObjectCache, async_tensor_h2d\n+from vllm.utils import PyObjectCache, async_tensor_h2d, current_stream\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -498,7 +498,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         #   appended sampler output from last iteration\n         #   - also maybe pythonize if CPU is ahead of GPU\n \n-        current_stream = torch.cuda.current_stream()\n+        stream = current_stream()\n         if not model_input.is_first_multi_step:\n             # Explicitly block on the previous step's forward to make sure we\n             # don't clobber any GPU tensors still in use.\n@@ -541,7 +541,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                                        num_steps=1)\n \n         # record the event for the current step so that the next step can sync\n-        model_input.record_step_event(current_stream)\n+        model_input.record_step_event(stream)\n \n         if get_pp_group().is_last_rank and self.is_driver_worker:\n             assert isinstance(output, list)\n@@ -552,7 +552,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             # event for the pythonization so that we only pythonize if the\n             # tensors are ready. May be able to be combined with the step event\n             output_ready_event = torch.cuda.Event()\n-            output_ready_event.record(current_stream)\n+            output_ready_event.record(stream)\n             if self.parallel_config.pipeline_parallel_size > 1:\n                 output[0].sampled_token_ids_cpu = output[\n                     0].sampled_token_ids.cpu()", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\nfrom importlib.util import find_spec\n\nimport pytest\nimport torch\n\nimport vllm.envs as envs\nfrom vllm.compilation.collective_fusion import AllReduceFusionPass\nfrom vllm.compilation.fix_functionalization import FixFunctionalizationPass\nfrom vllm.compilation.noop_elimination import NoOpEliminationPass\nfrom vllm.config import (CompilationConfig, CompilationLevel, DeviceConfig,\n                         ModelConfig, PassConfig, VllmConfig)\nfrom vllm.distributed import tensor_model_parallel_all_reduce\nfrom vllm.distributed.parallel_state import (init_distributed_environment,\n                                             initialize_model_parallel)\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.quantization.utils.w8a8_utils import (\n    GroupShape, QuantFP8)\nfrom vllm.platforms import current_platform\nfrom vllm.utils import update_environment_variables\n\nfrom ..utils import has_module_attribute, multi_gpu_test\nfrom .backend import TestBackend\n\n\nclass TestAllReduceRMSNormModel(torch.nn.Module):\n\n    def __init__(self, hidden_size=16, token_num=16, eps=1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.norm = RMSNorm(hidden_size, eps)\n\n    def forward(self, hidden_states, residual):\n        view = hidden_states.reshape(-1, self.hidden_size)\n        all_reduce = tensor_model_parallel_all_reduce(view)\n        norm = self.norm(all_reduce)\n        return norm\n\n    def ops_in_model_before(self):\n        return [torch.ops.vllm.all_reduce.default]\n\n    def ops_in_model_after(self):\n        return [torch.ops.vllm.flashinfer_trtllm_fused_allreduce_norm.default]\n\n\nclass TestAllReduceFusedAddRMSNormModel(torch.nn.Module):\n\n    def __init__(self, hidden_size=16, token_num=16, eps=1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.norm = RMSNorm(hidden_size, eps)\n\n    def forward(self, hidden_states, residual):\n        view = hidden_states.reshape(-1, self.hidden_size)\n        all_reduce = tensor_model_parallel_all_reduce(view)\n        norm, _ = self.norm(all_reduce, residual)\n        return norm\n\n    def ops_in_model_before(self):\n        return [torch.ops.vllm.all_reduce.default]\n\n    def ops_in_model_after(self):\n        return [torch.ops.vllm.flashinfer_trtllm_fused_allreduce_norm.default]\n\n\nclass TestAllReduceFusedAddRMSNormStaticQuantFP8Model(torch.nn.Module):\n\n    def __init__(self, hidden_size=16, token_num=16, eps=1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.norm = RMSNorm(hidden_size, eps)\n        self.quant_fp8 = QuantFP8(static=True,\n                                  group_shape=GroupShape.PER_TENSOR)\n        self.scale = torch.rand(1, dtype=torch.float32)\n        self.output = torch.empty((token_num, hidden_size),\n                                  dtype=torch.float32)\n\n    def forward(self, hidden_states, residual):\n        view = hidden_states.reshape(-1, self.hidden_size)\n        all_reduce = tensor_model_parallel_all_reduce(view)\n        norm_output, residual_output = self.norm(all_reduce, residual)\n        torch.ops._C.static_scaled_fp8_quant(self.output,\n                                             norm_output.contiguous(),\n                                             self.scale)\n        return self.output, residual_output\n\n    def ops_in_model_after(self):\n        return [torch.ops.vllm.flashinfer_trtllm_fused_allreduce_norm.default]\n\n    def ops_in_model_before(self):\n        return [\n            torch.ops.vllm.all_reduce.default,\n            torch.ops._C.static_scaled_fp8_quant.default\n        ]\n\n\nclass TestAllReduceFusedAddRMSNormStaticQuantFP4Model(torch.nn.Module):\n\n    def __init__(self, hidden_size=16, token_num=16, eps=1e-6):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.eps = eps\n        self.norm = RMSNorm(hidden_size, eps)\n        self.scale = torch.rand(1, dtype=torch.float32)\n        self.output = torch.empty((token_num, hidden_size),\n                                  dtype=torch.float32)\n\n        round_up = lambda x, y: (x + y - 1) // y * y\n        rounded_m = round_up(token_num, 128)\n        scale_n = hidden_size // 16\n        rounded_n = round_up(scale_n, 4)\n        self.output_scale = torch.empty((rounded_m, rounded_n // 4),\n                                        dtype=torch.int32)\n\n    def forward(self, hidden_states, residual):\n        view = hidden_states.reshape(-1, self.hidden_size)\n        all_reduce = tensor_model_parallel_all_reduce(view)\n        norm_output, residual_output = self.norm(all_reduce, residual)\n        norm_output = norm_output.reshape(-1, norm_output.shape[-1])\n        torch.ops._C.scaled_fp4_quant(self.output, norm_output,\n                                      self.output_scale, self.scale)\n        return self.output, residual_output, self.output_scale\n\n    def ops_in_model_after(self):\n        return [torch.ops.vllm.flashinfer_trtllm_fused_allreduce_norm.default]\n\n    def ops_in_model_before(self):\n        return [\n            torch.ops.vllm.all_reduce.default,\n            torch.ops._C.scaled_fp4_quant.default\n        ]\n\n\n@multi_gpu_test(num_gpus=2)\n@pytest.mark.parametrize(\n    \"test_model\",\n    [\n        TestAllReduceRMSNormModel,\n        TestAllReduceFusedAddRMSNormModel,\n        TestAllReduceFusedAddRMSNormStaticQuantFP8Model,\n        # TODO: Enable with torch==2.8.0\n        # TestAllReduceFusedAddRMSNormStaticQuantFP4Model,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"seq_len\", [8])\n@pytest.mark.parametrize(\"hidden_size\", [16])\n@pytest.mark.parametrize(\"dtype\", [torch.bfloat16])\n@pytest.mark.skipif(envs.VLLM_TARGET_DEVICE not in [\"cuda\"],\n                    reason=\"Only test on CUDA\")\n@pytest.mark.skipif(\n    not find_spec(\"flashinfer\")\n    or not has_module_attribute(\"flashinfer.comm\", \"trtllm_allreduce_fusion\"),\n    reason=\"flashinfer is not found or flashinfer \"\n    \"is not compiled with trtllm_allreduce_fusion\")\ndef test_all_reduce_fusion_pass_replace(test_model: torch.nn.Module,\n                                        batch_size: int, seq_len: int,\n                                        hidden_size: int, dtype: torch.dtype):\n    num_processes = 2\n    if (test_model == TestAllReduceFusedAddRMSNormStaticQuantFP4Model\n            and not current_platform.has_device_capability(100)):\n        pytest.skip(\"Skip as nvfp4 is only supported on \"\n                    \"devices with compute capability 10.0 (Blackwell)\")\n\n    def run_torch_spawn(fn, nprocs):\n        torch.multiprocessing.spawn(fn,\n                                    args=(num_processes, test_model,\n                                          batch_size, seq_len, hidden_size,\n                                          dtype),\n                                    nprocs=nprocs)\n\n    run_torch_spawn(all_reduce_fusion_pass_on_test_model, num_processes)\n\n\ndef all_reduce_fusion_pass_on_test_model(local_rank: int, world_size: int,\n                                         test_model_cls: torch.nn.Module,\n                                         batch_size: int, seq_len: int,\n                                         hidden_size: int, dtype: torch.dtype):\n    current_platform.seed_everything(0)\n\n    device = torch.device(f\"cuda:{local_rank}\")\n    torch.cuda.set_device(device)\n    torch.set_default_device(device)\n    torch.set_default_dtype(dtype)\n\n    update_environment_variables({\n        'RANK': str(local_rank),\n        'LOCAL_RANK': str(local_rank),\n        'WORLD_SIZE': str(world_size),\n        'MASTER_ADDR': 'localhost',\n        'MASTER_PORT': '12345',\n    })\n\n    init_distributed_environment()\n    initialize_model_parallel(tensor_model_parallel_size=world_size)\n\n    vllm_config = VllmConfig(compilation_config=CompilationConfig(\n        level=CompilationLevel.PIECEWISE,\n        custom_ops=[\"+rms_norm\", \"+quant_fp8\"]))\n    vllm_config.compilation_config.pass_config = PassConfig(\n        enable_fi_allreduce_fusion=True, enable_noop=True)\n    vllm_config.device_config = DeviceConfig(device=torch.device(\"cuda\"))\n\n    # this is a fake model name to construct the model config\n    # in the vllm_config, it's not really used.\n    model_name = \"nm-testing/TinyLlama-1.1B-Chat-v1.0-FP8-e2e\"\n    vllm_config.model_config = ModelConfig(model=model_name,\n                                           trust_remote_code=True,\n                                           dtype=dtype,\n                                           seed=42)\n\n    all_reduce_fusion_pass = AllReduceFusionPass(vllm_config)\n    noop_pass = NoOpEliminationPass(vllm_config)\n    func_pass = FixFunctionalizationPass(vllm_config)\n\n    backend = TestBackend(all_reduce_fusion_pass, noop_pass, func_pass)\n\n    token_num = batch_size * seq_len\n    model = test_model_cls(hidden_size, token_num)\n\n    hidden_states = torch.randn((token_num, hidden_size), requires_grad=False)\n    residual = torch.randn((token_num, hidden_size), requires_grad=False)\n\n    compiled_model = torch.compile(model, backend=backend)\n    compiled_model(hidden_states, residual)\n\n    backend.check_before_ops(model.ops_in_model_before(), fully_replaced=False)\n    backend.check_after_ops(model.ops_in_model_after())\n    del all_reduce_fusion_pass\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[perf]fix current stream (#11870)\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/a732900e_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/310aca88_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 172, "symbols_collected": 3515, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.cpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_gpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_xpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.openvino_executor", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand_slice", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 172, "symbols_collected": 3516, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.cpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_gpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_xpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.openvino_executor", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand_slice", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-379da6d", "created_at": "2024-05-09T23:38:07+00:00", "base_commit": "ebce310b7433e050086f52ca48571807df467f50", "head_commit": "379da6dcb5f5d062d0452b2fc23291e5113dcf04", "patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 5b5643748..829c47003 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -189,8 +189,34 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n def scaled_fp8_quant(\n     input: torch.Tensor,\n     scale: Optional[torch.Tensor] = None,\n+    batch_dim_padding: Optional[int] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+    \"\"\"\n+    Quantize input tensor to FP8 and return quantized tensor and scale.\n+\n+    This function supports both static and dynamic quantization: If you\n+    provide the scale, it will use static scaling and if you omit it,\n+    the scale will be determined dynamically. The function also allows\n+    optional padding of the output tensor for downstream kernels that\n+    will benefit from padding.\n+\n+    Args:\n+        input: The input tensor to be quantized to FP8\n+        scale: Optional scaling factor for the FP8 quantization\n+        batch_dim_padding: If specified, pad the first dimension\n+            of the output to at least this value.\n+\n+    Returns:\n+        Tuple[torch.Tensor, torch.Tensor]: The output tensor in FP8 and\n+            scaling factor.\n+    \"\"\"\n+    if batch_dim_padding:\n+        shape = (max(batch_dim_padding, input.shape[0]), *input.shape[1:])\n+        output = torch.empty(shape,\n+                             device=input.device,\n+                             dtype=torch.float8_e4m3fn)\n+    else:\n+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n         scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex b57e1dde8..ff996741c 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -231,9 +231,14 @@ class Fp8LinearMethod(LinearMethodBase):\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n-\n-        # Fused GEMM_DQ\n+        qinput, x_scale = ops.scaled_fp8_quant(x,\n+                                               layer.act_scale,\n+                                               batch_dim_padding=17)\n+\n+        # Fused GEMM_DQ -- note we padded the input above because\n+        # torch._scaled_mm is more performant for matrices with\n+        # batch dimension > 16. Note that this could change\n+        # in the future.\n         output, _ = torch._scaled_mm(\n             qinput,\n             layer.weight,\n@@ -243,7 +248,7 @@ class Fp8LinearMethod(LinearMethodBase):\n             bias=bias,\n         )\n \n-        return output\n+        return torch.narrow(output, 0, 0, x.shape[0])\n \n \n def all_close_1d(x: torch.Tensor) -> bool:", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\nimport torch\n\nfrom vllm.config import CompilationConfig, VllmConfig, set_current_vllm_config\nfrom vllm.model_executor.custom_op import CustomOp\nfrom vllm.model_executor.layers.activation import (GeluAndMul,\n                                                   ReLUSquaredActivation,\n                                                   SiluAndMul)\nfrom vllm.model_executor.layers.fused_moe.fused_moe import (dispatch_topk_func,\n                                                            vllm_topk_softmax)\nfrom vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (\n    is_rocm_aiter_moe_enabled)\nfrom vllm.model_executor.layers.layernorm import (\n    RMSNorm, dispatch_cuda_rmsnorm_func, fused_add_rms_norm, rms_norm,\n    rocm_aiter_fused_add_rms_norm, rocm_aiter_rms_norm)\nfrom vllm.model_executor.layers.quantization.utils.fp8_utils import (\n    cutlass_scaled_mm, dispatch_w8a8_blockscale_func, w8a8_block_fp8_matmul)\nfrom vllm.platforms import current_platform\n\n\n# Registered subclass for test\n@CustomOp.register(\"relu3\")\nclass Relu3(ReLUSquaredActivation):\n    pass\n\n\n@pytest.mark.parametrize(\n    \"env, torch_level, use_inductor, ops_enabled, default_on\",\n    [\n        # Default values based on compile level\n        # - All by default (no Inductor compilation)\n        (\"\", 0, False, [True] * 4, True),\n        (\"\", 1, True, [True] * 4, True),\n        (\"\", 2, False, [True] * 4, True),\n        # - None by default (with Inductor)\n        (\"\", 3, True, [False] * 4, False),\n        (\"\", 4, True, [False] * 4, False),\n        # - All by default (without Inductor)\n        (\"\", 3, False, [True] * 4, True),\n        (\"\", 4, False, [True] * 4, True),\n        # Explicitly enabling/disabling\n        #\n        # Default: all\n        #\n        # All but SiluAndMul\n        (\"+rms_norm,-silu_and_mul\", 0, True, [1, 0, 1, 1], True),\n        # Only ReLU3\n        (\"none,-rms_norm,+relu3\", 1, False, [0, 0, 0, 1], False),\n        # All but SiluAndMul\n        (\"all,-silu_and_mul\", 2, True, [1, 0, 1, 1], True),\n        # All but ReLU3 (even if ReLU2 is on)\n        (\"-relu3,relu2\", 3, False, [1, 1, 1, 0], True),\n        # RMSNorm and SiluAndMul\n        (\"none,-relu3,+rms_norm,+silu_and_mul\", 4, False, [1, 1, 0, 0], False),\n        # All but RMSNorm\n        (\"-rms_norm\", 3, False, [0, 1, 1, 1], True),\n        #\n        # Default: none\n        #\n        # Only ReLU3\n        (\"-silu_and_mul,+relu3\", 3, True, [0, 0, 0, 1], False),\n        # All but RMSNorm\n        (\"all,-rms_norm\", 4, True, [0, 1, 1, 1], True),\n    ])\ndef test_enabled_ops(env: str, torch_level: int, use_inductor: bool,\n                     ops_enabled: list[int], default_on: bool):\n    vllm_config = VllmConfig(\n        compilation_config=CompilationConfig(use_inductor=bool(use_inductor),\n                                             level=torch_level,\n                                             custom_ops=env.split(\",\")))\n    with set_current_vllm_config(vllm_config):\n        assert CustomOp.default_on() == default_on\n\n        ops_enabled = [bool(x) for x in ops_enabled]\n\n        assert RMSNorm(1024).enabled() == ops_enabled[0]\n        assert CustomOp.op_registry[\"rms_norm\"].enabled() == ops_enabled[0]\n\n        assert SiluAndMul().enabled() == ops_enabled[1]\n        assert CustomOp.op_registry[\"silu_and_mul\"].enabled() == ops_enabled[1]\n\n        assert GeluAndMul().enabled() == ops_enabled[2]\n        assert CustomOp.op_registry[\"gelu_and_mul\"].enabled() == ops_enabled[2]\n\n        # If registered, subclasses should follow their own name\n        assert Relu3().enabled() == ops_enabled[3]\n        assert CustomOp.op_registry[\"relu3\"].enabled() == ops_enabled[3]\n\n        # Unregistered subclass\n        class SiluAndMul2(SiluAndMul):\n            pass\n\n        # Subclasses should not require registration\n        assert SiluAndMul2().enabled() == SiluAndMul().enabled()\n\n\n@pytest.mark.parametrize(\n    \"env\", [\"all,none\", \"all,+rms_norm,all\", \"+rms_norm,-rms_norm\"])\ndef test_enabled_ops_invalid(env: str):\n    with pytest.raises(Exception):  # noqa\n        vllm_config = VllmConfig(compilation_config=CompilationConfig(\n            custom_ops=env.split(\",\")))\n        with set_current_vllm_config(vllm_config):\n            RMSNorm(1024).enabled()\n\n\n@pytest.mark.skipif(\n    not current_platform.is_rocm() or not current_platform.is_fp8_fnuz(),\n    reason=\"AITER is a feature exclusive for ROCm and FP8_FNUZ\")\n@pytest.mark.parametrize(\"use_cutlass\", [True, False])\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\n@pytest.mark.parametrize(\"use_rocm_aiter_gemm_w8a8_blockscale\", [\"0\", \"1\"])\ndef test_w8a8_blockscale_dispatch(use_cutlass: bool, use_rocm_aiter: str,\n                                  use_rocm_aiter_gemm_w8a8_blockscale: str,\n                                  monkeypatch):\n\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER_LINEAR\",\n                       use_rocm_aiter_gemm_w8a8_blockscale)\n\n    use_aiter_and_is_supported = (bool(int(use_rocm_aiter)) and bool(\n        int(use_rocm_aiter_gemm_w8a8_blockscale)))\n    block_scale_func = dispatch_w8a8_blockscale_func(\n        use_cutlass, use_aiter_and_is_supported=use_aiter_and_is_supported)\n    if use_cutlass:\n        assert block_scale_func == cutlass_scaled_mm\n    elif current_platform.is_rocm() and int(use_rocm_aiter) and int(\n            use_rocm_aiter_gemm_w8a8_blockscale):\n        assert block_scale_func == (\n            torch.ops.vllm.rocm_aiter_gemm_w8a8_blockscale)\n    else:\n        assert block_scale_func == w8a8_block_fp8_matmul\n\n\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\ndef test_topk_dispatch(use_rocm_aiter: str, monkeypatch):\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    topk_func = dispatch_topk_func()\n    is_rocm_aiter_moe_enabled.cache_clear()\n    if current_platform.is_rocm() and int(use_rocm_aiter):\n        from vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (\n            rocm_aiter_topk_softmax)\n        assert topk_func == rocm_aiter_topk_softmax\n    else:\n        assert topk_func == vllm_topk_softmax\n\n\n@pytest.mark.parametrize(\"add_residual\", [True, False])\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\n@pytest.mark.parametrize(\"use_rocm_aiter_norm\", [\"0\", \"1\"])\n@pytest.mark.skipif(not current_platform.is_rocm(),\n                    reason=\"AITER is a feature exclusive for ROCm\")\ndef test_rms_norm_dispatch(add_residual: bool, use_rocm_aiter: str,\n                           use_rocm_aiter_norm: str, monkeypatch):\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER_RMSNORM\", use_rocm_aiter_norm)\n    rms_norm_func = dispatch_cuda_rmsnorm_func(add_residual)\n\n    if not add_residual:\n        if current_platform.is_rocm() and int(use_rocm_aiter) and int(\n                use_rocm_aiter_norm):\n            assert rms_norm_func == rocm_aiter_rms_norm\n        else:\n            assert rms_norm_func == rms_norm\n    elif current_platform.is_rocm() and int(use_rocm_aiter) and int(\n            use_rocm_aiter_norm):\n        assert rms_norm_func == rocm_aiter_fused_add_rms_norm\n    else:\n        assert rms_norm_func == fused_add_rms_norm\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] [FP8] Improve FP8 linear layer performance (#4691)\n\nThis PR improves the FP8 performance of linear layers, which had been lacking before (#4118 (comment) and #4118 (comment)).\n\nWe noticed that CUBLASLt can find a better algorithm if the first dimension of the matrix is greater than 16. So this PR enlarges matrices appropriately during quantization. This improves FP8 performance and removes the performance regression vs. FP16, in many cases exceeding FP16 performance.\n\nHere are benchmarks on llama3 70b (ITL numbers for 1000 input and 50 output tokens at fixed qps and at TP 4), all FP8 measurements are for dynamic quantization:\n\nqps = 1: 24 ms (FP8, this PR), 32 ms (FP8, previous main), 26 ms (FP16)\nqps = 2: 26 ms (FP8, this PR), 34ms (FP8, previous main), 28 ms (FP16) \nqps = 4: 33 ms (FP8, this PR), 44 ms (FP8, previous main), 36 ms (FP16)\nqps = 6: 46 ms (FP8, this PR), 56 ms (FP8, previous main), 54 ms (FP16)\nqps = 8: 85 ms (FP8, this PR), 85 ms (FP8, previous main), 138 ms (FP16)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/ebce310b_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/379da6dc_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-3a24309", "created_at": "2024-03-25T23:03:02+00:00", "base_commit": "64172a976c8d975b3aec946f1675716d2532d94f", "head_commit": "3a243095e5e7b655b63ab08fbd5936cb40850415", "patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex d07527304..06135192c 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -506,22 +506,23 @@ def _sample(\n     #                                   sampling_tensors)\n \n \n-def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n+def _get_ranks(x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     This function calculates the ranks of the chosen tokens in a logprob tensor.\n \n     Args:\n         x (torch.Tensor): 2D logprob tensor of shape (N, M)\n                         where N is the no. of tokens and M is the vocab dim.\n-        indices (List[int]): List of chosen token indices.\n+        indices (torch.Tensor): List of chosen token indices.\n \n     Returns:\n         torch.Tensor: 1D tensor of shape (N,) where N is the no. of tokens.\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[range(len(x)), indices]\n-    return (x > vals[:, None]).long().sum(1) + 1\n+    vals = x[torch.arange(0, len(x), device=x.device, dtype=indices.dtype),\n+             indices]\n+    return (x > vals[:, None]).long().sum(1).add_(1)\n \n \n def _get_logprobs(\n@@ -561,12 +562,21 @@ def _get_logprobs(\n         sample_idx += num_parent_seqs\n     assert sample_idx == logprobs.size(0)\n \n+    batched_logprobs_query_seq_indices_gpu = torch.tensor(\n+        batched_logprobs_query_seq_indices, device=logprobs.device)\n+    batched_logprobs_query_token_indices_gpu = torch.tensor(\n+        batched_logprobs_query_token_indices, device=logprobs.device)\n+\n     # Batched query for logprobs of selected token\n     batched_logprobs_query_result = logprobs[[\n-        batched_logprobs_query_seq_indices,\n-        batched_logprobs_query_token_indices\n+        batched_logprobs_query_seq_indices_gpu,\n+        batched_logprobs_query_token_indices_gpu\n     ]]\n \n+    batched_ranks_query_result = _get_ranks(\n+        logprobs[batched_logprobs_query_seq_indices_gpu],\n+        batched_logprobs_query_token_indices_gpu)\n+\n     # Batched query for logprobs of topk tokens\n     if largest_num_logprobs > 0:\n         top_logprobs, top_token_ids = torch.topk(logprobs,\n@@ -578,10 +588,7 @@ def _get_logprobs(\n         top_logprobs, top_token_ids = None, None\n \n     batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n-\n-    batched_ranks_query_result = _get_ranks(\n-        logprobs[batched_logprobs_query_seq_indices],\n-        batched_logprobs_query_token_indices)\n+    batched_ranks_query_result = batched_ranks_query_result.cpu()\n \n     # Gather results\n     result_prompt_logprobs: List[Optional[PromptLogprobs]] = []", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport asyncio\nimport os\nfrom typing import Any, Callable, Optional, Union\n\nimport pytest\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.executor.uniproc_executor import UniProcExecutor\nfrom vllm.sampling_params import SamplingParams\n\n\nclass Mock:\n    ...\n\n\nclass CustomUniExecutor(UniProcExecutor):\n\n    def collective_rpc(self,\n                       method: Union[str, Callable],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict] = None) -> list[Any]:\n        # Drop marker to show that this was ran\n        with open(\".marker\", \"w\"):\n            ...\n        return super().collective_rpc(method, timeout, args, kwargs)\n\n\nCustomUniExecutorAsync = CustomUniExecutor\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor_type_checking(model):\n    with pytest.raises(ValueError):\n        engine_args = EngineArgs(model=model,\n                                 distributed_executor_backend=Mock)\n        LLMEngine.from_engine_args(engine_args)\n    with pytest.raises(ValueError):\n        engine_args = AsyncEngineArgs(model=model,\n                                      distributed_executor_backend=Mock)\n        AsyncLLMEngine.from_engine_args(engine_args)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor(model, tmp_path):\n    cwd = os.path.abspath(\".\")\n    os.chdir(tmp_path)\n    try:\n        assert not os.path.exists(\".marker\")\n\n        engine_args = EngineArgs(\n            model=model,\n            distributed_executor_backend=CustomUniExecutor,\n            enforce_eager=True,  # reduce test time\n        )\n        engine = LLMEngine.from_engine_args(engine_args)\n        sampling_params = SamplingParams(max_tokens=1)\n\n        engine.add_request(\"0\", \"foo\", sampling_params)\n        engine.step()\n\n        assert os.path.exists(\".marker\")\n    finally:\n        os.chdir(cwd)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor_async(model, tmp_path):\n    cwd = os.path.abspath(\".\")\n    os.chdir(tmp_path)\n    try:\n        assert not os.path.exists(\".marker\")\n\n        engine_args = AsyncEngineArgs(\n            model=model,\n            distributed_executor_backend=CustomUniExecutorAsync,\n            enforce_eager=True,  # reduce test time\n        )\n        engine = AsyncLLMEngine.from_engine_args(engine_args)\n        sampling_params = SamplingParams(max_tokens=1)\n\n        async def t():\n            stream = await engine.add_request(\"0\", \"foo\", sampling_params)\n            async for x in stream:\n                ...\n\n        asyncio.run(t())\n\n        assert os.path.exists(\".marker\")\n    finally:\n        os.chdir(cwd)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_respect_ray(model):\n    # even for TP=1 and PP=1,\n    # if users specify ray, we should use ray.\n    # users might do this if they want to manage the\n    # resources using ray.\n    engine_args = EngineArgs(\n        model=model,\n        distributed_executor_backend=\"ray\",\n        enforce_eager=True,  # reduce test time\n    )\n    engine = LLMEngine.from_engine_args(engine_args)\n    assert engine.model_executor.uses_ray\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "Optimize `_get_ranks` in Sampler (#3623)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/64172a97_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/3a243095_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-526de82", "created_at": "2025-01-08T20:23:15+00:00", "base_commit": "56fe4c297c7d9d872eccc19e3edbf1d75e1a30e2", "head_commit": "526de822d501c792b051c864ba873a836d78d5bf", "patch": "diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\nindex 3ff162170..2659afcdc 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/triton_scaled_mm.py\n@@ -128,7 +128,8 @@ def triton_scaled_mm(input: torch.Tensor,\n                      bias: Optional[torch.Tensor] = None,\n                      block_size_m: int = 32,\n                      block_size_n: int = 32,\n-                     block_size_k: int = 32) -> torch.Tensor:\n+                     block_size_k: int = 32,\n+                     use_heuristic=True) -> torch.Tensor:\n     M, K = input.shape\n     N = weight.shape[1]\n \n@@ -152,6 +153,20 @@ def triton_scaled_mm(input: torch.Tensor,\n \n     has_scalar = lambda x: x.shape[0] == 1 and x.shape[1] == 1\n \n+    if use_heuristic:\n+        is_small_N = N < 8192\n+        next_power_of_2_M = max(32, triton.next_power_of_2(M))\n+        if next_power_of_2_M <= 32:\n+            tile_shape = (64, 64, 256) if is_small_N else (64, 128, 256)\n+        elif next_power_of_2_M <= 64:\n+            tile_shape = (64, 64, 256)\n+        elif next_power_of_2_M <= 128:\n+            tile_shape = (64, 128, 128)\n+        else:\n+            tile_shape = (128, 128, 128)\n+\n+    block_size_m, block_size_n, block_size_k = tile_shape\n+\n     block_size_sa = 1 if has_scalar(scale_a) else block_size_m\n     block_size_sb = 1 if has_scalar(scale_b) else block_size_n", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"Tests for cutlass kernels\n\nRun `pytest tests/kernels/quantization/test_cutlass_scaled_mm.py`.\n\"\"\"\nimport random\n\nimport pytest\nimport torch\n\nfrom tests.kernels.utils import baseline_scaled_mm, opcheck, to_fp8, to_int8\nfrom vllm import _custom_ops as ops\nfrom vllm.platforms import current_platform\nfrom vllm.utils import cdiv\n\nMNK_FACTORS = [\n    (1, 256, 128),\n    (1, 16384, 1024),\n    (1, 24576, 496),\n    (16, 256, 496),\n    (16, 16384, 128),\n    (16, 24576, 4096),\n    (32, 8192, 4096),\n    (32, 16384, 4096),\n    (33, 1024, 1024),\n    (33, 8192, 128),\n    (64, 2048, 496),\n    (64, 16384, 1024),\n    (100, 8192, 496),\n    (128, 32768, 4096),\n    (256, 4096, 4096),\n    (512, 256, 1024),\n    (512, 8192, 4096),\n    (512, 16384, 128),\n    (512, 24576, 128),\n]\n\nCUDA_DEVICES = [\n    f\"cuda:{i}\" for i in range(1 if torch.cuda.device_count() == 1 else 2)\n]\n\n# -1 means full extent in that dimension\nTENSORWISE_GROUP_SHAPE = (-1, -1)\nPER_TOKEN_GROUP_SHAPE = (1, -1)\nPER_OUT_CH_GROUP_SHAPE = (-1, 1)\n\ncapability = current_platform.get_device_capability()\ncapability = capability[0] * 10 + capability[1]\n\n\ndef rand_int8(shape: tuple, device: str = \"cuda\"):\n    return to_int8(torch.rand(shape, device=device) * 255 - 128)\n\n\ndef group_scale_helper(shape, group_shape):\n    return [shape[i] if s < 0 else s for i, s in enumerate(group_shape)]\n\n\ndef scale_shape(shape, group_shape):\n    assert len(shape) == len(group_shape)\n    group_shape = group_scale_helper(shape, group_shape)\n    return tuple(\n        cdiv(shape[i], group_shape[i]) for i in range(len(group_shape)))\n\n\ndef cutlass_fp8_gemm_helper(m: int,\n                            n: int,\n                            k: int,\n                            a_scale_group_shape: tuple,\n                            b_scale_group_shape: tuple,\n                            use_bias: bool,\n                            out_dtype: type[torch.dtype] = torch.bfloat16,\n                            device: str = \"cuda\"):\n    # Test for a cutlass kernel with per-token activation quantization\n    # and per-output channel weight quantization.\n    a = to_fp8(torch.randn((m, k), device=device))\n    b = to_fp8(torch.randn((n, k), device=device).t())\n\n    a_scales_shape = scale_shape(a.shape, a_scale_group_shape)\n    b_scales_shape = scale_shape(b.shape, b_scale_group_shape)\n\n    scale_a = (torch.randn(a_scales_shape, device=device, dtype=torch.float32))\n    scale_b = (torch.randn(b_scales_shape, device=device, dtype=torch.float32))\n\n    # make scales M-major for blockwise quant, doesn't affect 1D scales\n    scale_a = scale_a.t().contiguous().t()\n    # make scales K-major for blockwise quant, doesn't affect 1D scales\n    scale_b = scale_b.t().contiguous().t()\n\n    if use_bias:\n        bias = torch.rand((n, ), device=device, dtype=out_dtype) * 10\n    else:\n        bias = None\n\n    out = ops.cutlass_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)\n    baseline = baseline_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)\n\n    torch.testing.assert_close(out, baseline, rtol=5e-1, atol=1.5e-1)\n\n    opcheck(torch.ops._C.cutlass_scaled_mm,\n            (out, a, b, scale_a, scale_b, bias))\n\n\ndef cutlass_int8_gemm_helper(m: int,\n                             n: int,\n                             k: int,\n                             a_scale_group_shape: tuple,\n                             b_scale_group_shape: tuple,\n                             use_bias: bool,\n                             out_dtype: type[torch.dtype] = torch.bfloat16,\n                             device: str = \"cuda\"):\n    # Test for a cutlass kernel with per-token activation quantization\n    # and per-output channel weight quantization.\n    a = to_int8(torch.randn((m, k), device=device) * 5)\n    b = to_int8(torch.randn((n, k), device=device).t() * 5)\n\n    a_scales_shape = scale_shape(a.shape, a_scale_group_shape)\n    b_scales_shape = scale_shape(b.shape, b_scale_group_shape)\n\n    scale_a = (torch.randn(a_scales_shape, device=device, dtype=torch.float32))\n    scale_b = (torch.randn(b_scales_shape, device=device, dtype=torch.float32))\n\n    if use_bias:\n        bias = torch.rand((n, ), device=device, dtype=out_dtype) * 10\n    else:\n        bias = None\n\n    out = ops.cutlass_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)\n    baseline = baseline_scaled_mm(a, b, scale_a, scale_b, out_dtype, bias)\n\n    torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)\n\n    opcheck(torch.ops._C.cutlass_scaled_mm,\n            (out, a, b, scale_a, scale_b, bias))\n\n\n@pytest.mark.parametrize(\"m,n,k\", MNK_FACTORS)\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.skipif(not current_platform.has_device_capability(89),\n                    reason=\"FP8 is not supported on this GPU type.\")\ndef test_cutlass_fp8_gemm(m: int, n: int, k: int, a_scale_group_shape,\n                          b_scale_group_shape, use_bias: bool):\n    cutlass_fp8_gemm_helper(m, n, k, a_scale_group_shape, b_scale_group_shape,\n                            use_bias)\n\n\n@pytest.mark.parametrize(\"m,n,k\", MNK_FACTORS)\n@pytest.mark.parametrize(\"a_scale_group_shape,b_scale_group_shape\",\n                         [((1, 128), (128, 128))])\n@pytest.mark.parametrize(\"use_bias\", [False])\n@pytest.mark.skipif(not current_platform.has_device_capability(90),\n                    reason=\"FP8 blockwise is not supported on this GPU type.\")\ndef test_cutlass_fp8_blockwise_scale_gemm(m: int, n: int, k: int,\n                                          a_scale_group_shape,\n                                          b_scale_group_shape, use_bias: bool):\n    if k % b_scale_group_shape[0] != 0 or n % b_scale_group_shape[1] != 0:\n        return\n    if m % a_scale_group_shape[0] != 0 or k % a_scale_group_shape[1] != 0:\n        return\n    if m % 4 != 0 and current_platform.has_device_capability(100):\n        return\n    cutlass_fp8_gemm_helper(m, n, k, a_scale_group_shape, b_scale_group_shape,\n                            use_bias)\n\n\n@pytest.mark.parametrize(\"m,n,k\", MNK_FACTORS)\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\ndef test_cutlass_int8_gemm(m: int, n: int, k: int, a_scale_group_shape,\n                           b_scale_group_shape, use_bias: bool):\n    cutlass_int8_gemm_helper(m, n, k, a_scale_group_shape, b_scale_group_shape,\n                             use_bias)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"out_dtype\", [torch.bfloat16, torch.float16])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\ndef test_cutlass_int8_gemm_output_dtype(a_scale_group_shape,\n                                        b_scale_group_shape,\n                                        out_dtype: type[torch.dtype],\n                                        use_bias: bool):\n    cutlass_int8_gemm_helper(512,\n                             512,\n                             512,\n                             a_scale_group_shape,\n                             b_scale_group_shape,\n                             use_bias,\n                             out_dtype=out_dtype)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"out_dtype\", [torch.bfloat16, torch.float16])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.skipif(not current_platform.has_device_capability(89),\n                    reason=\"FP8 is not supported on this GPU type.\")\ndef test_cutlass_fp8_gemm_output_dtype(a_scale_group_shape,\n                                       b_scale_group_shape,\n                                       out_dtype: type[torch.dtype],\n                                       use_bias: bool):\n    cutlass_fp8_gemm_helper(512,\n                            512,\n                            512,\n                            a_scale_group_shape,\n                            b_scale_group_shape,\n                            use_bias,\n                            out_dtype=out_dtype)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape,b_scale_group_shape\",\n                         [((1, 128), (128, 128))])\n@pytest.mark.parametrize(\"out_dtype\", [torch.bfloat16, torch.float16])\n@pytest.mark.parametrize(\"use_bias\", [False])\n@pytest.mark.skipif(not current_platform.has_device_capability(90),\n                    reason=\"FP8 blockwise is not supported on this GPU type.\")\ndef test_cutlass_fp8_blockwise_scale_gemm_dtype(a_scale_group_shape,\n                                                b_scale_group_shape,\n                                                out_dtype: type[torch.dtype],\n                                                use_bias: bool):\n    cutlass_fp8_gemm_helper(512,\n                            512,\n                            512,\n                            a_scale_group_shape,\n                            b_scale_group_shape,\n                            use_bias,\n                            out_dtype=out_dtype)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n@pytest.mark.skipif(not current_platform.has_device_capability(89),\n                    reason=\"FP8 is not supported on this GPU type.\")\ndef test_cutlass_fp8_gemm_devices(a_scale_group_shape, b_scale_group_shape,\n                                  use_bias: bool, device: str):\n    cutlass_fp8_gemm_helper(512, 512, 512, a_scale_group_shape,\n                            b_scale_group_shape, use_bias, torch.bfloat16,\n                            device)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.parametrize(\"device\", CUDA_DEVICES)\ndef test_cutlass_int8_gemm_devices(a_scale_group_shape, b_scale_group_shape,\n                                   use_bias: bool, device: str):\n    cutlass_int8_gemm_helper(512,\n                             512,\n                             512,\n                             a_scale_group_shape,\n                             b_scale_group_shape,\n                             use_bias,\n                             out_dtype=torch.bfloat16,\n                             device=device)\n\n\n# For the following two tests:\n# N and K correspond to the size of the weight matrix and likely to be multiples\n# of a large power of two. In any case, the kernel will have a naive fallback\n# when N and K are not divisible by 16. But M is the number of tokens and the\n# kernel must handle any M thrown at it.\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.skipif(not current_platform.has_device_capability(89),\n                    reason=\"FP8 is not supported on this GPU type.\")\ndef test_cutlass_fp8_gemm_m_sweep(a_scale_group_shape, b_scale_group_shape,\n                                  use_bias: bool):\n    for nk in range(32, 128, 32):\n        for m in range(1, 128):\n            cutlass_fp8_gemm_helper(m, nk, nk, a_scale_group_shape,\n                                    b_scale_group_shape, use_bias)\n\n\n@pytest.mark.parametrize(\"a_scale_group_shape\",\n                         [PER_TOKEN_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"b_scale_group_shape\",\n                         [PER_OUT_CH_GROUP_SHAPE, TENSORWISE_GROUP_SHAPE])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\ndef test_cutlass_int8_gemm_m_sweep(a_scale_group_shape, b_scale_group_shape,\n                                   use_bias: bool):\n    for nk in range(32, 128, 32):\n        for m in range(1, 128):\n            cutlass_int8_gemm_helper(m, nk, nk, a_scale_group_shape,\n                                     b_scale_group_shape, use_bias)\n\n\n@pytest.mark.parametrize(\"m\", [32, 64, 128])\n@pytest.mark.parametrize(\"n\", [16, 32, 64])\n@pytest.mark.parametrize(\"k\", [64, 128, 256])\n@pytest.mark.parametrize(\"out_dtype\", [torch.bfloat16, torch.float16])\n@pytest.mark.skip\ndef test_cutlass_int8_azp_bias_fold(m: int, n: int, k: int,\n                                    out_dtype: torch.dtype):\n    # Currently, the test is failing because folding azp into\n    # 16-bit bias loses too much precision\n    scale_a = torch.randn((1, 1), device=\"cuda\", dtype=torch.float32) / 10\n    scale_b = torch.randn((1, n), device=\"cuda\", dtype=torch.float32) / 10\n\n    aq_i8 = rand_int8((m, k))\n    bq_i8 = rand_int8((n, k)).t()\n\n    aq_i32 = aq_i8.to(dtype=torch.int32)\n    bq_i32 = bq_i8.to(dtype=torch.int32)\n\n    aq_f32 = aq_i8.to(dtype=torch.float32)\n    bq_f32 = bq_i8.to(dtype=torch.float32)\n\n    b_dq = scale_b * bq_f32\n\n    azp_a = torch.rand((1, ), device=\"cuda\", dtype=torch.float32) * 10 + 1.5\n    azp_aq_i8 = (azp_a / scale_a).to(dtype=torch.int8)\n    azp_a = azp_aq_i8.to(dtype=torch.float32) * scale_a  # correct for rounding\n\n    a_dq = scale_a * (aq_i32 + azp_aq_i8).to(dtype=torch.float32)\n    torch.testing.assert_close(a_dq, scale_a * aq_f32 + azp_a)\n\n    baseline_dq = torch.mm(a_dq, b_dq).to(out_dtype)\n\n    J = torch.ones((1, k), device=\"cuda\", dtype=torch.float32)\n    azp_bias = (azp_a * scale_b * (J @ bq_f32)).to(out_dtype)\n    assert azp_bias.shape == (1, n)\n    assert azp_bias[0, :].shape == (n, )\n\n    baseline_q = (scale_a.to(device='cpu') * scale_b.to(device='cpu') * (\n        (aq_i32 + azp_aq_i8).to(device='cpu') @ bq_i32.to(device='cpu'))).to(\n            dtype=out_dtype, device='cuda')\n\n    out = ops.cutlass_scaled_mm(aq_i8,\n                                bq_i8,\n                                scale_a,\n                                scale_b,\n                                out_dtype=out_dtype,\n                                bias=azp_bias[0, :])\n    torch.testing.assert_close(out, baseline_dq, rtol=1e-2, atol=1e0)\n    torch.testing.assert_close(out, baseline_q, rtol=1e-2, atol=1e0)\n\n\n@pytest.mark.parametrize(\"m\", [32, 64, 128])\n@pytest.mark.parametrize(\"n\", [16, 32, 64])\n@pytest.mark.parametrize(\"k\", [64, 128, 256])\n@pytest.mark.parametrize(\"out_dtype\", [torch.bfloat16, torch.float16])\n@pytest.mark.parametrize(\"use_bias\", [True, False])\n@pytest.mark.parametrize(\"azp_per_token\", [True, False])\ndef test_cutlass_int8_azp(m: int, n: int, k: int, out_dtype: torch.dtype,\n                          use_bias: bool, azp_per_token: bool):\n    m_azp = m if azp_per_token else 1\n    scale_a = torch.randn((m_azp, 1), device=\"cuda\", dtype=torch.float32) / 10\n    scale_b = torch.randn((1, n), device=\"cuda\", dtype=torch.float32) / 10\n\n    aq_i8 = rand_int8((m, k))\n    aq_i32 = aq_i8.to(dtype=torch.int32)\n    aq_f32 = aq_i8.to(dtype=torch.float32)\n\n    bq_i8 = rand_int8((n, k)).t()\n    bq_i32 = bq_i8.to(dtype=torch.int32)\n    bq_f32 = bq_i8.to(dtype=torch.float32)\n    b_dq = scale_b * bq_f32\n\n    azp_a = torch.rand(\n        (m_azp, 1), device=\"cuda\", dtype=torch.float32) * 10 + 1.5\n    azp_aq_i8 = (azp_a / scale_a).to(dtype=torch.int8)\n    azp_a = azp_aq_i8.to(dtype=torch.float32) * scale_a  # correct for rounding\n\n    a_dq = scale_a * (aq_i32 - azp_aq_i8).to(dtype=torch.float32)\n    torch.testing.assert_close(a_dq,\n                               scale_a * aq_f32 - azp_a,\n                               rtol=1e-4,\n                               atol=1e-3)\n\n    if use_bias:\n        bias = torch.rand((1, n), device=\"cuda\", dtype=out_dtype) * 10 + 2.5\n    else:\n        bias = torch.zeros((1, n), device=\"cuda\", dtype=out_dtype)\n\n    baseline_dq = (torch.mm(a_dq, b_dq) + bias).to(out_dtype)\n\n    # int32 mm not supported on CUDA\n    a_noazp_i32_cpu = (aq_i32 - azp_aq_i8).to(device='cpu')\n    cq = (a_noazp_i32_cpu @ bq_i32.to(device='cpu')).to(device='cuda')\n    baseline_q = (scale_a * scale_b * cq + bias).to(dtype=out_dtype)\n\n    # Hadamard is just the sum of the cols\n    azp_adj_i32 = bq_i32.sum(dim=0, keepdim=True, dtype=torch.int32)\n    azp_i32 = azp_aq_i8.to(dtype=torch.int32)\n    func_bias = bias if use_bias else None\n\n    if azp_per_token:\n        out = ops.cutlass_scaled_mm_azp(aq_i8, bq_i8, scale_a, scale_b,\n                                        out_dtype, azp_adj_i32, azp_i32,\n                                        func_bias)\n    else:\n        azp_with_adj_i32 = azp_i32 * azp_adj_i32\n        out = ops.cutlass_scaled_mm_azp(aq_i8, bq_i8, scale_a, scale_b,\n                                        out_dtype, azp_with_adj_i32, None,\n                                        func_bias)\n\n    # bfloat16 precision is 7-bit mantissa -> 2^-8 ~ 0.4%\n    # float16 precision is 10-bit mantissa -> 2^-11 ~ 0.05%\n    rtol = 1e-2 if out_dtype == torch.bfloat16 else 1e-3\n    atol = 1e-3\n    torch.testing.assert_close(out, baseline_dq, rtol=rtol, atol=atol)\n    torch.testing.assert_close(out, baseline_q, rtol=rtol, atol=atol)\n\n    if azp_per_token:\n        opcheck(torch.ops._C.cutlass_scaled_mm_azp,\n                (out, aq_i8, bq_i8, scale_a, scale_b, azp_adj_i32, azp_i32,\n                 func_bias))\n    else:\n        opcheck(torch.ops._C.cutlass_scaled_mm_azp,\n                (out, aq_i8, bq_i8, scale_a, scale_b, azp_with_adj_i32, None,\n                 func_bias))\n\n\n# Test working with a subset of A and B\ndef test_cutlass_subset():\n    big_m, big_n, big_k = 1024, 1024, 1024\n    m, n, k = 512, 512, 512\n\n    whole_a = to_int8(torch.randn((big_m, big_k), device=\"cuda\") * 5)\n    whole_b = to_int8(torch.randn((big_n, big_k), device=\"cuda\").t() * 5)\n    a = whole_a[0:m, 0:k]\n    b = whole_b[0:k, 0:n]\n\n    scale_a = torch.randn((1, 1), device=\"cuda\", dtype=torch.float32) / 10\n    scale_b = torch.randn((1, 1), device=\"cuda\", dtype=torch.float32) / 10\n\n    out = ops.cutlass_scaled_mm(a,\n                                b,\n                                scale_a,\n                                scale_b,\n                                out_dtype=torch.bfloat16)\n    baseline = baseline_scaled_mm(a,\n                                  b,\n                                  scale_a,\n                                  scale_b,\n                                  out_dtype=torch.bfloat16)\n\n    torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)\n\n\n# Test to make sure cuda graphs work\nclass CutlassLayer(torch.nn.Module):\n\n    def __init__(self, b, scale_a, scale_b, out_dtype):\n        super().__init__()\n        self.b = b\n        self.scale_a = scale_a\n        self.scale_b = scale_b\n        self.out_dtype = out_dtype\n\n    def forward(self, a):\n        return ops.cutlass_scaled_mm(a, self.b, self.scale_a, self.scale_b,\n                                     self.out_dtype)\n\n\n@pytest.mark.parametrize(\"per_act_token\", [True, False])\n@pytest.mark.parametrize(\"per_out_ch\", [True, False])\ndef test_cutlass_cuda_graph(per_act_token: bool, per_out_ch: bool):\n    m, n, k = 512, 512, 512\n\n    a = to_int8(torch.randn((m, k), device=\"cuda\"))\n    b = to_int8(torch.randn((n, k), device=\"cuda\").t())\n\n    m_a_scales = m if per_act_token else 1\n    n_b_scales = n if per_out_ch else 1\n\n    scale_a = (torch.randn(\n        (m_a_scales, 1), device=\"cuda\", dtype=torch.float32) / 10)\n    scale_b = (torch.randn(\n        (1, n_b_scales), device=\"cuda\", dtype=torch.float32) / 10)\n\n    # Construct a trivial model with a single layer that calls a CUTLASS kernel\n    model = CutlassLayer(b, scale_a, scale_b, torch.bfloat16)\n\n    # Run the model with a cuda graph\n    stream = torch.cuda.Stream()\n    with torch.cuda.stream(stream):\n        g = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(g):\n            out = model(a)\n    out.zero_()\n    g.replay()\n\n    baseline = torch.mm(scale_a * a.to(dtype=torch.float32),\n                        scale_b * b.to(dtype=torch.float32)).to(torch.bfloat16)\n    torch.testing.assert_close(out, baseline, rtol=1e-1, atol=1e0)\n\n\ndef test_cutlass_support_opcheck():\n    opcheck(torch.ops._C.cutlass_scaled_mm_supports_fp8, (capability, ))\n\n\n@pytest.mark.parametrize(\"num_experts\", [8, 64])\n@pytest.mark.parametrize(\"per_act_token\", [True, False])\n@pytest.mark.parametrize(\"per_out_ch\", [True, False])\n@pytest.mark.parametrize(\"use_bias\", [False])\n@pytest.mark.skipif(\n    (lambda x: x is None or not ops.cutlass_group_gemm_supported(x.to_int()))(\n        current_platform.get_device_capability()),\n    reason=\"Grouped gemm is not supported on this GPU type.\")\ndef test_cutlass_fp8_group_gemm(num_experts: int, per_act_token: bool,\n                                per_out_ch: bool, use_bias: bool):\n\n    # Device and dtype setup\n    device = \"cuda\"\n    out_dtype = torch.half\n\n    # Create separate A, B, C tensors for each group\n    a_tensors = []\n    b_tensors = []\n    a_scales_tensors = []\n    b_scales_tensors = []\n    baseline_tensors = []\n\n    expert_offsets = torch.zeros((num_experts + 1),\n                                 device=device,\n                                 dtype=torch.int64)\n\n    problem_sizes = torch.zeros((num_experts, 3),\n                                device=device,\n                                dtype=torch.int32)\n\n    if not per_act_token:\n        one_scale_a = torch.randn((1, 1), device=device, dtype=torch.float32)\n\n    alignment = 16  # 128 // 8\n    # For variation, each group has dimensions\n    n_g = alignment * random.randint(1, 64)\n    k_g = alignment * random.randint(1, 64)\n    for g in range(num_experts):\n        m_g = alignment * random.randint(1, 64)\n\n        expert_offsets[g + 1] = expert_offsets[g] + m_g\n        problem_sizes[g][0] = m_g\n        problem_sizes[g][1] = n_g\n        problem_sizes[g][2] = k_g\n\n        m_a_scales = m_g if per_act_token else 1\n        n_b_scales = n_g if per_out_ch else 1\n\n        # Create group-specific A and B (FP8) and output (FP16/FP32)\n        a_g = to_fp8(torch.randn((m_g, k_g), device=device))\n        b_g = to_fp8(torch.randn((n_g, k_g), device=device).t())\n        a_tensors.append(a_g)\n        b_tensors.append(b_g)\n\n        # Set up A/B scales\n        scale_b = torch.randn((1, n_b_scales),\n                              device=device,\n                              dtype=torch.float32)\n        b_scales_tensors.append(scale_b)\n\n        if per_act_token:\n            scale_a = torch.randn((m_a_scales, 1),\n                                  device=device,\n                                  dtype=torch.float32)\n            a_scales_tensors.append(scale_a)\n        else:\n            scale_a = one_scale_a\n\n        # Compute baseline result for this group\n        baseline_g = baseline_scaled_mm(a_g, b_g, scale_a, scale_b, out_dtype,\n                                        None)\n        baseline_tensors.append(baseline_g)\n\n    a_tensors_stacked = torch.empty((expert_offsets[num_experts], k_g),\n                                    device=device,\n                                    dtype=torch.float8_e4m3fn)\n    b_tensors_stacked = torch.empty((num_experts, n_g, k_g),\n                                    device=device,\n                                    dtype=torch.float8_e4m3fn)\n\n    for g in range(num_experts):\n        a_tensors_stacked[expert_offsets[g]:expert_offsets[g +\n                                                           1]] = a_tensors[g]\n        b_tensors_stacked[g] = b_tensors[g].t()\n    b_tensors_stacked = b_tensors_stacked.transpose(1, 2)\n\n    if per_act_token:\n        a_scales_tensors_stacked = torch.empty(\n            (expert_offsets[num_experts], 1),\n            device=device,\n            dtype=torch.float32)\n        for g in range(num_experts):\n            a_scales_tensors_stacked[\n                expert_offsets[g]:expert_offsets[g + 1]] = a_scales_tensors[g]\n    else:\n        a_scales_tensors_stacked = one_scale_a\n\n    b_scales_tensors_stacked = torch.empty((num_experts, n_b_scales),\n                                           device=device,\n                                           dtype=torch.float32)\n    for g in range(num_experts):\n        b_scales_tensors_stacked[g] = b_scales_tensors[g]\n\n    out_tensors_stacked = torch.zeros((expert_offsets[num_experts], n_g),\n                                      device=device,\n                                      dtype=out_dtype)\n\n    ab_strides = torch.full((num_experts, ),\n                            a_tensors_stacked.stride(0),\n                            device=\"cuda\",\n                            dtype=torch.int64)\n    c_strides = torch.full((num_experts, ),\n                           out_tensors_stacked.stride(0),\n                           device=\"cuda\",\n                           dtype=torch.int64)\n\n    ops.cutlass_moe_mm(out_tensors_stacked, a_tensors_stacked,\n                       b_tensors_stacked, a_scales_tensors_stacked,\n                       b_scales_tensors_stacked, expert_offsets[:-1],\n                       problem_sizes, ab_strides, ab_strides, c_strides,\n                       per_act_token, per_out_ch)\n\n    # Validate each group's result against the baseline\n    for g in range(num_experts):\n        baseline = baseline_tensors[g]\n        c = out_tensors_stacked[expert_offsets[g]:expert_offsets[g + 1]]\n        torch.testing.assert_close(c, baseline, rtol=1e-2, atol=5e-4)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/56fe4c29_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/526de822_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 172, "symbols_collected": 3515, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.cpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_gpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_xpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.openvino_executor", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand_slice", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 172, "symbols_collected": 3515, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.cpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_gpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_xpu_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.openvino_executor", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_expand_slice", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.bgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_expand", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.sgmv_shrink", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-6ce01f3", "created_at": "2024-08-02T01:29:52+00:00", "base_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5", "head_commit": "6ce01f30667bbae33f112152e07a3b66b841078f", "patch": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex e29eba375..d81648caa 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -700,5 +700,5 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         if self.enable_caching:\n-            for seq in seq_group.seqs_dict.values():\n+            for seq in seq_group.get_seqs():\n                 self.compute_full_blocks_in_seq(seq)\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ab50cfdfd..7ef9387c6 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -444,6 +444,7 @@ class SequenceGroup:\n         prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n     ) -> None:\n         self.request_id = request_id\n+        self.seqs = seqs\n         self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n         self.sampling_params = sampling_params\n         self.metrics = RequestMetrics(arrival_time=arrival_time,\n@@ -458,25 +459,24 @@ class SequenceGroup:\n         self.prompt_adapter_request = prompt_adapter_request\n         self.encoder_seq = encoder_seq\n         self.trace_headers = trace_headers\n-        self._first_seq = next(iter(self.seqs_dict.values()))\n \n     @property\n     def prompt(self) -> Optional[str]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return self._first_seq.prompt\n+        return self.seqs[0].prompt\n \n     @property\n     def prompt_token_ids(self) -> List[int]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return self._first_seq.prompt_token_ids\n+        return self.seqs[0].prompt_token_ids\n \n     @property\n     def multi_modal_data(self) -> \"MultiModalDataDict\":\n         # All sequences in the group should have the same multi-modal data.\n         # We use the multi-modal data of an arbitrary sequence.\n-        return self._first_seq.multi_modal_data\n+        return self.seqs[0].multi_modal_data\n \n     @property\n     def lora_int_id(self) -> int:\n@@ -512,7 +512,7 @@ class SequenceGroup:\n         #   in TPOT, rather than recalculating TTFT (since from the )\n         #   POV of the user, there is simply a long generation delay.\n         if (self.metrics.first_token_time is None\n-                and self.get_seqs()[0].get_output_len() == 1):\n+                and self.seqs[0].get_output_len() == 1):\n             self.metrics.first_token_time = time\n \n     def maybe_set_first_scheduled_time(self, time: float) -> None:\n@@ -548,9 +548,9 @@ class SequenceGroup:\n         self,\n         status: Optional[SequenceStatus] = None,\n     ) -> List[Sequence]:\n-        return list(self.seqs_dict.values()) if status is None else [\n-            seq for seq in self.seqs_dict.values() if seq.status == status\n-        ]\n+        if status is None:\n+            return self.seqs\n+        return [seq for seq in self.seqs if seq.status == status]\n \n     def is_encoder_decoder(self) -> bool:\n         return self.encoder_seq is not None\n@@ -559,22 +559,20 @@ class SequenceGroup:\n         return self.encoder_seq\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n-        return [\n-            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n-        ]\n+        return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n-        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n+        return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n-        for seq in self.seqs_dict.values():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n     def get_num_uncomputed_tokens(self) -> int:\n         num_uncomputed_tokens = 0\n-        for seq in self.get_seqs():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n         return num_uncomputed_tokens\n@@ -583,7 +581,7 @@ class SequenceGroup:\n         # Optimization. We don't need to call get_seqs if we don't need to\n         # filter by states.\n         if status is None:\n-            return len(self.seqs_dict)\n+            return len(self.seqs)\n \n         return len(self.get_seqs(status))\n \n@@ -602,23 +600,25 @@ class SequenceGroup:\n         if seq.seq_id in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n         self.seqs_dict[seq.seq_id] = seq\n+        self.seqs.append(seq)\n \n     def remove(self, seq_id: int) -> None:\n-        if seq_id not in self.seqs_dict:\n+        seq = self.seqs_dict.pop(seq_id, None)\n+        if seq is None:\n             raise ValueError(f\"Sequence {seq_id} not found.\")\n-        del self.seqs_dict[seq_id]\n+        self.seqs.remove(seq)\n \n     def is_finished(self) -> bool:\n-        return all(seq.is_finished() for seq in self.get_seqs())\n+        return all(seq.is_finished() for seq in self.seqs)\n \n     def is_prefill(self) -> bool:\n         # Every sequence should be in the same stage.\n-        return self.get_seqs()[0].is_prefill()\n+        return self.seqs[0].is_prefill()\n \n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\n                 f\"sampling_params={self.sampling_params}, \"\n-                f\"num_seqs={len(self.seqs_dict)})\")\n+                f\"num_seqs={len(self.seqs)})\")\n \n \n class SequenceGroupMetadata:\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nindex 76f418674..001af67f3 100644\n--- a/vllm/transformers_utils/detokenizer.py\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -40,7 +40,7 @@ class Detokenizer:\n         assert prms is not None\n \n         # We can pick any sequence for the prompt.\n-        seq = next(iter(seq_group.seqs_dict.values()))\n+        seq = seq_group.get_seqs()[0]\n         # Only prompt, without the generated token.\n         all_token_ids = seq.get_token_ids()\n         prompt_token_ids = all_token_ids[:-1]", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm import SamplingParams\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\n@pytest.mark.parametrize(\"block_size\", [16])\ndef test_computed_prefix_blocks(model: str, block_size: int):\n    # This test checks if we are able to run the engine to completion\n    # without triggering asserts.\n    # We are in a scenario where all blocks from the second request's prompt\n    # are full and already computed when the second request arrives.\n    prompt = (\n        \"You are a helpful assistant. How do I build a car from cardboard and \"\n        \"paper clips? Is there an easy to follow video tutorial available \"\n        \"online for free?\")\n    prompt2 = (\n        \" Please recommend to me some resources where I can learn not only to \"\n        \"handle technical difficulties of building a car, but also \"\n        \"decoration.\")\n\n    engine_args = EngineArgs(model=model,\n                             block_size=block_size,\n                             enable_prefix_caching=True)\n\n    engine = LLMEngine.from_engine_args(engine_args)\n    sampling_params = SamplingParams()\n\n    engine.add_request(\"0\", prompt + prompt2, sampling_params)\n    engine.step()\n    engine.add_request(\"1\", prompt, sampling_params)\n    engine.step()"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Performance] Optimize `get_seqs` (#7051)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/6a11fdfb_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/6ce01f30_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-6d646d0", "created_at": "2024-09-03T18:50:29+00:00", "base_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889", "head_commit": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e", "patch": "diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex d054ca341..0cbe8371e 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -103,13 +103,13 @@ async def test_multi_step(\n         model,\n         server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n     test_completions = await completions_with_server_args(\n         prompts,\n         model,\n         ms_server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n \n     # Assert multi-step scheduling produces identical tokens\n     # to single-step scheduling.\ndiff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 159281dab..7fe8053ff 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -280,40 +280,27 @@ class _AsyncLLMEngine(LLMEngine):\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n \n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -351,26 +338,20 @@ class _AsyncLLMEngine(LLMEngine):\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             # Execute the model.\n             output = await self.model_executor.execute_model_async(\n                 execute_model_req)\n+\n             # we need to do this here so that last step's sampled_token_ids can\n             # be passed to the next iteration for PP.\n             if self.scheduler_config.is_multi_step:\n                 self._update_cached_scheduler_output(virtual_engine, output)\n         else:\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             output = []\n \n         # Finish the current step for all the sequence groups.\n@@ -384,24 +365,22 @@ class _AsyncLLMEngine(LLMEngine):\n                 self.cached_scheduler_outputs[\n                     virtual_engine] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(\n-                        output\n-                    ) == 1, \"Multi step decoding does not work with async output processing.\"  # noqa: E501\n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+            if output and allow_async_output_proc:\n+                assert len(\n+                    output\n+                ) == 1, \"Async postprocessor expects only a single output set\"\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -411,17 +390,12 @@ class _AsyncLLMEngine(LLMEngine):\n \n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n         return ctx.request_outputs\n@@ -640,6 +614,17 @@ class AsyncLLMEngine:\n         self.log_requests = log_requests\n         self.engine = self._init_engine(*args, **kwargs)\n \n+        # This ensures quick processing of request outputs\n+        # so the append to asyncio queues is not delayed,\n+        # especially for multi-step.\n+        #\n+        # TODO: Currently, disabled for engine_use_ray, ask\n+        # Cody/Will/Woosuk about this case.\n+        self.use_process_request_outputs_callback = not self.engine_use_ray\n+        if self.use_process_request_outputs_callback:\n+            self.engine.process_request_outputs_callback = \\\n+                self.process_request_outputs\n+\n         if self.engine_use_ray:\n             print_warning_once(\n                 \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n@@ -883,13 +868,27 @@ class AsyncLLMEngine:\n             request_outputs = await self.engine.step_async(virtual_engine)\n \n         # Put the outputs into the corresponding streams.\n-        finished = True\n+        # If used as a callback, then already invoked inside\n+        # LLMEngine's _process_model_outputs\n+        if not self.use_process_request_outputs_callback:\n+            all_finished = self.process_request_outputs(request_outputs)\n+        else:\n+            # For callback case, we only need to detect when all\n+            # requests are finished\n+            all_finished = all(request_output.finished\n+                               for request_output in request_outputs)\n+\n+        return not all_finished\n+\n+    def process_request_outputs(self, request_outputs) -> bool:\n+        # Put the outputs into the corresponding streams.\n+        all_finished = True\n         for request_output in request_outputs:\n             self._request_tracker.process_request_output(\n                 request_output, verbose=self.log_requests)\n-            finished = finished and request_output.finished\n+            all_finished = all_finished and request_output.finished\n \n-        return not finished\n+        return all_finished\n \n     async def _engine_abort(self, request_ids: Iterable[str]):\n         if self.engine_use_ray:\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 1eab83f3b..8c5ca81fb 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -93,13 +93,14 @@ class SchedulerOutputState:\n @dataclass\n class SchedulerContext:\n     output_queue: Deque[Tuple[Optional[List[SamplerOutput]],\n-                              List[SequenceGroupMetadata],\n-                              SchedulerOutputs]] = field(\n-                                  default_factory=lambda: deque())\n-\n+                              List[SequenceGroupMetadata], SchedulerOutputs,\n+                              bool,\n+                              bool]] = field(default_factory=lambda: deque())\n     request_outputs: List[Union[RequestOutput,\n                                 EmbeddingRequestOutput]] = field(\n                                     default_factory=lambda: [])\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n \n class LLMEngine:\n@@ -357,6 +358,26 @@ class LLMEngine:\n             # different process.\n             self.tokenizer.ping()\n \n+        self.cached_scheduler_outputs = [\n+            SchedulerOutputState()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.scheduler_contexts = [\n+            SchedulerContext()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.async_callbacks = [\n+            functools.partial(self._process_model_outputs,\n+                              ctx=self.scheduler_contexts[v_id])\n+            for v_id in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        # Currently used by AsyncLLMEngine to ensure quick append\n+        # of request outputs to asyncio queues\n+        self.process_request_outputs_callback = None\n+\n         # Create the scheduler.\n         # NOTE: the cache_config here have been updated with the numbers of\n         # GPU and CPU blocks, which are profiled in the distributed executor.\n@@ -364,9 +385,7 @@ class LLMEngine:\n             Scheduler(\n                 scheduler_config, cache_config, lora_config,\n                 parallel_config.pipeline_parallel_size,\n-                functools.partial(self._process_model_outputs,\n-                                  virtual_engine=v_id,\n-                                  is_async=True)\n+                self.async_callbacks[v_id]\n                 if model_config.use_async_output_proc else None)\n             for v_id in range(parallel_config.pipeline_parallel_size)\n         ]\n@@ -417,30 +436,6 @@ class LLMEngine:\n                 ),\n             ))\n \n-        self.cached_scheduler_outputs = [\n-            SchedulerOutputState()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.scheduler_contexts = [\n-            SchedulerContext()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=True)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback_multi_step = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=False)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n     def _initialize_kv_caches(self) -> None:\n         \"\"\"Initialize the KV cache in the worker(s).\n \n@@ -1249,11 +1244,7 @@ class LLMEngine:\n \n         return\n \n-    def _process_model_outputs(self,\n-                               virtual_engine: int,\n-                               is_async: bool,\n-                               sampler_output: Optional[SamplerOutput] = None,\n-                               is_last_output: bool = False) -> None:\n+    def _process_model_outputs(self, ctx: SchedulerContext) -> None:\n         \"\"\"Apply the model output to the sequences in the scheduled seq groups.\n \n         virtual_engine: The engine id to operate on\n@@ -1273,24 +1264,12 @@ class LLMEngine:\n         \"\"\"\n         now = time.time()\n \n-        is_multi_step = sampler_output is not None\n-\n-        ctx: SchedulerContext = self.scheduler_contexts[virtual_engine]\n-\n         if len(ctx.output_queue) == 0:\n             return None\n \n-        if is_multi_step:\n-            # Async + multi-step case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue[0]\n-            assert outputs is None\n-            outputs = [sampler_output]\n-        else:\n-            # Async standard case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue.popleft()\n-\n+        # Get pending async postprocessor\n+        (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n+         is_last_step) = ctx.output_queue.popleft()\n         assert outputs is not None\n \n         # Sanity check\n@@ -1306,6 +1285,7 @@ class LLMEngine:\n             outputs_by_sequence_group = outputs\n \n         finished_before: List[int] = []\n+        finished_now: List[int] = []\n         for i, seq_group_meta in enumerate(seq_group_metadata_list):\n             scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n@@ -1343,26 +1323,44 @@ class LLMEngine:\n \n             if self.model_config.embedding_mode:\n                 self._process_sequence_group_outputs(seq_group, output)\n-                continue\n+            else:\n+                self.output_processor.process_prompt_logprob(seq_group, output)\n+                if seq_group_meta.do_sample:\n+                    self.output_processor.process_outputs(\n+                        seq_group, output, is_async)\n \n-            self.output_processor.process_prompt_logprob(seq_group, output)\n-            if seq_group_meta.do_sample:\n-                self.output_processor.process_outputs(seq_group, output,\n-                                                      is_async)\n+            if seq_group.is_finished():\n+                finished_now.append(i)\n \n-        # For async + multi-step, free finished seqs and create outputs\n-        # only on the final step.\n-        if is_multi_step and not is_last_output:\n-            return\n+        # Generate outputs for the requests that finished this iteration\n+        for i in finished_now:\n+            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n-        for scheduler in self.scheduler:\n-            scheduler.free_finished_seq_groups()\n+            seq_group = scheduled_seq_group.seq_group\n+            seq_group.maybe_set_first_token_time(now)\n+            request_output = RequestOutputFactory.create(seq_group)\n+            ctx.request_outputs.append(request_output)\n \n-        # Create the outputs.\n-        for i, _ in enumerate(seq_group_metadata_list):\n-            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n+        # Free currently finished requests\n+        if finished_now:\n+            for scheduler in self.scheduler:\n+                scheduler.free_finished_seq_groups()\n+\n+        # For multi-step, do not create outputs each iteration\n+        if not is_last_step:\n+            # Immediately process request outputs here (if callback is given)\n+            if (finished_now\n+                    and self.process_request_outputs_callback is not None):\n+                self.process_request_outputs_callback(ctx.request_outputs)\n+            return\n+\n+        # Create the outputs\n+        # Note: scheduled_seq_groups and seq_group_metadata_list\n+        # must match with the indices\n+        for i, scheduled_seq_group in enumerate(\n+                scheduler_outputs.scheduled_seq_groups):\n \n-            if not is_multi_step and i in finished_before:\n+            if i in finished_before or i in finished_now:\n                 continue  # Avoids double processing\n \n             seq_group = scheduled_seq_group.seq_group\n@@ -1376,11 +1374,15 @@ class LLMEngine:\n             request_output = RequestOutputFactory.create(seq_group)\n             ctx.request_outputs.append(request_output)\n \n-        # For async + multi-step, do stats only on the last output.\n-        # Otherwise, do stats if the execution is async\n-        do_stats = is_multi_step or is_async\n+        # Immediately process request outputs here (if callback is given)\n+        if (ctx.request_outputs\n+                and self.process_request_outputs_callback is not None):\n+            self.process_request_outputs_callback(ctx.request_outputs)\n \n-        if do_stats:\n+        # For async case, we need to record the stats here.\n+        # For non-async case, the stats are done in the\n+        # LLMEngine/AsyncLLMEngine directly\n+        if is_async:\n             # Log stats.\n             self.do_log_stats(scheduler_outputs, outputs, finished_before)\n \n@@ -1485,40 +1487,26 @@ class LLMEngine:\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # Skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n-\n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -1555,13 +1543,8 @@ class LLMEngine:\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             output = self.model_executor.execute_model(\n                 execute_model_req=execute_model_req)\n@@ -1573,10 +1556,8 @@ class LLMEngine:\n         else:\n             # Nothing scheduled => If there is pending async postprocessor,\n             # then finish it here.\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             # No outputs in this case\n             output = []\n \n@@ -1590,28 +1571,24 @@ class LLMEngine:\n             if self.scheduler_config.is_multi_step:\n                 self.cached_scheduler_outputs[0] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                # Add results to the output_queue\n-                # (for async or non-async postprocessing)\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            # Add results to the output_queue\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(output) == 1, (\n-                        \"Multi step decoding does not work \"\n-                        \"with async output processing.\")\n+            if output and allow_async_output_proc:\n+                assert len(output) == 1, (\n+                    \"Async postprocessor expects only a single output set\")\n \n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             # Check if need to run the usual non-async path\n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -1620,17 +1597,12 @@ class LLMEngine:\n                 self.do_tracing(scheduler_outputs)\n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n             # Stop the execute model loop in parallel workers until there are\ndiff --git a/vllm/engine/output_processor/multi_step.py b/vllm/engine/output_processor/multi_step.py\nindex e182cee8b..c73db765f 100644\n--- a/vllm/engine/output_processor/multi_step.py\n+++ b/vllm/engine/output_processor/multi_step.py\n@@ -85,9 +85,6 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             no tokens need to be appended since it is already done\n             externally (before the next schedule() call)\n         \"\"\"\n-        # TODO: Add support for async if necessary\n-        assert not is_async\n-\n         # Sequences can be in RUNNING or FINISHED_ABORTED state\n         # once scheduled, as a sequence is moved to FINSIHED_ABORTED\n         # if a client disconnects from the api server.\n@@ -101,19 +98,41 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             \"Beam search not supported in multi-step decoding.\")\n         seq = seqs[0]\n \n-        # Since there's only one sequence per sequence group, we can take the\n-        # first sample.\n-        samples = [output.samples[0] for output in outputs]\n-\n-        # -1 means the output token is not valid (eg. due to spec decode\n-        # rejecting tokens).\n-        valid_samples = [\n-            sample for sample in samples if sample.output_token != -1\n-        ]\n-        assert valid_samples\n-\n-        self._process_seq_outputs(seq, valid_samples,\n-                                  sequence_group.sampling_params)\n+        if is_async:\n+            # Async case: We process tokens one by one. Here, we know the token\n+            # was already appended, so we only need to do the rest of the\n+            # postprocessor: Detokenization + stopping logic\n+            self._process_decode_and_stop(seq, sequence_group.sampling_params)\n+        else:\n+            # Standard multi-step case\n+\n+            # Since there's only one sequence per sequence group,\n+            # we can take the first sample.\n+            samples = [output.samples[0] for output in outputs]\n+\n+            # -1 means the output token is not valid (eg. due to spec decode\n+            # rejecting tokens).\n+            valid_samples = [\n+                sample for sample in samples if sample.output_token != -1\n+            ]\n+            assert valid_samples\n+\n+            self._process_seq_outputs(seq, valid_samples,\n+                                      sequence_group.sampling_params)\n+\n+    def _process_decode_and_stop(self, seq: Sequence,\n+                                 sampling_params: SamplingParams) -> None:\n+        new_char_count = 0\n+        if sampling_params.detokenize:\n+            new_char_count = self.detokenizer.decode_sequence_inplace(\n+                seq, sampling_params)\n+\n+        # TODO(sang): Support lora.\n+        self.stop_checker.maybe_stop_sequence(\n+            seq,\n+            new_char_count=new_char_count,\n+            sampling_params=sampling_params,\n+        )\n \n     def _process_seq_outputs(self, seq: Sequence,\n                              valid_samples: List[SequenceOutput],\n@@ -151,16 +170,7 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n                 logprobs=output_logprob,\n             )\n \n-            new_char_count = 0\n-            if sampling_params.detokenize:\n-                new_char_count = self.detokenizer.decode_sequence_inplace(\n-                    seq, sampling_params)\n+            self._process_decode_and_stop(seq, sampling_params)\n \n-            # TODO(sang): Support lora.\n-            self.stop_checker.maybe_stop_sequence(\n-                seq,\n-                new_char_count=new_char_count,\n-                sampling_params=sampling_params,\n-            )\n             if seq.is_finished():\n                 break\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 87b3d21fa..a5ebf152c 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1225,7 +1225,6 @@ class ExecuteModelRequest(\n     last_sampled_token_ids: Optional[torch.Tensor] = None\n     # Async callback\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n \n     @property\n     def is_first_multi_step(self) -> bool:\n@@ -1272,5 +1271,4 @@ class ExecuteModelRequest(\n             finished_requests_ids=self.finished_requests_ids,\n             last_sampled_token_ids=self.last_sampled_token_ids.clone()\n             if self.last_sampled_token_ids is not None else None,\n-            async_callback=self.async_callback,\n-            use_async_and_multi_step=self.use_async_and_multi_step)\n+            async_callback=self.async_callback)\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 8a3c99a45..74f7d4e08 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -21,6 +21,7 @@ from vllm.attention.backends.utils import CommonAttentionState\n from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                          ModelConfig, ObservabilityConfig, ParallelConfig,\n                          PromptAdapterConfig, SchedulerConfig)\n+from vllm.core.scheduler import SchedulerOutputs\n from vllm.distributed import get_pp_group\n from vllm.distributed.parallel_state import graph_capture\n from vllm.inputs import INPUT_REGISTRY, InputRegistry\n@@ -96,7 +97,8 @@ class ModelInputForGPU(ModelRunnerInputBase):\n     finished_requests_ids: Optional[List[str]] = None\n     virtual_engine: int = 0\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n         tensor_dict = {\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex be0c75bc0..b52f2a07e 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -22,6 +22,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n+from vllm.utils import PyObjectCache\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -37,6 +38,29 @@ if TYPE_CHECKING:\n logger = init_logger(__name__)\n \n \n+def seq_output_builder():\n+    return SequenceOutput(\n+        0, 0,\n+        {0: Logprob(logprob=float('inf'), rank=None, decoded_token=None)})\n+\n+\n+def completion_seq_group_output_builder():\n+    return CompletionSequenceGroupOutput([], None)\n+\n+\n+# Used by pythonization to reduce python object allocations\n+class PythonizationCache:\n+\n+    def __init__(self):\n+        self.cached_seq_output = PyObjectCache(seq_output_builder)\n+        self.cached_completion_seq_group_output = PyObjectCache(\n+            completion_seq_group_output_builder)\n+\n+    def reset(self):\n+        self.cached_seq_output.reset()\n+        self.cached_completion_seq_group_output.reset()\n+\n+\n @dataclass\n class ModelOutput:\n     \"\"\"The output of a single model forward pass.\n@@ -59,6 +83,7 @@ class ModelOutput:\n     pythonized: bool = False\n     # On-device tensor containing the logprobs of each token.\n     logprobs: Optional[\"torch.Tensor\"] = None\n+    pythonization_cache: Optional[PythonizationCache] = None\n \n     def pythonize(self, input_metadata: \"StatefulModelInput\",\n                   copy_stream: torch.cuda.Stream,\n@@ -97,7 +122,8 @@ class ModelOutput:\n         with torch.cuda.stream(copy_stream):\n             _pythonize_sampler_output(input_metadata, self.sampler_output,\n                                       pinned_sampled_token_buffer,\n-                                      self.sampled_token_ids, self.logprobs)\n+                                      self.sampled_token_ids, self.logprobs,\n+                                      self.pythonization_cache)\n \n         # Erase the logprobs GPU-side tensor.\n         # Note that although _pythonize_sampler_output() runs in its\n@@ -209,6 +235,8 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         self._copy_stream = torch.cuda.Stream()\n         self.pinned_sampled_token_ids: Optional[torch.Tensor] = None\n \n+        self.pythonization_cache = PythonizationCache()\n+\n     def make_model_input_from_broadcasted_tensor_dict(\n             self, tensor_dict: Dict[str, Any]) -> StatefulModelInput:\n         model_input = (StatefulModelInput.from_broadcasted_tensor_dict(\n@@ -237,14 +265,22 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Callable):\n         # Proceed with pythonization and output_proc in order.\n         # Stop on the first one that fails to pythonize\n+        output_proc_callback()\n+\n         cont = True\n         for model_output in model_input.cached_outputs:\n             if not model_output.pythonized:\n                 model_output.maybe_pythonize(model_input, self._copy_stream,\n                                              self.pinned_sampled_token_ids)\n                 if model_output.pythonized:\n-                    output_proc_callback(\n-                        sampler_output=model_output.sampler_output)\n+                    ctx = output_proc_callback.keywords[\"ctx\"]\n+                    is_async = False\n+                    is_last_step = False\n+                    ctx.output_queue.append(\n+                        ([model_output.sampler_output\n+                          ], ctx.seq_group_metadata_list,\n+                         ctx.scheduler_outputs, is_async, is_last_step))\n+                    output_proc_callback()\n                 else:\n                     cont = False\n \n@@ -255,21 +291,46 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Optional[Callable]):\n         assert model_input.frozen_model_input is not None\n \n+        has_async_callback = output_proc_callback is not None\n+\n         outputs = []\n         for output_id in range(len(model_input.cached_outputs)):\n-            is_last_output = output_id == len(model_input.cached_outputs) - 1\n-\n             output = model_input.cached_outputs[output_id]\n-            if not output.pythonized:\n+            is_last_step = output_id == len(model_input.cached_outputs) - 1\n+\n+            # For non-async case:\n+            #   -- We simply add the outputs\n+            # For async case:\n+            #   -- Invoke callback, pythonize, add to callback queue and repeat\n+            #   -- For last output, just add to callback queue\n+            if has_async_callback:\n+                assert output_proc_callback is not None\n+\n+                # Invoke callback before pythonize (to overlap with GPU)\n+                output_proc_callback()\n+\n+                # Pythonize\n+                if not output.pythonized:\n+                    output.pythonize(model_input, self._copy_stream,\n+                                     self.pinned_sampled_token_ids)\n+\n+                    # For non last step, add to callback queue to chain\n+                    # callbacks=>pythonize pairs (for GPU overlap)\n+                    if not is_last_step:\n+                        ctx = output_proc_callback.keywords[  # type: ignore\n+                            \"ctx\"]  # type: ignore\n+                        is_async = False\n+                        is_last_step = False\n+                        ctx.output_queue.append(\n+                            ([output.sampler_output\n+                              ], ctx.seq_group_metadata_list,\n+                             ctx.scheduler_outputs, is_async, is_last_step))\n+                    else:\n+                        outputs.append(output.sampler_output)\n+            else:\n                 output.pythonize(model_input, self._copy_stream,\n                                  self.pinned_sampled_token_ids)\n-\n-                if model_input.frozen_model_input.use_async_and_multi_step:\n-                    assert output_proc_callback is not None\n-                    output_proc_callback(sampler_output=output.sampler_output,\n-                                         is_last_output=is_last_output)\n-\n-            outputs.append(output.sampler_output)\n+                outputs.append(output.sampler_output)\n \n         return outputs\n \n@@ -330,7 +391,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                 model_input, model_input.cached_outputs[-1].sampler_output)\n \n         output_proc_callback = None\n-        if frozen_model_input.use_async_and_multi_step:\n+        if frozen_model_input.async_callback is not None:\n             output_proc_callback = frozen_model_input.async_callback\n             assert output_proc_callback is not None\n             async_callback = functools.partial(\n@@ -367,7 +428,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             model_input.cached_outputs.append(\n                 ModelOutput(output[0], output_ready_event,\n                             output[0].sampled_token_ids, False,\n-                            output[0].logprobs))\n+                            output[0].logprobs, self.pythonization_cache))\n \n             # These GPU tensors are not required by multi-step;\n             # erase them to ensure they are not pythonized or\n@@ -378,7 +439,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n \n             # Pythonize the output if CPU is ahead and the previous step is\n             # ready.\n-            if not frozen_model_input.use_async_and_multi_step:\n+            if frozen_model_input.async_callback is None:\n                 for model_output in model_input.cached_outputs:\n                     model_output.maybe_pythonize(model_input,\n                                                  self._copy_stream,\n@@ -397,6 +458,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         if model_input.is_last_step:\n             outputs = self._final_process_outputs(model_input,\n                                                   output_proc_callback)\n+            self.pythonization_cache.reset()\n             return outputs\n \n         # should be [SamplerOutput]\n@@ -537,6 +599,7 @@ def _pythonize_sampler_output(\n     pinned_sampled_token_buffer: torch.Tensor,\n     sampled_token_ids: torch.Tensor,\n     logprobs_tensor: Optional[torch.Tensor],\n+    cache: Optional[PythonizationCache],\n ) -> None:\n     \"\"\" This function is only called when the output tensors are ready. \n     See :class:`ModelOutput`. \n@@ -597,6 +660,9 @@ def _pythonize_sampler_output(\n \n     for sgdx, (seq_group,\n                sample_result) in enumerate(zip(seq_groups, samples_list)):\n+        if seq_group.sampling_params.logits_processors:\n+            assert len(seq_group.sampling_params.logits_processors) == 0, (\n+                \"Logits Processors are not supported in multi-step decoding\")\n \n         if do_pythonize_logprobs:\n             assert prompt_logprobs is not None\n@@ -621,23 +687,56 @@ def _pythonize_sampler_output(\n         seq_ids = seq_group.seq_ids\n         next_token_ids = sample_result\n         parent_ids = [0]\n-        seq_outputs: List[SequenceOutput] = []\n-        if seq_group.sampling_params.logits_processors:\n-            assert len(seq_group.sampling_params.logits_processors) == 0, (\n-                \"Logits Processors are not supported in multi-step decoding\")\n+\n+        if cache is not None:\n+            completion_seq_group_output: CompletionSequenceGroupOutput = \\\n+                cache.cached_completion_seq_group_output.get_object()\n+            completion_seq_group_output.samples.clear()\n+            seq_outputs: List[\n+                SequenceOutput] = completion_seq_group_output.samples\n+        else:\n+            seq_outputs = []\n+\n         for tdx, (parent_id,\n                   next_token_id) in enumerate(zip(parent_ids, next_token_ids)):\n-            seq_outputs.append(\n-                SequenceOutput(seq_ids[parent_id], next_token_id,\n-                               (group_sample_logprobs[tdx]\n-                                if logprobs_are_requested else {\n-                                    next_token_id:\n-                                    Logprob(logprob=float('inf'),\n-                                            rank=None,\n-                                            decoded_token=None)\n-                                })))\n-        output.outputs.append(\n-            CompletionSequenceGroupOutput(\n-                seq_outputs,\n-                (group_prompt_logprobs if logprobs_are_requested else None)))\n+            if cache is not None:\n+                seq_output: SequenceOutput = cache.cached_seq_output.get_object(\n+                )\n+                seq_output.parent_seq_id = seq_ids[parent_id]\n+                seq_output.output_token = next_token_id\n+\n+                if logprobs_are_requested:\n+                    seq_output.logprobs = group_sample_logprobs[tdx]\n+                else:\n+                    logprobs = next(iter(seq_output.logprobs.values()))\n+                    seq_output.logprobs.clear()\n+\n+                    logprobs.logprob = float('inf')\n+                    logprobs.rank = None\n+                    logprobs.decoded_token = None\n+\n+                    seq_output.logprobs[next_token_id] = logprobs\n+\n+                seq_outputs.append(seq_output)\n+\n+            else:\n+                seq_outputs.append(\n+                    SequenceOutput(seq_ids[parent_id], next_token_id,\n+                                   (group_sample_logprobs[tdx]\n+                                    if logprobs_are_requested else {\n+                                        next_token_id:\n+                                        Logprob(logprob=float('inf'),\n+                                                rank=None,\n+                                                decoded_token=None)\n+                                    })))\n+        if cache is not None:\n+            completion_seq_group_output.prompt_logprobs = \\\n+                group_prompt_logprobs if logprobs_are_requested else None\n+            output.outputs.append(completion_seq_group_output)\n+        else:\n+            output.outputs.append(\n+                CompletionSequenceGroupOutput(\n+                    seq_outputs, (group_prompt_logprobs\n+                                  if logprobs_are_requested else None)))\n+\n     assert len(output.outputs) > 0\ndiff --git a/vllm/worker/multi_step_worker.py b/vllm/worker/multi_step_worker.py\nindex 517b0ab78..562285f82 100644\n--- a/vllm/worker/multi_step_worker.py\n+++ b/vllm/worker/multi_step_worker.py\n@@ -67,9 +67,7 @@ class MultiStepWorker(Worker):\n             if execute_model_req.async_callback:\n                 model_input.frozen_model_input = dataclasses.replace(  # type: ignore\n                     model_input.frozen_model_input,\n-                    async_callback=execute_model_req.async_callback,\n-                    use_async_and_multi_step=execute_model_req.\n-                    use_async_and_multi_step)\n+                    async_callback=execute_model_req.async_callback)\n         else:\n             # on subsequent steps we reuse the worker input and model input\n             multi_step_state = self.multi_step_states[virtual_engine]", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport multiprocessing\nimport os\nimport random\n\nimport pytest\nimport torch\nimport torch.distributed\n\nfrom vllm.distributed.eplb.rebalance_execute import (\n    rearrange_expert_weights_inplace)\nfrom vllm.distributed.parallel_state import (ensure_model_parallel_initialized,\n                                             get_tp_group,\n                                             init_distributed_environment)\nfrom vllm.utils import update_environment_variables\n\n\ndef distributed_run(fn, world_size):\n    number_of_processes = world_size\n    processes: list[multiprocessing.Process] = []\n    for i in range(number_of_processes):\n        env: dict[str, str] = {}\n        env['RANK'] = str(i)\n        env['LOCAL_RANK'] = str(i)\n        env['WORLD_SIZE'] = str(number_of_processes)\n        env['LOCAL_WORLD_SIZE'] = str(number_of_processes)\n        env['MASTER_ADDR'] = 'localhost'\n        env['MASTER_PORT'] = '12345'\n        p = multiprocessing.Process(target=fn, args=(env, ))\n        processes.append(p)\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    for p in processes:\n        assert p.exitcode == 0\n\n\ndef worker_fn_wrapper(fn):\n    # `multiprocessing.Process` cannot accept environment variables directly\n    # so we need to pass the environment variables as arguments\n    # and update the environment variables in the function\n    def wrapped_fn(env):\n        update_environment_variables(env)\n        local_rank = os.environ['LOCAL_RANK']\n        device = torch.device(f\"cuda:{local_rank}\")\n        torch.cuda.set_device(device)\n        init_distributed_environment()\n\n        # Ensure each worker process has the same random seed\n        random.seed(42)\n        torch.manual_seed(42)\n\n        fn()\n\n    return wrapped_fn\n\n\ndef create_expert_indices_with_redundancy(\n        num_layers: int,\n        num_logical_experts: int,\n        total_physical_experts: int,\n        redundancy_config: list[int],  # redundancy for each logical expert\n) -> torch.Tensor:\n    \"\"\"\n    Create expert indices with redundancy.\n    \n    Args:\n        num_layers: number of layers\n        num_logical_experts: number of logical experts\n        total_physical_experts: total number of physical experts\n        redundancy_config: redundancy for each logical expert\n    \n    Returns:\n        indices: Shape (num_layers, total_physical_experts)\n    \"\"\"\n    assert sum(redundancy_config) == total_physical_experts\n    assert len(redundancy_config) == num_logical_experts\n\n    indices = torch.zeros(num_layers, total_physical_experts, dtype=torch.long)\n\n    for layer in range(num_layers):\n        physical_pos = 0\n        for logical_expert_id, redundancy in enumerate(redundancy_config):\n            for _ in range(redundancy):\n                indices[layer, physical_pos] = logical_expert_id\n                physical_pos += 1\n\n    # Shuffle the indices at dim 1\n    for layer in range(num_layers):\n        indices[layer] = indices[layer][torch.randperm(indices.shape[1])]\n\n    return indices\n\n\ndef create_expert_weights(\n    num_layers: int,\n    num_local_experts: int,\n    hidden_sizes: list[int],\n    rank: int,\n    device: torch.device,\n    physical_to_logical_mapping: torch.Tensor,\n) -> list[list[torch.Tensor]]:\n    \"\"\"\n    Create fake expert weights tensor for testing.\n    \n    Use `arange` to generate predictable weights values, based on logical\n    expert ID.\n    All replicas of the same logical expert should have the same weights.\n    \n    Args:\n        physical_to_logical_mapping: Shape (num_layers, num_local_experts)\n            mapping[layer, physical_pos] = logical_expert_id\n    \"\"\"\n    expert_weights = []\n\n    for layer in range(num_layers):\n        layer_weights = []\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            weight_tensor = torch.zeros(num_local_experts,\n                                        hidden_size,\n                                        device=device,\n                                        dtype=torch.float32)\n\n            for local_expert in range(num_local_experts):\n                # Get the logical expert ID for this physical expert\n                global_pos = rank * num_local_experts + local_expert\n                logical_expert_id = physical_to_logical_mapping[\n                    layer, global_pos].item()\n\n                # Generate weights based on logical expert ID\n                # (so that all replicas of the same logical expert have the\n                # same weights)\n                base_value = (logical_expert_id * 1000 + layer * 100 +\n                              weight_idx * 10)\n                weight_tensor[local_expert] = torch.arange(base_value,\n                                                           base_value +\n                                                           hidden_size,\n                                                           device=device,\n                                                           dtype=torch.float32)\n\n            layer_weights.append(weight_tensor)\n        expert_weights.append(layer_weights)\n\n    return expert_weights\n\n\ndef create_redundancy_config(\n    num_logical_experts: int,\n    num_physical_experts: int,\n) -> list[int]:\n    \"\"\"Create a redundancy configuration.\"\"\"\n    redundancy_config = [1] * num_logical_experts\n    remaining = num_physical_experts - num_logical_experts\n    # Randomly assign the remaining physical experts to the logical experts\n    for _ in range(remaining):\n        redundancy_config[random.choice(range(num_logical_experts))] += 1\n    return redundancy_config\n\n\ndef verify_expert_weights_after_shuffle(\n    expert_weights: list[list[torch.Tensor]],\n    new_indices: torch.Tensor,\n    hidden_sizes: list[int],\n    ep_rank: int,\n    num_local_experts: int,\n):\n    \"\"\"Verify the weights after shuffling are correct.\"\"\"\n    num_layers = len(expert_weights)\n\n    for layer in range(num_layers):\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            weight_tensor = expert_weights[layer][weight_idx]\n\n            for local_expert in range(num_local_experts):\n                # Calculate the global expert ID for this local expert\n                global_pos = ep_rank * num_local_experts + local_expert\n                expected_logical_expert = new_indices[layer, global_pos].item()\n\n                # Check if the weights are correct\n                actual_weights = weight_tensor[local_expert]\n                expected_base = (expected_logical_expert * 1000 + layer * 100 +\n                                 weight_idx * 10)\n                expected_weights = torch.arange(expected_base,\n                                                expected_base + hidden_size,\n                                                device=actual_weights.device,\n                                                dtype=actual_weights.dtype)\n\n                torch.testing.assert_close(\n                    actual_weights,\n                    expected_weights,\n                    msg=f\"Layer {layer}, weight {weight_idx},\"\n                    f\"local expert {local_expert}: \"\n                    f\"weights do not match. \"\n                    f\"Expected logical expert {expected_logical_expert}\")\n\n\ndef verify_redundant_experts_have_same_weights(\n    expert_weights: list[list[torch.Tensor]],\n    indices: torch.Tensor,\n    hidden_sizes: list[int],\n    world_size: int,\n    num_local_experts: int,\n):\n    \"\"\"\n    Verify that all replicas of the same logical expert have the same weights.\n    \"\"\"\n    num_layers = len(expert_weights)\n    total_physical_experts = world_size * num_local_experts\n\n    for layer in range(num_layers):\n        # Collect weights for all physical experts for each weight matrix\n        all_weights: list[torch.Tensor] = []\n\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            # Create tensor to store all expert weights\n            # Shape: [total_physical_experts, hidden_size]\n            gathered_weights = torch.zeros(\n                total_physical_experts,\n                hidden_size,\n                device=expert_weights[layer][weight_idx].device,\n                dtype=expert_weights[layer][weight_idx].dtype)\n\n            # Use all_gather to collect expert weights from current node\n            # expert_weights[layer][weight_idx] shape:\n            # [num_local_experts, hidden_size]\n            local_weights = expert_weights[layer][\n                weight_idx]  # [num_local_experts, hidden_size]\n\n            # Split tensor along dim 0 into a list for all_gather\n            gathered_weights_list = torch.chunk(gathered_weights,\n                                                world_size,\n                                                dim=0)\n\n            torch.distributed.all_gather(\n                # Output list: each element corresponds to one rank's weights\n                list(gathered_weights_list),\n                local_weights  # Input: current rank's local weights\n            )\n\n            all_weights.append(gathered_weights)\n\n        # Verify that all replicas of the same logical expert have the same\n        # weights\n        logical_expert_weights: dict[int, dict[int, torch.Tensor]] = {}\n\n        for physical_pos in range(total_physical_experts):\n            logical_expert_id = int(indices[layer, physical_pos].item())\n\n            if logical_expert_id not in logical_expert_weights:\n                # First time encountering this logical expert, save its weights\n                logical_expert_weights[logical_expert_id] = {\n                    weight_idx: all_weights[weight_idx][physical_pos]\n                    for weight_idx in range(len(hidden_sizes))\n                }\n            else:\n                # Verify that current physical expert's weights match the\n                # previously saved logical expert weights\n                for weight_idx in range(len(hidden_sizes)):\n                    torch.testing.assert_close(\n                        all_weights[weight_idx][physical_pos],\n                        logical_expert_weights[logical_expert_id][weight_idx],\n                        msg=f\"Layer {layer}, weight {weight_idx},\"\n                        f\"logical expert {logical_expert_id}: \"\n                        f\"Physical expert {physical_pos} has different weights\"\n                        f\"than expected\")\n\n\n@pytest.mark.parametrize(\n    \"world_size,num_layers,num_local_experts,num_logical_experts\",\n    [\n        # 2 GPU, 2 experts per GPU\n        # 3 logical experts, 4 physical experts, 1 redundant experts\n        (2, 1, 2, 3),\n        # 2 GPU, 3 experts per GPU\n        # 4 logical experts, 6 physical experts, 2 redundant experts\n        (2, 2, 3, 4),\n        # 2 GPU, 8 experts per GPU\n        # 16 logical experts, 16 physical experts, 0 redundant experts\n        (2, 4, 8, 16),\n        # 4 GPU, 2 experts per GPU\n        # 6 logical experts, 8 physical experts, 2 redundant experts\n        (4, 1, 2, 6),\n        # 4 GPU, 2 experts per GPU\n        # 5 logical experts, 8 physical experts, 3 redundant experts\n        (4, 2, 2, 5),\n        # 4 GPU, 8 experts per GPU\n        # 16 logical experts, 32 physical experts, 16 redundant experts\n        (4, 8, 8, 16),\n    ])\ndef test_rearrange_expert_weights_with_redundancy(world_size, num_layers,\n                                                  num_local_experts,\n                                                  num_logical_experts):\n    \"\"\"Test the functionality of rearranging expert weights with redundancy.\"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        # Initialize model parallel (using tensor parallel as an entrypoint\n        # to expert parallel)\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        # Test parameters\n        total_physical_experts = world_size * num_local_experts\n        hidden_sizes = [32, 64]  # Two different weight matrices\n\n        # Create old expert indices (with redundancy)\n        redundancy_config = create_redundancy_config(num_logical_experts,\n                                                     total_physical_experts)\n\n        old_indices = create_expert_indices_with_redundancy(\n            num_layers,\n            num_logical_experts,\n            total_physical_experts,\n            redundancy_config,\n        )\n\n        # Create new expert indices (with redundancy)\n        new_redundancy_config = create_redundancy_config(\n            num_logical_experts, total_physical_experts)\n        new_indices = create_expert_indices_with_redundancy(\n            num_layers,\n            num_logical_experts,\n            total_physical_experts,\n            new_redundancy_config,\n        )\n\n        # Create expert weights\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               old_indices)\n\n        # Execute weight rearrangement\n        rearrange_expert_weights_inplace(\n            old_indices,\n            new_indices,\n            expert_weights,\n            ep_group,\n            is_profile=False,\n        )\n\n        # Verify the rearrangement result\n        verify_expert_weights_after_shuffle(\n            expert_weights,\n            new_indices,\n            hidden_sizes,\n            ep_rank,\n            num_local_experts,\n        )\n\n        verify_redundant_experts_have_same_weights(\n            expert_weights,\n            new_indices,\n            hidden_sizes,\n            world_size,\n            num_local_experts,\n        )\n\n    distributed_run(worker_fn, world_size)\n\n\n@pytest.mark.parametrize(\"world_size\", [2, 4])\ndef test_rearrange_expert_weights_no_change(world_size):\n    \"\"\"\n    Test that when the indices do not change, the weights should remain\n    unchanged.\n    \"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        num_layers = 2\n        num_local_experts = 2\n        total_physical_experts = world_size * num_local_experts\n        num_logical_experts = total_physical_experts // 2  # Some redundancy\n        hidden_sizes = [32, 64]\n\n        # Create redundancy configuration\n        redundancy_config = [2] * num_logical_experts\n\n        # Same indices - no change\n        indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            redundancy_config)\n\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               indices)\n\n        # Save original weights\n        original_weights = []\n        for layer_weights in expert_weights:\n            layer_copy = []\n            for weight in layer_weights:\n                layer_copy.append(weight.clone())\n            original_weights.append(layer_copy)\n\n        # Execute rearrangement (should be no change)\n        rearrange_expert_weights_inplace(\n            indices,\n            indices,  # Same indices\n            expert_weights,\n            ep_group,\n            is_profile=False)\n\n        # Verify that the weights have not changed\n        for layer in range(num_layers):\n            for weight_idx in range(len(hidden_sizes)):\n                torch.testing.assert_close(\n                    expert_weights[layer][weight_idx],\n                    original_weights[layer][weight_idx],\n                    msg=f\"Layer {layer}, weight {weight_idx} should remain \"\n                    f\"unchanged\")\n\n    distributed_run(worker_fn, world_size)\n\n\n@pytest.mark.parametrize(\"world_size\", [2, 4])\ndef test_rearrange_expert_weights_profile_mode(world_size):\n    \"\"\"Test profile mode (should not copy actual weights)\"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        num_layers = 1\n        num_local_experts = 2\n        total_physical_experts = world_size * num_local_experts\n        num_logical_experts = total_physical_experts // 2\n        hidden_sizes = [32]\n\n        # Create different index distributions\n        old_redundancy = create_redundancy_config(num_logical_experts,\n                                                  total_physical_experts)\n        new_redundancy = create_redundancy_config(num_logical_experts,\n                                                  total_physical_experts)\n\n        old_indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            old_redundancy)\n        new_indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            new_redundancy)\n\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               old_indices)\n\n        # Save original weights\n        original_weights = []\n        for layer_weights in expert_weights:\n            layer_copy = []\n            for weight in layer_weights:\n                layer_copy.append(weight.clone())\n            original_weights.append(layer_copy)\n\n        # Execute profile mode rearrangement\n        rearrange_expert_weights_inplace(\n            old_indices,\n            new_indices,\n            expert_weights,\n            ep_group,\n            is_profile=True  # Profile mode\n        )\n\n        # In profile mode, the weights should remain unchanged\n        for layer in range(num_layers):\n            for weight_idx in range(len(hidden_sizes)):\n                torch.testing.assert_close(\n                    expert_weights[layer][weight_idx],\n                    original_weights[layer][weight_idx],\n                    msg=\"In profile mode, the weights should remain unchanged\")\n\n    distributed_run(worker_fn, world_size)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Optimize Async + Multi-step (#8050)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/95a178f8_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/6d646d08_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-6dd94db", "created_at": "2025-01-24T03:34:27+00:00", "base_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad", "head_commit": "6dd94dbe94c1820a1e224cba65efcf0befa97995", "patch": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex cf2f1c6b3..bf1a40d48 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -455,7 +455,6 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                       is not None)\n         self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n-        self.decode_only = True\n \n         # Attention metadata inputs.\n         if self.attn_backend is not None:\n@@ -477,6 +476,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 finished_requests_ids: Optional[List[str]] = None) -> None:\n         self.finished_requests_ids = finished_requests_ids\n \n+        # if the current batch is decode-only.\n+        # will be set to False if there is any non-decode request.\n+        self.decode_only = True\n+\n         # Intermediate data (data in CPU before going to GPU) for\n         # the current sequence group.\n         self.inter_data_list: List[", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nfrom functools import partial\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom vllm import LLM\nfrom vllm.config import ModelImpl\nfrom vllm.engine.llm_engine import LLMEngine as V0LLMEngine\nfrom vllm.utils import GiB_bytes\nfrom vllm.v1.core.kv_cache_utils import get_kv_cache_config\nfrom vllm.v1.engine.core import EngineCore as V1EngineCore\n\nfrom ..utils import create_new_process_for_each_test\nfrom .registry import (_TRANSFORMERS_BACKEND_MODELS, AUTO_EXAMPLE_MODELS,\n                       HF_EXAMPLE_MODELS, HfExampleModels)\nfrom .utils import dummy_hf_overrides\n\n\n@create_new_process_for_each_test()\ndef can_initialize(model_arch: str, monkeypatch: pytest.MonkeyPatch,\n                   EXAMPLE_MODELS: HfExampleModels):\n    \"\"\"The reason for using create_new_process_for_each_test is to avoid\n    the WARNING:\n        \"We must use the 'spawn' multiprocessing start method. Overriding\n        VLLM_WORKER_MULTIPROC_METHOD to 'spawn'.\"\n    The spawn process causes the _initialize_kv_caches_v1 function below to\n    become ineffective.\n    \"\"\"\n\n    model_info = EXAMPLE_MODELS.get_hf_info(model_arch)\n    model_info.check_available_online(on_fail=\"skip\")\n    model_info.check_transformers_version(on_fail=\"skip\")\n\n    hf_overrides_fn = partial(dummy_hf_overrides,\n                              model_arch=model_arch,\n                              exist_overrides=model_info.hf_overrides)\n\n    # Avoid calling model.forward()\n    def _initialize_kv_caches_v0(self) -> None:\n        self.cache_config.num_gpu_blocks = 0\n        self.cache_config.num_cpu_blocks = 0\n\n    def _initialize_kv_caches_v1(self, vllm_config):\n        kv_cache_specs = self.model_executor.get_kv_cache_specs()\n        scheduler_kv_cache_config = get_kv_cache_config(\n            vllm_config,\n            kv_cache_specs[0],\n            10 * GiB_bytes,\n        )\n\n        # gpu_blocks (> 0), cpu_blocks, scheduler_kv_cache_config\n        return 1, 0, scheduler_kv_cache_config\n\n    with (patch.object(V0LLMEngine, \"_initialize_kv_caches\",\n                       _initialize_kv_caches_v0),\n          patch.object(V1EngineCore, \"_initialize_kv_caches\",\n                       _initialize_kv_caches_v1), monkeypatch.context() as m):\n        if model_info.v0_only:\n            m.setenv(\"VLLM_USE_V1\", \"0\")\n        if model_arch == \"Phi4FlashForCausalLM\":\n            # Phi4FlashForCausalLM only supports DIFFERENTIAL_FLASH_ATTN backend\n            m.setenv(\"VLLM_ATTENTION_BACKEND\", \"DIFFERENTIAL_FLASH_ATTN\")\n        if model_arch == \"GptOssForCausalLM\":\n            # FIXME: A hack to bypass FA3 assertion because our CI's L4 GPU\n            # has cc==8.9 which hasn't supported FA3 yet. Remove this hack when\n            # L4 supports FA3.\n            m.setenv(\"VLLM_ATTENTION_BACKEND\", \"TRITON_ATTN_VLLM_V1\")\n        LLM(\n            model_info.default,\n            tokenizer=model_info.tokenizer,\n            tokenizer_mode=model_info.tokenizer_mode,\n            revision=model_info.revision,\n            speculative_config={\n                \"model\": model_info.speculative_model,\n                \"num_speculative_tokens\": 1,\n            } if model_info.speculative_model else None,\n            trust_remote_code=model_info.trust_remote_code,\n            max_model_len=model_info.max_model_len,\n            # these tests seem to produce leftover memory\n            gpu_memory_utilization=0.80,\n            load_format=\"dummy\",\n            model_impl=ModelImpl.TRANSFORMERS\n            if model_arch in _TRANSFORMERS_BACKEND_MODELS else ModelImpl.VLLM,\n            hf_overrides=hf_overrides_fn,\n        )\n\n\n@pytest.mark.parametrize(\"model_arch\", HF_EXAMPLE_MODELS.get_supported_archs())\ndef test_can_initialize(model_arch: str, monkeypatch: pytest.MonkeyPatch):\n    if model_arch == \"Lfm2ForCausalLM\":\n        pytest.skip(\"Skipping until test supports V1-only models\")\n    can_initialize(model_arch, monkeypatch, HF_EXAMPLE_MODELS)\n\n\n@pytest.mark.parametrize(\"model_arch\",\n                         AUTO_EXAMPLE_MODELS.get_supported_archs())\ndef test_implicit_converted_models(model_arch: str,\n                                   monkeypatch: pytest.MonkeyPatch):\n    can_initialize(model_arch, monkeypatch, AUTO_EXAMPLE_MODELS)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[perf] fix perf regression from #12253 (#12380)\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/0e74d797_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/6dd94dbe_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 179, "symbols_collected": 3625, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 179, "symbols_collected": 3625, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-6e36f4f", "created_at": "2024-09-02T21:20:12+00:00", "base_commit": "dd2a6a82e3f41b4673b1dbb24b2e99230ea96981", "head_commit": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65", "patch": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex fc6f829c3..a63ac380e 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -116,6 +116,9 @@ def test_models_with_fp8_kv_cache(\n         pytest.skip(\n             \"#7378: CUDA illegal memory access (undiagnosed) facebook/opt-125m\"\n         )\n+    if ((model, kv_cache_dtype, chunked_prefill_token_size) == (\n+            \"nm-testing/Qwen2-1.5B-Instruct-FP8-K-V\", \"fp8_e4m3\", 4)):\n+        pytest.skip(\"flakey test, see: #7874 #8051\")\n \n     max_num_seqs = chunked_prefill_token_size\n     max_num_batched_tokens = chunked_prefill_token_size\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 4c2f71582..81c78bda3 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -1027,16 +1027,21 @@ class Scheduler:\n \n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n+\n         # Update new running requests.\n-        self.running.extend([s.seq_group for s in prefills.seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in running_scheduled.decode_seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n+        # By default, vLLM scheduler prioritizes prefills.\n+        # Once chunked prefill is enabled,\n+        # the policy is changed to prioritize decode requests.\n         self.running.extend(\n             [s.seq_group for s in swapped_in.decode_seq_groups])\n         self.running.extend(\n             [s.seq_group for s in swapped_in.prefill_seq_groups])\n+        self.running.extend(\n+            [s.seq_group for s in running_scheduled.decode_seq_groups])\n+        self.running.extend(\n+            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n+        self.running.extend([s.seq_group for s in prefills.seq_groups])\n+\n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n         return SchedulerOutputs(", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport copy\nimport time\nimport uuid\nfrom concurrent.futures import Future, ThreadPoolExecutor\n\nimport pytest\nfrom transformers import AutoTokenizer\n\nfrom vllm import SamplingParams\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.platforms import current_platform\nfrom vllm.utils import set_default_torch_num_threads\nfrom vllm.v1.engine import EngineCoreRequest\nfrom vllm.v1.engine.core import EngineCore\nfrom vllm.v1.executor.abstract import Executor, UniProcExecutor\nfrom vllm.v1.kv_cache_interface import KVCacheConfig\nfrom vllm.v1.outputs import ModelRunnerOutput\n\nfrom ...utils import create_new_process_for_each_test, multi_gpu_test\n\nif not current_platform.is_cuda():\n    pytest.skip(reason=\"V1 currently only supported on CUDA.\",\n                allow_module_level=True)\n\nMODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\nPROMPT = \"Hello my name is Robert and I love quantization kernels\"\nPROMPT_TOKENS = TOKENIZER(PROMPT).input_ids\n\n\ndef make_request() -> EngineCoreRequest:\n    return EngineCoreRequest(\n        request_id=str(uuid.uuid4()),\n        prompt_token_ids=PROMPT_TOKENS,\n        mm_features=None,\n        sampling_params=SamplingParams(),\n        pooling_params=None,\n        eos_token_id=None,\n        arrival_time=time.time(),\n        lora_request=None,\n        cache_salt=None,\n        data_parallel_rank=None,\n    )\n\n\n@create_new_process_for_each_test()\ndef test_engine_core(monkeypatch: pytest.MonkeyPatch):\n\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        \"\"\"Setup the EngineCore.\"\"\"\n        engine_args = EngineArgs(model=MODEL_NAME)\n        vllm_config = engine_args.create_engine_config()\n        executor_class = Executor.get_class(vllm_config)\n\n        with set_default_torch_num_threads(1):\n            engine_core = EngineCore(vllm_config=vllm_config,\n                                     executor_class=executor_class,\n                                     log_stats=True)\n        \"\"\"Test basic request lifecycle.\"\"\"\n\n        # First request.\n        engine_core.add_request(\n            *engine_core.preprocess_add_request(make_request()))\n        assert len(engine_core.scheduler.waiting) == 1\n        assert len(engine_core.scheduler.running) == 0\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 1\n\n        # Second request.\n        engine_core.add_request(\n            *engine_core.preprocess_add_request(make_request()))\n        assert len(engine_core.scheduler.waiting) == 1\n        assert len(engine_core.scheduler.running) == 1\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 2\n\n        # Add two requests in a row.\n        engine_core.add_request(\n            *engine_core.preprocess_add_request(make_request()))\n        engine_core.add_request(\n            *engine_core.preprocess_add_request(make_request()))\n        assert len(engine_core.scheduler.waiting) == 2\n        assert len(engine_core.scheduler.running) == 2\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 4\n\n        # Loop through until they are all done.\n        while (outs := engine_core.step()[0].get(0)) and outs.outputs:\n            pass\n\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 0\n        \"\"\"Test abort cycle.\"\"\"\n\n        # Basic abort.\n        req = make_request()\n        request_id = req.request_id\n\n        engine_core.add_request(*engine_core.preprocess_add_request(req))\n        assert len(engine_core.scheduler.waiting) == 1\n        assert len(engine_core.scheduler.running) == 0\n        assert engine_core.scheduler.has_unfinished_requests()\n        assert not engine_core.scheduler.has_finished_requests()\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 1\n        assert engine_core.scheduler.has_unfinished_requests()\n        assert not engine_core.scheduler.has_finished_requests()\n\n        engine_core.abort_requests([request_id])\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 0\n        assert not engine_core.scheduler.has_unfinished_requests()\n        assert engine_core.scheduler.has_finished_requests()\n\n        _ = engine_core.step()\n        assert not engine_core.scheduler.has_unfinished_requests()\n        assert not engine_core.scheduler.has_finished_requests()\n\n        # Add, step, abort 1 of the 3.\n        req0 = make_request()\n        req1 = make_request()\n        req2 = make_request()\n\n        engine_core.add_request(*engine_core.preprocess_add_request(req0))\n        engine_core.add_request(*engine_core.preprocess_add_request(req1))\n        assert len(engine_core.scheduler.waiting) == 2\n        assert len(engine_core.scheduler.running) == 0\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 2\n\n        engine_core.add_request(*engine_core.preprocess_add_request(req2))\n        assert len(engine_core.scheduler.waiting) == 1\n        assert len(engine_core.scheduler.running) == 2\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 3\n\n        # Abort just one.\n        engine_core.abort_requests([req1.request_id])\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 2\n\n        _ = engine_core.step()\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 2\n\n        # Abort the other requests at the same time.\n        engine_core.abort_requests([req2.request_id, req0.request_id])\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 0\n\n        # Sending duplicate requests with same request_id\n        req0 = make_request()\n        req1 = make_request()\n        req0.request_id = req1.request_id = \"test\"\n        engine_core.add_request(*engine_core.preprocess_add_request(req0))\n\n        while (outs := engine_core.step()[0].get(0)) and outs.outputs:\n            pass\n\n        engine_core.add_request(*engine_core.preprocess_add_request(req1))\n        while (outs := engine_core.step()[0].get(0)) and outs.outputs:\n            pass\n\n        assert len(engine_core.scheduler.waiting) == 0\n        assert len(engine_core.scheduler.running) == 0\n\n\n@create_new_process_for_each_test()\ndef test_engine_core_advanced_sampling(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"\n    A basic end-to-end test to verify that the engine functions correctly\n    when additional sampling parameters, such as top_p, min_tokens, and\n    presence_penalty, are set.\n    \"\"\"\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        \"\"\"Setup the EngineCore.\"\"\"\n        engine_args = EngineArgs(model=MODEL_NAME)\n        vllm_config = engine_args.create_engine_config()\n        executor_class = Executor.get_class(vllm_config)\n\n        with set_default_torch_num_threads(1):\n            engine_core = EngineCore(vllm_config=vllm_config,\n                                     executor_class=executor_class,\n                                     log_stats=True)\n        \"\"\"Test basic request lifecycle.\"\"\"\n        # First request.\n        request: EngineCoreRequest = make_request()\n        request.sampling_params = SamplingParams(\n            min_tokens=4,\n            presence_penalty=1.0,\n            frequency_penalty=1.0,\n            repetition_penalty=0.1,\n            stop_token_ids=[1001, 1002],\n        )\n        engine_core.add_request(*engine_core.preprocess_add_request(request))\n\n        def _check_engine_state():\n            assert len(engine_core.scheduler.waiting) == 1\n            assert len(engine_core.scheduler.running) == 0\n            # Loop through until they are all done.\n            while (outs := engine_core.step()[0].get(0)) and outs.outputs:\n                pass\n            assert len(engine_core.scheduler.waiting) == 0\n            assert len(engine_core.scheduler.running) == 0\n\n        _check_engine_state()\n\n        # Second request.\n        request2 = make_request()\n        request2.sampling_params = SamplingParams(\n            top_p=0.99,\n            top_k=50,\n        )\n        engine_core.add_request(*engine_core.preprocess_add_request(request2))\n        _check_engine_state()\n\n\n@create_new_process_for_each_test()\ndef test_engine_core_concurrent_batches(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"\n    Test that the engine can handle multiple concurrent batches.\n    \"\"\"\n\n    def make_request_with_max_tokens(req_id: str,\n                                     max_tokens: int) -> EngineCoreRequest:\n        request = make_request()\n        request.request_id = req_id\n        request.sampling_params.max_tokens = max_tokens\n        return request\n\n    class DummyExecutor(UniProcExecutor):\n\n        def initialize_from_config(\n                self, kv_cache_configs: list[KVCacheConfig]) -> None:\n            super().initialize_from_config(kv_cache_configs)\n\n            # Create a thread pool with a single worker\n            self.thread_pool = ThreadPoolExecutor(max_workers=1)\n\n        def execute_model(\n            self,\n            scheduler_output,\n        ) -> Future[ModelRunnerOutput]:\n            \"\"\"Make execute_model non-blocking.\"\"\"\n\n            def _execute():\n                output = self.collective_rpc(\"execute_model\",\n                                             args=(scheduler_output, ))\n                # Make a copy because output[0] may be reused\n                # by the next batch.\n                return copy.deepcopy(output[0])\n\n            # Use the thread pool instead of creating a new thread\n            return self.thread_pool.submit(_execute)\n\n        @property\n        def max_concurrent_batches(self) -> int:\n            return 2\n\n        def shutdown(self):\n            if hasattr(self, 'thread_pool'):\n                self.thread_pool.shutdown(wait=False)\n\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n\n        engine_args = EngineArgs(\n            model=MODEL_NAME,\n            # To test concurrent batches.\n            max_num_seqs=2,\n            # Avoid all requests being scheduled once.\n            enable_prefix_caching=False,\n            max_num_batched_tokens=10,\n            # Reduce startup time.\n            enforce_eager=True,\n        )\n        vllm_config = engine_args.create_engine_config()\n        with set_default_torch_num_threads(1):\n            engine_core = EngineCore(vllm_config=vllm_config,\n                                     log_stats=False,\n                                     executor_class=DummyExecutor)\n        assert engine_core.batch_queue is not None\n\n        # Add two requests in a row. Each request have 12 prompt tokens.\n        req0 = make_request_with_max_tokens(\"0\", 5)\n        engine_core.add_request(*engine_core.preprocess_add_request(req0))\n        req1 = make_request_with_max_tokens(\"1\", 5)\n        engine_core.add_request(*engine_core.preprocess_add_request(req1))\n\n        # Schedule Batch 1: (10, req0)\n        assert engine_core.step_with_batch_queue()[0] is None\n        assert len(engine_core.batch_queue) == 1\n        scheduler_output = engine_core.batch_queue[-1][1]\n        assert scheduler_output.num_scheduled_tokens[\"0\"] == 10\n        # num_computed_tokens should have been updated immediately.\n        assert engine_core.scheduler.requests[\n            req0.request_id].num_computed_tokens == 10\n\n        # Schedule Batch 2: (2, req0), (8, req1)\n        assert engine_core.step_with_batch_queue()[0] == {}\n        assert len(engine_core.batch_queue) == 1\n        scheduler_output = engine_core.batch_queue[-1][1]\n        assert scheduler_output.num_scheduled_tokens[\"0\"] == 2\n        assert scheduler_output.num_scheduled_tokens[\"1\"] == 8\n        # num_computed_tokens should have been updated immediately.\n        assert engine_core.scheduler.requests[\"0\"].num_computed_tokens == 12\n        assert engine_core.scheduler.requests[\"1\"].num_computed_tokens == 8\n\n        assert engine_core.scheduler.get_num_unfinished_requests() == 2\n\n        # Finish Batch 1 and schedule Batch 3: (4, req1).\n        # Note that req0 cannot be scheduled\n        # because it is in the decoding stage now.\n        engine_core.step_with_batch_queue()\n        assert len(engine_core.batch_queue) == 1\n        scheduler_output = engine_core.batch_queue[-1][1]\n        assert scheduler_output.num_scheduled_tokens[\"1\"] == 4\n\n        # Finish Batch 2. Get first token of req0.\n        # Schedule Batch 4: (1, req0).\n        output = engine_core.step_with_batch_queue()[0].get(0)\n        assert output is not None\n        assert len(output.outputs) == 1\n        assert engine_core.scheduler.requests[req0.request_id].num_tokens == 13\n        scheduler_output = engine_core.batch_queue[-1][1]\n        assert scheduler_output.num_scheduled_tokens[\"0\"] == 1\n\n        # Finish Batch 3. Get first token of req1. Schedule Batch 5: (1, req1).\n        output = engine_core.step_with_batch_queue()[0].get(0)\n        assert output is not None\n        assert len(output.outputs) == 1\n        assert engine_core.scheduler.requests[req1.request_id].num_tokens == 13\n        scheduler_output = engine_core.batch_queue[-1][1]\n        assert scheduler_output.num_scheduled_tokens[\"1\"] == 1\n\n        # Loop until req0 is finished.\n        req_id = 0\n        expected_num_tokens = [\n            engine_core.scheduler.requests[\"0\"].num_tokens + 1,\n            engine_core.scheduler.requests[\"1\"].num_tokens + 1,\n        ]\n        while engine_core.scheduler.get_num_unfinished_requests() == 2:\n            output = engine_core.step_with_batch_queue()[0]\n            # Every step consumes an output.\n            assert output is not None\n            assert len(output[0].outputs) == 1\n            if req_id in engine_core.scheduler.requests:\n                assert engine_core.scheduler.requests[\n                    req_id].num_tokens == expected_num_tokens[req_id]\n            expected_num_tokens[req_id] += 1\n            req_id = (req_id + 1) % 2\n\n\n@multi_gpu_test(num_gpus=2)\ndef test_engine_core_tp(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"\n    Test engine can initialize worker in tp properly\n    \"\"\"\n\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        \"\"\"Setup the EngineCore.\"\"\"\n        engine_args = EngineArgs(\n            model=MODEL_NAME,\n            tensor_parallel_size=2,\n            # Reduce startup time.\n            enforce_eager=True,\n        )\n        vllm_config = engine_args.create_engine_config()\n        executor_class = Executor.get_class(vllm_config)\n\n        with set_default_torch_num_threads(1):\n            engine_core = EngineCore(vllm_config=vllm_config,\n                                     executor_class=executor_class,\n                                     log_stats=True)\n\n        def get_worker_cache_config_field(worker, key: str):\n            return getattr(worker.cache_config, key)\n\n        num_gpu_blocks = engine_core.collective_rpc(\n            get_worker_cache_config_field, args=(\"num_gpu_blocks\", ))\n        num_cpu_blocks = engine_core.collective_rpc(\n            get_worker_cache_config_field, args=(\"num_cpu_blocks\", ))\n        assert all(x is not None for x in num_gpu_blocks)\n        assert all(x is not None for x in num_cpu_blocks)\n\n\n@create_new_process_for_each_test()\ndef test_engine_core_invalid_request_id_type(monkeypatch: pytest.MonkeyPatch):\n    \"\"\"Test that engine raises TypeError for non-string request_id.\"\"\"\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n\n        engine_args = EngineArgs(model=MODEL_NAME)\n        vllm_config = engine_args.create_engine_config()\n        executor_class = Executor.get_class(vllm_config)\n\n        with set_default_torch_num_threads(1):\n            engine_core = EngineCore(vllm_config=vllm_config,\n                                     executor_class=executor_class,\n                                     log_stats=True)\n\n        # Test with UUID object (common mistake)\n        uuid_request = make_request()\n        uuid_request.request_id = uuid.uuid4()  # UUID object instead of string\n\n        with pytest.raises(TypeError,\n                           match=\"request_id must be a string, got.*UUID\"):\n            engine_core.add_request(\n                *engine_core.preprocess_add_request(uuid_request))\n\n        # Test with integer\n        int_request = make_request()\n        int_request.request_id = 12345\n\n        with pytest.raises(TypeError,\n                           match=\"request_id must be a string, got.*int\"):\n            engine_core.add_request(\n                *engine_core.preprocess_add_request(int_request))\n\n        # Test with None\n        none_request = make_request()\n        none_request.request_id = None\n\n        with pytest.raises(TypeError,\n                           match=\"request_id must be a string, got.*NoneType\"):\n            engine_core.add_request(\n                *engine_core.preprocess_add_request(none_request))\n\n        # Verify engine is still functional after errors\n        valid_request = make_request()\n        engine_core.add_request(\n            *engine_core.preprocess_add_request(valid_request))\n        assert len(engine_core.scheduler.waiting) == 1\n        assert len(engine_core.scheduler.running) == 0\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "improve chunked prefill performance\n\n[Bugfix] Fix #7592 vllm 0.5.4 enable_chunked_prefill throughput is slightly lower than 0.5.3~0.5.0. (#7874)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/dd2a6a82_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/6e36f4fa_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-7c01f70", "created_at": "2024-06-29T12:47:53+00:00", "base_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14", "head_commit": "7c01f706418d593b3cf23d2ec9110dca7151c539", "patch": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 13746cef2..22cb26dc0 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -39,24 +39,21 @@ PromptLogprobs = List[Optional[Dict[int, Logprob]]]\n SampleLogprobs = List[Dict[int, Logprob]]\n \n \n-class SequenceStatus(enum.Enum):\n+class SequenceStatus(enum.IntEnum):\n     \"\"\"Status of a sequence.\"\"\"\n-    WAITING = enum.auto()\n-    RUNNING = enum.auto()\n-    SWAPPED = enum.auto()\n-    FINISHED_STOPPED = enum.auto()\n-    FINISHED_LENGTH_CAPPED = enum.auto()\n-    FINISHED_ABORTED = enum.auto()\n-    FINISHED_IGNORED = enum.auto()\n+    WAITING = 0\n+    RUNNING = 1\n+    SWAPPED = 2\n+    # Note: anything after SWAPPED (2) will be considered\n+    # as a finished status.\n+    FINISHED_STOPPED = 3\n+    FINISHED_LENGTH_CAPPED = 4\n+    FINISHED_ABORTED = 5\n+    FINISHED_IGNORED = 6\n \n     @staticmethod\n     def is_finished(status: \"SequenceStatus\") -> bool:\n-        return status in [\n-            SequenceStatus.FINISHED_STOPPED,\n-            SequenceStatus.FINISHED_LENGTH_CAPPED,\n-            SequenceStatus.FINISHED_ABORTED,\n-            SequenceStatus.FINISHED_IGNORED,\n-        ]\n+        return status > SequenceStatus.SWAPPED\n \n     @staticmethod\n     def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\nimport torch\n\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                           SequenceData, SequenceOutput)\n\nfrom .core.utils import create_dummy_prompt\n\n\n@pytest.fixture\ndef sample_outputs():\n    return [\n        CompletionSequenceGroupOutput(samples=[\n            SequenceOutput(parent_seq_id=0, output_token=i, logprobs={})\n        ],\n                                      prompt_logprobs=None) for i in range(5)\n    ]\n\n\n@pytest.fixture\ndef sampler_output(sample_outputs):\n    return SamplerOutput(outputs=sample_outputs)\n\n\ndef test_sampler_output_initialization(sampler_output, sample_outputs):\n    assert len(sampler_output) == len(sample_outputs)\n    assert sampler_output.sampled_token_probs is None\n    assert sampler_output.sampled_token_ids is None\n\n\ndef test_sampler_output_getitem(sampler_output, sample_outputs):\n    assert sampler_output[2] == sample_outputs[2]\n\n\ndef test_sampler_output_setitem(sampler_output):\n    new_output = CompletionSequenceGroupOutput(samples=[\n        SequenceOutput(parent_seq_id=0, output_token=99, logprobs={})\n    ],\n                                               prompt_logprobs=None)\n    sampler_output[2] = new_output\n    assert sampler_output[2] == new_output\n\n\ndef test_sampler_output_len(sampler_output, sample_outputs):\n    assert len(sampler_output) == len(sample_outputs)\n\n\ndef test_sampler_output_eq(sample_outputs):\n    sampler_output1 = SamplerOutput(outputs=sample_outputs)\n    sampler_output2 = SamplerOutput(outputs=sample_outputs.copy())\n    sampler_output3 = SamplerOutput(outputs=sample_outputs[:-1])\n    assert sampler_output1 == sampler_output2\n    assert sampler_output1 != sampler_output3\n\n\ndef test_sequence_data_prefill():\n    seq_data = SequenceData.from_seqs([1, 2, 3, 4])\n    assert seq_data.get_num_uncomputed_tokens() == 4\n    assert seq_data.get_num_computed_tokens() == 0\n    # advance by 2\n    seq_data.update_num_computed_tokens(2)\n    assert seq_data.get_num_uncomputed_tokens() == 2\n    assert seq_data.get_num_computed_tokens() == 2\n\n    # advance by 1\n    seq_data.update_num_computed_tokens(1)\n    assert seq_data.get_num_uncomputed_tokens() == 1\n    assert seq_data.get_num_computed_tokens() == 3\n\n    # append tokens and reset, simulating recompute\n    seq_data.append_token_id(1, logprob=0.0)\n    seq_data.reset_state_for_recompute()\n    assert seq_data.get_num_uncomputed_tokens() == 5\n    assert seq_data.get_num_computed_tokens() == 0\n\n\ndef test_sequence_group_stage():\n    _, seq_group = create_dummy_prompt(\"1\", 12)\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(6)\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(5)\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(1)\n    assert seq_group.is_prefill() is False\n    seqs = seq_group.get_seqs()\n    assert len(seqs) == 1\n    seqs[0].data.append_token_id(1, logprob=0.0)\n    for seq in seq_group.get_seqs():\n        seq.reset_state_for_recompute()\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(5)\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(7)\n    assert seq_group.is_prefill() is True\n    seq_group.update_num_computed_tokens(1)\n    assert seq_group.is_prefill() is False\n\n\ndef test_sequence_intermediate_tensors_equal():\n\n    class AnotherIntermediateTensors(IntermediateTensors):\n        pass\n\n    intermediate_tensors = IntermediateTensors({})\n    another_intermediate_tensors = AnotherIntermediateTensors({})\n    assert intermediate_tensors != another_intermediate_tensors\n\n    empty_intermediate_tensors_1 = IntermediateTensors({})\n    empty_intermediate_tensors_2 = IntermediateTensors({})\n    assert empty_intermediate_tensors_1 == empty_intermediate_tensors_2\n\n    different_key_intermediate_tensors_1 = IntermediateTensors(\n        {\"1\": torch.zeros([2, 4], dtype=torch.int32)})\n    difference_key_intermediate_tensors_2 = IntermediateTensors(\n        {\"2\": torch.zeros([2, 4], dtype=torch.int32)})\n    assert (different_key_intermediate_tensors_1\n            != difference_key_intermediate_tensors_2)\n\n    same_key_different_value_intermediate_tensors_1 = IntermediateTensors(\n        {\"1\": torch.zeros([2, 4], dtype=torch.int32)})\n    same_key_different_value_intermediate_tensors_2 = IntermediateTensors(\n        {\"1\": torch.zeros([2, 5], dtype=torch.int32)})\n    assert (same_key_different_value_intermediate_tensors_1\n            != same_key_different_value_intermediate_tensors_2)\n\n    same_key_same_value_intermediate_tensors_1 = IntermediateTensors(\n        {\"1\": torch.zeros([2, 4], dtype=torch.int32)})\n    same_key_same_value_intermediate_tensors_2 = IntermediateTensors(\n        {\"1\": torch.zeros([2, 4], dtype=torch.int32)})\n    assert (same_key_same_value_intermediate_tensors_1 ==\n            same_key_same_value_intermediate_tensors_2)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/51e971d3_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/7c01f706_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-89a84b0", "created_at": "2024-07-26T04:31:31+00:00", "base_commit": "084a01fd3544557990f8af8af6fd3c1185bae848", "head_commit": "89a84b0bb7b30706a02836234a94493ea8f780bf", "patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 5c376797a..121458f81 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -220,7 +220,7 @@ def _apply_min_tokens_penalty(\n             seqs_to_penalize: List[int] = []\n             for j, seq_id in enumerate(seq_ids):\n                 seq_data = seq_group.seq_data[seq_id]\n-                if len(seq_data.output_token_ids) < min_tokens:\n+                if len(seq_data.output_token_ids_array) < min_tokens:\n                     seqs_to_penalize.append(j)\n \n             if seqs_to_penalize:\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 390b5d173..27b37a9d5 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -1,4 +1,5 @@\n import random\n+from array import array\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple\n \n@@ -329,8 +330,8 @@ class SamplingTensors:\n             user-defined seed for each sequence.\n         extra_entropy: extra entropy to use when generating seeds.\n         \"\"\"\n-        prompt_tokens: List[List[int]] = []\n-        output_tokens: List[List[int]] = []\n+        prompt_tokens: List[array] = []\n+        output_tokens: List[array] = []\n         top_ks: List[int] = []\n         temperatures: List[float] = []\n         top_ps: List[float] = []\n@@ -432,13 +433,15 @@ class SamplingTensors:\n                 if (seq_group.is_prompt\n                         and sampling_params.prompt_logprobs is not None):\n                     prefill_len = len(seq_group.prompt_logprob_indices)\n-                    prompt_tokens.extend([] for _ in range(prefill_len))\n-                    output_tokens.extend([] for _ in range(prefill_len))\n+                    prompt_tokens.extend(\n+                        array('l') for _ in range(prefill_len))\n+                    output_tokens.extend(\n+                        array('l') for _ in range(prefill_len))\n                 if seq_group.do_sample:\n                     for seq_id in seq_ids:\n                         seq_data = seq_group.seq_data[seq_id]\n-                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n-                        output_tokens.append(list(seq_data.output_token_ids))\n+                        prompt_tokens.append(seq_data.prompt_token_ids_array)\n+                        output_tokens.append(seq_data.output_token_ids_array)\n \n         sampling_tensors = SamplingTensors.from_lists(\n             temperatures, top_ps, top_ks, min_ps, presence_penalties,\n@@ -454,9 +457,9 @@ class SamplingTensors:\n                    frequency_penalties: List[float],\n                    repetition_penalties: List[float],\n                    sampling_seeds: List[int], sample_indices: List[int],\n-                   prompt_tokens: List[List[int]],\n-                   output_tokens: List[List[int]], vocab_size: int,\n-                   extra_seeds_to_generate: int, device: torch.device,\n+                   prompt_tokens: List[array], output_tokens: List[array],\n+                   vocab_size: int, extra_seeds_to_generate: int,\n+                   device: torch.device,\n                    dtype: torch.dtype) -> \"SamplingTensors\":\n         # Note that the performance will be very bad without\n         # pinned memory.\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 0cd4c7e71..72821ecea 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -3,6 +3,7 @@ import copy\n import enum\n import math\n from abc import ABC, abstractmethod\n+from array import array\n from collections import defaultdict\n from dataclasses import dataclass, field\n from typing import (TYPE_CHECKING, Dict, List, Mapping, Optional, Set, Tuple,\n@@ -119,10 +120,10 @@ class SequenceData:\n         prompt_token_ids: List[int],\n         output_token_ids: Optional[List[int]] = None,\n     ) -> None:\n-        self._prompt_token_ids: List[int] = list(prompt_token_ids)\n+        self._prompt_token_ids = array('l', prompt_token_ids)\n         self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n-        self._output_token_ids: List[int] = (\n-            list(output_token_ids) if output_token_ids is not None else [])\n+        self._output_token_ids = array(\n+            'l', output_token_ids if output_token_ids is not None else [])\n \n         self.cumulative_logprob = 0.0\n         # The number of tokens that are computed (that run against the model).\n@@ -132,8 +133,8 @@ class SequenceData:\n         self._update_cached_all_tokens()\n \n     def _update_cached_all_tokens(self):\n-        self._cached_all_token_ids: List[int] = (self._prompt_token_ids +\n-                                                 self._output_token_ids)\n+        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n+                                                     self._output_token_ids)\n \n     @property\n     def prompt_token_ids(self) -> Tuple[int, ...]:\n@@ -141,19 +142,27 @@ class SequenceData:\n \n     @prompt_token_ids.setter\n     def prompt_token_ids(self, new_prompt_token_ids) -> None:\n-        self._prompt_token_ids = list(new_prompt_token_ids)\n+        self._prompt_token_ids = array('l', new_prompt_token_ids)\n         self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n         self._update_cached_all_tokens()\n \n+    @property\n+    def prompt_token_ids_array(self) -> array:\n+        return self._prompt_token_ids\n+\n     @property\n     def output_token_ids(self) -> Tuple[int, ...]:\n         return tuple(self._output_token_ids)\n \n     @output_token_ids.setter\n     def output_token_ids(self, new_output_token_ids) -> None:\n-        self._output_token_ids = list(new_output_token_ids)\n+        self._output_token_ids = array('l', new_output_token_ids)\n         self._update_cached_all_tokens()\n \n+    @property\n+    def output_token_ids_array(self) -> array:\n+        return self._output_token_ids\n+\n     def append_token_id(self, token_id: int, logprob: float) -> None:\n         self._output_token_ids.append(token_id)\n         self._cached_all_token_ids.append(token_id)", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport asyncio\nimport os\nfrom typing import Any, Callable, Optional, Union\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport pytest\n\nfrom vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.executor.uniproc_executor import UniProcExecutor\nfrom vllm import SamplingParams\n\n\nclass Mock:\n    ...\n\n\nclass CustomUniExecutor(UniProcExecutor):\n\n    def collective_rpc(self,\n                       method: Union[str, Callable],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict] = None) -> list[Any]:\n        # Drop marker to show that this was ran\n        with open(\".marker\", \"w\"):\n            ...\n        return super().collective_rpc(method, timeout, args, kwargs)\n\n\nCustomUniExecutorAsync = CustomUniExecutor\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor_type_checking(model):\n    with pytest.raises(ValueError):\n        engine_args = EngineArgs(model=model,\n                                 distributed_executor_backend=Mock)\n        LLMEngine.from_engine_args(engine_args)\n    with pytest.raises(ValueError):\n        engine_args = AsyncEngineArgs(model=model,\n                                      distributed_executor_backend=Mock)\n        AsyncLLMEngine.from_engine_args(engine_args)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor(model, tmp_path):\n    cwd = os.path.abspath(\".\")\n    os.chdir(tmp_path)\n    try:\n        assert not os.path.exists(\".marker\")\n\n        engine_args = EngineArgs(\n            model=model,\n            distributed_executor_backend=CustomUniExecutor,\n            enforce_eager=True,  # reduce test time\n        )\n        engine = LLMEngine.from_engine_args(engine_args)\n        sampling_params = SamplingParams(max_tokens=1)\n\n        engine.add_request(\"0\", \"foo\", sampling_params)\n        engine.step()\n\n        assert os.path.exists(\".marker\")\n    finally:\n        os.chdir(cwd)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_custom_executor_async(model, tmp_path):\n    cwd = os.path.abspath(\".\")\n    os.chdir(tmp_path)\n    try:\n        assert not os.path.exists(\".marker\")\n\n        engine_args = AsyncEngineArgs(\n            model=model,\n            distributed_executor_backend=CustomUniExecutorAsync,\n            enforce_eager=True,  # reduce test time\n        )\n        engine = AsyncLLMEngine.from_engine_args(engine_args)\n        sampling_params = SamplingParams(max_tokens=1)\n\n        async def t():\n            stream = await engine.add_request(\"0\", \"foo\", sampling_params)\n            async for x in stream:\n                ...\n\n        asyncio.run(t())\n\n        assert os.path.exists(\".marker\")\n    finally:\n        os.chdir(cwd)\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_respect_ray(model):\n    # even for TP=1 and PP=1,\n    # if users specify ray, we should use ray.\n    # users might do this if they want to manage the\n    # resources using ray.\n    engine_args = EngineArgs(\n        model=model,\n        distributed_executor_backend=\"ray\",\n        enforce_eager=True,  # reduce test time\n    )\n    engine = LLMEngine.from_engine_args(engine_args)\n    assert engine.model_executor.uses_ray"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Use array to speedup padding (#6779)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/084a01fd_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/89a84b0b_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-8bc68e1", "created_at": "2024-05-13T21:57:07+00:00", "base_commit": "0fca3cdcf265cd375bca684d951702b6b7adf65a", "head_commit": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd", "patch": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 4feea786f..3c3da41c3 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -60,11 +60,13 @@ steps:\n   mirror_hardwares: [amd]\n   commands:\n     # install aws cli for llava_example.py\n-    - pip install awscli\n+    # install tensorizer for tensorize_vllm_model.py\n+    - pip install awscli tensorizer\n     - python3 offline_inference.py\n     - python3 offline_inference_with_prefix.py\n     - python3 llm_engine_example.py\n     - python3 llava_example.py\n+    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors\n \n - label: Kernels Test %N\n   command: pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT\ndiff --git a/examples/tensorize_vllm_model.py b/examples/tensorize_vllm_model.py\nindex e2456168d..8b74ae1d7 100644\n--- a/examples/tensorize_vllm_model.py\n+++ b/examples/tensorize_vllm_model.py\n@@ -1,23 +1,20 @@\n import argparse\n import dataclasses\n+import json\n import os\n-import time\n import uuid\n from functools import partial\n-from typing import Type\n \n-import torch\n-import torch.nn as nn\n-from tensorizer import (DecryptionParams, EncryptionParams, TensorDeserializer,\n-                        TensorSerializer, stream_io)\n-from tensorizer.utils import convert_bytes, get_mem_usage, no_init_or_tensor\n-from transformers import AutoConfig, PretrainedConfig\n+from tensorizer import stream_io\n \n-from vllm.distributed import initialize_model_parallel\n+from vllm import LLM\n+from vllm.distributed import (init_distributed_environment,\n+                              initialize_model_parallel)\n from vllm.engine.arg_utils import EngineArgs\n from vllm.engine.llm_engine import LLMEngine\n-from vllm.model_executor.model_loader.tensorizer import TensorizerArgs\n-from vllm.model_executor.models import ModelRegistry\n+from vllm.model_executor.model_loader.tensorizer import (TensorizerArgs,\n+                                                         TensorizerConfig,\n+                                                         serialize_vllm_model)\n \n # yapf conflicts with isort for this docstring\n # yapf: disable\n@@ -27,25 +24,25 @@ deserialize vLLM models. These models can be loaded using tensorizer\n to the GPU extremely quickly over an HTTP/HTTPS endpoint, an S3 endpoint,\n or locally. Tensor encryption and decryption is also supported, although \n libsodium must be installed to use it. Install vllm with tensorizer support \n-using `pip install vllm[tensorizer]`.\n+using `pip install vllm[tensorizer]`. To learn more about tensorizer, visit\n+https://github.com/coreweave/tensorizer\n \n To serialize a model, install vLLM from source, then run something \n like this from the root level of this repository:\n \n python -m examples.tensorize_vllm_model \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n+   --model facebook/opt-125m \\\n    serialize \\\n-   --serialized-directory s3://my-bucket/ \\\n-   --suffix vllm\n+   --serialized-directory s3://my-bucket \\\n+   --suffix v1\n    \n Which downloads the model from HuggingFace, loads it into vLLM, serializes it,\n and saves it to your S3 bucket. A local directory can also be used. This\n assumes your S3 credentials are specified as environment variables\n-in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and `S3_ENDPOINT`.\n-To provide S3 credentials directly, you can provide `--s3-access-key-id` and \n-`--s3-secret-access-key`, as well as `--s3-endpoint` as CLI args to this \n-script.\n+in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and \n+`S3_ENDPOINT_URL`. To provide S3 credentials directly, you can provide \n+`--s3-access-key-id` and `--s3-secret-access-key`, as well as `--s3-endpoint` \n+as CLI args to this script.\n \n You can also encrypt the model weights with a randomly-generated key by \n providing a `--keyfile` argument.\n@@ -57,7 +54,7 @@ python -m examples.tensorize_vllm_model \\\n    --model EleutherAI/gpt-j-6B \\\n    --dtype float16 \\\n    deserialize \\\n-   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/vllm/model.tensors\n+   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/v1/model.tensors\n \n Which downloads the model tensors from your S3 bucket and deserializes them.\n \n@@ -71,26 +68,30 @@ Or for deserializing:\n \n `python -m examples.tensorize_vllm_model deserialize --help`.\n \n-Once a model is serialized, it can be used to load the model when running the\n-OpenAI inference client at `vllm/entrypoints/openai/api_server.py` by providing\n-the `--tensorizer-uri` CLI argument that is functionally the same as the\n-`--path-to-tensors` argument in this script, along with `--vllm-tensorized`, to\n-signify that the model to be deserialized is a vLLM model, rather than a \n-HuggingFace `PreTrainedModel`, which can also be deserialized using tensorizer\n-in the same inference server, albeit without the speed optimizations. To\n-deserialize an encrypted file, the `--encryption-keyfile` argument can be used\n-to provide the path to the keyfile used to encrypt the model weights. For\n-information on all the arguments that can be used to configure tensorizer's\n-deserialization, check out the tensorizer options argument group in the\n-`vllm/entrypoints/openai/api_server.py` script with `--help`.\n-\n-Tensorizer can also be invoked with the `LLM` class directly to load models:\n+Once a model is serialized, tensorizer can be invoked with the `LLM` class \n+directly to load models:\n \n     llm = LLM(model=\"facebook/opt-125m\",\n               load_format=\"tensorizer\",\n-              tensorizer_uri=path_to_opt_tensors,\n-              num_readers=3,\n-              vllm_tensorized=True)\n+              model_loader_extra_config=TensorizerConfig(\n+                    tensorizer_uri = path_to_tensors,\n+                    num_readers=3,\n+                    )\n+              )\n+            \n+A serialized model can be used during model loading for the vLLM OpenAI\n+inference server. `model_loader_extra_config` is exposed as the CLI arg\n+`--model-loader-extra-config`, and accepts a JSON string literal of the\n+TensorizerConfig arguments desired.\n+\n+In order to see all of the available arguments usable to configure \n+loading with tensorizer that are given to `TensorizerConfig`, run:\n+\n+`python -m examples.tensorize_vllm_model deserialize --help`\n+\n+under the `tensorizer options` section. These can also be used for\n+deserialization in this example script, although `--tensorizer-uri` and\n+`--path-to-tensors` are functionally the same in this case.\n \"\"\"\n \n \n@@ -158,95 +159,35 @@ def parse_args():\n         help=(\"Path to a binary key to use to decrypt the model weights,\"\n               \" if the model was serialized with encryption\"))\n \n-    return parser.parse_args()\n-\n-\n-def make_model_contiguous(model):\n-    # Ensure tensors are saved in memory contiguously\n-    for param in model.parameters():\n-        param.data = param.data.contiguous()\n-\n-\n-def _get_vllm_model_architecture(config: PretrainedConfig) -> Type[nn.Module]:\n-    architectures = getattr(config, \"architectures\", [])\n-    for arch in architectures:\n-        model_cls = ModelRegistry.load_model_cls(arch)\n-        if model_cls is not None:\n-            return model_cls\n-    raise ValueError(\n-        f\"Model architectures {architectures} are not supported for now. \"\n-        f\"Supported architectures: {ModelRegistry.get_supported_archs()}\")\n-\n-\n-def serialize():\n-\n-    eng_args_dict = {f.name: getattr(args, f.name) for f in\n-                     dataclasses.fields(EngineArgs)}\n-    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n-    engine = LLMEngine.from_engine_args(engine_args)\n+    TensorizerArgs.add_cli_args(deserialize_parser)\n \n-    model = (engine.model_executor.driver_worker.\n-             model_runner.model)\n-\n-    encryption_params = EncryptionParams.random() if keyfile else None\n-    if keyfile:\n-        with _write_stream(keyfile) as stream:\n-            stream.write(encryption_params.key)\n-\n-    with _write_stream(model_path) as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-        serializer.close()\n+    return parser.parse_args()\n \n-    print(\"Serialization complete. Model tensors saved to\", model_path)\n-    if keyfile:\n-        print(\"Key saved to\", keyfile)\n \n \n def deserialize():\n-    config = AutoConfig.from_pretrained(model_ref)\n-\n-    with no_init_or_tensor():\n-        model_class = _get_vllm_model_architecture(config)\n-        model = model_class(config)\n-\n-    before_mem = get_mem_usage()\n-    start = time.time()\n-\n-    if keyfile:\n-        with _read_stream(keyfile) as stream:\n-            key = stream.read()\n-            decryption_params = DecryptionParams.from_key(key)\n-            tensorizer_args.deserializer_params['encryption'] = \\\n-                decryption_params\n-\n-    with (_read_stream(model_path)) as stream, TensorDeserializer(\n-            stream, **tensorizer_args.deserializer_params) as deserializer:\n-        deserializer.load_into_module(model)\n-        end = time.time()\n-\n-    # Brag about how fast we are.\n-    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n-    duration = end - start\n-    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n-    after_mem = get_mem_usage()\n-    print(\n-        f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\"\n+    llm = LLM(model=args.model,\n+              load_format=\"tensorizer\",\n+              model_loader_extra_config=tensorizer_config\n     )\n-    print(f\"Memory usage before: {before_mem}\")\n-    print(f\"Memory usage after: {after_mem}\")\n+    return llm\n \n-    return model\n \n \n args = parse_args()\n \n-s3_access_key_id = (args.s3_access_key_id or os.environ.get(\"S3_ACCESS_KEY_ID\")\n-                    or None)\n-s3_secret_access_key = (args.s3_secret_access_key\n-                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\") or None)\n+s3_access_key_id = (getattr(args, 's3_access_key_id', None)\n+                    or os.environ.get(\"S3_ACCESS_KEY_ID\", None))\n+s3_secret_access_key = (getattr(args, 's3_secret_access_key', None)\n+                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\", None))\n+s3_endpoint = (getattr(args, 's3_endpoint', None)\n+               or os.environ.get(\"S3_ENDPOINT_URL\", None))\n \n-s3_endpoint = (args.s3_endpoint or os.environ.get(\"S3_ENDPOINT_URL\") or None)\n+credentials = {\n+    \"s3_access_key_id\": s3_access_key_id,\n+    \"s3_secret_access_key\": s3_secret_access_key,\n+    \"s3_endpoint\": s3_endpoint\n+}\n \n _read_stream, _write_stream = (partial(\n     stream_io.open_stream,\n@@ -263,20 +204,41 @@ model_name = model_ref.split(\"/\")[1]\n os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n os.environ[\"MASTER_PORT\"] = \"8080\"\n \n-torch.distributed.init_process_group(world_size=1, rank=0)\n+init_distributed_environment(world_size=1, rank=0, local_rank=0)\n initialize_model_parallel()\n \n keyfile = args.keyfile if args.keyfile else None\n \n+\n+if args.model_loader_extra_config:\n+    config = json.loads(args.model_loader_extra_config)\n+    tensorizer_args = TensorizerConfig(**config)._construct_tensorizer_args()\n+    tensorizer_args.tensorizer_uri = args.path_to_tensors\n+else:\n+    tensorizer_args = None\n+\n if args.command == \"serialize\":\n+    eng_args_dict = {f.name: getattr(args, f.name) for f in\n+                     dataclasses.fields(EngineArgs)}\n+\n+    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n+    engine = LLMEngine.from_engine_args(engine_args)\n+\n     input_dir = args.serialized_directory.rstrip('/')\n     suffix = args.suffix if args.suffix else uuid.uuid4().hex\n     base_path = f\"{input_dir}/vllm/{model_ref}/{suffix}\"\n     model_path = f\"{base_path}/model.tensors\"\n-    serialize()\n+    tensorizer_config = TensorizerConfig(\n+        tensorizer_uri=model_path,\n+        **credentials)\n+    serialize_vllm_model(engine, tensorizer_config, keyfile)\n elif args.command == \"deserialize\":\n-    tensorizer_args = TensorizerArgs.from_cli_args(args)\n-    model_path = args.path_to_tensors\n+    if not tensorizer_args:\n+        tensorizer_config = TensorizerConfig(\n+            tensorizer_uri=args.path_to_tensors,\n+            encryption_keyfile = keyfile,\n+            **credentials\n+        )\n     deserialize()\n else:\n     raise ValueError(\"Either serialize or deserialize must be specified.\")\ndiff --git a/requirements-dev.txt b/requirements-dev.txt\nindex 796c9e37d..4f6c27d95 100644\n--- a/requirements-dev.txt\n+++ b/requirements-dev.txt\n@@ -14,7 +14,7 @@ types-setuptools\n \n # testing\n pytest\n-tensorizer==2.9.0\n+tensorizer>=2.9.0\n pytest-forked\n pytest-asyncio\n pytest-rerunfailures\ndiff --git a/setup.py b/setup.py\nindex 0dc8818b4..a66af2c5d 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -426,7 +426,7 @@ setup(\n     install_requires=get_requirements(),\n     ext_modules=ext_modules,\n     extras_require={\n-        \"tensorizer\": [\"tensorizer==2.9.0\"],\n+        \"tensorizer\": [\"tensorizer>=2.9.0\"],\n     },\n     cmdclass={\"build_ext\": cmake_build_ext} if not _is_neuron() else {},\n     package_data=package_data,\ndiff --git a/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py b/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py\ndeleted file mode 100644\nindex 0e113ab64..000000000\n--- a/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py\n+++ /dev/null\n@@ -1,245 +0,0 @@\n-import argparse\n-import dataclasses\n-import os\n-import time\n-import uuid\n-from functools import partial\n-from typing import Type\n-\n-import torch.nn as nn\n-from tensorizer import (DecryptionParams, EncryptionParams, TensorDeserializer,\n-                        TensorSerializer, stream_io)\n-from tensorizer.utils import convert_bytes, get_mem_usage, no_init_or_tensor\n-from transformers import AutoConfig, PretrainedConfig\n-\n-from vllm.distributed import (init_distributed_environment,\n-                              initialize_model_parallel)\n-from vllm.engine.arg_utils import EngineArgs\n-from vllm.engine.llm_engine import LLMEngine\n-from vllm.model_executor.model_loader.tensorizer import TensorizerArgs\n-from vllm.model_executor.models import ModelRegistry\n-\n-# yapf conflicts with isort for this docstring\n-# yapf: disable\n-\"\"\"\n-tensorize_vllm_model.py is a script that can be used to serialize and \n-deserialize vLLM models. These models can be loaded using tensorizer directly \n-to the GPU extremely quickly. Tensor encryption and decryption is also \n-supported, although libsodium must be installed to use it. Install\n-vllm with tensorizer support using `pip install vllm[tensorizer]`.\n-\n-To serialize a model, you can run something like this:\n-\n-python tensorize_vllm_model.py \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n-   serialize \\\n-   --serialized-directory s3://my-bucket/ \\\n-   --suffix vllm\n-\n-Which downloads the model from HuggingFace, loads it into vLLM, serializes it,\n-and saves it to your S3 bucket. A local directory can also be used.\n-\n-You can also encrypt the model weights with a randomly-generated key by \n-providing a `--keyfile` argument.\n-\n-To deserialize a model, you can run something like this:\n-\n-python tensorize_vllm_model.py \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n-   deserialize \\\n-   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/vllm/model.tensors\n-\n-Which downloads the model tensors from your S3 bucket and deserializes them.\n-To provide S3 credentials, you can provide `--s3-access-key-id` and \n-`--s3-secret-access-key`, as well as `--s3-endpoint` as CLI args to this script,\n-the OpenAI entrypoint, as arguments for LLM(), or as environment variables\n-in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and `S3_ENDPOINT`.\n-\n-\n-You can also provide a `--keyfile` argument to decrypt the model weights if \n-they were serialized with encryption.\n-\n-For more information on the available arguments, run \n-`python tensorize_vllm_model.py --help`.\n-\"\"\"\n-\n-\n-def parse_args():\n-    parser = argparse.ArgumentParser(\n-        description=\"An example script that can be used to serialize and \"\n-                    \"deserialize vLLM models. These models \"\n-                    \"can be loaded using tensorizer directly to the GPU \"\n-                    \"extremely quickly. Tensor encryption and decryption is \"\n-                    \"also supported, although libsodium must be installed to \"\n-                    \"use it.\")\n-    parser = TensorizerArgs.add_cli_args(EngineArgs.add_cli_args(parser))\n-    subparsers = parser.add_subparsers(dest='command')\n-\n-    serialize_parser = subparsers.add_parser(\n-        'serialize', help=\"Serialize a model to `--serialized-directory`\")\n-\n-    serialize_parser.add_argument(\n-        \"--suffix\",\n-        type=str,\n-        required=False,\n-        help=(\n-            \"The suffix to append to the serialized model directory, which is \"\n-            \"used to construct the location of the serialized model tensors, \"\n-            \"e.g. if `--serialized-directory` is `s3://my-bucket/` and \"\n-            \"`--suffix` is `v1`, the serialized model tensors will be \"\n-            \"saved to \"\n-            \"`s3://my-bucket/vllm/EleutherAI/gpt-j-6B/v1/model.tensors`. \"\n-            \"If none is provided, a random UUID will be used.\"))\n-    serialize_parser.add_argument(\n-        \"--serialized-directory\",\n-        type=str,\n-        required=True)\n-\n-    serialize_parser.add_argument(\n-        \"--keyfile\",\n-        type=str,\n-        required=False,\n-        help=(\"Encrypt the model weights with a randomly-generated binary key,\"\n-              \" and save the key at this path\"))\n-\n-    deserialize_parser = subparsers.add_parser(\n-        'deserialize',\n-        help=(\"Deserialize a model from `--path-to-tensors`\"\n-              \" to verify it can be loaded and used.\"))\n-\n-    deserialize_parser.add_argument(\n-        \"--path-to-tensors\",\n-        type=str,\n-        required=True,\n-        help=\"The local path or S3 URI to the model tensors to deserialize. \")\n-\n-    deserialize_parser.add_argument(\n-        \"--keyfile\",\n-        type=str,\n-        required=False,\n-        help=(\"Path to a binary key to use to decrypt the model weights,\"\n-              \" if the model was serialized with encryption\"))\n-\n-    return parser.parse_args()\n-\n-\n-def make_model_contiguous(model):\n-    # Ensure tensors are saved in memory contiguously\n-    for param in model.parameters():\n-        param.data = param.data.contiguous()\n-\n-\n-def _get_vllm_model_architecture(config: PretrainedConfig) -> Type[nn.Module]:\n-    architectures = getattr(config, \"architectures\", [])\n-    for arch in architectures:\n-        model_cls = ModelRegistry.load_model_cls(arch)\n-        if model_cls is not None:\n-            return model_cls\n-    raise ValueError(\n-        f\"Model architectures {architectures} are not supported for now. \"\n-        f\"Supported architectures: {ModelRegistry.get_supported_archs()}\")\n-\n-\n-def serialize():\n-    eng_args_dict = {f.name: getattr(args, f.name) for f in\n-                     dataclasses.fields(EngineArgs)}\n-    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n-    engine = LLMEngine.from_engine_args(engine_args)\n-\n-    model = (engine.model_executor.driver_worker.\n-             model_runner.model)\n-\n-    encryption_params = EncryptionParams.random() if keyfile else None\n-    if keyfile:\n-        with _write_stream(keyfile) as stream:\n-            stream.write(encryption_params.key)\n-\n-    with _write_stream(model_path) as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-        serializer.close()\n-\n-    print(\"Serialization complete. Model tensors saved to\", model_path)\n-    if keyfile:\n-        print(\"Key saved to\", keyfile)\n-\n-\n-def deserialize():\n-    config = AutoConfig.from_pretrained(model_ref)\n-\n-    with no_init_or_tensor():\n-        model_class = _get_vllm_model_architecture(config)\n-        model = model_class(config)\n-\n-    before_mem = get_mem_usage()\n-    start = time.time()\n-\n-    if keyfile:\n-        with _read_stream(keyfile) as stream:\n-            key = stream.read()\n-            decryption_params = DecryptionParams.from_key(key)\n-            tensorizer_args.deserializer_params['encryption'] = \\\n-                decryption_params\n-\n-    with (_read_stream(model_path)) as stream, TensorDeserializer(\n-            stream, **tensorizer_args.deserializer_params) as deserializer:\n-        deserializer.load_into_module(model)\n-        end = time.time()\n-\n-    # Brag about how fast we are.\n-    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n-    duration = end - start\n-    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n-    after_mem = get_mem_usage()\n-    print(\n-        f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\"\n-    )\n-    print(f\"Memory usage before: {before_mem}\")\n-    print(f\"Memory usage after: {after_mem}\")\n-\n-    return model\n-\n-\n-args = parse_args()\n-\n-s3_access_key_id = (args.s3_access_key_id or os.environ.get(\"S3_ACCESS_KEY_ID\")\n-                    or None)\n-s3_secret_access_key = (args.s3_secret_access_key\n-                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\") or None)\n-\n-s3_endpoint = (args.s3_endpoint or os.environ.get(\"S3_ENDPOINT_URL\") or None)\n-\n-_read_stream, _write_stream = (partial(\n-    stream_io.open_stream,\n-    mode=mode,\n-    s3_access_key_id=s3_access_key_id,\n-    s3_secret_access_key=s3_secret_access_key,\n-    s3_endpoint=s3_endpoint,\n-) for mode in (\"rb\", \"wb+\"))\n-\n-model_ref = args.model\n-\n-model_name = model_ref.split(\"/\")[1]\n-\n-os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n-os.environ[\"MASTER_PORT\"] = \"8080\"\n-\n-init_distributed_environment(world_size=1, rank=0, local_rank=0)\n-initialize_model_parallel()\n-\n-keyfile = args.keyfile if args.keyfile else None\n-\n-if args.command == \"serialize\":\n-    input_dir = args.serialized_directory.rstrip('/')\n-    suffix = args.suffix if args.suffix else uuid.uuid4().hex\n-    base_path = f\"{input_dir}/vllm/{model_ref}/{suffix}\"\n-    model_path = f\"{base_path}/model.tensors\"\n-    serialize()\n-elif args.command == \"deserialize\":\n-    tensorizer_args = TensorizerArgs.from_cli_args(args)\n-    model_path = args.path_to_tensors\n-    deserialize()\n-else:\n-    raise ValueError(\"Either serialize or deserialize must be specified.\")\ndiff --git a/tests/tensorizer_loader/test_tensorizer.py b/tests/tensorizer_loader/test_tensorizer.py\nindex ad4748c5e..1579d53a7 100644\n--- a/tests/tensorizer_loader/test_tensorizer.py\n+++ b/tests/tensorizer_loader/test_tensorizer.py\n@@ -10,12 +10,19 @@ import ray\n import torch\n \n from vllm import SamplingParams\n-from vllm.model_executor.model_loader.tensorizer import (\n-    EncryptionParams, TensorizerConfig, TensorSerializer,\n-    is_vllm_serialized_tensorizer, load_with_tensorizer, open_stream)\n+# yapf: disable\n+from vllm.model_executor.model_loader.tensorizer import (TensorizerConfig,\n+                                                         TensorSerializer,\n+                                                         is_vllm_tensorized,\n+                                                         load_with_tensorizer,\n+                                                         open_stream,\n+                                                         serialize_vllm_model)\n \n from ..utils import ServerRunner\n \n+# yapf conflicts with isort for this docstring\n+\n+\n prompts = [\n     \"Hello, my name is\",\n     \"The president of the United States is\",\n@@ -40,7 +47,7 @@ def is_curl_installed():\n \n @pytest.fixture(autouse=True)\n def tensorizer_config():\n-    config = TensorizerConfig(tensorizer_uri=\"vllm\", vllm_tensorized=True)\n+    config = TensorizerConfig(tensorizer_uri=\"vllm\")\n     return config\n \n \n@@ -59,47 +66,6 @@ def test_load_with_tensorizer(mock_agent, tensorizer_config):\n     assert result == mock_agent_instance.deserialize.return_value\n \n \n-def test_is_vllm_model_with_vllm_in_uri(tensorizer_config):\n-    tensorizer_config.vllm_tensorized = True\n-\n-    result = is_vllm_serialized_tensorizer(tensorizer_config)\n-\n-    assert result is True\n-\n-\n-def test_is_vllm_model_without_vllm_in_uri(tensorizer_config):\n-    tensorizer_config.vllm_tensorized = False\n-\n-    result = is_vllm_serialized_tensorizer(tensorizer_config)\n-\n-    assert result is False\n-\n-\n-def test_deserialized_vllm_model_has_same_outputs(vllm_runner, tmp_path):\n-    vllm_model = vllm_runner(model_ref)\n-    model_path = tmp_path / (model_ref + \".tensors\")\n-    outputs = vllm_model.generate(prompts, sampling_params)\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream)\n-        serializer.write_module(model)\n-    del vllm_model, model\n-    gc.collect()\n-    torch.cuda.empty_cache()\n-    loaded_vllm_model = vllm_runner(\n-        model_ref,\n-        load_format=\"tensorizer\",\n-        model_loader_extra_config=TensorizerConfig(tensorizer_uri=model_path,\n-                                                   num_readers=1,\n-                                                   vllm_tensorized=True),\n-    )\n-    deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n-\n-    # Assumes SamplingParams being seeded ensures the outputs are deterministic\n-    assert outputs == deserialized_outputs\n-\n-\n @pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n def test_can_deserialize_s3(vllm_runner):\n     model_ref = \"EleutherAI/pythia-1.4b\"\n@@ -110,7 +76,6 @@ def test_can_deserialize_s3(vllm_runner):\n                                   model_loader_extra_config=TensorizerConfig(\n                                       tensorizer_uri=tensorized_path,\n                                       num_readers=1,\n-                                      vllm_tensorized=False,\n                                       s3_endpoint=\"object.ord1.coreweave.com\",\n                                   ))\n \n@@ -126,29 +91,26 @@ def test_deserialized_encrypted_vllm_model_has_same_outputs(\n     model_path = tmp_path / (model_ref + \".tensors\")\n     key_path = tmp_path / (model_ref + \".key\")\n     outputs = vllm_model.generate(prompts, sampling_params)\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n \n-    encryption_params = EncryptionParams.random()\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-    with open_stream(key_path, \"wb+\") as stream:\n-        stream.write(encryption_params.key)\n-    del vllm_model, model\n+    config_for_serializing = TensorizerConfig(tensorizer_uri=model_path)\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         config_for_serializing,\n+                         encryption_key_path=key_path)\n+\n+    del vllm_model\n     gc.collect()\n     torch.cuda.empty_cache()\n-    loaded_vllm_model = vllm_runner(model_ref,\n-                                    load_format=\"tensorizer\",\n-                                    model_loader_extra_config=TensorizerConfig(\n-                                        tensorizer_uri=model_path,\n-                                        encryption_keyfile=key_path,\n-                                        num_readers=1,\n-                                        vllm_tensorized=True))\n+\n+    config_for_deserializing = TensorizerConfig(tensorizer_uri=model_path,\n+                                                encryption_keyfile=key_path)\n+\n+    loaded_vllm_model = vllm_runner(\n+        model_ref,\n+        load_format=\"tensorizer\",\n+        model_loader_extra_config=config_for_deserializing)\n \n     deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n \n-    # Assumes SamplingParams being seeded ensures the outputs are deterministic\n     assert outputs == deserialized_outputs\n \n \n@@ -169,7 +131,7 @@ def test_deserialized_hf_model_has_same_outputs(hf_runner, vllm_runner,\n                                   model_loader_extra_config=TensorizerConfig(\n                                       tensorizer_uri=model_path,\n                                       num_readers=1,\n-                                      vllm_tensorized=False))\n+                                  ))\n \n     deserialized_outputs = loaded_hf_model.generate_greedy(\n         prompts, max_tokens=max_tokens)\n@@ -190,12 +152,11 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n     # Serialize model before deserializing and binding LoRA adapters\n     vllm_model = vllm_runner(model_ref, )\n     model_path = tmp_path / (model_ref + \".tensors\")\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream)\n-        serializer.write_module(model)\n-    del vllm_model, model\n+\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         TensorizerConfig(tensorizer_uri=model_path))\n+\n+    del vllm_model\n     gc.collect()\n     torch.cuda.empty_cache()\n     loaded_vllm_model = vllm_runner(\n@@ -204,7 +165,6 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n         model_loader_extra_config=TensorizerConfig(\n             tensorizer_uri=model_path,\n             num_readers=1,\n-            vllm_tensorized=True,\n         ),\n         enable_lora=True,\n         max_loras=1,\n@@ -220,58 +180,28 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n \n def test_load_without_tensorizer_load_format(vllm_runner):\n     with pytest.raises(ValueError):\n-        vllm_runner(model_ref,\n-                    model_loader_extra_config=TensorizerConfig(\n-                        tensorizer_uri=\"test\", vllm_tensorized=False))\n-\n-\n-@pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n-def test_tensorize_vllm_model(tmp_path):\n-    # Test serialize command\n-    serialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"serialize\", \"--serialized-directory\", tmp_path,\n-        \"--suffix\", \"tests\"\n-    ]\n-    result = subprocess.run(serialize_args, capture_output=True, text=True)\n-    print(result.stdout)  # Print the output of the serialize command\n-\n-    assert result.returncode == 0, (f\"Serialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n-\n-    path_to_tensors = f\"{tmp_path}/vllm/{model_ref}/tests/model.tensors\"\n-\n-    # Test deserialize command\n-    deserialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"deserialize\", \"--path-to-tensors\",\n-        path_to_tensors\n-    ]\n-    result = subprocess.run(deserialize_args, capture_output=True, text=True)\n-    assert result.returncode == 0, (f\"Deserialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n+        vllm_runner(\n+            model_ref,\n+            model_loader_extra_config=TensorizerConfig(tensorizer_uri=\"test\"))\n \n \n @pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n-def test_openai_apiserver_with_tensorizer(tmp_path):\n+def test_openai_apiserver_with_tensorizer(vllm_runner, tmp_path):\n     ## Serialize model\n-    serialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"serialize\", \"--serialized-directory\", tmp_path,\n-        \"--suffix\", \"tests\"\n-    ]\n-    result = subprocess.run(serialize_args, capture_output=True, text=True)\n-    print(result.stdout)  # Print the output of the serialize command\n+    vllm_model = vllm_runner(model_ref, )\n+    model_path = tmp_path / (model_ref + \".tensors\")\n \n-    assert result.returncode == 0, (f\"Serialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         TensorizerConfig(tensorizer_uri=model_path))\n \n-    path_to_tensors = f\"{tmp_path}/vllm/{model_ref}/tests/model.tensors\"\n     model_loader_extra_config = {\n-        \"tensorizer_uri\": path_to_tensors,\n-        \"vllm_tensorized\": True\n+        \"tensorizer_uri\": str(model_path),\n     }\n \n+    del vllm_model\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+\n     ## Start OpenAI API server\n     openai_args = [\n         \"--model\", model_ref, \"--dtype\", \"float16\", \"--load-format\",\n@@ -304,10 +234,10 @@ def test_openai_apiserver_with_tensorizer(tmp_path):\n \n def test_raise_value_error_on_invalid_load_format(vllm_runner):\n     with pytest.raises(ValueError):\n-        vllm_runner(model_ref,\n-                    load_format=\"safetensors\",\n-                    model_loader_extra_config=TensorizerConfig(\n-                        tensorizer_uri=\"test\", vllm_tensorized=False))\n+        vllm_runner(\n+            model_ref,\n+            load_format=\"safetensors\",\n+            model_loader_extra_config=TensorizerConfig(tensorizer_uri=\"test\"))\n \n \n def test_tensorizer_with_tp(vllm_runner):\n@@ -321,8 +251,29 @@ def test_tensorizer_with_tp(vllm_runner):\n             model_loader_extra_config=TensorizerConfig(\n                 tensorizer_uri=tensorized_path,\n                 num_readers=1,\n-                vllm_tensorized=False,\n                 s3_endpoint=\"object.ord1.coreweave.com\",\n             ),\n             tensor_parallel_size=2,\n         )\n+\n+\n+def test_vllm_tensorized_model_has_same_outputs(vllm_runner, tmp_path):\n+    model_ref = \"facebook/opt-125m\"\n+    model_path = tmp_path / (model_ref + \".tensors\")\n+    config = TensorizerConfig(tensorizer_uri=str(model_path))\n+\n+    vllm_model = vllm_runner(model_ref)\n+    outputs = vllm_model.generate(prompts, sampling_params)\n+    serialize_vllm_model(vllm_model.model.llm_engine, config)\n+\n+    assert is_vllm_tensorized(config)\n+    del vllm_model\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+\n+    loaded_vllm_model = vllm_runner(model_ref,\n+                                    load_format=\"tensorizer\",\n+                                    model_loader_extra_config=config)\n+    deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n+\n+    assert outputs == deserialized_outputs\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 163723b4b..fd5338c46 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -167,8 +167,8 @@ class EngineArgs:\n             '* \"dummy\" will initialize the weights with random values, '\n             'which is mainly for profiling.\\n'\n             '* \"tensorizer\" will load the weights using tensorizer from '\n-            'CoreWeave which assumes tensorizer_uri is set to the location of '\n-            'the serialized weights.')\n+            'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n+            'section for more information.\\n')\n         parser.add_argument(\n             '--dtype',\n             type=str,\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 91cc8f3be..68d8a074d 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -145,7 +145,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n \n     # S3 access information, used for tensorizer to load model from S3\n     \"S3_ACCESS_KEY_ID\":\n-    lambda: os.environ.get(\"S3_ACCESS_KEY\", None),\n+    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n     \"S3_SECRET_ACCESS_KEY\":\n     lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n     \"S3_ENDPOINT_URL\":\ndiff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py\nindex fc9c8aa0a..b14824a35 100644\n--- a/vllm/model_executor/model_loader/loader.py\n+++ b/vllm/model_executor/model_loader/loader.py\n@@ -17,7 +17,7 @@ from vllm.logger import init_logger\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig)\n from vllm.model_executor.model_loader.tensorizer import (\n-    TensorizerConfig, is_vllm_serialized_tensorizer, load_with_tensorizer,\n+    TensorizerConfig, is_vllm_tensorized, load_with_tensorizer,\n     tensorizer_weights_iterator)\n from vllm.model_executor.model_loader.utils import (get_model_architecture,\n                                                     set_default_torch_dtype)\n@@ -291,7 +291,7 @@ class TensorizerLoader(BaseModelLoader):\n         tensorizer_args = self.tensorizer_config._construct_tensorizer_args()\n         return tensorizer_weights_iterator(tensorizer_args)\n \n-    def _load_model_unserialized(\n+    def _load_model_serialized_cpu(\n         self,\n         model_config: ModelConfig,\n         device_config: DeviceConfig,\n@@ -299,11 +299,12 @@ class TensorizerLoader(BaseModelLoader):\n         vision_language_config: Optional[VisionLanguageConfig],\n         cache_config: CacheConfig,\n     ) -> nn.Module:\n-        \"\"\"Load an unserialized model with tensorizer.\n+        \"\"\"Load a serialized model with tensorizer to the CPU.\n \n-        Unserialized here means \"not serialized with tensorizer\". This\n-        should still be faster than default HuggingFace loading, but will\n-        be slower than loading a tensorizer-serialized model.\n+        This is only necessary when the model isn't vLLM-tensorized (see\n+        examples/tensorize_vllm_model.py) This should still be faster than\n+        default HuggingFace loading, but will be slower than loading a\n+        vLLM-tensorized model.\n         \"\"\"\n         with set_default_torch_dtype(model_config.dtype):\n             with torch.device(device_config.device):\n@@ -324,8 +325,9 @@ class TensorizerLoader(BaseModelLoader):\n     ) -> nn.Module:\n         \"\"\"Load a serialized model with tensorizer.\n \n-        See the examples/tensorize_vllm_model.py example \"\n-        script for serializing vLLM models.\"\"\"\n+        Expects a vLLM-tensorized model. See the\n+        examples/tensorize_vllm_model.py example script\n+        for serializing vLLM models.\"\"\"\n         with set_default_torch_dtype(model_config.dtype):\n             with torch.device(device_config.device):\n                 model_class = get_model_architecture(model_config)[0]\n@@ -353,15 +355,15 @@ class TensorizerLoader(BaseModelLoader):\n                    cache_config: CacheConfig) -> nn.Module:\n         self._verify_config(model_config, parallel_config)\n \n-        if is_vllm_serialized_tensorizer(self.tensorizer_config):\n+        if is_vllm_tensorized(self.tensorizer_config):\n             return self._load_model_serialized(model_config, device_config,\n                                                lora_config,\n                                                vision_language_config,\n                                                cache_config)\n-        return self._load_model_unserialized(model_config, device_config,\n-                                             lora_config,\n-                                             vision_language_config,\n-                                             cache_config)\n+        return self._load_model_serialized_cpu(model_config, device_config,\n+                                               lora_config,\n+                                               vision_language_config,\n+                                               cache_config)\n \n \n def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:\ndiff --git a/vllm/model_executor/model_loader/tensorizer.py b/vllm/model_executor/model_loader/tensorizer.py\nindex 219a2a392..2cf4ce5f8 100644\n--- a/vllm/model_executor/model_loader/tensorizer.py\n+++ b/vllm/model_executor/model_loader/tensorizer.py\n@@ -5,6 +5,7 @@ import os\n import time\n import typing\n from dataclasses import dataclass\n+from functools import partial\n from typing import Generator, Optional, Tuple, Type, Union\n \n import torch\n@@ -13,6 +14,7 @@ from transformers import PretrainedConfig\n \n import vllm.envs as envs\n from vllm.config import ModelConfig, ParallelConfig\n+from vllm.engine.llm_engine import LLMEngine\n from vllm.logger import init_logger\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig)\n@@ -27,6 +29,11 @@ try:\n     from tensorizer.stream_io import open_stream\n     from tensorizer.utils import (convert_bytes, get_mem_usage,\n                                   no_init_or_tensor)\n+\n+    _read_stream, _write_stream = (partial(\n+        open_stream,\n+        mode=mode,\n+    ) for mode in (\"rb\", \"wb+\"))\n except ImportError as e:\n     tensorizer_error_msg = str(e)\n \n@@ -43,7 +50,7 @@ logger = init_logger(__name__)\n class TensorizerConfig:\n     tensorizer_uri: Union[io.BufferedIOBase, io.RawIOBase, typing.BinaryIO,\n                           str, bytes, os.PathLike, int]\n-    vllm_tensorized: bool\n+    vllm_tensorized: Optional[bool] = False\n     verify_hash: Optional[bool] = False\n     num_readers: Optional[int] = None\n     encryption_keyfile: Optional[str] = None\n@@ -93,17 +100,11 @@ def load_with_tensorizer(tensorizer_config: TensorizerConfig,\n     return tensorizer.deserialize()\n \n \n-def is_vllm_serialized_tensorizer(tensorizer_config: TensorizerConfig) -> bool:\n-    if tensorizer_config is None:\n-        return False\n-    return tensorizer_config.vllm_tensorized\n-\n-\n @dataclass\n class TensorizerArgs:\n     tensorizer_uri: Union[io.BufferedIOBase, io.RawIOBase, typing.BinaryIO,\n                           str, bytes, os.PathLike, int]\n-    vllm_tensorized: bool\n+    vllm_tensorized: Optional[bool] = False\n     verify_hash: Optional[bool] = False\n     num_readers: Optional[int] = None\n     encryption_keyfile: Optional[str] = None\n@@ -121,7 +122,9 @@ class TensorizerArgs:\n           vLLM model. This is used to determine the behavior of the \n           TensorDeserializer when loading tensors from a serialized model.\n           It is far faster to deserialize a vLLM model as it utilizes\n-          tensorizer's optimized GPU loading.\n+          tensorizer's optimized GPU loading. Note that this is now\n+          deprecated, as serialized vLLM models are now automatically\n+          inferred as vLLM models.\n       verify_hash: If True, the hashes of each tensor will be verified against \n           the hashes stored in the metadata. A `HashMismatchError` will be \n           raised if any of the hashes do not match.\n@@ -158,6 +161,7 @@ class TensorizerArgs:\n             \"encryption\": self.encryption_keyfile,\n             \"num_readers\": self.num_readers\n         }\n+\n         if self.encryption_keyfile:\n             with open_stream(\n                     self.encryption_keyfile,\n@@ -177,7 +181,14 @@ class TensorizerArgs:\n             'tensorizer options',\n             description=('Options for configuring the behavior of the'\n                          ' tensorizer deserializer when '\n-                         '--load-format=tensorizer'))\n+                         'load_format=tensorizer is specified when '\n+                         'initializing an LLMEngine, either via the CLI '\n+                         'when running the vLLM OpenAI inference server '\n+                         'with a JSON string passed to '\n+                         '--model-loader-extra-config or as arguments given '\n+                         'to TensorizerConfig when passed to '\n+                         'model_loader_extra_config in the constructor '\n+                         'for LLMEngine.'))\n \n         group.add_argument(\n             \"--tensorizer-uri\",\n@@ -222,13 +233,6 @@ class TensorizerArgs:\n             help=\"The endpoint for the S3 bucket. Can also be set via the \"\n             \"S3_ENDPOINT_URL environment variable.\",\n         )\n-        group.add_argument(\n-            \"--vllm-tensorized\",\n-            action=\"store_true\",\n-            help=\"If enabled, indicates that the serialized model is a vLLM \"\n-            \"model. This is used to determine the behavior of the \"\n-            \"TensorDeserializer when loading tensors from a \"\n-            \"serialized model.\")\n \n         return parser\n \n@@ -322,10 +326,9 @@ class TensorizerAgent:\n         \"\"\"\n         before_mem = get_mem_usage()\n         start = time.perf_counter()\n-        with open_stream(\n-                self.tensorizer_args.tensorizer_uri,\n-                mode=\"rb\",\n-                **self.tensorizer_args.stream_params,\n+        with _read_stream(\n+                self.tensorizer_config.tensorizer_uri,\n+                **self.tensorizer_args.stream_params\n         ) as stream, TensorDeserializer(\n                 stream,\n                 dtype=self.tensorizer_config.dtype,\n@@ -345,6 +348,7 @@ class TensorizerAgent:\n \n         self._check_tensors_on_meta_device()\n         self._resize_lora_embeddings()\n+        del self.model.vllm_tensorized_marker\n         return self.model.eval()\n \n \n@@ -366,3 +370,63 @@ def tensorizer_weights_iterator(\n         for name, param in state.items():\n             yield name, param\n     del state\n+\n+\n+def is_vllm_tensorized(tensorizer_config: \"TensorizerConfig\") -> bool:\n+    \"\"\"\n+    Infer if the model is a vLLM model by checking the weights for\n+    a vLLM tensorized marker.\n+\n+    Args:\n+        tensorizer_config: The TensorizerConfig object containing the\n+            tensorizer_uri to the serialized model.\n+\n+    Returns:\n+        bool: True if the model is a vLLM model, False otherwise.\n+    \"\"\"\n+    tensorizer_args = tensorizer_config._construct_tensorizer_args()\n+    deserializer = TensorDeserializer(open_stream(\n+        tensorizer_args.tensorizer_uri, **tensorizer_args.stream_params),\n+                                      **tensorizer_args.deserializer_params,\n+                                      lazy_load=True)\n+    if tensorizer_config.vllm_tensorized:\n+        logger.warning(\n+            \"Please note that newly serialized vLLM models are automatically \"\n+            \"inferred as vLLM models, so setting vllm_tensorized=True is \"\n+            \"only necessary for models serialized prior to this change.\")\n+        return True\n+    if (\".vllm_tensorized_marker\" in deserializer):\n+        return True\n+    return False\n+\n+\n+def get_pretensorized_vllm_model(engine: \"LLMEngine\") -> nn.Module:\n+    model = (engine.model_executor.driver_worker.model_runner.model)\n+    model.register_parameter(\n+        \"vllm_tensorized_marker\",\n+        nn.Parameter(torch.tensor((1, ), device=\"meta\"), requires_grad=False))\n+    return model\n+\n+\n+def serialize_vllm_model(engine: \"LLMEngine\",\n+                         tensorizer_config : TensorizerConfig,\n+                         encryption_key_path: Optional[str] = None) \\\n+        -> nn.Module:\n+\n+    model = get_pretensorized_vllm_model(engine)\n+    tensorizer_args = tensorizer_config._construct_tensorizer_args()\n+    encryption_params = None\n+    if encryption_key_path is not None:\n+        encryption_params = EncryptionParams.random()\n+        with _write_stream(encryption_key_path,\n+                           **tensorizer_args.stream_params) as stream:\n+            stream.write(encryption_params.key)\n+\n+    with _write_stream(tensorizer_args.tensorizer_uri,\n+                       **tensorizer_args.stream_params) as stream:\n+        serializer = TensorSerializer(stream, encryption=encryption_params)\n+        serializer.write_module(model)\n+        serializer.close()\n+    logger.info(\"Successfully serialized model to %s\",\n+                str(tensorizer_args.tensorizer_uri))\n+    return model", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport json\nfrom argparse import ArgumentError\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass, field\nfrom typing import Annotated, Literal, Optional, Union\n\nimport pytest\n\nfrom vllm.config import CompilationConfig, config\nfrom vllm.engine.arg_utils import (EngineArgs, contains_type, get_kwargs,\n                                   get_type, get_type_hints, is_not_builtin,\n                                   is_type, literal_to_kwargs, optional_type,\n                                   parse_type)\nfrom vllm.utils import FlexibleArgumentParser\n\n\n@pytest.mark.parametrize((\"type\", \"value\", \"expected\"), [\n    (int, \"42\", 42),\n    (float, \"3.14\", 3.14),\n    (str, \"Hello World!\", \"Hello World!\"),\n    (json.loads, '{\"foo\":1,\"bar\":2}', {\n        \"foo\": 1,\n        \"bar\": 2\n    }),\n])\ndef test_parse_type(type, value, expected):\n    parse_type_func = parse_type(type)\n    assert parse_type_func(value) == expected\n\n\ndef test_optional_type():\n    optional_type_func = optional_type(int)\n    assert optional_type_func(\"None\") is None\n    assert optional_type_func(\"42\") == 42\n\n\n@pytest.mark.parametrize((\"type_hint\", \"type\", \"expected\"), [\n    (int, int, True),\n    (int, float, False),\n    (list[int], list, True),\n    (list[int], tuple, False),\n    (Literal[0, 1], Literal, True),\n])\ndef test_is_type(type_hint, type, expected):\n    assert is_type(type_hint, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"type\", \"expected\"), [\n    ({float, int}, int, True),\n    ({int, tuple[int]}, int, True),\n    ({int, tuple[int]}, float, False),\n    ({str, Literal[\"x\", \"y\"]}, Literal, True),\n])\ndef test_contains_type(type_hints, type, expected):\n    assert contains_type(type_hints, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"type\", \"expected\"), [\n    ({int, float}, int, int),\n    ({int, float}, str, None),\n    ({str, Literal[\"x\", \"y\"]}, Literal, Literal[\"x\", \"y\"]),\n])\ndef test_get_type(type_hints, type, expected):\n    assert get_type(type_hints, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"expected\"), [\n    ({Literal[1, 2]}, {\n        \"type\": int,\n        \"choices\": [1, 2]\n    }),\n    ({str, Literal[\"x\", \"y\"]}, {\n        \"type\": str,\n        \"metavar\": [\"x\", \"y\"]\n    }),\n    ({Literal[1, \"a\"]}, Exception),\n])\ndef test_literal_to_kwargs(type_hints, expected):\n    context = nullcontext()\n    if expected is Exception:\n        context = pytest.raises(expected)\n    with context:\n        assert literal_to_kwargs(type_hints) == expected\n\n\n@config\n@dataclass\nclass NestedConfig:\n    field: int = 1\n    \"\"\"field\"\"\"\n\n\n@config\n@dataclass\nclass DummyConfig:\n    regular_bool: bool = True\n    \"\"\"Regular bool with default True\"\"\"\n    optional_bool: Optional[bool] = None\n    \"\"\"Optional bool with default None\"\"\"\n    optional_literal: Optional[Literal[\"x\", \"y\"]] = None\n    \"\"\"Optional literal with default None\"\"\"\n    tuple_n: tuple[int, ...] = field(default_factory=lambda: (1, 2, 3))\n    \"\"\"Tuple with variable length\"\"\"\n    tuple_2: tuple[int, int] = field(default_factory=lambda: (1, 2))\n    \"\"\"Tuple with fixed length\"\"\"\n    list_n: list[int] = field(default_factory=lambda: [1, 2, 3])\n    \"\"\"List with variable length\"\"\"\n    list_literal: list[Literal[1, 2]] = field(default_factory=list)\n    \"\"\"List with literal choices\"\"\"\n    list_union: list[Union[str, type[object]]] = field(default_factory=list)\n    \"\"\"List with union type\"\"\"\n    literal_literal: Literal[Literal[1], Literal[2]] = 1\n    \"\"\"Literal of literals with default 1\"\"\"\n    json_tip: dict = field(default_factory=dict)\n    \"\"\"Dict which will be JSON in CLI\"\"\"\n    nested_config: NestedConfig = field(default_factory=NestedConfig)\n    \"\"\"Nested config\"\"\"\n\n\n@pytest.mark.parametrize((\"type_hint\", \"expected\"), [\n    (int, False),\n    (DummyConfig, True),\n])\ndef test_is_not_builtin(type_hint, expected):\n    assert is_not_builtin(type_hint) == expected\n\n\n@pytest.mark.parametrize(\n    (\"type_hint\", \"expected\"), [\n        (Annotated[int, \"annotation\"], {int}),\n        (Optional[int], {int, type(None)}),\n        (Annotated[Optional[int], \"annotation\"], {int, type(None)}),\n        (Optional[Annotated[int, \"annotation\"]], {int, type(None)}),\n    ],\n    ids=[\"Annotated\", \"Optional\", \"Annotated_Optional\", \"Optional_Annotated\"])\ndef test_get_type_hints(type_hint, expected):\n    assert get_type_hints(type_hint) == expected\n\n\ndef test_get_kwargs():\n    kwargs = get_kwargs(DummyConfig)\n    print(kwargs)\n\n    # bools should not have their type set\n    assert kwargs[\"regular_bool\"].get(\"type\") is None\n    assert kwargs[\"optional_bool\"].get(\"type\") is None\n    # optional literals should have None as a choice\n    assert kwargs[\"optional_literal\"][\"choices\"] == [\"x\", \"y\", \"None\"]\n    # tuples should have the correct nargs\n    assert kwargs[\"tuple_n\"][\"nargs\"] == \"+\"\n    assert kwargs[\"tuple_2\"][\"nargs\"] == 2\n    # lists should work\n    assert kwargs[\"list_n\"][\"type\"] is int\n    assert kwargs[\"list_n\"][\"nargs\"] == \"+\"\n    # lists with literals should have the correct choices\n    assert kwargs[\"list_literal\"][\"type\"] is int\n    assert kwargs[\"list_literal\"][\"nargs\"] == \"+\"\n    assert kwargs[\"list_literal\"][\"choices\"] == [1, 2]\n    # lists with unions should become str type.\n    # If not, we cannot know which type to use for parsing\n    assert kwargs[\"list_union\"][\"type\"] is str\n    # literals of literals should have merged choices\n    assert kwargs[\"literal_literal\"][\"choices\"] == [1, 2]\n    # dict should have json tip in help\n    json_tip = \"Should either be a valid JSON string or JSON keys\"\n    assert json_tip in kwargs[\"json_tip\"][\"help\"]\n    # nested config should should construct the nested config\n    assert kwargs[\"nested_config\"][\"type\"]('{\"field\": 2}') == NestedConfig(2)\n\n\n@pytest.mark.parametrize(\n    (\"arg\", \"expected\"),\n    [\n        (None, dict()),\n        ('{\"video\": {\"num_frames\": 123} }', {\n            \"video\": {\n                \"num_frames\": 123\n            }\n        }),\n        (\n            '{\"video\": {\"num_frames\": 123, \"fps\": 1.0, \"foo\": \"bar\"}, \"image\": {\"foo\": \"bar\"} }',  # noqa\n            {\n                \"video\": {\n                    \"num_frames\": 123,\n                    \"fps\": 1.0,\n                    \"foo\": \"bar\"\n                },\n                \"image\": {\n                    \"foo\": \"bar\"\n                }\n            }),\n    ])\ndef test_media_io_kwargs_parser(arg, expected):\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    if arg is None:\n        args = parser.parse_args([])\n    else:\n        args = parser.parse_args([\"--media-io-kwargs\", arg])\n\n    assert args.media_io_kwargs == expected\n\n\ndef test_compilation_config():\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n\n    # default value\n    args = parser.parse_args([])\n    assert args.compilation_config == CompilationConfig()\n\n    # set to O3\n    args = parser.parse_args([\"-O0\"])\n    assert args.compilation_config.level == 0\n\n    # set to O 3 (space)\n    args = parser.parse_args([\"-O\", \"1\"])\n    assert args.compilation_config.level == 1\n\n    # set to O 3 (equals)\n    args = parser.parse_args([\"-O=2\"])\n    assert args.compilation_config.level == 2\n\n    # set to O.level 3\n    args = parser.parse_args([\"-O.level\", \"3\"])\n    assert args.compilation_config.level == 3\n\n    # set to string form of a dict\n    args = parser.parse_args([\n        \"-O\",\n        '{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8], '\n        '\"use_inductor\": false}',\n    ])\n    assert (args.compilation_config.level == 3 and\n            args.compilation_config.cudagraph_capture_sizes == [1, 2, 4, 8]\n            and not args.compilation_config.use_inductor)\n\n    # set to string form of a dict\n    args = parser.parse_args([\n        \"--compilation-config=\"\n        '{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8], '\n        '\"use_inductor\": true}',\n    ])\n    assert (args.compilation_config.level == 3 and\n            args.compilation_config.cudagraph_capture_sizes == [1, 2, 4, 8]\n            and args.compilation_config.use_inductor)\n\n\ndef test_prefix_cache_default():\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    args = parser.parse_args([])\n\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert (not engine_args.enable_prefix_caching\n            ), \"prefix caching defaults to off.\"\n\n    # with flag to turn it on.\n    args = parser.parse_args([\"--enable-prefix-caching\"])\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert engine_args.enable_prefix_caching\n\n    # with disable flag to turn it off.\n    args = parser.parse_args([\"--no-enable-prefix-caching\"])\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert not engine_args.enable_prefix_caching\n\n\n# yapf: disable\n@pytest.mark.parametrize((\"arg\", \"expected\", \"option\"), [\n    (None, None, \"mm-processor-kwargs\"),\n    (\"{}\", {}, \"mm-processor-kwargs\"),\n    (\n        '{\"num_crops\": 4}',\n        {\n            \"num_crops\": 4\n        },\n        \"mm-processor-kwargs\"\n    ),\n    (\n        '{\"foo\": {\"bar\": \"baz\"}}',\n        {\n            \"foo\":\n            {\n                \"bar\": \"baz\"\n            }\n        },\n        \"mm-processor-kwargs\"\n    ),\n    (\n        '{\"cast_logits_dtype\":\"bfloat16\",\"sequence_parallel_norm\":true,\"sequence_parallel_norm_threshold\":2048}',\n        {\n            \"cast_logits_dtype\": \"bfloat16\",\n            \"sequence_parallel_norm\": True,\n            \"sequence_parallel_norm_threshold\": 2048,\n        },\n        \"override-neuron-config\"\n    ),\n])\n# yapf: enable\ndef test_composite_arg_parser(arg, expected, option):\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    if arg is None:\n        args = parser.parse_args([])\n    else:\n        args = parser.parse_args([f\"--{option}\", arg])\n    assert getattr(args, option.replace(\"-\", \"_\")) == expected\n\n\ndef test_human_readable_model_len():\n    # `exit_on_error` disabled to test invalid values below\n    parser = EngineArgs.add_cli_args(\n        FlexibleArgumentParser(exit_on_error=False))\n\n    args = parser.parse_args([])\n    assert args.max_model_len is None\n\n    args = parser.parse_args([\"--max-model-len\", \"1024\"])\n    assert args.max_model_len == 1024\n\n    # Lower\n    args = parser.parse_args([\"--max-model-len\", \"1m\"])\n    assert args.max_model_len == 1_000_000\n    args = parser.parse_args([\"--max-model-len\", \"10k\"])\n    assert args.max_model_len == 10_000\n\n    # Capital\n    args = parser.parse_args([\"--max-model-len\", \"3K\"])\n    assert args.max_model_len == 1024 * 3\n    args = parser.parse_args([\"--max-model-len\", \"10M\"])\n    assert args.max_model_len == 2**20 * 10\n\n    # Decimal values\n    args = parser.parse_args([\"--max-model-len\", \"10.2k\"])\n    assert args.max_model_len == 10200\n    # ..truncated to the nearest int\n    args = parser.parse_args([\"--max-model-len\", \"10.212345k\"])\n    assert args.max_model_len == 10212\n\n    # Invalid (do not allow decimals with binary multipliers)\n    for invalid in [\"1a\", \"pwd\", \"10.24\", \"1.23M\"]:\n        with pytest.raises(ArgumentError):\n            args = parser.parse_args([\"--max-model-len\", invalid])\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/0fca3cdc_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/8bc68e19_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-8d75fe4", "created_at": "2024-06-07T08:42:35+00:00", "base_commit": "388596c91437a51d428a447594e9faec340c29b2", "head_commit": "8d75fe48ca5f46b7af0f5201d8500b9604eed769", "patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a75..cae682216 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -179,7 +179,7 @@ def gptq_marlin_24_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n \n # cutlass\n def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n-                         a_scales: torch.Tensor, b_scales: torch.Tensor,\n+                         scale_a: torch.Tensor, scale_b: torch.Tensor,\n                          out_dtype: Type[torch.dtype]) -> torch.Tensor:\n     assert (b.shape[0] % 16 == 0 and b.shape[1] % 16 == 0)\n     assert (out_dtype is torch.bfloat16 or out_dtype is torch.float16)\n@@ -188,7 +188,7 @@ def cutlass_scaled_mm_dq(a: torch.Tensor, b: torch.Tensor,\n     n = b.shape[1]\n     out = torch.empty((m, n), dtype=out_dtype, device=a.device)\n \n-    vllm_ops.cutlass_scaled_mm_dq(out, a, b, a_scales, b_scales)\n+    vllm_ops.cutlass_scaled_mm_dq(out, a, b, scale_a, scale_b)\n \n     return out\n \ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e3d..136a64623 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -17,6 +17,24 @@ ACTIVATION_SCHEMES = [\"static\", \"dynamic\"]\n logger = init_logger(__name__)\n \n \n+def cutlass_fp8_supported() -> bool:\n+    capability = torch.cuda.get_device_capability()\n+    capability = capability[0] * 10 + capability[1]\n+    version = torch.version.cuda\n+    version = version[0] * 10 + version[1]\n+\n+    # CUTLASS FP8 kernels need at least\n+    #   CUDA 12.0 on SM90 systems (Hopper)\n+    #   CUDA 12.4 on SM89 systems (Lovelace)\n+    gpu_is_supported = False\n+    if capability >= 900:\n+        gpu_is_supported = version > 120\n+    elif capability >= 890:\n+        gpu_is_supported = version > 124\n+\n+    return gpu_is_supported\n+\n+\n class Fp8Config(QuantizationConfig):\n     \"\"\"Config class for FP8.\"\"\"\n \n@@ -92,6 +110,7 @@ class Fp8LinearMethod(LinearMethodBase):\n \n     def __init__(self, quant_config: Fp8Config):\n         self.quant_config = quant_config\n+        self.cutlass_fp8_supported = cutlass_fp8_supported()\n \n     def _create_scale_param(\n         self,\n@@ -233,25 +252,40 @@ class Fp8LinearMethod(LinearMethodBase):\n               layer: torch.nn.Module,\n               x: torch.Tensor,\n               bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n+\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x,\n-                                               layer.act_scale,\n-                                               batch_dim_padding=17)\n-\n-        # Fused GEMM_DQ -- note we padded the input above because\n-        # torch._scaled_mm is more performant for matrices with\n-        # batch dimension > 16. Note that this could change\n-        # in the future.\n-        output, _ = torch._scaled_mm(\n-            qinput,\n-            layer.weight,\n-            out_dtype=x.dtype,\n-            scale_a=x_scale,\n-            scale_b=layer.weight_scale,\n-            bias=bias,\n-        )\n+\n+        if bias is None and self.cutlass_fp8_supported:\n+            qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n+\n+            # Fused GEMM_DQ\n+            output = ops.cutlass_scaled_mm_dq(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+            )\n+\n+        else:\n+            qinput, x_scale = ops.scaled_fp8_quant(x,\n+                                                   layer.act_scale,\n+                                                   batch_dim_padding=17)\n+\n+            # Fused GEMM_DQ -- note we padded the input above because\n+            # torch._scaled_mm is more performant for matrices with\n+            # batch dimension > 16. Note that this could change\n+            # in the future.\n+            output, _ = torch._scaled_mm(\n+                qinput,\n+                layer.weight,\n+                out_dtype=x.dtype,\n+                scale_a=x_scale,\n+                scale_b=layer.weight_scale,\n+                bias=bias,\n+            )\n \n         return torch.narrow(output, 0, 0, x.shape[0])", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\nimport torch\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nfrom vllm.config import CompilationConfig, VllmConfig, set_current_vllm_config\nfrom vllm.model_executor.custom_op import CustomOp\nfrom vllm.model_executor.layers.activation import (GeluAndMul,\n                                                   ReLUSquaredActivation,\n                                                   SiluAndMul)\nfrom vllm.model_executor.layers.fused_moe.fused_moe import (dispatch_topk_func,\n                                                            vllm_topk_softmax)\nfrom vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (\n    is_rocm_aiter_moe_enabled)\nfrom vllm.model_executor.layers.layernorm import (\n    RMSNorm, dispatch_cuda_rmsnorm_func, fused_add_rms_norm, rms_norm,\n    rocm_aiter_fused_add_rms_norm, rocm_aiter_rms_norm)\nfrom vllm.model_executor.layers.quantization.utils.fp8_utils import (\n    cutlass_scaled_mm, dispatch_w8a8_blockscale_func, w8a8_block_fp8_matmul)\nfrom vllm._custom_ops import current_platform\n\n\n# Registered subclass for test\n@CustomOp.register(\"relu3\")\nclass Relu3(ReLUSquaredActivation):\n    pass\n\n\n@pytest.mark.parametrize(\n    \"env, torch_level, use_inductor, ops_enabled, default_on\",\n    [\n        # Default values based on compile level\n        # - All by default (no Inductor compilation)\n        (\"\", 0, False, [True] * 4, True),\n        (\"\", 1, True, [True] * 4, True),\n        (\"\", 2, False, [True] * 4, True),\n        # - None by default (with Inductor)\n        (\"\", 3, True, [False] * 4, False),\n        (\"\", 4, True, [False] * 4, False),\n        # - All by default (without Inductor)\n        (\"\", 3, False, [True] * 4, True),\n        (\"\", 4, False, [True] * 4, True),\n        # Explicitly enabling/disabling\n        #\n        # Default: all\n        #\n        # All but SiluAndMul\n        (\"+rms_norm,-silu_and_mul\", 0, True, [1, 0, 1, 1], True),\n        # Only ReLU3\n        (\"none,-rms_norm,+relu3\", 1, False, [0, 0, 0, 1], False),\n        # All but SiluAndMul\n        (\"all,-silu_and_mul\", 2, True, [1, 0, 1, 1], True),\n        # All but ReLU3 (even if ReLU2 is on)\n        (\"-relu3,relu2\", 3, False, [1, 1, 1, 0], True),\n        # RMSNorm and SiluAndMul\n        (\"none,-relu3,+rms_norm,+silu_and_mul\", 4, False, [1, 1, 0, 0], False),\n        # All but RMSNorm\n        (\"-rms_norm\", 3, False, [0, 1, 1, 1], True),\n        #\n        # Default: none\n        #\n        # Only ReLU3\n        (\"-silu_and_mul,+relu3\", 3, True, [0, 0, 0, 1], False),\n        # All but RMSNorm\n        (\"all,-rms_norm\", 4, True, [0, 1, 1, 1], True),\n    ])\ndef test_enabled_ops(env: str, torch_level: int, use_inductor: bool,\n                     ops_enabled: list[int], default_on: bool):\n    vllm_config = VllmConfig(\n        compilation_config=CompilationConfig(use_inductor=bool(use_inductor),\n                                             level=torch_level,\n                                             custom_ops=env.split(\",\")))\n    with set_current_vllm_config(vllm_config):\n        assert CustomOp.default_on() == default_on\n\n        ops_enabled = [bool(x) for x in ops_enabled]\n\n        assert RMSNorm(1024).enabled() == ops_enabled[0]\n        assert CustomOp.op_registry[\"rms_norm\"].enabled() == ops_enabled[0]\n\n        assert SiluAndMul().enabled() == ops_enabled[1]\n        assert CustomOp.op_registry[\"silu_and_mul\"].enabled() == ops_enabled[1]\n\n        assert GeluAndMul().enabled() == ops_enabled[2]\n        assert CustomOp.op_registry[\"gelu_and_mul\"].enabled() == ops_enabled[2]\n\n        # If registered, subclasses should follow their own name\n        assert Relu3().enabled() == ops_enabled[3]\n        assert CustomOp.op_registry[\"relu3\"].enabled() == ops_enabled[3]\n\n        # Unregistered subclass\n        class SiluAndMul2(SiluAndMul):\n            pass\n\n        # Subclasses should not require registration\n        assert SiluAndMul2().enabled() == SiluAndMul().enabled()\n\n\n@pytest.mark.parametrize(\n    \"env\", [\"all,none\", \"all,+rms_norm,all\", \"+rms_norm,-rms_norm\"])\ndef test_enabled_ops_invalid(env: str):\n    with pytest.raises(Exception):  # noqa\n        vllm_config = VllmConfig(compilation_config=CompilationConfig(\n            custom_ops=env.split(\",\")))\n        with set_current_vllm_config(vllm_config):\n            RMSNorm(1024).enabled()\n\n\n@pytest.mark.skipif(\n    not current_platform.is_rocm() or not current_platform.is_fp8_fnuz(),\n    reason=\"AITER is a feature exclusive for ROCm and FP8_FNUZ\")\n@pytest.mark.parametrize(\"use_cutlass\", [True, False])\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\n@pytest.mark.parametrize(\"use_rocm_aiter_gemm_w8a8_blockscale\", [\"0\", \"1\"])\ndef test_w8a8_blockscale_dispatch(use_cutlass: bool, use_rocm_aiter: str,\n                                  use_rocm_aiter_gemm_w8a8_blockscale: str,\n                                  monkeypatch):\n\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER_LINEAR\",\n                       use_rocm_aiter_gemm_w8a8_blockscale)\n\n    use_aiter_and_is_supported = (bool(int(use_rocm_aiter)) and bool(\n        int(use_rocm_aiter_gemm_w8a8_blockscale)))\n    block_scale_func = dispatch_w8a8_blockscale_func(\n        use_cutlass, use_aiter_and_is_supported=use_aiter_and_is_supported)\n    if use_cutlass:\n        assert block_scale_func == cutlass_scaled_mm\n    elif current_platform.is_rocm() and int(use_rocm_aiter) and int(\n            use_rocm_aiter_gemm_w8a8_blockscale):\n        assert block_scale_func == (\n            torch.ops.vllm.rocm_aiter_gemm_w8a8_blockscale)\n    else:\n        assert block_scale_func == w8a8_block_fp8_matmul\n\n\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\ndef test_topk_dispatch(use_rocm_aiter: str, monkeypatch):\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    topk_func = dispatch_topk_func()\n    is_rocm_aiter_moe_enabled.cache_clear()\n    if current_platform.is_rocm() and int(use_rocm_aiter):\n        from vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (\n            rocm_aiter_topk_softmax)\n        assert topk_func == rocm_aiter_topk_softmax\n    else:\n        assert topk_func == vllm_topk_softmax\n\n\n@pytest.mark.parametrize(\"add_residual\", [True, False])\n@pytest.mark.parametrize(\"use_rocm_aiter\", [\"0\", \"1\"])\n@pytest.mark.parametrize(\"use_rocm_aiter_norm\", [\"0\", \"1\"])\n@pytest.mark.skipif(not current_platform.is_rocm(),\n                    reason=\"AITER is a feature exclusive for ROCm\")\ndef test_rms_norm_dispatch(add_residual: bool, use_rocm_aiter: str,\n                           use_rocm_aiter_norm: str, monkeypatch):\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER\", use_rocm_aiter)\n    monkeypatch.setenv(\"VLLM_ROCM_USE_AITER_RMSNORM\", use_rocm_aiter_norm)\n    rms_norm_func = dispatch_cuda_rmsnorm_func(add_residual)\n\n    if not add_residual:\n        if current_platform.is_rocm() and int(use_rocm_aiter) and int(\n                use_rocm_aiter_norm):\n            assert rms_norm_func == rocm_aiter_rms_norm\n        else:\n            assert rms_norm_func == rms_norm\n    elif current_platform.is_rocm() and int(use_rocm_aiter) and int(\n            use_rocm_aiter_norm):\n        assert rms_norm_func == rocm_aiter_fused_add_rms_norm\n    else:\n        assert rms_norm_func == fused_add_rms_norm"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)\n\nSwitching from torch._scaled_mm to vLLM's cutlass fp8 kernels when supported as we are seeing 5-15% improvement in e2e performance on neuralmagic/Meta-Llama-3-8B-Instruct-FP8\n\nsee https://docs.google.com/spreadsheets/d/1GiAnmzyGHgZ6zL_LDSTm35Bdrt4A8AaFEurDlISYYA4/ for some quick e2e benchmarks and #5144 for comparisons across different GEMM sizes.", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/388596c9_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/8d75fe48_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-9474e89", "created_at": "2024-03-20T07:11:11+00:00", "base_commit": "20478c4d3abcd0aa8a1d9ace9c76ea3a2e04cb5e", "head_commit": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01", "patch": "diff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex 44ac05a14..9473a33f0 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -4,7 +4,7 @@ from typing import List\n \n from vllm import SamplingParams\n from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager import (BlockAllocator, BlockSpaceManager,\n+from vllm.core.block_manager import (UncachedBlockAllocator, BlockSpaceManager,\n                                      AllocStatus)\n from vllm.utils import Device\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus, Logprob\n@@ -15,7 +15,8 @@ from .utils import create_dummy_prompt\n def test_block_allocator_allocate():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     num_free = num_cpu_blocks\n@@ -24,7 +25,7 @@ def test_block_allocator_allocate():\n         block = cpu_allocator.allocate()\n         num_free -= 1\n \n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n     with pytest.raises(ValueError):\n@@ -34,14 +35,15 @@ def test_block_allocator_allocate():\n def test_block_allocator_free():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     blocks: List[PhysicalTokenBlock] = []\n     for _ in range(num_cpu_blocks):\n         block = cpu_allocator.allocate()\n         blocks.append(block)\n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n \n     # Free all allocated cpu blocks.\n     num_free = 0\n@@ -49,7 +51,7 @@ def test_block_allocator_free():\n     for block in blocks:\n         cpu_allocator.free(block)\n         num_free += 1\n-        assert block.block_hash in cpu_allocator.evictor\n+        assert block in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n         with pytest.raises(ValueError):\ndiff --git a/tests/prefix_caching/test_prefix_caching.py b/tests/prefix_caching/test_prefix_caching.py\nindex c83551c36..cb61aac39 100644\n--- a/tests/prefix_caching/test_prefix_caching.py\n+++ b/tests/prefix_caching/test_prefix_caching.py\n@@ -4,7 +4,7 @@ Run `pytest tests/prefix_caching/test_prefix_caching.py`.\n \"\"\"\n import pytest\n \n-from vllm.core.block_manager import BlockAllocator\n+from vllm.core.block_manager import CachedBlockAllocator\n from vllm.utils import Device\n \n \n@@ -15,10 +15,7 @@ def test_block_allocator(\n     num_blocks: int,\n ):\n     block_hash = 1\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n \n     # Allocate two PysicalTokenBlocks with the same hash and check\n     # that they are the same PhysicalTokenBlock\n@@ -45,10 +42,7 @@ def test_block_allocator(\n @pytest.mark.parametrize(\"num_blocks\", [16])\n def test_eviction(num_blocks: int, ):\n     block_size = 16\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n     blocks = []\n \n     for i in range(num_blocks):\ndiff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nindex 8b089a565..ad9b557fd 100644\n--- a/vllm/core/block_manager.py\n+++ b/vllm/core/block_manager.py\n@@ -3,6 +3,7 @@ import enum\n from itertools import count, takewhile\n from os.path import commonprefix\n from typing import Dict, List, Optional, Set, Tuple\n+from abc import ABC, abstractmethod\n \n from vllm.block import BlockTable, PhysicalTokenBlock\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n@@ -10,7 +11,7 @@ from vllm.utils import Device\n from vllm.core.evictor import Evictor, EvictionPolicy, make_evictor\n \n \n-class BlockAllocator:\n+class BlockAllocatorBase(ABC):\n     \"\"\"Manages free physical token blocks for a device.\n \n     The allocator maintains a list of free blocks and allocates a block when\n@@ -18,23 +19,57 @@ class BlockAllocator:\n     the reference count becomes zero, the block is added back to the free list.\n     \"\"\"\n \n+    @abstractmethod\n     def __init__(self,\n                  device: Device,\n                  block_size: int,\n                  num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n-                 enable_caching: bool = False) -> None:\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n+        pass\n+\n+    @abstractmethod\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        pass\n+\n+    @abstractmethod\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        pass\n+\n+    @abstractmethod\n+    def get_num_free_blocks(self) -> int:\n+        pass\n+\n+    @abstractmethod\n+    def contains_block(self, block_hash: int) -> bool:\n+        pass\n+\n+    @abstractmethod\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        pass\n+\n+\n+class CachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 device: Device,\n+                 block_size: int,\n+                 num_blocks: int,\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n         self.device = device\n         self.block_size = block_size\n         self.num_blocks = num_blocks\n-        self.enable_caching = enable_caching\n \n         self.current_num_blocks = 0\n         self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n \n-        # Switch over to FIFO eviction when caching is disabled\n-        if not self.enable_caching:\n-            eviction_policy = EvictionPolicy.FIFO\n         self.evictor: Evictor = make_evictor(eviction_policy)\n \n         self.default_hash_ctr = count()\n@@ -57,13 +92,6 @@ class BlockAllocator:\n     def allocate(self,\n                  block_hash: Optional[int] = None,\n                  num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        # If caching is disabled, just allocate a new block and return it\n-        if not self.enable_caching:\n-            block = self.allocate_block(next(self.default_hash_ctr),\n-                                        num_hashed_tokens)\n-            block.ref_count += 1\n-            return block\n-\n         if block_hash is None:\n             block_hash = next(self.default_hash_ctr)\n         if block_hash in self.evictor:\n@@ -90,9 +118,8 @@ class BlockAllocator:\n             assert block.block_hash not in self.evictor\n             self.evictor.add(block)\n \n-            # If caching is enabled, remove the block from the cached_blocks\n-            if self.enable_caching:\n-                del self.cached_blocks[block.block_hash]\n+            # Remove the block from the cached_blocks\n+            del self.cached_blocks[block.block_hash]\n \n     def get_num_free_blocks(self) -> int:\n         return (self.num_blocks - self.current_num_blocks +\n@@ -102,14 +129,68 @@ class BlockAllocator:\n         return block_hash in self.cached_blocks or block_hash in self.evictor\n \n     def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        # If caching is enabled, update the hash of block and the\n-        # cached_blocks dictionary.\n-        if self.enable_caching:\n-            assert not self.contains_block(block_hash)\n-            old_hash = block.block_hash\n-            block.block_hash = block_hash\n-            del self.cached_blocks[old_hash]\n-            self.cached_blocks[block_hash] = block\n+        # Update the hash of block and the cached_blocks dictionary.\n+        assert not self.contains_block(block_hash)\n+        old_hash = block.block_hash\n+        block.block_hash = block_hash\n+        del self.cached_blocks[old_hash]\n+        self.cached_blocks[block_hash] = block\n+\n+\n+class UncachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        device: Device,\n+        block_size: int,\n+        num_blocks: int,\n+    ) -> None:\n+        self.device = device\n+        self.block_size = block_size\n+        self.num_blocks = num_blocks\n+\n+        # Initialize the free blocks.\n+        self.free_blocks: BlockTable = []\n+        for i in range(num_blocks):\n+            block = PhysicalTokenBlock(device=device,\n+                                       block_number=i,\n+                                       block_size=block_size,\n+                                       block_hash=-1,\n+                                       num_hashed_tokens=0)\n+            self.free_blocks.append(block)\n+\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        if not self.free_blocks:\n+            raise ValueError(\"Out of memory! No free blocks are available.\")\n+        block = self.free_blocks.pop()\n+        block.ref_count = 1\n+        return block\n+\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        if block.ref_count == 0:\n+            raise ValueError(f\"Double free! {block} is already freed.\")\n+        block.ref_count -= 1\n+        if block.ref_count == 0:\n+            self.free_blocks.append(block)\n+\n+    def get_num_free_blocks(self) -> int:\n+        return len(self.free_blocks)\n+\n+    def contains_block(self, block_hash: int) -> bool:\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n+\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n \n \n class AllocStatus(enum.Enum):\n@@ -142,6 +223,10 @@ class BlockSpaceManager:\n         self.num_total_gpu_blocks = num_gpu_blocks\n         self.num_total_cpu_blocks = num_cpu_blocks\n \n+        if enable_caching and sliding_window is not None:\n+            raise NotImplementedError(\n+                \"Sliding window is not allowed with prefix caching enabled!\")\n+\n         self.block_sliding_window = None\n         if sliding_window is not None:\n             assert sliding_window % block_size == 0, (sliding_window,\n@@ -154,14 +239,17 @@ class BlockSpaceManager:\n         self.enable_caching = enable_caching\n \n         self.watermark_blocks = int(watermark * num_gpu_blocks)\n-        self.gpu_allocator = BlockAllocator(Device.GPU,\n-                                            block_size,\n-                                            num_gpu_blocks,\n-                                            enable_caching=enable_caching)\n-        self.cpu_allocator = BlockAllocator(Device.CPU,\n-                                            block_size,\n-                                            num_cpu_blocks,\n-                                            enable_caching=enable_caching)\n+\n+        if self.enable_caching:\n+            self.gpu_allocator = CachedBlockAllocator(Device.GPU, block_size,\n+                                                      num_gpu_blocks)\n+            self.cpu_allocator = CachedBlockAllocator(Device.CPU, block_size,\n+                                                      num_cpu_blocks)\n+        else:\n+            self.gpu_allocator = UncachedBlockAllocator(\n+                Device.GPU, block_size, num_gpu_blocks)\n+            self.cpu_allocator = UncachedBlockAllocator(\n+                Device.CPU, block_size, num_cpu_blocks)\n         # Mapping: seq_id -> BlockTable.\n         self.block_tables: Dict[int, BlockTable] = {}\n \n@@ -198,10 +286,16 @@ class BlockSpaceManager:\n             if (self.block_sliding_window is not None\n                     and logical_idx >= self.block_sliding_window):\n                 block = block_table[logical_idx % self.block_sliding_window]\n-            else:\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n+            elif self.enable_caching:\n                 block = self.gpu_allocator.allocate(\n                     seq.hash_of_block(logical_idx),\n                     seq.num_hashed_tokens_of_block(logical_idx))\n+            else:\n+                block = self.gpu_allocator.allocate()\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n             block_table.append(block)\n \n         # Assign the block table for each sequence.\n@@ -220,8 +314,10 @@ class BlockSpaceManager:\n         seq: Sequence,\n         last_block: PhysicalTokenBlock,\n     ) -> PhysicalTokenBlock:\n-        # Compute a new hash for the block so that it can be shared by\n-        # other Sequences\n+        assert self.enable_caching\n+\n+        # Compute a new hash for the block so that it can be shared by other\n+        # Sequences\n         new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n \n         # if new_hash is already in the cached table, then free last_block\n@@ -254,6 +350,8 @@ class BlockSpaceManager:\n         self,\n         seq: Sequence,\n     ) -> PhysicalTokenBlock:\n+        if not self.enable_caching:\n+            return self.gpu_allocator.allocate()\n         block_hash: Optional[int] = None\n         if (self._is_last_block_full(seq)):\n             block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n@@ -293,10 +391,12 @@ class BlockSpaceManager:\n         assert last_block.device == Device.GPU\n         if last_block.ref_count == 1:\n             # Not shared with other sequences. Appendable.\n-            # If the last block is now complete, promote it to a full block so\n-            # that it can be shared\n-            new_block = self._maybe_promote_last_block(seq, last_block)\n-            block_table[-1] = new_block\n+            if self.enable_caching:\n+                # If the last block is now complete, we may reuse an old block\n+                # to save memory.\n+                maybe_new_block = self._maybe_promote_last_block(\n+                    seq, last_block)\n+                block_table[-1] = maybe_new_block\n             return None\n         else:\n             # The last block is shared with other sequences.\n@@ -440,9 +540,12 @@ class BlockSpaceManager:\n         seq: Sequence,\n         access_time: float,\n     ) -> None:\n-        block_table = self.block_tables[seq.seq_id]\n-        for block in block_table:\n-            block.last_accessed = access_time\n+        if self.enable_caching:\n+            # Update the last accessed time of all the blocks accessed\n+            # in this step.\n+            block_table = self.block_tables[seq.seq_id]\n+            for block in block_table:\n+                block.last_accessed = access_time\n \n     def compute_full_blocks_in_seq(self, seq: Sequence):\n         if seq.seq_id not in self.block_tables:\ndiff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex 1d81f5a97..9f401cba3 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,5 +1,5 @@\n import enum\n-from typing import Dict, List, Optional\n+from typing import Dict\n from abc import ABC, abstractmethod, abstractproperty\n \n from vllm.block import PhysicalTokenBlock\n@@ -10,7 +10,6 @@ class EvictionPolicy(enum.Enum):\n        Evictor subclass.\n     \"\"\"\n     LRU = enum.auto()\n-    FIFO = enum.auto()\n \n \n class Evictor(ABC):\n@@ -66,37 +65,18 @@ class LRUEvictor(Evictor):\n \n     # TODO: The performance of this evict function can be optimized further.\n     def evict(self) -> PhysicalTokenBlock:\n-        free_blocks: List[PhysicalTokenBlock] = list(self.free_table.values())\n-        if len(free_blocks) == 0:\n+        if len(self.free_table) == 0:\n             raise ValueError(\"No usable cache memory left\")\n+        free_blocks = self.free_table.values()\n \n-        # Find lowest timestamp\n-        lowest_timestamp = free_blocks[0].last_accessed\n-        for block in free_blocks:\n-            if block.last_accessed < lowest_timestamp:\n-                lowest_timestamp = block.last_accessed\n+        # Get evicted block\n+        evicted_block: PhysicalTokenBlock = next(iter(free_blocks))\n \n-        # Find all blocks with the lowest timestamp\n-        least_recent: List[PhysicalTokenBlock] = []\n         for block in free_blocks:\n-            if block.last_accessed == lowest_timestamp:\n-                least_recent.append(block)\n-\n-        # Find highest prefix count per block\n-        highest_num_hashed_tokens = 0\n-        for block in least_recent:\n-            if block.num_hashed_tokens > highest_num_hashed_tokens:\n-                highest_num_hashed_tokens = block.num_hashed_tokens\n-\n-        evicted_block: Optional[PhysicalTokenBlock] = None\n-\n-        # Find the first block with the lowest timestamp\n-        for block in least_recent:\n-            if block.num_hashed_tokens == highest_num_hashed_tokens:\n+            if (block.last_accessed < evicted_block.last_accessed\n+                    or block.last_accessed == evicted_block.last_accessed and\n+                    block.num_hashed_tokens > evicted_block.num_hashed_tokens):\n                 evicted_block = block\n-                break\n-\n-        assert evicted_block is not None\n \n         del self.free_table[evicted_block.block_hash]\n \n@@ -119,43 +99,8 @@ class LRUEvictor(Evictor):\n         return len(self.free_table)\n \n \n-class RandomEvictor(Evictor):\n-    \"\"\"Evicts in a first-in-first-out order\"\"\"\n-\n-    def __init__(self):\n-        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n-\n-    def __contains__(self, block_hash: int) -> bool:\n-        return block_hash in self.free_table\n-\n-    def evict(self) -> PhysicalTokenBlock:\n-        if len(self.free_table) == 0:\n-            raise ValueError(\"No usable cache memory left\")\n-        evicted_block = next(iter(self.free_table.values()))\n-        evicted_block.computed = False\n-        del self.free_table[evicted_block.block_hash]\n-        return evicted_block\n-\n-    def add(self, block: PhysicalTokenBlock):\n-        self.free_table[block.block_hash] = block\n-\n-    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n-        if block_hash not in self.free_table:\n-            raise ValueError(\n-                \"Attempting to remove block that's not in the evictor\")\n-        block: PhysicalTokenBlock = self.free_table[block_hash]\n-        del self.free_table[block_hash]\n-        return block\n-\n-    @property\n-    def num_blocks(self) -> int:\n-        return len(self.free_table)\n-\n-\n def make_evictor(eviction_policy: EvictionPolicy) -> Evictor:\n     if eviction_policy == EvictionPolicy.LRU:\n         return LRUEvictor()\n-    elif eviction_policy == EvictionPolicy.FIFO:\n-        return RandomEvictor()\n     else:\n         raise ValueError(f\"Unknown cache eviction policy: {eviction_policy}\")", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"Test hashing of cache blocks.\n\nRun `pytest tests/test_cache_block_hashing.py`.\n\"\"\"\nfrom typing import Optional\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport pytest\n\nfrom vllm.inputs import token_inputs\nfrom vllm.beam_search import LoRARequest\nfrom vllm.compilation.backends import Sequence\nfrom vllm.engine.llm_engine import TokenizerGroup\n\n# Make two prefixes with different first blocks.\nprefix_start = [(\"You are an expert\"), (\"You are a\")]\nprefix_common = (\n    \" school principal, skilled in effectively managing \"\n    \"faculty and staff. Draft 10-15 questions for a potential first grade \"\n    \"Head Teacher for my K-12, all-girls', independent school that emphasizes \"\n    \"community, joyful discovery, and life-long learning. The candidate is \"\n    \"coming in for a first-round panel interview for a 8th grade Math \"\n    \"teaching role. They have 5 years of previous teaching experience \"\n    \"as an assistant teacher at a co-ed, public school with experience \"\n    \"in middle school math teaching. Based on this, fulfill \"\n    \"the following: \")\nprefixes = [start + prefix_common for start in prefix_start]\n\n# Sample prompts.\nsample_prompts = [\n    \"Hello, my name is\", \"The president of the United States is\",\n    \"The capital of France is\", \"The future of AI is\"\n]\n\n\n# Helper function.\ndef flatten_2d(li):\n    return [lss for ls in li for lss in ls]\n\n\n@pytest.mark.parametrize(\"model\", [\"facebook/opt-125m\"])\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"max_num_seqs\", [256])\n@pytest.mark.parametrize(\"concurrent_lora_int_ids\",\n                         [[None], [1], [None, 1], [None, 1, 2], [1, 2]])\ndef test_auto_prefix_caching(model: str, block_size: int, max_num_seqs: int,\n                             concurrent_lora_int_ids: list[Optional[int]]):\n\n    tokenizer = TokenizerGroup(\n        tokenizer_id=\"facebook/opt-125m\",\n        enable_lora=False,\n        max_num_seqs=max_num_seqs,\n        max_input_length=None,\n    )\n\n    hashes: list[list[list[int]]] = []\n\n    for prefix in prefixes:\n        for lora_int_id in concurrent_lora_int_ids:\n            lora_request = None\n\n            if lora_int_id is not None:\n                lora_request = LoRARequest(\n                    f\"example_lora_{lora_int_id}\",\n                    lora_int_id,\n                    f\"example/path/to/lora_{lora_int_id}\",\n                )\n\n            hashes.append([])\n            prompts = [prefix + prompt for prompt in sample_prompts]\n            for seq_id, prompt in enumerate(prompts):\n                hashes[-1].append([])\n                prompt_token_ids = tokenizer.encode(prompt)\n                seq = Sequence(seq_id,\n                               inputs=token_inputs(prompt_token_ids,\n                                                   prompt=prompt),\n                               block_size=block_size,\n                               eos_token_id=tokenizer.tokenizer.eos_token_id,\n                               lora_request=lora_request)\n\n                num_blocks = len(prompt_token_ids) // block_size\n                for idx in range(num_blocks):\n                    hashes[-1][-1].append(seq.hash_of_block(idx))\n\n    # Check that hashes made with two prefixes with different first blocks are\n    # different everywhere.\n    for hash0, hash1 in zip(flatten_2d(hashes[0]), flatten_2d(hashes[1])):\n        assert (hash0 != hash1)\n\n    # Check that hashes of different prompts made with the same prefix are the\n    # same until the hashes that contain the prompt.\n    for hash_pref in hashes:\n        same_hashes = [tuple(h[:-1]) for h in hash_pref]\n        different_hashes = [h[-1] for h in hash_pref]\n        assert (len(set(same_hashes)) == 1)\n        assert (len(set(different_hashes)) == len(different_hashes))"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)\n\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/20478c4d_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/9474e89b_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-aea9436", "created_at": "2025-01-22T22:22:12+00:00", "base_commit": "7206ce4ce112ed117796a59045c968a6d353f691", "head_commit": "aea94362c9bdd08ed2b346701bdc09d278e85f66", "patch": "diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..f510c4150 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -1,5 +1,6 @@\n import asyncio\n import atexit\n+import gc\n import importlib\n import inspect\n import multiprocessing\n@@ -104,6 +105,11 @@ async def lifespan(app: FastAPI):\n             task.add_done_callback(_running_tasks.remove)\n         else:\n             task = None\n+\n+        # Mark the startup heap as static so that it's ignored by GC.\n+        # Reduces pause times of oldest generation collections.\n+        gc.collect()\n+        gc.freeze()\n         try:\n             yield\n         finally:\ndiff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..80403f77d 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -3,7 +3,7 @@\n import re\n import time\n from argparse import Namespace\n-from typing import Any, Dict, List, Literal, Optional, Union\n+from typing import Any, ClassVar, Dict, List, Literal, Optional, Set, Union\n \n import torch\n from pydantic import BaseModel, ConfigDict, Field, model_validator\n@@ -42,23 +42,31 @@ class OpenAIBaseModel(BaseModel):\n     # OpenAI API does allow extra fields\n     model_config = ConfigDict(extra=\"allow\")\n \n+    # Cache class field names\n+    field_names: ClassVar[Optional[Set[str]]] = None\n+\n     @model_validator(mode=\"before\")\n     @classmethod\n     def __log_extra_fields__(cls, data):\n-        if isinstance(data, dict):\n+\n+        field_names = cls.field_names\n+        if field_names is None:\n+            if not isinstance(data, dict):\n+                return data\n             # Get all class field names and their potential aliases\n             field_names = set()\n             for field_name, field in cls.model_fields.items():\n                 field_names.add(field_name)\n-                if hasattr(field, 'alias') and field.alias:\n-                    field_names.add(field.alias)\n-\n-            # Compare against both field names and aliases\n-            extra_fields = data.keys() - field_names\n-            if extra_fields:\n-                logger.warning(\n-                    \"The following fields were present in the request \"\n-                    \"but ignored: %s\", extra_fields)\n+                if alias := getattr(field, 'alias', None):\n+                    field_names.add(alias)\n+            cls.field_names = field_names\n+\n+        # Compare against both field names and aliases\n+        if any(k not in field_names for k in data):\n+            logger.warning(\n+                \"The following fields were present in the request \"\n+                \"but ignored: %s\",\n+                data.keys() - field_names)\n         return data\n \n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 1e68326b2..3a15e00e7 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -73,6 +73,7 @@ if TYPE_CHECKING:\n     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n     VLLM_DISABLE_COMPILE_CACHE: bool = False\n     VLLM_SERVER_DEV_MODE: bool = False\n+    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n \n \n def get_default_cache_root():\n@@ -474,6 +475,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # e.g. `/reset_prefix_cache`\n     \"VLLM_SERVER_DEV_MODE\":\n     lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n+\n+    # Controls the maximum number of requests to handle in a\n+    # single asyncio task when processing per-token outputs in the\n+    # V1 AsyncLLM interface. It is applicable when handling a high\n+    # concurrency of streaming requests.\n+    # Setting this too high can result in a higher variance of\n+    # inter-message latencies. Setting it too low can negatively impact\n+    # TTFT and overall throughput.\n+    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n+    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n }\n \n # end-env-vars-definition\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e4411..1505b6250 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -2,9 +2,12 @@ import asyncio\n import os\n from typing import AsyncGenerator, List, Mapping, Optional, Type, Union\n \n+import numpy as np\n+\n from vllm.config import ModelConfig, VllmConfig\n from vllm.engine.arg_utils import AsyncEngineArgs\n from vllm.engine.protocol import EngineClient\n+from vllm.envs import VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n from vllm.inputs import INPUT_REGISTRY, InputRegistry, PromptType\n from vllm.inputs.preprocess import InputPreprocessor\n from vllm.logger import init_logger\n@@ -16,7 +19,7 @@ from vllm.sampling_params import SamplingParams\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\n from vllm.usage.usage_lib import UsageContext\n-from vllm.utils import kill_process_tree\n+from vllm.utils import cdiv, kill_process_tree\n from vllm.v1.engine.core_client import EngineCoreClient\n from vllm.v1.engine.output_processor import OutputProcessor\n from vllm.v1.engine.processor import Processor\n@@ -205,17 +208,15 @@ class AsyncLLM(EngineClient):\n \n             # The output_handler task pushes items into the queue.\n             # This task pulls from the queue and yields to caller.\n-            while True:\n+            finished = False\n+            while not finished:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                out = q.get_nowait() if not q.empty() else await q.get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\n-                if out.finished:\n-                    yield out\n-                    break\n-\n+                finished = out.finished\n                 yield out\n \n         # If the request is disconnected by the client, the\n@@ -233,22 +234,41 @@ class AsyncLLM(EngineClient):\n                 # 1) Pull EngineCoreOutputs from the EngineCore.\n                 outputs = await self.engine_core.get_output_async()\n \n-                # 2) Process EngineCoreOutputs.\n-                processed_outputs = self.output_processor.process_outputs(\n-                    outputs.outputs)\n-                # NOTE: RequestOutputs are pushed to their queues.\n-                assert len(processed_outputs.request_outputs) == 0\n-\n-                # 3) Abort any reqs that finished due to stop strings.\n-                await self.engine_core.abort_requests_async(\n-                    processed_outputs.reqs_to_abort)\n+                # Split outputs into chunks of at most\n+                # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the\n+                # event loop for too long.\n+                num_outputs = len(outputs.outputs)\n+                if num_outputs <= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:\n+                    slices = (outputs.outputs, )\n+                else:\n+                    slices = np.array_split(\n+                        outputs.outputs,\n+                        cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n+\n+                iteration_stats = None\n+                for i, outputs_slice in enumerate(slices):\n+                    # 2) Process EngineCoreOutputs.\n+                    processed_outputs = self.output_processor.process_outputs(\n+                        outputs_slice, iteration_stats)\n+                    # NOTE: RequestOutputs are pushed to their queues.\n+                    assert not processed_outputs.request_outputs\n+                    iteration_stats = processed_outputs.iteration_stats\n+\n+                    # Allow other asyncio tasks to run between chunks\n+                    if i + 1 < len(slices):\n+                        await asyncio.sleep(0)\n+\n+                    # 3) Abort any reqs that finished due to stop strings.\n+                    await self.engine_core.abort_requests_async(\n+                        processed_outputs.reqs_to_abort)\n \n                 # 4) Logging.\n                 # TODO(rob): make into a coroutine and launch it in\n                 # background thread once we add Prometheus.\n+                assert iteration_stats is not None\n                 self._log_stats(\n                     scheduler_stats=outputs.scheduler_stats,\n-                    iteration_stats=processed_outputs.iteration_stats,\n+                    iteration_stats=iteration_stats,\n                 )\n \n         except Exception as e:\ndiff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py\nindex 19b89003c..f3b992d68 100644\n--- a/vllm/v1/engine/core_client.py\n+++ b/vllm/v1/engine/core_client.py\n@@ -1,8 +1,9 @@\n+import asyncio\n import os\n import signal\n import weakref\n from abc import ABC, abstractmethod\n-from typing import List, Type\n+from typing import List, Optional, Type\n \n import msgspec\n import zmq\n@@ -255,10 +256,24 @@ class AsyncMPClient(MPClient):\n             log_stats=True,\n         )\n \n+        self.outputs_queue: Optional[asyncio.Queue[bytes]] = None\n+        self.queue_task: Optional[asyncio.Task] = None\n+\n     async def get_output_async(self) -> EngineCoreOutputs:\n+        if self.outputs_queue is None:\n+            # Perform IO in separate task to parallelize as much as possible\n+            self.outputs_queue = asyncio.Queue()\n+\n+            async def process_outputs_socket():\n+                assert self.outputs_queue is not None\n+                while True:\n+                    (frame, ) = await self.output_socket.recv_multipart(\n+                        copy=False)\n+                    self.outputs_queue.put_nowait(frame.buffer)\n+\n+            self.queue_task = asyncio.create_task(process_outputs_socket())\n \n-        frames = await self.output_socket.recv_multipart(copy=False)\n-        return self.decoder.decode(frames[0].buffer)\n+        return self.decoder.decode(await self.outputs_queue.get())\n \n     async def _send_input(self, request_type: EngineCoreRequestType,\n                           request: EngineCoreRequestUnion) -> None:\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..564eab51b 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -101,6 +101,7 @@ class OutputProcessor:\n     def process_outputs(\n         self,\n         engine_core_outputs: List[EngineCoreOutput],\n+        iteration_stats: Optional[IterationStats] = None,\n     ) -> OutputProcessorOutput:\n         \"\"\"\n         Process the EngineCoreOutputs:\n@@ -133,7 +134,8 @@ class OutputProcessor:\n \n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n-        iteration_stats = IterationStats(self.log_stats)\n+        if not iteration_stats:\n+            iteration_stats = IterationStats(self.log_stats)\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n             req_state = self.request_states.get(req_id)\n@@ -175,8 +177,8 @@ class OutputProcessor:\n             iteration_stats=iteration_stats,\n         )\n \n+    @staticmethod\n     def _make_request_output(\n-        self,\n         request_state: RequestState,\n         detokenizer_output: Optional[DetokenizerOutput],\n     ) -> Optional[RequestOutput]:\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..eefcdaf29 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -64,6 +64,12 @@ class Request:\n         # recomputing.\n         self._kv_block_hashes: List[BlockHashType] = []\n \n+        # Read-only views\n+        # Prevent directly appending to the these lists since\n+        # they should also be updated simultaneously.\n+        self.output_token_ids = ConstantList(self._output_token_ids)\n+        self.all_token_ids = ConstantList(self._all_token_ids)\n+\n     @classmethod\n     def from_engine_core_request(cls, request: EngineCoreRequest) -> \"Request\":\n         return cls(\n@@ -79,18 +85,6 @@ class Request:\n             lora_request=request.lora_request,\n         )\n \n-    @property\n-    def output_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the output_token_ids since\n-        # all_token_ids should also be updated simultaneously.\n-        return ConstantList(self._output_token_ids)\n-\n-    @property\n-    def all_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the all_token_ids since\n-        # output_token_ids should also be updated simultaneously\n-        return ConstantList(self._all_token_ids)\n-\n     def append_output_token_ids(\n         self,\n         token_ids: Union[int, List[int]],", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport os\nimport subprocess\nimport sys\nimport time\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport pytest\nimport requests\n\n\ndef _query_server(prompt: str, max_tokens: int = 5) -> dict:\n    response = requests.post(\"http://localhost:8000/generate\",\n                             json={\n                                 \"prompt\": prompt,\n                                 \"max_tokens\": max_tokens,\n                                 \"temperature\": 0,\n                                 \"ignore_eos\": True\n                             })\n    response.raise_for_status()\n    return response.json()\n\n\ndef _query_server_long(prompt: str) -> dict:\n    return _query_server(prompt, max_tokens=500)\n\n\n@pytest.fixture\ndef api_server(distributed_executor_backend: str):\n    script_path = Path(__file__).parent.joinpath(\n        \"api_server_async_engine.py\").absolute()\n    commands = [\n        sys.executable,\n        \"-u\",\n        str(script_path),\n        \"--model\",\n        \"facebook/opt-125m\",\n        \"--host\",\n        \"127.0.0.1\",\n        \"--distributed-executor-backend\",\n        distributed_executor_backend,\n    ]\n\n    # API Server Test Requires V0.\n    my_env = os.environ.copy()\n    my_env[\"VLLM_USE_V1\"] = \"0\"\n    uvicorn_process = subprocess.Popen(commands, env=my_env)\n    yield\n    uvicorn_process.terminate()\n\n\n@pytest.mark.parametrize(\"distributed_executor_backend\", [\"mp\", \"ray\"])\ndef test_api_server(api_server, distributed_executor_backend: str):\n    \"\"\"\n    Run the API server and test it.\n\n    We run both the server and requests in separate processes.\n\n    We test that the server can handle incoming requests, including\n    multiple requests at the same time, and that it can handle requests\n    being cancelled without crashing.\n    \"\"\"\n    with Pool(32) as pool:\n        # Wait until the server is ready\n        prompts = [\"warm up\"] * 1\n        result = None\n        while not result:\n            try:\n                for r in pool.map(_query_server, prompts):\n                    result = r\n                    break\n            except requests.exceptions.ConnectionError:\n                time.sleep(1)\n\n        # Actual tests start here\n        # Try with 1 prompt\n        for result in pool.map(_query_server, prompts):\n            assert result\n\n        num_aborted_requests = requests.get(\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\n        assert num_aborted_requests == 0\n\n        # Try with 100 prompts\n        prompts = [\"test prompt\"] * 100\n        for result in pool.map(_query_server, prompts):\n            assert result\n\n    with Pool(32) as pool:\n        # Cancel requests\n        prompts = [\"canceled requests\"] * 100\n        pool.map_async(_query_server_long, prompts)\n        time.sleep(0.01)\n        pool.terminate()\n        pool.join()\n\n        # check cancellation stats\n        # give it some times to update the stats\n        time.sleep(1)\n\n        num_aborted_requests = requests.get(\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\n        assert num_aborted_requests > 0\n\n    # check that server still runs after cancellations\n    with Pool(32) as pool:\n        # Try with 100 prompts\n        prompts = [\"test prompt after canceled\"] * 100\n        for result in pool.map(_query_server, prompts):\n            assert result\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Frontend][V1] Online serving performance improvements (#12287)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/7206ce4c_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/aea94362_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 179, "symbols_collected": 3614, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 179, "symbols_collected": 3618, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-b2e0ad3", "created_at": "2024-11-15T00:38:20+00:00", "base_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b", "head_commit": "b2e0ad3b598ed0e022cdbd678a20821d411873c2", "patch": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..e53631ef1 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,8 +90,8 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n-        x = self.act_fn(gate_up)\n+        x, _ = self.gate_up_proj(x)\n+        x = self.act_fn(x)\n         x, _ = self.down_proj(x)\n         return x", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"Test that we handle an Error in model forward and shutdown.\"\"\"\n\nimport asyncio\n\nimport pytest\n\nfrom tests.utils import wait_for_gpu_memory_to_clear\nfrom tests.v1.shutdown.utils import (SHUTDOWN_TEST_THRESHOLD_BYTES,\n                                     SHUTDOWN_TEST_TIMEOUT_SEC)\nfrom vllm import LLM, AsyncEngineArgs, SamplingParams\nfrom vllm.distributed import get_tensor_model_parallel_rank\nfrom vllm.model_executor.models.llama import LlamaForCausalLM\nfrom vllm.utils import cuda_device_count_stateless\nfrom vllm.v1.engine.async_llm import AsyncLLM\nfrom vllm.v1.engine.exceptions import EngineDeadError\n\nMODELS = [\"meta-llama/Llama-3.2-1B\"]\n\n\ndef evil_forward(self, *args, **kwargs):\n    \"\"\"Evil forward method that raise an exception after 10 calls.\"\"\"\n    NUMBER_OF_GOOD_PASSES = 10\n\n    if not hasattr(self, \"num_calls\"):\n        self.num_calls = 0\n\n    if (self.num_calls == NUMBER_OF_GOOD_PASSES\n            and get_tensor_model_parallel_rank() == 0):\n        raise Exception(\"Simulated illegal memory access on Rank 0!\")\n    self.num_calls += 1\n\n    return self.model(*args, **kwargs)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"tensor_parallel_size\", [2, 1])\n@pytest.mark.parametrize(\"model\", MODELS)\nasync def test_async_llm_model_error(monkeypatch, tensor_parallel_size: int,\n                                     model: str) -> None:\n    \"\"\"Test that AsyncLLM propagates a forward pass error and frees memory.\n    \n    AsyncLLM always uses an MP client.\n    \"\"\"\n    if cuda_device_count_stateless() < tensor_parallel_size:\n        pytest.skip(reason=\"Not enough CUDA devices\")\n\n    # Monkeypatch an error in the model.\n    monkeypatch.setattr(LlamaForCausalLM, \"forward\", evil_forward)\n\n    engine_args = AsyncEngineArgs(model=model,\n                                  enforce_eager=True,\n                                  tensor_parallel_size=tensor_parallel_size)\n    async_llm = AsyncLLM.from_engine_args(engine_args)\n\n    async def generate(request_id: str):\n        generator = async_llm.generate(\"Hello my name is\",\n                                       request_id=request_id,\n                                       sampling_params=SamplingParams())\n        try:\n            async for _ in generator:\n                pass\n        except Exception as e:\n            return e\n\n    NUM_REQS = 3\n    tasks = [generate(f\"request-{idx}\") for idx in range(NUM_REQS)]\n    outputs = await asyncio.gather(*tasks)\n\n    # Every request should get an EngineDeadError.\n    for output in outputs:\n        assert isinstance(output, EngineDeadError)\n\n    # AsyncLLM should be errored.\n    assert async_llm.errored\n\n    # We should not be able to make another request.\n    with pytest.raises(EngineDeadError):\n        async for _ in async_llm.generate(\"Hello my name is\",\n                                          request_id=\"abc\",\n                                          sampling_params=SamplingParams()):\n            raise Exception(\"We should not get here.\")\n\n    # Confirm all the processes are cleaned up.\n    wait_for_gpu_memory_to_clear(\n        devices=list(range(tensor_parallel_size)),\n        threshold_bytes=2 * 2**30,\n        timeout_s=60,\n    )\n\n    # NOTE: shutdown is handled by the API Server if an exception\n    # occurs, so it is expected that we would need to call this.\n    async_llm.shutdown()\n\n\n@pytest.mark.timeout(SHUTDOWN_TEST_TIMEOUT_SEC)\n@pytest.mark.parametrize(\"enable_multiprocessing\", [True])\n@pytest.mark.parametrize(\"tensor_parallel_size\", [2, 1])\n@pytest.mark.parametrize(\"model\", MODELS)\ndef test_llm_model_error(monkeypatch, tensor_parallel_size: int,\n                         enable_multiprocessing: bool, model: str) -> None:\n    \"\"\"Test that LLM propagates a forward pass error and frees memory.\n    TODO(andy) - LLM without multiprocessing; LLM with multiprocessing\n    and >1 rank\n    \"\"\"\n    if cuda_device_count_stateless() < tensor_parallel_size:\n        pytest.skip(reason=\"Not enough CUDA devices\")\n\n    with monkeypatch.context() as m:\n\n        MP_VALUE = \"1\" if enable_multiprocessing else \"0\"\n        m.setenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", MP_VALUE)\n\n        # Monkeypatch an error in the model.\n        m.setattr(LlamaForCausalLM, \"forward\", evil_forward)\n\n        llm = LLM(model=model,\n                  enforce_eager=True,\n                  tensor_parallel_size=tensor_parallel_size)\n\n        with pytest.raises(\n                EngineDeadError if enable_multiprocessing else Exception):\n            llm.generate(\"Hello my name is Robert and I\")\n\n        # Confirm all the processes are cleaned up.\n        wait_for_gpu_memory_to_clear(\n            devices=list(range(tensor_parallel_size)),\n            threshold_bytes=SHUTDOWN_TEST_THRESHOLD_BYTES,\n        )\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Perf] Reduce peak memory usage of llama (#10339)\n\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/4a18fd14_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/b2e0ad3b_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-b6d1035", "created_at": "2024-03-30T21:26:38+00:00", "base_commit": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447", "head_commit": "b6d103542c654fb63013a1e45a586d654ae36a2a", "patch": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex 6bf5d5130..c7d3d8538 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+      list(REMOVE_ITEM GPU_FLAGS\n+        \"-D__CUDA_NO_HALF_OPERATORS__\"\n+        \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+        \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\"\n+        \"-D__CUDA_NO_HALF2_OPERATORS__\")\n     endif()\n \n   elseif(${GPU_LANG} STREQUAL \"HIP\")\ndiff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex 6d34d014c..ea30fa274 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -4,6 +4,16 @@\n \n #include \"dispatch_utils.h\"\n #include \"reduction_utils.cuh\"\n+#ifndef USE_ROCM\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n+#else\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n+\n+  using __nv_bfloat16 = __hip_bfloat16;\n+  using __nv_bfloat162 = __hip_bfloat162;\n+#endif\n \n namespace vllm {\n \n@@ -35,9 +45,199 @@ __global__ void rms_norm_kernel(\n   }\n }\n \n-// TODO: Further optimize this kernel.\n-template<typename scalar_t>\n-__global__ void fused_add_rms_norm_kernel(\n+\n+/* Converter structs for the conversion from torch types to HIP/CUDA types,\n+   and the associated type conversions within HIP/CUDA. These helpers need\n+   to be implemented for now because the relevant type conversion\n+   operators/constructors are not consistently implemented by HIP/CUDA, so\n+   a generic conversion via type casts cannot be implemented.\n+\n+   Each struct should have the member static constexpr bool `exists`:\n+   If false, the optimized kernel is not used for the corresponding torch type.\n+   If true, the struct should be fully defined as shown in the examples below. \n+ */\n+template<typename torch_type>\n+struct _typeConvert { static constexpr bool exists = false; };\n+\n+template<>\n+struct _typeConvert<c10::Half> {\n+  static constexpr bool exists = true;\n+  using hip_type = __half;\n+  using packed_hip_type = __half2;\n+\n+  __device__ static inline float convert(hip_type x) { return __half2float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __half22float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2half_rn(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22half2_rn(x); }\n+};\n+\n+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800\n+// CUDA_ARCH < 800 does not have BF16 support\n+// TODO: Add in ROCm support once public headers handle bf16 maturely\n+template<>\n+struct _typeConvert<c10::BFloat16> {\n+  static constexpr bool exists = true;\n+  using hip_type = __nv_bfloat16;\n+  using packed_hip_type = __nv_bfloat162;\n+\n+  __device__ static inline float convert(hip_type x) { return __bfloat162float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __bfloat1622float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2bfloat16(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22bfloat162_rn(x); }\n+};\n+#endif\n+\n+\n+/* Vector POD struct to generate vectorized and packed FP16/BF16 ops\n+   for appropriate specializations of fused_add_rms_norm_kernel.\n+   Only functions that are necessary in that kernel are implemented.\n+   Alignment to 16 bytes is required to use 128-bit global memory ops.\n+ */\n+template<typename scalar_t, int width>\n+struct alignas(16) _f16Vec {\n+  /* Not theoretically necessary that width is a power of 2 but should \n+     almost always be the case for optimization purposes */ \n+  static_assert(width > 0 && (width & (width - 1)) == 0,\n+                \"Width is not a positive power of 2!\");\n+  using Converter = _typeConvert<scalar_t>;\n+  using T1 = typename Converter::hip_type;\n+  using T2 = typename Converter::packed_hip_type;\n+  T1 data[width];\n+\n+  __device__ _f16Vec& operator+=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp += T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] += other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp *= T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] *= other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const float scale) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 temp_f = Converter::convert(T2{data[i], data[i+1]});\n+        temp_f.x *= scale;\n+        temp_f.y *= scale;\n+        T2 temp = Converter::convert(temp_f);\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float temp = Converter::convert(data[i]) * scale;\n+        data[i] = Converter::convert(temp);\n+      }\n+    }\n+    return *this;\n+  }\n+\n+  __device__ float sum_squares() const {\n+    float result = 0.0f;\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 z = Converter::convert(T2{data[i], data[i+1]});\n+        result += z.x * z.x + z.y * z.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float x = Converter::convert(data[i]);\n+        result += x * x;\n+      }\n+    }\n+    return result;\n+  }\n+};\n+\n+/* Function specialization in the case of FP16/BF16 tensors.\n+   Additional optimizations we can make in this case are\n+   packed and vectorized operations, which help with the\n+   memory latency bottleneck. */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width > 0) && _typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n+  scalar_t* __restrict__ input,           // [..., hidden_size]\n+  scalar_t* __restrict__ residual,        // [..., hidden_size]\n+  const scalar_t* __restrict__ weight,    // [hidden_size]\n+  const float epsilon,\n+  const int num_tokens,\n+  const int hidden_size) {\n+  // Sanity checks on our vector struct and type-punned pointer arithmetic\n+  static_assert(std::is_pod_v<_f16Vec<scalar_t, width>>);\n+  static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);\n+\n+  const int vec_hidden_size = hidden_size / width;\n+  __shared__ float s_variance;\n+  float variance = 0.0f;\n+  /* These and the argument pointers are all declared `restrict` as they are\n+     not aliased in practice. Argument pointers should not be dereferenced\n+     in this kernel as that would be undefined behavior */\n+  auto* __restrict__ input_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(input);\n+  auto* __restrict__ residual_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(residual);\n+  auto* __restrict__ weight_v = reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = input_v[id];\n+    temp += residual_v[id];\n+    variance += temp.sum_squares();\n+    residual_v[id] = temp;\n+  }\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n+  if (threadIdx.x == 0) {\n+    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+  }\n+  __syncthreads();\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = residual_v[id];\n+    temp *= s_variance;\n+    temp *= weight_v[idx];\n+    input_v[id] = temp;\n+  }\n+}\n+\n+\n+/* Generic fused_add_rms_norm_kernel\n+   The width field is not used here but necessary for other specializations.\n+ */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width == 0) || !_typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n   scalar_t* __restrict__ input,           // [..., hidden_size]\n   scalar_t* __restrict__ residual,        // [..., hidden_size]\n   const scalar_t* __restrict__ weight,    // [hidden_size]\n@@ -48,12 +248,17 @@ __global__ void fused_add_rms_norm_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    x += (float) residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[blockIdx.x * hidden_size + idx];\n+    z += residual[blockIdx.x * hidden_size + idx];\n+    float x = (float) z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n+    residual[blockIdx.x * hidden_size + idx] = z;\n   }\n-  variance = blockReduceSum<float>(variance);\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n   }\n@@ -93,6 +298,21 @@ void rms_norm(\n     });\n }\n \n+#define LAUNCH_FUSED_ADD_RMS_NORM(width)              \\\n+  VLLM_DISPATCH_FLOATING_TYPES(                       \\\n+    input.scalar_type(),                              \\\n+    \"fused_add_rms_norm_kernel\",                      \\\n+    [&] {                                             \\\n+      vllm::fused_add_rms_norm_kernel                 \\\n+      <scalar_t, width><<<grid, block, 0, stream>>>(  \\\n+        input.data_ptr<scalar_t>(),                   \\\n+        residual.data_ptr<scalar_t>(),                \\\n+        weight.data_ptr<scalar_t>(),                  \\\n+        epsilon,                                      \\\n+        num_tokens,                                   \\\n+        hidden_size);                                 \\\n+    });\n+\n void fused_add_rms_norm(\n   torch::Tensor& input,    // [..., hidden_size]\n   torch::Tensor& residual, // [..., hidden_size]\n@@ -102,19 +322,29 @@ void fused_add_rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  /* This kernel is memory-latency bound in many scenarios.\n+     When num_tokens is large, a smaller block size allows\n+     for increased block occupancy on CUs and better latency\n+     hiding on global mem ops. */\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-  VLLM_DISPATCH_FLOATING_TYPES(\n-    input.scalar_type(),\n-    \"fused_add_rms_norm_kernel\",\n-    [&] {\n-      vllm::fused_add_rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        input.data_ptr<scalar_t>(),\n-        residual.data_ptr<scalar_t>(),\n-        weight.data_ptr<scalar_t>(),\n-        epsilon,\n-        num_tokens,\n-        hidden_size);\n-    });\n+  /*If the tensor types are FP16/BF16, try to use the optimized kernel\n+    with packed + vectorized ops.\n+    Max optimization is achieved with a width-8 vector of FP16/BF16s\n+    since we can load at most 128 bits at once in a global memory op.\n+    However, this requires each tensor's data to be aligned to 16\n+    bytes.\n+   */\n+  auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());\n+  auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());\n+  auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());\n+  bool ptrs_are_aligned = inp_ptr % 16 == 0 && res_ptr % 16 == 0 \\\n+                          && wt_ptr % 16 == 0;\n+  if (ptrs_are_aligned && hidden_size % 8 == 0) {\n+    LAUNCH_FUSED_ADD_RMS_NORM(8);\n+  } else {\n+    LAUNCH_FUSED_ADD_RMS_NORM(0);\n+  }\n }\ndiff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nindex c25464e86..bb5171f85 100644\n--- a/csrc/reduction_utils.cuh\n+++ b/csrc/reduction_utils.cuh\n@@ -20,43 +20,45 @@\n #include \"cuda_compat.h\"\n \n namespace vllm {\n-\n-template<typename T>\n+template<typename T, int numLanes = WARP_SIZE>\n __inline__ __device__ T warpReduceSum(T val) {\n-#pragma unroll\n-  for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1)\n+  static_assert(numLanes > 0 && (numLanes & (numLanes - 1)) == 0,\n+                \"numLanes is not a positive power of 2!\");\n+  static_assert(numLanes <= WARP_SIZE);\n+  #pragma unroll\n+  for (int mask = numLanes >> 1; mask > 0; mask >>= 1)\n     val += VLLM_SHFL_XOR_SYNC(val, mask);\n   return val;\n }\n \n-__inline__ __device__ constexpr int _calculateLaneMask(int warp_size) {\n-  return warp_size - 1;\n-}\n-\n-__inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n-  return 5 + (warp_size >> 6);\n+// Helper function to return the next largest power of 2\n+static constexpr int _nextPow2(unsigned int num) {\n+  if (num <= 1) return num;\n+  return 1 << (CHAR_BIT * sizeof(num) - __builtin_clz(num - 1));\n }\n \n /* Calculate the sum of all elements in a block */\n-template<typename T>\n+template<typename T, int maxBlockSize = 1024>\n __inline__ __device__ T blockReduceSum(T val) {\n-  static __shared__ T shared[WARP_SIZE];\n-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n-  int lane = threadIdx.x & LANE_MASK;\n-  int wid = threadIdx.x >> WID_SHIFT;\n-\n-  val = warpReduceSum<T>(val);\n-\n-  if (lane == 0)\n-    shared[wid] = val;\n+  static_assert(maxBlockSize <= 1024);\n+  if constexpr (maxBlockSize > WARP_SIZE) {\n+    val = warpReduceSum<T>(val);\n+    // Calculates max number of lanes that need to participate in the last warpReduce\n+    constexpr int maxActiveLanes = (maxBlockSize + WARP_SIZE - 1) / WARP_SIZE;\n+    static __shared__ T shared[maxActiveLanes];\n+    int lane = threadIdx.x % WARP_SIZE;\n+    int wid = threadIdx.x / WARP_SIZE;\n+    if (lane == 0)\n+      shared[wid] = val;\n \n-  __syncthreads();\n+    __syncthreads();\n \n-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n-  // blockDim.x is not divided by 32\n-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n-  val = warpReduceSum<T>(val);\n+    val = (threadIdx.x < blockDim.x / float(WARP_SIZE)) ? shared[lane] : (T)(0.0f);\n+    val = warpReduceSum<T, _nextPow2(maxActiveLanes)>(val);\n+  } else {\n+    // A single warpReduce is equal to blockReduce\n+    val = warpReduceSum<T, _nextPow2(maxBlockSize)>(val);\n+  }\n   return val;\n }\n \ndiff --git a/tests/kernels/test_layernorm.py b/tests/kernels/test_layernorm.py\nindex b1e3c1a7f..210d59e4f 100644\n--- a/tests/kernels/test_layernorm.py\n+++ b/tests/kernels/test_layernorm.py\n@@ -5,7 +5,8 @@ from vllm.model_executor.layers.layernorm import RMSNorm\n \n DTYPES = [torch.half, torch.bfloat16, torch.float]\n NUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\n-HIDDEN_SIZES = [768, 5120, 8192]  # Arbitrary values for testing\n+HIDDEN_SIZES = [768, 769, 770, 771, 5120, 5124, 5125, 5126, 8192,\n+                8199]  # Arbitrary values for testing\n ADD_RESIDUAL = [False, True]\n SEEDS = [0]\n CUDA_DEVICES = [", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport ctypes\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport pytest\nimport torch\n\nfrom vllm.platforms import current_platform\n\n\ndef check_cuda_context():\n    \"\"\"Check CUDA driver context status\"\"\"\n    try:\n        cuda = ctypes.CDLL('libcuda.so')\n        device = ctypes.c_int()\n        result = cuda.cuCtxGetDevice(ctypes.byref(device))\n        return (True, device.value) if result == 0 else (False, None)\n    except Exception:\n        return False, None\n\n\ndef run_cuda_test_in_thread(device_input, expected_device_id):\n    \"\"\"Run CUDA context test in separate thread for isolation\"\"\"\n    try:\n        # New thread should have no CUDA context initially\n        valid_before, device_before = check_cuda_context()\n        if valid_before:\n            return False, \\\n                \"CUDA context should not exist in new thread, \" \\\n                f\"got device {device_before}\"\n\n        # Test setting CUDA context\n        current_platform.set_device(device_input)\n\n        # Verify context is created correctly\n        valid_after, device_id = check_cuda_context()\n        if not valid_after:\n            return False, \"CUDA context should be valid after set_cuda_context\"\n        if device_id != expected_device_id:\n            return False, \\\n                f\"Expected device {expected_device_id}, got {device_id}\"\n\n        return True, \"Success\"\n    except Exception as e:\n        return False, f\"Exception in thread: {str(e)}\"\n\n\nclass TestSetCudaContext:\n    \"\"\"Test suite for the set_cuda_context function.\"\"\"\n\n    @pytest.mark.skipif(not current_platform.is_cuda(),\n                        reason=\"CUDA not available\")\n    @pytest.mark.parametrize(argnames=\"device_input,expected_device_id\",\n                             argvalues=[\n                                 (0, 0),\n                                 (torch.device('cuda:0'), 0),\n                                 ('cuda:0', 0),\n                             ],\n                             ids=[\"int\", \"torch_device\", \"string\"])\n    def test_set_cuda_context_parametrized(self, device_input,\n                                           expected_device_id):\n        \"\"\"Test setting CUDA context in isolated threads.\"\"\"\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(run_cuda_test_in_thread, device_input,\n                                     expected_device_id)\n            success, message = future.result(timeout=30)\n        assert success, message\n\n    @pytest.mark.skipif(not current_platform.is_cuda(),\n                        reason=\"CUDA not available\")\n    def test_set_cuda_context_invalid_device_type(self):\n        \"\"\"Test error handling for invalid device type.\"\"\"\n        with pytest.raises(ValueError, match=\"Expected a cuda device\"):\n            current_platform.set_device(torch.device('cpu'))\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] Layernorm performance optimization (#3662)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/51c31bc1_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/b6d10354_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-baeded2", "created_at": "2025-02-01T05:52:51+00:00", "base_commit": "3e1c76cf3a87854396d9e86a56a335e7d750c85f", "head_commit": "baeded25699f9f4851843306f27f685c4d4ee7c5", "patch": "diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py\nindex c6c8a6034..e8fec234c 100644\n--- a/vllm/attention/backends/mla/utils.py\n+++ b/vllm/attention/backends/mla/utils.py\n@@ -1,17 +1,29 @@\n from abc import abstractmethod\n from dataclasses import dataclass\n-from typing import Any, Dict, Generic, List, Optional\n+from typing import Any, Dict, Generic, List, Optional, Tuple\n \n import torch\n+from compressed_tensors.quantization import QuantizationStrategy\n \n from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionLayer,\n                                               AttentionMetadata,\n                                               MLAAttentionImpl, T)\n-from vllm.distributed import get_tensor_model_parallel_world_size\n+from vllm.distributed import (get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n-                                               RowParallelLinear)\n+                                               LinearBase, RowParallelLinear,\n+                                               UnquantizedLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501\n+    CompressedTensorsLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.schemes import (\n+    CompressedTensorsW8A8Fp8)\n+from vllm.model_executor.layers.quantization.fp8 import Fp8LinearMethod\n+from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n+    apply_fp8_linear_generic, current_platform_fp8_dtype, is_fp8)\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    scaled_dequantize, scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n from vllm.vllm_flash_attn import flash_attn_varlen_func\n \n@@ -25,11 +37,11 @@ class MLACommonMetadata(AttentionMetadata):\n \n class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     \"\"\"\n-    Common class for implementing repeated parts \n-    \n+    Common class for implementing repeated parts\n+\n     Main reference: DeepseekV2 paper, and FlashInfer Implementation\n     (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n-    \n+\n     Deepseek's MLA attention works the following way:\n     * Use a single latent vector to represent the entire KV cache.\n     * The attention \"simulates\" a multi-head attention, while the compute is\n@@ -46,7 +58,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         * V: V head dim.\n         * kv_c: latent/compressed KV\n         * q_c: latent/compressed Q\n-        \n+\n         #\n         # Outside the MLA attention backend\n         #\n@@ -55,21 +67,21 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n            kv_c_k_pe (B, Lkv+R).\n         2. The kv_c_k_pe is split into kv_c (B, Lkv) and k_pe (B, R). cq\n            and kv_c are normalized.\n-        \n+\n         #\n         # Inside the MLA attention backend\n         #\n \n         * if prefill:\n-        \n-        3. The q_c is then projected up into the multi-head version. \n-           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope \n-             (B, N, P) and q_pe (B, N, R). \n+\n+        3. The q_c is then projected up into the multi-head version.\n+           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope\n+             (B, N, P) and q_pe (B, N, R).\n         4. q_pe, k_pe are then passed through rotary embeddings.\n         5. kv_c and k_pe are concatenated and inserted into the cache\n-        6. The kv_c is then projected up into the multi-head version. \n-           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope \n-             dimensions for K and V, which is split into k_nope (B, N, P) \n+        6. The kv_c is then projected up into the multi-head version.\n+           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope\n+             dimensions for K and V, which is split into k_nope (B, N, P)\n              and v (B, N, V).\n         7. q (B, N, (P+R)) and k (B, N, (P+R)) matrices are assembled from\n            q_nope, q_pe, k_nope, k_pe.\n@@ -112,7 +124,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     From @tsu-bin's calculation, we only want to use the absorption technique\n     for decode. The prefill algorithm should still use the up-projected MHA\n     for less flops and memory usage.\n-    \n+\n     \"\"\"\n \n     def __init__(\n@@ -162,8 +174,19 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _v_up_proj_and_o_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n-            return self.o_proj_absorbed(\n-                x.reshape(-1, self.num_heads * self.kv_lora_rank))[0]\n+            if is_fp8(self.W_UV_O):\n+                output_parallel = apply_fp8_linear_generic(\n+                    x.flatten(start_dim=1), self.W_UV_O, self.W_UV_O_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape)\n+            else:\n+                output_parallel = torch.matmul(x.flatten(start_dim=1),\n+                                               self.W_UV_O)\n+            if self.tp_size > 1:\n+                output = tensor_model_parallel_all_reduce(output_parallel)\n+            else:\n+                output = output_parallel\n+            return output\n         else:\n             x = torch.einsum(\"bnl,lnv->bnv\", x, self.W_UV)\n             return self.o_proj(x.reshape(-1,\n@@ -171,6 +194,12 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _q_proj_and_k_up_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            if is_fp8(self.W_Q_UK):\n+                return apply_fp8_linear_generic(\n+                    x, self.W_Q_UK, self.W_Q_UK_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape).view(\n+                        -1, self.num_heads, self.kv_lora_rank)\n             return torch.matmul(x, self.W_Q_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n         else:\n@@ -179,8 +208,91 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             return torch.einsum(\"bnp,lnp->bnl\", x, self.W_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n \n-    def process_weights_after_loading(self):\n-        kv_b_proj_weight = self.kv_b_proj.weight.T\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n+\n+        def is_layer_fp8(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, Fp8LinearMethod) or\\\n+                (isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8))\n+\n+        def quantization_scheme_supported(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, UnquantizedLinearMethod) or \\\n+                is_layer_fp8(layer)\n+\n+        # TODO(lucas) This is very gross, we need a more wide scale refactor of\n+        # all the FP8 code with a more standard way of\n+        # defining schemes/group-shapes, we should also potentially force\n+        # quant_methods to support a decompress function\n+        #\n+        # returns input_group_shape, weight_group_shape\n+        def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \\\n+            Tuple[Tuple[int, int], Tuple[int, int]]:\n+            if isinstance(layer.quant_method, Fp8LinearMethod):\n+                if layer.quant_method.block_quant is not None:\n+                    weight_block_size = \\\n+                        layer.quant_method.quant_config.weight_block_size\n+                    # per-token-group (1, X), block-quantized (X, Y)\n+                    return (1, weight_block_size[-1]), weight_block_size\n+                else:\n+                    return (-1, -1), (-1, -1)  # per-tensor, per-tensor\n+            elif isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                # this is hacky but we always assume the for\n+                # CompressedTensorsW8A8Fp8 the input is dynamic per-token\n+                # we ignore if it is static-per-tensor since we are going to\n+                # requantize after later anyways\n+                strategy = layer.scheme.strategy\n+                if strategy == QuantizationStrategy.TENSOR:\n+                    return (1, -1), (-1, -1)  # per-token, per-tensor\n+                elif strategy == QuantizationStrategy.CHANNEL:\n+                    return (1, -1), (-1, 1)  # per-token, per-channel\n+                else:\n+                    raise NotImplementedError(\n+                        f\"QuantizationStrategy.{strategy} is not supported for \"\n+                        \"fp8 MLA, please run with VLLM_MLA_DISABLE=1\")\n+            else:\n+                raise NotImplementedError(\n+                    \"Can't determine scale group shapes for \"\n+                    f\"{layer.quant_method}, please run with VLLM_MLA_DISABLE=1\"\n+                )\n+\n+        def get_scales(layer: LinearBase) -> torch.Tensor:\n+            if hasattr(layer, \"weight_scale_inv\"):\n+                return layer.weight_scale_inv\n+            return layer.weight_scale\n+\n+        def get_and_maybe_dequant_weights(layer: LinearBase):\n+            if is_layer_fp8(layer):\n+                if isinstance(layer.quant_method, \\\n+                    CompressedTensorsLinearMethod) and \\\n+                    isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                    # NOTE(lucas): note sure why but `CompressedTensorsW8A8Fp8`\n+                    # seems to store weights as (input, output) instead of\n+                    # (output, input) so we need to transpose\n+                    weight = layer.weight.T  # standardize to (output, input)\n+                else:\n+                    weight = layer.weight\n+                _, weight_scale_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(layer)\n+                scales = get_scales(layer)\n+\n+                return scaled_dequantize(weight, scales,\n+                                         weight_scale_group_shape)\n+            else:\n+                return layer.weight\n+\n+        if not (quantization_scheme_supported(self.kv_b_proj) and\\\n+            quantization_scheme_supported(self.q_proj) and\\\n+                quantization_scheme_supported(self.o_proj)):\n+            raise NotImplementedError(\n+                \"Only FP8 and UnquantizedLinearMethod are supported for MLA\"\n+                \", please run with VLLM_MLA_DISABLE=1\")\n+\n+        weight_dtype = self.kv_b_proj.weight.dtype\n+        assert self.o_proj.weight.dtype == weight_dtype\n+        assert self.q_proj.weight.dtype == weight_dtype\n+\n+        kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T\n         assert kv_b_proj_weight.shape == (\n             self.kv_lora_rank,\n             self.num_heads * (self.qk_nope_head_dim + self.v_head_dim)), (\n@@ -198,18 +310,35 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         W_UK, W_UV = kv_b_proj_weight.split(\n             [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n \n-        q_proj = self.q_proj.weight.T\\\n+        q_proj_weight = get_and_maybe_dequant_weights(self.q_proj).T\\\n                 .view(-1, self.num_heads, self.qk_head_dim)\n \n         # can be W_Q or W_UQ depending q_lora_rank, the former if\n         # q_lora_rank is None, the latter otherwise. From the Attention backend\n         # perspective though we call these both W_Q and rely on the layer\n         # to pass in the correct matrix\n-        W_Q = q_proj[..., :self.qk_nope_head_dim]\n-        self.W_QR = q_proj[..., self.qk_nope_head_dim:]\\\n+        W_Q = q_proj_weight[..., :self.qk_nope_head_dim]\n+        self.W_QR = q_proj_weight[..., self.qk_nope_head_dim:]\\\n             .flatten(start_dim=1).contiguous()\n \n+        # W_QR is small so for simplicity we dont bother requantizing it\n+        self.W_QR = self.W_QR.to(act_dtype)\n+\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            requantization_enabled = not envs.VLLM_MLA_DISABLE_REQUANTIZATION\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                # This assumes it wise to requantize using the same group shapes\n+                # (i.e. strategy, per-tensor, per-channel, block etc.) that the\n+                # weights were originally quantized\n+                requant_input_group_shape, requant_weight_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(self.q_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.kv_b_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.o_proj)\n+                self.reqaunt_input_group_shape = requant_input_group_shape\n+                self.reqaunt_weight_group_shape = requant_weight_group_shape\n+\n             #\n             # Perform matrix-absorption following\n             #     https://github.com/flashinfer-ai/flashinfer/pull/551\n@@ -223,25 +352,44 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             # latter otherwise\n             # basically if q_lora_rank is none we are absorbing into q_proj\n             # instead of UQ\n-            self.W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n+            W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n                 .flatten(start_dim=1).contiguous()\n \n-            W_O = self.o_proj.weight\\\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_Q_UK, W_Q_UK_scales = scaled_quantize(\n+                    W_Q_UK,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_Q_UK = W_Q_UK.T.contiguous()\n+                self.W_Q_UK_scales = W_Q_UK_scales.T.contiguous()\n+            else:\n+                self.W_Q_UK = W_Q_UK.to(act_dtype)\n+\n+            W_O = get_and_maybe_dequant_weights(self.o_proj)\\\n                 .view(-1, self.num_heads, self.v_head_dim)\n-            self.W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n+            W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n                 .flatten(start_dim=0, end_dim=1).contiguous()\n \n-            tp_size = get_tensor_model_parallel_world_size()\n-            self.o_proj_absorbed = RowParallelLinear(\n-                self.W_UV_O.shape[0] * tp_size,\n-                self.W_UV_O.shape[1],\n-                bias=False,\n-                # TODO(lucas) figure out how to properly forward quant_method\n-                #quant_config=self.o_proj.quant_method,\n-            )\n-\n-            self.o_proj_absorbed.weight = torch.nn.Parameter(self.W_UV_O.T)\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_UV_O, W_UV_O_scales = scaled_quantize(\n+                    W_UV_O,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_UV_O = W_UV_O.T.contiguous()\n+                self.W_UV_O_scales = W_UV_O_scales.T.contiguous()\n+            else:\n+                self.W_UV_O = W_UV_O.to(act_dtype)\n+\n+            self.tp_size = get_tensor_model_parallel_world_size()\n         else:\n+            if is_fp8(weight_dtype):\n+                raise NotImplementedError(\n+                    \"Currently fp8 requires matrix absorption\")\n+\n             self.W_UV = W_UV\n             self.W_UK = W_UK\n             self.W_Q = W_Q.flatten(start_dim=1)\ndiff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex da09bb70b..95dc119a4 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -57,14 +57,12 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_kv_cache_shape(\n-            num_blocks: int,\n-            block_size: int,\n-            num_kv_heads: int,  # assumed to be 1 for MLA\n-            kv_lora_rank: int,  # passed via head_size\n+        num_blocks: int,\n+        block_size: int,\n+        num_kv_heads: int,  # assumed to be 1 for MLA\n+        head_size: int,\n     ) -> Tuple[int, ...]:\n-        # TODO(lucas): remove hardcoding k_pe size as 1/8th of kv_lora_rank\n-        k_pe_size = kv_lora_rank // 8\n-        return (num_blocks, block_size, kv_lora_rank + k_pe_size)\n+        return (num_blocks, block_size, head_size)\n \n     @staticmethod\n     def swap_blocks(\n@@ -83,7 +81,7 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_supported_head_sizes() -> List[int]:\n-        return [512]\n+        return [576]\n \n \n class TritonMLAState(AttentionState):\n@@ -624,8 +622,6 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             self.multimodal_placeholder_maps.items()\n         }\n \n-        num_kv_splits = 8\n-\n         return TritonMLAMetadata(\n             num_prefills=self.num_prefills,\n             slot_mapping=slot_mapping_tensor,\n@@ -645,7 +641,7 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             context_lens_tensor=context_lens_tensor,\n             block_tables=block_tables,\n             use_cuda_graph=use_captured_graph,\n-            num_kv_splits=num_kv_splits,\n+            num_kv_splits=4,  # TODO(lucas) add heuristic\n             head_dim=self.runner.model_config.get_head_size(),\n         )\n \ndiff --git a/vllm/attention/layer.py b/vllm/attention/layer.py\nindex 9b804a29a..b97165f62 100644\n--- a/vllm/attention/layer.py\n+++ b/vllm/attention/layer.py\n@@ -200,9 +200,9 @@ class Attention(nn.Module):\n         s += f\", backend={self.impl.__class__.__name__}\"\n         return s\n \n-    def process_weights_after_loading(self):\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n         if hasattr(self.impl, \"process_weights_after_loading\"):\n-            self.impl.process_weights_after_loading()\n+            self.impl.process_weights_after_loading(act_dtype)\n \n \n class MultiHeadAttention(nn.Module):\ndiff --git a/vllm/config.py b/vllm/config.py\nindex f6bd8b1ad..f998502ee 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -739,18 +739,19 @@ class ModelConfig:\n     @property\n     def is_deepseek_mla(self) -> bool:\n         # TODO add deepseek_v3\n-        return hasattr(self.hf_text_config,\n-                       \"model_type\") and (self.hf_text_config.model_type\n-                                          in ('deepseek_v2'))\n+        return (hasattr(self.hf_text_config, \"model_type\")) \\\n+                and (self.hf_text_config.model_type in \\\n+                    ('deepseek_v2', 'deepseek_v3'))\\\n+                and (self.hf_text_config.kv_lora_rank is not None)\n \n     def get_head_size(self) -> int:\n         # TODO remove hard code\n         if self.is_deepseek_mla:\n+            qk_rope_head_dim = getattr(self.hf_text_config, \"qk_rope_head_dim\",\n+                                       0)\n             if self.use_mla:\n-                return self.hf_text_config.kv_lora_rank\n+                return self.hf_text_config.kv_lora_rank + qk_rope_head_dim\n             else:\n-                qk_rope_head_dim = getattr(self.hf_text_config,\n-                                           \"qk_rope_head_dim\", 0)\n                 qk_nope_head_dim = getattr(self.hf_text_config,\n                                            \"qk_nope_head_dim\", 0)\n                 if qk_rope_head_dim and qk_nope_head_dim:\n@@ -969,6 +970,32 @@ class ModelConfig:\n \n     @property\n     def use_mla(self) -> bool:\n+        if self.quantization is not None and self.quantization not in [\\\n+            \"fp8\", \"compressed-tensors\"]:\n+            logger.warning(\n+                \"MLA is not supported with %s quantization. \"\n+                \"Disabling MLA.\", self.quantization)\n+            return False\n+\n+        # If using a \"compressed-tensors\" checkpoint, check that all groups\n+        # have fp8 for both weights and activations.\n+        if self.quantization == \"compressed-tensors\":\n+            quant_config = self._parse_quant_hf_config()\n+            for group_name, cfg in quant_config.get(\"config_groups\",\n+                                                    (\"\", {})).items():\n+                act_cfg = cfg.get(\"input_activations\", {})\n+                act_type = None if act_cfg is None else act_cfg.get(\"type\", \"\")\n+                w_cfg = cfg.get(\"weights\", {})\n+                w_type = None if w_cfg is None else w_cfg.get(\"type\", \"\")\n+                if act_type != \"fp8\" or w_type != \"fp8\":\n+                    logger.warning(\n+                        \"compressed-tensors MLA support requires fp8 \"\n+                        \"activations and weights in group '%s', but got \"\n+                        \"activations type '%s' and weights type '%s'.\\n \"\n+                        \"Full config: %s\", group_name, act_type, w_type,\n+                        quant_config)\n+                    return False\n+\n         use_mla = (self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE)\n         return use_mla\n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 2a18e3b9b..25098070b 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -79,6 +79,7 @@ if TYPE_CHECKING:\n     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n     VLLM_MLA_DISABLE: bool = False\n     VLLM_MLA_PERFORM_MATRIX_ABSORPTION: bool = True\n+    VLLM_MLA_DISABLE_REQUANTIZATION: bool = False\n \n \n def get_default_cache_root():\n@@ -519,7 +520,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # storing more weights, W_Q_UK and W_UV_O, so can increase memory usage,\n     # the is enabled by default\n     \"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\":\n-    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\")))\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\"))),\n+\n+    # When running MLA with matrix-absorption enabled and fp8 quantized weights\n+    # we perform the matrix-absorption in float32 precision, after the matrices\n+    # are absorbed we requantize the weights back to fp8, this flag can be used\n+    # to disable the requantization step, and instead convert the absorbed\n+    # matrices to match the activation type. This can lead to higher memory and\n+    # compute usage but better preserves the accuracy of the original model.\n+    \"VLLM_MLA_DISABLE_REQUANTIZATION\":\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE_REQUANTIZATION\", \"0\")))\n }\n \n # end-env-vars-definition\ndiff --git a/vllm/model_executor/layers/quantization/utils/fp8_utils.py b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\nindex ccebff341..850820f66 100644\n--- a/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n@@ -2,7 +2,7 @@\n import functools\n import json\n import os\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import triton\n@@ -10,10 +10,24 @@ import triton.language as tl\n \n from vllm import _custom_ops as ops\n from vllm.logger import init_logger\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    _normalize_quant_group_shape, scaled_dequantize)\n+from vllm.model_executor.layers.quantization.utils.w8a8_utils import (\n+    apply_fp8_linear)\n from vllm.platforms import current_platform\n \n logger = init_logger(__name__)\n \n+current_platform_fp8_dtype = (torch.float8_e4m3fnuz\n+                              if current_platform.is_rocm() else\n+                              torch.float8_e4m3fn)\n+\n+\n+def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:\n+    if isinstance(x, torch.Tensor):\n+        x = x.dtype\n+    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz\n+\n \n def apply_w8a8_block_fp8_linear(\n     input: torch.Tensor,\n@@ -55,6 +69,42 @@ def apply_w8a8_block_fp8_linear(\n     return output.to(dtype=input.dtype).view(*output_shape)\n \n \n+# Unify the interface between `apply_w8a8_block_fp8_linear` and\n+# `apply_fp8_linear`\n+# NOTE(lucas): this is quite messy, we should think through this more formally\n+def apply_fp8_linear_generic(\n+        input: torch.Tensor,\n+        weight: torch.Tensor,\n+        weight_scale: torch.Tensor,\n+        input_group_shape: Tuple[int, int],\n+        weight_group_shape: Tuple[int, int],\n+        input_scale: Optional[torch.Tensor] = None,  # static scale if one\n+) -> torch.Tensor:\n+    # View input as 2D matrix for fp8 methods\n+    input = input.view(-1, input.shape[-1])\n+\n+    weight_group_shape = _normalize_quant_group_shape(\\\n+        weight, weight_group_shape)\n+    input_group_shape = _normalize_quant_group_shape(input, input_group_shape)\n+\n+    def is_dim_blocked(dim, shape, group_shape):\n+        return group_shape < shape[dim] and group_shape > 1\n+\n+    if is_dim_blocked(0, weight.shape, weight_group_shape[0])\\\n+     and is_dim_blocked(1, weight.shape, weight_group_shape[1]) and\\\n+     input_group_shape == (1, weight_group_shape[1]):\n+        return apply_w8a8_block_fp8_linear(input, weight,\n+                                           list(weight_group_shape),\n+                                           weight_scale)\n+    else:\n+        # Despite having linear in the it doesn't conform to\n+        # `torch.nn.functional.linear` which is defined as `input @ weight.T`\n+        # so we explicitly transpose the weight matrix here\n+        return apply_fp8_linear(input, weight.T, weight_scale.T,\n+                         use_per_token_if_dynamic=\\\n+                             (input_group_shape == (1, input.shape[1])))\n+\n+\n def input_to_float8(\n         x: torch.Tensor,\n         dtype: Optional[torch.dtype] = None\n@@ -75,7 +125,6 @@ def input_to_float8(\n def block_quant_to_tensor_quant(\n     x_q_block: torch.Tensor,\n     x_s: torch.Tensor,\n-    block_size: List[int],\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"This function converts block-wise quantization to tensor-wise\n     quantization. The inputs are block-wise quantization tensor `x_q_block`,\n@@ -83,26 +132,7 @@ def block_quant_to_tensor_quant(\n     The outputs are tensor-wise quantization tensor and tensor-wise\n     quantization scale. Note only float8 is supported for now.\n     \"\"\"\n-    block_n, block_k = block_size[0], block_size[1]\n-    n, k = x_q_block.shape\n-    n_tiles = (n + block_n - 1) // block_n\n-    k_tiles = (k + block_k - 1) // block_k\n-    assert n_tiles == x_s.shape[0]\n-    assert k_tiles == x_s.shape[1]\n-\n-    x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [[\n-        x_dq_block[\n-            j * block_n:min((j + 1) * block_n, n),\n-            i * block_k:min((i + 1) * block_k, k),\n-        ] for i in range(k_tiles)\n-    ] for j in range(n_tiles)]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n-\n+    x_dq_block = scaled_dequantize(x_q_block, x_s)\n     x_q_tensor, scale = input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n     return x_q_tensor, scale\n \ndiff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 83055d600..95e785dcc 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -1,5 +1,5 @@\n \"\"\"This file is used for /tests and /benchmarks\"\"\"\n-from typing import List, Optional\n+from typing import List, Optional, Tuple\n \n import numpy\n import torch\n@@ -20,6 +20,120 @@ FUSED_LAYER_NAME_MAPPING = {\n }\n \n \n+# Normalize the group_shape to the full extent for any dims that are -1\n+def _normalize_quant_group_shape(x: torch.Tensor, group_shape: Tuple[int,\n+                                                                     int]):\n+    # -1 means full extent\n+    return (group_shape[0] if group_shape[0] > 0 else x.shape[-2],\n+            group_shape[1] if group_shape[1] > 0 else x.shape[-1])\n+\n+\n+# Useful when treating N-dimensional group scaling as extended numpy-style\n+# broadcasting in numpy simply stretches dimensions with an extent of 1 to match\n+# the target shape by repeating the data along that dimension (broadcasting)\n+# , we extend these semantics to say if the extent of a dimension in the\n+# source shape is not 1 and does not match the target shape we repeat each\n+# element along that dimension src_shape[dim] // target_shape[dim] times\n+# example if we have:\n+#       a = [[1, 2], and target_shape = (2, 4)\n+#            [3, 4]]\n+# then we would expand a to:\n+#       a = [[1, 1, 2, 2],\n+#            [3, 3, 4, 4]]\n+# NOTE this function this function does not explicitly broadcast dimensions\n+# with an extent of 1, since this can be done implicitly by pytorch\n+def group_broadcast(t, shape):\n+    for i, s in enumerate(shape):\n+        if t.shape[i] != s and t.shape[i] != 1:\n+            assert s % t.shape[i] == 0\n+            t = t.unsqueeze(i + 1)\\\n+                .expand(*t.shape[:i+1], s // t.shape[i], *t.shape[i+1:])\\\n+                .flatten(i, i + 1)\n+    return t\n+\n+\n+# Quantize assuming once scale per group of elements with shape group_shape,\n+# example group shapes:\n+#  * (-1, -1)   for per-tensor quantization\n+#  * (1, -1)    for per-row quantization\n+#  * (-1, 1)    for per-column quantization\n+#  * (128, 128) for 128x128 deepseek style block quantization\n+#  * (1, 128)   for deepseek style activation quantization\n+#               (i.e. per-token-per-group)\n+def scaled_quantize(\n+    x: torch.Tensor,\n+    group_shape: Tuple[int, int],\n+    quant_dtype: torch.dtype,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    group_shape = _normalize_quant_group_shape(x, group_shape)\n+    assert quant_dtype.is_floating_point, \\\n+        \"currently `scaled_quantize` only supports floating point dtypes \" \\\n+        \"but could be extended to support other dtypes\"\n+\n+    finfo = torch.finfo(quant_dtype)\n+\n+    # Reshape (M, N) into (BLK_M, BLOCK_SIZE_M, BLK_N, BLOCK_SIZE_N)\n+    assert x.ndim == 2\n+    assert x.shape[0] % group_shape[0] == 0 and x.shape[1] % group_shape[1] == 0\n+    blk_m, blk_n = x.shape[0] // group_shape[0], x.shape[1] // group_shape[1]\n+    x_blkd = x.reshape(blk_m, group_shape[0], blk_n, group_shape[1])\n+\n+    # Permute to (BLK_M, BLK_N, BLOCK_SIZE_M, BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd.permute(0, 2, 1, 3)\n+    # Flatten to (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd_permd.flatten(start_dim=2)\n+\n+    # Compute scales\n+    min_val, max_val = x_blkd_permd.aminmax(dim=-1)\n+    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)\n+    scale = finfo.max / amax\n+\n+    # Apply scale and convert form:\n+    # (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N) to (M, N)\n+    x_scl_sat = (x_blkd_permd * scale.unsqueeze(-1))\\\n+        .clamp(min=finfo.min, max=finfo.max)\\\n+        .reshape(blk_m, blk_n, group_shape[0], group_shape[1])\\\n+        .permute(0, 2, 1, 3)\\\n+        .reshape(x.shape)\n+\n+    return x_scl_sat.to(quant_dtype).contiguous(), scale.float().reciprocal()\n+\n+\n+# inverses `scaled_quantize`\n+def scaled_dequantize(\n+    x_q: torch.Tensor,\n+    x_s: torch.Tensor,\n+    group_shape: Optional[Tuple[int, int]] = None,\n+    out_dtype: torch.dtype = torch.float32,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if group_shape is not None:\n+        group_shape = _normalize_quant_group_shape(x_q, group_shape)\n+\n+    if x_s.ndim == 0:  # scalar\n+        x_s = x_s.unsqueeze(-1).unsqueeze(-1)  # convert to (1, 1) tensor\n+    if x_s.ndim == 1:\n+        if group_shape is None:\n+            raise AssertionError(\n+                \"if x_s is 1D tensor, group_shape must be provided otherwise \"\n+                \"its ambiguous which dimension to broadcast x_s to\")\n+        # unsqueeze the scales for the dimension where we want to broadcast\n+        # across the full extent\n+        if group_shape[0] == x_q.shape[-2]:\n+            x_s = x_s.unsqueeze(-2)\n+        elif group_shape[1] == x_q.shape[-1]:\n+            x_s = x_s.unsqueeze(-1)\n+        else:\n+            raise AssertionError(\n+                \"if x_s is a vector we should be broadcasting it to the full \"\n+                \"extent of one of the dimensions\")\n+\n+    if group_shape is not None:\n+        assert x_s.shape[-1] == x_q.shape[-1] // group_shape[1]\n+        assert x_s.shape[-2] == x_q.shape[-2] // group_shape[0]\n+    x_s = group_broadcast(x_s.to(torch.float32), x_q.shape)\n+    return (x_q.to(torch.float32) * x_s).to(out_dtype)\n+\n+\n def pack_quantized_values_into_int32(w_q: torch.Tensor,\n                                      wtype: ScalarType,\n                                      packed_dim: int = 0):\ndiff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py\nindex 62babcddd..4be511d12 100644\n--- a/vllm/model_executor/model_loader/loader.py\n+++ b/vllm/model_executor/model_loader/loader.py\n@@ -398,11 +398,13 @@ class DefaultModelLoader(BaseModelLoader):\n                     # parameters onto device for processing and back off after.\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n-                elif isinstance(module, Attention) and \\\n+                if isinstance(module, Attention) and \\\n                     hasattr(module, \"process_weights_after_loading\"):\n                     # When attention modules need to process weights after\n                     # currently only used by MLA\n-                    module.process_weights_after_loading()\n+                    # TODO(lucas): see if there is a way to unify the signatures\n+                    # of process_weights_after_loading\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -439,6 +441,11 @@ class DummyModelLoader(BaseModelLoader):\n                     with device_loading_context(\n                             module, torch.device(device_config.device)):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -633,6 +640,12 @@ class ShardedStateLoader(BaseModelLoader):\n                     quant_method = getattr(module, \"quant_method\", None)\n                     if quant_method is not None:\n                         quant_method.process_weights_after_loading(module)\n+                    if isinstance(module, Attention) and \\\n+                        hasattr(module, \"process_weights_after_loading\"):\n+                        # When attention modules need to process weights after\n+                        # currently only used by MLA\n+                        module.process_weights_after_loading(\n+                            model_config.dtype)\n             rank = get_tensor_model_parallel_rank()\n             pattern = os.path.join(\n                 local_model_path,\n@@ -1272,7 +1285,7 @@ class GGUFModelLoader(BaseModelLoader):\n \n class RunaiModelStreamerLoader(BaseModelLoader):\n     \"\"\"\n-        Model loader that can load safetensors \n+        Model loader that can load safetensors\n         files from local FS or S3 bucket.\n     \"\"\"\n \n@@ -1369,6 +1382,11 @@ class RunaiModelStreamerLoader(BaseModelLoader):\n                 if quant_method is not None:\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \ndiff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py\nindex 0b44f0d06..f6ab53c85 100644\n--- a/vllm/model_executor/models/deepseek_v3.py\n+++ b/vllm/model_executor/models/deepseek_v3.py\n@@ -27,7 +27,7 @@ from torch import nn\n from transformers import PretrainedConfig\n \n from vllm.attention import Attention, AttentionMetadata\n-from vllm.config import CacheConfig, VllmConfig\n+from vllm.config import CacheConfig, ModelConfig, VllmConfig\n from vllm.distributed import (get_pp_group,\n                               get_tensor_model_parallel_world_size,\n                               tensor_model_parallel_all_reduce)\n@@ -333,12 +333,156 @@ class DeepseekV3Attention(nn.Module):\n         return output\n \n \n+class DeepseekV3MLAAttention(nn.Module):\n+    \"\"\"\n+    Main reference: DeepseekV2 paper, and FlashInfer Implementation\n+    (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n+    \n+    For more info see MLACommonImpl in: vllm/attention/backends/mla/utils.py\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        hidden_size: int,\n+        num_heads: int,\n+        qk_nope_head_dim: int,\n+        qk_rope_head_dim: int,\n+        v_head_dim: int,\n+        q_lora_rank: Optional[int],\n+        kv_lora_rank: int,\n+        rope_theta: float = 10000,\n+        rope_scaling: Optional[Dict[str, Any]] = None,\n+        max_position_embeddings: int = 8192,\n+        cache_config: Optional[CacheConfig] = None,\n+        quant_config: Optional[QuantizationConfig] = None,\n+        prefix: str = \"\",\n+    ) -> None:\n+        super().__init__()\n+        self.hidden_size = hidden_size\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+\n+        self.q_lora_rank = q_lora_rank\n+        self.kv_lora_rank = kv_lora_rank\n+\n+        self.num_heads = num_heads\n+        tp_size = get_tensor_model_parallel_world_size()\n+        assert num_heads % tp_size == 0\n+        self.num_local_heads = num_heads // tp_size\n+\n+        self.scaling = self.qk_head_dim**-0.5\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+\n+        if self.q_lora_rank is not None:\n+            self.q_a_proj = ReplicatedLinear(self.hidden_size,\n+                                             self.q_lora_rank,\n+                                             bias=False,\n+                                             quant_config=quant_config,\n+                                             prefix=f\"{prefix}.q_a_proj\")\n+            self.q_a_layernorm = RMSNorm(self.q_lora_rank,\n+                                         eps=config.rms_norm_eps)\n+            self.q_b_proj = ColumnParallelLinear(q_lora_rank,\n+                                                 self.num_heads *\n+                                                 self.qk_head_dim,\n+                                                 bias=False,\n+                                                 quant_config=quant_config,\n+                                                 prefix=f\"{prefix}.q_b_proj\")\n+        else:\n+            self.q_proj = ColumnParallelLinear(self.hidden_size,\n+                                               self.num_heads *\n+                                               self.qk_head_dim,\n+                                               bias=False,\n+                                               quant_config=quant_config,\n+                                               prefix=f\"{prefix}.q_proj\")\n+\n+        self.kv_a_proj_with_mqa = ReplicatedLinear(\n+            self.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n+        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,\n+                                      eps=config.rms_norm_eps)\n+        self.kv_b_proj = ColumnParallelLinear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_b_proj\")\n+        self.o_proj = RowParallelLinear(self.num_heads * self.v_head_dim,\n+                                        self.hidden_size,\n+                                        bias=False,\n+                                        quant_config=quant_config,\n+                                        prefix=f\"{prefix}.o_proj\")\n+\n+        rope_scaling[\"rope_type\"] = 'deepseek_yarn'\n+        self.rotary_emb = get_rope(qk_rope_head_dim,\n+                                   rotary_dim=qk_rope_head_dim,\n+                                   max_position=max_position_embeddings,\n+                                   base=rope_theta,\n+                                   rope_scaling=rope_scaling,\n+                                   is_neox_style=False)\n+        if rope_scaling:\n+            mscale_all_dim = rope_scaling.get(\"mscale_all_dim\", False)\n+            scaling_factor = rope_scaling[\"factor\"]\n+            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))\n+            self.scaling = self.scaling * mscale * mscale\n+\n+        self.mla_attn = Attention(\n+            num_heads=self.num_local_heads,\n+            head_size=self.kv_lora_rank,\n+            scale=self.scaling,\n+            num_kv_heads=1,\n+            cache_config=cache_config,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.attn\",\n+            use_mla=True,\n+            # MLA Args\n+            q_lora_rank=self.q_lora_rank,\n+            kv_lora_rank=self.kv_lora_rank,\n+            qk_nope_head_dim=self.qk_nope_head_dim,\n+            qk_rope_head_dim=self.qk_rope_head_dim,\n+            qk_head_dim=self.qk_head_dim,\n+            v_head_dim=self.v_head_dim,\n+            rotary_emb=self.rotary_emb,\n+            q_proj=self.q_proj if self.q_lora_rank is None else self.q_b_proj,\n+            kv_b_proj=self.kv_b_proj,\n+            o_proj=self.o_proj,\n+        )\n+\n+        self.prefix = prefix\n+        self.debug_layer_idx = int(self.prefix.split(\".\")[-2])\n+\n+    def forward(\n+        self,\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        kv_cache: torch.Tensor,\n+        attn_metadata: AttentionMetadata,\n+    ) -> torch.Tensor:\n+        if self.q_lora_rank is not None:\n+            ckq = self.q_a_proj(hidden_states)[0]\n+            hidden_states_or_q_c = self.q_a_layernorm(ckq)\n+        else:\n+            hidden_states_or_q_c = hidden_states\n+        kv_c, k_pe = self.kv_a_proj_with_mqa(hidden_states)[0].split(\n+            [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+        kv_c_normed = self.kv_a_layernorm(kv_c.contiguous())\n+        return self.mla_attn(hidden_states_or_q_c, kv_c_normed, k_pe, kv_cache,\n+                             attn_metadata)\n+\n+\n class DeepseekV3DecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: PretrainedConfig,\n         prefix: str,\n+        model_config: ModelConfig,\n         cache_config: Optional[CacheConfig] = None,\n         quant_config: Optional[QuantizationConfig] = None,\n     ) -> None:\n@@ -351,7 +495,11 @@ class DeepseekV3DecoderLayer(nn.Module):\n         # DecoderLayers are created with `make_layers` which passes the prefix\n         # with the layer's index.\n         layer_idx = int(prefix.split(sep='.')[-1])\n-        self.self_attn = DeepseekV3Attention(\n+        if model_config.use_mla:\n+            attn_cls = DeepseekV3MLAAttention\n+        else:\n+            attn_cls = DeepseekV3Attention\n+        self.self_attn = attn_cls(\n             config=config,\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n@@ -428,6 +576,7 @@ class DeepseekV3Model(nn.Module):\n         super().__init__()\n \n         config = vllm_config.model_config.hf_config\n+        model_config = vllm_config.model_config\n         cache_config = vllm_config.cache_config\n         quant_config = vllm_config.quant_config\n \n@@ -447,6 +596,7 @@ class DeepseekV3Model(nn.Module):\n             lambda prefix: DeepseekV3DecoderLayer(\n                 config,\n                 prefix,\n+                model_config=model_config,\n                 cache_config=cache_config,\n                 quant_config=quant_config,\n             ),\ndiff --git a/vllm/worker/cache_engine.py b/vllm/worker/cache_engine.py\nindex 08316ba74..c427b759b 100644\n--- a/vllm/worker/cache_engine.py\n+++ b/vllm/worker/cache_engine.py\n@@ -110,7 +110,9 @@ class CacheEngine:\n             parallel_config, LayerBlockType.attention)\n \n         key_cache_block = cache_config.block_size * num_heads * head_size\n-        value_cache_block = key_cache_block\n+        # For MLA there is no value cache, since the latent vector\n+        # is joint keys and values.\n+        value_cache_block = key_cache_block if not model_config.use_mla else 0\n         total = num_attention_layers * (key_cache_block + value_cache_block)\n         if cache_config.cache_dtype == \"auto\":\n             dtype = model_config.dtype", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\"\"\"Integration tests for FlexAttention backend vs default backend\"\"\"\n\nimport random\n\nimport numpy as np\nimport pytest\nimport torch\nfrom packaging import version\n\nfrom tests.v1.attention.utils import (BatchSpec, create_common_attn_metadata,\n                                      create_standard_kv_cache_spec,\n                                      create_vllm_config)\nfrom vllm.v1.attention.backends.flex_attention import (\n    FlexAttentionMetadataBuilder)\n\nfrom ..models.utils import check_embeddings_close, check_logprobs_close\n\nTORCH_VERSION = version.parse(torch.__version__)\nMINIMUM_TORCH_VERSION = version.parse(\"2.7.0\")\nDIRECT_BUILD_VERSION = version.parse(\"2.9.dev0\")\n\n\ndef set_seed(seed):\n    \"\"\"Set seeds for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() or TORCH_VERSION < MINIMUM_TORCH_VERSION,\n    reason=\"CUDA not available or PyTorch version < 2.7\",\n)\ndef test_flex_attention_vs_default_backend(vllm_runner, monkeypatch):\n    \"\"\"Test that FlexAttention produces the same outputs as the default backend.\n\n    This test compares the outputs from the FlexAttention backend with\n    the default backend, ensuring they are similar when using the same seed.\n    \"\"\"\n    model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n    seed = 42\n    max_tokens = 24\n    num_logprobs = 5\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n    ]\n\n    # Run with flex attention\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLEX_ATTENTION\")\n\n        set_seed(seed)\n        with vllm_runner(model_name,\n                         runner=\"generate\",\n                         tensor_parallel_size=1,\n                         num_gpu_blocks_override=128,\n                         enforce_eager=True) as llm_flex:\n            output_flex = llm_flex.generate_greedy_logprobs(\n                prompts, max_tokens, num_logprobs)\n\n    # Run with default backend\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        set_seed(seed)\n        with vllm_runner(model_name,\n                         runner=\"generate\",\n                         tensor_parallel_size=1,\n                         num_gpu_blocks_override=128,\n                         enforce_eager=True,\n                         gpu_memory_utilization=0.85) as llm_default:\n            output_default = llm_default.generate_greedy_logprobs(\n                prompts, max_tokens, num_logprobs)\n\n    check_logprobs_close(\n        outputs_0_lst=output_flex,\n        outputs_1_lst=output_default,\n        name_0=\"flex\",\n        name_1=\"default\",\n    )\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() or TORCH_VERSION < MINIMUM_TORCH_VERSION,\n    reason=\"CUDA not available or PyTorch version < 2.7\",\n)\ndef test_encoder_flex_attention_vs_default_backend(vllm_runner, monkeypatch):\n    \"\"\"Test that FlexAttention produces the same outputs as the default backend.\n\n    This test compares the outputs from the FlexAttention backend with\n    the default backend for encoder models.\n    \"\"\"\n    model_name = \"BAAI/bge-base-en-v1.5\"\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n    ]\n\n    # Run with flex attention\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLEX_ATTENTION\")\n        with vllm_runner(model_name,\n                         runner=\"pooling\",\n                         dtype=torch.bfloat16,\n                         tensor_parallel_size=1,\n                         max_model_len=100,\n                         enforce_eager=True) as llm_flex:\n            flex_outputs = llm_flex.embed(prompts)\n\n    # Run with default backend\n    with monkeypatch.context() as m:\n        m.setenv(\"VLLM_USE_V1\", \"1\")\n        with vllm_runner(model_name,\n                         runner=\"pooling\",\n                         dtype=torch.bfloat16,\n                         tensor_parallel_size=1,\n                         max_model_len=100,\n                         enforce_eager=True) as llm_default:\n            default_outputs = llm_default.embed(prompts)\n\n    check_embeddings_close(\n        embeddings_0_lst=flex_outputs,\n        embeddings_1_lst=default_outputs,\n        name_0=\"flex\",\n        name_1=\"default\",\n        tol=1e-2,\n    )\n\n\n@pytest.mark.skipif(\n    not torch.cuda.is_available() or TORCH_VERSION < DIRECT_BUILD_VERSION,\n    reason=\"CUDA not available or PyTorch version < 2.7\",\n)\ndef test_block_mask_direct_vs_slow_path():\n    \"\"\"Test that direct path block mask is a superset of slow path.\n\n    The direct path may include extra blocks for performance (over-estimation),\n    but must include all blocks that the slow path determines are necessary.\n    \"\"\"\n    device = torch.device(\"cuda\")\n\n    vllm_config = create_vllm_config(model_name=\"meta-llama/Meta-Llama-3-8B\",\n                                     block_size=16,\n                                     max_model_len=1024)\n    kv_cache_spec = create_standard_kv_cache_spec(vllm_config)\n\n    # Use a mixed batch that will create groups spanning multiple sequences\n    batch_spec = BatchSpec(seq_lens=[35, 64, 128, 256],\n                           query_lens=[33, 5, 32, 64],\n                           name=\"test_mixed_batch\")\n\n    common_attn_metadata = create_common_attn_metadata(\n        batch_spec, vllm_config.cache_config.block_size, device)\n\n    builder = FlexAttentionMetadataBuilder(kv_cache_spec, [], vllm_config,\n                                           device)\n\n    metadata_direct = builder.build(common_prefix_len=0,\n                                    common_attn_metadata=common_attn_metadata)\n    builder.direct_build = False\n    metadata_slow = builder.build(common_prefix_len=0,\n                                  common_attn_metadata=common_attn_metadata)\n\n    assert metadata_direct.block_mask is not None\n    assert metadata_slow.block_mask is not None\n\n    # Extract block indices for comparison, B, H are the same\n    direct_indices = metadata_direct.block_mask.kv_indices[0, 0]\n    slow_indices = metadata_slow.block_mask.kv_indices[0, 0]\n    direct_num = metadata_direct.block_mask.kv_num_blocks[0, 0]\n    slow_num = metadata_slow.block_mask.kv_num_blocks[0, 0]\n\n    # main test: every block needed by slow path must be in direct path\n    num_groups = direct_num.shape[0]\n    all_contained = True\n    missing_details = []\n\n    for group_idx in range(num_groups):\n        direct_blocks = set(\n            direct_indices[group_idx, :direct_num[group_idx]].tolist())\n        slow_blocks = set(\n            slow_indices[group_idx, :slow_num[group_idx]].tolist())\n\n        missing_blocks = slow_blocks - direct_blocks\n        if missing_blocks:\n            all_contained = False\n            missing_details.append(\n                f\"Group {group_idx}: missing {sorted(missing_blocks)}\")\n\n    assert all_contained, (\n        \"Direct path is missing blocks required by slow path:\\n\" +\n        \"\\n\".join(missing_details))\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/3e1c76cf_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/baeded25_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 176, "symbols_collected": 3578, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.triton_mla", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_decode_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 175, "symbols_collected": 3558, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.mla.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.triton_mla", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_decode_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-bfdb1ba", "created_at": "2024-03-22T20:44:12+00:00", "base_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019", "head_commit": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56", "patch": "diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 442173939..082034083 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -1,13 +1,17 @@\n import pytest\n \n from transformers import AutoTokenizer\n+from typing import List, Dict\n \n+from vllm.sequence import Sequence, Logprob, SamplingParams, SequenceGroup\n+from vllm.transformers_utils.tokenizer_group import get_tokenizer_group\n from vllm.transformers_utils.tokenizer import detokenize_incrementally\n+from vllm.transformers_utils.detokenizer import Detokenizer\n \n TRUTH = [\n-    \"Hello here, this is a simple test\",  # noqa: E501\n-    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",  # noqa: E501\n-    \"\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5\"  # noqa: E501\n+    \"Hello here, this is a simple test\",\n+    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",  # noqa\n+    \"\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5\"\n ]\n TOKENIZERS = [\n     \"facebook/opt-125m\",\n@@ -24,12 +28,12 @@ TOKENIZERS = [\n \n \n def _run_incremental_decode(tokenizer, all_input_ids,\n-                            skip_special_tokens: bool):\n+                            skip_special_tokens: bool, starting_index: int):\n     decoded_text = \"\"\n     offset = 0\n     token_offset = 0\n     prev_tokens = None\n-    for i in range(len(all_input_ids)):\n+    for i in range(starting_index, len(all_input_ids)):\n         new_tokens, text, offset, token_offset = detokenize_incrementally(\n             tokenizer,\n             all_input_ids[:i + 1],\n@@ -46,17 +50,152 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n \n \n @pytest.mark.parametrize(\"truth\", TRUTH)\n+@pytest.mark.parametrize(\"with_prompt\", [True, False])\n @pytest.mark.parametrize(\"tokenizer_id\", TOKENIZERS)\n @pytest.mark.parametrize(\"skip_special_tokens\", (True, False))\n-def test_decode_streaming(tokenizer_id, truth, skip_special_tokens):\n+def test_decode_streaming(tokenizer_id, truth, with_prompt,\n+                          skip_special_tokens):\n     tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n-    all_input_ids = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n+    if with_prompt:\n+        truth_tokens = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n+        prompt_input_ids = truth_tokens[:len(truth) // 2]\n+        generated_input_ids = truth_tokens[len(truth) // 2:]\n+        all_input_ids = prompt_input_ids + generated_input_ids\n+        starting_index = len(prompt_input_ids)\n+        prompt = tokenizer.decode(prompt_input_ids,\n+                                  skip_special_tokens=skip_special_tokens)\n+        generated = truth[len(prompt):]\n+    else:\n+        generated = truth\n+        starting_index = 0\n+        all_input_ids = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n     if skip_special_tokens:\n-        all_input_ids = ([tokenizer.bos_token_id]\n-                         if tokenizer.bos_token_id is not None else\n-                         []) + all_input_ids + [tokenizer.eos_token_id]\n+        if tokenizer.bos_token_id is not None:\n+            all_input_ids = [tokenizer.bos_token_id] + all_input_ids\n+            starting_index += 1\n+        all_input_ids = all_input_ids + [tokenizer.eos_token_id]\n \n     decoded_text = _run_incremental_decode(\n-        tokenizer, all_input_ids, skip_special_tokens=skip_special_tokens)\n+        tokenizer,\n+        all_input_ids,\n+        skip_special_tokens=skip_special_tokens,\n+        starting_index=starting_index)\n \n-    assert decoded_text == truth\n+    assert decoded_text == generated\n+\n+\n+@pytest.fixture\n+def detokenizer(tokenizer_name: str) -> Detokenizer:\n+    init_kwargs = dict(\n+        tokenizer_id=tokenizer_name,\n+        enable_lora=False,\n+        max_num_seqs=100,\n+        max_input_length=None,\n+        tokenizer_mode=\"auto\",\n+        trust_remote_code=False,\n+        revision=None,\n+    )\n+\n+    tokenizer_group = get_tokenizer_group(\n+        None,\n+        **init_kwargs,\n+    )\n+\n+    return Detokenizer(tokenizer_group)\n+\n+\n+@pytest.fixture(name=\"complete_sequence_token_ids\")\n+def create_complete_sequence_token_ids(complete_sequence: str,\n+                                       tokenizer_name: str) -> List[int]:\n+    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n+    complete_sequence_token_ids = tokenizer(complete_sequence)[\"input_ids\"]\n+    return complete_sequence_token_ids\n+\n+\n+def create_sequence(prompt_token_ids=None):\n+    prompt_token_ids = prompt_token_ids or [1]\n+    return Sequence(\n+        seq_id=0,\n+        prompt=\"<s>\",\n+        prompt_token_ids=prompt_token_ids,\n+        block_size=16,\n+    )\n+\n+\n+def create_dummy_logprobs(\n+        complete_sequence_token_ids: List[int]) -> List[Dict[int, Logprob]]:\n+    return [{\n+        token_id: Logprob(logprob=0.0),\n+        token_id + 1: Logprob(logprob=0.1)\n+    } for token_id in complete_sequence_token_ids]\n+\n+\n+@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n+@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n+@pytest.mark.parametrize(\"skip_special_tokens\", [True, False])\n+def test_decode_sequence_logprobs(complete_sequence: str,\n+                                  complete_sequence_token_ids: List[int],\n+                                  detokenizer: Detokenizer,\n+                                  skip_special_tokens: bool):\n+    \"\"\"Verify Detokenizer decodes logprobs correctly.\"\"\"\n+    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n+                                     logprobs=2)\n+\n+    # Run sequentially.\n+    seq = create_sequence()\n+    dummy_logprobs = create_dummy_logprobs(complete_sequence_token_ids)\n+    sequential_logprobs_text_chosen_token = []\n+    sequential_logprobs_text_other_token = []\n+    for new_token, logprobs in zip(complete_sequence_token_ids,\n+                                   dummy_logprobs):\n+        seq.append_token_id(new_token, logprobs)\n+        detokenizer.decode_sequence_inplace(seq, sampling_params)\n+        sequential_logprobs_text_chosen_token.append(\n+            seq.output_logprobs[-1][new_token].decoded_token)\n+        sequential_logprobs_text_other_token.append(\n+            seq.output_logprobs[-1][new_token + 1].decoded_token)\n+    sequential_result = seq.output_text\n+\n+    assert sequential_result == \"\".join(sequential_logprobs_text_chosen_token)\n+    assert sequential_result != \"\".join(sequential_logprobs_text_other_token)\n+\n+    if skip_special_tokens:\n+        # Text for logprobs for the chosen token should be the same as the\n+        # generated text. Note that this will only be true if we skip\n+        # special tokens.\n+        assert sequential_result == complete_sequence\n+\n+\n+@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n+@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n+@pytest.mark.parametrize(\"skip_special_tokens\", [True])\n+def test_decode_prompt_logprobs(complete_sequence: str,\n+                                complete_sequence_token_ids: List[int],\n+                                detokenizer: Detokenizer,\n+                                skip_special_tokens: bool):\n+    \"\"\"Verify Detokenizer decodes prompt logprobs correctly.\"\"\"\n+    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n+                                     prompt_logprobs=1)\n+\n+    # Run sequentially.\n+    seq = create_sequence(complete_sequence_token_ids)\n+    seq_group = SequenceGroup(request_id=\"1\",\n+                              seqs=[seq],\n+                              sampling_params=sampling_params,\n+                              arrival_time=0.0)\n+    dummy_logprobs = create_dummy_logprobs(complete_sequence_token_ids)\n+    detokenizer.decode_prompt_logprobs_inplace(seq_group, dummy_logprobs)\n+    decoded_prompt_logprobs = dummy_logprobs\n+\n+    if skip_special_tokens:\n+        # Text for logprobs for the chosen token should be the same as the\n+        # prompt text. Note that this will only be true if we skip\n+        # special tokens.\n+        assert complete_sequence == \"\".join([\n+            logprobs[token_id].decoded_token for token_id, logprobs in zip(\n+                complete_sequence_token_ids, decoded_prompt_logprobs)\n+        ])\n+        assert complete_sequence != \"\".join([\n+            logprobs[token_id + 1].decoded_token for token_id, logprobs in zip(\n+                complete_sequence_token_ids, decoded_prompt_logprobs)\n+        ])\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 724782841..283b5d9ac 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1,5 +1,5 @@\n import time\n-from typing import Dict, Iterable, List, Optional, Tuple, Type, Union\n+from typing import Iterable, List, Optional, Tuple, Type, Union\n \n from transformers import PreTrainedTokenizer\n \n@@ -15,11 +15,11 @@ from vllm.engine.ray_utils import initialize_ray_cluster\n from vllm.logger import init_logger\n from vllm.outputs import RequestOutput\n from vllm.sampling_params import SamplingParams\n-from vllm.sequence import (Logprob, SamplerOutput, Sequence, SequenceGroup,\n+from vllm.sequence import (SamplerOutput, Sequence, SequenceGroup,\n                            SequenceGroupOutput, SequenceOutput, SequenceStatus)\n-from vllm.transformers_utils.tokenizer import detokenize_incrementally\n from vllm.transformers_utils.tokenizer_group import (BaseTokenizerGroup,\n                                                      get_tokenizer_group)\n+from vllm.transformers_utils.detokenizer import Detokenizer\n from vllm.utils import Counter\n \n logger = init_logger(__name__)\n@@ -97,6 +97,7 @@ class LLMEngine:\n         self._verify_args()\n \n         self._init_tokenizer()\n+        self.detokenizer = Detokenizer(self.tokenizer)\n         self.seq_counter = Counter()\n \n         self.model_executor = executor_class(model_config, cache_config,\n@@ -153,7 +154,7 @@ class LLMEngine:\n         raise RuntimeError(\"LLMEngine should not be pickled!\")\n \n     def get_tokenizer(self) -> \"PreTrainedTokenizer\":\n-        return self.tokenizer.get_lora_tokenizer()\n+        return self.tokenizer.get_lora_tokenizer(None)\n \n     def get_tokenizer_for_seq(self,\n                               sequence: Sequence) -> \"PreTrainedTokenizer\":\n@@ -370,13 +371,8 @@ class LLMEngine:\n         # Process prompt logprobs\n         prompt_logprobs = outputs.prompt_logprobs\n         if prompt_logprobs is not None:\n-            # We can pick any sequence for the prompt.\n-            seq = next(iter(seq_group.seqs_dict.values()))\n-            all_token_ids = seq.get_token_ids()\n-            for i, prompt_logprobs_for_token in enumerate(prompt_logprobs):\n-                self._decode_logprobs(seq, seq_group.sampling_params,\n-                                      prompt_logprobs_for_token,\n-                                      all_token_ids[:i])\n+            self.detokenizer.decode_prompt_logprobs_inplace(\n+                seq_group, prompt_logprobs)\n             seq_group.prompt_logprobs = prompt_logprobs\n \n         # Process samples\n@@ -420,7 +416,8 @@ class LLMEngine:\n             child_seqs.append((parent, parent))\n \n         for seq, _ in child_seqs:\n-            self._decode_sequence(seq, seq_group.sampling_params)\n+            self.detokenizer.decode_sequence_inplace(seq,\n+                                                     seq_group.sampling_params)\n             self._check_stop(seq, seq_group.sampling_params)\n \n         # Non-beam search case\n@@ -713,51 +710,6 @@ class LLMEngine:\n             time_e2e_requests=time_e2e_requests,\n         )\n \n-    def _decode_logprobs(self, seq: Sequence, prms: SamplingParams,\n-                         logprobs: Dict[int, Logprob],\n-                         all_input_ids: List[int]) -> None:\n-        if not logprobs:\n-            return\n-        for token_id, sample_logprob in logprobs.items():\n-            if (sample_logprob.decoded_token is None and token_id != -1):\n-                all_input_ids_with_logprob = all_input_ids[:-1] + [token_id]\n-                (_, new_text, prefix_offset,\n-                 read_offset) = detokenize_incrementally(\n-                     self.get_tokenizer_for_seq(seq),\n-                     all_input_ids=all_input_ids_with_logprob,\n-                     prev_tokens=seq.tokens,\n-                     prefix_offset=seq.prefix_offset,\n-                     read_offset=seq.read_offset,\n-                     skip_special_tokens=prms.skip_special_tokens,\n-                     spaces_between_special_tokens=prms.\n-                     spaces_between_special_tokens,\n-                 )\n-                sample_logprob.decoded_token = new_text\n-\n-    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:\n-        \"\"\"Decodes the new token for a sequence.\"\"\"\n-        all_input_ids = seq.get_token_ids()\n-        self._decode_logprobs(seq, prms, seq.output_logprobs[-1],\n-                              all_input_ids)\n-\n-        (new_tokens, new_output_text, prefix_offset,\n-         read_offset) = detokenize_incrementally(\n-             self.get_tokenizer_for_seq(seq),\n-             all_input_ids=all_input_ids,\n-             prev_tokens=seq.tokens,\n-             prefix_offset=seq.prefix_offset,\n-             read_offset=seq.read_offset,\n-             skip_special_tokens=prms.skip_special_tokens,\n-             spaces_between_special_tokens=prms.spaces_between_special_tokens,\n-         )\n-        if seq.tokens is None:\n-            seq.tokens = new_tokens\n-        else:\n-            seq.tokens.extend(new_tokens)\n-        seq.prefix_offset = prefix_offset\n-        seq.read_offset = read_offset\n-        seq.output_text += new_output_text\n-\n     def _check_stop(self, seq: Sequence,\n                     sampling_params: SamplingParams) -> None:\n         \"\"\"Stop the finished sequences.\"\"\"\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nnew file mode 100644\nindex 000000000..1f322b367\n--- /dev/null\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -0,0 +1,155 @@\n+from typing import List, Dict, Optional\n+from transformers import PreTrainedTokenizer\n+from vllm.sequence import Sequence, Logprob, SequenceGroup, SamplingParams\n+from vllm.transformers_utils.tokenizer import (detokenize_incrementally,\n+                                               convert_prompt_ids_to_tokens)\n+from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n+    BaseTokenizerGroup)\n+\n+# Used eg. for marking rejected tokens in spec decoding.\n+INVALID_TOKEN_ID = -1\n+\n+\n+class Detokenizer:\n+    \"\"\"Provides methods to decode the output of a model into text.\"\"\"\n+\n+    def __init__(self, tokenizer_group: BaseTokenizerGroup):\n+        self.tokenizer_group = tokenizer_group\n+\n+    def get_tokenizer_for_seq(self,\n+                              sequence: Sequence) -> \"PreTrainedTokenizer\":\n+        \"\"\"Returns the HF tokenizer to use for a given sequence.\"\"\"\n+        return self.tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n+\n+    def decode_prompt_logprobs_inplace(\n+            self, seq_group: SequenceGroup,\n+            prompt_logprobs: List[Optional[Dict[int, Logprob]]]) -> None:\n+        \"\"\"Decodes the logprobs for the prompt of a sequence group.\n+\n+        Args:\n+            seq_group: The sequence group to decode.\n+            prompt_logprobs: The logprobs to decode.\n+        \n+        Returns:\n+            The prompt logprobs with the decoded tokens.\n+        \"\"\"\n+        prms = seq_group.sampling_params\n+        # We can pick any sequence for the prompt.\n+        seq = next(iter(seq_group.seqs_dict.values()))\n+        # Only prompt, without the generated token.\n+        all_token_ids = seq.get_token_ids()\n+        prompt_token_ids = all_token_ids[:-1]\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+        prefix_offset = 0\n+        read_offset = 0\n+        next_iter_prefix_offset = 0\n+        next_iter_read_offset = 0\n+        next_iter_tokens = []\n+        prev_tokens = None\n+\n+        for token_position, prompt_logprobs_for_token in enumerate(\n+                prompt_logprobs):\n+            if not prompt_logprobs_for_token:\n+                continue\n+            for token_id, sample_logprob in prompt_logprobs_for_token.items():\n+                if (sample_logprob.decoded_token is None\n+                        and token_id != INVALID_TOKEN_ID):\n+                    prompt_token_ids_with_token = (\n+                        prompt_token_ids[:token_position] + [token_id])\n+                    (new_tokens, new_text, new_prefix_offset,\n+                     new_read_offset) = detokenize_incrementally(\n+                         tokenizer=tokenizer,\n+                         all_input_ids=prompt_token_ids_with_token,\n+                         prev_tokens=prev_tokens,\n+                         prefix_offset=prefix_offset,\n+                         read_offset=read_offset,\n+                         skip_special_tokens=prms.skip_special_tokens,\n+                         spaces_between_special_tokens=prms.\n+                         spaces_between_special_tokens,\n+                     )\n+\n+                    sample_logprob.decoded_token = new_text\n+\n+                    # Use the offsets & prev tokens corresponding to\n+                    # real tokens to ensure detokenization is consistent\n+                    # actual with prompt.\n+                    if token_id == all_token_ids[token_position]:\n+                        next_iter_prefix_offset = new_prefix_offset\n+                        next_iter_read_offset = new_read_offset\n+                        next_iter_tokens = new_tokens\n+\n+            # Advance to the next token position.\n+            prefix_offset = next_iter_prefix_offset\n+            read_offset = next_iter_read_offset\n+            if prev_tokens is None:\n+                prev_tokens = next_iter_tokens\n+            else:\n+                prev_tokens.extend(next_iter_tokens)\n+\n+    def decode_sequence_inplace(self, seq: Sequence,\n+                                prms: SamplingParams) -> None:\n+        \"\"\"Decodes the new token for a sequence. In-place operation.\n+\n+        Args:\n+            seq: The sequence to decode.\n+            prms: The sampling parameters used to generate the sequence.\n+        \"\"\"\n+        all_input_ids = seq.get_token_ids()\n+        token_id_generated_this_iteration = all_input_ids[-1]\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+\n+        # Convert prompt token IDs to tokens if necessary.\n+        # Do it here so that we don't have to repeat this\n+        # computation for each logprob.\n+        if seq.tokens is None:\n+            (seq.tokens, seq.prefix_offset,\n+             seq.read_offset) = convert_prompt_ids_to_tokens(\n+                 tokenizer=tokenizer,\n+                 prompt_ids=all_input_ids[:-1],\n+                 skip_special_tokens=prms.skip_special_tokens,\n+             )\n+\n+        (new_tokens, new_decoded_token_text, prefix_offset,\n+         read_offset) = detokenize_incrementally(\n+             tokenizer=tokenizer,\n+             all_input_ids=all_input_ids,\n+             prev_tokens=seq.tokens,\n+             prefix_offset=seq.prefix_offset,\n+             read_offset=seq.read_offset,\n+             skip_special_tokens=prms.skip_special_tokens,\n+             spaces_between_special_tokens=prms.spaces_between_special_tokens,\n+         )\n+\n+        # Decode logprobs\n+        logprobs = seq.output_logprobs[-1]\n+        if logprobs:\n+            previous_tokens = all_input_ids[:-1]\n+            for token_id, sample_logprob in logprobs.items():\n+                # If the token was generated this iteration,\n+                # use the provided text.\n+                if token_id == token_id_generated_this_iteration:\n+                    sample_logprob.decoded_token = new_decoded_token_text\n+                    continue\n+\n+                if (sample_logprob.decoded_token is None\n+                        and token_id != INVALID_TOKEN_ID):\n+                    all_input_ids_with_logprob = previous_tokens + [token_id]\n+                    (_, new_text, _, _) = detokenize_incrementally(\n+                        tokenizer=tokenizer,\n+                        all_input_ids=all_input_ids_with_logprob,\n+                        prev_tokens=seq.tokens,\n+                        prefix_offset=seq.prefix_offset,\n+                        read_offset=seq.read_offset,\n+                        skip_special_tokens=prms.skip_special_tokens,\n+                        spaces_between_special_tokens=prms.\n+                        spaces_between_special_tokens,\n+                    )\n+                    sample_logprob.decoded_token = new_text\n+\n+        if seq.tokens is None:\n+            seq.tokens = new_tokens\n+        else:\n+            seq.tokens.extend(new_tokens)\n+        seq.prefix_offset = prefix_offset\n+        seq.read_offset = read_offset\n+        seq.output_text += new_decoded_token_text\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19a8..eebdacc49 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -158,6 +158,34 @@ def _convert_tokens_to_string_with_added_encoders(\n         return \"\".join(sub_texts)\n \n \n+# 5 is an arbitrary value that should work for all\n+# tokenizers (bigger = more conservative).\n+INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET = 5\n+\n+\n+def convert_prompt_ids_to_tokens(\n+    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n+    prompt_ids: List[int],\n+    skip_special_tokens: bool = False,\n+) -> Tuple[List[str], int, int]:\n+    \"\"\"Converts the prompt ids to tokens and returns the tokens and offsets\n+    for incremental detokenization.\n+\n+    Note that not all tokens are converted to strings. Only the tokens that\n+    are necessary for incremental detokenization are converted to strings.\n+    \"\"\"\n+    # Offset a little more in case we have special tokens.\n+    prefix_offset = max(\n+        len(prompt_ids) - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET - 2, 0)\n+    # We do not need to convert the whole prompt to tokens.\n+    new_tokens = tokenizer.convert_ids_to_tokens(\n+        prompt_ids[prefix_offset:], skip_special_tokens=skip_special_tokens)\n+    prefix_offset = max(\n+        len(new_tokens) - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET, 0)\n+    read_offset = len(new_tokens)\n+    return new_tokens, prefix_offset, read_offset\n+\n+\n # Based on\n # https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\n # under Apache 2.0 license\n@@ -165,31 +193,53 @@ def detokenize_incrementally(\n     tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n     all_input_ids: List[int],\n     prev_tokens: Optional[List[str]],\n-    prefix_offset: int = 0,\n-    read_offset: int = 0,\n+    prefix_offset: int,\n+    read_offset: int,\n     skip_special_tokens: bool = False,\n     spaces_between_special_tokens: bool = True,\n ) -> Tuple[List[str], str, int, int]:\n+    \"\"\"Detokenizes the input ids incrementally and returns the new tokens\n+    and the new text.\n+\n+    If `prev_tokens` is None, this function will convert the input ids to\n+    tokens and return the tokens and the new text. Otherwise, it will return the\n+    new tokens and the new text.\n+\n+    This function will also return the new prefix offset and the new read\n+    offset to be used in the next iteration.\n+\n+    The offsets are necessary to defeat cleanup algorithms in the decode which\n+    decide to add a space or not depending on the surrounding ids.\n+\n+    Args:\n+        tokenizer: The tokenizer to use.\n+        all_input_ids: The input ids. The last id is the new token id.\n+        prev_tokens: The previous tokens. If None, this function will convert\n+            the input ids to tokens and return the tokens and the new text.\n+        prefix_offset: The prefix offset.\n+        read_offset: The read offset.\n+        skip_special_tokens: Whether to skip special tokens.\n+        spaces_between_special_tokens: Whether to add spaces between special\n+            tokens.\n+    \"\"\"\n     new_token_id = all_input_ids[-1]\n     # This is the first iteration for this sequence\n-    if prev_tokens is None:\n-        new_tokens = tokenizer.convert_ids_to_tokens(\n-            all_input_ids, skip_special_tokens=skip_special_tokens)\n-        output_tokens = new_tokens\n-        # 5 is an arbitrary value that should work for all\n-        # tokenizers (bigger = more conservative).\n-        # Subtract 1 extra to account for the generated token.\n-        prefix_offset = max(len(output_tokens) - 6, 0)\n-        # If the first new token is a special token, we can't skip 1 extra token\n-        if skip_special_tokens and new_token_id in tokenizer.all_special_ids:\n-            read_offset = max(len(output_tokens), 0)\n-        else:\n-            read_offset = max(len(output_tokens) - 1, 0)\n-    else:\n-        # Put new_token_id in a list so skip_special_tokens is respected\n-        new_tokens = tokenizer.convert_ids_to_tokens(\n-            [new_token_id], skip_special_tokens=skip_special_tokens)\n-        output_tokens = prev_tokens + new_tokens\n+    is_first_iter = prev_tokens is None\n+    if is_first_iter:\n+        (prev_tokens, prefix_offset,\n+         read_offset) = convert_prompt_ids_to_tokens(\n+             tokenizer,\n+             all_input_ids[:-1],\n+             skip_special_tokens=skip_special_tokens)\n+\n+    # Put new_token_id in a list so skip_special_tokens is respected\n+    new_tokens = tokenizer.convert_ids_to_tokens(\n+        [new_token_id], skip_special_tokens=skip_special_tokens)\n+    output_tokens = prev_tokens + new_tokens\n+\n+    # If this is the first iteration, return all tokens.\n+    if is_first_iter:\n+        new_tokens = output_tokens\n \n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\n\nfrom vllm.entrypoints.llm import LLM\nfrom vllm.sampling_params import SamplingParams\n\n\n@pytest.mark.skip_v1\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\ndef test_computed_prefix_blocks(model: str):\n    # This test checks if the engine generates completions both with and\n    # without optional detokenization, that detokenization includes text\n    # and no-detokenization doesn't, and that both completions have the same\n    # token_ids.\n    prompt = (\n        \"You are a helpful assistant. How do I build a car from cardboard and \"\n        \"paper clips? Is there an easy to follow video tutorial available \"\n        \"online for free?\")\n\n    llm = LLM(model=model)\n    sampling_params = SamplingParams(max_tokens=10,\n                                     temperature=0.0,\n                                     detokenize=False)\n\n    outputs_no_detokenization = llm.generate(prompt,\n                                             sampling_params)[0].outputs[0]\n    sampling_params.detokenize = True\n    outputs_with_detokenization = llm.generate(prompt,\n                                               sampling_params)[0].outputs[0]\n\n    assert outputs_no_detokenization.text == ''\n    assert outputs_with_detokenization.text != ''\n    assert outputs_no_detokenization.token_ids == \\\n        outputs_with_detokenization.token_ids\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Improve detokenization performance for prefill (#3469)\n\nCo-authored-by: MeloYang <meloyang05@gmail.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/cf2f084d_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/bfdb1ba5_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-cf2f084", "created_at": "2024-03-22T19:28:14+00:00", "base_commit": "f721096d48a7e3b98dffcb9b400bf58989cef64d", "head_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019", "patch": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 397101fa8..4a690e24e 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -1,5 +1,6 @@\n from typing import List\n import pytest  # noqa\n+import time\n \n from vllm.config import CacheConfig, SchedulerConfig\n from vllm.core.scheduler import Scheduler\n@@ -168,3 +169,36 @@ def test_scheduler_max_seqs():\n     # and one is prompting.\n     _, out = scheduler.schedule()\n     assert set(out.scheduled_seq_groups) == set([all_seq_groups[1]])\n+\n+\n+def test_scheduler_delay_factor():\n+\n+    block_size = 4\n+    scheduler_config = SchedulerConfig(100, 64, 16, delay_factor=0.5)\n+    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n+    cache_config.num_cpu_blocks = 8\n+    cache_config.num_gpu_blocks = 8\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+\n+    # schedule first prompt\n+    _, seq_group = create_dummy_prompt(\"0\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for a second before scheduling next prompt\n+    time.sleep(1)\n+    _, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+\n+    # second prompt should *not* be scheduled\n+    seq_group_meta, out = scheduler.schedule()\n+    assert not out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for more than 0.5 second and try again\n+    time.sleep(0.6)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '1'\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6dfb51586..2003563e4 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -517,6 +517,8 @@ class SchedulerConfig:\n             iteration.\n         max_model_len: Maximum length of a sequence (including prompt\n             and generated text).\n+        delay_factor: Apply a delay (of delay factor multiplied by previous\n+            prompt latency) before scheduling next prompt.\n     \"\"\"\n \n     def __init__(\n@@ -524,6 +526,7 @@ class SchedulerConfig:\n         max_num_batched_tokens: Optional[int],\n         max_num_seqs: int,\n         max_model_len: int,\n+        delay_factor: float = 0.0,\n     ) -> None:\n         if max_num_batched_tokens is not None:\n             self.max_num_batched_tokens = max_num_batched_tokens\n@@ -533,6 +536,7 @@ class SchedulerConfig:\n             self.max_num_batched_tokens = max(max_model_len, 2048)\n         self.max_num_seqs = max_num_seqs\n         self.max_model_len = max_model_len\n+        self.delay_factor = delay_factor\n         self._verify_args()\n \n     def _verify_args(self) -> None:\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex be55e8520..4bd0ef360 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -103,6 +103,13 @@ class Scheduler:\n         # Sequence groups in the SWAPPED state.\n         self.swapped: Deque[SequenceGroup] = deque()\n \n+        # Time at previous scheduling step\n+        self.prev_time = 0.0\n+        # Did we schedule a prompt at previous step?\n+        self.prev_prompt = False\n+        # Latency of the last prompt step\n+        self.last_prompt_latency = 0.0\n+\n     @property\n     def lora_enabled(self) -> bool:\n         return bool(self.lora_config)\n@@ -179,7 +186,7 @@ class Scheduler:\n             # are added to the back.\n             leftover_waiting_sequences = deque()\n             num_batched_tokens = 0\n-            while self.waiting:\n+            while self._passed_delay(now) and self.waiting:\n                 seq_group = self.waiting[0]\n                 waiting_seqs = seq_group.get_seqs(\n                     status=SequenceStatus.WAITING)\n@@ -246,6 +253,7 @@ class Scheduler:\n             self.waiting.extendleft(leftover_waiting_sequences)\n \n             if scheduled or ignored_seq_groups:\n+                self.prev_prompt = True\n                 scheduler_outputs = SchedulerOutputs(\n                     scheduled_seq_groups=scheduled,\n                     prompt_run=True,\n@@ -491,3 +499,19 @@ class Scheduler:\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         self.block_manager.mark_blocks_as_computed(seq_group)\n+\n+    def _passed_delay(self, now: float) -> bool:\n+        if self.prev_prompt:\n+            self.last_prompt_latency = now - self.prev_time\n+        self.prev_time, self.prev_prompt = now, False\n+        # Delay scheduling prompts to let waiting queue fill up\n+        if self.scheduler_config.delay_factor > 0 and self.waiting:\n+            earliest_arrival_time = min(\n+                [e.metrics.arrival_time for e in self.waiting])\n+            passed_delay = (\n+                (now - earliest_arrival_time) >\n+                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n+                or not self.running)\n+        else:\n+            passed_delay = True\n+        return passed_delay\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 94c80f428..2070686ea 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -51,6 +51,7 @@ class EngineArgs:\n     max_cpu_loras: Optional[int] = None\n     device: str = 'auto'\n     ray_workers_use_nsight: bool = False\n+    scheduler_delay_factor: float = 0.0\n \n     def __post_init__(self):\n         if self.tokenizer is None:\n@@ -305,6 +306,12 @@ class EngineArgs:\n                             default=EngineArgs.device,\n                             choices=[\"auto\", \"cuda\", \"neuron\"],\n                             help='Device type for vLLM execution.')\n+        parser.add_argument(\n+            '--scheduler-delay-factor',\n+            type=float,\n+            default=EngineArgs.scheduler_delay_factor,\n+            help='Apply a delay (of delay factor multiplied by previous'\n+            'prompt latency) before scheduling next prompt.')\n         return parser\n \n     @classmethod\n@@ -342,7 +349,8 @@ class EngineArgs:\n             ), self.ray_workers_use_nsight)\n         scheduler_config = SchedulerConfig(self.max_num_batched_tokens,\n                                            self.max_num_seqs,\n-                                           model_config.max_model_len)\n+                                           model_config.max_model_len,\n+                                           self.scheduler_delay_factor)\n         lora_config = LoRAConfig(\n             max_lora_rank=self.max_lora_rank,\n             max_loras=self.max_loras,", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport json\nfrom argparse import ArgumentError\nfrom contextlib import nullcontext\nfrom dataclasses import dataclass, field\nfrom typing import Annotated, Literal, Optional, Union\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport pytest\n\nfrom vllm.config import CompilationConfig, config\nfrom vllm.engine.arg_utils import (EngineArgs, contains_type, get_kwargs,\n                                   get_type, get_type_hints, is_not_builtin,\n                                   is_type, literal_to_kwargs, optional_type,\n                                   parse_type)\nfrom vllm.benchmarks.datasets import FlexibleArgumentParser\n\n\n@pytest.mark.parametrize((\"type\", \"value\", \"expected\"), [\n    (int, \"42\", 42),\n    (float, \"3.14\", 3.14),\n    (str, \"Hello World!\", \"Hello World!\"),\n    (json.loads, '{\"foo\":1,\"bar\":2}', {\n        \"foo\": 1,\n        \"bar\": 2\n    }),\n])\ndef test_parse_type(type, value, expected):\n    parse_type_func = parse_type(type)\n    assert parse_type_func(value) == expected\n\n\ndef test_optional_type():\n    optional_type_func = optional_type(int)\n    assert optional_type_func(\"None\") is None\n    assert optional_type_func(\"42\") == 42\n\n\n@pytest.mark.parametrize((\"type_hint\", \"type\", \"expected\"), [\n    (int, int, True),\n    (int, float, False),\n    (list[int], list, True),\n    (list[int], tuple, False),\n    (Literal[0, 1], Literal, True),\n])\ndef test_is_type(type_hint, type, expected):\n    assert is_type(type_hint, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"type\", \"expected\"), [\n    ({float, int}, int, True),\n    ({int, tuple[int]}, int, True),\n    ({int, tuple[int]}, float, False),\n    ({str, Literal[\"x\", \"y\"]}, Literal, True),\n])\ndef test_contains_type(type_hints, type, expected):\n    assert contains_type(type_hints, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"type\", \"expected\"), [\n    ({int, float}, int, int),\n    ({int, float}, str, None),\n    ({str, Literal[\"x\", \"y\"]}, Literal, Literal[\"x\", \"y\"]),\n])\ndef test_get_type(type_hints, type, expected):\n    assert get_type(type_hints, type) == expected\n\n\n@pytest.mark.parametrize((\"type_hints\", \"expected\"), [\n    ({Literal[1, 2]}, {\n        \"type\": int,\n        \"choices\": [1, 2]\n    }),\n    ({str, Literal[\"x\", \"y\"]}, {\n        \"type\": str,\n        \"metavar\": [\"x\", \"y\"]\n    }),\n    ({Literal[1, \"a\"]}, Exception),\n])\ndef test_literal_to_kwargs(type_hints, expected):\n    context = nullcontext()\n    if expected is Exception:\n        context = pytest.raises(expected)\n    with context:\n        assert literal_to_kwargs(type_hints) == expected\n\n\n@config\n@dataclass\nclass NestedConfig:\n    field: int = 1\n    \"\"\"field\"\"\"\n\n\n@config\n@dataclass\nclass DummyConfig:\n    regular_bool: bool = True\n    \"\"\"Regular bool with default True\"\"\"\n    optional_bool: Optional[bool] = None\n    \"\"\"Optional bool with default None\"\"\"\n    optional_literal: Optional[Literal[\"x\", \"y\"]] = None\n    \"\"\"Optional literal with default None\"\"\"\n    tuple_n: tuple[int, ...] = field(default_factory=lambda: (1, 2, 3))\n    \"\"\"Tuple with variable length\"\"\"\n    tuple_2: tuple[int, int] = field(default_factory=lambda: (1, 2))\n    \"\"\"Tuple with fixed length\"\"\"\n    list_n: list[int] = field(default_factory=lambda: [1, 2, 3])\n    \"\"\"List with variable length\"\"\"\n    list_literal: list[Literal[1, 2]] = field(default_factory=list)\n    \"\"\"List with literal choices\"\"\"\n    list_union: list[Union[str, type[object]]] = field(default_factory=list)\n    \"\"\"List with union type\"\"\"\n    literal_literal: Literal[Literal[1], Literal[2]] = 1\n    \"\"\"Literal of literals with default 1\"\"\"\n    json_tip: dict = field(default_factory=dict)\n    \"\"\"Dict which will be JSON in CLI\"\"\"\n    nested_config: NestedConfig = field(default_factory=NestedConfig)\n    \"\"\"Nested config\"\"\"\n\n\n@pytest.mark.parametrize((\"type_hint\", \"expected\"), [\n    (int, False),\n    (DummyConfig, True),\n])\ndef test_is_not_builtin(type_hint, expected):\n    assert is_not_builtin(type_hint) == expected\n\n\n@pytest.mark.parametrize(\n    (\"type_hint\", \"expected\"), [\n        (Annotated[int, \"annotation\"], {int}),\n        (Optional[int], {int, type(None)}),\n        (Annotated[Optional[int], \"annotation\"], {int, type(None)}),\n        (Optional[Annotated[int, \"annotation\"]], {int, type(None)}),\n    ],\n    ids=[\"Annotated\", \"Optional\", \"Annotated_Optional\", \"Optional_Annotated\"])\ndef test_get_type_hints(type_hint, expected):\n    assert get_type_hints(type_hint) == expected\n\n\ndef test_get_kwargs():\n    kwargs = get_kwargs(DummyConfig)\n    print(kwargs)\n\n    # bools should not have their type set\n    assert kwargs[\"regular_bool\"].get(\"type\") is None\n    assert kwargs[\"optional_bool\"].get(\"type\") is None\n    # optional literals should have None as a choice\n    assert kwargs[\"optional_literal\"][\"choices\"] == [\"x\", \"y\", \"None\"]\n    # tuples should have the correct nargs\n    assert kwargs[\"tuple_n\"][\"nargs\"] == \"+\"\n    assert kwargs[\"tuple_2\"][\"nargs\"] == 2\n    # lists should work\n    assert kwargs[\"list_n\"][\"type\"] is int\n    assert kwargs[\"list_n\"][\"nargs\"] == \"+\"\n    # lists with literals should have the correct choices\n    assert kwargs[\"list_literal\"][\"type\"] is int\n    assert kwargs[\"list_literal\"][\"nargs\"] == \"+\"\n    assert kwargs[\"list_literal\"][\"choices\"] == [1, 2]\n    # lists with unions should become str type.\n    # If not, we cannot know which type to use for parsing\n    assert kwargs[\"list_union\"][\"type\"] is str\n    # literals of literals should have merged choices\n    assert kwargs[\"literal_literal\"][\"choices\"] == [1, 2]\n    # dict should have json tip in help\n    json_tip = \"Should either be a valid JSON string or JSON keys\"\n    assert json_tip in kwargs[\"json_tip\"][\"help\"]\n    # nested config should should construct the nested config\n    assert kwargs[\"nested_config\"][\"type\"]('{\"field\": 2}') == NestedConfig(2)\n\n\n@pytest.mark.parametrize(\n    (\"arg\", \"expected\"),\n    [\n        (None, dict()),\n        ('{\"video\": {\"num_frames\": 123} }', {\n            \"video\": {\n                \"num_frames\": 123\n            }\n        }),\n        (\n            '{\"video\": {\"num_frames\": 123, \"fps\": 1.0, \"foo\": \"bar\"}, \"image\": {\"foo\": \"bar\"} }',  # noqa\n            {\n                \"video\": {\n                    \"num_frames\": 123,\n                    \"fps\": 1.0,\n                    \"foo\": \"bar\"\n                },\n                \"image\": {\n                    \"foo\": \"bar\"\n                }\n            }),\n    ])\ndef test_media_io_kwargs_parser(arg, expected):\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    if arg is None:\n        args = parser.parse_args([])\n    else:\n        args = parser.parse_args([\"--media-io-kwargs\", arg])\n\n    assert args.media_io_kwargs == expected\n\n\ndef test_compilation_config():\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n\n    # default value\n    args = parser.parse_args([])\n    assert args.compilation_config == CompilationConfig()\n\n    # set to O3\n    args = parser.parse_args([\"-O0\"])\n    assert args.compilation_config.level == 0\n\n    # set to O 3 (space)\n    args = parser.parse_args([\"-O\", \"1\"])\n    assert args.compilation_config.level == 1\n\n    # set to O 3 (equals)\n    args = parser.parse_args([\"-O=2\"])\n    assert args.compilation_config.level == 2\n\n    # set to O.level 3\n    args = parser.parse_args([\"-O.level\", \"3\"])\n    assert args.compilation_config.level == 3\n\n    # set to string form of a dict\n    args = parser.parse_args([\n        \"-O\",\n        '{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8], '\n        '\"use_inductor\": false}',\n    ])\n    assert (args.compilation_config.level == 3 and\n            args.compilation_config.cudagraph_capture_sizes == [1, 2, 4, 8]\n            and not args.compilation_config.use_inductor)\n\n    # set to string form of a dict\n    args = parser.parse_args([\n        \"--compilation-config=\"\n        '{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8], '\n        '\"use_inductor\": true}',\n    ])\n    assert (args.compilation_config.level == 3 and\n            args.compilation_config.cudagraph_capture_sizes == [1, 2, 4, 8]\n            and args.compilation_config.use_inductor)\n\n\ndef test_prefix_cache_default():\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    args = parser.parse_args([])\n\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert (not engine_args.enable_prefix_caching\n            ), \"prefix caching defaults to off.\"\n\n    # with flag to turn it on.\n    args = parser.parse_args([\"--enable-prefix-caching\"])\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert engine_args.enable_prefix_caching\n\n    # with disable flag to turn it off.\n    args = parser.parse_args([\"--no-enable-prefix-caching\"])\n    engine_args = EngineArgs.from_cli_args(args=args)\n    assert not engine_args.enable_prefix_caching\n\n\n# yapf: disable\n@pytest.mark.parametrize((\"arg\", \"expected\", \"option\"), [\n    (None, None, \"mm-processor-kwargs\"),\n    (\"{}\", {}, \"mm-processor-kwargs\"),\n    (\n        '{\"num_crops\": 4}',\n        {\n            \"num_crops\": 4\n        },\n        \"mm-processor-kwargs\"\n    ),\n    (\n        '{\"foo\": {\"bar\": \"baz\"}}',\n        {\n            \"foo\":\n            {\n                \"bar\": \"baz\"\n            }\n        },\n        \"mm-processor-kwargs\"\n    ),\n    (\n        '{\"cast_logits_dtype\":\"bfloat16\",\"sequence_parallel_norm\":true,\"sequence_parallel_norm_threshold\":2048}',\n        {\n            \"cast_logits_dtype\": \"bfloat16\",\n            \"sequence_parallel_norm\": True,\n            \"sequence_parallel_norm_threshold\": 2048,\n        },\n        \"override-neuron-config\"\n    ),\n])\n# yapf: enable\ndef test_composite_arg_parser(arg, expected, option):\n    parser = EngineArgs.add_cli_args(FlexibleArgumentParser())\n    if arg is None:\n        args = parser.parse_args([])\n    else:\n        args = parser.parse_args([f\"--{option}\", arg])\n    assert getattr(args, option.replace(\"-\", \"_\")) == expected\n\n\ndef test_human_readable_model_len():\n    # `exit_on_error` disabled to test invalid values below\n    parser = EngineArgs.add_cli_args(\n        FlexibleArgumentParser(exit_on_error=False))\n\n    args = parser.parse_args([])\n    assert args.max_model_len is None\n\n    args = parser.parse_args([\"--max-model-len\", \"1024\"])\n    assert args.max_model_len == 1024\n\n    # Lower\n    args = parser.parse_args([\"--max-model-len\", \"1m\"])\n    assert args.max_model_len == 1_000_000\n    args = parser.parse_args([\"--max-model-len\", \"10k\"])\n    assert args.max_model_len == 10_000\n\n    # Capital\n    args = parser.parse_args([\"--max-model-len\", \"3K\"])\n    assert args.max_model_len == 1024 * 3\n    args = parser.parse_args([\"--max-model-len\", \"10M\"])\n    assert args.max_model_len == 2**20 * 10\n\n    # Decimal values\n    args = parser.parse_args([\"--max-model-len\", \"10.2k\"])\n    assert args.max_model_len == 10200\n    # ..truncated to the nearest int\n    args = parser.parse_args([\"--max-model-len\", \"10.212345k\"])\n    assert args.max_model_len == 10212\n\n    # Invalid (do not allow decimals with binary multipliers)\n    for invalid in [\"1a\", \"pwd\", \"10.24\", \"1.23M\"]:\n        with pytest.raises(ArgumentError):\n            args = parser.parse_args([\"--max-model-len\", invalid])"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "Dynamic scheduler delay to improve ITL performance  (#3279)\n\nCo-authored-by: Jan van Lunteren <jvl@zurich.ibm.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/f721096d_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/cf2f084d_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-d7740ea", "created_at": "2024-05-08T15:42:28+00:00", "base_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87", "head_commit": "d7740ea4dcee4ab75d7d6eef723f33cae957b288", "patch": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 1f19d2053..e52e350d2 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -782,13 +782,14 @@ def _get_logprobs(\n         top_logprobs, top_token_ids = torch.topk(logprobs,\n                                                  largest_num_logprobs,\n                                                  dim=-1)\n-        top_logprobs = top_logprobs.cpu()\n-        top_token_ids = top_token_ids.cpu()\n     else:\n         top_logprobs, top_token_ids = None, None\n \n-    selected_logprobs = selected_logprobs.cpu()\n-    ranks = ranks.cpu()\n+    selected_logprobs = selected_logprobs.to('cpu')\n+    ranks = ranks.to('cpu')\n+    if top_logprobs is not None and top_token_ids is not None:\n+        top_logprobs = top_logprobs.to('cpu')\n+        top_token_ids = top_token_ids.to('cpu')\n \n     # Find prompt/sample logprobs.\n     prompt_logprobs_per_seq_group: List[Optional[PromptLogprobs]] = []\n@@ -828,37 +829,48 @@ def _get_prompt_logprob_if_needed(\n \n     # Find prompt logprobs\n     prompt_logprobs: Optional[PromptLogprobs] = None\n-    if (is_prompt and sampling_params.prompt_logprobs is not None):\n+    if is_prompt and sampling_params.prompt_logprobs is not None:\n         prompt_logprobs = []\n         num_logprobs = sampling_params.prompt_logprobs\n         next_prompt_tokens = _get_next_prompt_tokens(seq_group)\n-        for token_id in next_prompt_tokens:\n+        # Pre-select indexes and create a list. It is faster than calling .item\n+        # repetitively.\n+        selected_logprob_items = selected_logprobs[\n+            selected_logprobs_idx:selected_logprobs_idx +\n+            len(next_prompt_tokens)].tolist()\n+        rank_items = ranks[selected_logprobs_idx:selected_logprobs_idx +\n+                           len(next_prompt_tokens)].tolist()\n+\n+        for idx, token_id in enumerate(next_prompt_tokens):\n             # Calculate the prompt logprob of the real prompt tokens.\n-            # Use tuple here for performance (to use to_list()).\n             # {token_id: (logprob, rank_from_vocab)}\n             prompt_logprobs_dict: Dict[int, Tuple[float, int]] = {\n-                token_id: (selected_logprobs[selected_logprobs_idx].item(),\n-                           ranks[selected_logprobs_idx].item())\n+                token_id: (selected_logprob_items[idx], rank_items[idx])\n             }\n \n             # Add top K prompt logprobs along with its rank.\n             if num_logprobs > 0:\n-                prompt_logprobs_dict.update(\n-                    zip(\n-                        top_token_ids[top_logprob_idx, :num_logprobs].tolist(),\n-                        zip(\n-                            top_logprobs[\n-                                top_logprob_idx, :num_logprobs].tolist(),\n-                            # This is ranks. Since top_logprob is sorted,\n-                            # we can just use a range here.\n-                            range(1, num_logprobs + 1))))\n+                top_ids = top_token_ids[\n+                    top_logprob_idx, :num_logprobs].tolist()\n+                top_probs = top_logprobs[\n+                    top_logprob_idx, :num_logprobs].tolist()\n+                # Top K is already sorted by rank, so we can use 1 ~\n+                # num_logprobs + 1 for rank.\n+                top_ranks = range(1, num_logprobs + 1)\n+                prompt_logprobs_dict.update({\n+                    top_id: (top_prob, rank)\n+                    for top_id, top_prob, rank in zip(top_ids, top_probs,\n+                                                      top_ranks)\n+                })\n             prompt_logprobs.append({\n                 token_id: Logprob(*logprob_and_rank)\n                 for token_id, logprob_and_rank in prompt_logprobs_dict.items()\n             })\n             # + 1 to go to the next prompt token.\n             top_logprob_idx += 1\n-            selected_logprobs_idx += 1\n+\n+        # + len(next_prompt_tokens) to go to the next prompt.\n+        selected_logprobs_idx += len(next_prompt_tokens)\n     return prompt_logprobs, top_logprob_idx, selected_logprobs_idx\n \n \n@@ -874,47 +886,54 @@ def _get_sampled_logprob_if_needed(\n ):\n     \"\"\"Compute the sample logprob if needed.\"\"\"\n     seq_ids = seq_group.seq_ids\n-    num_logprobs = seq_group.sampling_params.logprobs\n-    if num_logprobs is None:\n-        num_logprobs = 0\n+    num_logprobs = seq_group.sampling_params.logprobs or 0\n     sampled_logprobs: SampleLogprobs = []\n     next_token_ids, parent_seq_ids = sample_result\n \n     if seq_group.do_sample:\n         assert len(next_token_ids) > 0\n-        for (next_token_id, parent_id) in zip(next_token_ids, parent_seq_ids):\n-            # Calculate the sample logprob of the real sampled tokens.\n-            # Use tuple here for performance (to use to_list()).\n-            # token_id: (logprob, rank_from_vocab)\n-            sampled_logprobs_dict: Dict[int, Tuple[float, int]] = {\n-                next_token_id:\n-                (selected_logprobs[selected_logprobs_idx].item(),\n-                 ranks[selected_logprobs_idx].item())\n+        # Pre-select items from tensor. tolist() is faster than repetitive\n+        # `.item()` calls.\n+        selected_logprob_items = selected_logprobs[\n+            selected_logprobs_idx:selected_logprobs_idx +\n+            len(next_token_ids)].tolist()\n+        rank_items = ranks[selected_logprobs_idx:selected_logprobs_idx +\n+                           len(next_token_ids)].tolist()\n+        for idx, (next_token_id,\n+                  parent_id) in enumerate(zip(next_token_ids, parent_seq_ids)):\n+            # Get the logprob of a sampled token.\n+            sampled_logprobs_dict = {\n+                next_token_id: (selected_logprob_items[idx], rank_items[idx])\n             }\n-            # +1 to go to the next sampled token. Note that\n-            # selected_logprobs can contain duplicates unlike top_logprobs\n-            # when beam search is enabled.\n-            selected_logprobs_idx += 1\n-\n-            # Second, add top K logprobs along with its rank.\n-            if num_logprobs >= 0:\n-                sampled_logprobs_dict.update(\n-                    zip(\n-                        top_token_ids[top_logprob_idx +\n-                                      parent_id, :num_logprobs].tolist(),\n-                        zip(\n-                            top_logprobs[top_logprob_idx +\n-                                         parent_id, :num_logprobs].tolist(),\n-                            # This is rank. Since top_logprob is sorted, we\n-                            # can just use a range here.\n-                            range(1, num_logprobs + 1))))\n+            # Get top K logprobs.\n+            if num_logprobs > 0:\n+                top_ids = top_token_ids[top_logprob_idx +\n+                                        parent_id, :num_logprobs].tolist()\n+                top_probs = top_logprobs[top_logprob_idx +\n+                                         parent_id, :num_logprobs].tolist()\n+                # Top K is already sorted by rank, so we can use 1 ~\n+                # num_logprobs + 1 for rank.\n+                top_ranks = range(1, num_logprobs + 1)\n+                sampled_logprobs_dict.update({\n+                    top_id: (top_prob, rank)\n+                    for top_id, top_prob, rank in zip(top_ids, top_probs,\n+                                                      top_ranks)\n+                })\n+\n             sampled_logprobs.append({\n                 token_id: Logprob(*logprob_and_rank)\n                 for token_id, logprob_and_rank in\n                 sampled_logprobs_dict.items()\n             })\n-        # There are len(seq_ids) number of sampled tokens for the current\n-        # sequence group in top_logprobs. Jump to the next seq_group.\n+\n+        # NOTE: This part of code is not intuitive. `selected_logprobs` include\n+        # logprobs for the current step, which has len(next_token_ids) tokens\n+        # per sequence group. `logprobs` includes logprobs from the previous\n+        # steps, which has len(seq_ids) tokens per sequence group.\n+\n+        # Iterate to the next sequence group in a batch.\n+        selected_logprobs_idx += len(next_token_ids)\n+        # Iterate to the next sequence group in a batch.\n         top_logprob_idx += len(seq_ids)\n     return sampled_logprobs, top_logprob_idx, selected_logprobs_idx", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\nfrom typing import Any, Optional\n\nimport pytest\nimport torch\nimport torch.nn.functional as F\n\nfrom vllm.platforms import current_platform\nfrom vllm.v1.sample.logits_processor import LogitsProcessors\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.sample.rejection_sampler import (PLACEHOLDER_TOKEN_ID,\n                                              RejectionSampler)\nfrom vllm.v1.spec_decode.metadata import SpecDecodeMetadata\n\nDEVICE = current_platform.device_type\n\n\n@pytest.fixture\ndef rejection_sampler():\n    return RejectionSampler()\n\n\ndef create_logits_tensor(output_token_ids: list[list[int]],\n                         vocab_size: int = 100) -> torch.Tensor:\n    \"\"\"Helper function to create logits tensor that\n       will produce desired token ids on argmax\"\"\"\n    token_ids = [tokens[:-1] for tokens in output_token_ids]\n    num_total_tokens = sum(len(tokens) for tokens in token_ids)\n    logits = torch.full((num_total_tokens, vocab_size), -100.0, device=DEVICE)\n    start_loc = 0\n    for tokens in token_ids:\n        for j, token_id in enumerate(tokens):\n            logits[start_loc + j, token_id] = 100.0\n        start_loc += len(tokens)\n    return logits\n\n\ndef create_sampling_metadata(\n    all_greedy: bool,\n    temperature: Optional[torch.Tensor] = None,\n    top_k: Optional[torch.Tensor] = None,\n    top_p: Optional[torch.Tensor] = None,\n    generators: Optional[dict[int, Any]] = None,\n) -> SamplingMetadata:\n    \"\"\"Create a v1 sampling metadata object with all_greedy set\n        to the given value. Either all greedy or all random sampling\n        is used.\n    \"\"\"\n    generators = generators or {}\n    if all_greedy:\n        temperature = None\n    else:\n        assert temperature is not None\n\n    return SamplingMetadata(\n        temperature=temperature,\n        all_greedy=all_greedy,\n        all_random=not all_greedy,\n        top_p=top_p,\n        top_k=top_k,\n        generators=generators,\n        max_num_logprobs=0,\n        no_penalties=False,\n        prompt_token_ids=None,\n        frequency_penalties=torch.tensor([]),\n        presence_penalties=torch.tensor([]),\n        repetition_penalties=torch.tensor([]),\n        output_token_ids=[],\n        allowed_token_ids_mask=None,\n        bad_words_token_ids={},\n        logitsprocs=LogitsProcessors(),\n    )\n\n\n########################### Tests for Greedy Sampling ###################\ndef test_perfect_match(rejection_sampler):\n    \"\"\"Test when output tokens perfectly match speculated tokens\"\"\"\n    spec_tokens = [[1, 2, 3]]\n    output_tokens = [[1, 2, 3, 4]]  # 4 is the bonus token\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                      device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor([[1, 2, 3, 4]],\n                            dtype=torch.int,\n                            device=logits.device)\n    assert torch.equal(output, expected)\n\n\ndef test_early_mismatch(rejection_sampler):\n    \"\"\"Test when there's an early mismatch in tokens\"\"\"\n    spec_tokens = [[1, 2, 3]]\n    output_tokens = [[1, 5, 3, 4]]  # Mismatch at position 1\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                      device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor(\n        [[1, 5, PLACEHOLDER_TOKEN_ID, PLACEHOLDER_TOKEN_ID]],\n        dtype=torch.int,\n        device=logits.device,\n    )\n    assert torch.equal(output, expected)\n\n\ndef test_multiple_sequences(rejection_sampler):\n    \"\"\"Test handling multiple sequences of speculated tokens\"\"\"\n    spec_tokens = [[1, 2], [3]]\n    output_tokens = [[1, 2, 5], [3,\n                                 4]]  # Two sequences with bonus tokens 5 and 4\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor(\n        [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor([[1, 2, 5], [3, 4, PLACEHOLDER_TOKEN_ID]],\n                            dtype=torch.int,\n                            device=logits.device)\n    assert torch.equal(output, expected)\n\n\ndef test_single_token_sequence(rejection_sampler):\n    \"\"\"Test handling sequences with single token\"\"\"\n    spec_tokens = [[1]]\n    output_tokens = [[1, 2]]  # Single token with bonus token 2\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                      device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor([[1, 2]], dtype=torch.int, device=logits.device)\n    assert torch.equal(output, expected)\n\n\ndef test_empty_sequence(rejection_sampler):\n    \"\"\"Test handling empty sequence of speculated tokens\"\"\"\n    spec_tokens: list[list[int]] = [[]]\n    output_tokens = [[5]]  # Just the bonus token\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                      device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor([[5]], dtype=torch.int, device=logits.device)\n    assert torch.equal(output, expected)\n\n\ndef test_multiple_mismatches(rejection_sampler):\n    \"\"\"Test handling multiple sequences with mismatches\"\"\"\n    spec_tokens = [[1, 2, 3], [4, 5, 6]]\n    output_tokens = [[1, 2, 7, 6], [4, 8, 6,\n                                    9]]  # Mismatches in both sequences\n\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor(\n        [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected = torch.tensor(\n        [[1, 2, 7, PLACEHOLDER_TOKEN_ID],\n         [4, 8, PLACEHOLDER_TOKEN_ID, PLACEHOLDER_TOKEN_ID]],\n        dtype=torch.int,\n        device=logits.device,\n    )\n    assert torch.equal(output, expected)\n\n\n@pytest.mark.parametrize(\n    \"spec_tokens,output_tokens,expected\",\n    [\n        ([[1, 2]], [[1, 2, 3]], [[1, 2, 3]]),  # Perfect match with bonus\n        ([[1]], [[2, 3]], [[2, PLACEHOLDER_TOKEN_ID]]),  # First mismatch\n        ([[1, 2], [3, 4]], [[1, 5, 6], [3, 4, 7]],\n         [[1, 5, PLACEHOLDER_TOKEN_ID], [3, 4, 7]]),  # Mixed matches\n    ])\ndef test_parametrized_cases(rejection_sampler, spec_tokens, output_tokens,\n                            expected):\n    \"\"\"Parametrized test for various matching scenarios\"\"\"\n    metadata = create_sampling_metadata(all_greedy=True)\n    logits = create_logits_tensor(output_tokens)\n    bonus_token_tensor = torch.tensor([tokens[-1] for tokens in output_tokens],\n                                      device=logits.device)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n                                                         device=logits.device)\n\n    output = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=None,\n        target_logits=logits,\n        bonus_token_ids=bonus_token_tensor,\n        sampling_metadata=metadata,\n    )\n    expected_tensor = torch.tensor(expected,\n                                   dtype=torch.int,\n                                   device=logits.device)\n    assert torch.equal(output, expected_tensor)\n\n\n########################### Tests for Random Sampling ###################\n@pytest.mark.parametrize(\"k\", [1, 3, 5])\n@pytest.mark.parametrize(\"vocab_size\", [1000])\n@pytest.mark.parametrize(\"batch_size\", [1, 4, 8])\n@pytest.mark.parametrize(\"frac_seeded\", [0.0, 0.5])\n@pytest.mark.parametrize(\"n_rep\", [20])\ndef test_deterministic_when_seeded(\n    rejection_sampler,\n    k: int,\n    vocab_size: int,\n    batch_size: int,\n    frac_seeded: float,\n    n_rep: int,\n):\n    num_tokens = batch_size * k\n    draft_probs = torch.rand(num_tokens,\n                             vocab_size,\n                             dtype=torch.float32,\n                             device=DEVICE)\n    draft_probs = F.softmax(draft_probs, dim=-1)\n    target_logits = torch.rand_like(draft_probs)\n    bonus_token_ids = torch.randint(low=0,\n                                    high=vocab_size,\n                                    size=(batch_size, 1),\n                                    dtype=torch.int64,\n                                    device=DEVICE)\n    draft_token_ids = torch.randint(low=0,\n                                    high=vocab_size,\n                                    size=(batch_size, k),\n                                    dtype=torch.int64,\n                                    device=DEVICE)\n\n    seeded_mask = torch.rand(batch_size, dtype=torch.float32) <= frac_seeded\n\n    results = []\n    for _ in range(n_rep):\n        seeded_seqs = {\n            i: torch.Generator(device=DEVICE).manual_seed(i)\n            for i in range(batch_size) if seeded_mask[i]\n        }\n\n        temperature = torch.ones(batch_size,\n                                 dtype=torch.float32,\n                                 device=DEVICE)\n        sampling_metadata = create_sampling_metadata(all_greedy=False,\n                                                     temperature=temperature,\n                                                     generators=seeded_seqs)\n        spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n            draft_token_ids.tolist(), device=DEVICE)\n        rep_result = rejection_sampler(\n            spec_decode_metadata,\n            draft_probs=draft_probs,\n            target_logits=target_logits,\n            bonus_token_ids=bonus_token_ids,\n            sampling_metadata=sampling_metadata,\n        )\n\n        results.append(rep_result)\n\n    for i in range(batch_size):\n        if seeded_mask[i]:\n            for j in range(1, n_rep):\n                assert torch.equal(results[j][i], results[0][i])\n\n\ndef test_rejection_sampling_approximates_target_distribution():\n    \"\"\"Verify rejection sampling approximates target distribution,\n    despite sampling from a potentially distinct draft distribution.\n\n    This is done by first creating a random target probability\n    distribution and a random draft probability distribution. We then\n    sample token ids from the rejection sampler using these draft\n    and target distributions. The samples are used to estimate\n    the output probability distribution, which we expect to approximate\n    the target distribution.\n\n    A basic distance metric is used to determine similarity between\n    distributions.\n\n    We expect that as we increase the number of samples,\n    the distance between the observed distribution and the target\n    distribution decreases. To measure this, we compare the distance\n    of the observed distribution against both the target distribution\n    and a uniform random distribution. We expect the distance between\n    the observed distribution and the target distribution to improve\n    much more than the distance improvement between the observed\n    distribution and the random distribution.\n    \"\"\"\n    torch.set_default_device(DEVICE)\n    vocab_size = 10\n    k = 2\n    num_reference_probs = 100\n\n    # Prepare draft, target, and reference probability distributions\n    draft_probs = F.softmax(torch.rand(vocab_size, dtype=torch.float32),\n                            dim=-1)\n    target_logits = torch.rand(vocab_size, dtype=torch.float32)\n    target_probs = F.softmax(target_logits, dim=-1)\n    reference_probs = F.softmax(\n        torch.rand(num_reference_probs, vocab_size, dtype=torch.float32),\n        dim=-1,\n    )\n\n    sample_sizes = [10, 100, 1_000, 10_000, 100_000]\n    distance_wrt_reference: list[float] = []\n    distance_wrt_target: list[float] = []\n\n    for num_samples in sample_sizes:\n        # Sample using rejection sampling.\n        rej_sample_probs = estimate_rejection_sampling_pdf(\n            draft_probs, target_logits, k, vocab_size, num_samples)\n        rej_sample_probs = rej_sample_probs.to(DEVICE)\n\n        # Average distance from reference probs.\n        reference_vs_rejsample_dist = torch.dist(\n            reference_probs,\n            rej_sample_probs).item() / reference_probs.shape[0]\n        target_vs_rejsample_dist = torch.dist(target_probs,\n                                              rej_sample_probs).item()\n\n        distance_wrt_reference.append(reference_vs_rejsample_dist)\n        distance_wrt_target.append(target_vs_rejsample_dist)\n\n        relative_change_in_distance_wrt_target = get_ratio_first_to_last(\n            distance_wrt_target)\n        relative_change_in_distance_wrt_reference = get_ratio_first_to_last(\n            distance_wrt_reference)\n\n        print(f\"{num_samples=} {target_vs_rejsample_dist=:.05f} \"\n              f\"{reference_vs_rejsample_dist=:.05f}\")\n        print(f\"{num_samples=} {relative_change_in_distance_wrt_target=:.02f} \"\n              f\"{relative_change_in_distance_wrt_reference=:.02f}\")\n\n    relative_change_in_distance_wrt_target = get_ratio_first_to_last(\n        distance_wrt_target)\n    relative_change_in_distance_wrt_reference = get_ratio_first_to_last(\n        distance_wrt_reference)\n\n    expected_improvement_multiplier = 20\n    assert (relative_change_in_distance_wrt_target\n            > relative_change_in_distance_wrt_reference *\n            expected_improvement_multiplier)\n\n\ndef get_ratio_first_to_last(elements: list[float]) -> float:\n    return elements[0] / elements[-1]\n\n\ndef estimate_rejection_sampling_pdf(\n    draft_probs: torch.Tensor,\n    target_logits: torch.Tensor,\n    k: int,\n    vocab_size: int,\n    num_samples: int,\n) -> torch.Tensor:\n    \"\"\"Estimate the probability distribution of the output tokens\n    using rejection sampling.\n\n    Args:\n        draft_probs: Draft probability distribution.\n        target_logits: Target logits.\n        num_samples: Number of samples to draw.\n\n    Returns:\n        Estimated probability distribution of the output tokens.\n    \"\"\"\n    rejection_sampler = RejectionSampler()\n    num_tokens = num_samples * k\n    # Repeat draft probs num_samples * k times.\n    draft_probs = draft_probs.reshape(1, 1,\n                                      vocab_size).repeat(num_samples, k, 1)\n\n    # Repeat target probs num_tokens times.\n    target_logits = target_logits.reshape(1, vocab_size).repeat(num_tokens, 1)\n\n    # Randomly sample draft token ids from draft probs.\n    draft_token_ids = torch.multinomial(draft_probs[:, 0, :],\n                                        num_samples=k,\n                                        replacement=True).reshape(\n                                            num_samples, k)\n    draft_probs = draft_probs.view(num_tokens, vocab_size)\n\n    # Bonus tokens not used but required.\n    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n                                  device=DEVICE).repeat(num_samples, 1)\n\n    temperature = torch.ones(num_samples, dtype=torch.float32, device=DEVICE)\n    sampling_metadata = create_sampling_metadata(all_greedy=False,\n                                                 temperature=temperature)\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n        draft_token_ids.tolist(), device=bonus_token_ids.device)\n    output_token_ids = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=draft_probs,\n        target_logits=target_logits,\n        bonus_token_ids=bonus_token_ids,\n        sampling_metadata=sampling_metadata,\n    )\n    output_token_ids = output_token_ids[:, :-1].flatten()\n\n    hist = torch.histogram(output_token_ids.to(dtype=torch.float,\n                                               device=\"cpu\"),\n                           bins=vocab_size,\n                           range=(0, vocab_size),\n                           density=True)\n\n    return hist.hist\n\n\ndef _test_masked_logits(\n    rejection_sampler,\n    batch_size: int,\n    num_draft_tokens: int,\n    vocab_size: int,\n    target_logits: torch.Tensor,\n    unmasked_indices: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n):\n    # Set up test parameters\n    num_tokens = batch_size * num_draft_tokens\n\n    # Create random draft probabilities.\n    draft_probs = torch.rand((num_tokens, vocab_size),\n                             dtype=torch.float32,\n                             device=DEVICE)\n    draft_probs = F.softmax(draft_probs, dim=-1)\n\n    # Randomly sample draft token ids from draft probs\n    draft_token_ids = torch.multinomial(draft_probs, num_samples=1)\n    draft_token_ids = draft_token_ids.reshape(batch_size, num_draft_tokens)\n    draft_token_ids = draft_token_ids.tolist()\n\n    # Bonus tokens not used but required\n    bonus_token_ids = torch.zeros((batch_size, 1),\n                                  dtype=torch.int64,\n                                  device=DEVICE)\n\n    # Create spec decode metadata\n    spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n        draft_token_ids,\n        device=DEVICE,\n    )\n\n    # Run rejection sampling\n    output_token_ids = rejection_sampler(\n        spec_decode_metadata,\n        draft_probs=draft_probs,\n        target_logits=target_logits,\n        bonus_token_ids=bonus_token_ids,\n        sampling_metadata=sampling_metadata,\n    )\n\n    # Remove bonus tokens and reshape\n    output_token_ids = output_token_ids[:, :-1].flatten().tolist()\n\n    # Check that all sampled tokens are within the unmasked indices.\n    for i in range(num_tokens):\n        token_id = output_token_ids[i]\n        if token_id == PLACEHOLDER_TOKEN_ID:\n            continue\n        assert token_id in unmasked_indices[i]\n\n\n@pytest.mark.parametrize(\"top_k\", [1, 5, 99])\ndef test_top_k(rejection_sampler, top_k):\n    \"\"\"Test rejection sampling with top-k sampling\"\"\"\n    vocab_size = 100\n    batch_size = 100\n    num_draft_tokens = 3\n    num_tokens = batch_size * num_draft_tokens\n\n    # Randomly create top-k indices.\n    top_k_indices = [\n        torch.randperm(vocab_size, device=DEVICE)[:top_k]\n        for _ in range(num_tokens)\n    ]\n    top_k_indices = torch.stack(top_k_indices)\n\n    # Create logits with the uniform distribution.\n    target_logits = torch.zeros((num_tokens, vocab_size), device=DEVICE)\n\n    # Increment the logits for top-k indices, a little bit more than the other\n    # ones. If the masking is effective, the non-topk indices will never be\n    # sampled despite the small difference in logits.\n    for i in range(num_tokens):\n        target_logits[i, top_k_indices[i]] += 0.1\n\n    # Create sampling metadata\n    temperature = torch.ones(batch_size, dtype=torch.float32, device=DEVICE)\n    sampling_metadata = create_sampling_metadata(\n        all_greedy=False,\n        temperature=temperature,\n        top_k=torch.tensor([top_k] * batch_size,\n                           device=DEVICE,\n                           dtype=torch.int64),\n    )\n\n    _test_masked_logits(\n        rejection_sampler,\n        batch_size=batch_size,\n        num_draft_tokens=num_draft_tokens,\n        vocab_size=vocab_size,\n        target_logits=target_logits,\n        unmasked_indices=top_k_indices,\n        sampling_metadata=sampling_metadata,\n    )\n\n\n@pytest.mark.parametrize(\"top_p\", [0.5, 0.9, 0.99])\ndef test_top_p(rejection_sampler, top_p):\n    \"\"\"Test rejection sampling with top-p sampling\"\"\"\n    vocab_size = 100\n    batch_size = 100\n    num_draft_tokens = 3\n    num_tokens = batch_size * num_draft_tokens\n\n    # Create logits with the uniform distribution.\n    target_logits = torch.randn((num_tokens, vocab_size), device=DEVICE)\n    temperature = torch.ones(batch_size, dtype=torch.float32, device=DEVICE)\n    rescaled_logits = target_logits / temperature\n\n    logits_sort, logits_idx = rescaled_logits.sort(dim=-1, descending=False)\n    probs_sort = logits_sort.softmax(dim=-1)\n    probs_sum = probs_sort.cumsum(dim=-1)\n    top_p_mask = probs_sum <= 1 - top_p\n    # at least one\n    top_p_mask[:, -1] = False\n\n    # Get the top-p indices.\n    top_p_indices = []\n    for i in range(num_tokens):\n        top_p_indices.append(logits_idx[i][~top_p_mask[i]].tolist())\n\n    # Create sampling metadata\n    sampling_metadata = create_sampling_metadata(\n        all_greedy=False,\n        temperature=temperature,\n        top_p=torch.tensor([top_p] * batch_size,\n                           device=DEVICE,\n                           dtype=torch.float32),\n    )\n\n    _test_masked_logits(\n        rejection_sampler,\n        batch_size=batch_size,\n        num_draft_tokens=num_draft_tokens,\n        vocab_size=vocab_size,\n        target_logits=target_logits,\n        unmasked_indices=top_p_indices,\n        sampling_metadata=sampling_metadata,\n    )\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Core] Optimize sampler get_logprobs (#4594)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/cc466a32_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/d7740ea4_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-e358053", "created_at": "2024-08-28T07:36:31+00:00", "base_commit": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a", "head_commit": "e3580537a41a46b0f3cd750b86b633c1857a8c90", "patch": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex 1211e6ba5..fc6f829c3 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -6,6 +6,7 @@ prefill requests are chunked.\n \n Run `pytest tests/models/test_chunked_prefill.py`.\n \"\"\"\n+from contextlib import nullcontext\n \n import pytest\n \n@@ -156,3 +157,68 @@ def test_models_with_fp8_kv_cache(\n         name_0=\"no_chunked_prefill\",\n         name_1=\"chunked_prefill\",\n     )\n+\n+\n+@pytest.mark.parametrize(\"max_tokens\", [16])\n+@pytest.mark.parametrize(\"enforce_eager\", [False])\n+@pytest.mark.parametrize(\"chunk_size\", [30, 32])\n+@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n+# NOTE: Increasing this in this suite will fail CI because we currently cannot\n+# reset distributed env properly. Use a value > 1 just when you test.\n+@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n+def test_with_prefix_caching(\n+    vllm_runner,\n+    max_tokens: int,\n+    enforce_eager: bool,\n+    chunk_size: int,\n+    use_v2_block_manager: bool,\n+    tensor_parallel_size: int,\n+) -> None:\n+    \"\"\"\n+    Checks exact match decode with and without prefix caching\n+    with chunked prefill enabled.\n+    \"\"\"\n+    model = \"meta-llama/Llama-2-7b-chat-hf\"\n+    # The common prompt has 142 tokens with Llama-2 tokenizer.\n+    common_prompt = \"You are a helpful AI assistant \" * 20\n+    unique_prompts = [\n+        \"Question\",  # Warmup\n+        \"Question\",  # Fully cached\n+        \"Another question\",  # Partial cached\n+    ]\n+    full_prompts = [f\"{common_prompt}\\n{p}\" for p in unique_prompts]\n+\n+    max_num_batched_tokens = max_num_seqs = chunk_size\n+    outputs = {}  # type: ignore\n+    check_result = True\n+    for enable in (True, False):\n+        with vllm_runner(\n+                model,\n+                dtype=\"half\",\n+                max_num_batched_tokens=max_num_batched_tokens,\n+                enable_chunked_prefill=True,\n+                enable_prefix_caching=enable,\n+                tensor_parallel_size=tensor_parallel_size,\n+                use_v2_block_manager=use_v2_block_manager,\n+                enforce_eager=enforce_eager,\n+                max_num_seqs=max_num_seqs,\n+        ) as vllm_model:\n+            # It should fail when prefix caching is enable and chunk\n+            # size is not a multiple of block size (16).\n+            should_fail = chunk_size % 16 != 0 and enable\n+            check_result &= not should_fail\n+            outputs[enable] = []\n+            # Send the request one-by-one to ensure the cache is populated.\n+            with pytest.raises(ValueError) if should_fail else nullcontext():\n+                for prompt in full_prompts:\n+                    outputs[enable] += vllm_model.generate_greedy([prompt],\n+                                                                  max_tokens)\n+\n+    # Check results only if we did not expect a failure.\n+    if check_result:\n+        check_outputs_equal(\n+            outputs_0_lst=outputs[False],\n+            outputs_1_lst=outputs[True],\n+            name_0=\"w/o prefix caching\",\n+            name_1=\"with prefix caching\",\n+        )\ndiff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex cd306b9e4..2ee9f2082 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -595,3 +595,43 @@ def test_sliding_window_multi_seq():\n \n     # assert all blocks are free now\n     assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n+\n+\n+def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n+    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n+    should only mark a chunk of blocks as computed instead of all blocks.\n+    \"\"\"\n+\n+    block_size = 4\n+    num_cpu_blocks = 0\n+    num_gpu_blocks = 16\n+    block_manager = BlockSpaceManagerV1(block_size,\n+                                        num_gpu_blocks,\n+                                        num_cpu_blocks,\n+                                        watermark=0,\n+                                        enable_caching=True)\n+\n+    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n+    prompt_length = block_size * num_gpu_blocks - 1\n+\n+    # Allocate (reserve) all blocks.\n+    _, seq_group = create_dummy_prompt(\"0\",\n+                                       prompt_length,\n+                                       block_size=block_size)\n+    block_manager.allocate(seq_group)\n+    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n+\n+    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n+    token_chunk_size = int(block_size * 2.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 2\n+\n+    # Actual computed tokens.\n+    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n+\n+    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n+    token_chunk_size = int(block_size * 4.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 7\ndiff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex 6d9c2f3eb..2f6ea632a 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -562,3 +562,42 @@ def test_chunked_prefill_max_seqs():\n     assert len(get_sequence_groups(out)) == max_seqs\n     assert not running[0].is_prefill()\n     assert not running[1].is_prefill()\n+\n+\n+def test_perfix_caching():\n+    \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n+    block_size = 4\n+    max_seqs = 10\n+    max_model_len = 80\n+    max_num_batched_tokens = 64\n+    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n+                                       max_seqs,\n+                                       max_model_len,\n+                                       enable_chunked_prefill=True)\n+    cache_config = CacheConfig(block_size,\n+                               1.0,\n+                               1,\n+                               \"auto\",\n+                               enable_prefix_caching=True)\n+    cache_config.num_cpu_blocks = 0\n+    cache_config.num_gpu_blocks = 32\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+    running: List[SequenceGroup] = []\n+\n+    # Add seq groups to scheduler.\n+    for i in range(2):\n+        _, seq_group = create_dummy_prompt(str(i),\n+                                           block_size=block_size,\n+                                           prompt_length=50)\n+        scheduler.add_seq_group(seq_group)\n+        running.append(seq_group)\n+\n+    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n+    assert set(get_sequence_groups(out)) == set(running)\n+    assert seq_group_meta[0].token_chunk_size == 50\n+    # Verify it is chunked. Note that although the budget is 64-50=14,\n+    # we only allocate full blocks for prefix caching, so only 4*(14//4)=12\n+    # tokens are allocated.\n+    assert seq_group_meta[1].token_chunk_size == 12\n+    assert out.num_prefill_groups == 2\n+    assert out.num_batched_tokens == 62\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 666723313..24ab9eb66 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -681,14 +681,20 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n             for block in block_table:\n                 block.last_accessed = access_time\n \n-    def compute_full_blocks_in_seq(self, seq: Sequence):\n+    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n         if seq.seq_id not in self.block_tables:\n             return\n-        max_full_block = seq.get_len() // self.block_size - 1\n+\n+        # When chunked prefill is enabled, the computed full blocks\n+        # should be calculated based on the number of computed tokens.\n+        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n+                               token_chunk_size)\n+        computed_full_blocks = max_computed_tokens // self.block_size\n+\n         block_table = self.block_tables[seq.seq_id]\n-        if max_full_block == -1:\n+        if computed_full_blocks == 0:\n             return\n-        for i in reversed(range(max_full_block)):\n+        for i in reversed(range(computed_full_blocks)):\n             if block_table[i].computed:\n                 break\n             block_table[i].computed = True\n@@ -718,10 +724,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n         return commonprefix([ids for ids in ids_list if ids != []])\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         if self.enable_caching:\n             for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq)\n+                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\n         if device == Device.GPU:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex 7d2db43cb..b06385b06 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -290,7 +290,8 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             self._last_access_blocks_tracker.update_last_access(\n                 seq.seq_id, now)\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         # If prefix caching is enabled, mark immutable blocks as computed\n         # right after they have been scheduled (for prefill). This assumes\n         # the scheduler is synchronous so blocks are actually computed when\ndiff --git a/vllm/core/embedding_model_block_manager.py b/vllm/core/embedding_model_block_manager.py\nindex f16f66e99..c47d7d8df 100644\n--- a/vllm/core/embedding_model_block_manager.py\n+++ b/vllm/core/embedding_model_block_manager.py\n@@ -80,7 +80,8 @@ class EmbeddingModelBlockSpaceManager(BlockSpaceManager):\n                                       seq_group: List[Sequence]) -> List[int]:\n         return []\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\ndiff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex becd0d2e7..96f8dd851 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -115,7 +115,8 @@ class BlockSpaceManager(ABC):\n         pass\n \n     @abstractmethod\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     @abstractmethod\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex fbc53afa3..51fde6e4e 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -1226,7 +1226,8 @@ class Scheduler:\n         # will crash the vLLM instance / will not retry.\n         for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n             self.block_manager.mark_blocks_as_computed(\n-                scheduled_seq_group.seq_group)\n+                scheduled_seq_group.seq_group,\n+                scheduled_seq_group.token_chunk_size)\n \n         self._seq_group_metadata_cache[self.next_cache_id].reset()\n \n@@ -1457,10 +1458,27 @@ class Scheduler:\n         for seq in seqs:\n             num_new_tokens += seq.get_num_new_tokens()\n         assert num_new_tokens > 0\n-        # Chunk if a running request cannot fit in.\n-        # If number of seq > 1, it means it is doing beam search in a\n-        # decode phase. Do not chunk in that case.\n+        # Chunk if a running request cannot fit in the given budget.\n+        # If number of seq > 1, it means it is doing beam search\n+        # in a decode phase. Do not chunk.\n         if enable_chunking and len(seqs) == 1:\n-            num_new_tokens = min(num_new_tokens,\n-                                 budget.remaining_token_budget())\n+            remaining_token_budget = budget.remaining_token_budget()\n+            if self.cache_config.enable_prefix_caching:\n+                # When prefix caching is enabled, we always allocate\n+                # the number of new tokens that is dividable by the block size\n+                # to avoid partial block matching.\n+                block_size = self.cache_config.block_size\n+                reminder = budget.token_budget % block_size\n+                if reminder != 0:\n+                    raise ValueError(\"When enabling chunked prefill and \"\n+                                     \"prefix caching, max_num_batched_tokens \"\n+                                     \"(chunk size) must be dividable by \"\n+                                     \"block size, but got chunk_size \"\n+                                     f\"({budget.token_budget}) % block_size \"\n+                                     f\"({block_size}) = {reminder}\")\n+                if remaining_token_budget < num_new_tokens:\n+                    num_new_tokens = (remaining_token_budget //\n+                                      block_size) * block_size\n+            else:\n+                num_new_tokens = min(num_new_tokens, remaining_token_budget)\n         return num_new_tokens\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4ea1..2b287a5d2 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -501,23 +501,48 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                             and self.sliding_window is None\n                             and inter_data.is_prompt)\n         inter_data.prefix_cache_hit = prefix_cache_hit\n-        if self.chunked_prefill_enabled and prefix_cache_hit:\n-            raise RuntimeError(\n-                \"chunked prefill cannot be used with prefix caching now.\")\n-\n-        # If prefix cache is hit, advance context length to bypass\n-        # hit blocks. Accordingly, input tokens, position and query length\n-        # have to be updated.\n-        if prefix_cache_hit:\n-            assert computed_block_nums is not None\n-            context_len = len(computed_block_nums) * self.block_size\n+\n+        if not prefix_cache_hit:\n+            return\n+\n+        assert computed_block_nums is not None\n+        # The cache hit prompt tokens in this sequence. Note that\n+        # this may be larger than the sequence length if chunked\n+        # prefill is enabled.\n+        prefix_cache_len = len(computed_block_nums) * self.block_size\n+        # The number of so far computed prompt tokens in this sequence.\n+        context_len = inter_data.context_lens[seq_idx]\n+        # The total number of prompt tokens in this sequence.\n+        # When chunked prefill is enabled, this is the token number of\n+        # computed chunks + current chunk.\n+        seq_len = inter_data.seq_lens[seq_idx]\n+        if prefix_cache_len <= context_len:\n+            # We already passed the cache hit region,\n+            # so do normal computation.\n+            pass\n+        elif context_len < prefix_cache_len < seq_len:\n+            # Partial hit. Compute the missing part.\n+            uncomputed_start = prefix_cache_len - context_len\n             inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n             inter_data.input_positions[seq_idx] = inter_data.input_positions[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n+            context_len = prefix_cache_len\n+\n             inter_data.context_lens[seq_idx] = context_len\n             inter_data.query_lens[\n                 seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n+        elif seq_len <= prefix_cache_len:\n+            # Full hit. Only compute the last token to avoid\n+            # erroneous behavior. FIXME: Ideally we should directly\n+            # mark all tokens as computed in the scheduler and do not\n+            # schedule this sequence, so this case should not happen.\n+            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n+                seq_idx][-1:]\n+            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n+                seq_idx][-1:]\n+            inter_data.query_lens[seq_idx] = 1\n+            inter_data.context_lens[seq_idx] = inter_data.seq_lens[seq_idx] - 1\n \n     def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                     seq_idx: int,", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport pytest\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm import SamplingParams\n\n\n@pytest.mark.parametrize(\"model\", [\"distilbert/distilgpt2\"])\n@pytest.mark.parametrize(\"block_size\", [16])\ndef test_computed_prefix_blocks(model: str, block_size: int):\n    # This test checks if we are able to run the engine to completion\n    # without triggering asserts.\n    # We are in a scenario where all blocks from the second request's prompt\n    # are full and already computed when the second request arrives.\n    prompt = (\n        \"You are a helpful assistant. How do I build a car from cardboard and \"\n        \"paper clips? Is there an easy to follow video tutorial available \"\n        \"online for free?\")\n    prompt2 = (\n        \" Please recommend to me some resources where I can learn not only to \"\n        \"handle technical difficulties of building a car, but also \"\n        \"decoration.\")\n\n    engine_args = EngineArgs(model=model,\n                             block_size=block_size,\n                             enable_prefix_caching=True)\n\n    engine = LLMEngine.from_engine_args(engine_args)\n    sampling_params = SamplingParams()\n\n    engine.add_request(\"0\", prompt + prompt2, sampling_params)\n    engine.step()\n    engine.add_request(\"1\", prompt, sampling_params)\n    engine.step()"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Performance] Enable chunked prefill and prefix caching together (#7753)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/f508e03e_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/e3580537_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-ec3b5ce", "created_at": "2023-10-13T16:59:07+00:00", "base_commit": "6368e777a8ead7fb62054d3779c6237361ec0d86", "head_commit": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9", "patch": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..49e7007ae 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -81,10 +81,11 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n+    all_special_tokens = set(tokenizer.all_special_tokens)\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in tokenizer.get_added_vocab():\n             if current_sub_text:\n                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                 sub_texts.append(sub_text)\n@@ -129,7 +130,7 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n+    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n         prefix_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:read_offset])\n         new_text = tokenizer.convert_tokens_to_string(", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nfrom collections.abc import Generator\nfrom typing import Any, Optional\n\nimport pytest\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\n                          PreTrainedTokenizerFast)\n\nfrom vllm.inputs import token_inputs\nfrom vllm.sequence import Logprob, SamplingParams, Sequence, SequenceGroup\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\nfrom vllm.transformers_utils.tokenizers.mistral import MistralTokenizer\nfrom vllm.v1.engine import EngineCoreRequest\nfrom vllm.v1.engine.detokenizer import (FastIncrementalDetokenizer,\n                                        IncrementalDetokenizer,\n                                        SlowIncrementalDetokenizer)\n\nSPECIAL_TOKS_TRUTH = [\n    \"Some text with adjacent special tokens                <|padding|><|padding|><fim_prefix><fim_middle><fim_suffix>other text<fim_pad>\",  # noqa\n]\n\nTRUTH = [\n    \"Hello here, this is a simple test\",\n    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",  # noqa\n    \"\u6211\u5f88\u611f\u8c22\u4f60\u7684\u70ed\u60c5\",\n    # Burmese text triggers an edge-case for Mistral's V3-Tekken tokenizer (eg.\n    # for mistralai/Pixtral-12B-2409) where tokens may map to bytes with\n    # incomplete UTF-8 characters\n    # see https://github.com/vllm-project/vllm/pull/9625\n    \"\u1015\u102f\u1036\u1015\u103c\u1004\u103a\u101c\u1031\u1038\u1015\u103c\u1031\u102c\u1015\u103c\u1015\u102b\u103a\",\n] + SPECIAL_TOKS_TRUTH\n\nTOKENIZERS = [\n    \"facebook/opt-125m\",\n    \"gpt2\",\n    \"bigcode/tiny_starcoder_py\",\n    \"EleutherAI/gpt-j-6b\",\n    \"EleutherAI/pythia-70m\",\n    \"bigscience/bloom-560m\",\n    \"mosaicml/mpt-7b\",\n    \"tiiuae/falcon-7b\",\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    \"codellama/CodeLlama-7b-hf\",\n    \"mistralai/Pixtral-12B-2409\",\n]\n\n\ndef _run_incremental_decode(tokenizer,\n                            all_input_ids,\n                            skip_special_tokens: bool,\n                            starting_index: int,\n                            spaces_between_special_tokens: bool = True,\n                            fast: Optional[bool] = None):\n\n    prompt_token_ids = all_input_ids[:starting_index]\n\n    params = SamplingParams(\n        skip_special_tokens=skip_special_tokens,\n        spaces_between_special_tokens=spaces_between_special_tokens,\n    )\n    request = EngineCoreRequest(\"\",\n                                prompt_token_ids,\n                                None,\n                                params,\n                                None,\n                                None,\n                                0.0,\n                                None,\n                                cache_salt=None,\n                                data_parallel_rank=None)\n\n    if fast is None:\n        detokenizer = IncrementalDetokenizer.from_new_request(\n            tokenizer, request)\n    elif fast:\n        detokenizer = FastIncrementalDetokenizer(tokenizer, request)\n    else:\n        detokenizer = SlowIncrementalDetokenizer(tokenizer, request)\n\n    output_text = \"\"\n    for i, token_id in enumerate(all_input_ids[starting_index:]):\n        detokenizer.update([token_id], False)\n        finished = i == len(all_input_ids) - 1\n        output_text += detokenizer.get_next_output_text(finished, delta=True)\n\n    return output_text, detokenizer.output_token_ids\n\n\n@pytest.fixture\ndef tokenizer(tokenizer_name):\n    return (MistralTokenizer.from_pretrained(tokenizer_name)\n            if \"mistral\" in tokenizer_name else\n            AutoTokenizer.from_pretrained(tokenizer_name))\n\n\n@pytest.mark.parametrize(\"tokenizer_name\", [\"mistralai/Pixtral-12B-2409\"])\n@pytest.mark.parametrize(\n    \"truth\",\n    [\n        # Burmese text triggers an edge-case where tokens may map to bytes with\n        # incomplete UTF-8 characters\n        \"\u1015\u102f\u1036\u1015\u103c\u1004\u103a\u101c\u1031\u1038\u1015\u103c\u1031\u102c\u1015\u103c\u1015\u102b\",\n        # Using \"URGENCY\" since \"CY\" has token id 130282\n        \"URGENCY\ud83c\udf36\ufe0f\",\n    ])\ndef test_mistral_edge_case(tokenizer, truth):\n    \"\"\"Test for a specific edge cases with V3-Tekken MistralTokenizer.\n\n    See https://github.com/vllm-project/vllm/pull/9625\n    \"\"\"\n    starting_index = 0\n    all_input_ids = tokenizer(truth, add_special_tokens=False).input_ids\n\n    decoded_text, out_ids = _run_incremental_decode(\n        tokenizer,\n        all_input_ids,\n        skip_special_tokens=True,\n        starting_index=starting_index)\n    assert decoded_text == truth\n    assert out_ids == all_input_ids[starting_index:]\n\n\n@pytest.fixture\ndef skip_special_tokens(request, tokenizer_name) -> Generator[bool, Any, None]:\n    if \"mistral\" in tokenizer_name:\n        yield (\n            True if request.param else\n            pytest.skip(\"mistral doesn't support skip_special_tokens=False\"))\n    else:\n        yield bool(request.param)\n\n\n@pytest.mark.parametrize(\"truth\", TRUTH)\n@pytest.mark.parametrize(\"with_prompt\", [True, False])\n@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n@pytest.mark.parametrize(\"skip_special_tokens\", (True, False), indirect=True)\n@pytest.mark.parametrize(\"spaces_between_special_tokens\", (True, False))\n@pytest.mark.parametrize(\"fast\", (True, False))\ndef test_decode_streaming(tokenizer, truth, with_prompt, skip_special_tokens,\n                          spaces_between_special_tokens, fast):\n    if fast and not isinstance(tokenizer, PreTrainedTokenizerFast):\n        pytest.skip()\n\n    if skip_special_tokens and not spaces_between_special_tokens:\n        pytest.skip()\n\n    if not fast and isinstance(tokenizer, PreTrainedTokenizerFast):\n        # Fix up inconsistency in fast/slow tokenizer behaviour.\n        tokenizer.add_special_tokens({\n            \"additional_special_tokens\": [\n                at for at in\n                tokenizer._tokenizer.get_added_tokens_decoder().values()\n                if at.special\n            ]\n        })\n\n    extra_decode_args = {} if not isinstance(tokenizer,  PreTrainedTokenizer) \\\n        else {\"spaces_between_special_tokens\": spaces_between_special_tokens}\n\n    truth_tokens = tokenizer(truth, add_special_tokens=False).input_ids\n    if tokenizer.bos_token_id is not None:\n        truth_tokens.insert(0, tokenizer.bos_token_id)\n    truth_tokens.append(tokenizer.eos_token_id)\n\n    new_truth = tokenizer.decode(truth_tokens,\n                                 skip_special_tokens=skip_special_tokens,\n                                 **extra_decode_args)\n\n    if with_prompt:\n        num_prompt_tokens = len(\n            tokenizer(truth[:len(truth) // 2],\n                      add_special_tokens=False).input_ids)\n        if tokenizer.bos_token_id is not None:\n            num_prompt_tokens += 1\n\n        prompt_input_ids = truth_tokens[:num_prompt_tokens]\n        generated_input_ids = truth_tokens[num_prompt_tokens:]\n        all_input_ids = prompt_input_ids + generated_input_ids\n        starting_index = len(prompt_input_ids)\n        prompt = tokenizer.decode(prompt_input_ids,\n                                  skip_special_tokens=skip_special_tokens,\n                                  **extra_decode_args)\n\n        generated = new_truth[len(prompt):]\n    else:\n        generated = new_truth\n        starting_index = 0\n        all_input_ids = truth_tokens\n\n    decoded_text, out_ids = _run_incremental_decode(\n        tokenizer,\n        all_input_ids,\n        skip_special_tokens=skip_special_tokens,\n        starting_index=starting_index,\n        spaces_between_special_tokens=spaces_between_special_tokens,\n        fast=fast)\n\n    assert decoded_text == generated\n    assert out_ids == all_input_ids[starting_index:]\n\n\n@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n@pytest.mark.parametrize(\"fast\", (True, False))\ndef test_oov_decode(tokenizer, fast):\n    if fast and not isinstance(tokenizer, PreTrainedTokenizerFast):\n        pytest.skip()\n\n    decoded_text, out_ids = _run_incremental_decode(\n        tokenizer, [len(tokenizer)],\n        skip_special_tokens=True,\n        starting_index=0,\n        spaces_between_special_tokens=True,\n        fast=fast)\n\n    assert decoded_text == ''\n    assert out_ids == [len(tokenizer)]\n\n\n@pytest.fixture\ndef detokenizer(tokenizer_name: str) -> Detokenizer:\n    tokenizer_group = TokenizerGroup(\n        tokenizer_id=tokenizer_name,\n        enable_lora=False,\n        max_num_seqs=100,\n        max_input_length=None,\n        tokenizer_mode=\"mistral\" if \"mistral\" in tokenizer_name else \"auto\",\n        trust_remote_code=False,\n        revision=None,\n    )\n\n    return Detokenizer(tokenizer_group)\n\n\n@pytest.fixture(name=\"complete_sequence_token_ids\")\ndef create_complete_sequence_token_ids(complete_sequence: str,\n                                       tokenizer) -> list[int]:\n    return tokenizer(complete_sequence, add_special_tokens=False).input_ids\n\n\ndef create_sequence(prompt_token_ids=None):\n    prompt_token_ids = prompt_token_ids or []\n    return Sequence(\n        seq_id=0,\n        inputs=token_inputs(prompt_token_ids),\n        block_size=16,\n    )\n\n\ndef create_dummy_logprobs(\n        complete_sequence_token_ids: list[int]) -> list[dict[int, Logprob]]:\n    return [{\n        token_id: Logprob(logprob=0.0),\n        token_id + 1: Logprob(logprob=0.1)\n    } for token_id in complete_sequence_token_ids]\n\n\ndef create_dummy_prompt_logprobs(\n        complete_sequence_token_ids: list[int]\n) -> list[Optional[dict[int, Any]]]:\n    # logprob for the first prompt token is None.\n    logprobs: list[Optional[dict[int, Any]]] = [None]\n    logprobs.extend(create_dummy_logprobs(complete_sequence_token_ids)[1:])\n    return logprobs\n\n\n@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n@pytest.mark.parametrize(\"skip_special_tokens\", [True, False], indirect=True)\ndef test_decode_sequence_logprobs(complete_sequence: str,\n                                  complete_sequence_token_ids: list[int],\n                                  detokenizer: Detokenizer,\n                                  skip_special_tokens: bool):\n    \"\"\"Verify Detokenizer decodes logprobs correctly.\"\"\"\n    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n                                     logprobs=2)\n\n    # Run sequentially.\n    seq = create_sequence()\n    dummy_logprobs = create_dummy_logprobs(complete_sequence_token_ids)\n    sequential_logprobs_text_chosen_token: list[str] = []\n    sequential_logprobs_text_other_token: list[str] = []\n    for new_token, logprobs in zip(complete_sequence_token_ids,\n                                   dummy_logprobs):\n        seq.append_token_id(new_token, logprobs)\n        detokenizer.decode_sequence_inplace(seq, sampling_params)\n        sequential_logprobs_text_chosen_token.append(\n            seq.output_logprobs[-1][new_token].decoded_token)\n        sequential_logprobs_text_other_token.append(\n            seq.output_logprobs[-1][new_token + 1].decoded_token)\n    sequential_result = seq.output_text\n\n    assert sequential_result == \"\".join(sequential_logprobs_text_chosen_token)\n    assert sequential_result != \"\".join(sequential_logprobs_text_other_token)\n\n    if not skip_special_tokens:\n        # Text for logprobs for the chosen token should be the same as the\n        # generated text. Note that this will only be true if we skip\n        # special tokens.\n        assert sequential_result == complete_sequence\n\n\n@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\ndef test_decode_prompt_logprobs(complete_sequence: str,\n                                complete_sequence_token_ids: list[int],\n                                detokenizer: Detokenizer):\n\n    # We want to use skip_special_tokens=False here but Mistral tokenizers\n    # don't support that.\n    if complete_sequence not in SPECIAL_TOKS_TRUTH:\n        skip_special_tokens = True\n    elif not isinstance(detokenizer.tokenizer_group.get_lora_tokenizer(None),\n                        MistralTokenizer):\n        skip_special_tokens = False\n    else:\n        pytest.skip(\"MistralTokenizers don't support \"\n                    \"skip_special_tokens=False\")\n        return\n    \"\"\"Verify Detokenizer decodes prompt logprobs correctly.\"\"\"\n    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n                                     prompt_logprobs=1)\n\n    # Run sequentially.\n    seq = create_sequence(complete_sequence_token_ids)\n    seq_group = SequenceGroup(request_id=\"1\",\n                              seqs=[seq],\n                              sampling_params=sampling_params,\n                              arrival_time=0.0)\n    dummy_logprobs = create_dummy_prompt_logprobs(complete_sequence_token_ids)\n    detokenizer.decode_prompt_logprobs_inplace(seq_group,\n                                               dummy_logprobs,\n                                               position_offset=0)\n    # First logprob is None.\n    decoded_prompt_logprobs: list[dict[int, Any]] = dummy_logprobs[\n        1:]  # type: ignore\n\n    # decoded_prompt_logprobs doesn't contain the first token.\n    token_ids = complete_sequence_token_ids\n    tokenizer = detokenizer.get_tokenizer_for_seq(seq)\n    text_full = tokenizer.decode(token_ids,\n                                 skip_special_tokens=skip_special_tokens)\n    text_first = tokenizer.decode(token_ids[0],\n                                  skip_special_tokens=skip_special_tokens)\n    text = text_full[len(text_first):]\n\n    # Text for logprobs for the chosen token should be the same as the\n    # prompt text. Note that the first logprob is None.\n    assert text == \"\".join([\n        logprobs[token_id].decoded_token\n        for token_id, logprobs in zip(token_ids[1:], decoded_prompt_logprobs)\n    ])\n    assert text != \"\".join([\n        logprobs[token_id + 1].decoded_token\n        for token_id, logprobs in zip(token_ids[1:], decoded_prompt_logprobs)\n    ])\n\n\n@pytest.mark.parametrize(\"model\", [\"facebook/opt-125m\"])\n@pytest.mark.parametrize(\"chunked_prefill_token_size\", [1, 4, 7, 16, -1])\ndef test_decode_prompt_logprobs_chunked_prefill(\n    vllm_runner,\n    model,\n    chunked_prefill_token_size: int,\n    example_prompts,\n    monkeypatch,\n):\n    # VLLM V1 does not use incremental detokenization for\n    # prompt logprobs, so this test strategy is irrelevant.\n    monkeypatch.setenv(\"VLLM_USE_V1\", \"0\")\n\n    max_num_seqs = 256\n    enable_chunked_prefill = False\n    max_num_batched_tokens = None\n    if chunked_prefill_token_size != -1:\n        enable_chunked_prefill = True\n        max_num_seqs = min(chunked_prefill_token_size, max_num_seqs)\n        max_num_batched_tokens = chunked_prefill_token_size\n\n    with vllm_runner(model,\n                     dtype=\"half\",\n                     max_logprobs=5,\n                     gpu_memory_utilization=0.5,\n                     enable_chunked_prefill=enable_chunked_prefill,\n                     max_num_batched_tokens=max_num_batched_tokens,\n                     max_num_seqs=max_num_seqs) as vllm_model:\n\n        vllm_sampling_params = SamplingParams(max_tokens=10,\n                                              logprobs=5,\n                                              prompt_logprobs=5,\n                                              temperature=0.0)\n        vllm_results = vllm_model.llm.generate(\n            example_prompts, sampling_params=vllm_sampling_params)\n\n        for idx, result in enumerate(vllm_results):\n            assert result.prompt_logprobs is not None\n            assert result.prompt_logprobs[0] is None\n\n            # Compared detokenized prompts ids to original prompt.\n            generated_string = \"\"\n            for (prompt_token,\n                 prompt_logprobs) in zip(result.prompt_token_ids[1:],\n                                         result.prompt_logprobs[1:]):\n                # prompt_logprobs is a dict of the token_id: logprob\n                # We select the token_id corresponding to the actual prompt\n                # Decoded token in the detokenized string corresponding to this\n                # prompt token.\n                generated_string += prompt_logprobs[prompt_token].decoded_token\n\n            assert generated_string == example_prompts[idx], (\n                \"Detokenized prompt logprobs do not match original prompt\")\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "Improve detokenization performance (#1338)", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/6368e777_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/ec3b5ce9_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-f092153", "created_at": "2024-12-12T07:14:20+00:00", "base_commit": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03", "head_commit": "f092153fbe349a9a1742940e3703bfcff6aa0a6d", "patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex 25d95ac6e..9046b37f6 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -53,14 +53,23 @@ class InputBatch:\n         self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n         self.req_id_to_index: Dict[str, int] = {}\n \n-        self.token_ids_cpu = np.empty((max_num_reqs, max_model_len),\n-                                      dtype=np.int32)\n+        # TODO(woosuk): This buffer could be too large if max_model_len is big.\n+        # Find a way to reduce the CPU memory usage.\n+        self.token_ids_cpu_tensor = torch.zeros(\n+            (max_num_reqs, max_model_len),\n+            device=\"cpu\",\n+            dtype=torch.int32,\n+            pin_memory=pin_memory,\n+        )\n+        self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),\n-                                       device=self.device,\n-                                       dtype=torch.int32)\n+        self.block_table = torch.zeros(\n+            (max_num_reqs, max_num_blocks_per_req),\n+            device=self.device,\n+            dtype=torch.int32,\n+        )\n         self.block_table_cpu_tensor = torch.zeros(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex e75be21ef..aa91255e6 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -67,6 +67,7 @@ class GPUModelRunner:\n         self.max_model_len = model_config.max_model_len\n         self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n         self.max_num_tokens = scheduler_config.max_num_batched_tokens\n+        self.max_num_reqs = scheduler_config.max_num_seqs\n \n         # Model-related.\n         self.num_attn_layers = model_config.get_num_layers_by_block_type(\n@@ -88,7 +89,7 @@ class GPUModelRunner:\n         self.requests: Dict[str, CachedRequestState] = {}\n         # Persistent batch.\n         self.input_batch = InputBatch(\n-            max_num_reqs=self.scheduler_config.max_num_seqs,\n+            max_num_reqs=self.max_num_reqs,\n             max_model_len=self.max_model_len,\n             max_num_blocks_per_req=self.max_num_blocks_per_req,\n             device=self.device,\n@@ -117,6 +118,32 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int32,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.input_ids_np = self.input_ids_cpu.numpy()\n+        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int64,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.positions_np = self.positions_cpu.numpy()\n+        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+                                            dtype=torch.int32,\n+                                            device=\"cpu\",\n+                                            pin_memory=self.pin_memory)\n+        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                               dtype=torch.int32,\n+                                               device=\"cpu\",\n+                                               pin_memory=self.pin_memory)\n+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                             dtype=torch.int32,\n+                                             device=\"cpu\",\n+                                             pin_memory=self.pin_memory)\n+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n         # Keep the states of the pre-empted requests.\n@@ -241,22 +268,14 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        indices = np.arange(num_reqs)\n-        req_indices = np.repeat(indices, num_scheduled_tokens)\n+        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange_matrix = np.tile(np.arange(max_num_scheduled_tokens),\n-                                (num_reqs, 1))\n-        mask = arange_matrix < num_scheduled_tokens[:, np.newaxis]\n-        arange = arange_matrix[mask]\n+        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n \n         # Get positions.\n-        positions = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        positions_np = positions.numpy()\n+        positions_np = self.positions_np[:total_num_scheduled_tokens]\n         np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n                arange,\n                out=positions_np)\n@@ -267,16 +286,13 @@ class GPUModelRunner:\n         # where M is the max_model_len.\n         token_indices = (positions_np +\n                          req_indices * self.input_batch.token_ids_cpu.shape[1])\n-        token_indices = torch.from_numpy(token_indices)\n-        input_ids = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        torch.index_select(torch.from_numpy(\n-            self.input_batch.token_ids_cpu).flatten(),\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                            0,\n-                           token_indices,\n-                           out=input_ids)\n+                           torch.from_numpy(token_indices),\n+                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n \n         # Calculate the slot mapping.\n         # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n@@ -284,45 +300,40 @@ class GPUModelRunner:\n         # where K is the max_num_blocks_per_req and the block size is 2.\n         # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n         # because M (max_model_len) is not necessarily divisible by block_size.\n-        block_numbers = self.input_batch.block_table_cpu_tensor.flatten()[\n-            req_indices * self.max_num_blocks_per_req +\n-            positions_np // self.block_size]\n-        block_offsets = torch.from_numpy(positions_np % self.block_size)\n-        slot_mapping = torch.empty((total_num_scheduled_tokens, ),\n-                                   dtype=torch.int32,\n-                                   device=\"cpu\",\n-                                   pin_memory=self.pin_memory)\n-        torch.add(block_numbers * self.block_size,\n-                  block_offsets,\n-                  out=slot_mapping)\n+        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n+                               positions_np // self.block_size)\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n+                         [block_table_indices].numpy())\n+        block_offsets = positions_np % self.block_size\n+        np.add(block_numbers * self.block_size,\n+               block_offsets,\n+               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n \n         # Prepare the attention metadata.\n-        query_start_loc = torch.empty((num_reqs + 1, ),\n-                                      dtype=torch.int32,\n-                                      device=\"cpu\",\n-                                      pin_memory=self.pin_memory)\n-        query_start_loc_np = query_start_loc.numpy()\n-        query_start_loc_np[0] = 0\n-        np.cumsum(num_scheduled_tokens, out=query_start_loc_np[1:])\n+        self.query_start_loc_np[0] = 0\n+        np.cumsum(num_scheduled_tokens,\n+                  out=self.query_start_loc_np[1:num_reqs + 1])\n \n         seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                     num_scheduled_tokens)\n         max_seq_len = seq_lens.max()\n-        seq_start_loc = torch.empty((num_reqs + 1, ),\n-                                    dtype=torch.int32,\n-                                    device=\"cpu\",\n-                                    pin_memory=self.pin_memory)\n-        seq_start_loc_np = seq_start_loc.numpy()\n-        seq_start_loc_np[0] = 0\n-        np.cumsum(seq_lens, out=seq_start_loc_np[1:])\n-\n-        self.input_ids[:total_num_scheduled_tokens].copy_(input_ids,\n-                                                          non_blocking=True)\n-        self.positions[:total_num_scheduled_tokens].copy_(positions,\n-                                                          non_blocking=True)\n-        query_start_loc = query_start_loc.to(self.device, non_blocking=True)\n-        seq_start_loc = seq_start_loc.to(self.device, non_blocking=True)\n-        slot_mapping = slot_mapping.to(self.device, non_blocking=True).long()\n+        self.seq_start_loc_np[0] = 0\n+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])\n+\n+        # Copy the tensors to the GPU.\n+        self.input_ids[:total_num_scheduled_tokens].copy_(\n+            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        self.positions[:total_num_scheduled_tokens].copy_(\n+            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n+            self.device, non_blocking=True).long()\n         attn_metadata = FlashAttentionMetadata(\n             num_actual_tokens=total_num_scheduled_tokens,\n             max_query_len=max_num_scheduled_tokens,", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport inspect\nfrom collections.abc import Sequence\nfrom typing import Optional\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom vllm.platforms import current_platform\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import is_pin_memory_available, make_tensor_with_pad\nfrom vllm.v1.pool.metadata import PoolingMetadata\nfrom vllm.v1.sample.logits_processor import LogitsProcessors\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.block_table import BlockTable, MultiGroupBlockTable\nfrom vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch\n\nVOCAB_SIZE = 1024\nNUM_OUTPUT_TOKENS = 20\nMAX_PROMPT_SIZE = 100\nCUDA_DEVICES = [\n    f\"{current_platform.device_type}:{i}\"\n    for i in range(min(current_platform.device_count(), 2))\n]\nMAX_NUM_PROMPT_TOKENS = 64\n\n\ndef _compare_objs(obj1,\n                  obj2,\n                  skip: Sequence = (\"logitsprocs\", \"batch_update_builder\")):\n    attrs = inspect.getmembers(obj1, lambda a: not (inspect.isroutine(a)))\n    attr_names = set([\n        a[0] for a in attrs\n        if not (a[0].startswith('__') and a[0].endswith('__'))\n    ])\n    for attr_name in attr_names:\n        if attr_name in skip:\n            continue\n\n        a = getattr(obj1, attr_name)\n        b = getattr(obj2, attr_name)\n\n        is_same = False\n        if isinstance(a, torch.Tensor):\n            if (a.numel() == 0 or b.numel() == 0):\n                is_same = (a.numel() == 0 and b.numel() == 0)\n            elif torch.allclose(a, b):\n                is_same = True\n        elif isinstance(a, np.ndarray):\n            if np.allclose(a, b):\n                is_same = True\n        elif isinstance(a, MultiGroupBlockTable):\n            for a_i, b_i in zip(a.block_tables, b.block_tables):\n                _compare_objs(a_i, b_i)\n            is_same = True\n        elif isinstance(a, (BlockTable, SamplingMetadata, PoolingMetadata)):\n            _compare_objs(a, b)\n            is_same = True  # if we make it here must be same\n        elif a == b:\n            is_same = True\n        assert is_same, f\"Attribute {attr_name} is different\"\\\n            f\" in {obj1} and {obj2}: {a} != {b}\"\n\n\ndef _remove_requests(input_batch: InputBatch, batch_size: int,\n                     reqs: list[CachedRequestState]) -> set[str]:\n    \"\"\"\n    Remove some requests randomly from the batch and returns\n    set of request removed\n    \"\"\"\n\n    num_reqs_to_remove = np.random.randint(0, batch_size)\n    req_indices_to_remove: set[int] = set()\n    for _ in range(num_reqs_to_remove):\n        req_index_to_remove = np.random.randint(0, batch_size)\n        req_indices_to_remove.add(req_index_to_remove)\n\n    req_ids_to_remove: set[str] = set()\n    for index in req_indices_to_remove:\n        input_batch.remove_request(reqs[index].req_id)\n        req_ids_to_remove.add(reqs[index].req_id)\n    return req_ids_to_remove\n\n\ndef _construct_expected_sampling_metadata(\n    reqs: list[CachedRequestState],\n    req_ids_retained: set[int],\n    req_id_index_in_input_batch: dict[str, int],\n    device: torch.device,\n) -> SamplingMetadata:\n    \"\"\"\n    Constructs and returns the expected SamplingMetadata for this\n    batch.\n    \"\"\"\n    num_reqs = len(req_ids_retained)\n    output_token_ids: list[list[int]] = [list() for _ in range(num_reqs)]\n    prompt_token_ids: list[list[int]] = [list() for _ in range(num_reqs)]\n    presence_penalties = [0.0 for _ in range(num_reqs)]\n    frequency_penalties = [0.0 for _ in range(num_reqs)]\n    repetition_penalties = [1.0 for _ in range(num_reqs)]\n    top_k = [0 for _ in range(num_reqs)]\n    top_p = [0.0 for _ in range(num_reqs)]\n    temperature = [0.0 for _ in range(num_reqs)]\n    min_tokens = {}\n    logit_bias = [None] * num_reqs\n    allowed_token_ids_mask = torch.zeros(num_reqs,\n                                         VOCAB_SIZE,\n                                         dtype=torch.bool,\n                                         device=device)\n    bad_words_token_ids = {}\n    for req in reqs:\n        if req.req_id not in req_ids_retained:\n            continue\n        index_in_input_batch = req_id_index_in_input_batch[req.req_id]\n        output_token_ids[index_in_input_batch] = req.output_token_ids\n        prompt_token_ids[index_in_input_batch] = req.prompt_token_ids\n        presence_penalties[\n            index_in_input_batch] = req.sampling_params.presence_penalty\n        frequency_penalties[index_in_input_batch] = (\n            req.sampling_params.frequency_penalty)\n        repetition_penalties[index_in_input_batch] = (\n            req.sampling_params.repetition_penalty)\n        top_k[index_in_input_batch] = req.sampling_params.top_k\n        top_p[index_in_input_batch] = req.sampling_params.top_p\n        temperature[index_in_input_batch] = req.sampling_params.temperature\n        min_tokens[index_in_input_batch] = (\n            req.sampling_params.min_tokens,\n            req.sampling_params.all_stop_token_ids)\n        logit_bias[index_in_input_batch] = req.sampling_params.logit_bias\n        if req.sampling_params.allowed_token_ids:\n            allowed_token_ids_mask[index_in_input_batch][\n                req.sampling_params.allowed_token_ids] = True\n        if req.sampling_params.bad_words_token_ids:\n            bad_words_token_ids[\n                index_in_input_batch] = req.sampling_params.bad_words_token_ids\n\n    return SamplingMetadata(\n        temperature=torch.tensor(temperature, dtype=torch.float,\n                                 device=device),\n        all_greedy=False,\n        all_random=True,\n        top_p=None if all(x == 1.0 for x in top_p) else torch.tensor(\n            top_p, dtype=torch.float, device=device),\n        top_k=None if all(x == 0 for x in top_k) else torch.tensor(\n            top_k, dtype=torch.int, device=device),\n        generators={},\n        max_num_logprobs=0,\n        prompt_token_ids=make_tensor_with_pad(\n            prompt_token_ids,\n            pad=VOCAB_SIZE,\n            device=torch.device(device),\n            dtype=torch.int64,\n        ),\n        frequency_penalties=torch.tensor(frequency_penalties,\n                                         dtype=torch.float,\n                                         device=device),\n        presence_penalties=torch.tensor(presence_penalties,\n                                        dtype=torch.float,\n                                        device=device),\n        repetition_penalties=torch.tensor(repetition_penalties,\n                                          dtype=torch.float,\n                                          device=device),\n        output_token_ids=output_token_ids,\n        no_penalties=(all(x == 0 for x in presence_penalties)\n                      and all(x == 0 for x in frequency_penalties)\n                      and all(x == 1 for x in repetition_penalties)),\n        allowed_token_ids_mask=allowed_token_ids_mask,\n        bad_words_token_ids=bad_words_token_ids,\n        logitsprocs=LogitsProcessors(),\n    )\n\n\ndef _create_sampling_params():\n    return SamplingParams(\n        top_k=np.random.randint(1, 10),\n        top_p=np.random.uniform(0.0, 1.0),\n        presence_penalty=np.random.uniform(-2.0, 2.0),\n        repetition_penalty=np.random.uniform(0.0, 2.0),\n        frequency_penalty=np.random.uniform(-2.0, 2.0),\n        min_tokens=np.random.randint(1, 10),\n        stop_token_ids=[\n            np.random.randint(0, VOCAB_SIZE)\n            for _ in range(np.random.randint(10))\n        ],\n        logit_bias={0: np.random.uniform(-3.0, 3.0)},\n    )\n\n\ndef _construct_cached_request_state(req_id_suffix: int):\n    prompt_token_ids = [\n        np.random.randint(0, VOCAB_SIZE)\n        for _ in range(np.random.randint(0, MAX_PROMPT_SIZE))\n    ]\n    output_token_ids = [\n        np.random.randint(0, VOCAB_SIZE)\n        for _ in range(np.random.randint(0, NUM_OUTPUT_TOKENS))\n    ]\n    return CachedRequestState(\n        req_id=f\"req_id_{req_id_suffix}\",\n        prompt_token_ids=prompt_token_ids,\n        sampling_params=_create_sampling_params(),\n        pooling_params=None,\n        mm_kwargs=[],\n        mm_positions=[],\n        mm_hashes=[],\n        block_ids=([], ),\n        generator=None,\n        num_computed_tokens=len(output_token_ids),\n        output_token_ids=output_token_ids,\n    )\n\n\n@pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n@pytest.mark.parametrize(\"batch_size\", [1, 2, 32, 64])\ndef test_sampling_metadata_in_input_batch(device: str, batch_size: int):\n    \"\"\"\n    Tests the logic for managing sampling metadata in the InputBatch.\n\n    This test involves adding a set of requests to the InputBatch,\n    followed by removing a subset of them. Afterward, the batch is compacted,\n    and the `make_sampling_metadata` method is invoked on the batch. The\n    output of `make_sampling_metadata` is then compared against the expected\n    results to ensure correctness.\n\n    Note: Ignore logits processor logic, which is tested separately\n    \"\"\"\n    input_batch: InputBatch = InputBatch(\n        max_num_reqs=batch_size,\n        max_model_len=1024,\n        max_num_batched_tokens=1024,\n        device=torch.device(device),\n        pin_memory=is_pin_memory_available(),\n        vocab_size=1024,\n        block_sizes=[1],\n    )\n    reqs: list[CachedRequestState] = []\n    req_id_reqs = {}\n    req_id_output_token_ids = {}\n\n    # Add requests\n    for req_index in range(batch_size):\n        req: CachedRequestState = _construct_cached_request_state(req_index)\n        assigned_req_index = input_batch.add_request(req)\n        assert req_index == assigned_req_index\n        reqs.append(req)\n        req_id_reqs[req.req_id] = req\n        req_id_output_token_ids[req.req_id] = req.output_token_ids\n\n    # Remove some requests\n    req_ids_to_remove = _remove_requests(input_batch, batch_size, reqs)\n    req_ids_retained = set(req_id_reqs.keys()) - req_ids_to_remove\n\n    # Compact the input batch\n    input_batch.condense()\n\n    # Generate the sampling metadata\n    sampling_metadata = input_batch._make_sampling_metadata()\n\n    # Create expected output.\n    expected_sampling_metadata = _construct_expected_sampling_metadata(\n        reqs,\n        req_ids_retained,\n        input_batch.req_id_to_index,\n        device=torch.device(device))\n\n    def same(t1: Optional[torch.Tensor], t2: Optional[torch.Tensor]) -> bool:\n        return (t1 is None\n                and t2 is None) or (t1 is not None and t2 is not None\n                                    and torch.allclose(t1, t2))\n\n    # Assert the actual and expected output.\n    assert torch.allclose(expected_sampling_metadata.temperature,\n                          sampling_metadata.temperature)\n    assert same(expected_sampling_metadata.top_p, sampling_metadata.top_p)\n    assert same(expected_sampling_metadata.top_k, sampling_metadata.top_k)\n    assert torch.allclose(\n        expected_sampling_metadata.frequency_penalties,\n        sampling_metadata.frequency_penalties,\n    )\n    assert torch.allclose(\n        expected_sampling_metadata.presence_penalties,\n        sampling_metadata.presence_penalties,\n    )\n    assert torch.allclose(\n        expected_sampling_metadata.repetition_penalties,\n        sampling_metadata.repetition_penalties,\n    )\n    assert torch.allclose(expected_sampling_metadata.prompt_token_ids,\n                          sampling_metadata.prompt_token_ids)\n    assert (expected_sampling_metadata.output_token_ids ==\n            sampling_metadata.output_token_ids)\n    assert expected_sampling_metadata.no_penalties == \\\n           sampling_metadata.no_penalties\n    if sampling_metadata.allowed_token_ids_mask:\n        assert torch.allclose(\n            expected_sampling_metadata.allowed_token_ids_mask,\n            sampling_metadata.allowed_token_ids_mask)\n    assert expected_sampling_metadata.bad_words_token_ids == \\\n        sampling_metadata.bad_words_token_ids\n\n\n@pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n@pytest.mark.parametrize(\"batch_size\", [32])\n@pytest.mark.parametrize(\"swap_list\", [((0, 1), )])\ndef test_swap_states_in_input_batch(device: str, batch_size: int,\n                                    swap_list: list):\n    \"\"\"\n    Tests the logic for managing sampling metadata in the InputBatch.\n\n    This test involves adding a set of requests to the InputBatch,\n    followed by removing a subset of them. Afterward, the batch is compacted,\n    and the `make_sampling_metadata` method is invoked on the batch. The\n    output of `make_sampling_metadata` is then compared against the expected\n    results to ensure correctness.\n\n    Note: Ignore logits processor logic, which is tested separately\n    \"\"\"\n    input_batch: InputBatch = InputBatch(\n        max_num_reqs=batch_size,\n        max_model_len=1024,\n        max_num_batched_tokens=1024,\n        device=torch.device(device),\n        pin_memory=is_pin_memory_available(),\n        vocab_size=1024,\n        block_sizes=[1],\n    )\n    ref_input_batch: InputBatch = InputBatch(\n        max_num_reqs=batch_size,\n        max_model_len=1024,\n        max_num_batched_tokens=1024,\n        device=torch.device(device),\n        pin_memory=is_pin_memory_available(),\n        vocab_size=1024,\n        block_sizes=[1],\n    )\n\n    reqs: list[CachedRequestState] = []\n    req_id_reqs = {}\n    req_id_output_token_ids = {}\n    # Add requests\n    for req_index in range(batch_size):\n        req: CachedRequestState = _construct_cached_request_state(req_index)\n        assigned_req_index = input_batch.add_request(req)\n        assert assigned_req_index == req_index\n        reqs.append(req)\n        req_id_reqs[req.req_id] = req\n        req_id_output_token_ids[req.req_id] = req.output_token_ids\n\n    reordered_reqs = reqs.copy()\n    for swap_pair in swap_list:\n        reordered_reqs[swap_pair[0]], reordered_reqs[swap_pair[1]] = \\\n            reordered_reqs[swap_pair[1]], reordered_reqs[swap_pair[0]]\n        input_batch.swap_states(swap_pair[0], swap_pair[1])\n\n    for req_index in range(batch_size):\n        req = reordered_reqs[req_index]\n        assigned_req_index = ref_input_batch.add_request(req)\n        assert assigned_req_index == req_index\n\n    input_batch.refresh_metadata()\n    ref_input_batch.refresh_metadata()\n\n    _compare_objs(input_batch, ref_input_batch)\n"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/1da8f0e1_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/f092153f_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "head": {"modules_scanned": 194, "symbols_collected": 3668, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"}, {"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-fa63e71", "created_at": "2025-01-26T08:42:37+00:00", "base_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c", "head_commit": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412", "patch": "diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex acc3a944e..32aee44e3 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -8,7 +8,7 @@ import torch\n class SamplerOutput:\n \n     # [num_reqs]\n-    sampled_token_ids: List[int]\n+    sampled_token_ids: torch.Tensor\n \n     # [num_reqs, max_num_logprobs + 1]\n     logprob_token_ids: Optional[torch.Tensor]\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 7cd42ca21..9ad665a64 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -50,9 +50,8 @@ class Sampler(nn.Module):\n         # Use int32 to reduce the tensor size.\n         sampled = sampled.to(torch.int32)\n \n-        # NOTE: CPU-GPU synchronization happens here.\n         sampler_output = SamplerOutput(\n-            sampled_token_ids=sampled.tolist(),\n+            sampled_token_ids=sampled,\n             logprob_token_ids=topk_indices,\n             logprobs=topk_logprobs,\n             prompt_logprob_token_ids=None,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4b3c325de..6339f1f03 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -775,10 +775,10 @@ class GPUModelRunner:\n             sampling_metadata=sampling_metadata,\n         )\n \n-        sampled_token_ids = sampler_output.sampled_token_ids\n         # TODO(woosuk): The following loop can be slow since it iterates over\n         # the requests one by one. Optimize.\n         num_reqs = self.input_batch.num_reqs\n+        request_seq_lens: List[Tuple[int, CachedRequestState, int]] = []\n         for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n             assert req_id is not None\n             req_state = self.requests[req_id]\n@@ -787,10 +787,10 @@ class GPUModelRunner:\n             assert seq_len <= req_state.num_tokens\n             if seq_len == req_state.num_tokens:\n                 # Append the sampled token to the output token ids.\n-                token_id = sampled_token_ids[i]\n-                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                 self.input_batch.num_tokens[i] += 1\n-                req_state.output_token_ids.append(token_id)\n+                # OPTIMIZATION: Priming the state updates for later updates.\n+                req_state.output_token_ids.append(0)\n+                request_seq_lens.append((i, req_state, seq_len))\n             else:\n                 # Ignore the sampled token from the partial request.\n                 # Rewind the generator state as if the token was not sampled.\n@@ -799,6 +799,21 @@ class GPUModelRunner:\n                     # This relies on cuda-specific torch-internal impl details\n                     generator.set_offset(generator.get_offset() - 4)\n \n+        # num_reqs entries should be non-None\n+        assert all(\n+            req_id is not None for req_id in\n+            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n+        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n+\n+        # NOTE: GPU -> CPU Sync happens here.\n+        # Move as many CPU operations as possible before this sync point.\n+        sampled_token_ids = sampler_output.sampled_token_ids.tolist()\n+        # Update with the actual token ids\n+        for i, req_state, seq_len in request_seq_lens:\n+            token_id = sampled_token_ids[i]\n+            self.input_batch.token_ids_cpu[i, seq_len] = token_id\n+            req_state.output_token_ids[-1] = token_id\n+\n         if sampler_output.logprob_token_ids is None:\n             logprob_token_ids = None\n         else:\n@@ -808,12 +823,6 @@ class GPUModelRunner:\n         else:\n             logprobs = sampler_output.logprobs.cpu()\n \n-        # num_reqs entries should be non-None\n-        assert all(\n-            req_id is not None for req_id in\n-            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n-        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n-\n         model_runner_output = ModelRunnerOutput(\n             req_ids=req_ids,\n             req_id_to_index=self.input_batch.req_id_to_index,", "test_patch": "", "efficiency_test": ["# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport multiprocessing\nimport os\nimport random\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm.sampling_params import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport inspect\nimport logging\n\n# API Probing helpers - auto-generated for compatibility\ndef safe_create_object(cls, **kwargs):\n    \"\"\"Create object with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(cls):\n            raise TypeError(f\"{cls} is not callable\")\n        sig = inspect.signature(cls)\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters and k != \"self\"}\n        return cls(**valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to create {cls.__name__ if hasattr(cls, '__name__') else cls} with args {list(kwargs.keys())}: {e}\")\n        raise\n\ndef safe_call_function(func, *args, **kwargs):\n    \"\"\"Call function with only valid arguments based on signature.\"\"\"\n    try:\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        sig = inspect.signature(func)\n        # Filter kwargs to only valid parameters\n        valid_kwargs = {k: v for k, v in kwargs.items() \n                       if k in sig.parameters}\n        return func(*args, **valid_kwargs)\n    except Exception as e:\n        logging.warning(f\"Failed to call {func.__name__ if hasattr(func, '__name__') else func} with args {list(kwargs.keys())}: {e}\")\n        raise\n\n# Specific helpers for common vllm classes\ndef safe_create_engine_output(**kwargs):\n    \"\"\"Create EngineCoreOutput with compatible arguments.\"\"\"\n    try:\n        from vllm.v1.engine import EngineCoreOutput\n        return safe_create_object(EngineCoreOutput, **kwargs)\n    except ImportError:\n        try:\n            from vllm.engine import EngineCoreOutput  \n            return safe_create_object(EngineCoreOutput, **kwargs)\n        except ImportError:\n            raise ImportError(\"EngineCoreOutput not found in vllm\")\n\ndef safe_create_sampling_params(**kwargs):\n    \"\"\"Create SamplingParams with compatible arguments.\"\"\"\n    try:\n        from vllm import SamplingParams\n        return safe_create_object(SamplingParams, **kwargs)\n    except ImportError:\n        try:\n            from vllm import SamplingParams\n            return safe_create_object(SamplingParams, **kwargs)\n        except ImportError:\n            raise ImportError(\"SamplingParams not found in vllm\")\n\ndef safe_create_llm(**kwargs):\n    \"\"\"Create LLM with compatible arguments.\"\"\"\n    try:\n        from vllm import LLM\n        return safe_create_object(LLM, **kwargs)\n    except ImportError:\n        raise ImportError(\"LLM not found in vllm\")\n\n\n\nimport pytest\nimport torch\nimport torch.distributed\n\nfrom vllm.distributed.eplb.rebalance_execute import (\n    rearrange_expert_weights_inplace)\nfrom vllm.distributed.parallel_state import (ensure_model_parallel_initialized,\n                                             get_tp_group,\n                                             init_distributed_environment)\nfrom vllm.utils import update_environment_variables\n\n\ndef distributed_run(fn, world_size):\n    number_of_processes = world_size\n    processes: list[multiprocessing.Process] = []\n    for i in range(number_of_processes):\n        env: dict[str, str] = {}\n        env['RANK'] = str(i)\n        env['LOCAL_RANK'] = str(i)\n        env['WORLD_SIZE'] = str(number_of_processes)\n        env['LOCAL_WORLD_SIZE'] = str(number_of_processes)\n        env['MASTER_ADDR'] = 'localhost'\n        env['MASTER_PORT'] = '12345'\n        p = multiprocessing.Process(target=fn, args=(env, ))\n        processes.append(p)\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    for p in processes:\n        assert p.exitcode == 0\n\n\ndef worker_fn_wrapper(fn):\n    # `multiprocessing.Process` cannot accept environment variables directly\n    # so we need to pass the environment variables as arguments\n    # and update the environment variables in the function\n    def wrapped_fn(env):\n        update_environment_variables(env)\n        local_rank = os.environ['LOCAL_RANK']\n        device = torch.device(f\"cuda:{local_rank}\")\n        torch.cuda.set_device(device)\n        init_distributed_environment()\n\n        # Ensure each worker process has the same random seed\n        random.seed(42)\n        torch.manual_seed(42)\n\n        fn()\n\n    return wrapped_fn\n\n\ndef create_expert_indices_with_redundancy(\n        num_layers: int,\n        num_logical_experts: int,\n        total_physical_experts: int,\n        redundancy_config: list[int],  # redundancy for each logical expert\n) -> torch.Tensor:\n    \"\"\"\n    Create expert indices with redundancy.\n    \n    Args:\n        num_layers: number of layers\n        num_logical_experts: number of logical experts\n        total_physical_experts: total number of physical experts\n        redundancy_config: redundancy for each logical expert\n    \n    Returns:\n        indices: Shape (num_layers, total_physical_experts)\n    \"\"\"\n    assert sum(redundancy_config) == total_physical_experts\n    assert len(redundancy_config) == num_logical_experts\n\n    indices = torch.zeros(num_layers, total_physical_experts, dtype=torch.long)\n\n    for layer in range(num_layers):\n        physical_pos = 0\n        for logical_expert_id, redundancy in enumerate(redundancy_config):\n            for _ in range(redundancy):\n                indices[layer, physical_pos] = logical_expert_id\n                physical_pos += 1\n\n    # Shuffle the indices at dim 1\n    for layer in range(num_layers):\n        indices[layer] = indices[layer][torch.randperm(indices.shape[1])]\n\n    return indices\n\n\ndef create_expert_weights(\n    num_layers: int,\n    num_local_experts: int,\n    hidden_sizes: list[int],\n    rank: int,\n    device: torch.device,\n    physical_to_logical_mapping: torch.Tensor,\n) -> list[list[torch.Tensor]]:\n    \"\"\"\n    Create fake expert weights tensor for testing.\n    \n    Use `arange` to generate predictable weights values, based on logical\n    expert ID.\n    All replicas of the same logical expert should have the same weights.\n    \n    Args:\n        physical_to_logical_mapping: Shape (num_layers, num_local_experts)\n            mapping[layer, physical_pos] = logical_expert_id\n    \"\"\"\n    expert_weights = []\n\n    for layer in range(num_layers):\n        layer_weights = []\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            weight_tensor = torch.zeros(num_local_experts,\n                                        hidden_size,\n                                        device=device,\n                                        dtype=torch.float32)\n\n            for local_expert in range(num_local_experts):\n                # Get the logical expert ID for this physical expert\n                global_pos = rank * num_local_experts + local_expert\n                logical_expert_id = physical_to_logical_mapping[\n                    layer, global_pos].item()\n\n                # Generate weights based on logical expert ID\n                # (so that all replicas of the same logical expert have the\n                # same weights)\n                base_value = (logical_expert_id * 1000 + layer * 100 +\n                              weight_idx * 10)\n                weight_tensor[local_expert] = torch.arange(base_value,\n                                                           base_value +\n                                                           hidden_size,\n                                                           device=device,\n                                                           dtype=torch.float32)\n\n            layer_weights.append(weight_tensor)\n        expert_weights.append(layer_weights)\n\n    return expert_weights\n\n\ndef create_redundancy_config(\n    num_logical_experts: int,\n    num_physical_experts: int,\n) -> list[int]:\n    \"\"\"Create a redundancy configuration.\"\"\"\n    redundancy_config = [1] * num_logical_experts\n    remaining = num_physical_experts - num_logical_experts\n    # Randomly assign the remaining physical experts to the logical experts\n    for _ in range(remaining):\n        redundancy_config[random.choice(range(num_logical_experts))] += 1\n    return redundancy_config\n\n\ndef verify_expert_weights_after_shuffle(\n    expert_weights: list[list[torch.Tensor]],\n    new_indices: torch.Tensor,\n    hidden_sizes: list[int],\n    ep_rank: int,\n    num_local_experts: int,\n):\n    \"\"\"Verify the weights after shuffling are correct.\"\"\"\n    num_layers = len(expert_weights)\n\n    for layer in range(num_layers):\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            weight_tensor = expert_weights[layer][weight_idx]\n\n            for local_expert in range(num_local_experts):\n                # Calculate the global expert ID for this local expert\n                global_pos = ep_rank * num_local_experts + local_expert\n                expected_logical_expert = new_indices[layer, global_pos].item()\n\n                # Check if the weights are correct\n                actual_weights = weight_tensor[local_expert]\n                expected_base = (expected_logical_expert * 1000 + layer * 100 +\n                                 weight_idx * 10)\n                expected_weights = torch.arange(expected_base,\n                                                expected_base + hidden_size,\n                                                device=actual_weights.device,\n                                                dtype=actual_weights.dtype)\n\n                torch.testing.assert_close(\n                    actual_weights,\n                    expected_weights,\n                    msg=f\"Layer {layer}, weight {weight_idx},\"\n                    f\"local expert {local_expert}: \"\n                    f\"weights do not match. \"\n                    f\"Expected logical expert {expected_logical_expert}\")\n\n\ndef verify_redundant_experts_have_same_weights(\n    expert_weights: list[list[torch.Tensor]],\n    indices: torch.Tensor,\n    hidden_sizes: list[int],\n    world_size: int,\n    num_local_experts: int,\n):\n    \"\"\"\n    Verify that all replicas of the same logical expert have the same weights.\n    \"\"\"\n    num_layers = len(expert_weights)\n    total_physical_experts = world_size * num_local_experts\n\n    for layer in range(num_layers):\n        # Collect weights for all physical experts for each weight matrix\n        all_weights: list[torch.Tensor] = []\n\n        for weight_idx, hidden_size in enumerate(hidden_sizes):\n            # Create tensor to store all expert weights\n            # Shape: [total_physical_experts, hidden_size]\n            gathered_weights = torch.zeros(\n                total_physical_experts,\n                hidden_size,\n                device=expert_weights[layer][weight_idx].device,\n                dtype=expert_weights[layer][weight_idx].dtype)\n\n            # Use all_gather to collect expert weights from current node\n            # expert_weights[layer][weight_idx] shape:\n            # [num_local_experts, hidden_size]\n            local_weights = expert_weights[layer][\n                weight_idx]  # [num_local_experts, hidden_size]\n\n            # Split tensor along dim 0 into a list for all_gather\n            gathered_weights_list = torch.chunk(gathered_weights,\n                                                world_size,\n                                                dim=0)\n\n            torch.distributed.all_gather(\n                # Output list: each element corresponds to one rank's weights\n                list(gathered_weights_list),\n                local_weights  # Input: current rank's local weights\n            )\n\n            all_weights.append(gathered_weights)\n\n        # Verify that all replicas of the same logical expert have the same\n        # weights\n        logical_expert_weights: dict[int, dict[int, torch.Tensor]] = {}\n\n        for physical_pos in range(total_physical_experts):\n            logical_expert_id = int(indices[layer, physical_pos].item())\n\n            if logical_expert_id not in logical_expert_weights:\n                # First time encountering this logical expert, save its weights\n                logical_expert_weights[logical_expert_id] = {\n                    weight_idx: all_weights[weight_idx][physical_pos]\n                    for weight_idx in range(len(hidden_sizes))\n                }\n            else:\n                # Verify that current physical expert's weights match the\n                # previously saved logical expert weights\n                for weight_idx in range(len(hidden_sizes)):\n                    torch.testing.assert_close(\n                        all_weights[weight_idx][physical_pos],\n                        logical_expert_weights[logical_expert_id][weight_idx],\n                        msg=f\"Layer {layer}, weight {weight_idx},\"\n                        f\"logical expert {logical_expert_id}: \"\n                        f\"Physical expert {physical_pos} has different weights\"\n                        f\"than expected\")\n\n\n@pytest.mark.parametrize(\n    \"world_size,num_layers,num_local_experts,num_logical_experts\",\n    [\n        # 2 GPU, 2 experts per GPU\n        # 3 logical experts, 4 physical experts, 1 redundant experts\n        (2, 1, 2, 3),\n        # 2 GPU, 3 experts per GPU\n        # 4 logical experts, 6 physical experts, 2 redundant experts\n        (2, 2, 3, 4),\n        # 2 GPU, 8 experts per GPU\n        # 16 logical experts, 16 physical experts, 0 redundant experts\n        (2, 4, 8, 16),\n        # 4 GPU, 2 experts per GPU\n        # 6 logical experts, 8 physical experts, 2 redundant experts\n        (4, 1, 2, 6),\n        # 4 GPU, 2 experts per GPU\n        # 5 logical experts, 8 physical experts, 3 redundant experts\n        (4, 2, 2, 5),\n        # 4 GPU, 8 experts per GPU\n        # 16 logical experts, 32 physical experts, 16 redundant experts\n        (4, 8, 8, 16),\n    ])\ndef test_rearrange_expert_weights_with_redundancy(world_size, num_layers,\n                                                  num_local_experts,\n                                                  num_logical_experts):\n    \"\"\"Test the functionality of rearranging expert weights with redundancy.\"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        # Initialize model parallel (using tensor parallel as an entrypoint\n        # to expert parallel)\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        # Test parameters\n        total_physical_experts = world_size * num_local_experts\n        hidden_sizes = [32, 64]  # Two different weight matrices\n\n        # Create old expert indices (with redundancy)\n        redundancy_config = create_redundancy_config(num_logical_experts,\n                                                     total_physical_experts)\n\n        old_indices = create_expert_indices_with_redundancy(\n            num_layers,\n            num_logical_experts,\n            total_physical_experts,\n            redundancy_config,\n        )\n\n        # Create new expert indices (with redundancy)\n        new_redundancy_config = create_redundancy_config(\n            num_logical_experts, total_physical_experts)\n        new_indices = create_expert_indices_with_redundancy(\n            num_layers,\n            num_logical_experts,\n            total_physical_experts,\n            new_redundancy_config,\n        )\n\n        # Create expert weights\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               old_indices)\n\n        # Execute weight rearrangement\n        rearrange_expert_weights_inplace(\n            old_indices,\n            new_indices,\n            expert_weights,\n            ep_group,\n            is_profile=False,\n        )\n\n        # Verify the rearrangement result\n        verify_expert_weights_after_shuffle(\n            expert_weights,\n            new_indices,\n            hidden_sizes,\n            ep_rank,\n            num_local_experts,\n        )\n\n        verify_redundant_experts_have_same_weights(\n            expert_weights,\n            new_indices,\n            hidden_sizes,\n            world_size,\n            num_local_experts,\n        )\n\n    distributed_run(worker_fn, world_size)\n\n\n@pytest.mark.parametrize(\"world_size\", [2, 4])\ndef test_rearrange_expert_weights_no_change(world_size):\n    \"\"\"\n    Test that when the indices do not change, the weights should remain\n    unchanged.\n    \"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        num_layers = 2\n        num_local_experts = 2\n        total_physical_experts = world_size * num_local_experts\n        num_logical_experts = total_physical_experts // 2  # Some redundancy\n        hidden_sizes = [32, 64]\n\n        # Create redundancy configuration\n        redundancy_config = [2] * num_logical_experts\n\n        # Same indices - no change\n        indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            redundancy_config)\n\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               indices)\n\n        # Save original weights\n        original_weights = []\n        for layer_weights in expert_weights:\n            layer_copy = []\n            for weight in layer_weights:\n                layer_copy.append(weight.clone())\n            original_weights.append(layer_copy)\n\n        # Execute rearrangement (should be no change)\n        rearrange_expert_weights_inplace(\n            indices,\n            indices,  # Same indices\n            expert_weights,\n            ep_group,\n            is_profile=False)\n\n        # Verify that the weights have not changed\n        for layer in range(num_layers):\n            for weight_idx in range(len(hidden_sizes)):\n                torch.testing.assert_close(\n                    expert_weights[layer][weight_idx],\n                    original_weights[layer][weight_idx],\n                    msg=f\"Layer {layer}, weight {weight_idx} should remain \"\n                    f\"unchanged\")\n\n    distributed_run(worker_fn, world_size)\n\n\n@pytest.mark.parametrize(\"world_size\", [2, 4])\ndef test_rearrange_expert_weights_profile_mode(world_size):\n    \"\"\"Test profile mode (should not copy actual weights)\"\"\"\n\n    if torch.cuda.device_count() < world_size:\n        pytest.skip(f\"Need at least {world_size} GPUs to run the test\")\n\n    @worker_fn_wrapper\n    def worker_fn():\n        ensure_model_parallel_initialized(\n            tensor_model_parallel_size=world_size,\n            pipeline_model_parallel_size=1)\n\n        ep_group = get_tp_group().cpu_group\n        ep_rank = torch.distributed.get_rank()\n        device = torch.device(f\"cuda:{ep_rank}\")\n\n        num_layers = 1\n        num_local_experts = 2\n        total_physical_experts = world_size * num_local_experts\n        num_logical_experts = total_physical_experts // 2\n        hidden_sizes = [32]\n\n        # Create different index distributions\n        old_redundancy = create_redundancy_config(num_logical_experts,\n                                                  total_physical_experts)\n        new_redundancy = create_redundancy_config(num_logical_experts,\n                                                  total_physical_experts)\n\n        old_indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            old_redundancy)\n        new_indices = create_expert_indices_with_redundancy(\n            num_layers, num_logical_experts, total_physical_experts,\n            new_redundancy)\n\n        expert_weights = create_expert_weights(num_layers, num_local_experts,\n                                               hidden_sizes, ep_rank, device,\n                                               old_indices)\n\n        # Save original weights\n        original_weights = []\n        for layer_weights in expert_weights:\n            layer_copy = []\n            for weight in layer_weights:\n                layer_copy.append(weight.clone())\n            original_weights.append(layer_copy)\n\n        # Execute profile mode rearrangement\n        rearrange_expert_weights_inplace(\n            old_indices,\n            new_indices,\n            expert_weights,\n            ep_group,\n            is_profile=True  # Profile mode\n        )\n\n        # In profile mode, the weights should remain unchanged\n        for layer in range(num_layers):\n            for weight_idx in range(len(hidden_sizes)):\n                torch.testing.assert_close(\n                    expert_weights[layer][weight_idx],\n                    original_weights[layer][weight_idx],\n                    msg=\"In profile mode, the weights should remain unchanged\")\n\n    distributed_run(worker_fn, world_size)"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>", "setup_commands": null, "install_commands": null, "notes": null, "api_manifest_paths": {"base": "/root/OmniPerf-Bench/.test_work/api_manifests/2a0309a6_vllm_api.json", "head": "/root/OmniPerf-Bench/.test_work/api_manifests/fa63e710_vllm_api.json", "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"}, "api_manifest_summaries": {"base": {"modules_scanned": 179, "symbols_collected": 3628, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "head": {"modules_scanned": 179, "symbols_collected": 3628, "errors": [{"stage": "import_module", "module": "vllm.attention.backends.blocksparse_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.flashinfer", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.hpu_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.ipex_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.openvino", "error": "ModuleNotFoundError(\"No module named 'openvino'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.pallas", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.rocm_flash_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.torch_sdpa", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.backends.xformers", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.blocksparse_attention_kernel", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.interface", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.blocksparse_attention.utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.hpu_paged_attn", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.paged_attn", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.prefix_prefill", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.triton_flash_attention", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.mp_distributed_executor", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.executor.multiproc_worker_utils", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.ops.triton_ops", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_gpu", "error": "ModuleNotFoundError(\"No module named 'setuptools'\")"}, {"stage": "import_module", "module": "vllm.lora.punica_wrapper.punica_hpu", "error": "ModuleNotFoundError(\"No module named 'vllm_hpu_extension'\")"}]}, "main": {"modules_scanned": 194, "symbols_collected": 3693, "errors": [{"stage": "import_module", "module": "vllm.attention.ops.nki_flash_attn", "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.pallas_kv_cache_update", "error": "ModuleNotFoundError(\"No module named 'jax'\")"}, {"stage": "import_module", "module": "vllm.attention.ops.rocm_aiter_paged_attn", "error": "ModuleNotFoundError(\"No module named 'aiter'\")"}, {"stage": "import_module", "module": "vllm.distributed.device_communicators.tpu_communicator", "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"}, {"stage": "import_module", "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector", "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"}, {"stage": "import_module", "module": "vllm.distributed.tpu_distributed_utils", "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"}]}}, "test_failed": "Yes"}
