{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-21d93c1", "created_at": "2023-12-14T07:55:07+00:00", "base_commit": "f1c8520146031a650404a6ab120ee11e91c10bed", "head_commit": "21d93c140d0a97af5f0c59e660cf04bd417fd424", "patch": "diff --git a/Dockerfile b/Dockerfile\nindex f41753aeb..6ef03b843 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -41,14 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads\n \n RUN python3 setup.py build_ext --inplace\n \n-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\n-RUN apt-get install -y git && \\\n-    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n-    cd megablocks && \\\n-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n-\n # image to run unit testing suite\n FROM dev AS test\n \n@@ -85,12 +77,8 @@ FROM vllm-base AS vllm-openai\n RUN --mount=type=cache,target=/root/.cache/pip \\\n     pip install accelerate\n \n-COPY vllm vllm\n COPY --from=build /workspace/vllm/*.so /workspace/vllm/\n-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/\n-RUN --mount=type=cache,target=/root/.cache/pip \\\n-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n+COPY vllm vllm\n \n ENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n \ndiff --git a/README.md b/README.md\nindex 84cadee48..e4b3b5026 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get\n ```bash\n pip install vllm\n ```\n-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n-```bash\n-pip install megablocks\n-```\n \n ## Getting Started\n \ndiff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst\nindex e21cdd65d..44e4fe5ea 100644\n--- a/docs/source/models/supported_models.rst\n+++ b/docs/source/models/supported_models.rst\n@@ -74,8 +74,7 @@ Otherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for in\n Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.\n \n .. note::\n-    Currently, the ROCm version of vLLM does not support Mixtral.\n-    Additionally, it only supports Mistral for context lengths up to 4096.\n+    Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.\n \n .. tip::\n     The easiest way to check if your model is supported is to run the program below:\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bafa73c7..eb1fee0f2 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -120,14 +120,16 @@ class ModelConfig:\n             if load_format == \"auto\":\n                 load_format = \"pt\"\n \n-        # FIXME(woosuk): This is a temporary hack. Support safetensor weights.\n+        # TODO: Remove this check once HF updates the pt weights of Mixtral.\n         architectures = getattr(self.hf_config, \"architectures\", [])\n-        if \"MixtralForCausalLM\" in architectures and load_format != \"pt\":\n-            logger.info(\n-                \"Currently, only 'pt' format is supported for Mixtral. \"\n-                \"Changing the format to 'pt'. This may re-download the \"\n-                \"weights if you have downloaded the safetensor weights.\")\n-            load_format = \"pt\"\n+        if \"MixtralForCausalLM\" in architectures:\n+            if load_format == \"pt\":\n+                raise ValueError(\n+                    \"Currently, the 'pt' format is not supported for Mixtral. \"\n+                    \"Please use the 'safetensors' format instead. \")\n+            elif load_format == \"auto\":\n+                # Do not fall back to pt weights.\n+                load_format = \"safetensors\"\n \n         self.load_format = load_format\n \ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 5596884f3..ab9a1636a 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -39,13 +39,15 @@ _MODELS = {\n }\n \n # Models not supported by ROCm.\n-_ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n+_ROCM_UNSUPPORTED_MODELS = []\n \n # Models partially supported by ROCm.\n # Architecture -> Reason.\n _ROCM_PARTIALLY_SUPPORTED_MODELS = {\n     \"MistralForCausalLM\":\n     \"Sliding window attention is not yet supported in ROCm's flash attention\",\n+    \"MixtralForCausalLM\":\n+    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n }\n \n \ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 8e0a094c7..b11e3713f 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -31,22 +31,11 @@ import torch.nn.functional as F\n from torch import nn\n from transformers import MixtralConfig\n \n-try:\n-    import megablocks.ops as ops\n-except ImportError as e:\n-    raise ImportError(\"MegaBlocks not found. \"\n-                      \"Please install it by `pip install megablocks`.\") from e\n-try:\n-    import stk\n-except ImportError as e:\n-    raise ImportError(\n-        \"STK not found. \"\n-        \"Please install it by `pip install stanford-stk`.\") from e\n-\n from vllm.model_executor.input_metadata import InputMetadata\n from vllm.model_executor.layers.attention import PagedAttention\n from vllm.model_executor.layers.layernorm import RMSNorm\n from vllm.model_executor.layers.linear import (LinearMethodBase,\n+                                               ReplicatedLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.rotary_embedding import get_rope\n@@ -66,8 +55,134 @@ from vllm.sequence import SamplerOutput\n KVCache = Tuple[torch.Tensor, torch.Tensor]\n \n \n-def promote_scalar(x: torch.Tensor) -> torch.Tensor:\n-    return x.view(1) if len(x.size()) == 0 else x\n+class MixtralMLP(nn.Module):\n+\n+    def __init__(\n+        self,\n+        num_experts: int,\n+        hidden_size: int,\n+        intermediate_size: int,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.num_experts = num_experts\n+        self.ffn_dim = intermediate_size\n+        self.hidden_dim = hidden_size\n+\n+        self.w1 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w2 = ReplicatedLinear(self.ffn_dim,\n+                                   self.hidden_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w3 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+\n+        # TODO: Use vllm's SiluAndMul\n+        self.act_fn = nn.SiLU()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        w1_out, _ = self.w1(hidden_states)\n+        w1_out = self.act_fn(w1_out)\n+        w3_out, _ = self.w3(hidden_states)\n+        current_hidden_states = w1_out * w3_out\n+        current_hidden_states, _ = self.w2(current_hidden_states)\n+        return current_hidden_states\n+\n+\n+class DummyModule(nn.Module):\n+\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+        self.w1 = nn.Linear(0, 0, bias=False)\n+        self.w2 = nn.Linear(0, 0, bias=False)\n+        self.w3 = nn.Linear(0, 0, bias=False)\n+\n+        set_weight_attrs(self.w1.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w2.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w3.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+\n+    def forward(self, *args, **kwargs) -> None:\n+        raise NotImplementedError()\n+\n+    def dummy_weight_loader(self, *args, **kwargs) -> None:  # pylint: disable=unused-argument\n+        # Noop\n+        return\n+\n+\n+class MixtralMoE(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.rank = get_tensor_model_parallel_rank()\n+        self.tp_size = get_tensor_model_parallel_world_size()\n+        self.num_total_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+        if self.tp_size > self.num_total_experts:\n+            raise ValueError(\n+                f\"Tensor parallel size {self.tp_size} is greater than \"\n+                f\"the number of experts {self.num_total_experts}.\")\n+        # Split experts equally between ranks\n+        self.expert_indicies = np.array_split(range(\n+            self.num_total_experts), self.tp_size)[self.rank].tolist()\n+        if not self.expert_indicies:\n+            raise ValueError(\n+                f\"Rank {self.rank} has no experts assigned to it.\")\n+\n+        self.experts = nn.ModuleList([\n+            MixtralMLP(self.num_total_experts,\n+                       config.hidden_size,\n+                       config.intermediate_size,\n+                       linear_method=linear_method)\n+            if idx in self.expert_indicies else DummyModule()\n+            for idx in range(self.num_total_experts)\n+        ])\n+        self.gate = ReplicatedLinear(config.hidden_size,\n+                                     self.num_total_experts,\n+                                     bias=False,\n+                                     linear_method=linear_method)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits, _ = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights,\n+                                                       self.top_k,\n+                                                       dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+\n+        final_hidden_states = None\n+        for expert_idx in self.expert_indicies:\n+            expert_layer = self.experts[expert_idx]\n+            expert_mask = (selected_experts == expert_idx)\n+            expert_weights = (routing_weights * expert_mask).sum(dim=-1,\n+                                                                 keepdim=True)\n+\n+            current_hidden_states = expert_layer(hidden_states).mul_(\n+                expert_weights)\n+            if final_hidden_states is None:\n+                final_hidden_states = current_hidden_states\n+            else:\n+                final_hidden_states.add_(current_hidden_states)\n+\n+        return tensor_model_parallel_all_reduce(final_hidden_states).view(\n+            batch_size, sequence_length, hidden_dim)\n \n \n class MixtralAttention(nn.Module):\n@@ -78,6 +193,7 @@ class MixtralAttention(nn.Module):\n                  num_kv_heads: int,\n                  max_position: int = 4096 * 32,\n                  rope_theta: float = 10000,\n+                 linear_method: Optional[LinearMethodBase] = None,\n                  sliding_window: Optional[int] = None) -> None:\n         super().__init__()\n         self.hidden_size = hidden_size\n@@ -102,24 +218,26 @@ class MixtralAttention(nn.Module):\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n \n-        self.wqkv = QKVParallelLinear(\n+        self.qkv_proj = QKVParallelLinear(\n             hidden_size,\n             self.head_dim,\n             self.total_num_heads,\n             self.total_num_kv_heads,\n             bias=False,\n+            linear_method=linear_method,\n         )\n-        self.wo = RowParallelLinear(\n+        self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             hidden_size,\n             bias=False,\n+            linear_method=linear_method,\n         )\n         self.rotary_emb = get_rope(\n             self.head_dim,\n             rotary_dim=self.head_dim,\n             max_position=max_position,\n             base=int(self.rope_theta),\n-            is_neox_style=False,  # weights not in HF format\n+            is_neox_style=True,\n         )\n         self.attn = PagedAttention(\n             self.num_heads,\n@@ -137,310 +255,74 @@ class MixtralAttention(nn.Module):\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:\n-        qkv, _ = self.wqkv(hidden_states)\n+        qkv, _ = self.qkv_proj(hidden_states)\n         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n         q, k = self.rotary_emb(positions, q, k)\n         k_cache, v_cache = kv_cache\n         attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\n                                 cache_event)\n-        output, _ = self.wo(attn_output)\n+        output, _ = self.o_proj(attn_output)\n         return output\n \n \n-class BlockSparseMoE(nn.Module):\n-    \"\"\"\n-    Built on the paper and library Megablocks as described in\n-    https://arxiv.org/abs/2211.15841. This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-    \"\"\"\n-\n-    def __init__(self, hidden_dim: int, ffn_dim: int, num_experts: int,\n-                 top_k: int):\n-        super().__init__()\n-        self.hidden_dim = hidden_dim\n-        self.ffn_dim = ffn_dim\n-        self.num_experts = num_experts\n-        self.top_k = top_k\n-\n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim,\n-                              self.num_experts,\n-                              bias=False,\n-                              device=torch.cuda.current_device())\n-\n-        tp_size = get_tensor_model_parallel_world_size()\n-        assert self.ffn_dim % tp_size == 0\n-        self.ffn_dim_per_partition = self.ffn_dim // tp_size\n-        # merged expert weights, all of size  (ffn_dim * n_experts, model_dim)\n-        self.w1 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w1, {\"weight_loader\": self.moe_weight_loader})\n-        self.w2 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w2, {\"weight_loader\": self.moe_weight_loader})\n-        self.w3 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w3, {\"weight_loader\": self.moe_weight_loader})\n-\n-        # Calculate the number of bits needed to represent the expert indices\n-        # so that we can pass it to radix sort.\n-        self.sort_end_bit = max(int(np.ceil(np.log2(self.num_experts))), 1)\n-        self.blocking = 128\n-        self.quantize_scatter_num_bits = -1\n-\n-        # Calculate the number of bits needed to represent the column indices\n-        # in the intermediate sparse matrix.\n-        max_column_index = (self.ffn_dim * self.num_experts) // self.blocking\n-        self.transpose_sort_end_bit = max(\n-            int(np.ceil(np.log2(max_column_index))), 1)\n-\n-    def moe_weight_loader(self, param: nn.Parameter,\n-                          loaded_weight: torch.Tensor) -> None:\n-        \"\"\"\n-        Load the weights for the MoE linear layer.\n-        \"\"\"\n-        tp_rank = get_tensor_model_parallel_rank()\n-        shard_size = self.ffn_dim_per_partition\n-        loaded_weight = loaded_weight.view(self.num_experts, self.ffn_dim, -1)\n-        loaded_weight = loaded_weight[:, shard_size * tp_rank:shard_size *\n-                                      (tp_rank + 1)]\n-        loaded_weight = loaded_weight.reshape_as(param)\n-        param.data.copy_(loaded_weight)\n-\n-    def sparse_transpose(\n-            self, size: int, row_indices,\n-            column_indices) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        block_columns = size[1] // self.blocking\n-\n-        # Sort row indices by column indices to get the transposed matrix's\n-        # column indices.\n-        #\n-        # NOTE: Our sort operation uses the same width indices as the input\n-        # values. To avoid overflow when we have large activation matrices\n-        # we cast to 32-bit before sorting.\n-        _, gather_indices = ops.sort(column_indices.int(),\n-                                     self.transpose_sort_end_bit)\n-\n-        # There are a constant number of blocks in every row of the sparse\n-        # matrix. A blocks offset is:\n-        #\n-        # row_index * blocks_per_row + column_index % blocks_per_row\n-        #\n-        # Once we have the block offsets ordered for transposition we can\n-        # divide by blocks_per_row to get the transposed column indices.\n-        column_indices_t = row_indices.gather(0, gather_indices.long())\n-        block_offsets_t = gather_indices.int()\n-\n-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)\n-        nnz_per_column = ops.histogram(column_indices, block_columns)\n-        nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)\n-        offsets_t = torch.cat([zero, nnz_per_column])\n-        return column_indices_t, offsets_t, block_offsets_t\n-\n-    def topology(self, x: torch.Tensor,\n-                 padded_bins: torch.Tensor) -> \"stk.Matrix\":\n-        padded_tokens, _ = x.size()\n-        assert padded_tokens % self.blocking == 0\n-        assert self.ffn_dim_per_partition % self.blocking == 0\n-\n-        # Offsets for the sparse matrix. All rows have the\n-        # same number of nonzero blocks dictated by the\n-        # dimensionality of a single expert.\n-        block_rows = padded_tokens // self.blocking\n-        blocks_per_row = self.ffn_dim_per_partition // self.blocking\n-        offsets = torch.arange(\n-            0,\n-            block_rows * blocks_per_row + 1,\n-            blocks_per_row,\n-            dtype=torch.int32,\n-            device=x.device,\n-        )\n-\n-        # Indices for the sparse matrix. The indices for\n-        # the intermediate matrix are dynamic depending\n-        # on the mapping of tokens to experts.\n-        column_indices = ops.topology(padded_bins, self.blocking, block_rows,\n-                                      blocks_per_row)\n-\n-        # TODO(tgale): This is unused. Remove the need for this in stk.\n-        # For now, use meta init to save the device memory.\n-        data = torch.empty(\n-            column_indices.numel(),\n-            self.blocking,\n-            self.blocking,\n-            dtype=x.dtype,\n-            device=\"meta\",\n-        )\n-        shape = (padded_tokens, self.ffn_dim_per_partition * self.num_experts)\n-        row_indices = stk.ops.row_indices(shape, data, offsets, column_indices)\n-        column_indices_t, offsets_t, block_offsets_t = self.sparse_transpose(\n-            shape, row_indices, column_indices)\n-        return stk.Matrix(\n-            shape,\n-            data,\n-            row_indices,\n-            column_indices,\n-            offsets,\n-            column_indices_t,\n-            offsets_t,\n-            block_offsets_t,\n-        )\n-\n-    def indices_and_padded_bins(\n-        self, selected_experts: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n-               torch.Tensor]:\n-        # Sort the expert ids to produce the scatter/gather\n-        # indices for the permutation.\n-        selected_experts = selected_experts.int()\n-        bin_ids, indices = ops.sort(selected_experts, self.sort_end_bit)\n-\n-        # Histogram the expert ids to identify the number of\n-        # tokens routed to each expert.\n-        tokens_per_expert = ops.histogram(selected_experts, self.num_experts)\n-\n-        # Round the token counts up to the block size used in\n-        # the matrix muliplications. Caculate the starting\n-        # position of each bin.\n-        padded_tokens_per_expert = ops.round_up(tokens_per_expert,\n-                                                self.blocking)\n-        padded_bins = ops.inclusive_cumsum(padded_tokens_per_expert, 0)\n-        padded_bins = promote_scalar(padded_bins)\n-\n-        # Calculate the bin bounds for the sorted tokens.\n-        bins = ops.inclusive_cumsum(tokens_per_expert, 0)\n-        bins = promote_scalar(bins)\n-        return indices, bin_ids, bins, padded_bins, tokens_per_expert\n-\n-    @torch.inference_mode()\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n-        x: (sequence_length, model_dim)\n-        gate_logits: (sequence_length, n_experts)\n-        \"\"\"\n-        # optional reshape\n-        input_shape = x.shape\n-        x = x.view(-1, input_shape[-1])\n-\n-        # gate_logits: (sequence_length, n_experts)\n-        gate_logits = self.gate(x)\n-        # all_probs: (sequence_length, n_experts) and upcast for softmax\n-        all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)\n-        # weights, selected_experts: (sequence_length, top-k)\n-        weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)\n-        weights /= weights.sum(dim=-1, keepdim=True)\n-        weights = weights.flatten().to(x.dtype)\n-        selected_experts = selected_experts.flatten()\n-\n-        (indices, bin_ids, bins, padded_bins,\n-         _) = self.indices_and_padded_bins(selected_experts)\n-\n-        # Permute tokens and pad to prepare expert computation\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = ops.padded_gather(x, indices, bin_ids, bins, padded_bins,\n-                              self.top_k)\n-\n-        # Create the sparse matrix topology\n-        with torch.no_grad():\n-            topo = self.topology(x, padded_bins)\n-\n-        # Perform the expert computation\n-        # First Dense x Dense -> Sparse for w1 and w3,\n-        # (top_k * sequence_length + padding, ffn_dim * n_experts)\n-        x = stk.Matrix(\n-            topo.size(),\n-            F.silu(stk.ops.sdd(x, self.w1.t(), topo).data) *\n-            stk.ops.sdd(x, self.w3.t(), topo).data,\n-            topo.row_indices,\n-            topo.column_indices,\n-            topo.offsets,\n-            topo.column_indices_t,\n-            topo.offsets_t,\n-            topo.block_offsets_t,\n-        )\n-\n-        # Then Sparse x Dense -> Dense for w2\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = stk.ops.dsd(x, self.w2)\n-\n-        x = tensor_model_parallel_all_reduce(x)\n-\n-        # Permute back and remove padding\n-        # (top_k * sequence_length, model_dim)\n-        x = ops.padded_scatter(\n-            x,\n-            indices,\n-            bin_ids,\n-            weights,\n-            bins,\n-            padded_bins,\n-            self.top_k,\n-            self.quantize_scatter_num_bits,\n-        )\n-        return x.view(*input_shape)\n-\n-\n class MixtralDecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         # Requires transformers > 4.32.0\n         rope_theta = getattr(config, \"rope_theta\", 10000)\n-        self.attention = MixtralAttention(\n+        self.self_attn = MixtralAttention(\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n             max_position=config.max_position_embeddings,\n             num_kv_heads=config.num_key_value_heads,\n             rope_theta=rope_theta,\n-            sliding_window=config.sliding_window)\n-        self.block_sparse_moe = BlockSparseMoE(\n-            hidden_dim=self.hidden_size,\n-            ffn_dim=config.intermediate_size,\n-            num_experts=config.num_local_experts,\n-            top_k=config.num_experts_per_tok,\n-        )\n-        self.attention_norm = RMSNorm(config.hidden_size,\n-                                      eps=config.rms_norm_eps)\n-        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+            sliding_window=config.sliding_window,\n+            linear_method=linear_method)\n+        self.block_sparse_moe = MixtralMoE(config=config,\n+                                           linear_method=linear_method)\n+        self.input_layernorm = RMSNorm(config.hidden_size,\n+                                       eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n+                                                eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         positions: torch.Tensor,\n-        x: torch.Tensor,\n+        hidden_states: torch.Tensor,\n         kv_cache: KVCache,\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n+        residual: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n-        r = self.attention(\n+        # Self Attention\n+        if residual is None:\n+            residual = hidden_states\n+            hidden_states = self.input_layernorm(hidden_states)\n+        else:\n+            hidden_states, residual = self.input_layernorm(\n+                hidden_states, residual)\n+        hidden_states = self.self_attn(\n             positions=positions,\n-            hidden_states=self.attention_norm(x),\n+            hidden_states=hidden_states,\n             kv_cache=kv_cache,\n             input_metadata=input_metadata,\n             cache_event=cache_event,\n         )\n-        h = x + r\n-        r = self.block_sparse_moe(self.ffn_norm(h))\n-        out = h + r\n-        return out\n \n+        # Fully Connected\n+        hidden_states, residual = self.post_attention_layernorm(\n+            hidden_states, residual)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n+        return hidden_states, residual\n \n-class MixtralForCausalLM(nn.Module):\n+\n+class MixtralModel(nn.Module):\n \n     def __init__(\n         self,\n@@ -448,23 +330,18 @@ class MixtralForCausalLM(nn.Module):\n         linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n-        self.config = config\n-        assert linear_method is None\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n-        self.tok_embeddings = VocabParallelEmbedding(\n+\n+        self.embed_tokens = VocabParallelEmbedding(\n             config.vocab_size,\n             config.hidden_size,\n         )\n-\n-        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.output = ParallelLMHead(config.vocab_size, config.hidden_size)\n-        self.sampler = Sampler(config.vocab_size)\n-\n         self.layers = nn.ModuleList([\n-            MixtralDecoderLayer(config)\n+            MixtralDecoderLayer(config, linear_method=linear_method)\n             for _ in range(config.num_hidden_layers)\n         ])\n+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -474,20 +351,42 @@ class MixtralForCausalLM(nn.Module):\n         input_metadata: InputMetadata,\n         cache_events: Optional[List[torch.cuda.Event]],\n     ) -> SamplerOutput:\n-        hidden_states = self.tok_embeddings(input_ids)\n-\n-        # forward\n+        hidden_states = self.embed_tokens(input_ids)\n+        residual = None\n         for i in range(len(self.layers)):\n             cache_event = None if cache_events is None else cache_events[i]\n             layer = self.layers[i]\n-            hidden_states = layer(\n-                positions,\n-                hidden_states,\n-                kv_caches[i],\n-                input_metadata,\n-                cache_event,\n-            )\n-        hidden_states = self.norm(hidden_states)\n+            hidden_states, residual = layer(positions, hidden_states,\n+                                            kv_caches[i], input_metadata,\n+                                            cache_event, residual)\n+        hidden_states, _ = self.norm(hidden_states, residual)\n+        return hidden_states\n+\n+\n+class MixtralForCausalLM(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.linear_method = linear_method\n+        self.model = MixtralModel(config, linear_method)\n+        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n+        self.sampler = Sampler(config.vocab_size)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        positions: torch.Tensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n+    ) -> torch.Tensor:\n+        hidden_states = self.model(input_ids, positions, kv_caches,\n+                                   input_metadata, cache_events)\n         return hidden_states\n \n     def sample(\n@@ -495,7 +394,7 @@ class MixtralForCausalLM(nn.Module):\n         hidden_states: Optional[torch.Tensor],\n         sampling_metadata: SamplingMetadata,\n     ) -> SamplerOutput:\n-        next_tokens = self.sampler(self.output.weight, hidden_states,\n+        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens\n \n@@ -506,10 +405,11 @@ class MixtralForCausalLM(nn.Module):\n                      revision: Optional[str] = None):\n         stacked_params_mapping = [\n             # (param_name, shard_name, shard_id)\n-            (\"wqkv\", \"wq\", \"q\"),\n-            (\"wqkv\", \"wk\", \"k\"),\n-            (\"wqkv\", \"wv\", \"v\"),\n+            (\"qkv_proj\", \"q_proj\", \"q\"),\n+            (\"qkv_proj\", \"k_proj\", \"k\"),\n+            (\"qkv_proj\", \"v_proj\", \"v\"),\n         ]\n+\n         params_dict = dict(self.named_parameters())\n         for name, loaded_weight in hf_model_weights_iterator(\n                 model_name_or_path, cache_dir, load_format, revision):\n", "test_patch": "", "efficiency_test": ["import os\nimport json\nimport time\nimport importlib\nimport types\nfrom typing import Dict, Any, Tuple, Callable, Optional\n\nimport numpy as np\nimport torch\n\n# -------------------------------\n# Error taxonomy (deterministic)\n# -------------------------------\nE_IMPORT_MISSING = \"IMPORT_MISSING_SYMBOL\"\nE_OPT_PATH_NOT_TRIGGERED = \"OPT_PATH_NOT_TRIGGERED\"\nE_CAPABILITY = \"CAPABILITY_UNSUPPORTED\"\nE_EQFAIL = \"EQUIVALENCE_FAILED\"\n\n# -------------------------------\n# Determinism & policy helpers\n# -------------------------------\ndef ensure_determinism() -> None:\n    torch.manual_seed(1234)\n    np.random.seed(1234)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(1234)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    # Be strict by default unless commit requires TF32\n    torch.backends.cuda.matmul.allow_tf32 = False\n    torch.backends.cudnn.allow_tf32 = False\n\ndef pick_device() -> torch.device:\n    want = os.getenv(\"PROB_DEVICE\", \"auto\").lower()\n    if want == \"cuda\" and torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    if want == \"cpu\":\n        return torch.device(\"cpu\")\n    # auto: prefer CUDA, fallback to CPU\n    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef pick_dtype() -> torch.dtype:\n    key = os.getenv(\"PROB_FORCE_DTYPE\", \"auto\").lower()\n    map_ = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16, \"auto\": torch.float32}\n    return map_.get(key, torch.float32)\n\ndef parse_opt_gates() -> Dict[str, Any]:\n    raw = os.getenv(\"PROB_OPT_GATES\", '')\n    if not raw:\n        return {}\n    try:\n        return json.loads(raw)\n    except Exception:\n        gates = {}\n        for kv in raw.split(\",\"):\n            if \"=\" in kv:\n                k, v = kv.split(\"=\", 1)\n                gates[k.strip()] = v.strip()\n        return gates\n\n# -------------------------------\n# Diff-aware import resolution\n# -------------------------------\ndef resolve_target() -> Tuple[Callable, Dict[str, Any], str]:\n    mod_path, sym_name, candidates = \"vllm.model_executor.models.mixtral\", \"MixtralForCausalLM\", [(\"vllm.model_executor.models.mixtral\", \"MixtralForCausalLM\")]\n\n    m = importlib.import_module(mod_path)\n    target = m\n    for part in sym_name.split(\".\"):\n        if not hasattr(target, part):\n            raise ImportError(f\"{E_IMPORT_MISSING}: {mod_path}.{sym_name} not found; nearest candidates={candidates}\")\n        target = getattr(target, part)\n\n    fq = f\"{mod_path}.{sym_name}\"\n    return target, {}, fq\n\n# -------------------------------\n# Setup: workload reflecting commit\n# -------------------------------\ndef _cap_by_memory(nelms: int, bytes_per: int, frac: float = 0.7) -> int:\n    try:\n        if torch.cuda.is_available():\n            free, total = torch.cuda.mem_get_info()\n            cap = int((total * frac) // max(bytes_per, 1))\n            return min(nelms, cap)\n    except Exception:\n        pass\n    return nelms\n\ndef setup() -> Dict[str, Any]:\n    ensure_determinism()\n    device = pick_device()\n    dtype = pick_dtype()\n\n    B = 8\n    H = 32\n    D = 128\n    T_q = 128\n    T_kv = 2048\n    hidden = H * D\n\n    bytes_per = 2 if dtype in (torch.float16, torch.bfloat16) else 4\n    _ = _cap_by_memory(B * T_q * hidden, bytes_per)\n\n    q = torch.randn((B, H, T_q, D), dtype=dtype, device=device)\n    k = torch.randn((B, H, T_kv, D), dtype=dtype, device=device)\n    v = torch.randn((B, H, T_kv, D), dtype=dtype, device=device)\n\n    causal = True\n    attn_mask = None\n\n    opt_gates = parse_opt_gates()\n    for k_env, v_env in opt_gates.items():\n        os.environ[str(k_env)] = str(v_env)\n\n    return {\n        \"device\": device,\n        \"dtype\": dtype,\n        \"B\": B, \"H\": H, \"D\": D, \"T_q\": T_q, \"T_kv\": T_kv,\n        \"q\": q, \"k\": k, \"v\": v,\n        \"causal\": causal,\n        \"attn_mask\": attn_mask,\n        \"opt_gates\": opt_gates,\n    }\n\n# -------------------------------\n# Experiment: EXACT optimized path\n# -------------------------------\ndef experiment(data: Dict[str, Any]) -> Any:\n    target, call_kwargs, fqname = resolve_target()\n\n    with torch.no_grad():\n        q = data[\"q\"]\n        k = data[\"k\"]\n        v = data[\"v\"]\n        result = target(q, k, v, attn_mask=data[\"attn_mask\"], is_causal=data[\"causal\"], **call_kwargs)\n\n    return result\n\n# -------------------------------\n# Result I/O for equivalence\n# -------------------------------\ndef store_result(result: Any, filepath: str) -> None:\n    if isinstance(result, torch.Tensor):\n        payload = {\n            \"type\": \"torch_tensor\",\n            \"shape\": tuple(result.shape),\n            \"dtype\": str(result.dtype),\n            \"device\": \"cpu\",\n            \"sample\": result.flatten()[:4096].detach().cpu(),\n        }\n        torch.save(payload, filepath)\n    else:\n        torch.save({\"type\": \"generic\", \"value\": result}, filepath)\n\ndef load_result(filepath: str) -> Any:\n    return torch.load(filepath)\n\n# -------------------------------\n# Equivalence with dtype-aware tolerances\n# -------------------------------\ndef _eq_tolerances(dtype: torch.dtype, level: str) -> Tuple[float, float]:\n    if level == \"exact\":\n        return (0.0, 0.0)\n    if dtype in (torch.float16, torch.bfloat16):\n        return (1e-3, 1e-4)\n    if dtype == torch.float32:\n        return (1e-5, 1e-7)\n    return (1e-5, 1e-7)\n\ndef check_equivalence(current_result: Any, reference_payload: Any) -> None:\n    level = os.getenv(\"PROB_EQ_LEVEL\", \"numeric\").lower()\n    if isinstance(current_result, torch.Tensor) and reference_payload.get(\"type\") == \"torch_tensor\":\n        ref_sample = reference_payload[\"sample\"]\n        assert tuple(current_result.shape) == tuple(reference_payload[\"shape\"]), \\\n            f\"Shape mismatch: {tuple(current_result.shape)} vs {tuple(reference_payload['shape'])}\"\n        assert str(current_result.dtype) == reference_payload[\"dtype\"], \\\n            f\"Dtype mismatch: {current_result.dtype} vs {reference_payload['dtype']}\"\n        rtol, atol = _eq_tolerances(current_result.dtype, level)\n        torch.testing.assert_close(\n            current_result.flatten()[: ref_sample.numel()].cpu(),\n            ref_sample,\n            rtol=rtol,\n            atol=atol,\n            msg=f\"{E_EQFAIL}: deviation beyond tolerances (level={level})\"\n        )\n    else:\n        assert current_result == reference_payload.get(\"value\"), f\"{E_EQFAIL}: non-tensor results not equal\"\n\n# -------------------------------\n# Timing utilities\n# -------------------------------\ndef _time_gpu(run: Callable, iters: int) -> Tuple[float, float, float]:\n    torch.cuda.synchronize()\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    times = []\n    for _ in range(iters):\n        start.record()\n        _ = run()\n        end.record()\n        torch.cuda.synchronize()\n        times.append(start.elapsed_time(end))\n    times.sort()\n    avg = sum(times) / len(times)\n    p50 = times[len(times) // 2]\n    p95 = times[int(len(times) * 0.95) - 1]\n    return avg, p50, p95\n\ndef _time_cpu(run: Callable, iters: int) -> Tuple[float, float, float]:\n    times = []\n    for _ in range(iters):\n        t0 = time.perf_counter()\n        _ = run()\n        t1 = time.perf_counter()\n        times.append((t1 - t0) * 1000.0)\n    times.sort()\n    avg = sum(times) / len(times)\n    p50 = times[len(times) // 2]\n    p95 = times[int(len(times) * 0.95) - 1]\n    return avg, p50, p95\n\n# -------------------------------\n# Main entry: run_test\n# -------------------------------\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    data = setup()\n    impl_tag = os.getenv(\"PROB_IMPL_TAG\", \"child\")\n    commit_hash = os.getenv(\"PROB_COMMIT_HASH\", \"21d93c140d0a97af5f0c59e660cf04bd417fd424\")\n\n    # Warmup\n    warmup = 5 if torch.cuda.is_available() else 3\n    for _ in range(warmup):\n        _ = experiment(data)\n\n    # Timing iterations\n    iters = 50 if torch.cuda.is_available() else 10\n    if torch.cuda.is_available():\n        avg_ms, p50_ms, p95_ms = _time_gpu(lambda: experiment(data), iters)\n    else:\n        avg_ms, p50_ms, p95_ms = _time_cpu(lambda: experiment(data), iters)\n\n    # Equivalence/reference I/O\n    result = experiment(data)\n    ref_path = f\"{prefix}_{impl_tag}_{commit_hash}_reference.pt\"\n    if reference:\n        store_result(result, ref_path)\n    if eqcheck:\n        reference_payload = load_result(ref_path)\n        check_equivalence(result, reference_payload)\n\n    # Summary JSON (single line)\n    summary = {\n        \"impl_tag\": impl_tag,\n        \"commit_hash\": commit_hash,\n        \"device\": str(data[\"device\"]),\n        \"dtype\": str(data[\"dtype\"]),\n        \"iters\": iters,\n        \"warmup\": warmup,\n        \"avg_ms\": round(avg_ms, 6),\n        \"p50_ms\": round(p50_ms, 6),\n        \"p95_ms\": round(p95_ms, 6),\n        \"eq_level\": os.getenv(\"PROB_EQ_LEVEL\", \"numeric\"),\n        \"opt_path_hit\": True\n    }\n    print(json.dumps(summary, sort_keys=True))\n    return avg_ms\n\n# End of script"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": "vLLM", "gt_commit_message": "Optimize Mixtral with expert parallelism (#2090)", "setup_commands": null, "install_commands": null, "notes": null}
