{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-8c1e77f", "created_at": "2024-11-28T16:31:28+00:00", "base_commit": "5fc5ce0fe45f974fc8840175e8321652238400f0", "head_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f", "patch": "diff --git a/CMakeLists.txt b/CMakeLists.txt\nindex 45a3b484e..f43bf8143 100644\n--- a/CMakeLists.txt\n+++ b/CMakeLists.txt\n@@ -522,7 +522,7 @@ else()\n   FetchContent_Declare(\n           vllm-flash-attn\n           GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git\n-          GIT_TAG d886f88165702b3c7e7744502772cd98b06be9e1\n+          GIT_TAG fdf6d72b48aea41f4ae6a89139a453dae554abc8\n           GIT_PROGRESS TRUE\n           # Don't share the vllm-flash-attn build between build types\n           BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn", "test_patch": "", "efficiency_test": ["import os\nimport time\nimport random\nimport sys\nfrom typing import List\n\nimport torch\nfrom vllm import LLM, SamplingParams\n\n\ndef cuda_sync():\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n    except Exception:\n        # If synchronize fails for any reason, ignore to keep test running.\n        pass\n\n\ndef timed(func, *args, **kwargs):\n    cuda_sync()\n    t0 = time.perf_counter()\n    result = func(*args, **kwargs)\n    cuda_sync()\n    t1 = time.perf_counter()\n    return result, t1 - t0\n\n\ndef build_prompts(num: int, base: str, min_words: int = 5, max_words: int = 12) -> List[str]:\n    rng = random.Random(42)\n    words = [\"performance\", \"latency\", \"throughput\", \"accuracy\", \"model\", \"kernel\",\n             \"attention\", \"batch\", \"compute\", \"memory\", \"cache\", \"optimize\"]\n    prompts = []\n    for i in range(num):\n        n = rng.randint(min_words, max_words)\n        topic = \" \".join(rng.choice(words) for _ in range(n))\n        prompts.append(f\"{base} {topic}.\")\n    return prompts\n\n\ndef long_prompt(repeat_words: int = 256) -> str:\n    # Aim to create a prompt that tokenizes into a moderately long sequence.\n    # We avoid extreme lengths to keep total runtime < 60s.\n    base = \"In a detailed technical note, explain how flash attention improves inference efficiency \"\n    filler = (\"efficiency optimization attention kernel math compute memory bandwidth \" * 8).strip()\n    words = (filler + \" \") * max(1, repeat_words // 8)\n    return (base + words).strip()\n\n\ndef choose_dtype() -> str:\n    try:\n        if torch.cuda.is_available():\n            major, minor = torch.cuda.get_device_capability(0)\n            # Prefer bf16 on Ampere+ if available, else fp16\n            return \"bfloat16\" if major >= 8 else \"float16\"\n    except Exception:\n        pass\n    # Fallback; vLLM typically requires GPU, but keep a reasonable default.\n    return \"float16\"\n\n\ndef main():\n    total_t0 = time.perf_counter()\n    err = None\n    try:\n        # Quick environment configuration\n        model_name = os.getenv(\"VLLM_MODEL\", \"facebook/opt-125m\")\n        small_n = int(os.getenv(\"SMALL_N\", \"48\"))       # number of small sequential requests\n        small_max_tokens = int(os.getenv(\"SMALL_MAX\", \"8\"))\n        large_batch = int(os.getenv(\"LARGE_BATCH\", \"4\"))  # number of prompts in large batch\n        large_repeat = int(os.getenv(\"LARGE_REPEAT\", \"220\"))  # word repeats to build long prompts\n        large_max_tokens = int(os.getenv(\"LARGE_MAX\", \"64\"))\n\n        # vLLM requires CUDA; handle gracefully if not available\n        if not torch.cuda.is_available():\n            # If no GPU, exit early but still produce timing output.\n            return\n\n        dtype = choose_dtype()\n\n        # Initialize LLM\n        llm_kwargs = {\n            \"model\": model_name,\n            \"dtype\": dtype,\n            \"tensor_parallel_size\": int(os.getenv(\"TP_SIZE\", \"1\")),\n            \"trust_remote_code\": True,\n        }\n\n        llm, init_time = timed(LLM, **llm_kwargs)\n\n        # Warmup to trigger any initial compilation / kernel selection\n        warmup_prompt = \"Warmup: Briefly describe kernel fusion benefits for attention mechanisms.\"\n        warmup_params = SamplingParams(max_tokens=8, temperature=0.0, top_p=1.0)\n        _, warmup_gen_time = timed(llm.generate, warmup_prompt, warmup_params)\n\n        # Scenario 1: Many small requests (CPU overhead sensitive)\n        small_params = SamplingParams(\n            max_tokens=small_max_tokens,\n            temperature=0.0,\n            top_p=1.0,\n            n=1,\n        )\n        small_prompts = build_prompts(small_n, \"Write one concise sentence about\")\n        small_total_time = 0.0\n        # Sequential small calls to emphasize per-request overhead\n        for p in small_prompts:\n            try:\n                _, elapsed = timed(llm.generate, p, small_params)\n                small_total_time += elapsed\n            except Exception:\n                # Keep going; this test should be resilient\n                continue\n\n        # Scenario 2: Few large batched requests (GPU-heavy, less CPU overhead)\n        large_params = SamplingParams(\n            max_tokens=large_max_tokens,\n            temperature=0.0,\n            top_p=1.0,\n            n=1,\n        )\n        large_prompts = [long_prompt(large_repeat) for _ in range(large_batch)]\n        _, large_time = timed(llm.generate, large_prompts, large_params)\n\n        # Optional additional quick stress: mixed sizes in a single batch\n        mixed_params = SamplingParams(max_tokens=16, temperature=0.0, top_p=1.0, n=1)\n        mixed_prompts = build_prompts(8, \"Summarize\") + [long_prompt(96) for _ in range(2)]\n        try:\n            _, _ = timed(llm.generate, mixed_prompts, mixed_params)\n        except Exception:\n            pass\n\n    except Exception as e:\n        err = e\n    finally:\n        total_t1 = time.perf_counter()\n        # Print only the final required line\n        print(f\"Execution time: {total_t1 - total_t0:.6f}s\")\n        # If there was an error, exit non-zero to signal failure while still printing time\n        if err is not None:\n            try:\n                # Flush stdout before exiting with error code\n                sys.stdout.flush()\n            except Exception:\n                pass\n            sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>", "setup_commands": null, "install_commands": null, "notes": null}
