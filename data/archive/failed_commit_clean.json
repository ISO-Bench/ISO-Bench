{
  "repo": "OmniPerf-Bench/vllm",
  "instance_id": "OmniPerf-Bench__vllm-25ebed2",
  "created_at": "2024-12-15T21:33:00+00:00",
  "base_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310",
  "head_commit": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
  "patch": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex abcd4b007..67166fb05 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -118,6 +118,12 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n+        self.arange_np = np.arange(max(self.max_num_reqs, self.max_model_len),\n+                                   dtype=np.int32)\n+        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n+        # a faster version of creating a new tensor every time. Thus, we should\n+        # not make any assumptions about the values in these tensors.\n         self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n@@ -269,11 +275,13 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n+        req_indices = np.repeat(self.arange_np[:num_reqs],\n+                                num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n+        arange = np.concatenate(\n+            [self.arange_np[:n] for n in num_scheduled_tokens])\n \n         # Get positions.\n         positions_np = self.positions_np[:total_num_scheduled_tokens]",
  "test_patch": "",
  "efficiency_test": [
    "# SPDX-License-Identifier: Apache-2.0\n# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n\nimport random\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom vllm.attention import Attention\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                         SchedulerConfig, VllmConfig, set_current_vllm_config)\nfrom vllm.distributed.parallel_state import (init_distributed_environment,\n                                             initialize_model_parallel)\nfrom vllm.model_executor.layers.mamba.mamba_mixer2 import MambaMixer2\nfrom vllm.platforms import current_platform\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.utils import GiB_bytes, update_environment_variables\nfrom vllm.v1.core.kv_cache_utils import (estimate_max_model_len,\n                                         get_kv_cache_config)\nfrom vllm.v1.core.sched.output import (CachedRequestData, NewRequestData,\n                                       SchedulerOutput)\nfrom vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n                                        KVCacheGroupSpec, KVCacheTensor)\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.gpu_input_batch import InputBatch\nfrom vllm.v1.worker.gpu_model_runner import GPUModelRunner\n\nBLOCK_SIZE = 16\nNUM_BLOCKS = 10\nDEVICE = current_platform.device_type\n\n\ndef initialize_kv_cache(runner: GPUModelRunner):\n    \"\"\"\n    Only perform necessary steps in GPUModelRunner.initialize_kv_cache()\n    \"\"\"\n    attn_spec = FullAttentionSpec(\n        block_size=BLOCK_SIZE,\n        num_kv_heads=runner.model_config.get_num_kv_heads(\n            runner.parallel_config),\n        head_size=runner.model_config.get_head_size(),\n        dtype=runner.kv_cache_dtype,\n        use_mla=False,\n    )\n    tensor_size = attn_spec.page_size_bytes * NUM_BLOCKS\n    kv_cache_config = KVCacheConfig(\n        num_blocks=NUM_BLOCKS,\n        kv_cache_tensors=[\n            KVCacheTensor(size=tensor_size, shared_by=[\"layer.0\"]),\n        ],\n        kv_cache_groups=[\n            KVCacheGroupSpec(layer_names=[\"layer.0\"], kv_cache_spec=attn_spec)\n        ],\n    )\n    runner.kv_cache_config = kv_cache_config\n    runner.input_batch = InputBatch(\n        max_num_reqs=runner.max_num_reqs,\n        max_model_len=runner.max_model_len,\n        max_num_batched_tokens=runner.max_num_tokens,\n        device=runner.device,\n        pin_memory=runner.pin_memory,\n        vocab_size=runner.model_config.get_vocab_size(),\n        block_sizes=[\n            kv_cache_config.kv_cache_groups[0].kv_cache_spec.block_size\n        ],\n    )\n    runner.initialize_attn_backend(kv_cache_config)\n\n\ndef get_vllm_config():\n    scheduler_config = SchedulerConfig(\n        max_num_seqs=10,\n        max_num_batched_tokens=512,\n        max_model_len=512,\n    )\n    model_config = ModelConfig(\n        model=\"facebook/opt-125m\",\n        dtype=\"float16\",\n        seed=42,\n    )\n    cache_config = CacheConfig(\n        block_size=BLOCK_SIZE,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=\"auto\",\n    )\n    parallel_config = ParallelConfig()\n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        scheduler_config=scheduler_config,\n        parallel_config=parallel_config,\n    )\n    return vllm_config\n\n\n@pytest.fixture\ndef model_runner():\n    vllm_config = get_vllm_config()\n    model_config = vllm_config.model_config\n    num_heads = model_config.get_num_kv_heads(vllm_config.parallel_config)\n    head_size = model_config.get_head_size()\n    vllm_config.compilation_config.static_forward_context[\n        \"layer.0\"] = Attention(num_heads, head_size, 0.1)\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    initialize_kv_cache(runner)\n    return runner\n\n\nmodel_runner_2 = model_runner\n\n\ndef _schedule_new_request(*req_ids: str) -> SchedulerOutput:\n    new_reqs = []\n    num_scheduled_tokens = {}\n    total_num_scheduled_tokens = 0\n    for req_id in req_ids:\n        new_reqs.append(\n            NewRequestData(\n                req_id=req_id,\n                prompt_token_ids=[1, 2, 3],\n                mm_kwargs=[],\n                mm_hashes=[],\n                mm_positions=[],\n                sampling_params=SamplingParams(),\n                pooling_params=None,\n                block_ids=([0], ),\n                num_computed_tokens=0,\n                lora_request=None,\n            ))\n        num_scheduled_tokens[req_id] = 3\n        total_num_scheduled_tokens += num_scheduled_tokens[req_id]\n\n    return SchedulerOutput(\n        scheduled_new_reqs=new_reqs,\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens=num_scheduled_tokens,\n        total_num_scheduled_tokens=total_num_scheduled_tokens,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n\ndef _is_req_scheduled(model_runner, req_id: str) -> bool:\n    return req_id in model_runner.input_batch.req_id_to_index\n\n\ndef _is_req_added(model_runner, req_id: str) -> bool:\n    return req_id in model_runner.requests\n\n\ndef _is_sampling_metadata_changed(model_runner,\n                                  sampling_metadata_before: SamplingMetadata):\n    return model_runner.input_batch.sampling_metadata is not (\n        sampling_metadata_before)\n\n\ndef _is_req_state_block_table_match(model_runner, req_id: str) -> bool:\n    req_index = model_runner.input_batch.req_id_to_index[req_id]\n    block_table = model_runner.input_batch.block_table[0]\n    req_state = model_runner.requests[req_id]\n    if block_table.num_blocks_per_row[req_index] != len(\n            req_state.block_ids[0]):\n        return False\n    num_blocks = block_table.num_blocks_per_row[req_index]\n    return (block_table.block_table_np[req_index, :num_blocks] ==\n            req_state.block_ids[0]).all()\n\n\ndef test_update_states_new_request(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_update_states_request_finished(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # finish req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={},\n        total_num_scheduled_tokens=0,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids={req_id},\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert not _is_req_added(model_runner, req_id)\n    assert not _is_req_scheduled(model_runner, req_id)\n\n\ndef test_update_states_request_resumed(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # unschedule req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={},\n        total_num_scheduled_tokens=0,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert not _is_req_scheduled(model_runner, req_id)\n\n    # resume req\n    cached_req_data = CachedRequestData(\n        req_ids=[req_id],\n        resumed_from_preemption=[False],\n        new_token_ids=[[]],\n        new_block_ids=([[0]], ),\n        num_computed_tokens=[0],\n    )\n\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=cached_req_data,\n        num_scheduled_tokens={req_id: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_get_nans_in_logits(model_runner, dist_init):\n    req_ids = (\"req_0\", \"req_1\")\n\n    scheduler_output = _schedule_new_request(*req_ids)\n    model_runner._update_states(scheduler_output)\n\n    logits = torch.tensor([\n        [1.0, 2.0, 3.0],\n        [3.0, 2.0, 1.0],\n    ], device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 0, \"req_1\": 0}\n\n    logits = torch.tensor([\n        [1.0, float('nan'), 3.0],\n        [4.0, float('nan'), float('nan')],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 1, \"req_1\": 2}\n\n    logits = torch.tensor([\n        [1.0, 2.0, 3.0],\n        [4.0, float('nan'), float('nan')],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {\"req_0\": 0, \"req_1\": 2}\n\n    result = model_runner._get_nans_in_logits(logits=None)\n    assert result == {\"req_0\": 0, \"req_1\": 0}\n\n    logits = torch.tensor([\n        [1.0, float('nan'), 3.0],\n    ], device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {'req_0': 1, 'req_1': 0}\n\n    logits = torch.tensor([\n        [float('nan'), float('nan'), 2.0],\n        [1.0, 2.0, 3.0],\n        [float('nan'), 2.0, 3.0],\n    ],\n                          device=DEVICE)\n    result = model_runner._get_nans_in_logits(logits)\n    assert result == {'req_0': 2, 'req_1': 0}\n\n\ndef test_update_states_no_changes(model_runner, dist_init):\n    req_id = \"req_0\"\n\n    # new req\n    scheduler_output = _schedule_new_request(req_id)\n\n    model_runner._update_states(scheduler_output)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n\n    # schedule req\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={req_id: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner.input_batch.sampling_metadata\n    model_runner._update_states(scheduler_output)\n    assert not _is_sampling_metadata_changed(model_runner, metadata_before)\n    assert _is_req_added(model_runner, req_id)\n    assert _is_req_scheduled(model_runner, req_id)\n    assert _is_req_state_block_table_match(model_runner, req_id)\n\n\ndef test_update_states_request_unscheduled(model_runner, dist_init):\n    req_ids = (\"req_0\", \"req_1\")\n\n    # new reqs\n    scheduler_output = _schedule_new_request(*req_ids)\n\n    model_runner._update_states(scheduler_output)\n\n    assert _is_req_added(model_runner, req_ids[0])\n    assert _is_req_scheduled(model_runner, req_ids[0])\n\n    assert _is_req_added(model_runner, req_ids[1])\n    assert _is_req_scheduled(model_runner, req_ids[1])\n\n    # unschedule req_1\n    scheduler_output = SchedulerOutput(\n        scheduled_new_reqs=[],\n        scheduled_cached_reqs=CachedRequestData.make_empty(),\n        num_scheduled_tokens={req_ids[0]: 1},\n        total_num_scheduled_tokens=1,\n        scheduled_spec_decode_tokens={},\n        scheduled_encoder_inputs={},\n        num_common_prefix_blocks=0,\n        finished_req_ids=set(),\n        free_encoder_mm_hashes=[],\n        structured_output_request_ids={},\n        grammar_bitmask=None,\n    )\n\n    metadata_before = model_runner._update_states(scheduler_output)\n    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n\n    assert _is_req_added(model_runner, req_ids[0])\n    assert _is_req_scheduled(model_runner, req_ids[0])\n\n    assert _is_req_added(model_runner, req_ids[1])\n    assert not _is_req_scheduled(model_runner, req_ids[1])\n\n\ndef test_kv_cache_stride_order(monkeypatch, model_runner):\n    # This test checks if GPUModelRunner initializes correctly when an attention\n    # backend enforces a non-default KV cache stride order.\n    n_heads = model_runner.model_config.get_num_kv_heads(\n        model_runner.parallel_config)\n    expected_kv_cache_shape = [\n        2, NUM_BLOCKS, BLOCK_SIZE, n_heads,\n        model_runner.model_config.get_head_size()\n    ]\n    # TODO mla test\n    default_stride = list(range(5))\n    # Permutation that gets you back to expected kv shape\n    rnd_stride = tuple(random.sample(default_stride, len(default_stride)))\n\n    def rnd_stride_order():\n        return rnd_stride\n\n    # Patch the attention backend class and re-trigger the KV cache creation.\n    for attn_group in model_runner._attn_group_iterator():\n        attn_backend = attn_group.backend\n        monkeypatch.setattr(attn_backend, \"get_kv_cache_stride_order\",\n                            rnd_stride_order)\n\n    model_runner.attn_groups = []\n    model_runner.initialize_kv_cache(model_runner.kv_cache_config)\n\n    # Shape is unchanged, but layout may differ\n    kv_cache_shape = model_runner.kv_caches[0].shape\n    assert list(kv_cache_shape) == expected_kv_cache_shape\n    if default_stride == rnd_stride:\n        assert all(kv.is_contiguous() for kv in model_runner.kv_caches)\n    else:\n        assert all(not kv.is_contiguous() for kv in model_runner.kv_caches)\n\n\ndef test_update_config(model_runner):\n    # Simple update\n    model_runner.update_config({\"load_config\": {\"load_format\": \"dummy\"}})\n    assert model_runner.load_config.load_format == \"dummy\"\n    # Raise error on non-existing config\n    with pytest.raises(AssertionError):\n        model_runner.update_config({\"do_not_exist_config\": \"dummy\"})\n\n\ndef test_load_model_weights_inplace(dist_init, model_runner, model_runner_2):\n    # In this test, model_runner loads model + weights in one go, while\n    # model_runner_2 loads dummy weights first then load real weights inplace\n    model_runner.load_model()\n    original_load_format = model_runner_2.load_config.load_format\n    model_runner_2.update_config({\"load_config\": {\"load_format\": \"dummy\"}})\n    model_runner_2.load_model()  # Initial model loading with dummy weights\n    assert str(model_runner.get_model().state_dict()) != str(\n        model_runner_2.get_model().state_dict())\n    model_runner_2.update_config(\n        {\"load_config\": {\n            \"load_format\": original_load_format\n        }})\n    model_runner_2.reload_weights()  # Load real weights inplace\n    assert str(model_runner.get_model().state_dict()) == str(\n        model_runner_2.get_model().state_dict())\n\n\ndef test_reload_weights_before_load_model(model_runner):\n    with pytest.raises(AssertionError):\n        model_runner.reload_weights()\n\n\ndef test_init_kv_cache_with_kv_sharing_invalid_target_layer_order():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    error_msg = f\"{layer_1} must come before the current layer\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            # initialization below will fail because target layer is invalid;\n            # the target layer needs to come before layer 1\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n                kv_sharing_target_layer_name=layer_1,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_with_kv_sharing_target_layer_not_exist():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    invalid_layer = \"model.layers.0.cross_attn.attn\"\n    error_msg = f\"{invalid_layer} is not a valid Attention layer in the model\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                # invalid layer: cross_attn.atn doesn't exist!\n                kv_sharing_target_layer_name=invalid_layer,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_with_kv_sharing_target_same_as_current():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    error_msg = f\"{layer_1} cannot be the same as the current layer\"\n    with pytest.raises(ValueError, match=error_msg):\n        fwd_context = {\n            # initialization below will fail because target layer is invalid;\n            # the target layer needs to come before layer 1\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                kv_sharing_target_layer_name=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n\n\ndef test_init_kv_cache_without_kv_sharing():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    vllm_config = get_vllm_config()\n    with set_current_vllm_config(vllm_config):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n    # Set high context length to test max context length estimation\n    vllm_config.model_config.max_model_len = 3_000_000\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    kv_cache_spec = runner.get_kv_cache_spec()\n    assert len(kv_cache_spec) == 2\n    assert len(runner.shared_kv_cache_layers) == 0\n\n    available_memory = 20 * GiB_bytes\n    # page size for layer 0's kv_cache_spec is 32KB\n    num_expected_blocks = 327680  # 20GB / 32KB / 2 (num layers)\n    kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                          available_memory)\n    assert kv_cache_config.num_blocks == num_expected_blocks\n    assert len(kv_cache_config.kv_cache_tensors) == 2\n    assert kv_cache_config.kv_cache_tensors[0].size == available_memory // 2\n    assert kv_cache_config.kv_cache_tensors[1].size == available_memory // 2\n\n    max_context_len =\n        estimate_max_model_len(vllm_config, kv_cache_spec, 5 * GiB_bytes)\n    # max context len with KV sharing should be 2x as large as without\n    assert max_context_len == 1310720\n\n    # important: override tensor size to prevent large mem alloc during test\n    # this will only allocate 2 block worth of memory (2 * 32kb)\n    kv_cache_config.num_blocks = 1\n    for kv_cache_tensor in kv_cache_config.kv_cache_tensors:\n        kv_cache_tensor.size = (\n            kv_cache_spec[kv_cache_tensor.shared_by[0]].page_size_bytes)\n\n    runner.initialize_kv_cache(kv_cache_config)\n\n    layer_0_kv = vllm_ctx[layer_0].kv_cache[0]\n    layer_1_kv = vllm_ctx[layer_1].kv_cache[0]\n    # check layer 1 kv cache does NOT share memory with layer 0\n    assert id(layer_1_kv) != id(layer_0_kv)\n\n    # check layer 1 added to kv cache group's layer names\n    assert len(kv_cache_config.kv_cache_groups) == 1\n    assert len(kv_cache_config.kv_cache_groups[0].layer_names) == 2\n    assert kv_cache_config.kv_cache_groups[0].layer_names[0] == layer_0\n    assert kv_cache_config.kv_cache_groups[0].layer_names[1] == layer_1\n\n\ndef test_init_kv_cache_with_kv_sharing_valid():\n    torch.set_default_dtype(torch.float16)\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    vllm_config = get_vllm_config()\n    with set_current_vllm_config(vllm_config):\n        fwd_context = {\n            layer_0:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_0,\n            ),\n            layer_1:\n            Attention(\n                num_heads=8,\n                head_size=64,\n                scale=1.0,\n                prefix=layer_1,\n                kv_sharing_target_layer_name=\"model.layers.0.self_attn.attn\",\n            )\n        }\n        # suppress var not used error\n        assert fwd_context is not None\n    # Set high context length to test max context length estimation\n    vllm_config.model_config.max_model_len = 3_000_000\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n    runner = GPUModelRunner(vllm_config, DEVICE)\n    kv_cache_spec = runner.get_kv_cache_spec()\n    assert len(kv_cache_spec) == 1\n    assert layer_0 in kv_cache_spec\n    assert runner.shared_kv_cache_layers[layer_1] == layer_0\n\n    available_memory = 20 * GiB_bytes\n    # page size for layer 0's kv_cache_spec is 32KB\n    # with KV sharing, we can allocate (available_mem//page_size//1) blocks\n    # which is twice as many as without KV sharing\n    num_expected_blocks = 655360  # 20GB / 32KB\n    kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                          available_memory)\n    assert kv_cache_config.num_blocks == num_expected_blocks\n    assert len(kv_cache_config.kv_cache_tensors) == 1\n    # Each layer now has twice the available memory for KV cache\n    # compared to no KV sharing\n    assert kv_cache_config.kv_cache_tensors[0].size == available_memory\n\n    max_context_len =\n        estimate_max_model_len(vllm_config, kv_cache_spec, 5 * GiB_bytes)\n    # max context len with KV sharing should be 2x as large as without\n    assert max_context_len == 2 * 1310720\n\n    # important: override tensor size to prevent large mem alloc during test\n    # this will only allocate 1 block worth of memory (32kb)\n    kv_cache_config.num_blocks = 1\n    kv_cache_config.kv_cache_tensors[0].size =\n        kv_cache_spec[layer_0].page_size_bytes\n\n    runner.initialize_kv_cache(kv_cache_config)\n    kv_cache_config_after_init = runner.kv_cache_config\n\n    layer_0_kv = vllm_ctx[layer_0].kv_cache[0]\n    layer_1_kv = vllm_ctx[layer_1].kv_cache[0]\n    # check layer 1 kv cache shares memory with layer 0\n    assert id(layer_1_kv) == id(layer_0_kv)\n\n    # check layer 1 added to kv cache group's layer names\n    assert len(kv_cache_config_after_init.kv_cache_groups) == 1\n    assert len(kv_cache_config_after_init.kv_cache_groups[0].layer_names) == 2\n    assert kv_cache_config_after_init.kv_cache_groups[0].layer_names[\n        0] == layer_0\n    assert kv_cache_config_after_init.kv_cache_groups[0].layer_names[\n        1] == layer_1\n\n\ndef test_hybrid_attention_mamba_tensor_shapes(monkeypatch):\n    '''\n    The GPU model runner creates different views into the\n    KVCacheTensors for the attention and mamba layers\n    (via _reshape_kv_cache_tensors function). This test verifies\n    that the views are compatible: writing a mamba block\n    will not corrupt an attention block and vice-versa\n    '''\n\n    current_platform.seed_everything(42)\n\n    update_environment_variables({\n        'RANK': \"0\",\n        'LOCAL_RANK': \"0\",\n        'WORLD_SIZE': \"1\",\n        'MASTER_ADDR': 'localhost',\n        'MASTER_PORT': '12345',\n    })\n    init_distributed_environment()\n    initialize_model_parallel(tensor_model_parallel_size=1)\n    torch.set_default_dtype(torch.float16)\n\n    scheduler_config = SchedulerConfig(\n        max_num_seqs=10,\n        max_num_batched_tokens=512,\n        max_model_len=512,\n    )\n    model_config = ModelConfig(\n        model=\"ibm-granite/granite-4.0-tiny-preview\",\n        dtype=\"float16\",\n    )\n    cache_config = CacheConfig(\n        block_size=BLOCK_SIZE,\n        gpu_memory_utilization=0.9,\n        swap_space=0,\n        cache_dtype=\"auto\",\n    )\n    parallel_config = ParallelConfig()\n    vllm_config = VllmConfig(\n        model_config=model_config,\n        cache_config=cache_config,\n        scheduler_config=scheduler_config,\n        parallel_config=parallel_config,\n    )\n\n    layer_0 = \"model.layers.0.self_attn.attn\"\n    layer_1 = \"model.layers.1.self_attn.attn\"\n    layer_2 = \"model.layers.2.mixer\"\n    layer_3 = \"model.layers.3.mixer\"\n    layer_4 = \"model.layers.4.mixer\"\n    layer_5 = \"model.layers.5.mixer\"\n\n    with set_current_vllm_config(vllm_config), monkeypatch.context() as m:\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLASHINFER\")\n        hf_config = vllm_config.model_config.hf_config\n        fwd_context = {}\n        for key in [layer_0, layer_1]:\n            fwd_context[key] = Attention(\n                num_heads=model_config.get_num_attention_heads(\n                    parallel_config),\n                num_kv_heads=model_config.get_num_kv_heads(parallel_config),\n                head_size=model_config.get_head_size(),\n                scale=1.0,\n                prefix=key,\n            )\n        for key in [layer_2, layer_3, layer_4, layer_5]:\n            fwd_context[key] = MambaMixer2(\n                hidden_size = hf_config.hidden_size,\n                ssm_state_size = hf_config.mamba_d_state,\n                conv_kernel_size = hf_config.mamba_d_conv,\n                intermediate_size = hf_config.mamba_expand *\n                                    hf_config.hidden_size,\n                use_conv_bias = hf_config.mamba_conv_bias,\n                use_bias = hf_config.mamba_proj_bias,\n                n_groups=hf_config.mamba_n_groups,\n                num_heads=hf_config.mamba_n_heads,\n                head_dim=hf_config.mamba_d_head,\n                rms_norm_eps=hf_config.rms_norm_eps,\n                activation=hf_config.hidden_act,\n                cache_config=cache_config,\n                model_config=model_config,\n                prefix=key,\n            )\n        # suppress var not used error\n        assert fwd_context is not None\n    vllm_ctx = vllm_config.compilation_config.static_forward_context\n\n    with monkeypatch.context() as m:\n\n        m.setenv(\"VLLM_ATTENTION_BACKEND\", \"FLASHINFER\")\n\n        runner = GPUModelRunner(vllm_config, DEVICE)\n        kv_cache_spec = runner.get_kv_cache_spec()\n\n        available_memory = 5 * GiB_bytes\n        kv_cache_config = get_kv_cache_config(vllm_config, kv_cache_spec,\n                                              available_memory)\n        runner.initialize_kv_cache(kv_cache_config)\n\n        # random partition of blocks\n        # blocks0 will be assigned to attention layers\n        # blocks1 will be assigned to mamba layers\n        num_blocks = kv_cache_config.num_blocks\n        ind = np.arange(num_blocks)\n        np.random.shuffle(ind)\n        blocks0, blocks1 = ind[:(num_blocks // 2)], ind[(num_blocks // 2):]\n\n        attn_shape = vllm_ctx[layer_0].kv_cache[0].shape\n        conv_shape = vllm_ctx[layer_2].kv_cache[0][0].shape\n        ssm_shape = vllm_ctx[layer_2].kv_cache[0][1].shape\n\n        # assert we are using FlashInfer\n        assert attn_shape[0] == num_blocks\n\n        attn_blocks_constant = torch.full((len(blocks0), *attn_shape[1:]),\n                                          device=DEVICE,\n                                          fill_value=3.33)\n        conv_blocks_constant = torch.full((len(blocks1), *conv_shape[1:]),\n                                          device=DEVICE,\n                                          fill_value=6.66)\n        ssm_blocks_constant = torch.full((len(blocks1), *ssm_shape[1:]),\n                                         device=DEVICE,\n                                         fill_value=9.99)\n\n        # fill all attention blocks with constant\n        for layer in [layer_0, layer_1]:\n            vllm_ctx[layer].kv_cache[0][\n                blocks0, :] = attn_blocks_constant.detach().clone()\n\n        # fill all mamba blocks with constant\n        for layer in [layer_2, layer_3, layer_4, layer_5]:\n            vllm_ctx[layer].kv_cache[0][0][\n                blocks1, :] = conv_blocks_constant.detach().clone()\n            vllm_ctx[layer].kv_cache[0][1][\n                blocks1, :] = ssm_blocks_constant.detach().clone()\n\n        # verify attention and mamba contents are correct\n        for layer in [layer_0, layer_1]:\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][blocks0, :],\n                               attn_blocks_constant)\n        for layer in [layer_2, layer_3, layer_4, layer_5]:\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][0][blocks1, :],\n                               conv_blocks_constant)\n            assert torch.equal(vllm_ctx[layer].kv_cache[0][1][blocks1, :],\n                               ssm_blocks_constant)\n"
  ],
  "duration_changes": [
    {
      "base": [
        Infinity
      ],
      "head": [
        Infinity
      ],
      "main": [
        Infinity
      ]
    }
  ],
  "human_performance": NaN,
  "version": "python==unknown;arch=x86_64;image=local;install_sha=na",
  "patch_functions": null,
  "test_functions": [],
  "api": null,
  "gt_commit_message": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "setup_commands": null,
  "install_commands": null,
  "notes": null,
  "api_manifest_paths": {
    "base": "/root/OmniPerf-Bench/.test_work/api_manifests/d263bd9d_vllm_api.json",
    "head": "/root/OmniPerf-Bench/.test_work/api_manifests/25ebed2f_vllm_api.json",
    "main": "/root/OmniPerf-Bench/.test_work/api_manifests/5490d633_vllm_api.json"
  },
  "api_manifest_summaries": {
    "base": {
      "modules_scanned": 194,
      "symbols_collected": 3668,
      "errors": [
        {
          "stage": "import_module",
          "module": "vllm.attention.backends.flashinfer",
          "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.nki_flash_attn",
          "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.pallas_kv_cache_update",
          "error": "ModuleNotFoundError(\"No module named 'jax'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.rocm_aiter_paged_attn",
          "error": "ModuleNotFoundError(\"No module named 'aiter'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector",
          "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.tpu_distributed_utils",
          "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"
        }
      ]
    },
    "head": {
      "modules_scanned": 194,
      "symbols_collected": 3668,
      "errors": [
        {
          "stage": "import_module",
          "module": "vllm.attention.backends.flashinfer",
          "error": "ImportError('FlashInfer is not installed. Please install it from https://github.com/flashinfer-ai/flashinfer')"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.nki_flash_attn",
          "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.pallas_kv_cache_update",
          "error": "ModuleNotFoundError(\"No module named 'jax'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.rocm_aiter_paged_attn",
          "error": "ModuleNotFoundError(\"No module named 'aiter'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector",
          "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.tpu_distributed_utils",
          "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"
        }
      ]
    },
    "main": {
      "modules_scanned": 194,
      "symbols_collected": 3693,
      "errors": [
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.nki_flash_attn",
          "error": "ModuleNotFoundError(\"No module named 'neuronxcc'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.pallas_kv_cache_update",
          "error": "ModuleNotFoundError(\"No module named 'jax'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.attention.ops.rocm_aiter_paged_attn",
          "error": "ModuleNotFoundError(\"No module named 'aiter'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.device_communicators.tpu_communicator",
          "error": "ModuleNotFoundError(\"No module named 'tpu_info'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector",
          "error": "ModuleNotFoundError(\"No module named 'lmcache'\")"
        },
        {
          "stage": "import_module",
          "module": "vllm.distributed.tpu_distributed_utils",
          "error": "ModuleNotFoundError(\"No module named 'torch_xla'\")"
        }
      ]
    }
  },
  "test_failed": "Yes"
}