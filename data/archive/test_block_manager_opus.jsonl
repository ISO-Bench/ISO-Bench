{"repo": "OmniPerf-Bench/vllm", "instance_id": "OmniPerf-Bench__vllm-2deb029", "created_at": "2024-08-26T18:24:53+00:00", "base_commit": "029c71de11bc3bcf84a1b3cf9d91e79ab6949799", "head_commit": "2deb029d115dadd012ce5ea70487a207cb025493", "patch": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..25be2dd13 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -708,6 +708,37 @@ class TestPrefixCachingBlockAllocator:\n                                                token_ids=token_ids)\n         assert allocator.get_prefix_cache_hit_rate() > 0.99\n \n+    # Test case for marking cache hit blocks as computed right after\n+    # a batch of prefill sequences are scheduled.\n+    @staticmethod\n+    def test_touch_block():\n+        block_size = 16\n+        common_blocks = 4\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+\n+        common_token_ids = list(range(block_size * common_blocks))\n+\n+        # Mimic the behavior of allocating the same block chain\n+        # (i.e., common prefix) for a batch of 3 different prefill sequences.\n+        for _ in range(3):\n+            blocks = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+                block_size=block_size,\n+                token_ids=common_token_ids,\n+                allocator=allocator,\n+            )\n+            block_ids = [block.block_id for block in blocks]\n+            # The allocated blocks should  be marked as touched\n+            # but not computed.\n+            computed_block_ids = allocator.get_computed_block_ids(\n+                [], block_ids, skip_last_block_id=False)\n+            assert len(computed_block_ids) == 0\n+\n+        allocator.mark_blocks_as_computed([])\n+        computed_block_ids = allocator.get_computed_block_ids(\n+            [], block_ids, skip_last_block_id=False)\n+        assert len(computed_block_ids) == common_blocks\n+\n     @staticmethod\n     def create_immutable_chain(\n         block_size: int,\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..a87e814cf 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -1,6 +1,6 @@\n \"\"\"Token blocks.\"\"\"\n from os.path import commonprefix\n-from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n+from typing import Dict, FrozenSet, Iterable, List, Optional, Set, Tuple\n \n from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,\n                                     get_all_blocks_recursively)\n@@ -73,6 +73,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # prefix hash will be in this dict, even if they have refcount 0.\n         self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n \n+        # A list of immutable block IDs that have been touched by scheduler\n+        # and should be marked as computed after an entire batch of sequences\n+        # are scheduled.\n+        self._touched_blocks: Set[BlockId] = set()\n+\n         # Used to track status of each physical block id\n         self._block_tracker: Dict[BlockId, BlockTracker] = {}\n         for block_id in block_ids:\n@@ -438,10 +443,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert self._refcounter.get(block.block_id) > 0\n \n         if block.content_hash not in self._cached_blocks:\n-            # No cached content hash => Set this block as cached\n-            # (Note that this block is not computed yet =>\n-            #  Will be computed after free())\n+            # No cached content hash => Set this block as cached.\n+            # Note that this block cannot be marked as computed yet\n+            # because other sequences in the same batch cannot reuse\n+            # this block.\n             self._cached_blocks[block.content_hash] = block.block_id\n+            # Mark this block as touched so that it can be marked as\n+            # computed after the entire batch of sequences are scheduled.\n+            self._touched_blocks.add(block.block_id)\n             return block.block_id\n \n         # Reuse the cached content hash\n@@ -507,7 +516,10 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        # Mark all touched blocks as computed.\n+        for block_id in self._touched_blocks:\n+            self._block_tracker[block_id].computed = True\n+        self._touched_blocks.clear()\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..7d4919a0d 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,11 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        # If prefix caching is enabled, mark immutable blocks as computed\n+        # right after they have been scheduled (for prefill). This assumes\n+        # the scheduler is synchronous so blocks are actually computed when\n+        # scheduling the next batch.\n+        self.block_allocator.mark_blocks_as_computed([])\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:", "test_patch": "", "efficiency_test": ["import torch\nimport numpy as np\nfrom typing import Dict, Any, Tuple, List\n\n# Import the actual optimized modules from the commit\ntry:\n    # Core allocator implementing prefix caching and the new touched/computed logic\n    from vllm.core.block.prefix_caching_block import PrefixCachingBlockAllocator\nexcept Exception as e:\n    print(f\"Required vLLM modules not available or import failed: {e}\")\n    raise\n\n# Helper to create a chain of immutable blocks (replicates the test helper pattern)\ndef _create_immutable_chain(\n    block_size: int,\n    token_ids: List[int],\n    allocator: PrefixCachingBlockAllocator,\n):\n    blocks = []\n    num_blocks = (len(token_ids) + block_size - 1) // block_size\n    prev_block = None\n    for i in range(num_blocks):\n        block_token_ids = token_ids[i * block_size:(i + 1) * block_size]\n        prev_block = allocator.allocate_immutable_block(\n            prev_block=prev_block, token_ids=block_token_ids\n        )\n        blocks.append(prev_block)\n    return blocks\n\n\ndef setup() -> Dict[str, Any]:\n    \"\"\"Create realistic workload that exercises the optimization.\"\"\"\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    # Based on commit tests: block_size 16, and reuse the same prefix across a batch.\n    block_size = 16\n    # Choose a reasonably large number of common blocks to stress the touched->computed path.\n    # Keep within allocator capacity while allowing room for management structures.\n    common_blocks = 2048  # Length of common prefix (number of blocks)\n    num_blocks = common_blocks + 64  # total physical blocks in allocator\n\n    allocator = PrefixCachingBlockAllocator(num_blocks=num_blocks, block_size=block_size)\n\n    # Create token ids that form a single long chain of full blocks\n    common_token_ids = list(range(block_size * common_blocks))\n\n    # Mimic allocating the same chain for a batch of 3 sequences (as in new test case).\n    # The first allocation will populate _touched_blocks with the new block_ids.\n    # Subsequent allocations will reuse the cached blocks and mark them computed per-block.\n    last_blocks = None\n    for _ in range(3):\n        last_blocks = _create_immutable_chain(\n            block_size=block_size,\n            token_ids=common_token_ids,\n            allocator=allocator,\n        )\n\n    assert last_blocks is not None\n    block_ids = [b.block_id for b in last_blocks]\n    # Sanity: ensure no None ids\n    assert all(bid is not None for bid in block_ids)\n\n    # At this point:\n    # - allocator._touched_blocks contains the unique block_ids introduced by the first sequence\n    # - Calling mark_blocks_as_computed([]) will mark them computed and clear touched set\n    #\n    # For repeated performance iterations, we will repopulate the touched set with the same block ids\n    # to isolate the cost of the new path.\n\n    return {\n        'allocator': allocator,\n        'block_ids': block_ids,\n        'block_size': block_size,\n        'common_blocks': common_blocks,\n        'num_blocks': num_blocks,\n    }\n\n\ndef experiment(data: Dict[str, Any]) -> Any:\n    \"\"\"\n    Execute the performance-critical code path being optimized.\n\n    This focuses on the new/changed code path:\n      - PrefixCachingBlockAllocator.mark_blocks_as_computed\n    We repopulate allocator._touched_blocks with a large set of block_ids,\n    then call mark_blocks_as_computed([]) to exercise the marking path.\n    \"\"\"\n    allocator: PrefixCachingBlockAllocator = data['allocator']\n    block_ids: List[int] = data['block_ids']\n\n    # Repopulate touched set to simulate \"after schedule\" state for a batch.\n    # This mirrors the behavior where blocks promoted during scheduling are\n    # recorded and later marked as computed en-masse.\n    allocator._touched_blocks.update(block_ids)  # type: ignore[attr-defined]\n\n    # Mark all touched blocks as computed (the operation added by this commit).\n    allocator.mark_blocks_as_computed([])\n\n    # Optionally, verify that these blocks are now considered computed by querying once.\n    # This also returns a stable result for equivalence checking.\n    computed_block_ids = allocator.get_computed_block_ids(\n        prev_computed_block_ids=[],\n        block_ids=block_ids,\n        skip_last_block_id=False,\n    )\n    return torch.tensor(computed_block_ids, dtype=torch.int64)\n\n\ndef store_result(result: Any, filepath: str) -> None:\n    \"\"\"Store result for future equivalence checking.\"\"\"\n    if isinstance(result, torch.Tensor):\n        torch.save({\n            'type': 'tensor',\n            'shape': tuple(result.shape),\n            'dtype': str(result.dtype),\n            'device': str(result.device),\n            'result': result.cpu(),\n        }, filepath)\n    else:\n        torch.save({'type': 'object', 'result': result}, filepath)\n\n\ndef load_result(filepath: str) -> Any:\n    \"\"\"Load reference result for equivalence checking.\"\"\"\n    data = torch.load(filepath, map_location='cpu')\n    if data.get('type') == 'tensor':\n        return data['result']\n    return data['result']\n\n\ndef check_equivalence(current_result: Any, reference_result: Any) -> None:\n    \"\"\"Check that current result is equivalent to reference.\"\"\"\n    if isinstance(current_result, torch.Tensor) and isinstance(reference_result, torch.Tensor):\n        assert current_result.shape == reference_result.shape, \\\n            f\"Shape mismatch: {current_result.shape} vs {reference_result.shape}\"\n        assert current_result.dtype == reference_result.dtype, \\\n            f\"Dtype mismatch: {current_result.dtype} vs {reference_result.dtype}\"\n        # Values must match exactly (these are integer block IDs).\n        torch.testing.assert_close(current_result, reference_result, rtol=0, atol=0)\n    else:\n        assert current_result == reference_result, \"Non-tensor results differ\"\n\n\ndef run_test(eqcheck: bool = False, reference: bool = False, prefix: str = '') -> float:\n    \"\"\"\n    Run the performance test and return average execution time in milliseconds.\n\n    Args:\n        eqcheck: Whether to perform equivalence checking.\n        reference: Whether to store result as reference.\n        prefix: Prefix for reference files.\n\n    Returns:\n        Average execution time in milliseconds.\n    \"\"\"\n    data = setup()\n\n    # CPU timing (allocator logic runs on CPU)\n    import time\n    num_iterations = 200  # iterations to achieve stable timing\n\n    # Warmup\n    for _ in range(5):\n        _ = experiment(data)\n\n    start_time = time.time()\n    for _ in range(num_iterations):\n        result = experiment(data)\n    end_time = time.time()\n    elapsed_time_ms = (end_time - start_time) * 1000.0\n\n    # Equivalence handling on a fresh run\n    if eqcheck or reference:\n        result = experiment(data)\n        if reference:\n            store_result(result, f'{prefix}_reference.pt')\n        elif eqcheck:\n            reference_result = load_result(f'{prefix}_reference.pt')\n            check_equivalence(result, reference_result)\n\n    return elapsed_time_ms / num_iterations\n\n# The harness will be automatically appended here with argument parsing"], "duration_changes": [{"base": [Infinity], "head": [Infinity], "main": [Infinity]}], "human_performance": NaN, "version": "python==unknown;arch=x86_64;image=local;install_sha=na", "patch_functions": null, "test_functions": [], "api": null, "gt_commit_message": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)", "setup_commands": null, "install_commands": null, "notes": null}
