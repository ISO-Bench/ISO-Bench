# 39 Benchmarkable vLLM Commits
# Source: commit_v5.md (git commit 949c1702)
# Generated: 2026-01-27

# Format: commit_hash | date | subject

ad8d696a | 2024-04-22 | [Core] Scheduler perf fix (#4270)
d7740ea4 | 2024-05-08 | [Core] Optimize sampler get_logprobs (#4594)
660470e5 | 2024-08-06 | [Core] Optimize evictor-v2 performance (#7193)
89a84b0b | 2024-07-25 | [Core] Use array to speedup padding (#6779)
3476ed08 | 2024-07-01 | [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)
19d98e0c | 2025-03-03 | [Kernel] Optimize moe intermediate_cache usage (#13625)
fa63e710 | 2025-01-26 | [V1][Perf] Reduce scheduling overhead in model runner after cuda sync
b690e348 | 2025-08-02 | [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead
6e36f4fa | 2024-09-02 | improve chunked prefill performance
fc7b8d1e | 2024-08-09 | [Performance] e2e overheads reduction: Small followup diff (#7364)
30172b49 | 2025-02-18 | [V1] Optimize handling of sampling metadata and req_ids list (#13244)
2deb029d | 2024-08-26 | [Performance][BlockManagerV2] Mark prefix cache block as computed after schedule
4c822298 | 2025-02-18 | [V1][Spec Decode] Optimize N-gram matching with Numba (#13365)
b55ed6ef | 2025-01-02 | [V1][Minor] Optimize token_ids_cpu copy (#11692)
015069b0 | 2025-05-01 | [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content
e7b20426 | 2025-07-21 | Revert "[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE"
35fad35a | 2025-03-26 | [V1][Sampler] Faster top-k only implementation (#15478)
a3223766 | 2025-07-22 | [Core] Optimize update checks in LogitsProcessor (#21245)
99abb8b6 | 2025-03-18 | [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels
310aca88 | 2025-01-09 | [perf]fix current stream (#11870)
58eee5f2 | 2025-08-02 | [PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion
9f1710f1 | 2025-03-06 | Fix mla prefill context performance (#13897)
6dd94dbe | 2025-01-24 | [perf] fix perf regression from #12253 (#12380)
8c1e77fb | 2024-11-28 | [Kernel] Update vllm-flash-attn version to reduce CPU overheads
bc7c4d20 | 2025-04-22 | [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)
e3580537 | 2024-08-28 | [Performance] Enable chunked prefill and prefix caching together (#7753)
9474e89b | 2024-03-20 | [PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance
98f47f2a | 2024-11-28 | [V1] Optimize the CPU overheads in FlashAttention custom op (#10733)
fc542144 | 2025-01-31 | [Feature] Fix guided decoding blocking bitmask memcpy (#12563)
3b61cb45 | 2024-12-09 | [V1] Further reduce CPU overheads in flash-attn (#10989)
22d33bac | 2025-03-19 | [FrontEnd][Perf] `merge_async_iterators` fast-path for single-prompt requests
299ebb62 | 2025-04-21 | [Core] Speed up decode by remove synchronizing operation in sampler
9badee53 | 2025-03-04 | Fix performance when `--generation-config` is not `None` (#14223)
9ed82e70 | 2024-07-19 | [Misc] Small perf improvements (#6520)
6a417b86 | 2025-02-20 | fix neuron performance issue (#13589)
296f927f | 2025-03-20 | [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies
e206b543 | 2025-02-26 | [v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine
70b808fe | 2025-03-11 | [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)
fe66b347 | 2025-03-14 | [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies

# --- Excluded commits (14 total) ---
#
# Wrong perf command (9 commits): not listed individually
# Corrupted Docker image (1): 6ce01f30
# RoPE incompatibility (2): 7c01f706, 3a243095
# Non-standard benchmarks (2): ce6bf3a2 (TPU-specific), ccf02fcb (Mamba2/lm_eval)
