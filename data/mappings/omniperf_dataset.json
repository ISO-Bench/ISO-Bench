[
  {
    "commit_hash": "baeded25699f9f4851843306f27f685c4d4ee7c5",
    "commit_short": "baeded25",
    "models": [
      "deepseek-ai/DeepSeek-V3"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model deepseek-ai/DeepSeek-V3 --batch-size 32 --input-len 512 --output-len 128",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)"
  },
  {
    "commit_hash": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "commit_short": "fc542144",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)"
  },
  {
    "commit_hash": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "commit_short": "fa63e710",
    "models": [
      "meta-llama/Llama-3-8B"
    ],
    "hardware": "H100",
    "perf_command": "VLLM_USE_V1=1 python benchmarks/benchmark_latency.py --model meta-llama/Llama-3-8B --tensor-parallel-size 1 --input-len 1000 --batch-size 32",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)"
  },
  {
    "commit_hash": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "commit_short": "6dd94dbe",
    "models": [
      "meta-llama/Meta-Llama-3-8B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[perf] fix perf regression from #12253 (#12380)"
  },
  {
    "commit_hash": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
    "commit_short": "aea94362",
    "models": [
      "meta-llama/Llama-3.2-1B-Instruct"
    ],
    "hardware": "A100",
    "perf_command": "python benchmarks/benchmark_serving.py --backend vllm --model meta-llama/Llama-3.2-1B-Instruct --dataset-name sharegpt --num-prompts 6000 --request-rate inf --max-concurrency 400",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Frontend][V1] Online serving performance improvements (#12287)"
  },
  {
    "commit_hash": "310aca88c984983189a57f1b72e3b1dde89fb92f",
    "commit_short": "310aca88",
    "models": [
      "meta-llama/Meta-Llama-3-70B"
    ],
    "hardware": "H100-TP4",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[perf]fix current stream (#11870)"
  },
  {
    "commit_hash": "526de822d501c792b051c864ba873a836d78d5bf",
    "commit_short": "526de822",
    "models": [
      "Qwen/Qwen2-7B-Instruct",
      "microsoft/Phi-3-medium-128k-instruct",
      "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "mistralai/Mistral-7B-Instruct-v0.3"
    ],
    "hardware": "AMD-MI300X",
    "perf_command": "python benchmarks/benchmark_latency.py --dtype bfloat16 --enable-chunked-prefill False --load-format dummy --batch-size BS --num-iters-warmup 2 --num-iters 5 --input-len INPUT_LEN --output-len OUTPUT_LEN --model MODEL",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)"
  },
  {
    "commit_hash": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "commit_short": "b55ed6ef",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)"
  },
  {
    "commit_hash": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
    "commit_short": "f26c4aee",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100-TP4",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4 --batch-size 8 --input-len 128 --output-len 256 --distributed-executor-backend ray",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Misc] Optimize ray worker initialization time (#11275)"
  },
  {
    "commit_hash": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "commit_short": "25ebed2f",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)"
  },
  {
    "commit_hash": "886936837ca89e5645bc1f71cc0e1492b65b1590",
    "commit_short": "88693683",
    "models": [
      "meta-llama/Llama-3-8B"
    ],
    "hardware": "A100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --enable-prefix-caching",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion (#7209)"
  },
  {
    "commit_hash": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "commit_short": "f092153f",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)"
  },
  {
    "commit_hash": "3b61cb450d899dc423feb264c297d4d18d701678",
    "commit_short": "3b61cb45",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)"
  },
  {
    "commit_hash": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
    "commit_short": "9323a315",
    "models": [
      "meta-llama/Llama-3.2-3B-Instruct"
    ],
    "hardware": "A100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-3B-Instruct --guided-decoding-backend xgrammar",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)"
  },
  {
    "commit_hash": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "commit_short": "98f47f2a",
    "models": [
      "facebook/opt-125m"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model facebook/opt-125m",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)"
  },
  {
    "commit_hash": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "commit_short": "8c1e77fb",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)"
  },
  {
    "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "commit_short": "b2e0ad3b",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)"
  },
  {
    "commit_hash": "83450458339b07765b0e72a822e5fe93eeaf5258",
    "commit_short": "83450458",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)"
  },
  {
    "commit_hash": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
    "commit_short": "6d646d08",
    "models": [
      "meta-llama/Llama-3-8B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --dataset-name sharegpt --multi-step",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Optimize Async + Multi-step (#8050)"
  },
  {
    "commit_hash": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
    "commit_short": "6e36f4fa",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "improve chunked prefill performance"
  },
  {
    "commit_hash": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "commit_short": "ce6bf3a2",
    "models": [
      "google/gemma-2b"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)"
  },
  {
    "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
    "commit_short": "e3580537",
    "models": [
      "neuralmagic/Meta-Llama-3-8B-Instruct-FP8"
    ],
    "hardware": "L4",
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Performance] Enable chunked prefill and prefix caching together (#7753)"
  },
  {
    "commit_hash": "2deb029d115dadd012ce5ea70487a207cb025493",
    "commit_short": "2deb029d",
    "models": [
      "neuralmagic/Meta-Llama-3-8B-Instruct-FP8"
    ],
    "hardware": "L4",
    "perf_command": "python benchmarks/benchmark_prefix_caching.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --output-len 200 --enable-prefix-caching",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)"
  },
  {
    "commit_hash": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
    "commit_short": "fc7b8d1e",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Performance] e2e overheads reduction: Small followup diff (#7364)"
  },
  {
    "commit_hash": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
    "commit_short": "660470e5",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Optimize evictor-v2 performance (#7193)"
  },
  {
    "commit_hash": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "commit_short": "6ce01f30",
    "models": [
      "meta-llama/Llama-3-8B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Performance] Optimize `get_seqs` (#7051)"
  },
  {
    "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "commit_short": "89a84b0b",
    "models": [
      "Qwen/Qwen1.5-0.5B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Use array to speedup padding (#6779)"
  },
  {
    "commit_hash": "9ed82e7074a18e25680ab106fc846364ad97bc00",
    "commit_short": "9ed82e70",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Misc] Small perf improvements (#6520)"
  },
  {
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "commit_short": "3476ed08",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)"
  },
  {
    "commit_hash": "7c01f706418d593b3cf23d2ec9110dca7151c539",
    "commit_short": "7c01f706",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)"
  },
  {
    "commit_hash": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
    "commit_short": "80aa7e91",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "Qwen/Qwen2.5-7B-Instruct"
    ],
    "hardware": "Intel-CPU",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)"
  },
  {
    "commit_hash": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
    "commit_short": "8d75fe48",
    "models": [
      "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
      "nm-testing/Meta-Llama-3-70B-Instruct-FP8",
      "nm-testing/Meta-Llama-3-8B-Instruct-FP8-KV"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)"
  },
  {
    "commit_hash": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
    "commit_short": "8bc68e19",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)"
  },
  {
    "commit_hash": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
    "commit_short": "379da6dc",
    "models": [
      "meta-llama/Llama-3-70B"
    ],
    "hardware": "H100-TP4",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-70B --dtype float8 --input-len 1000 --output-len 50",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Kernel] [FP8] Improve FP8 linear layer performance (#4691)"
  },
  {
    "commit_hash": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
    "commit_short": "d7740ea4",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.1-8B-Instruct --input-len 256 --output-len 256",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[Core] Optimize sampler get_logprobs (#4594)"
  },
  {
    "commit_hash": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
    "commit_short": "2a052011",
    "models": [
      "nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_throughput.py --model nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#4527)"
  },
  {
    "commit_hash": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
    "commit_short": "ad8d696a",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Scheduler perf fix (#4270)"
  },
  {
    "commit_hash": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "commit_short": "2f192835",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] latency optimization (#3890)"
  },
  {
    "commit_hash": "b6d103542c654fb63013a1e45a586d654ae36a2a",
    "commit_short": "b6d10354",
    "models": [
      "meta-llama/Llama-2-70b-hf"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-2-70b-hf --dtype float16 --tensor-parallel-size 1",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Kernel] Layernorm performance optimization (#3662)"
  },
  {
    "commit_hash": "3a243095e5e7b655b63ab08fbd5936cb40850415",
    "commit_short": "3a243095",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Optimize `_get_ranks` in Sampler (#3623)"
  },
  {
    "commit_hash": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "commit_short": "bfdb1ba5",
    "models": [
      "meta-llama/Llama-2-7b-chat-hf"
    ],
    "hardware": "H100",
    "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)"
  },
  {
    "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "commit_short": "cf2f084d",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)"
  },
  {
    "commit_hash": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
    "commit_short": "9474e89b",
    "models": [
      "huggyllama/llama-7b"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_throughput.py --model huggyllama/llama-7b --dataset-name sharegpt --num-prompts 2000",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)"
  },
  {
    "commit_hash": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
    "commit_short": "21d93c14",
    "models": [
      "mistralai/Mixtral-8x7B-v0.1"
    ],
    "hardware": "A100-TP8",
    "perf_command": "python benchmarks/benchmark_throughput.py --model mistralai/Mixtral-8x7B-v0.1 --tensor-parallel-size 8",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "Optimize Mixtral with expert parallelism (#2090)"
  },
  {
    "commit_hash": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
    "commit_short": "ec3b5ce9",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Improve detokenization performance (#1338)"
  },
  {
    "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "commit_short": "c45f3c3a",
    "models": [
      "facebook/opt-13b"
    ],
    "hardware": "H100",
    "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "Optimize tensor parallel execution speed (#17)"
  },
  {
    "commit_hash": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
    "commit_short": "d4bc1a4d",
    "models": [
      "facebook/opt-125m",
      "facebook/opt-350m",
      "facebook/opt-1.3b",
      "facebook/opt-2.7b",
      "facebook/opt-6.7b"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Add unoptimized OPT Attention"
  },
  {
    "commit_hash": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "commit_short": "b690e348",
    "models": [
      "ibm-ai-platform/Bamba-9B-v2"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dataset-name sharegpt",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)"
  },
  {
    "commit_hash": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "commit_short": "58eee5f2",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)"
  },
  {
    "commit_hash": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "commit_short": "eefbf4a6",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model Qwen/Qwen3-30B-A3B-FP8 --batch-size 32 --input-len 512 --output-len 128",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)"
  },
  {
    "commit_hash": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "commit_short": "ac45c44d",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "hardware": "H100-TP2",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2 --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)"
  },
  {
    "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "commit_short": "8aa1485f",
    "models": [
      "meta-llama/Llama-4-Scout-17B-16E-Instruct"
    ],
    "hardware": "H100-TP4",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --tensor-parallel-size 4 --max-model-len 16384",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Perf] Disable chunked local attention by default with llama4 (#21761)"
  },
  {
    "commit_hash": "61b8cea3b42feab021d506e9143551de18f9165c",
    "commit_short": "61b8cea3",
    "models": [
      "meta-llama/Meta-Llama-3-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.2-3B-Instruct --dataset-name random --input-len 256 --output-len 128 --num-prompts 100",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)"
  },
  {
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "commit_short": "4fb56914",
    "models": [
      "deepseek-ai/DeepSeek-V3-0324"
    ],
    "hardware": "H200-TP8",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)"
  },
  {
    "commit_hash": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "commit_short": "ed250545",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21222)"
  },
  {
    "commit_hash": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "commit_short": "a3223766",
    "models": [
      "facebook/opt-125m"
    ],
    "hardware": "H100",
    "perf_command": "vllm bench serve --dataset-name random --model facebook/opt-125m --served-model-name facebook/opt-125m --random-input-len 700 --random-output-len 1 --endpoint /v1/completions --ignore-eos --host localhost --port 8000 --request-rate 200 --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Optimize update checks in LogitsProcessor (#21245)"
  },
  {
    "commit_hash": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
    "commit_short": "e7b20426",
    "models": [
      "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Maverick-17B-128E-Instruct --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)"
  },
  {
    "commit_hash": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
    "commit_short": "0ec82edd",
    "models": [
      "Qwen/Qwen3-30B-A3B",
      "Qwen/Qwen3-30B-A3B-FP8",
      "ibm-granite/granite-4.0-tiny-preview"
    ],
    "hardware": "H100",
    "perf_command": "vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[perf] Speed up align sum kernels (#21079)"
  },
  {
    "commit_hash": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "commit_short": "6d0734c5",
    "models": [
      "mistralai/Mistral-7B-Instruct-v0.3",
      "deepseek-ai/DeepSeek-R1"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)"
  },
  {
    "commit_hash": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
    "commit_short": "dcc6cfb9",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "hardware": "H100-TP2",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dataset-name sharegpt",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)"
  },
  {
    "commit_hash": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "commit_short": "8a4e5c5f",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)"
  },
  {
    "commit_hash": "c0569dbc82b5e945a77878190114d1b68027828b",
    "commit_short": "c0569dbc",
    "models": [
      "Qwen/Qwen3-30B-A3B-FP8"
    ],
    "hardware": "H100-TP2",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)"
  },
  {
    "commit_hash": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b",
    "commit_short": "22dd9c27",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "VLLM_ATTENTION_BACKEND=TRITON_ATTN_VLLM_V1 VLLM_USE_V1=1 python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --input-len 16000 --output-len 4 --batch-size 1",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel (#20308)"
  },
  {
    "commit_hash": "9a3b88328f7e434cac35b90ee463de6689f9a833",
    "commit_short": "9a3b8832",
    "models": [
      "Qwen/Qwen2.5-VL-3B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)"
  },
  {
    "commit_hash": "7661e92ef85e552936195ae4b803e292b9a96776",
    "commit_short": "7661e92e",
    "models": [
      "nvidia/Nemotron-4-340B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model nvidia/Nemotron-4-340B-Instruct --dataset-name sharegpt --request-rate 1",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Model] Optimize nemotron_h implementation (#19249)"
  },
  {
    "commit_hash": "e7523c2e031bc96740723ab63833d1cf94229ab4",
    "commit_short": "e7523c2e",
    "models": [
      "google/gemma-3-12b-it"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --backend openai-chat --model google/gemma-3-12b-it --endpoint /v1/chat/completions --dataset-name hf --dataset-path lmarena-ai/VisionArena-Chat --hf-split train --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs (#18608)"
  },
  {
    "commit_hash": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
    "commit_short": "d55e446d",
    "models": [
      "meta-llama/Llama-3-8B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --batch-size 2",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Spec Decode] Small refactors to improve eagle bookkeeping performance (#18424)"
  },
  {
    "commit_hash": "e493e48524e9e78ab33eafec6461b3940e361189",
    "commit_short": "e493e485",
    "models": [
      "microsoft/phi-1_5"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model microsoft/phi-1_5 --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled (#17731)"
  },
  {
    "commit_hash": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
    "commit_short": "67da5720",
    "models": [
      "Qwen/Qwen2.5-VL-3B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct --dataset-name sharegpt --num-prompts 50",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding (#17973)"
  },
  {
    "commit_hash": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
    "commit_short": "015069b0",
    "models": [
      "Qwen/Qwen3-7B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --request-rate 1",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content (#17515)"
  },
  {
    "commit_hash": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "commit_short": "bc7c4d20",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct",
      "mistralai/Mistral-7B-Instruct-v0.3"
    ],
    "hardware": "AMD-MI300X",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)"
  },
  {
    "commit_hash": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "commit_short": "299ebb62",
    "models": [
      "Qwen/Qwen2.5-1.5B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)"
  },
  {
    "commit_hash": "3092375e274e9e003961e600e10a6192d33ceaa0",
    "commit_short": "3092375e",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Performance] Implement custom serializaton for MultiModalKwargs [Rebased] (#16432)"
  },
  {
    "commit_hash": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66",
    "commit_short": "93e5f3c5",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Perf] Optimize Preparing Inputs for GPU Model Runner (#16484)"
  },
  {
    "commit_hash": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
    "commit_short": "bd6028d6",
    "models": [
      "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic"
    ],
    "hardware": "H100-TP2",
    "perf_command": "python benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "Optimized topk for topk=1 (Llama-4) (#16512)"
  },
  {
    "commit_hash": "b10e51989551cd80dd74079429ccf91f0807bd92",
    "commit_short": "b10e5198",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Minor] Optimize get_cached_block (#16135)"
  },
  {
    "commit_hash": "35fad35a485eac9195c510731ba4a9d297dfd963",
    "commit_short": "35fad35a",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Sampler] Faster top-k only implementation (#15478)"
  },
  {
    "commit_hash": "9d72daf4ced05a5fec1ad8ea2914a39296f402da",
    "commit_short": "9d72daf4",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Perf] Simpler request output queues (#15156)"
  },
  {
    "commit_hash": "296f927f2493908984707354e3cc5d7b2e41650b",
    "commit_short": "296f927f",
    "models": [
      "ibm-ai-platform/Bamba-9B-v2"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dataset-name sharegpt",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14857)"
  },
  {
    "commit_hash": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "commit_short": "22d33bac",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path for single-prompt requests (#15150)"
  },
  {
    "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "commit_short": "99abb8b6",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 3 --dataset-name sharegpt",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)"
  },
  {
    "commit_hash": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
    "commit_short": "ccf02fcb",
    "models": [
      "ibm-ai-platform/Bamba-9B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U\u2026 (#14848)"
  },
  {
    "commit_hash": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "commit_short": "fe66b347",
    "models": [
      "ibm-ai-platform/Bamba-9B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14778)"
  },
  {
    "commit_hash": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "commit_short": "70b808fe",
    "models": [
      "Qwen/Qwen2-VL-2B",
      "Qwen/Qwen2-VL-7B",
      "Qwen/Qwen2.5-VL-3B",
      "Qwen/Qwen2.5-VL-7B"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)"
  },
  {
    "commit_hash": "fb0acb6c72874e98617cabee4ff4851569374fc9",
    "commit_short": "fb0acb6c",
    "models": [
      "deepseek-ai/DeepSeek-R1"
    ],
    "hardware": "H200-TP8",
    "perf_command": "python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 6000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8",
    "has_serving": false,
    "has_latency": false,
    "has_throughput": true,
    "commit_subject": "[Perf] Improve MLA on V1 (#14540)"
  },
  {
    "commit_hash": "ca7a2d5f28eac9621474563cdda0e08596222755",
    "commit_short": "ca7a2d5f",
    "models": [
      "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
    ],
    "hardware": "H100-TP2",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14384)\" (#14471)"
  },
  {
    "commit_hash": "dae68969774e41b93b01cd31171ca033a92b574a",
    "commit_short": "dae68969",
    "models": [
      "deepseek-ai/DeepSeek-R1"
    ],
    "hardware": "H100",
    "perf_command": "VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHMLA python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)"
  },
  {
    "commit_hash": "9f1710f1ace3535920c0bb6d4cc329c36289080e",
    "commit_short": "9f1710f1",
    "models": [
      "deepseek-ai/DeepSeek-V2-Lite-Chat"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2-Lite-Chat --input-len 28000 --output-len 64",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Fix mla prefill context performance (#13897)"
  },
  {
    "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "commit_short": "9badee53",
    "models": [
      "meta-llama/Llama-3.2-1B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "Fix performance when `--generation-config` is not `None` (#14223)"
  },
  {
    "commit_hash": "19d98e0c7db96713f0e2201649159431177a56e2",
    "commit_short": "19d98e0c",
    "models": [
      "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --dataset-name sharegpt --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[Kernel] Optimize moe intermediate_cache usage (#13625)"
  },
  {
    "commit_hash": "e206b5433109d298e53451015465b2bf8f03ef0a",
    "commit_short": "e206b543",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --guided-decoding-backend xgrammar",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine (#13837)"
  },
  {
    "commit_hash": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "commit_short": "6a417b86",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "Neuron-AWS",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "fix neuron performance issue (#13589)"
  },
  {
    "commit_hash": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "commit_short": "0d243f2a",
    "models": [
      "mistralai/Mixtral-8x7B-Instruct-v0.1"
    ],
    "hardware": "AMD-MI300X",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS (#13577)"
  },
  {
    "commit_hash": "4c822298981a8f7521492075ff72659985fc4c3f",
    "commit_short": "4c822298",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model meta-llama/Llama-3.2-1B-Instruct --num-speculative-tokens 5",
    "has_serving": false,
    "has_latency": true,
    "has_throughput": false,
    "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)"
  },
  {
    "commit_hash": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "commit_short": "30172b49",
    "models": [
      "meta-llama/Llama-3.1-8B-Instruct"
    ],
    "hardware": "H100",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "has_serving": true,
    "has_latency": false,
    "has_throughput": false,
    "commit_subject": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)"
  }
]