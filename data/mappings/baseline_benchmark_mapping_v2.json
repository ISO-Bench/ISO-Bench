[
  {
    "human_commit_short": "19d98e0c",
    "human_commit_full": "19d98e0c7db96713f0e2201649159431177a56e2",
    "parent_commit_short": "2b04c209",
    "parent_commit_full": "2b04c209ee98174f29f1fc98f0dc3222d652a7bd",
    "benchmark_type": "serving",
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13625",
    "commit_subject": "[Kernel] Optimize moe intermediate_cache usage (#13625)"
  },
  {
    "human_commit_short": "25ebed2f",
    "human_commit_full": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "parent_commit_short": "d263bd9d",
    "parent_commit_full": "d263bd9df7b2f5586910e5d006a11ff11ba7c310",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11214",
    "commit_subject": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)"
  },
  {
    "human_commit_short": "299ebb62",
    "human_commit_full": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "parent_commit_short": "f728ab8e",
    "parent_commit_full": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9",
    "benchmark_type": "serving",
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/16436",
    "commit_subject": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)"
  },
  {
    "human_commit_short": "2f192835",
    "human_commit_full": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "parent_commit_short": "95baec82",
    "parent_commit_full": "95baec828f3ee046074dace1d88202a920b7dc15",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/3890",
    "commit_subject": "[Core] latency optimization (#3890)"
  },
  {
    "human_commit_short": "30172b49",
    "human_commit_full": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "parent_commit_short": "a4d577b3",
    "parent_commit_full": "a4d577b37944cbfa1bc62e4869667d1e2739d62a",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13244",
    "commit_subject": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)"
  },
  {
    "human_commit_short": "3b61cb45",
    "human_commit_full": "3b61cb450d899dc423feb264c297d4d18d701678",
    "parent_commit_short": "edc4fa31",
    "parent_commit_full": "edc4fa31888b4a41060acb7b16250540f051ad59",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10989",
    "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)"
  },
  {
    "human_commit_short": "4c822298",
    "human_commit_full": "4c822298981a8f7521492075ff72659985fc4c3f",
    "parent_commit_short": "c8d70e24",
    "parent_commit_full": "c8d70e2437feecdb3762ce17298df33439ae1bd1",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13365",
    "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)"
  },
  {
    "human_commit_short": "58eee5f2",
    "human_commit_full": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "parent_commit_short": "067c34a1",
    "parent_commit_full": "067c34a1559400e956311f067ddd185f54207a2b",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20000",
    "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list convers"
  },
  {
    "human_commit_short": "61b8cea3",
    "human_commit_full": "61b8cea3b42feab021d506e9143551de18f9165c",
    "parent_commit_short": "526078a9",
    "parent_commit_full": "526078a96c52af678a1ddbdc3ecf78265e358f2b",
    "benchmark_type": "throughput",
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21137",
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)"
  },
  {
    "human_commit_short": "6d0734c5",
    "human_commit_full": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "parent_commit_short": "7d945771",
    "parent_commit_full": "7d94577138e3d4c7bcfd781337ee1e5a2befa685",
    "benchmark_type": "serving",
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20645",
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645"
  },
  {
    "human_commit_short": "6dd94dbe",
    "human_commit_full": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "parent_commit_short": "0e74d797",
    "parent_commit_full": "0e74d797ce8618fdb685126e0ff8576fb966e6ad",
    "benchmark_type": "latency",
    "model": "meta-llama/Meta-Llama-3-8B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12380",
    "commit_subject": "[perf] fix perf regression from #12253 (#12380)"
  },
  {
    "human_commit_short": "70b808fe",
    "human_commit_full": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "parent_commit_short": "63d635d1",
    "parent_commit_full": "63d635d17962377df089cdc9d4a2684f0b007208",
    "benchmark_type": "serving",
    "model": "Qwen/Qwen2-VL-7B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14377",
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)"
  },
  {
    "human_commit_short": "7661e92e",
    "human_commit_full": "7661e92ef85e552936195ae4b803e292b9a96776",
    "parent_commit_short": "f168b857",
    "parent_commit_full": "f168b85725202915b5719c62b46d310a608b13dd",
    "benchmark_type": "serving",
    "model": "nvidia/Nemotron-4-340B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/19249",
    "commit_subject": "[Model] Optimize nemotron_h implementation (#19249)"
  },
  {
    "human_commit_short": "8a4e5c5f",
    "human_commit_full": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "parent_commit_short": "76b49444",
    "parent_commit_full": "76b494444fd864ffc53a623420668d1865c804b9",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20906",
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)"
  },
  {
    "human_commit_short": "8c1e77fb",
    "human_commit_full": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "parent_commit_short": "5fc5ce0f",
    "parent_commit_full": "5fc5ce0fe45f974fc8840175e8321652238400f0",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10742",
    "commit_subject": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)"
  },
  {
    "human_commit_short": "98f47f2a",
    "human_commit_full": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "parent_commit_short": "8c1e77fb",
    "parent_commit_full": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "benchmark_type": "latency",
    "model": "facebook/opt-125m",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10733",
    "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)"
  },
  {
    "human_commit_short": "9a3b8832",
    "human_commit_full": "9a3b88328f7e434cac35b90ee463de6689f9a833",
    "parent_commit_short": "3014c920",
    "parent_commit_full": "3014c920dae5a2360b9b4141395522cc52b59193",
    "benchmark_type": "serving",
    "model": "Qwen/Qwen2.5-VL-3B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/19939",
    "commit_subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)"
  },
  {
    "human_commit_short": "9f1710f1",
    "human_commit_full": "9f1710f1ace3535920c0bb6d4cc329c36289080e",
    "parent_commit_short": "e642ec96",
    "parent_commit_full": "e642ec962cf2283f9aa44492727e6efc17a32129",
    "benchmark_type": "serving",
    "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13897",
    "commit_subject": "Fix mla prefill context performance (#13897)"
  },
  {
    "human_commit_short": "a3223766",
    "human_commit_full": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "parent_commit_short": "bc8a8ce5",
    "parent_commit_full": "bc8a8ce5ec374dd18e86f59be7cb0057a4b21992",
    "benchmark_type": "serving",
    "model": "facebook/opt-125m",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21245",
    "commit_subject": "[Core] Optimize update checks in LogitsProcessor (#21245)"
  },
  {
    "human_commit_short": "b2e0ad3b",
    "human_commit_full": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "parent_commit_short": "4a18fd14",
    "parent_commit_full": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10339",
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)"
  },
  {
    "human_commit_short": "b55ed6ef",
    "human_commit_full": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "parent_commit_short": "2f385183",
    "parent_commit_full": "2f385183f35497e030ef22c9820d83b83bc4f6db",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11692",
    "commit_subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)"
  },
  {
    "human_commit_short": "b690e348",
    "human_commit_full": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "parent_commit_short": "25373b6c",
    "parent_commit_full": "25373b6c6cc2068e3914fa906d3240088f7af157",
    "benchmark_type": "serving",
    "model": "ibm-ai-platform/Bamba-9B-v2",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21075",
    "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)"
  },
  {
    "human_commit_short": "baeded25",
    "human_commit_full": "baeded25699f9f4851843306f27f685c4d4ee7c5",
    "parent_commit_short": "3e1c76cf",
    "parent_commit_full": "3e1c76cf3a87854396d9e86a56a335e7d750c85f",
    "benchmark_type": "latency",
    "model": "deepseek-ai/DeepSeek-V3",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12601",
    "commit_subject": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)"
  },
  {
    "human_commit_short": "bfdb1ba5",
    "human_commit_full": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "parent_commit_short": "cf2f084d",
    "parent_commit_full": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/3469",
    "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)"
  },
  {
    "human_commit_short": "ce6bf3a2",
    "human_commit_full": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "parent_commit_short": "3cdfe1f3",
    "parent_commit_full": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0",
    "benchmark_type": "throughput",
    "model": "google/gemma-2b",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/7898",
    "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)"
  },
  {
    "human_commit_short": "dae68969",
    "human_commit_full": "dae68969774e41b93b01cd31171ca033a92b574a",
    "parent_commit_short": "c34eeec5",
    "parent_commit_full": "c34eeec58d3a94437c5311e256f8ba21d1912a39",
    "benchmark_type": "serving",
    "model": "deepseek-ai/DeepSeek-R1",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14384",
    "commit_subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)"
  },
  {
    "human_commit_short": "e7523c2e",
    "human_commit_full": "e7523c2e031bc96740723ab63833d1cf94229ab4",
    "parent_commit_short": "a869baca",
    "parent_commit_full": "a869baca73eb90ae7bd18402915dc4bfc36cf06b",
    "benchmark_type": "serving",
    "model": "google/gemma-3-12b-it",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/18608",
    "commit_subject": "[V1][Sampler] Improve performance of FlashInfer sampling by sampling logits inst"
  },
  {
    "human_commit_short": "ed250545",
    "human_commit_full": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "parent_commit_short": "10904e6d",
    "parent_commit_full": "10904e6d755051260a7c3ce98659d8907c74caa9",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21222",
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further opti"
  },
  {
    "human_commit_short": "eefbf4a6",
    "human_commit_full": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "parent_commit_short": "88faa466",
    "parent_commit_full": "88faa466d788e25082c02dc9688931d7976361f9",
    "benchmark_type": "latency",
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/22036",
    "commit_subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)"
  },
  {
    "human_commit_short": "f092153f",
    "human_commit_full": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "parent_commit_short": "1da8f0e1",
    "parent_commit_full": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11111",
    "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111"
  },
  {
    "human_commit_short": "fa63e710",
    "human_commit_full": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "parent_commit_short": "2a0309a6",
    "parent_commit_full": "2a0309a646b1ed83a0c40974e08c8dc628726d3c",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3-8B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12094",
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)"
  },
  {
    "human_commit_short": "fc542144",
    "human_commit_full": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "parent_commit_short": "eb5741ad",
    "parent_commit_full": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12563",
    "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)"
  },
  {
    "human_commit_short": "fe66b347",
    "human_commit_full": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "parent_commit_short": "270a5da4",
    "parent_commit_full": "270a5da495d24e947a71e2fa0c56635f4fad2dc3",
    "benchmark_type": "serving",
    "model": "ibm-ai-platform/Bamba-9B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14778",
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory C"
  }
]