{
  "015069b0": {
    "commit_full": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-7B-Instruct --dataset-name sharegpt --request-rate 1",
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "parent_commit": "fbefc8a78d22b20eac042c586805c7dcbfc66b1c"
  },
  "0d243f2a": {
    "commit_full": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1",
    "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "parent_commit": "88f6ba3281f727d5641d362476ae68562b666081"
  },
  "0ec82edd": {
    "commit_full": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
    "benchmark_mode": "serving",
    "perf_command": "vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100",
    "model": "Qwen/Qwen3-30B-A3B",
    "parent_commit": "005ae9be6c22dfa2c2c5580b50b41e67faee4a87"
  },
  "21d93c14": {
    "commit_full": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8",
    "model": "mistralai/Mixtral-8x7B-v0.1",
    "parent_commit": "f1c8520146031a650404a6ab120ee11e91c10bed"
  },
  "22d33bac": {
    "commit_full": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "b0e96aaebbfbe8e70478e4192a5a13864ffdefa6"
  },
  "22dd9c27": {
    "commit_full": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "a6d795d593046abd490b16349bcd9b40feedd334"
  },
  "25ebed2f": {
    "commit_full": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310"
  },
  "296f927f": {
    "commit_full": "296f927f2493908984707354e3cc5d7b2e41650b",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "model": "ibm-ai-platform/Bamba-9B",
    "parent_commit": "0032903a5bb7c7c655f52f4efdfcc221947e9ca8"
  },
  "299ebb62": {
    "commit_full": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "benchmark_mode": "serving",
    "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos",
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "parent_commit": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9"
  },
  "2a052011": {
    "commit_full": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --quantization fp8",
    "model": "nm-testing/Mixtral-8x7B-Instruct-v0.1-FP8",
    "parent_commit": "36fb68f94792a8cec8df5b58bab7ab4d4d6158b4"
  },
  "2deb029d": {
    "commit_full": "2deb029d115dadd012ce5ea70487a207cb025493",
    "benchmark_mode": "prefix_caching",
    "perf_command": "python3 benchmarks/benchmark_prefix_caching.py --model RedHatAI/Meta-Llama-3-8B-Instruct-FP8 --output-len 200 --enable-prefix-caching",
    "model": "RedHatAI/Meta-Llama-3-8B-Instruct-FP8",
    "parent_commit": "029c71de11bc3bcf84a1b3cf9d91e79ab6949799"
  },
  "2f192835": {
    "commit_full": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "95baec828f3ee046074dace1d88202a920b7dc15"
  },
  "30172b49": {
    "commit_full": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "a4d577b37944cbfa1bc62e4869667d1e2739d62a"
  },
  "3092375e": {
    "commit_full": "3092375e274e9e003961e600e10a6192d33ceaa0",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "3cd91dc9555e6f10e55f23d37782c65b0366f7cf"
  },
  "310aca88": {
    "commit_full": "310aca88c984983189a57f1b72e3b1dde89fb92f",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
    "model": "meta-llama/Meta-Llama-3-70B",
    "parent_commit": "a732900efc4eb0d4393e3885d5df8ef3516d4834"
  },
  "3127e975": {
    "commit_full": "3127e975fb9417d10513e25b80820870f594c627",
    "benchmark_mode": null,
    "perf_command": null,
    "model": null,
    "parent_commit": null
  },
  "3476ed08": {
    "commit_full": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "54600709b6d419fb243ce718a48ab7d40f5c3eb7"
  },
  "35fad35a": {
    "commit_full": "35fad35a485eac9195c510731ba4a9d297dfd963",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "733e7c9e95f5b066ac420b00701eef7ea164a79e"
  },
  "379da6dc": {
    "commit_full": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-70B --input-len 1000 --output-len 50 --tensor-parallel-size 4 --quantization fp8",
    "model": "meta-llama/Meta-Llama-3-70B",
    "parent_commit": "ebce310b7433e050086f52ca48571807df467f50"
  },
  "3a243095": {
    "commit_full": "3a243095e5e7b655b63ab08fbd5936cb40850415",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "64172a976c8d975b3aec946f1675716d2532d94f"
  },
  "3b61cb45": {
    "commit_full": "3b61cb450d899dc423feb264c297d4d18d701678",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "edc4fa31888b4a41060acb7b16250540f051ad59"
  },
  "4c822298": {
    "commit_full": "4c822298981a8f7521492075ff72659985fc4c3f",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model meta-llama/Llama-3.2-1B-Instruct --num-speculative-tokens 5",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "c8d70e2437feecdb3762ce17298df33439ae1bd1"
  },
  "4fb56914": {
    "commit_full": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "model": null,
    "parent_commit": null
  },
  "526de822": {
    "commit_full": "526de822d501c792b051c864ba873a836d78d5bf",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_latency.py --dtype bfloat16 --enable-chunked-prefill False --load-format dummy --batch-size BS --num-iters-warmup 2 --num-iters 5 --input-len INPUT_LEN --output-len OUTPUT_LEN --model MODEL",
    "model": null,
    "parent_commit": null
  },
  "58eee5f2": {
    "commit_full": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "067c34a1559400e956311f067ddd185f54207a2b"
  },
  "61b8cea3": {
    "commit_full": "61b8cea3b42feab021d506e9143551de18f9165c",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b"
  },
  "660470e5": {
    "commit_full": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "8d59dbb00044a588cab96bcdc028006ed922eb06"
  },
  "67da5720": {
    "commit_full": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-7B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "parent_commit": "5c04bb8b863bfdef8122b193631479315cc764f5"
  },
  "6a417b86": {
    "commit_full": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "d3ea50113c08bdd3c5cfda42ec6ecbc72328d7d1"
  },
  "6ce01f30": {
    "commit_full": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --backend vllm --num-prompts 100",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "6a11fdfbb8d6701c7ad38648aead23d8cbe6aac5"
  },
  "6d0734c5": {
    "commit_full": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "parent_commit": "7d94577138e3d4c7bcfd781337ee1e5a2befa685"
  },
  "6d646d08": {
    "commit_full": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "95a178f86120f42d183b3af5ee1ce58ee05c8889"
  },
  "6dd94dbe": {
    "commit_full": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-8B --load-format dummy",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad"
  },
  "6e36f4fa": {
    "commit_full": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "dd2a6a82e3f41b4673b1dbb24b2e99230ea96981"
  },
  "70b808fe": {
    "commit_full": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
    "model": "Qwen/Qwen2-VL-7B",
    "parent_commit": "63d635d17962377df089cdc9d4a2684f0b007208"
  },
  "7661e92e": {
    "commit_full": "7661e92ef85e552936195ae4b803e292b9a96776",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model nvidia/Nemotron-4-340B-Instruct --dataset-name sharegpt --request-rate 1",
    "model": "nvidia/Nemotron-4-340B-Instruct",
    "parent_commit": "f168b85725202915b5719c62b46d310a608b13dd"
  },
  "7c01f706": {
    "commit_full": "7c01f706418d593b3cf23d2ec9110dca7151c539",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "51e971d39e1272f1c5b070a5da6b38ccfa92fc14"
  },
  "80aa7e91": {
    "commit_full": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "bd43973522ea17be50e10fbb222a22f673c8067e"
  },
  "83450458": {
    "commit_full": "83450458339b07765b0e72a822e5fe93eeaf5258",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --speculative-model '[ngram]' --num-speculative-tokens 5 --input-len 550 --output-len 150",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "5b8a1fde84224e24ec121e0dc149d775330d911b"
  },
  "88693683": {
    "commit_full": "886936837ca89e5645bc1f71cc0e1492b65b1590",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B --enable-prefix-caching",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "6d917d0eebd03990edf2443780a5f2506026ea78"
  },
  "89a84b0b": {
    "commit_full": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen1.5-0.5B --backend vllm --num-prompts 2048 --input-len 1024",
    "model": "Qwen/Qwen1.5-0.5B",
    "parent_commit": "084a01fd3544557990f8af8af6fd3c1185bae848"
  },
  "8a4e5c5f": {
    "commit_full": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "76b494444fd864ffc53a623420668d1865c804b9"
  },
  "8aa1485f": {
    "commit_full": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --trust-remote-code --max-model-len 16384",
    "model": "meta-llama/Llama-4-Scout-17B-16E-Instruct",
    "parent_commit": "89ac266b262f08d25ebf25fc66122d1b2367ae64"
  },
  "8bc68e19": {
    "commit_full": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "0fca3cdcf265cd375bca684d951702b6b7adf65a"
  },
  "8c1e77fb": {
    "commit_full": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --batch-size 32 --input-len 512 --output-len 128",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "5fc5ce0fe45f974fc8840175e8321652238400f0"
  },
  "8d75fe48": {
    "commit_full": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --dataset-name sharegpt --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json",
    "model": "neuralmagic/Meta-Llama-3-8B-Instruct-FP8",
    "parent_commit": "388596c91437a51d428a447594e9faec340c29b2"
  },
  "9323a315": {
    "commit_full": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
    "benchmark_mode": "serving",
    "perf_command": "python benchmark_guided.py --model meta-llama/Llama-3.1-8B-Instruct --dataset xgrammar_bench --async-engine --output-len 512 --num-prompts 20 --enable-chunked-prefill --guided-decoding-ratio 1",
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "parent_commit": "3257d449fa0fd3e05aa20cc8c5fff79ad101984f"
  },
  "93e5f3c5": {
    "commit_full": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "70363bccfac1a6a0818ea577ad9cf8123a0ec3ae"
  },
  "9474e89b": {
    "commit_full": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmark_throughput_cache.py --backend vllm --model huggyllama/llama-7b --dataset ../data/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 2000",
    "model": "huggyllama/llama-7b",
    "parent_commit": "20478c4d3abcd0aa8a1d9ace9c76ea3a2e04cb5e"
  },
  "98f47f2a": {
    "commit_full": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py",
    "model": "unknown",
    "parent_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f"
  },
  "99abb8b6": {
    "commit_full": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 1000",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "3a1e6481586ed7f079275b5d5072a6e246af691e"
  },
  "9a3b8832": {
    "commit_full": "9a3b88328f7e434cac35b90ee463de6689f9a833",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct --dataset-name random --num-prompts 1000",
    "model": "Qwen/Qwen2.5-VL-3B-Instruct",
    "parent_commit": "3014c920dae5a2360b9b4141395522cc52b59193"
  },
  "9badee53": {
    "commit_full": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "parent_commit": "beebf4742af80296d3c3a657c66d512615c550c1"
  },
  "9d72daf4": {
    "commit_full": "9d72daf4ced05a5fec1ad8ea2914a39296f402da",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "6dd55af6c9dde9174e0616739d783133f5e45d42"
  },
  "9ed82e70": {
    "commit_full": "9ed82e7074a18e25680ab106fc846364ad97bc00",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "51f8aa90ad409cc77bfab208be7f5907bf7d5330"
  },
  "a3223766": {
    "commit_full": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "benchmark_mode": "serving",
    "perf_command": "vllm bench serve --dataset-name random --model facebook/opt-125m --served-model-name facebook/opt-125m --random-input-len 700 --random-output-len 1 --endpoint /v1/completions --ignore-eos --host localhost --port 8000 --request-rate 200 --num-prompts 100",
    "model": "facebook/opt-125m",
    "parent_commit": "bc8a8ce5ec374dd18e86f59be7cb0057a4b21992"
  },
  "ac45c44d": {
    "commit_full": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2",
    "model": null,
    "parent_commit": null
  },
  "ad8d696a": {
    "commit_full": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "3d925165f2b18379640a63fbb42de95440d63b64"
  },
  "aea94362": {
    "commit_full": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.2-1B-Instruct",
    "parent_commit": "7206ce4ce112ed117796a59045c968a6d353f691"
  },
  "b10e5198": {
    "commit_full": "b10e51989551cd80dd74079429ccf91f0807bd92",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "9bde5ba12709ea0fe9e1a1eeee1e8d7b4c7ea668"
  },
  "b2e0ad3b": {
    "commit_full": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b"
  },
  "b55ed6ef": {
    "commit_full": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "2f385183f35497e030ef22c9820d83b83bc4f6db"
  },
  "b690e348": {
    "commit_full": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dtype float16 --num-prompts 300 --seed 0",
    "model": "ibm-ai-platform/Bamba-9B-v2",
    "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157"
  },
  "b6d10354": {
    "commit_full": "b6d103542c654fb63013a1e45a586d654ae36a2a",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Llama-2-70b-hf --dtype float16 --tensor-parallel-size 1",
    "model": "meta-llama/Llama-2-70b-hf",
    "parent_commit": "51c31bc10ca7c48b580cd58fcd741ba4d6db4447"
  },
  "baeded25": {
    "commit_full": "baeded25699f9f4851843306f27f685c4d4ee7c5",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3 --dtype float16",
    "model": null,
    "parent_commit": null
  },
  "bc7c4d20": {
    "commit_full": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "f67e9e9f221e9791733b827585d6eb6dbc23133c"
  },
  "bd6028d6": {
    "commit_full": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000 --batch-size 1 --num-iters-warmup 5 --num-iters 5",
    "model": "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic",
    "parent_commit": "802329dee9e5b70c0c73df93c9db1ecdc4632664"
  },
  "bfdb1ba5": {
    "commit_full": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "benchmark_mode": "standalone",
    "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019"
  },
  "c0569dbc": {
    "commit_full": "c0569dbc82b5e945a77878190114d1b68027828b",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "parent_commit": "8bb43b9c9ee878e07038d3f36aaf279ffb2fabab"
  },
  "c45f3c3a": {
    "commit_full": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
    "model": "facebook/opt-13b",
    "parent_commit": "7a7929abe8e2fd6a4688487c471a1ee1fde0edd2"
  },
  "ca7a2d5f": {
    "commit_full": "ca7a2d5f28eac9621474563cdda0e08596222755",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct --dtype float16 --num-prompts 300 --seed 0",
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "parent_commit": "333681408feabb97193880303b23f6571ba39045"
  },
  "ccf02fcb": {
    "commit_full": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "model": "ibm-ai-platform/Bamba-9B",
    "parent_commit": "acaea3bb07883c80b71643ebee1cd08d555797bc"
  },
  "ce6bf3a2": {
    "commit_full": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "benchmark_mode": null,
    "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
    "model": "google/gemma-2b",
    "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0"
  },
  "cf2f084d": {
    "commit_full": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "f721096d48a7e3b98dffcb9b400bf58989cef64d"
  },
  "d4bc1a4d": {
    "commit_full": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
    "model": "facebook/opt-125m",
    "parent_commit": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f"
  },
  "d55e446d": {
    "commit_full": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B --batch-size 2",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "ec82c3e388b962a30a02fa376c222cef787b3c14"
  },
  "d7740ea4": {
    "commit_full": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.1-8B-Instruct --input-len 256 --output-len 256",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "cc466a32903d53d0ceca459b766d74ad668c8f87"
  },
  "dae68969": {
    "commit_full": "dae68969774e41b93b01cd31171ca033a92b574a",
    "benchmark_mode": "serving",
    "perf_command": "VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHMLA python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --tensor-parallel-size 8",
    "model": "deepseek-ai/DeepSeek-R1",
    "parent_commit": "c34eeec58d3a94437c5311e256f8ba21d1912a39"
  },
  "dcc6cfb9": {
    "commit_full": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "parent_commit": "dd572c0ab3effa539b74f9a1288bb61ce83ada76"
  },
  "e206b543": {
    "commit_full": "e206b5433109d298e53451015465b2bf8f03ef0a",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100 --guided-decoding-backend xgrammar",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "1d35662e6dc199431bfe4004cc84d66fd9b297b1"
  },
  "e3580537": {
    "commit_full": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model RedHatAI/Meta-Llama-3-8B-Instruct-FP8 --enable-prefix-caching --enable-chunked-prefill --max-num-batched-tokens 2048",
    "model": "RedHatAI/Meta-Llama-3-8B-Instruct-FP8",
    "parent_commit": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a"
  },
  "e493e485": {
    "commit_full": "e493e48524e9e78ab33eafec6461b3940e361189",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model microsoft/phi-1_5 --backend vllm --num-prompts 100",
    "model": "microsoft/phi-1_5",
    "parent_commit": "4ce64e2df48649c4873f828b8bf71790aa1e56ee"
  },
  "e7523c2e": {
    "commit_full": "e7523c2e031bc96740723ab63833d1cf94229ab4",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --backend openai-chat --model google/gemma-3-12b-it --endpoint /v1/chat/completions --dataset-name hf --dataset-path lmarena-ai/VisionArena-Chat --hf-split train --num-prompts 1000",
    "model": "google/gemma-3-12b-it",
    "parent_commit": "a869baca73eb90ae7bd18402915dc4bfc36cf06b"
  },
  "e7b20426": {
    "commit_full": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model 01-ai/Yi-1.5-9B-Chat --dtype float16 --num-prompts 300 --seed 0",
    "model": "01-ai/Yi-1.5-9B-Chat",
    "parent_commit": "90f1e55421f1b61394ba25abe34bf5abd82a71af"
  },
  "ec3b5ce9": {
    "commit_full": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "6368e777a8ead7fb62054d3779c6237361ec0d86"
  },
  "ed250545": {
    "commit_full": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "10904e6d755051260a7c3ce98659d8907c74caa9"
  },
  "eefbf4a6": {
    "commit_full": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9"
  },
  "f092153f": {
    "commit_full": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03"
  },
  "f26c4aee": {
    "commit_full": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
    "benchmark_mode": "standalone",
    "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "8936316d587ca0afb5ef058584c407d404c0ffb0"
  },
  "fa63e710": {
    "commit_full": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "benchmark_mode": "standalone",
    "perf_command": "VLLM_USE_V1=1 python3 benchmarks/benchmark_latency.py --model \"/data/users/ktong/llama/llm_8b_oss\" --tensor-parallel-size 1 --input_len 1000 --batch_size 32",
    "model": "meta-llama/Meta-Llama-3-8B",
    "parent_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c"
  },
  "fb0acb6c": {
    "commit_full": "fb0acb6c72874e98617cabee4ff4851569374fc9",
    "benchmark_mode": "standalone",
    "perf_command": "python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 6000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8",
    "model": "deepseek-ai/DeepSeek-R1",
    "parent_commit": "92b0ce2ac75e251fe683f5b720f07001782054ff"
  },
  "fc542144": {
    "commit_full": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c"
  },
  "fc7b8d1e": {
    "commit_full": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --backend vllm --num-prompts 100",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "parent_commit": "67abdbb42fdbb59c274130368981c0d0ac3539e3"
  },
  "fe66b347": {
    "commit_full": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "benchmark_mode": "serving",
    "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
    "model": "ibm-ai-platform/Bamba-9B",
    "parent_commit": "270a5da495d24e947a71e2fa0c56635f4fad2dc3"
  },
  "9f1710f1": {
    "commit_full": "9f1710f1",
    "benchmark_mode": "serving",
    "perf_command": null,
    "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
    "parent_commit": "e642ec962cf2283f9aa44492727e6efc17a32129"
  },
  "19d98e0c": {
    "commit_full": "19d98e0c",
    "benchmark_mode": "serving",
    "perf_command": null,
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "parent_commit": "2b04c209ee98174f29f1fc98f0dc3222d652a7bd"
  }
}