[
  {
    "commit_hash": "baeded25699f9f4851843306f27f685c4d4ee7c5",
    "commit_date": "2025-01-31T21:52:51-08:00",
    "commit_subject": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)"
  },
  {
    "commit_hash": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "commit_date": "2025-01-31T15:37:30-08:00",
    "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)"
  },
  {
    "commit_hash": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "commit_date": "2025-01-26T00:42:37-08:00",
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)"
  },
  {
    "commit_hash": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "commit_date": "2025-01-24T11:34:27+08:00",
    "commit_subject": "[perf] fix perf regression from #12253 (#12380)"
  },
  {
    "commit_hash": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
    "commit_date": "2025-01-22T22:22:12Z",
    "commit_subject": "[Frontend][V1] Online serving performance improvements (#12287)"
  },
  {
    "commit_hash": "310aca88c984983189a57f1b72e3b1dde89fb92f",
    "commit_date": "2025-01-09T07:18:21Z",
    "commit_subject": "[perf]fix current stream (#11870)"
  },
  {
    "commit_hash": "526de822d501c792b051c864ba873a836d78d5bf",
    "commit_date": "2025-01-08T20:23:15Z",
    "commit_subject": "[Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models (#11698)"
  },
  {
    "commit_hash": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "commit_date": "2025-01-02T12:04:58-07:00",
    "commit_subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)"
  },
  {
    "commit_hash": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
    "commit_date": "2024-12-18T23:38:02-08:00",
    "commit_subject": "[Misc] Optimize ray worker initialization time (#11275)"
  },
  {
    "commit_hash": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "commit_date": "2024-12-15T13:33:00-08:00",
    "commit_subject": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)"
  },
  {
    "commit_hash": "886936837ca89e5645bc1f71cc0e1492b65b1590",
    "commit_date": "2024-12-14T11:38:10-08:00",
    "commit_subject": "[Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and l"
  },
  {
    "commit_hash": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "commit_date": "2024-12-11T23:14:20-08:00",
    "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)"
  },
  {
    "commit_hash": "3b61cb450d899dc423feb264c297d4d18d701678",
    "commit_date": "2024-12-09T12:38:46-08:00",
    "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)"
  },
  {
    "commit_hash": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
    "commit_date": "2024-12-03T15:17:00+08:00",
    "commit_subject": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)"
  },
  {
    "commit_hash": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "commit_date": "2024-11-28T09:01:02-08:00",
    "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)"
  },
  {
    "commit_hash": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "commit_date": "2024-11-28T08:31:28-08:00",
    "commit_subject": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)"
  },
  {
    "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "commit_date": "2024-11-15T00:38:20Z",
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)"
  },
  {
    "commit_hash": "83450458339b07765b0e72a822e5fe93eeaf5258",
    "commit_date": "2024-10-16T13:37:45-06:00",
    "commit_subject": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)"
  },
  {
    "commit_hash": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
    "commit_date": "2024-09-03T18:50:29Z",
    "commit_subject": "[Core] Optimize Async + Multi-step (#8050)"
  },
  {
    "commit_hash": "6e36f4fa6ce64619b9ea94c88a157f5783a63a65",
    "commit_date": "2024-09-02T14:20:12-07:00",
    "commit_subject": "improve chunked prefill performance"
  },
  {
    "commit_hash": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "commit_date": "2024-08-28T16:10:12-07:00",
    "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)"
  },
  {
    "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
    "commit_date": "2024-08-28T00:36:31-07:00",
    "commit_subject": "[Performance] Enable chunked prefill and prefix caching together (#7753)"
  },
  {
    "commit_hash": "2deb029d115dadd012ce5ea70487a207cb025493",
    "commit_date": "2024-08-26T11:24:53-07:00",
    "commit_subject": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)"
  },
  {
    "commit_hash": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
    "commit_date": "2024-08-09T15:49:36Z",
    "commit_subject": "[Performance] e2e overheads reduction: Small followup diff (#7364)"
  },
  {
    "commit_hash": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
    "commit_date": "2024-08-06T12:34:25-07:00",
    "commit_subject": "[Core] Optimize evictor-v2 performance (#7193)"
  },
  {
    "commit_hash": "6ce01f30667bbae33f112152e07a3b66b841078f",
    "commit_date": "2024-08-01T18:29:52-07:00",
    "commit_subject": "[Performance] Optimize `get_seqs` (#7051)"
  },
  {
    "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
    "commit_date": "2024-07-25T21:31:31-07:00",
    "commit_subject": "[Core] Use array to speedup padding (#6779)"
  },
  {
    "commit_hash": "9ed82e7074a18e25680ab106fc846364ad97bc00",
    "commit_date": "2024-07-19T12:10:56-07:00",
    "commit_subject": "[Misc] Small perf improvements (#6520)"
  },
  {
    "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
    "commit_date": "2024-07-01T20:10:37-07:00",
    "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)"
  },
  {
    "commit_hash": "7c01f706418d593b3cf23d2ec9110dca7151c539",
    "commit_date": "2024-06-29T12:47:53Z",
    "commit_subject": "[Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum (#5974)"
  },
  {
    "commit_hash": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
    "commit_date": "2024-06-13T09:33:14-07:00",
    "commit_subject": "[Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)"
  },
  {
    "commit_hash": "8d75fe48ca5f46b7af0f5201d8500b9604eed769",
    "commit_date": "2024-06-07T08:42:35Z",
    "commit_subject": "[Kernel] Switch fp8 layers to use the CUTLASS kernels (#5183)"
  },
  {
    "commit_hash": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
    "commit_date": "2024-05-13T14:57:07-07:00",
    "commit_subject": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2"
  },
  {
    "commit_hash": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
    "commit_date": "2024-05-09T16:38:07-07:00",
    "commit_subject": "[Kernel] [FP8] Improve FP8 linear layer performance (#4691)"
  },
  {
    "commit_hash": "d7740ea4dcee4ab75d7d6eef723f33cae957b288",
    "commit_date": "2024-05-08T08:42:28-07:00",
    "commit_subject": "[Core] Optimize sampler get_logprobs (#4594)"
  },
  {
    "commit_hash": "2a052011ca473a9dc8160f3daa1f5f63a2ad1fe3",
    "commit_date": "2024-05-04T11:45:16-07:00",
    "commit_subject": "[Kernel] Support MoE Fp8 Checkpoints for Mixtral (Static Weights with Dynamic/Static Activations) (#"
  },
  {
    "commit_hash": "ad8d696a99ca1eee19f1404e16e8e82df592ff85",
    "commit_date": "2024-04-22T21:11:06Z",
    "commit_subject": "[Core] Scheduler perf fix (#4270)"
  },
  {
    "commit_hash": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "commit_date": "2024-04-06T19:14:06-07:00",
    "commit_subject": "[Core] latency optimization (#3890)"
  },
  {
    "commit_hash": "b6d103542c654fb63013a1e45a586d654ae36a2a",
    "commit_date": "2024-03-30T14:26:38-07:00",
    "commit_subject": "[Kernel] Layernorm performance optimization (#3662)"
  },
  {
    "commit_hash": "3a243095e5e7b655b63ab08fbd5936cb40850415",
    "commit_date": "2024-03-25T16:03:02-07:00",
    "commit_subject": "Optimize `_get_ranks` in Sampler (#3623)"
  },
  {
    "commit_hash": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "commit_date": "2024-03-22T13:44:12-07:00",
    "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)"
  },
  {
    "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
    "commit_date": "2024-03-22T12:28:14-07:00",
    "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)"
  },
  {
    "commit_hash": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
    "commit_date": "2024-03-20T00:11:11-07:00",
    "commit_subject": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix cac"
  },
  {
    "commit_hash": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
    "commit_date": "2023-12-13T23:55:07-08:00",
    "commit_subject": "Optimize Mixtral with expert parallelism (#2090)"
  },
  {
    "commit_hash": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
    "commit_date": "2023-10-13T09:59:07-07:00",
    "commit_subject": "Improve detokenization performance (#1338)"
  },
  {
    "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
    "commit_date": "2023-04-01T00:51:08+08:00",
    "commit_subject": "Optimize tensor parallel execution speed (#17)"
  },
  {
    "commit_hash": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
    "commit_date": "2023-02-23T09:31:55Z",
    "commit_subject": "Add unoptimized OPT Attention"
  },
  {
    "commit_hash": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "commit_date": "2025-08-02T01:59:34-07:00",
    "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)"
  },
  {
    "commit_hash": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "commit_date": "2025-08-02T01:43:52-07:00",
    "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)"
  },
  {
    "commit_hash": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "commit_date": "2025-08-01T19:18:51-04:00",
    "commit_subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)"
  },
  {
    "commit_hash": "ac45c44d98e77f30e47b8fb69134f4635183070d",
    "commit_date": "2025-08-01T10:14:38-07:00",
    "commit_subject": "[Bugfix] [Performance] DeepEPHighThroughput + DeepSeek : Quant before Dispatch (#21837)"
  },
  {
    "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
    "commit_date": "2025-07-28T18:49:04-04:00",
    "commit_subject": "[Perf] Disable chunked local attention by default with llama4 (#21761)"
  },
  {
    "commit_hash": "61b8cea3b42feab021d506e9143551de18f9165c",
    "commit_date": "2025-07-24T03:21:46-07:00",
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)"
  },
  {
    "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
    "commit_date": "2025-07-22T07:07:44-07:00",
    "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)"
  },
  {
    "commit_hash": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "commit_date": "2025-07-22T06:17:47-07:00",
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further optimize block_pool (#21"
  },
  {
    "commit_hash": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "commit_date": "2025-07-22T05:27:18-07:00",
    "commit_subject": "[Core] Optimize update checks in LogitsProcessor (#21245)"
  },
  {
    "commit_hash": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
    "commit_date": "2025-07-21T21:49:01-07:00",
    "commit_subject": "Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)"
  },
  {
    "commit_hash": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
    "commit_date": "2025-07-21T11:19:23-07:00",
    "commit_subject": "[perf] Speed up align sum kernels (#21079)"
  },
  {
    "commit_hash": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "commit_date": "2025-07-19T02:33:01-07:00",
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)"
  },
  {
    "commit_hash": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
    "commit_date": "2025-07-18T23:09:51-07:00",
    "commit_subject": "[Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)"
  },
  {
    "commit_hash": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "commit_date": "2025-07-16T22:13:00-07:00",
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)"
  },
  {
    "commit_hash": "c0569dbc82b5e945a77878190114d1b68027828b",
    "commit_date": "2025-07-14T19:47:16Z",
    "commit_subject": "[Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)"
  },
  {
    "commit_hash": "22dd9c2730dc1124b9d0ac15fff223d0b8d9020b",
    "commit_date": "2025-07-07T19:08:12Z",
    "commit_subject": "[Kernel] Optimize Prefill Attention in Unified Triton Attention Kernel (#20308)"
  },
  {
    "commit_hash": "9a3b88328f7e434cac35b90ee463de6689f9a833",
    "commit_date": "2025-06-23T23:01:26-07:00",
    "commit_subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)"
  },
  {
    "commit_hash": "7661e92ef85e552936195ae4b803e292b9a96776",
    "commit_date": "2025-06-06T10:05:14Z",
    "commit_subject": "[Model] Optimize nemotron_h implementation (#19249)"
  },
  {
    "commit_hash": "e7523c2e031bc96740723ab63833d1cf94229ab4",
    "commit_date": "2025-05-26T11:49:36-04:00",
    "commit_subject": "[V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs (#18608"
  },
  {
    "commit_hash": "d55e446d1320d0f5f22bc3584f81f18d7924f166",
    "commit_date": "2025-05-24T06:51:22Z",
    "commit_subject": "[V1][Spec Decode] Small refactors to improve eagle bookkeeping performance (#18424)"
  },
  {
    "commit_hash": "e493e48524e9e78ab33eafec6461b3940e361189",
    "commit_date": "2025-05-23T03:38:23-07:00",
    "commit_subject": "[V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled (#17731)"
  },
  {
    "commit_hash": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
    "commit_date": "2025-05-15T23:31:02-07:00",
    "commit_subject": "[PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding (#17973)"
  },
  {
    "commit_hash": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
    "commit_date": "2025-05-01T03:29:01-07:00",
    "commit_subject": "[Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content (#17515)"
  },
  {
    "commit_hash": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
    "commit_date": "2025-04-22T19:11:56-07:00",
    "commit_subject": "[Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)"
  },
  {
    "commit_hash": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "commit_date": "2025-04-21T18:18:22Z",
    "commit_subject": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)"
  },
  {
    "commit_hash": "3092375e274e9e003961e600e10a6192d33ceaa0",
    "commit_date": "2025-04-16T19:28:32-07:00",
    "commit_subject": "[V1][Performance] Implement custom serializaton for MultiModalKwargs [Rebased] (#16432)"
  },
  {
    "commit_hash": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66",
    "commit_date": "2025-04-12T22:54:37+08:00",
    "commit_subject": "[Perf] Optimize Preparing Inputs for GPU Model Runner (#16484)"
  },
  {
    "commit_hash": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
    "commit_date": "2025-04-12T14:21:08+08:00",
    "commit_subject": "Optimized topk for topk=1 (Llama-4) (#16512)"
  },
  {
    "commit_hash": "b10e51989551cd80dd74079429ccf91f0807bd92",
    "commit_date": "2025-04-06T20:48:14Z",
    "commit_subject": "[V1][Minor] Optimize get_cached_block (#16135)"
  },
  {
    "commit_hash": "35fad35a485eac9195c510731ba4a9d297dfd963",
    "commit_date": "2025-03-26T10:56:47-07:00",
    "commit_subject": "[V1][Sampler] Faster top-k only implementation (#15478)"
  },
  {
    "commit_hash": "9d72daf4ced05a5fec1ad8ea2914a39296f402da",
    "commit_date": "2025-03-24T22:44:08Z",
    "commit_subject": "[V1][Perf] Simpler request output queues (#15156)"
  },
  {
    "commit_hash": "296f927f2493908984707354e3cc5d7b2e41650b",
    "commit_date": "2025-03-20T19:21:08-07:00",
    "commit_subject": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14857)"
  },
  {
    "commit_hash": "22d33baca2c0c639cfd45c48e99803e56c3efa74",
    "commit_date": "2025-03-19T21:04:41Z",
    "commit_subject": "[FrontEnd][Perf] `merge_async_iterators` fast-path for single-prompt requests (#15150)"
  },
  {
    "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
    "commit_date": "2025-03-18T14:31:54-07:00",
    "commit_subject": "[V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)"
  },
  {
    "commit_hash": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
    "commit_date": "2025-03-14T20:45:42-07:00",
    "commit_subject": "Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U\u2026 (#14848)"
  },
  {
    "commit_hash": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "commit_date": "2025-03-14T16:36:18-04:00",
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14778)"
  },
  {
    "commit_hash": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
    "commit_date": "2025-03-11T07:39:56Z",
    "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)"
  },
  {
    "commit_hash": "fb0acb6c72874e98617cabee4ff4851569374fc9",
    "commit_date": "2025-03-10T12:06:58-07:00",
    "commit_subject": "[Perf] Improve MLA on V1 (#14540)"
  },
  {
    "commit_hash": "ca7a2d5f28eac9621474563cdda0e08596222755",
    "commit_date": "2025-03-07T22:18:53-08:00",
    "commit_subject": "Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14384)\" (#14471)"
  },
  {
    "commit_hash": "dae68969774e41b93b01cd31171ca033a92b574a",
    "commit_date": "2025-03-06T19:59:14-08:00",
    "commit_subject": "[Perf] Reduce MLA CPU overheads in V1 (#14384)"
  },
  {
    "commit_hash": "9f1710f1ace3535920c0bb6d4cc329c36289080e",
    "commit_date": "2025-03-06T09:35:49-08:00",
    "commit_subject": "Fix mla prefill context performance (#13897)"
  },
  {
    "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
    "commit_date": "2025-03-04T20:59:22+01:00",
    "commit_subject": "Fix performance when `--generation-config` is not `None` (#14223)"
  },
  {
    "commit_hash": "19d98e0c7db96713f0e2201649159431177a56e2",
    "commit_date": "2025-03-03T16:29:53-05:00",
    "commit_subject": "[Kernel] Optimize moe intermediate_cache usage (#13625)"
  },
  {
    "commit_hash": "e206b5433109d298e53451015465b2bf8f03ef0a",
    "commit_date": "2025-02-26T14:58:24+08:00",
    "commit_subject": "[v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine (#13837)"
  },
  {
    "commit_hash": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
    "commit_date": "2025-02-20T10:59:36-08:00",
    "commit_subject": "fix neuron performance issue (#13589)"
  },
  {
    "commit_hash": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
    "commit_date": "2025-02-20T04:01:02Z",
    "commit_subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS (#13577)"
  },
  {
    "commit_hash": "4c822298981a8f7521492075ff72659985fc4c3f",
    "commit_date": "2025-02-18T13:19:58-08:00",
    "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)"
  },
  {
    "commit_hash": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "commit_date": "2025-02-18T12:15:33-08:00",
    "commit_subject": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)"
  }
]