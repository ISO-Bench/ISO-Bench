[
  {
    "human_commit_short": "19d98e0c",
    "human_commit_full": "19d98e0c7db96713f0e2201649159431177a56e2",
    "benchmark_type": "serving",
    "model": "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13625",
    "commit_subject": "[Kernel] Optimize moe intermediate_cache usage (#13625)",
    "parent_commit": "2b04c209ee98174f29f1fc98f0dc3222d652a7bd"
  },
  {
    "human_commit_short": "25ebed2f",
    "human_commit_full": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11214",
    "commit_subject": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)",
    "parent_commit": "d263bd9df7b2f5586910e5d006a11ff11ba7c310"
  },
  {
    "human_commit_short": "299ebb62",
    "human_commit_full": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
    "benchmark_type": "serving",
    "model": "Qwen/Qwen2.5-1.5B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/16436",
    "commit_subject": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)",
    "parent_commit": "f728ab8e3578c22b42ed53e51b5e8ec35328d8b9"
  },
  {
    "human_commit_short": "2f192835",
    "human_commit_full": "2f1928354903ae0c6edfe76cc90081eb513ead2c",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/3890",
    "commit_subject": "[Core] latency optimization (#3890)",
    "parent_commit": "95baec828f3ee046074dace1d88202a920b7dc15"
  },
  {
    "human_commit_short": "30172b49",
    "human_commit_full": "30172b4947c52890b808c6da3a6c7580f55cbb74",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13244",
    "commit_subject": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)",
    "parent_commit": "a4d577b37944cbfa1bc62e4869667d1e2739d62a"
  },
  {
    "human_commit_short": "3b61cb45",
    "human_commit_full": "3b61cb450d899dc423feb264c297d4d18d701678",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10989",
    "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)",
    "parent_commit": "edc4fa31888b4a41060acb7b16250540f051ad59"
  },
  {
    "human_commit_short": "4c822298",
    "human_commit_full": "4c822298981a8f7521492075ff72659985fc4c3f",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13365",
    "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)",
    "parent_commit": "c8d70e2437feecdb3762ce17298df33439ae1bd1"
  },
  {
    "human_commit_short": "58eee5f2",
    "human_commit_full": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20000",
    "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list convers",
    "parent_commit": "067c34a1559400e956311f067ddd185f54207a2b"
  },
  {
    "human_commit_short": "61b8cea3",
    "human_commit_full": "61b8cea3b42feab021d506e9143551de18f9165c",
    "benchmark_type": "throughput",
    "model": "meta-llama/Llama-3.2-3B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21137",
    "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)",
    "parent_commit": "526078a96c52af678a1ddbdc3ecf78265e358f2b"
  },
  {
    "human_commit_short": "6d0734c5",
    "human_commit_full": "6d0734c562e759fdb7076d762222b3881e62ab1f",
    "benchmark_type": "serving",
    "model": "mistralai/Mistral-7B-Instruct-v0.3",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20645",
    "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645",
    "parent_commit": "7d94577138e3d4c7bcfd781337ee1e5a2befa685"
  },
  {
    "human_commit_short": "6dd94dbe",
    "human_commit_full": "6dd94dbe94c1820a1e224cba65efcf0befa97995",
    "benchmark_type": "latency",
    "model": "meta-llama/Meta-Llama-3-8B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12380",
    "commit_subject": "[perf] fix perf regression from #12253 (#12380)",
    "parent_commit": "0e74d797ce8618fdb685126e0ff8576fb966e6ad"
  },
  {
    "human_commit_short": "8a4e5c5f",
    "human_commit_full": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/20906",
    "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)",
    "parent_commit": "76b494444fd864ffc53a623420668d1865c804b9"
  },
  {
    "human_commit_short": "8c1e77fb",
    "human_commit_full": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10742",
    "commit_subject": "[Kernel] Update vllm-flash-attn version to reduce CPU overheads (#10742)",
    "parent_commit": "5fc5ce0fe45f974fc8840175e8321652238400f0"
  },
  {
    "human_commit_short": "98f47f2a",
    "human_commit_full": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
    "benchmark_type": "latency",
    "model": "facebook/opt-125m",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10733",
    "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)",
    "parent_commit": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f"
  },
  {
    "human_commit_short": "9f1710f1",
    "human_commit_full": "9f1710f1ace3535920c0bb6d4cc329c36289080e",
    "benchmark_type": "serving",
    "model": "deepseek-ai/DeepSeek-V2-Lite-Chat",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/13897",
    "commit_subject": "Fix mla prefill context performance (#13897)",
    "parent_commit": "e642ec962cf2283f9aa44492727e6efc17a32129"
  },
  {
    "human_commit_short": "a3223766",
    "human_commit_full": "a32237665df876fcb51196dc209e8aff9fd89d29",
    "benchmark_type": "serving",
    "model": "facebook/opt-125m",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21245",
    "commit_subject": "[Core] Optimize update checks in LogitsProcessor (#21245)",
    "parent_commit": "bc8a8ce5ec374dd18e86f59be7cb0057a4b21992"
  },
  {
    "human_commit_short": "b2e0ad3b",
    "human_commit_full": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/10339",
    "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)",
    "parent_commit": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b"
  },
  {
    "human_commit_short": "b55ed6ef",
    "human_commit_full": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11692",
    "commit_subject": "[V1][Minor] Optimize token_ids_cpu copy (#11692)",
    "parent_commit": "2f385183f35497e030ef22c9820d83b83bc4f6db"
  },
  {
    "human_commit_short": "b690e348",
    "human_commit_full": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
    "benchmark_type": "serving",
    "model": "ibm-ai-platform/Bamba-9B-v2",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21075",
    "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)",
    "parent_commit": "25373b6c6cc2068e3914fa906d3240088f7af157"
  },
  {
    "human_commit_short": "bfdb1ba5",
    "human_commit_full": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/3469",
    "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)",
    "parent_commit": "cf2f084d56a1293cb08da2393984cdc7685ac019"
  },
  {
    "human_commit_short": "ce6bf3a2",
    "human_commit_full": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
    "benchmark_type": "throughput",
    "model": "google/gemma-2b",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/7898",
    "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)",
    "parent_commit": "3cdfe1f38b2c07a10a1681cd2d60c3bea1bae2f0"
  },
  {
    "human_commit_short": "ed250545",
    "human_commit_full": "ed25054577f7abca2aee32a5290200c4a1aed561",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/21222",
    "commit_subject": "[Core] Introduce popleft_n and append_n in FreeKVCacheBlockQueue to further opti",
    "parent_commit": "10904e6d755051260a7c3ce98659d8907c74caa9"
  },
  {
    "human_commit_short": "eefbf4a6",
    "human_commit_full": "eefbf4a68b7b0a5b8364a59647906be1b7f043e2",
    "benchmark_type": "latency",
    "model": "Qwen/Qwen3-30B-A3B-FP8",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/22036",
    "commit_subject": "[Perf] Optimize `reshape_and_cache_flash` CUDA Kernel (#22036)",
    "parent_commit": "88faa466d788e25082c02dc9688931d7976361f9"
  },
  {
    "human_commit_short": "f092153f",
    "human_commit_full": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/11111",
    "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111",
    "parent_commit": "1da8f0e1dddaf8625829e7ecca7fce93eb685c03"
  },
  {
    "human_commit_short": "fa63e710",
    "human_commit_full": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
    "benchmark_type": "latency",
    "model": "meta-llama/Llama-3-8B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12094",
    "commit_subject": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)",
    "parent_commit": "2a0309a646b1ed83a0c40974e08c8dc628726d3c"
  },
  {
    "human_commit_short": "fc542144",
    "human_commit_full": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
    "benchmark_type": "serving",
    "model": "meta-llama/Llama-3.1-8B-Instruct",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/12563",
    "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)",
    "parent_commit": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c"
  },
  {
    "human_commit_short": "fe66b347",
    "human_commit_full": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
    "benchmark_type": "serving",
    "model": "ibm-ai-platform/Bamba-9B",
    "hardware": "H100",
    "pr_url": "https://github.com/vllm-project/vllm/pull/14778",
    "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory C",
    "parent_commit": "270a5da495d24e947a71e2fa0c56635f4fad2dc3"
  }
]