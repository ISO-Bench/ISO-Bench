{
  "commit_hash": "35fad35a485eac9195c510731ba4a9d297dfd963",
  "pr_url": "https://github.com/vllm-project/vllm/pull/15478",
  "pr_date": "2025-03-26",
  "timeline_text": "Copy link Member njhill commented Mar 25, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . When there's top-k in the batch but no top-p. For 128k vocab, 1024 batch size, 500 ops on A100, where max top k is 10: Before: 11.571 sec After: 2.136 sec Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions [V1][Sampler] Faster top-k only implementation ‚Ä¶ bcee0c4 Signed-off-by: Nick Hill <nhill@redhat.com> njhill requested review from WoosukKwon , robertgshaw2-redhat , ywang96 , comaniac and alexm-redhat as code owners March 25, 2025 15:43 Copy link github-actions bot commented Mar 25, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Mar 25, 2025 njhill mentioned this pull request Mar 25, 2025 [V1][TPU] Speed up top-k on TPU by using torch.topk #15242 Merged njhill commented Mar 25, 2025 View reviewed changes vllm/v1/sample/ops/topk_topp_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . NickLucche approved these changes Mar 25, 2025 View reviewed changes Copy link Contributor NickLucche left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Tested on TPU this won't work out of the box due to some broadcasting issue. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Also in-place cumsum for top-p ‚Ä¶ 7156150 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Mar 25, 2025 @NickLucche that's strange. Which op has that issue? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor NickLucche commented Mar 25, 2025 Not too surprising, torch xla has more constraining rules on broadcasting. This is the first error I have encountered F0325 16:28:32.957930 1304047 debug_macros.h:21] Non-OK-status: status.status()\nStatus: INVALID_ARGUMENT: Input dimension should be either 1 or equal to the output dimension it is broadcasting into; the 0th operand dimension is 4, the 0th output dimension is 1.\n*** Begin stack trace ***\n\ttsl::CurrentStackTrace[abi:cxx11]()\n\txla::Shape const* ConsumeValue<xla::Shape const*>(absl::lts_20230802::StatusOr<xla::Shape const*>&&)\n\ttorch_xla::ShapeHelper::ShapeOfXlaOp(xla::XlaOp)\n\ttorch_xla::InferOutputShape(absl::lts_20230802::Span<xla::Shape const>, std::function<xla::XlaOp (absl::lts_20230802::Span<xla::XlaOp const>)> const&)\n\t\n\t\n\ttorch_xla::XlaNode::GetOpShape(std::function<xla::Shape ()> const&) const\n\ttorch_xla::XlaNode::XlaNode(torch::lazy::OpKind, c10::ArrayRef<torch::lazy::Value>, std::function<xla::Shape ()> const&, unsigned long, torch::lazy::hash_t)\n\ttorch_xla::Gather::Gather(torch::lazy::Value const&, long, torch::lazy::Value const&)\n\tstd::shared_ptr<torch::lazy::Node> torch_xla::MakeNode<torch_xla::Gather, torch::lazy::Value, long&, torch::lazy::Value>(torch::lazy::Value&&, long&, torch::lazy::Value&&)\n\ttorch_xla::tensor_methods::gather(c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > const&, long, c10::intrusive_ptr<torch_xla::XLATensor, c10::detail::intrusive_target_default_null_type<torch_xla::XLATensor> > const&)\n\ttorch_xla::XLANativeFunctions::gather(at::Tensor const&, long, at::Tensor const&, bool)\n\t\n\t\n\tat::_ops::gather::redispatch(c10::DispatchKeySet, at::Tensor const&, long, at::Tensor const&, bool)\n\t\n\t\n\tat::_ops::gather::call(at::Tensor const&, long, at::Tensor const&, bool) on the .gather op. I expanded k but then ran into another issue. üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon reviewed Mar 25, 2025 View reviewed changes vllm/v1/sample/ops/topk_topp_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Add comments ‚Ä¶ 1feffb0 Signed-off-by: Nick Hill <nhill@redhat.com> WoosukKwon reviewed Mar 25, 2025 View reviewed changes vllm/v1/sample/ops/topk_topp_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/sample/ops/topk_topp_sampler.py @@ -138,8 +138,25 @@ def apply_top_k_top_p( This function sorts the logits tensor, which can be slow for large batches. \"\"\" if k is None and p is None: if p is None: if k is None: Copy link Collaborator WoosukKwon Mar 25, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Do we have a unit test checking the correctness of this? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author njhill Mar 26, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment We should really have blanket coverage for this kind of thing, including different combinations of parameters (i.e. top-k with/without top-p etc.). I'm not sure whether we do though. I will check and add a unit test to compare the two impls. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Add comments about in-place logits updates. ‚Ä¶ be9e5d7 Signed-off-by: Nick Hill <nhill@redhat.com> NickLucche suggested changes Mar 26, 2025 View reviewed changes Copy link Contributor NickLucche left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I tested this version again today and it's working on TPU too, nice one @njhill thanks! I was wondering could we still factor-out this topk opt into its own function so I can call it from TPU side? We agreed with @WoosukKwon to try and keep things separated, I'd like to keep forward_tpu around. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor NickLucche commented Mar 26, 2025 Something like a5bf849 #diff-6047245d864bf5fd68b5b947b735beca94723bad40d20bfc0803d9b3eea5c1edR121-R136 . Wdyt? Of course I'd wait for this PR to land and then rebase, I've shamelessly just copy-pasted your code there. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . NickLucche mentioned this pull request Mar 26, 2025 [V1][TPU] Enable Top K #15489 Merged njhill added 2 commits March 26, 2025 07:17 Add test ‚Ä¶ c09dd00 Signed-off-by: Nick Hill <nhill@redhat.com> Move to separate function per @NickLucche 's request ‚Ä¶ e47f5b9 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Mar 26, 2025 Thanks @NickLucche , I've split into separate function. And @WoosukKwon I've added a correctness test. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . njhill added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 26, 2025 WoosukKwon approved these changes Mar 26, 2025 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Thanks for addressing my comments. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details njhill merged commit 35fad35 into vllm-project : main Mar 26, 2025 39 checks passed Uh oh! There was an error while loading. Please reload this page . njhill deleted the torch-topk branch March 26, 2025 17:56 hyeygit mentioned this pull request Mar 30, 2025 [V1][TPU] TPU-optimized top-p implementation (avoids scattering). #15736 Merged Copy link Contributor hyeygit commented Mar 30, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @njhill really neat idea to threshold the logits! However I think one corner case where this would break is if there are duplicate elements in the logit that equal the cut off value (i.e. top_k_mask ). For example, given an input of [1, 2, 2, 2, 3] and k=3 , the current apply_top_k_only would return [-inf, 2, 2, 2, 3] while the correct result should be [-inf, -inf, 2, 2, 3] . In #15736 I use a similar thresholding logic for top-p, but introduced a small random perturbation to break the ties. Maybe the same idea can be used here for top-k as well. üëç 1 NickLucche reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . NickLucche mentioned this pull request Apr 1, 2025 [Core] Optimize topp/topk calculation in sampler #12156 Closed Alex4210987 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Apr 5, 2025 [V1][Sampler] Faster top-k only implementation ( vllm-project#15478 ) ‚Ä¶ 0e57df7 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: xinyuxiao <xinyuxiao2024@gmail.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1][Sampler] Faster top-k only implementation ( vllm-project#15478 ) ‚Ä¶ c116565 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [V1][Sampler] Faster top-k only implementation ( vllm-project#15478 ) ‚Ä¶ 2b30424 Signed-off-by: Nick Hill <nhill@redhat.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [V1][Sampler] Faster top-k only implementation ( vllm-project#15478 ) ‚Ä¶ eaded4b Signed-off-by: Nick Hill <nhill@redhat.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [V1][Sampler] Faster top-k only implementation ( vllm-project#15478 ) ‚Ä¶ c7eb537 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:35",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: Faster, Faster, Faster | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:35",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1][Sampler] Faster top-k only implementation (#15478)",
  "commit_message": "[V1][Sampler] Faster top-k only implementation (#15478)\n\nSigned-off-by: Nick Hill <nhill@redhat.com>",
  "commit_date": "2025-03-26T10:56:47-07:00",
  "files_changed": [
    "tests/v1/sample/test_topk_topp_sampler.py",
    "vllm/v1/sample/ops/topk_topp_sampler.py",
    "vllm/v1/sample/sampler.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 3,
    "num_hunks": 7,
    "num_edited_lines": 96,
    "num_non_test_edited_lines": 59,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/v1/sample/test_topk_topp_sampler.py b/tests/v1/sample/test_topk_topp_sampler.py\nnew file mode 100644\nindex 000000000..8a5076412\n--- /dev/null\n+++ b/tests/v1/sample/test_topk_topp_sampler.py\n@@ -0,0 +1,37 @@\n+# SPDX-License-Identifier: Apache-2.0\n+import torch\n+from torch import Generator\n+\n+from vllm.v1.sample.ops.topk_topp_sampler import apply_top_k_top_p\n+\n+DEVICE = \"cuda\"\n+\n+BATCH_SIZE = 1024\n+VOCAB_SIZE = 128 * 1024\n+\n+\n+def test_topk_impl_equivalance():\n+\n+    with torch.device(DEVICE):\n+        generator = Generator(device=DEVICE).manual_seed(33)\n+\n+        logits = torch.rand((BATCH_SIZE, VOCAB_SIZE), generator=generator)\n+\n+        # Random top-k values between 1 and 9.\n+        k = torch.randint(1, 10, (BATCH_SIZE, ), generator=generator)\n+\n+        # Set k=vocab_size for ~50% of requests in the batch (top-k disabled).\n+        k.masked_fill_(\n+            torch.randint(0,\n+                          2, (BATCH_SIZE, ),\n+                          generator=generator,\n+                          dtype=bool), VOCAB_SIZE)\n+\n+        # Top-k only implementation\n+        result1 = apply_top_k_top_p(logits=logits.clone(), k=k, p=None)\n+\n+        # Top-p + top-k\n+        no_op_top_p = torch.tensor([1.0])\n+        result2 = apply_top_k_top_p(logits=logits.clone(), k=k, p=no_op_top_p)\n+\n+        assert torch.allclose(result1, result2)\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea71187..5dfcae08b 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -19,6 +19,12 @@ except ImportError:\n \n \n class TopKTopPSampler(nn.Module):\n+    \"\"\"\n+    Module that performs optional top-k and top-p filtering followed by\n+    weighted random sampling of logits.\n+\n+    Implementations may update the logits tensor in-place.\n+    \"\"\"\n \n     def __init__(self):\n         super().__init__()\n@@ -84,7 +90,11 @@ class TopKTopPSampler(nn.Module):\n         k: Optional[torch.Tensor],\n         p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n-        \"\"\"PyTorch-native implementation of top-k and top-p sampling.\"\"\"\n+        \"\"\"\n+        PyTorch-native implementation of top-k and top-p sampling.\n+\n+        The logits tensor may be updated in-place.\n+        \"\"\"\n         logits = apply_top_k_top_p(logits, k, p)\n         probs = logits.softmax(dim=-1, dtype=torch.float32)\n         return random_sample(probs, generators)\n@@ -136,10 +146,18 @@ def apply_top_k_top_p(\n ) -> torch.Tensor:\n     \"\"\"Apply top-k and top-p masks to the logits.\n \n-    This function sorts the logits tensor, which can be slow for large batches.\n+    If a top-p is used, this function will sort the logits tensor,\n+    which can be slow for large batches.\n+\n+    The logits tensor may be updated in-place.\n     \"\"\"\n-    if k is None and p is None:\n-        return logits\n+    if p is None:\n+        if k is None:\n+            return logits\n+\n+        # Avoid sorting vocab for top-k only case.\n+        return apply_top_k_only(logits, k)\n+\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n     if k is not None:\n@@ -153,7 +171,7 @@ def apply_top_k_top_p(\n     if p is not None:\n         # Apply top-p.\n         probs_sort = logits_sort.softmax(dim=-1)\n-        probs_sum = probs_sort.cumsum(dim=-1)\n+        probs_sum = torch.cumsum(probs_sort, dim=-1, out=probs_sort)\n         top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n         # at least one\n         top_p_mask[:, -1] = False\n@@ -164,6 +182,31 @@ def apply_top_k_top_p(\n     return logits\n \n \n+def apply_top_k_only(\n+    logits: torch.Tensor,\n+    k: torch.Tensor,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Apply top-k mask to the logits.\n+\n+    This implementation doesn't involve sorting the entire vocab.\n+\n+    The logits tensor may be updated in-place.\n+    \"\"\"\n+    no_top_k_mask = k == logits.shape[1]\n+    # Set non-top-k rows to 1 so that we can gather.\n+    k = k.masked_fill(no_top_k_mask, 1)\n+    max_top_k = k.max()\n+    # topk.values tensor has shape [batch_size, max_top_k].\n+    # Convert top k to 0-based index in range [0, max_top_k).\n+    k_index = k.sub_(1).unsqueeze(1)\n+    top_k_mask = logits.topk(max_top_k, dim=1).values.gather(1, k_index)\n+    # Handle non-topk rows.\n+    top_k_mask.masked_fill_(no_top_k_mask.unsqueeze(1), -float(\"inf\"))\n+    logits.masked_fill_(logits < top_k_mask, -float(\"inf\"))\n+    return logits\n+\n+\n def random_sample(\n     probs: torch.Tensor,\n     generators: dict[int, torch.Generator],\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049dc..004f98496 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -87,6 +87,12 @@ class Sampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> torch.Tensor:\n+        \"\"\"Sample logits based on sampling metadata.\n+\n+        The various logits processing functions called in this method\n+        may update the logits tensor in-place.\n+        \"\"\"\n+\n         assert not (sampling_metadata.all_greedy\n                     and sampling_metadata.all_random)\n         if sampling_metadata.all_random:",
  "apis": [
    "vllm.v1.sample.ops.topk_topp_sampler.apply_top_k_top_p",
    "vllm.v1.sample.ops.topk_topp_sampler.apply_top_k_only",
    "vllm.v1.sample.ops.topk_topp_sampler.TopKTopPSampler.forward_native",
    "vllm.v1.sample.sampler.Sampler.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/ops/topk_topp_sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/sampler.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces a faster top-k only implementation by adding a new function (\"apply_top_k_only\") that avoids sorting the entire vocabulary, which is a costly operation. This change directly affects a core sampling function, making it more efficient. The modifications are made in source code files (vllm/v1/sample/ops/topk_topp_sampler.py and vllm/v1/sample/sampler.py) and are not limited to tests or documentation. While the commit message mentions \"Faster\", it directly implements performance improvement rather than just refactoring or renaming functions, and it impacts high-level API performance without being hardware specific. Therefore, the commit is performance/optimization related.",
  "llm_api_reason": "This commit improves the performance of top-k sampling by adding a specialized ‚Äútop-k only‚Äù implementation. It updates the apply_top_k_top_p function to bypass an expensive sort when only top-k filtering is needed and introduces a new helper function, apply_top_k_only, that directly applies the top-k mask without sorting the whole vocabulary. In addition, the docstrings in TopKTopPSampler and the Sampler layer are enhanced to reflect that the logits tensor may be updated in-place. These changes affect the sampling API functions that process and transform logits before token selection."
}