{
  "commit_hash": "e7b204268132cb775c139574c1ff4ad7e15c8f66",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21334",
  "pr_date": "2025-07-22",
  "timeline_text": "Copy link Contributor minosfuture commented Jul 21, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Purpose This reverts commit 9fb2d22 to fix #21322 Test Plan pytest -v -s tests/models/multimodal/generation/test_maverick.py lm_eval maverick Test Result UT passed lm_eval result: local-chat-completions (model=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8,base_url= http://127.0.0.1:8000/v1/chat/completions,num_concurrent=32 ), gen_kwargs: (None), limit: 200.0, num_fewshot: 5, batch_size: 1 Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ‚Üë 0.93 ¬± 0.0181 strict-match 5 exact_match ‚Üë 0.92 ¬± 0.0192 (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 roywei reacted with thumbs up emoji All reactions üëç 1 reaction Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 51db38e ‚Ä¶UTLASS MoE ( vllm-project#20762 )\"\n\nThis reverts commit 9fb2d22 .\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> minosfuture requested review from tlrmchlsmth , WoosukKwon , mgoin and robertgshaw2-redhat as code owners July 21, 2025 21:30 Copy link github-actions bot commented Jul 21, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the performance Performance-related issues label Jul 21, 2025 gemini-code-assist bot reviewed Jul 21, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request reverts a previous performance improvement to fix a correctness issue. The changes are mostly removing code related to the optimization. However, I've identified a critical issue where the revert breaks CUDA graph compatibility by creating new tensors inside a function that can be captured by a CUDA graph. This will cause benchmarks and potentially other features relying on CUDA graphs to fail. I've provided detailed comments and code suggestions across multiple files to address this by re-introducing the practice of passing stride tensors as arguments, which was the behavior before the original performance-enhancing change. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/kernels/moe/test_cutlass_moe.py @@ -207,10 +207,6 @@ def run_8_bit(moe_tensors: MOETensors8Bit, 'topk_ids': topk_ids, 'w1_scale': moe_tensors.w1_scale, 'w2_scale': moe_tensors.w2_scale, Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment To align with the proposed fix for CUDA graph compatibility, the stride tensors need to be passed to cutlass_moe_fp8 for testing. 'w2_scale' : moe_tensors . w2_scale , 'ab_strides1' : moe_tensors . ab_strides1 , 'ab_strides2' : moe_tensors . ab_strides2 , 'c_strides1' : moe_tensors . c_strides1 , 'c_strides2' : moe_tensors . c_strides2 , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/kernels/moe/test_cutlass_moe.py @@ -444,11 +440,6 @@ def test_run_cutlass_moe_fp8( expert_map[start:end] = list(range(num_local_experts)) expert_map = torch.tensor(expert_map, dtype=torch.int32, device=\"cuda\") Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors need to be created for the test to be consistent with the proposed fix for CUDA graph compatibility. expert_map = torch . tensor ( expert_map , dtype = torch . int32 , device = \"cuda\" ) ab_strides1 = torch . full (( e , ), k , device = \"cuda\" , dtype = torch . int64 ) ab_strides2 = torch . full (( e , ), n , device = \"cuda\" , dtype = torch . int64 ) c_strides1 = torch . full (( e , ), 2 * n , device = \"cuda\" , dtype = torch . int64 ) c_strides2 = torch . full (( e , ), k , device = \"cuda\" , dtype = torch . int64 ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/kernels/moe/test_cutlass_moe.py Comment on lines +451 to +452 a1q_scale, None, workspace13, workspace2, None, mt.a.dtype, per_act_token, per_out_channel, False) Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be passed to run_cutlass_moe_fp8 to align with the proposed fix for CUDA graph compatibility. a1q_scale , None , ab_strides1 , ab_strides2 , c_strides1 , c_strides2 , workspace13 , workspace2 , None , mt . a . dtype , per_act_token , per_out_channel , False ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/kernels/moe/test_pplx_cutlass_moe.py Comment on lines 126 to 131 experts = CutlassExpertsFp8(num_local_experts, out_dtype, per_act_token, per_out_ch, ab_strides1, ab_strides2, c_strides1, c_strides2, num_dispatchers=num_dispatchers, use_batched_format=True) Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors need to be created and passed to CutlassExpertsFp8 for the test to be consistent with the proposed fix for CUDA graph compatibility. You'll also need to re-introduce intermediate_dim which was removed in this PR. intermediate_dim = w2 . shape [ 2 ] ab_strides1 = torch . full (( num_local_experts , ), hidden_dim , device = \"cuda\" , dtype = torch . int64 ) ab_strides2 = torch . full (( num_local_experts , ), intermediate_dim , device = \"cuda\" , dtype = torch . int64 ) c_strides1 = torch . full (( num_local_experts , ), 2 * intermediate_dim , device = \"cuda\" , dtype = torch . int64 ) c_strides2 = torch . full (( num_local_experts , ), hidden_dim , device = \"cuda\" , dtype = torch . int64 ) experts = CutlassExpertsFp8 ( num_local_experts , out_dtype , per_act_token , per_out_ch , ab_strides1 , ab_strides2 , c_strides1 , c_strides2 , num_dispatchers = num_dispatchers , use_batched_format = True ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/fused_moe/cutlass_moe.py @@ -34,10 +35,6 @@ def run_cutlass_moe_fp8( w2_scale: Optional[torch.Tensor], a1q_scale: Optional[torch.Tensor], a2_scale: Optional[torch.Tensor], Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment To maintain CUDA graph compatibility, the stride tensors should be passed as arguments instead of being created inside this function. Please add them back to the function signature. Suggested change a2_scale : Optional [ torch . Tensor ], a2_scale : Optional [ torch . Tensor ], ab_strides1 : torch . Tensor , ab_strides2 : torch . Tensor , c_strides1 : torch . Tensor , c_strides2 : torch . Tensor , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 4 hidden conversations Load more‚Ä¶ vllm/model_executor/layers/fused_moe/cutlass_moe.py @@ -329,10 +332,6 @@ def cutlass_moe_fp8( topk_ids: torch.Tensor, w1_scale: torch.Tensor, w2_scale: torch.Tensor, Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be passed as arguments here as well to maintain CUDA graph compatibility. w2_scale : torch . Tensor , ab_strides1 : torch . Tensor , ab_strides2 : torch . Tensor , c_strides1 : torch . Tensor , c_strides2 : torch . Tensor , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/fused_moe/cutlass_moe.py @@ -403,10 +391,6 @@ def cutlass_moe_fp8( out_dtype=a.dtype, per_act_token_quant=per_act_token, per_out_ch_quant=per_out_ch, Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be passed to the CutlassExpertsFp8 constructor. per_out_ch_quant = per_out_ch , ab_strides1 = ab_strides1 , ab_strides2 = ab_strides2 , c_strides1 = c_strides1 , c_strides2 = c_strides2 , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py @@ -859,21 +859,6 @@ def process_weights_after_loading(self, layer: torch.nn.Module) -> None: layer.w13_weight_scale = torch.nn.Parameter(max_w13_scales, requires_grad=False) Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be pre-computed and stored here to be passed to the MoE kernel. This is necessary for CUDA graph compatibility. Suggested change device = layer . w13_weight . device # ab_strides1 and c_strides2 are the same self . ab_strides1_c_strides2 = torch . full (( layer . local_num_experts , ), layer . hidden_size , device = device , dtype = torch . int64 ) self . ab_strides2 = torch . full (( layer . local_num_experts , ), layer . intermediate_size_per_partition , device = device , dtype = torch . int64 ) self . c_strides1 = torch . full (( layer . local_num_experts , ), 2 * layer . intermediate_size_per_partition , device = device , dtype = torch . int64 ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py @@ -896,10 +881,6 @@ def select_gemm_impl( moe.in_dtype, self.input_quant.strategy == QuantizationStrategy.TOKEN, self.weight_quant.strategy == QuantizationStrategy.CHANNEL, Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be passed to the CutlassExpertsFp8 constructor. self . weight_quant . strategy == QuantizationStrategy . CHANNEL , ab_strides1 = self . ab_strides1_c_strides2 , ab_strides2 = self . ab_strides2 , c_strides1 = self . c_strides1 , c_strides2 = self . ab_strides1_c_strides2 , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py @@ -968,10 +948,6 @@ def apply( expert_map=None if self.disable_expert_map else expert_map, w1_scale=layer.w13_weight_scale, w2_scale=layer.w2_weight_scale, Copy link Contributor gemini-code-assist bot Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The stride tensors should be passed to cutlass_moe_fp8 . w2_scale = layer . w2_weight_scale , ab_strides1 = self . ab_strides1_c_strides2 , ab_strides2 = self . ab_strides2 , c_strides1 = self . c_strides1 , c_strides2 = self . ab_strides1_c_strides2 , Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions houseroad approved these changes Jul 21, 2025 View reviewed changes Copy link Collaborator houseroad left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for reverting the original PR to help recover the trunk health. This will unblock our code sync as well. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator houseroad commented Jul 21, 2025 cc: @ElizaWszola , @tlrmchlsmth , @mgoin , @robertgshaw2-redhat this is blocking our internal work, so need to revert for now to unblock. Sorry about the inconvenience, and happy to help on landing the fixed version. Also if forward-fix is easy to land, we are happy to switch to that as well. :-) üëç 2 mgoin and minosfuture reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . houseroad enabled auto-merge (squash) July 21, 2025 22:04 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 21, 2025 houseroad added\n  the llama Related to Llama models label Jul 21, 2025 mgoin added this to the v0.10.0 milestone Jul 22, 2025 mgoin approved these changes Jul 22, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Okay let's revert for now. Thanks for identifying this Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 2 houseroad and ElizaWszola reacted with thumbs up emoji ‚ù§Ô∏è 1 minosfuture reacted with heart emoji All reactions üëç 2 reactions ‚ù§Ô∏è 1 reaction simon-mo disabled auto-merge July 22, 2025 04:48 Hide details View details simon-mo merged commit e7b2042 into vllm-project : main Jul 22, 2025 109 of 111 checks passed Uh oh! There was an error while loading. Please reload this page . minosfuture added a commit\n        to minosfuture/vllm\n      that referenced\n      this pull request Jul 22, 2025 Reapply \"[Performance] Performance improvements in non-blockwise fp8 ‚Ä¶ ‚Ä¶ 2f39358 ‚Ä¶CUTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nThis reverts commit e7b2042 . minosfuture added a commit\n        to minosfuture/vllm\n      that referenced\n      this pull request Jul 23, 2025 Reapply \"[Performance] Performance improvements in non-blockwise fp8 ‚Ä¶ ‚Ä¶ 291c923 ‚Ä¶CUTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nThis reverts commit e7b2042 .\n\nThe original PR vllm-project#20762 is:\n\nAuthored-by: ElizaWszola <ewszola@redhat.com>\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> zixi-qi pushed a commit\n        to zixi-qi/vllm\n      that referenced\n      this pull request Jul 23, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ e780c7d ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: qizixi <qizixi@meta.com> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 663b3f1 ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ c24051b ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 5cf3120 ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: shuw <shuw@nvidia.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 5418f5a ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 4c1cd4d ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 26384dc ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 45b2eb2 ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 680fa6d ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 19f1d60 ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ a397d4d ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ c9e26e8 ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 Revert \"[Performance] Performance improvements in non-blockwise fp8 C‚Ä¶ ‚Ä¶ 27299ac ‚Ä¶UTLASS MoE ( vllm-project#20762 ) ( vllm-project#21334 )\n\nSigned-off-by: Ming Yang <minos.future@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:20",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, gsm8k | PERF: optimization, improvement | TEST: Test, Test, test",
  "analysis_extracted_at": "2025-09-07 17:50:20",
  "models": [
    "01-ai/Yi-1.5-9B-Chat"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=01-ai/Yi-1.5-9B-Chat,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model 01-ai/Yi-1.5-9B-Chat --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)",
  "commit_message": "Revert \"[Performance] Performance improvements in non-blockwise fp8 CUTLASS MoE (#20762) (#21334)\n\nSigned-off-by: Ming Yang <minos.future@gmail.com>",
  "commit_date": "2025-07-21T21:49:01-07:00",
  "files_changed": [
    "benchmarks/kernels/benchmark_grouped_gemm_cutlass.py",
    "csrc/moe/moe_permute_unpermute_op.cu",
    "tests/kernels/moe/test_cutlass_moe.py",
    "tests/kernels/moe/test_pplx_cutlass_moe.py",
    "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
    "vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 4,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 6,
    "num_hunks": 30,
    "num_edited_lines": 212,
    "num_non_test_edited_lines": 176,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/benchmarks/kernels/benchmark_grouped_gemm_cutlass.py b/benchmarks/kernels/benchmark_grouped_gemm_cutlass.py\nindex a6b42406b..1d4e730f9 100644\n--- a/benchmarks/kernels/benchmark_grouped_gemm_cutlass.py\n+++ b/benchmarks/kernels/benchmark_grouped_gemm_cutlass.py\n@@ -80,11 +80,6 @@ def bench_run(\n         a, score, topk, renormalize=False\n     )\n \n-    ab_strides1 = torch.full((num_experts,), k, device=\"cuda\", dtype=torch.int64)\n-    ab_strides2 = torch.full((num_experts,), n, device=\"cuda\", dtype=torch.int64)\n-    c_strides1 = torch.full((num_experts,), 2 * n, device=\"cuda\", dtype=torch.int64)\n-    c_strides2 = torch.full((num_experts,), k, device=\"cuda\", dtype=torch.int64)\n-\n     def run_triton_moe(\n         a: torch.Tensor,\n         w1: torch.Tensor,\n@@ -116,10 +111,6 @@ def bench_run(\n         w2: torch.Tensor,\n         w1_scale: torch.Tensor,\n         w2_scale: torch.Tensor,\n-        ab_strides1: torch.Tensor,\n-        ab_strides2: torch.Tensor,\n-        c_strides1: torch.Tensor,\n-        c_strides2: torch.Tensor,\n         topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         per_act_token: bool,\n@@ -134,10 +125,6 @@ def bench_run(\n                 topk_ids,\n                 w1_scale,\n                 w2_scale,\n-                ab_strides1,\n-                ab_strides2,\n-                c_strides1,\n-                c_strides2,\n                 per_act_token,\n                 a1_scale=None,\n             )\n@@ -149,10 +136,6 @@ def bench_run(\n         w2_q: torch.Tensor,\n         w1_scale: torch.Tensor,\n         w2_scale: torch.Tensor,\n-        ab_strides1: torch.Tensor,\n-        ab_strides2: torch.Tensor,\n-        c_strides1: torch.Tensor,\n-        c_strides2: torch.Tensor,\n         topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n     ):\n@@ -167,10 +150,6 @@ def bench_run(\n                 topk_ids,\n                 w1_scale,\n                 w2_scale,\n-                ab_strides1,\n-                ab_strides2,\n-                c_strides1,\n-                c_strides2,\n                 per_act_token,\n                 a1_scale=None,\n             )\n@@ -215,10 +194,6 @@ def bench_run(\n             w2_q,\n             w1_scale,\n             w2_scale,\n-            ab_strides1,\n-            ab_strides2,\n-            c_strides1,\n-            c_strides2,\n             topk_weights,\n             topk_ids,\n         )\n@@ -256,10 +231,6 @@ def bench_run(\n         \"w1_scale\": w1_scale,\n         \"w2_scale\": w2_scale,\n         \"per_act_token\": per_act_token,\n-        \"ab_strides1\": ab_strides1,\n-        \"ab_strides2\": ab_strides2,\n-        \"c_strides1\": c_strides1,\n-        \"c_strides2\": c_strides2,\n         # cuda graph params\n         \"cutlass_graph\": cutlass_graph,\n         \"triton_graph\": triton_graph,\n@@ -318,10 +289,6 @@ def bench_run(\n         w2_q,\n         w1_scale,\n         w2_scale,\n-        ab_strides1,\n-        ab_strides2,\n-        c_strides1,\n-        c_strides2,\n         topk_weights,\n         topk_ids,\n         per_act_token,\n@@ -330,7 +297,7 @@ def bench_run(\n \n     results.append(\n         benchmark.Timer(\n-            stmt=\"run_cutlass_moe(a, a_scale, w1_q, w2_q, w1_scale, w2_scale, ab_strides1, ab_strides2, c_strides1, c_strides2, topk_weights, topk_ids, per_act_token, num_runs)\",  # noqa: E501\n+            stmt=\"run_cutlass_moe(a, a_scale, w1_q, w2_q, w1_scale, w2_scale, topk_weights, topk_ids, per_act_token, num_runs)\",  # noqa: E501\n             globals=globals,\n             label=label,\n             sub_label=sub_label,\ndiff --git a/csrc/moe/moe_permute_unpermute_op.cu b/csrc/moe/moe_permute_unpermute_op.cu\nindex 13aecd800..a77471a7f 100644\n--- a/csrc/moe/moe_permute_unpermute_op.cu\n+++ b/csrc/moe/moe_permute_unpermute_op.cu\n@@ -160,30 +160,6 @@ __global__ void shuffleInputRowsKernel(const T* input,\n   }\n }\n \n-template <typename T>\n-__global__ void shuffleInputRowsKernelSlow(const T* input,\n-                                           const int32_t* dst2src_map,\n-                                           T* output, int64_t num_src_rows,\n-                                           int64_t num_dst_rows,\n-                                           int64_t num_cols) {\n-  int64_t dest_row_idx = blockIdx.x;\n-  int64_t const source_row_idx = dst2src_map[dest_row_idx];\n-\n-  if (blockIdx.x < num_dst_rows) {\n-    // Duplicate and permute rows\n-    auto const* source_row_ptr = input + source_row_idx * num_cols;\n-    auto* dest_row_ptr = output + dest_row_idx * num_cols;\n-\n-    int64_t const start_offset = threadIdx.x;\n-    int64_t const stride = blockDim.x;\n-\n-    for (int elem_index = start_offset; elem_index < num_cols;\n-         elem_index += stride) {\n-      dest_row_ptr[elem_index] = source_row_ptr[elem_index];\n-    }\n-  }\n-}\n-\n void shuffle_rows(const torch::Tensor& input_tensor,\n                   const torch::Tensor& dst2src_map,\n                   torch::Tensor& output_tensor) {\n@@ -197,24 +173,17 @@ void shuffle_rows(const torch::Tensor& input_tensor,\n   int64_t const num_src_rows = input_tensor.size(0);\n   int64_t const num_cols = input_tensor.size(1);\n \n-  if (num_cols % (128 / sizeof(input_tensor.scalar_type()) / 8)) {\n-    // use slow kernel if num_cols can't be aligned to 128 bits\n-    MOE_DISPATCH(input_tensor.scalar_type(), [&] {\n-      shuffleInputRowsKernelSlow<scalar_t><<<blocks, threads, 0, stream>>>(\n-          reinterpret_cast<scalar_t*>(input_tensor.data_ptr()),\n-          dst2src_map.data_ptr<int32_t>(),\n-          reinterpret_cast<scalar_t*>(output_tensor.data_ptr()), num_src_rows,\n-          num_dest_rows, num_cols);\n-    });\n-  } else {\n-    MOE_DISPATCH(input_tensor.scalar_type(), [&] {\n-      shuffleInputRowsKernel<scalar_t><<<blocks, threads, 0, stream>>>(\n-          reinterpret_cast<scalar_t*>(input_tensor.data_ptr()),\n-          dst2src_map.data_ptr<int32_t>(),\n-          reinterpret_cast<scalar_t*>(output_tensor.data_ptr()), num_src_rows,\n-          num_dest_rows, num_cols);\n-    });\n-  }\n+  TORCH_CHECK(!(num_cols % (128 / sizeof(input_tensor.scalar_type()) / 8)),\n+              \"num_cols must be divisible by 128 / \"\n+              \"sizeof(input_tensor.scalar_type()) / 8\");\n+\n+  MOE_DISPATCH(input_tensor.scalar_type(), [&] {\n+    shuffleInputRowsKernel<scalar_t><<<blocks, threads, 0, stream>>>(\n+        reinterpret_cast<scalar_t*>(input_tensor.data_ptr()),\n+        dst2src_map.data_ptr<int32_t>(),\n+        reinterpret_cast<scalar_t*>(output_tensor.data_ptr()), num_src_rows,\n+        num_dest_rows, num_cols);\n+  });\n }\n \n #else\ndiff --git a/tests/kernels/moe/test_cutlass_moe.py b/tests/kernels/moe/test_cutlass_moe.py\nindex 37727b75b..81fb3ec1d 100644\n--- a/tests/kernels/moe/test_cutlass_moe.py\n+++ b/tests/kernels/moe/test_cutlass_moe.py\n@@ -207,10 +207,6 @@ def run_8_bit(moe_tensors: MOETensors8Bit,\n         'topk_ids': topk_ids,\n         'w1_scale': moe_tensors.w1_scale,\n         'w2_scale': moe_tensors.w2_scale,\n-        'ab_strides1': moe_tensors.ab_strides1,\n-        'ab_strides2': moe_tensors.ab_strides2,\n-        'c_strides1': moe_tensors.c_strides1,\n-        'c_strides2': moe_tensors.c_strides2,\n         'per_act_token': per_act_token,\n         'a1_scale': None  #moe_tensors.a_scale\n     }\n@@ -444,11 +440,6 @@ def test_run_cutlass_moe_fp8(\n         expert_map[start:end] = list(range(num_local_experts))\n         expert_map = torch.tensor(expert_map, dtype=torch.int32, device=\"cuda\")\n \n-        ab_strides1 = torch.full((e, ), k, device=\"cuda\", dtype=torch.int64)\n-        ab_strides2 = torch.full((e, ), n, device=\"cuda\", dtype=torch.int64)\n-        c_strides1 = torch.full((e, ), 2 * n, device=\"cuda\", dtype=torch.int64)\n-        c_strides2 = torch.full((e, ), k, device=\"cuda\", dtype=torch.int64)\n-\n         activation = lambda o, i: torch.ops._C.silu_and_mul(o, i)\n         a1q, a1q_scale = moe_kernel_quantize_input(mt.a, mt.a_scale,\n                                                    torch.float8_e4m3fn,\n@@ -457,9 +448,8 @@ def test_run_cutlass_moe_fp8(\n         func = lambda output: run_cutlass_moe_fp8(\n             output, a1q, mt.w1_q, mt.w2_q, topk_ids, activation,\n             global_num_experts, expert_map, mt.w1_scale, mt.w2_scale,\n-            a1q_scale, None, ab_strides1, ab_strides2, c_strides1, c_strides2,\n-            workspace13, workspace2, None, mt.a.dtype, per_act_token,\n-            per_out_channel, False)\n+            a1q_scale, None, workspace13, workspace2, None, mt.a.dtype,\n+            per_act_token, per_out_channel, False)\n \n         workspace13.random_()\n         output_random_workspace = torch.empty(output_shape,\ndiff --git a/tests/kernels/moe/test_pplx_cutlass_moe.py b/tests/kernels/moe/test_pplx_cutlass_moe.py\nindex 77adc89ea..e4f4a393d 100644\n--- a/tests/kernels/moe/test_pplx_cutlass_moe.py\n+++ b/tests/kernels/moe/test_pplx_cutlass_moe.py\n@@ -75,7 +75,6 @@ def pplx_cutlass_moe(\n     assert torch.cuda.current_device() == pgi.local_rank\n \n     num_tokens, hidden_dim = a.shape\n-    intermediate_dim = w2.shape[2]\n     num_experts = w1.shape[0]\n     block_size = hidden_dim  # TODO support more cases\n     device = pgi.device\n@@ -124,31 +123,10 @@ def pplx_cutlass_moe(\n         num_local_experts=num_local_experts,\n         num_dispatchers=num_dispatchers)\n \n-    ab_strides1 = torch.full((num_local_experts, ),\n-                             hidden_dim,\n-                             device=\"cuda\",\n-                             dtype=torch.int64)\n-    ab_strides2 = torch.full((num_local_experts, ),\n-                             intermediate_dim,\n-                             device=\"cuda\",\n-                             dtype=torch.int64)\n-    c_strides1 = torch.full((num_local_experts, ),\n-                            2 * intermediate_dim,\n-                            device=\"cuda\",\n-                            dtype=torch.int64)\n-    c_strides2 = torch.full((num_local_experts, ),\n-                            hidden_dim,\n-                            device=\"cuda\",\n-                            dtype=torch.int64)\n-\n     experts = CutlassExpertsFp8(num_local_experts,\n                                 out_dtype,\n                                 per_act_token,\n                                 per_out_ch,\n-                                ab_strides1,\n-                                ab_strides2,\n-                                c_strides1,\n-                                c_strides2,\n                                 num_dispatchers=num_dispatchers,\n                                 use_batched_format=True)\n \ndiff --git a/vllm/model_executor/layers/fused_moe/cutlass_moe.py b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\nindex ff49d7bb7..2585a2953 100644\n--- a/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n@@ -13,7 +13,8 @@ from vllm.model_executor.layers.fused_moe.prepare_finalize import (\n     MoEPrepareAndFinalizeNoEP)\n from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (\n     TopKWeightAndReduceDelegate)\n-from vllm.model_executor.layers.fused_moe.utils import (_fp8_quantize,\n+from vllm.model_executor.layers.fused_moe.utils import (_fp8_perm,\n+                                                        _fp8_quantize,\n                                                         _resize_cache,\n                                                         extract_required_args)\n from vllm.scalar_type import scalar_types\n@@ -34,10 +35,6 @@ def run_cutlass_moe_fp8(\n     w2_scale: Optional[torch.Tensor],\n     a1q_scale: Optional[torch.Tensor],\n     a2_scale: Optional[torch.Tensor],\n-    ab_strides1: torch.Tensor,\n-    ab_strides2: torch.Tensor,\n-    c_strides1: torch.Tensor,\n-    c_strides2: torch.Tensor,\n     workspace13: torch.Tensor,\n     workspace2: torch.Tensor,\n     expert_num_tokens: Optional[torch.Tensor],\n@@ -156,11 +153,27 @@ def run_cutlass_moe_fp8(\n                                     problem_sizes1, problem_sizes2, a_map,\n                                     c_map, global_num_experts, N, K)\n \n-        a1q = ops.shuffle_rows(a1q, a_map)\n-        a1q_scale = (ops.shuffle_rows(a1q_scale, a_map)\n-                     if per_act_token else a1q_scale)\n+        a1q = _fp8_perm(a1q, a_map)\n+        a1q_scale = a1q_scale[a_map] if per_act_token else a1q_scale\n         expert_offsets = expert_offsets[:-1]\n \n+    ab_strides1 = torch.full((w1.size(0), ),\n+                             K,\n+                             device=device,\n+                             dtype=torch.int64)\n+    c_strides1 = torch.full((w1.size(0), ),\n+                            2 * N,\n+                            device=device,\n+                            dtype=torch.int64)\n+    ab_strides2 = torch.full((w1.size(0), ),\n+                             N,\n+                             device=device,\n+                             dtype=torch.int64)\n+    c_strides2 = torch.full((w1.size(0), ),\n+                            K,\n+                            device=device,\n+                            dtype=torch.int64)\n+\n     if use_batched_format:\n         c1 = _resize_cache(workspace13, (local_E * padded_M, N * 2))\n         c2 = _resize_cache(workspace2, (local_E * padded_M, N))\n@@ -197,8 +210,7 @@ def run_cutlass_moe_fp8(\n     else:\n         # We can't do this inplace because output may point to the same tensor\n         # as c3.\n-        output.copy_(ops.shuffle_rows(c3, c_map).view(M * topk, K),\n-                     non_blocking=True)\n+        output.copy_(c3[c_map].view(M * topk, K), non_blocking=True)\n \n \n # TODO (bnell): split class batched vs. non-batched?\n@@ -211,10 +223,6 @@ class CutlassExpertsFp8(mk.FusedMoEPermuteExpertsUnpermute):\n         out_dtype: Optional[torch.dtype],\n         per_act_token_quant: bool,\n         per_out_ch_quant: bool,\n-        ab_strides1: torch.Tensor,\n-        ab_strides2: torch.Tensor,\n-        c_strides1: torch.Tensor,\n-        c_strides2: torch.Tensor,\n         block_shape: Optional[list[int]] = None,\n         num_dispatchers: Optional[int] = None,\n         use_batched_format: bool = False,\n@@ -231,10 +239,6 @@ class CutlassExpertsFp8(mk.FusedMoEPermuteExpertsUnpermute):\n         self.max_experts_per_worker = max_experts_per_worker\n         self.num_dispatchers = num_dispatchers\n         self.out_dtype = out_dtype\n-        self.ab_strides1 = ab_strides1\n-        self.ab_strides2 = ab_strides2\n-        self.c_strides1 = c_strides1\n-        self.c_strides2 = c_strides2\n         self.use_batched_format = use_batched_format\n \n     @property\n@@ -314,8 +318,7 @@ class CutlassExpertsFp8(mk.FusedMoEPermuteExpertsUnpermute):\n         run_cutlass_moe_fp8(\n             output, hidden_states, w1, w2, topk_ids, activation_callable,\n             global_num_experts, expert_map, w1_scale, w2_scale, a1q_scale,\n-            a2_scale, self.ab_strides1, self.ab_strides2, self.c_strides1,\n-            self.c_strides2, workspace13, workspace2, expert_num_tokens,\n+            a2_scale, workspace13, workspace2, expert_num_tokens,\n             self.out_dtype if self.out_dtype is not None else in_dtype,\n             self.per_act_token_quant, self.per_out_ch_quant,\n             self.use_batched_format)\n@@ -329,10 +332,6 @@ def cutlass_moe_fp8(\n     topk_ids: torch.Tensor,\n     w1_scale: torch.Tensor,\n     w2_scale: torch.Tensor,\n-    ab_strides1: torch.Tensor,\n-    ab_strides2: torch.Tensor,\n-    c_strides1: torch.Tensor,\n-    c_strides2: torch.Tensor,\n     per_act_token: Optional[bool] = None,\n     activation: str = \"silu\",\n     a1_scale: Optional[torch.Tensor] = None,\n@@ -360,17 +359,6 @@ def cutlass_moe_fp8(\n         Shape: [num_experts] or [num_experts, 2N]\n     - w2_scale (torch.Tensor): The fp32 scale to dequantize w2_q.\n         Shape: [num_experts] or [num_experts, K]\n-    - ab_strides1 (torch.Tensor): The input/weight strides for the first gemm.\n-        Shape: [num_experts]\n-    - ab_strides2 (torch.Tensor): The input/weight strides for the second gemm.\n-        Shape: [num_experts]\n-    - c_strides1 (torch.Tensor): The output strides for the first gemm.\n-        Shape: [num_experts]\n-    - c_strides2 (torch.Tensor): The output strides for the second gemm.\n-        Shape: [num_experts]\n-    - per_act_token (Optional[bool]): Whether the scale is per-token or\n-                                      per-tensor.\n-    - activation (str): The activation function to use.\n     - a1_scale (Optional[torch.Tensor]): The optional fp32 scale to quantize a.\n         Shape: scalar or [M]\n     - a2_scale (Optional[torch.Tensor]): The optional fp32 scale to\n@@ -403,10 +391,6 @@ def cutlass_moe_fp8(\n             out_dtype=a.dtype,\n             per_act_token_quant=per_act_token,\n             per_out_ch_quant=per_out_ch,\n-            ab_strides1=ab_strides1,\n-            ab_strides2=ab_strides2,\n-            c_strides1=c_strides1,\n-            c_strides2=c_strides2,\n             use_batched_format=False,\n         ),\n     )\ndiff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\nindex 1a31410c3..2c93977be 100644\n--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\n+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py\n@@ -859,21 +859,6 @@ class CompressedTensorsW8A8Fp8MoECutlassMethod(CompressedTensorsMoEMethod):\n             layer.w13_weight_scale = torch.nn.Parameter(max_w13_scales,\n                                                         requires_grad=False)\n \n-        device = layer.w13_weight.device\n-        # ab_strides1 and c_strides2 are the same\n-        self.ab_strides1_c_strides2 = torch.full((layer.local_num_experts, ),\n-                                                 layer.hidden_size,\n-                                                 device=device,\n-                                                 dtype=torch.int64)\n-        self.ab_strides2 = torch.full((layer.local_num_experts, ),\n-                                      layer.intermediate_size_per_partition,\n-                                      device=device,\n-                                      dtype=torch.int64)\n-        self.c_strides1 = torch.full((layer.local_num_experts, ),\n-                                     2 * layer.intermediate_size_per_partition,\n-                                     device=device,\n-                                     dtype=torch.int64)\n-\n     def select_gemm_impl(\n         self,\n         prepare_finalize: FusedMoEPrepareAndFinalize,\n@@ -896,10 +881,6 @@ class CompressedTensorsW8A8Fp8MoECutlassMethod(CompressedTensorsMoEMethod):\n             moe.in_dtype,\n             self.input_quant.strategy == QuantizationStrategy.TOKEN,\n             self.weight_quant.strategy == QuantizationStrategy.CHANNEL,\n-            ab_strides1=self.ab_strides1_c_strides2,\n-            ab_strides2=self.ab_strides2,\n-            c_strides1=self.c_strides1,\n-            c_strides2=self.ab_strides1_c_strides2,\n             num_dispatchers=num_dispatchers,\n             use_batched_format=use_batched_format,\n         )\n@@ -946,8 +927,7 @@ class CompressedTensorsW8A8Fp8MoECutlassMethod(CompressedTensorsMoEMethod):\n             num_expert_group=num_expert_group,\n             custom_routing_function=custom_routing_function,\n             scoring_func=scoring_func,\n-            e_score_correction_bias=e_score_correction_bias,\n-            indices_type=self.topk_indices_dtype)\n+            e_score_correction_bias=e_score_correction_bias)\n \n         per_act_token = (\n             self.input_quant.strategy == QuantizationStrategy.TOKEN)\n@@ -968,10 +948,6 @@ class CompressedTensorsW8A8Fp8MoECutlassMethod(CompressedTensorsMoEMethod):\n                 expert_map=None if self.disable_expert_map else expert_map,\n                 w1_scale=layer.w13_weight_scale,\n                 w2_scale=layer.w2_weight_scale,\n-                ab_strides1=self.ab_strides1_c_strides2,\n-                ab_strides2=self.ab_strides2,\n-                c_strides1=self.c_strides1,\n-                c_strides2=self.ab_strides1_c_strides2,\n                 a1_scale=layer.w13_input_scale,\n                 a2_scale=layer.w2_input_scale,\n             )",
  "apis": [
    "vllm.cutlass_moe_fp8",
    "CutlassExpertsFp8.apply",
    "CompressedTensorsW8A8Fp8MoECutlassMethod.select_gemm_impl"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/cutlass_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit is a revert of a previous performance improvements commit. It touches non-test source code files (including benchmarking scripts, CUDA kernels, and fused operators) by removing parameters and code paths that were introduced for performance optimization of the fp8 CUTLASS MoE implementation. Although the commit rolls back performance improvements, the changes are directly related to performance by targeting the efficient execution of GEMM operations and fused kernels on CUDA devices (CPU performance tests are applicable). Thus, the commit satisfies the criteria for being performance/optimization related.",
  "llm_api_reason": "This commit reverts several changes made to the fp8 CUTLASS MoE implementations. In the benchmarks and tests the extra stride‚Äêparameters (ab_strides and c_strides) are removed from all function calls. In the CUTLASS MoE module the function run_cutlass_moe_fp8 and the CutlassExpertsFp8 class have been adjusted to no longer require or pass these stride tensors. Similarly, in the compressed tensors MoE method for FP8 (CompressedTensorsW8A8Fp8MoECutlassMethod) the use of these stride parameters is reverted. Overall, the commit eliminates the performance-related API changes by reverting the changes to the stride-related interfaces."
}