{
  "commit_hash": "fb0acb6c72874e98617cabee4ff4851569374fc9",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14540",
  "pr_date": "2025-03-10",
  "timeline_text": "Copy link Collaborator simon-mo commented Mar 10, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR helps V1 to mostly match and exceed (in most cases) V0's performance for MLA. Mostly by two things Fix @LucasWilkinson 's rotary_emb specialization ( [Perf] Reduce MLA CPU overheads in V1 #14384 , Revert \"[Perf] Reduce MLA CPU overheads in V1 (#14384)\" #14471 , [Bugfix] DeepSeek Accuracy #14476 ) to reduce CPU overhead. Identified that the cause of 0 GSM8K score comes from the cuda kernel needs the input to be continuous. Fixed it by make the input contiguous if possible. A better fix will be to change the kernel (help wanted). Reordered some operation in the build function, which ended up costing quite a bit overhead in my timing (p99 tail latency up to 1ms) This is by ensuring there is not GPU -> CPU communication. CPU -> GPU is fine. All the following ran in 8xH200. Performance Test (R1) We are still a bit worse on the short range but we became significantly better on longer range. 64% boost for 6k input. VLLM_USE_V1=1 python benchmarks/benchmark_throughput.py --model /home/vllm-dev/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 3000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8 Throughput: 1.09 requests/s, 4342.27 total tokens/s, 1085.57 output tokens/s VLLM_USE_V1=0 python benchmarks/benchmark_throughput.py --model /home/vllm-dev/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 3000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8 Throughput: 1.13 requests/s, 4536.67 total tokens/s, 1134.17 output tokens/s VLLM_USE_V1=1 python benchmarks/benchmark_throughput.py --model /home/vllm-dev/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 6000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8 Throughput: 0.87 requests/s, 6060.61 total tokens/s, 865.80 output tokens/s VLLM_USE_V1=0 python benchmarks/benchmark_throughput.py --model /home/vllm-dev/DeepSeek-R1 --load-format dummy --trust-remote-code --input-len 6000 --output-len 1000 --num-prompts 50 --tensor-parallel-size 8 Throughput: 0.53 requests/s, 3692.82 total tokens/s, 527.55 output tokens/s Performance Test (Small) We are 15% better for small model for 3k input. VLLM_USE_V1=1 python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-V2-Lite --load-format dummy --trust-remote-code --input-len 3000 --output-len 1000 --num-prompts 50 Throughput: 3.84 requests/s, 15364.27 total tokens/s, 3841.07 output tokens/s VLLM_USE_V1=0 python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-V2-Lite --load-format dummy --trust-remote-code --input-len 3000 --output-len 1000 --num-prompts 50 Throughput: 3.32 requests/s, 13275.67 total tokens/s, 3318.92 output tokens/s VLLM_USE_V1=0 python benchmarks/benchmark_throughput.py --model deepseek-ai/DeepSeek-V2-Lite --load-format dummy --trust-remote-code --input-len 3000 --output-len 1000 --num-prompts 50 --enable-chunked-prefill false Throughput: 3.32 requests/s, 13264.68 total tokens/s, 3316.17 output tokens/s Accuracy Test No regression. VLLM_USE_V1=\"1\" lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=16384 --task gsm8k --num_fewshot=5 --limit 100 --log_samples --output_path lmeval-results\n\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=16384), gen_kwargs: (None), limit: 100.0, num_fewshot: 5, batch_size: 1\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  | 0.65|¬±  |0.0479|\n|     |       |strict-match    |     5|exact_match|‚Üë  | 0.64|¬±  |0.0482|\n\n\nVLLM_USE_V1=\"0\" lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=16384 --task gsm8k --num_fewshot=5 --limit 100 --log_samples --output_path lmeval-results\n\nvllm (pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.9,trust_remote_code=True,max_model_len=16384), gen_kwargs: (None), limit: 100.0, num_fewshot: 5, batch_size: 1\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  | 0.66|¬±  |0.0476|\n|     |       |strict-match    |     5|exact_match|‚Üë  | 0.66|¬±  |0.0476| Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions [Perf] Improve MLA on V1 ‚Ä¶ e3c00a1 Signed-off-by: simon-mo <simon.mo@hey.com> simon-mo requested review from WoosukKwon , robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners March 10, 2025 05:50 Copy link github-actions bot commented Mar 10, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Mar 10, 2025 simon-mo requested a review\n  from LucasWilkinson March 10, 2025 05:51 simon-mo added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 10, 2025 fix lint ‚Ä¶ 8cf800f Signed-off-by: simon-mo <simon.mo@hey.com> tlrmchlsmth approved these changes Mar 10, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions LucasWilkinson approved these changes Mar 10, 2025 View reviewed changes Copy link Collaborator LucasWilkinson left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM left 1 nit. Thanks for working on this! (sorry this fell on your plate) good catch on number 2! my bad for not catching this! I was wondering if it would be better compute on the CPU in V1 but didn't really keep pushing on that, ill try to be more careful about reviewing CPU->GPU transfers in the future Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/v1/attention/backends/mla/common.py Outdated decode_q_pe_input = (decode_q_pe.clone().contiguous() if not decode_q_pe.is_contiguous() else decode_q_pe) Copy link Collaborator LucasWilkinson Mar 10, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: do we need clone here? my understanding is .continuous() will implicitly do a clone if its not contiguous and no-op if it already is: >>> x1 = torch.rand((4,4))\n>>> x2 = x1.t()\n>>> x1.is_contiguous()\nTrue\n>>> x2.is_contiguous()\nFalse\n>>> x1.data_ptr()\n94306274798528\n>>> x1.contiguous().data_ptr()\n94306274798528\n>>> x2.data_ptr()\n94306274798528\n>>> x2.contiguous().data_ptr()\n94306363886080 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator LucasWilkinson Mar 10, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment i.e. I think we can drop this line and just do: decode_q_pe[...], decode_k_pe[...] = self.rotary_emb(\n                 attn_metadata.decode.input_positions, decode_q_pe.contiguous(),\n                 decode_k_pe) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author simon-mo Mar 10, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Yup great point and i verified the perf. clone was a left over from previous debugging but your solution is great! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions simpler code from lucas ‚Ä¶ f8c28a4 Signed-off-by: simon-mo <simon.mo@hey.com> simon-mo enabled auto-merge (squash) March 10, 2025 16:13 simon-mo disabled auto-merge March 10, 2025 19:06 Hide details View details simon-mo merged commit fb0acb6 into vllm-project : main Mar 10, 2025 29 of 31 checks passed Uh oh! There was an error while loading. Please reload this page . LucasWilkinson mentioned this pull request Mar 11, 2025 [Bugfix] DeepSeek Accuracy #14476 Merged Copy link Contributor ZhongYingMatrix commented Mar 13, 2025 hi @simon-mo Thx for ur great work! Speaking of D2H operation, I notice that has_context on here would be a single element bool tensor, which incur H2D in following condition operation. Would it has an impact on performance? cc @LucasWilkinson All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author simon-mo commented Mar 13, 2025 good find. Fix welcomed! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hmellor mentioned this pull request Apr 2, 2025 [Performance]: 0.8.1 vs 0.7.4dev122 R1 H20 performance benchmark testÔºå0.8.1 What is the reason for the 14% performance improvement(throughput tokens/s) #15881 Closed 1 task lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [Perf] Improve MLA on V1 ( vllm-project#14540 ) ‚Ä¶ 8e41390 Signed-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Perf] Improve MLA on V1 ( vllm-project#14540 ) ‚Ä¶ ba35e3b Signed-off-by: simon-mo <simon.mo@hey.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:03",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, GSM8K | PERF: Throughput, Throughput, Throughput | TEST: Test, Test, Test",
  "analysis_extracted_at": "2025-09-07 17:52:03",
  "models": [
    "deepseek-ai/DeepSeek-V2-Lite",
    "deepseek-ai/DeepSeek-V2-Lite-Chat"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V2-Lite-Chat,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V2-Lite --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Perf] Improve MLA on V1 (#14540)",
  "commit_message": "[Perf] Improve MLA on V1 (#14540)\n\nSigned-off-by: simon-mo <simon.mo@hey.com>",
  "commit_date": "2025-03-10T12:06:58-07:00",
  "files_changed": [
    "vllm/v1/attention/backends/mla/common.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 7,
    "num_edited_lines": 68,
    "num_non_test_edited_lines": 68,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex 0b0f52167..526b792ab 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -223,6 +223,7 @@ from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n from vllm.model_executor.layers.quantization.utils.quant_utils import (\n     scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n+from vllm.platforms import current_platform\n from vllm.utils import cdiv, round_down\n \n try:\n@@ -471,18 +472,23 @@ class MLACommonMetadataBuilder(Generic[M]):\n               common_prefix_len: int) -> M:\n         assert self._num_decodes + self._num_prefills == num_reqs\n \n+        # Note(simon): be careful about the CPU <> GPU memory movement in this\n+        # function. We should avoid GPU -> CPU sync as much as possible because\n+        # it blocks on all previous kernels.\n         device = self.runner.device\n-        query_start_loc = self.runner.query_start_loc_cpu[:num_reqs + 1].to(\n-            device, non_blocking=True)\n-        seq_lens = self.runner.seq_lens_cpu[:num_reqs].to(device,\n-                                                          non_blocking=True)\n         block_table = (\n             self.runner.input_batch.block_table.get_device_tensor()[:num_reqs])\n+        query_start_loc = self.runner.query_start_loc_cpu[:num_reqs + 1].to(\n+            device, non_blocking=True)\n         slot_mapping = self.runner.slot_mapping_cpu[:num_actual_tokens].to(\n             device, non_blocking=True).long()\n         input_positions = self.runner.positions_cpu[:num_actual_tokens].to(\n             device, non_blocking=True).long()\n \n+        seq_lens_cpu = self.runner.seq_lens_cpu[:num_reqs]\n+        seq_lens = seq_lens_cpu.to(device, non_blocking=True)\n+        max_query_len = seq_lens_cpu.max().item()\n+\n         prefill_metadata = None\n         if self._num_prefills > 0:\n             reqs_start = self._num_decodes  # prefill_start\n@@ -490,24 +496,22 @@ class MLACommonMetadataBuilder(Generic[M]):\n \n             context_lens_cpu = self.runner.input_batch.\\\n                 num_computed_tokens_cpu_tensor[reqs_start:num_reqs]\n-            context_lens = context_lens_cpu.to(device, non_blocking=True)\n+            max_context_len_cpu = context_lens_cpu.max().item()\n+            num_prefills_with_context_cpu = (context_lens_cpu > 0).sum().item()\n \n             chunked_context_metadata = None\n             if self.chunked_prefill_enabled and self._num_prefills > 0 \\\n-                and context_lens.max() > 0:\n+                and max_context_len_cpu > 0:\n                 # NOTE: it is recommend you read the `Chunked Prefill` section\n                 # in the comment at the top of the file before trying to\n                 # understand the following code\n \n-                num_prefills_with_context = (context_lens > 0).sum().item()\n-\n                 # currently we allocate an equal amount of workspace for each\n                 # prefill in the batch, we could probably use a more advanced\n                 # algorithm here and allocate more workspace to prefills with\n                 # longer context lengths\n-                max_context_chunk = \\\n-                    self.chunked_prefill_workspace_size \\\n-                        // num_prefills_with_context\n+                max_context_chunk = (self.chunked_prefill_workspace_size //\n+                                     num_prefills_with_context_cpu)\n \n                 # align max_context_chunk to page_size by rounding down,\n                 # currently the `gather_cache` kernel cannot handle\n@@ -516,30 +520,35 @@ class MLACommonMetadataBuilder(Generic[M]):\n                                                self.page_size)\n \n                 assert max_context_chunk > 0\n-                num_chunks = cdiv(context_lens.max(), max_context_chunk)\n+                num_chunks = cdiv(max_context_len_cpu, max_context_chunk)\n \n                 # if `max_context_chunk = 256`, `num_chunks = 3`, and\n                 #   `num_prefills_with_context = 4`, create a tensor that looks\n                 # like\n                 #  [[0, 0, 0, 0], [256, 256, 256, 256], [512, 512, 512, 512]]\n+                # Note(simon): this is done in CPU because of downstream's\n+                # of `to_list`.\n                 chunk_starts = \\\n-                    torch.arange(num_chunks, device=device, dtype=torch.int32) \\\n+                    torch.arange(num_chunks, dtype=torch.int32) \\\n                     .unsqueeze(1).expand(-1, self._num_prefills) \\\n                     * max_context_chunk\n-                chunk_ends = torch.min(context_lens.unsqueeze(0),\n+                chunk_ends = torch.min(context_lens_cpu.unsqueeze(0),\n                                        chunk_starts + max_context_chunk)\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n-                _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n-                    torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+\n+                cu_seq_lens_cpu = torch.zeros(num_chunks,\n+                                              self._num_prefills + 1,\n+                                              dtype=torch.int32,\n+                                              pin_memory=True)\n+                torch.cumsum(chunk_seq_lens,\n+                             dim=1,\n+                             out=cu_seq_lens_cpu[:, 1:],\n+                             dtype=torch.int32)\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n-                    starts=chunk_starts,\n+                    cu_seq_lens=cu_seq_lens_cpu.to(device, non_blocking=True),\n+                    starts=chunk_starts.to(device, non_blocking=True),\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n                     workspace=self.chunked_prefill_workspace,\n@@ -553,7 +562,7 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 block_table=block_table[reqs_start:, ...],\n                 query_start_loc=query_start_loc[reqs_start:] -\n                 query_start_loc[reqs_start],\n-                max_query_len=seq_lens[reqs_start:].max().item(),\n+                max_query_len=max_query_len,\n                 chunked_context=chunked_context_metadata,\n             )\n \n@@ -629,7 +638,9 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         # already inside an attention custom op), pull out the forward\n         # method from the rotary embedding and call it directly\n         # TODO(lucas): we should probably find a cleaner way to do this\n-        self.rotary_emb = rotary_emb._forward_method\n+        self.rotary_emb = rotary_emb.forward_native\n+        if current_platform.is_cuda():\n+            self.rotary_emb = rotary_emb.forward_cuda\n \n         self.q_proj = q_proj\n         self.kv_b_proj = kv_b_proj\n@@ -1043,17 +1054,20 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n             decode_q_nope = self._q_proj_and_k_up_proj(decode_hs_or_q_c)\n             decode_q_pe = torch.matmul(decode_hs_or_q_c, self.W_QR)\\\n                 .view(-1, self.num_heads, self.qk_rope_head_dim)\n+\n             decode_q_pe[...], decode_k_pe[...] = self.rotary_emb(\n-                attn_metadata.decode.input_positions, decode_q_pe, decode_k_pe)\n+                attn_metadata.decode.input_positions, decode_q_pe.contiguous(),\n+                decode_k_pe)\n \n         if has_prefill:\n             assert attn_metadata.prefill is not None\n             prefill_q = self.q_proj(prefill_hs_or_q_c)[0]\\\n                 .view(-1, self.num_heads, self.qk_head_dim)\n             prefill_q_pe = prefill_q[..., self.qk_nope_head_dim:]\n+\n             prefill_q_pe[...], prefill_k_pe[...] = self.rotary_emb(\n-                attn_metadata.prefill.input_positions, prefill_q_pe,\n-                prefill_k_pe)\n+                attn_metadata.prefill.input_positions,\n+                prefill_q_pe.contiguous(), prefill_k_pe)\n \n         # write the latent and rope to kv cache\n         if kv_cache.numel() > 0:",
  "apis": [
    "MLACommonMetadataBuilder.build",
    "MLACommonImpl.__init__",
    "MLACommonImpl.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/common.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/mla/common.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/mla/common.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/mla/cutlass_mla.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/flashmla.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/ops/flashmla.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/mla/flashmla.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies MLA backend source code in a non-test file by reorganizing GPU/CPU memory operations and tensor movements to avoid unnecessary synchronization. The changes address performance by reducing potential blocking during memory transfers and optimizing control flow based on the current platform, switching between CUDA and native implementations. The modifications are non-trivial and affect key internal APIs used in top-level attention functionalities. The commit message ‚Äú[Perf] Improve MLA on V1‚Äù and the code changes suggest improvements to processing efficiency on CPU devices (and not just GPU/TPU specific hardware). Therefore, the commit meets the performance optimization criteria.",
  "llm_api_reason": "This commit improves the performance of the MLA backend in V1 by optimizing GPU‚ÄìCPU tensor transfers and ensuring that the rotary embedding is routed to the appropriate implementation based on the current platform. In the metadata builder, the build method is refined to minimize blocking transfers (by computing max_query_len on the CPU and avoiding unnecessary device transfers) and to correctly set up chunked context metadata. In the MLACommonImpl class, the rotary embedding method is updated to use forward_native by default and forward_cuda when running on CUDA, ensuring a more efficient execution path."
}