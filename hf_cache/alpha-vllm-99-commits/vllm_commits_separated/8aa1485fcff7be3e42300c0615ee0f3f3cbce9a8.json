{
  "commit_hash": "8aa1485fcff7be3e42300c0615ee0f3f3cbce9a8",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21761",
  "pr_date": "2025-07-28",
  "timeline_text": "Copy link Collaborator LucasWilkinson commented Jul 28, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results (Optional) The necessary documentation update, such as updating supported_models.md and examples for a new model. Purpose Currently using the hybrid kv-cache with llama4s chunked local attention causes a latency ~2ms since when the hybrid kv-cache manager is used we end up with 3 ChunkedLocalAttention kv-cache spec groups. We end up with the following groups: (FullAttention x 12) (ChunkedLocalAttention x 12) (ChunkedLocalAttention x 12) (ChunkedLocalAttention x 12) This results in attn metadata and local virtual batches for the local layers being constructed 3 times adding latency: Enabled: \n\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 4 --trust-remote-code --max-model-len 16384 --port 8081  --disable-log-requests\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  9.11      \nTotal input tokens:                      6299      \nTotal generated tokens:                  12509     \nRequest throughput (req/s):              10.97     \nOutput token throughput (tok/s):         1372.85   \nTotal Token throughput (tok/s):          2064.16   \n---------------Time to First Token----------------\nMean TTFT (ms):                          61.84     \nMedian TTFT (ms):                        61.53     \nP99 TTFT (ms):                           106.66    \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          28.46     \nMedian TPOT (ms):                        29.17     \nP99 TPOT (ms):                           30.99     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           28.44     \nMedian ITL (ms):                         28.65     \nP99 ITL (ms):                            38.05     \n==================================================\n\n\nDisabled:\n\nvllm serve meta-llama/Llama-4-Scout-17B-16E-Instruct -tp 4 --trust-remote-code --max-model-len 16384 --port 8081  --disable-log-requests --disable-hybrid-kv-cache-manager\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  8.84      \nTotal input tokens:                      6299      \nTotal generated tokens:                  12297     \nRequest throughput (req/s):              11.32     \nOutput token throughput (tok/s):         1391.49   \nTotal Token throughput (tok/s):          2104.26   \n---------------Time to First Token----------------\nMean TTFT (ms):                          58.69     \nMedian TTFT (ms):                        59.23     \nP99 TTFT (ms):                           90.65     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          26.48     \nMedian TPOT (ms):                        27.32     \nP99 TPOT (ms):                           28.90     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.55     \nMedian ITL (ms):                         26.54     \nP99 ITL (ms):                            39.40     \n================================================== Test Plan see: #21707 Test Result see: #21707 (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions disable chunked local attention by default â€¦ dd3ccf5 Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> LucasWilkinson requested review from simon-mo , WoosukKwon , youkaichao , robertgshaw2-redhat , mgoin , tlrmchlsmth , houseroad and hmellor as code owners July 28, 2025 14:27 Copy link github-actions bot commented Jul 28, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the llama Related to Llama models label Jul 28, 2025 gemini-code-assist bot reviewed Jul 28, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request correctly addresses a performance regression by disabling chunked local attention with the hybrid KV cache manager by default, while providing an environment variable to re-enable it. The implementation is sound. My only suggestion is to update a comment to more accurately reflect that the change is a performance optimization, which will improve code clarity and maintainability. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/config.py Comment on lines +4780 to +4781 # Hybrid KV cache manager is not yet supported with chunked # local attention. Copy link Contributor gemini-code-assist bot Jul 28, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This comment is slightly misleading as it suggests the feature is unsupported, whereas the PR description and warning log indicate it's a performance regression. To improve clarity for future maintenance, it would be better to state the performance-related reason for disabling it. Suggested change # Hybrid KV cache manager is not yet supported with chunked # local attention . # Disable hybrid KV cache manager with chunked local attention # due to a performance regression . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member mgoin Jul 28, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I kind of agree with Gemini here, although you say this in your log Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin approved these changes Jul 28, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM for the moment Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/config.py Comment on lines +4780 to +4781 # Hybrid KV cache manager is not yet supported with chunked # local attention. Copy link Member mgoin Jul 28, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I kind of agree with Gemini here, although you say this in your log Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/config.py self.scheduler_config.disable_hybrid_kv_cache_manager = True elif \\ not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE: logger.warning( Copy link Member mgoin Jul 28, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: warning_once Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added performance Performance-related issues ready ONLY add when PR is ready to merge/full CI is needed labels Jul 28, 2025 Copy link Member mgoin commented Jul 28, 2025 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Merging to solve the regression since we have better solutions on the way All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details mgoin merged commit 8aa1485 into vllm-project : main Jul 28, 2025 78 checks passed Uh oh! There was an error while loading. Please reload this page . liuyumoye pushed a commit\n        to liuyumoye/vllm\n      that referenced\n      this pull request Jul 31, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 47a6c89 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> sarckk mentioned this pull request Aug 2, 2025 [Bug]: [v1/core/block_pool.py] Assertion Failure: prev_block.block_hash is not None #21992 Open Copy link Collaborator luccafong commented Aug 2, 2025 @LucasWilkinson will we reduce metadata creation with refactoring? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author LucasWilkinson commented Aug 2, 2025 thats the plan; we are working towards: https://vllm-dev.slack.com/archives/C07R5Q1Q2BB/p1753727605258469?thread_ts=1753202489.248869&cid=C07R5Q1Q2BB but that will be a followup PR All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ e636a83 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: shuw <shuw@nvidia.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ b15f7a3 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: x22x22 <wadeking@qq.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 024f5de â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ be60f7a â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 94a185c â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ bda1d57 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> noamgat pushed a commit\n        to noamgat/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ b0119fd â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Noam Gat <noamgat@gmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 3712e58 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 63e3c03 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> BoyuanFeng pushed a commit\n        to BoyuanFeng/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 46cb6ce â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Boyuan Feng <boyuan@meta.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ afd3f01 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 1e8cef7 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> zhewenl pushed a commit\n        to zhewenl/vllm\n      that referenced\n      this pull request Aug 28, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ 47bfbc4 â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Perf] Disable chunked local attention by default with llama4 ( vllm-pâ€¦ â€¦ c1a10df â€¦roject#21761 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:55",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, vllm serve, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:49:55",
  "models": [
    "meta-llama/Llama-4-Scout-17B-16E-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-4-Scout-17B-16E-Instruct,trust_remote_code=True,max_model_len=16384 --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-4-Scout-17B-16E-Instruct --trust-remote-code --max-model-len 16384",
  "commit_subject": "[Perf] Disable chunked local attention by default with llama4 (#21761)",
  "commit_message": "[Perf] Disable chunked local attention by default with llama4 (#21761)\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>",
  "commit_date": "2025-07-28T18:49:04-04:00",
  "files_changed": [
    "vllm/config.py",
    "vllm/envs.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 3,
    "num_edited_lines": 35,
    "num_non_test_edited_lines": 35,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b76..3bcbbe606 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -4769,12 +4769,23 @@ class VllmConfig:\n                 # Hybrid KV cache manager is not compatible with KV events.\n                 self.scheduler_config.disable_hybrid_kv_cache_manager = True\n             if self.model_config is not None and \\\n-                self.model_config.attention_chunk_size is not None and \\\n-                self.speculative_config is not None and \\\n-                self.speculative_config.use_eagle():\n-                # Hybrid KV cache manager is not yet supported with chunked\n-                # local attention + eagle.\n-                self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                self.model_config.attention_chunk_size is not None:\n+                if self.speculative_config is not None and \\\n+                    self.speculative_config.use_eagle():\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention + eagle.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n+                elif \\\n+                    not envs.VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE:\n+                    logger.warning(\n+                        \"There is a latency regression when using chunked local\"\n+                        \" attention with the hybrid KV cache manager. Disabling\"\n+                        \" it, by default. To enable it, set the environment \"\n+                        \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE=1.\"\n+                    )\n+                    # Hybrid KV cache manager is not yet supported with chunked\n+                    # local attention.\n+                    self.scheduler_config.disable_hybrid_kv_cache_manager = True\n \n     def update_sizes_for_sequence_parallelism(self,\n                                               possible_sizes: list) -> list:\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff74151..fcfad4eec 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -143,6 +143,7 @@ if TYPE_CHECKING:\n     VLLM_USE_CUDNN_PREFILL: bool = False\n     VLLM_ENABLE_CUDAGRAPH_GC: bool = False\n     VLLM_LOOPBACK_IP: str = \"\"\n+    VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE: bool = False\n \n \n def get_default_cache_root():\n@@ -991,6 +992,17 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     # The default value is \"VLLM\".\n     \"VLLM_PROCESS_NAME_PREFIX\":\n     lambda: os.getenv(\"VLLM_PROCESS_NAME_PREFIX\", \"VLLM\"),\n+\n+    # Allow chunked local attention with hybrid kv cache manager.\n+    # Currently using the Hybrid KV cache manager with chunked local attention\n+    # in the Llama4 models (the only models currently using chunked local attn)\n+    # causes a latency regression. For this reason, we disable it by default.\n+    # This flag is used to allow users to enable it if they want to (to save on\n+    # kv-cache memory usage and enable longer contexts)\n+    # TODO(lucas): Remove this flag once latency regression is resolved.\n+    \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\":\n+    lambda: bool(int(os.getenv(\\\n+            \"VLLM_ALLOW_CHUNKED_LOCAL_ATTN_WITH_HYBRID_KV_CACHE\", \"0\"))),\n }\n \n # --8<-- [end:env-vars-definition]",
  "apis": [
    "vllm.core.scheduler.Scheduler"
  ],
  "affected_paths": [],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test, production code (vllm/config.py and vllm/envs.py) and contains non-trivial changes to disable the hybrid KV cache manager when using chunked local attention, in order to mitigate a latency regression. This flag change is directly connected to performance issues (latency regression) in specific model setups (e.g., llama4) while still being testable on CPU and not tied to specific hardware. The focus is on performance optimization and not on a bug fix or a refactoring of comments or documentation.",
  "llm_api_reason": "This commit modifies how the configuration is set in VllmConfig regarding the hybrid KV cache manager when using chunked local attention. The changes ensure that the scheduler configurationâ€™s flag disable_hybrid_kv_cache_manager is always set to True when chunked local attention is enabled, unless an environment override is provided. Since this flag is read by the scheduler during batching and scheduling, the behavior of vllm.core.scheduler.Scheduler is affected by these changes."
}