{
  "commit_hash": "310aca88c984983189a57f1b72e3b1dde89fb92f",
  "pr_url": "https://github.com/vllm-project/vllm/pull/11870",
  "pr_date": "2025-01-09",
  "timeline_text": "Copy link Member youkaichao commented Jan 9, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . fix the performance regression reported from #11744 (comment) on my local benchmark: python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4 main branch: Avg latency: 2.945735554069203 seconds\n10% percentile latency: 2.924619035271462 seconds\n25% percentile latency: 2.937671729727299 seconds\n50% percentile latency: 2.9460502695292234 seconds\n75% percentile latency: 2.955668824230088 seconds\n90% percentile latency: 2.9639973257959356 seconds\n99% percentile latency: 2.979829666109872 seconds this PR: Avg latency: 2.851606635436959 seconds\n10% percentile latency: 2.8231707043829375 seconds\n25% percentile latency: 2.834942308269092 seconds\n50% percentile latency: 2.85484445450129 seconds\n75% percentile latency: 2.8674310567148495 seconds\n90% percentile latency: 2.872856835933635 seconds\n99% percentile latency: 2.875793117735884 seconds it can have 3% perf diff. Hopefully this can fix the perf regression observed in the benchmark: Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions youkaichao added 2 commits January 9, 2025 09:42 fix stream ‚Ä¶ f5b7d78 Signed-off-by: youkaichao <youkaichao@gmail.com> fix code ‚Ä¶ e16f595 Signed-off-by: youkaichao <youkaichao@gmail.com> youkaichao requested a review\n  from tlrmchlsmth January 9, 2025 02:00 Copy link github-actions bot commented Jan 9, 2025 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author youkaichao commented Jan 9, 2025 I find measuring the pure forward time makes more sense, it will not be affected by the scheduling, etc: VLLM_LOG_BATCHSIZE_INTERVAL=1 python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4 main branch: Batchsize forward time stats (batchsize, count, median_time(ms)): [(8, 4998, 20.77), (256, 40, 28.99)] this PR: Batchsize forward time stats (batchsize, count, median_time(ms)): [(8, 5027, 20.45), (256, 40, 28.95)] The forward time for every step (batchsize 8) reduces from 20.77ms to 20.45ms. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth reviewed Jan 9, 2025 View reviewed changes vllm/utils.py Comment on lines +959 to +970 prev_set_stream = torch.cuda.set_stream _current_stream = None def _patched_set_stream(stream: torch.cuda.Stream) -> None: global _current_stream _current_stream = stream prev_set_stream(stream) torch.cuda.set_stream = _patched_set_stream Copy link Collaborator tlrmchlsmth Jan 9, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It looks like we're not using set_stream anywhere in the vllm codebase. Could you add a unit test for this to make sure it's exercised? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator tlrmchlsmth Jan 9, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment here we patch torch.cuda.set_stream to keep track of the current stream directly, so that we can avoid calling torch.cuda.current_stream() . I might be confused about how utils.current_stream() works though Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author youkaichao Jan 9, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment torch.cuda.graph will call it internally to switch streams. so any test cases with cudagraph + nccl will test the PR's code. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 tlrmchlsmth reacted with thumbs up emoji All reactions üëç 1 reaction tlrmchlsmth approved these changes Jan 9, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the fix! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions youkaichao enabled auto-merge (squash) January 9, 2025 03:40 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jan 9, 2025 Hide details View details youkaichao merged commit 310aca8 into vllm-project : main Jan 9, 2025 71 of 73 checks passed Uh oh! There was an error while loading. Please reload this page . youkaichao deleted the fix_current_stream branch January 9, 2025 07:37 gshtras added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Jan 14, 2025 Merge pull request #358 from ROCm/upstream_merge_25_01_13 ‚Ä¶ 5976f48 * [Bugfix][V1] Fix molmo text-only inputs ( vllm-project#11676 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Kernel] Move attn_type to Attention.__init__() ( vllm-project#11690 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [V1] Extend beyond image modality and support mixed-modality inference with Llava-OneVision ( vllm-project#11685 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix LLaVA-NeXT feature size precision error (for real) ( vllm-project#11772 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Future-proof Qwen2-Audio multi-modal processor ( vllm-project#11776 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [XPU] Make pp group initilized for pipeline-parallelism ( vllm-project#11648 )\n\nSigned-off-by: yisheng <yi.sheng@intel.com>\n\n* [Doc][3/N] Reorganize Serving section ( vllm-project#11766 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Kernel][LoRA]Punica prefill  kernels fusion ( vllm-project#11234 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\n\n* [Bugfix] Update attention interface in `Whisper` ( vllm-project#11784 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [CI] Fix neuron CI and run offline tests ( vllm-project#11779 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* fix init error for MessageQueue when n_local_reader is zero ( vllm-project#11768 )\n\n* [Doc] Create a vulnerability management team ( vllm-project#9925 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [CI][CPU] adding build number to docker image name ( vllm-project#11788 )\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\n\n* [V1][Doc] Update V1 support for `LLaVa-NeXT-Video` ( vllm-project#11798 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix] Comprehensively test and fix LLaVA-NeXT feature size calculation ( vllm-project#11800 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [doc] add doc to explain how to use uv ( vllm-project#11773 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] Support audio language models on V1 ( vllm-project#11733 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [doc] update how pip can install nightly wheels ( vllm-project#11806 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Add note to `gte-Qwen2` models ( vllm-project#11808 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [optimization] remove python function call for custom op ( vllm-project#11750 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] update the prefix for qwen2 ( vllm-project#11795 )\n\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\n\n* [Doc]Add documentation for using EAGLE in vLLM ( vllm-project#11417 )\n\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\n\n* [Bugfix] Significant performance drop on CPUs with --num-scheduler-steps > 1 ( vllm-project#11794 )\n\n* [Doc] Group examples into categories ( vllm-project#11782 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Bugfix] Fix image input for Pixtral-HF ( vllm-project#11741 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] sort torch profiler table by kernel timing ( vllm-project#11813 )\n\n* Remove the duplicate imports of MultiModalKwargs and PlaceholderRange‚Ä¶ ( vllm-project#11824 )\n\n* Fixed docker build for ppc64le ( vllm-project#11518 )\n\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\n\n* [OpenVINO] Fixed Docker.openvino build ( vllm-project#11732 )\n\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\n\n* [Bugfix] Add checks for LoRA and CPU offload ( vllm-project#11810 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Docs] reorganize sponsorship page ( vllm-project#11639 )\n\nSigned-off-by: simon-mo <simon.mo@hey.com>\n\n* [Bug] Fix pickling of `ModelConfig` when RunAI Model Streamer is used ( vllm-project#11825 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] improve memory profiling ( vllm-project#11809 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [doc] update wheels url ( vllm-project#11830 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docs] Update sponsor name: 'Novita' to 'Novita AI' ( vllm-project#11833 )\n\n* [Hardware][Apple] Native support for macOS Apple Silicon ( vllm-project#11696 )\n\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [torch.compile] consider relevant code in compilation cache ( vllm-project#11614 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Reorganize profiling/processing-related code ( vllm-project#11812 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Move examples into categories ( vllm-project#11840 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Doc][4/N] Reorganize API Reference ( vllm-project#11843 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build][Bugfix] Fix CPU CI image clean up ( vllm-project#11836 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [Bugfix][XPU] fix silu_and_mul ( vllm-project#11823 )\n\nSigned-off-by: yan ma <yan.ma@intel.com>\n\n* [Misc] Move some model utils into vision file ( vllm-project#11848 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Expand Multimodal API Reference ( vllm-project#11852 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc]add some explanations for BlockHashType ( vllm-project#11847 )\n\n* [TPU][Quantization] TPU `W8A8` ( vllm-project#11785 )\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models ( vllm-project#11698 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* [Docs] Add Google Cloud Meetup ( vllm-project#11864 )\n\n* [CI] Turn on basic correctness tests for V1 ( vllm-project#10864 )\n\n* treat do_lower_case in the same way as the sentence-transformers library ( vllm-project#11815 )\n\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\n\n* [Doc] Recommend uv and python 3.12 for quickstart guide ( vllm-project#11849 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Misc] Move `print_*_once` from utils to logger ( vllm-project#11298 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\n\n* [Doc] Intended links Python multiprocessing library ( vllm-project#11878 )\n\n* [perf]fix current stream ( vllm-project#11870 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Override dunder methods of placeholder modules ( vllm-project#11882 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] fix beam search input errors and latency benchmark script ( vllm-project#11875 )\n\nSigned-off-by: Ye Qi <yeq@meta.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\n\n* [Doc] Add model development API Reference ( vllm-project#11884 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [platform] Allow platform specify attention backend ( vllm-project#11609 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\n\n* [ci]try to fix flaky multi-step tests ( vllm-project#11894 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Provide correct Pixtral-HF chat template ( vllm-project#11891 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Docs] Add Modal to deployment frameworks ( vllm-project#11907 )\n\n* [Doc][5/N] Move Community and API Reference to the bottom ( vllm-project#11896 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\n\n* [VLM] Enable tokenized inputs for merged multi-modal processor ( vllm-project#11900 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Show default pooling method in a table ( vllm-project#11904 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [torch.compile] Hide KV cache behind torch.compile boundary ( vllm-project#11677 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Bugfix] Validate lora adapters to avoid crashing server ( vllm-project#11727 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [BUGFIX] Fix `UnspecifiedPlatform` package name ( vllm-project#11916 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [ci] fix gh200 tests ( vllm-project#11919 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [misc] remove python function call for custom activation op ( vllm-project#11885 )\n\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [platform] support pytorch custom op pluggable ( vllm-project#11328 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* Replace \"online inference\" with \"online serving\" ( vllm-project#11923 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [ci] Fix sampler tests ( vllm-project#11922 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] [1/N] Initial guide for merged multi-modal processor ( vllm-project#11925 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [platform] support custom torch.compile backend key ( vllm-project#11318 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Rename offline inference examples ( vllm-project#11927 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Docs] Fix docstring in `get_ip` function ( vllm-project#11932 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* Doc fix in `benchmark_long_document_qa_throughput.py` ( vllm-project#11933 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* [Hardware][CPU] Support MOE models on x86 CPU ( vllm-project#11831 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [Misc] Clean up debug code in Deepseek-V3 ( vllm-project#11930 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Misc] Update benchmark_prefix_caching.py fixed example usage ( vllm-project#11920 )\n\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\n\n* [Bugfix] Check that number of images matches number of <|image|> tokens with mllama ( vllm-project#11939 )\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* [mypy] Fix mypy warnings in api_server.py ( vllm-project#11941 )\n\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\n\n* [ci] fix broken distributed-tests-4-gpus ( vllm-project#11937 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix][SpecDecode] Adjust Eagle model architecture to align with intended design ( vllm-project#11672 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Bugfix] fused_experts_impl wrong compute type for float32 ( vllm-project#11921 )\n\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\n\n* [CI/Build] Move model-specific multi-modal processing tests ( vllm-project#11934 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Basic guide for writing unit tests for new models ( vllm-project#11951 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix RobertaModel loading ( vllm-project#11940 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [Model] Add cogagent model support vLLM ( vllm-project#11742 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [V1] Avoid sending text prompt to core engine ( vllm-project#11963 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [CI/Build] Add markdown linter ( vllm-project#11857 )\n\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\n\n* [Model] Initialize support for Deepseek-VL2 models ( vllm-project#11578 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Hardware][CPU] Multi-LoRA implementation for the CPU backend ( vllm-project#11100 )\n\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][TPU] workaround fix for MoE on TPU ( vllm-project#11764 )\n\n* [V1][Core][1/n] Logging and Metrics ( vllm-project#11962 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [Model] Support GGUF models newly added in `transformers` 4.46.0 ( vllm-project#9685 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction ( vllm-project#11973 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [MISC] fix typo in kv transfer send recv test ( vllm-project#11983 )\n\n* [Bug] Fix usage of `.transpose()` and `.view()` consecutively. ( vllm-project#11979 )\n\n* [CI][Spec Decode] fix: broken test for EAGLE model ( vllm-project#11972 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Misc] Fix Deepseek V2 fp8 kv-scale remapping ( vllm-project#11947 )\n\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\n\n* [Misc]Minor Changes about Worker ( vllm-project#11555 )\n\nSigned-off-by: Chenguang Li <757486878@qq.com>\n\n* [platform] add ray_device_key ( vllm-project#11948 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Fix Max Token ID for Qwen-VL-Chat ( vllm-project#11980 )\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* [Kernel] unified_attention for Attention.forward ( vllm-project#11967 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Doc][V1] Update model implementation guide for V1 support ( vllm-project#11998 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [Doc] Organise installation documentation into categories and tabs ( vllm-project#11935 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [platform] add device_control env var ( vllm-project#12009 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Platform] Move get_punica_wrapper() function to Platform ( vllm-project#11516 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function ( vllm-project#11982 )\n\nSigned-off-by: elijah <f1renze.142857@gmail.com>\n\n* Using list\n\n* Revert \"[misc] improve memory profiling ( vllm-project#11809 )\"\n\nThis reverts commit 889e662 .\n\n* Trying to make scales work with compileable attention\n\n* Docs lint\n\n---------\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: yisheng <yi.sheng@intel.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nSigned-off-by: Ye Qi <yeq@meta.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\nSigned-off-by: Chenguang Li <757486878@qq.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nSigned-off-by: Shanshan Shen <467638484@qq.com>\nSigned-off-by: elijah <f1renze.142857@gmail.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: YiSheng5 <yi.sheng@intel.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan <yuan.zhou@intel.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: jiangjiadi <34134495+jiangjiadi@users.noreply.github.com>\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: WangErXiao <863579016@qq.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Wallas Henrique <wallashss@users.noreply.github.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Guspan Tanadi <36249910+guspan-tanadi@users.noreply.github.com>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Charles Frye <cfrye59@gmail.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: cennn <61925104+cennn@users.noreply.github.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: minmin <rmm0811@gmail.com>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Fred Reiss <frreiss@us.ibm.com>\nCo-authored-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nCo-authored-by: shaochangxu <85155497+shaochangxu@users.noreply.github.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: sixgod <evethwillbeok@outlook.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Akshat Tripathi <Akshat.tripathi6568@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: Yangcheng Li <liyangcheng.lyc@alibaba-inc.com>\nCo-authored-by: Siyuan Li <94890248+liaoyanqing666@users.noreply.github.com>\nCo-authored-by: Concurrensee <yida.wu@amd.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: elijah <30852919+e1ijah1@users.noreply.github.com> hongxiayang pushed a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Jan 15, 2025 [MFM-20250115] Merge from ROCm/main to llama_fp8 ( #360 ) ‚Ä¶ d9385b4 * [Misc] Move weights mapper ( vllm-project#11443 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Bugfix] Fix issues in CPU build Dockerfile. Fixes vllm-project#9182 ( vllm-project#11435 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Model] Automatic conversion of classification and reward models ( vllm-project#11469 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Unify VLLM_ENABLE_V1_MULTIPROCESSING handling in RayExecutor ( vllm-project#11472 )\n\n* [Misc] Update disaggregation benchmark scripts and test logs ( vllm-project#11456 )\n\nSigned-off-by: Jiaxin Shan <seedjeffwan@gmail.com>\n\n* [Frontend] Enable decord to load video from base64 ( vllm-project#11492 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Improve GitHub links ( vllm-project#11491 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] Move some multimodal utils to modality-specific modules ( vllm-project#11494 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Mypy checking for vllm/compilation ( vllm-project#11496 )\n\nSigned-off-by: lucast2021 <lucast2021@headroyce.org>\nCo-authored-by: lucast2021 <lucast2021@headroyce.org>\n\n* [Misc][LoRA] Fix LoRA weight mapper ( vllm-project#11495 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Doc] Add `QVQ` and `QwQ` to the list of supported models ( vllm-project#11509 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [V1] Adding min tokens/repetition/presence/frequence penalties to V1 sampler ( vllm-project#10681 )\n\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Model]  Modify MolmoForCausalLM MLP  ( vllm-project#11510 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Add placeholder module ( vllm-project#11501 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Add video example to openai client for multimodal ( vllm-project#11521 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [1/N] API Server  (Remove Proxy) ( vllm-project#11529 )\n\n* [Model] [Quantization] Support deepseek_v3 w8a8 fp8 block-wise quantization ( vllm-project#11523 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: HandH1998 <1335248067@qq.com>\n\n* [2/N] API Server: Avoid ulimit footgun ( vllm-project#11530 )\n\n* Deepseek v3 ( vllm-project#11502 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: robertgshaw2-neuralmagic <rshaw@neuralmagic.com>\n\n* [Docs] Document Deepseek V3 support ( vllm-project#11535 )\n\nSigned-off-by: simon-mo <simon.mo@hey.com>\n\n* Update openai_compatible_server.md ( vllm-project#11536 )\n\nCo-authored-by: Simon Mo <simon.mo@hey.com>\n\n* [V1] Use FlashInfer Sampling Kernel for Top-P & Top-K Sampling ( vllm-project#11394 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1] Fix yapf ( vllm-project#11538 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [CI] Fix broken CI ( vllm-project#11543 )\n\n* [misc] fix typing ( vllm-project#11540 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1][3/N] API Server: Reduce Task Switching + Handle Abort Properly ( vllm-project#11534 )\n\n* [BugFix] Fix quantization for all other methods ( vllm-project#11547 )\n\n* [Platform] Move model arch check to platform ( vllm-project#11503 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* Update deploying_with_k8s.md with AMD ROCm GPU example ( vllm-project#11465 )\n\nSigned-off-by: Alex He <alehe@amd.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Bugfix] Fix TeleChat2ForCausalLM weights mapper ( vllm-project#11546 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Abstract the logic for reading and writing media content ( vllm-project#11527 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc]  Add xgrammar in doc ( vllm-project#11549 )\n\nSigned-off-by: ccjincong <chenjincong11@gmail.com>\n\n* [VLM] Support caching in merged multi-modal processor ( vllm-project#11396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [MODEL] LoRA support for Jamba model ( vllm-project#11209 )\n\nSigned-off-by: Erez Schwartz <erezs@ai21.com>\n\n* [Misc]Add BNB quantization for MolmoForCausalLM  ( vllm-project#11551 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Improve BNB loader to handle mixture of sharded and merged weights with same suffix ( vllm-project#11566 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Bugfix] Fix for ROCM compressed tensor support ( vllm-project#11561 )\n\n* [Doc] Update mllama example based on official doc ( vllm-project#11567 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [V1] [4/N] API Server: ZMQ/MP Utilities ( vllm-project#11541 )\n\n* [Bugfix] Last token measurement fix ( vllm-project#11376 )\n\nSigned-off-by: rajveerb <46040700+rajveerb@users.noreply.github.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\n\n* [Model] Support InternLM2 Reward models ( vllm-project#11571 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Model] Remove hardcoded image tokens ids from Pixtral ( vllm-project#11582 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Hardware][AMD]: Replace HIPCC version with more precise ROCm version ( vllm-project#11515 )\n\nSigned-off-by: hjwei <hjwei_xd@163.com>\n\n* [V1][Minor] Set pin_memory=False for token_ids_cpu tensor ( vllm-project#11581 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Doc] Minor documentation fixes ( vllm-project#11580 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [bugfix] interleaving sliding window for cohere2 model ( vllm-project#11583 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] [5/N] API Server: unify `Detokenizer` and  `EngineCore` input ( vllm-project#11545 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [Doc] Convert list tables to MyST ( vllm-project#11594 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [v1][bugfix] fix cudagraph with inplace buffer assignment ( vllm-project#11596 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] KV cache transfer connector registry ( vllm-project#11481 )\n\nSigned-off-by: KuntaiDu <kuntai@uchicago.edu>\n\n* Remove print statement in DeepseekScalingRotaryEmbedding ( vllm-project#11604 )\n\n* [v1] fix compilation cache ( vllm-project#11598 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docker] bump up neuron sdk v2.21 ( vllm-project#11593 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* [Build][Kernel] Update CUTLASS to v3.6.0 ( vllm-project#11607 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [CI/Build][CPU] Fix CPU CI by lazy importing triton FP8 kernels ( vllm-project#11618 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [platforms] enable platform plugins ( vllm-project#11602 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Abstract out multi-modal data parsing in merged processor ( vllm-project#11620 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] [6/N] API Server: Better Shutdown ( vllm-project#11586 )\n\n* [Bugfix] Validate and concatenate image embeddings in MiniCPMVBaseModel ( vllm-project#11631 )\n\n* [benchmark] Remove dependency for H100 benchmark step ( vllm-project#11572 )\n\n* [Model][LoRA]LoRA support added for MolmoForCausalLM ( vllm-project#11439 )\n\nSigned-off-by: Matthias Vogler <matthias.vogler@joesecurity.org>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Matthias Vogler <matthias.vogler@joesecurity.org>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Bugfix] Fix OpenAI parallel sampling when using xgrammar ( vllm-project#11637 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Misc][LoRA] Support Rank Stabilized LoRA (RSLoRA) ( vllm-project#6909 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Bugfix] Move the _touch(computed_blocks) call in the allocate_slots method to after the check for allocating new blocks. ( vllm-project#11565 )\n\n* [V1] Simpify vision block hash for prefix caching by removing offset from hash ( vllm-project#11646 )\n\n* [V1][VLM] V1 support for selected single-image models. ( vllm-project#11632 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Benchmark] Add benchmark script for CPU offloading  ( vllm-project#11533 )\n\nSigned-off-by: ApostaC <yihua98@uchicago.edu>\nCo-authored-by: KuntaiDu <kuntai@uchicago.edu>\n\n* [Bugfix][Refactor] Unify model management in frontend ( vllm-project#11660 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\n\n* [VLM] Add max-count checking in data parser for single image models ( vllm-project#11661 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Misc] Optimize Qwen2-VL LoRA test ( vllm-project#11663 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Replace space with - in the file names ( vllm-project#11667 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [Doc] Fix typo ( vllm-project#11666 )\n\nSigned-off-by: Kazuhiro Serizawa <nserihiro@gmail.com>\n\n* [V1] Implement Cascade Attention ( vllm-project#11635 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [VLM] Move supported limits and max tokens to merged multi-modal processor ( vllm-project#11669 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [VLM][Bugfix] Multi-modal processor compatible with V1 multi-input ( vllm-project#11674 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [mypy] Pass type checking in vllm/inputs ( vllm-project#11680 )\n\nSigned-off-by: Tobias Pitters <tobias.pitters@gmail.com>\n\n* [VLM] Merged multi-modal processor for LLaVA-NeXT ( vllm-project#11682 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* According to vllm.EngineArgs, the name should be distributed_executor_backend ( vllm-project#11689 )\n\n* [Bugfix] Free cross attention block table for preempted-for-recompute sequence group. ( vllm-project#10013 )\n\nSigned-off-by: Kathy Yu <feiyangyu@google.com>\n\n* [V1][Minor] Optimize token_ids_cpu copy ( vllm-project#11692 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Change kv scaling factor by param json on nvidia gpu ( vllm-project#11688 )\n\nSigned-off-by: bjmsong <bjmsong@126.com>\nCo-authored-by: bjmsong <bjmsong@126.com>\n\n* Resolve race conditions in Marlin kernel ( vllm-project#11493 )\n\nSigned-off-by: wchen61 <wchen61@foxmail.com>\n\n* [Misc] Minimum requirements for SageMaker compatibility ( vllm-project#11576 )\n\n* Update default max_num_batch_tokens for chunked prefill ( vllm-project#11694 )\n\n* [Bugfix] Check chain_speculative_sampling before calling it ( vllm-project#11673 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [perf-benchmark] Fix dependency for steps in benchmark pipeline ( vllm-project#11710 )\n\n* [Model] Whisper model implementation ( vllm-project#11280 )\n\nCo-authored-by: Aurick Qiao <aurick.qiao@snowflake.com>\n\n* [V1] Simplify Shutdown ( vllm-project#11659 )\n\n* [Bugfix] Fix ColumnParallelLinearWithLoRA slice ( vllm-project#11708 )\n\nSigned-off-by: ZincCat <zincchloride@outlook.com>\n\n* [V1] Improve TP>1 Error Handling + Stack Trace ( vllm-project#11721 )\n\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Misc]Add BNB quantization for Qwen2VL ( vllm-project#11719 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* Update requirements-tpu.txt to support python 3.9 and 3.11 ( vllm-project#11695 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [V1] Chore: cruft removal ( vllm-project#11724 )\n\n* [V1] log GPU blocks num for MultiprocExecutor ( vllm-project#11656 )\n\n* Update tool_calling.md ( vllm-project#11701 )\n\n* Update bnb.md with example for OpenAI ( vllm-project#11718 )\n\n* [V1] Add `RayExecutor` support for `AsyncLLM` (api server) ( vllm-project#11712 )\n\n* [V1] Add kv cache utils tests. ( vllm-project#11513 )\n\nSigned-off-by: xcnick <xcnick0412@gmail.com>\n\n* [Core][Bugfix] Use correct device to initialize GPU data during CUDA-graph-capture ( vllm-project#11233 )\n\nSigned-off-by: Yan Burman <yanburman@users.noreply.github.com>\nSigned-off-by: Ido Asraff <idoa@atero.ai>\n\n* [VLM] Merged multi-modal processors for LLaVA-NeXT-Video and LLaVA-OneVision ( vllm-project#11717 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix precision error in LLaVA-NeXT ( vllm-project#11735 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Remove unnecessary weight initialization logic ( vllm-project#11736 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Bugfix][V1] Fix test_kv_cache_utils.py ( vllm-project#11738 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [MISC] Replace c10::optional with std::optional ( vllm-project#11730 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [distributed] remove pynccl's redundant stream ( vllm-project#11744 )\n\n* fix: [doc] fix typo ( vllm-project#11751 )\n\nCo-authored-by: Lancer <maruixiang6688@gmail.com>\n\n* [Frontend] Improve `StreamingResponse` Exception Handling ( vllm-project#11752 )\n\n* [distributed] remove pynccl's redundant change_state ( vllm-project#11749 )\n\n* [Doc] [1/N] Reorganize Getting Started section ( vllm-project#11645 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Remove block size constraint ( vllm-project#11723 )\n\n* [V1] Add BlockTable class ( vllm-project#11693 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Misc] Fix typo for valid_tool_parses  ( vllm-project#11753 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [V1] Refactor get_executor_cls ( vllm-project#11754 )\n\n* [mypy] Forward pass function type hints in lora ( vllm-project#11740 )\n\nSigned-off-by: lucast2021 <lucast2021@headroyce.org>\nCo-authored-by: lucast2021 <lucast2021@headroyce.org>\n\n* k8s-config: Update the secret to use stringData ( vllm-project#11679 )\n\nSigned-off-by: Suraj Deshmukh <surajd.service@gmail.com>\n\n* [VLM] Separate out profiling-related logic ( vllm-project#11746 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc][2/N] Reorganize Models and Usage sections ( vllm-project#11755 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix max image size for LLaVA-Onevision ( vllm-project#11769 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [doc] explain how to add interleaving sliding window support ( vllm-project#11771 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix][V1] Fix molmo text-only inputs ( vllm-project#11676 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Kernel] Move attn_type to Attention.__init__() ( vllm-project#11690 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* format\n\n* [V1] Extend beyond image modality and support mixed-modality inference with Llava-OneVision ( vllm-project#11685 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* deepseek overflow fix ( #349 )\n\n* [Bugfix] Fix LLaVA-NeXT feature size precision error (for real) ( vllm-project#11772 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Future-proof Qwen2-Audio multi-modal processor ( vllm-project#11776 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [XPU] Make pp group initilized for pipeline-parallelism ( vllm-project#11648 )\n\nSigned-off-by: yisheng <yi.sheng@intel.com>\n\n* [Doc][3/N] Reorganize Serving section ( vllm-project#11766 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Kernel][LoRA]Punica prefill  kernels fusion ( vllm-project#11234 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\n\n* [Bugfix] Update attention interface in `Whisper` ( vllm-project#11784 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [CI] Fix neuron CI and run offline tests ( vllm-project#11779 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* fix init error for MessageQueue when n_local_reader is zero ( vllm-project#11768 )\n\n* [Doc] Create a vulnerability management team ( vllm-project#9925 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [CI][CPU] adding build number to docker image name ( vllm-project#11788 )\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\n\n* [V1][Doc] Update V1 support for `LLaVa-NeXT-Video` ( vllm-project#11798 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix] Comprehensively test and fix LLaVA-NeXT feature size calculation ( vllm-project#11800 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [doc] add doc to explain how to use uv ( vllm-project#11773 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] Support audio language models on V1 ( vllm-project#11733 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [doc] update how pip can install nightly wheels ( vllm-project#11806 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Add note to `gte-Qwen2` models ( vllm-project#11808 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [optimization] remove python function call for custom op ( vllm-project#11750 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] update the prefix for qwen2 ( vllm-project#11795 )\n\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\n\n* [Doc]Add documentation for using EAGLE in vLLM ( vllm-project#11417 )\n\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\n\n* [Bugfix] Significant performance drop on CPUs with --num-scheduler-steps > 1 ( vllm-project#11794 )\n\n* [Doc] Group examples into categories ( vllm-project#11782 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Bugfix] Fix image input for Pixtral-HF ( vllm-project#11741 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] sort torch profiler table by kernel timing ( vllm-project#11813 )\n\n* Remove the duplicate imports of MultiModalKwargs and PlaceholderRange‚Ä¶ ( vllm-project#11824 )\n\n* Fixed docker build for ppc64le ( vllm-project#11518 )\n\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\n\n* [OpenVINO] Fixed Docker.openvino build ( vllm-project#11732 )\n\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\n\n* [Bugfix] Add checks for LoRA and CPU offload ( vllm-project#11810 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Docs] reorganize sponsorship page ( vllm-project#11639 )\n\nSigned-off-by: simon-mo <simon.mo@hey.com>\n\n* [Bug] Fix pickling of `ModelConfig` when RunAI Model Streamer is used ( vllm-project#11825 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] improve memory profiling ( vllm-project#11809 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [doc] update wheels url ( vllm-project#11830 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docs] Update sponsor name: 'Novita' to 'Novita AI' ( vllm-project#11833 )\n\n* [Hardware][Apple] Native support for macOS Apple Silicon ( vllm-project#11696 )\n\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [torch.compile] consider relevant code in compilation cache ( vllm-project#11614 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Reorganize profiling/processing-related code ( vllm-project#11812 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Move examples into categories ( vllm-project#11840 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Doc][4/N] Reorganize API Reference ( vllm-project#11843 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build][Bugfix] Fix CPU CI image clean up ( vllm-project#11836 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [Bugfix][XPU] fix silu_and_mul ( vllm-project#11823 )\n\nSigned-off-by: yan ma <yan.ma@intel.com>\n\n* [Misc] Move some model utils into vision file ( vllm-project#11848 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Expand Multimodal API Reference ( vllm-project#11852 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc]add some explanations for BlockHashType ( vllm-project#11847 )\n\n* [TPU][Quantization] TPU `W8A8` ( vllm-project#11785 )\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Kernel][Triton][AMD] Use block size heuristic for avg 2.8x speedup for int8 models ( vllm-project#11698 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* [Docs] Add Google Cloud Meetup ( vllm-project#11864 )\n\n* Revert nccl changes ( #351 )\n\n* Revert \"[distributed] remove pynccl's redundant change_state ( vllm-project#11749 )\"\n\nThis reverts commit 9e764e7 .\n\n* Revert \"[distributed] remove pynccl's redundant stream ( vllm-project#11744 )\"\n\nThis reverts commit 635b897 .\n\n* [CI] Turn on basic correctness tests for V1 ( vllm-project#10864 )\n\n* treat do_lower_case in the same way as the sentence-transformers library ( vllm-project#11815 )\n\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\n\n* [Doc] Recommend uv and python 3.12 for quickstart guide ( vllm-project#11849 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Misc] Move `print_*_once` from utils to logger ( vllm-project#11298 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\n\n* [Doc] Intended links Python multiprocessing library ( vllm-project#11878 )\n\n* [perf]fix current stream ( vllm-project#11870 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Override dunder methods of placeholder modules ( vllm-project#11882 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] fix beam search input errors and latency benchmark script ( vllm-project#11875 )\n\nSigned-off-by: Ye Qi <yeq@meta.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\n\n* [Doc] Add model development API Reference ( vllm-project#11884 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [platform] Allow platform specify attention backend ( vllm-project#11609 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\n\n* [ci]try to fix flaky multi-step tests ( vllm-project#11894 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Provide correct Pixtral-HF chat template ( vllm-project#11891 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* fp8 support ( #352 )\n\nCo-authored-by: Yida Wu <yidawu@amd.com>\n\n* [Docs] Add Modal to deployment frameworks ( vllm-project#11907 )\n\n* [Doc][5/N] Move Community and API Reference to the bottom ( vllm-project#11896 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\n\n* [VLM] Enable tokenized inputs for merged multi-modal processor ( vllm-project#11900 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Show default pooling method in a table ( vllm-project#11904 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [torch.compile] Hide KV cache behind torch.compile boundary ( vllm-project#11677 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Bugfix] Validate lora adapters to avoid crashing server ( vllm-project#11727 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [BUGFIX] Fix `UnspecifiedPlatform` package name ( vllm-project#11916 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [ci] fix gh200 tests ( vllm-project#11919 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [misc] remove python function call for custom activation op ( vllm-project#11885 )\n\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [platform] support pytorch custom op pluggable ( vllm-project#11328 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* Replace \"online inference\" with \"online serving\" ( vllm-project#11923 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [ci] Fix sampler tests ( vllm-project#11922 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] [1/N] Initial guide for merged multi-modal processor ( vllm-project#11925 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [platform] support custom torch.compile backend key ( vllm-project#11318 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Rename offline inference examples ( vllm-project#11927 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Docs] Fix docstring in `get_ip` function ( vllm-project#11932 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* Doc fix in `benchmark_long_document_qa_throughput.py` ( vllm-project#11933 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* [Hardware][CPU] Support MOE models on x86 CPU ( vllm-project#11831 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [Misc] Clean up debug code in Deepseek-V3 ( vllm-project#11930 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Misc] Update benchmark_prefix_caching.py fixed example usage ( vllm-project#11920 )\n\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\n\n* [Bugfix] Check that number of images matches number of <|image|> tokens with mllama ( vllm-project#11939 )\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* [mypy] Fix mypy warnings in api_server.py ( vllm-project#11941 )\n\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\n\n* [ci] fix broken distributed-tests-4-gpus ( vllm-project#11937 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix][SpecDecode] Adjust Eagle model architecture to align with intended design ( vllm-project#11672 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Bugfix] fused_experts_impl wrong compute type for float32 ( vllm-project#11921 )\n\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\n\n* [CI/Build] Move model-specific multi-modal processing tests ( vllm-project#11934 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Basic guide for writing unit tests for new models ( vllm-project#11951 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix RobertaModel loading ( vllm-project#11940 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [Model] Add cogagent model support vLLM ( vllm-project#11742 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [V1] Avoid sending text prompt to core engine ( vllm-project#11963 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [CI/Build] Add markdown linter ( vllm-project#11857 )\n\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\n\n* [Model] Initialize support for Deepseek-VL2 models ( vllm-project#11578 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Hardware][CPU] Multi-LoRA implementation for the CPU backend ( vllm-project#11100 )\n\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][TPU] workaround fix for MoE on TPU ( vllm-project#11764 )\n\n* [V1][Core][1/n] Logging and Metrics ( vllm-project#11962 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [Model] Support GGUF models newly added in `transformers` 4.46.0 ( vllm-project#9685 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction ( vllm-project#11973 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [MISC] fix typo in kv transfer send recv test ( vllm-project#11983 )\n\n* [Bug] Fix usage of `.transpose()` and `.view()` consecutively. ( vllm-project#11979 )\n\n* [CI][Spec Decode] fix: broken test for EAGLE model ( vllm-project#11972 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Misc] Fix Deepseek V2 fp8 kv-scale remapping ( vllm-project#11947 )\n\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\n\n* [Misc]Minor Changes about Worker ( vllm-project#11555 )\n\nSigned-off-by: Chenguang Li <757486878@qq.com>\n\n* [platform] add ray_device_key ( vllm-project#11948 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Fix Max Token ID for Qwen-VL-Chat ( vllm-project#11980 )\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* [Kernel] unified_attention for Attention.forward ( vllm-project#11967 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Doc][V1] Update model implementation guide for V1 support ( vllm-project#11998 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [Doc] Organise installation documentation into categories and tabs ( vllm-project#11935 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [platform] add device_control env var ( vllm-project#12009 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Platform] Move get_punica_wrapper() function to Platform ( vllm-project#11516 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function ( vllm-project#11982 )\n\nSigned-off-by: elijah <f1renze.142857@gmail.com>\n\n* Using list\n\n* Revert \"[misc] improve memory profiling ( vllm-project#11809 )\"\n\nThis reverts commit 889e662 .\n\n* Multi-lingual P3L ( #356 )\n\n* Commiting the *multilingual* P3L test.\n\n* Created a *multi-lingual* P3L test.\n\n* Making ruff happy.\n\n* .\n\n* Added a reference to the language-scripture Confluence table.\n\n* Typo fixing.\n\n* Harmonizing naming.\n\n* Fixing comments in the header.\n\n---------\n\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* Trying to make scales work with compileable attention\n\n* Docs lint\n\n* linter formatting bug fixes\n\n* inherit config file updates under fused_moe from main branch.\n\n* match tests for the MOE layers with main.\n\n---------\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Jiaxin Shan <seedjeffwan@gmail.com>\nSigned-off-by: lucast2021 <lucast2021@headroyce.org>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Alex He <alehe@amd.com>\nSigned-off-by: ccjincong <chenjincong11@gmail.com>\nSigned-off-by: Erez Schwartz <erezs@ai21.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: rajveerb <46040700+rajveerb@users.noreply.github.com>\nSigned-off-by: hjwei <hjwei_xd@163.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: KuntaiDu <kuntai@uchicago.edu>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\nSigned-off-by: Matthias Vogler <matthias.vogler@joesecurity.org>\nSigned-off-by: ApostaC <yihua98@uchicago.edu>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Lu Fang <lufang@fb.com>\nSigned-off-by: Kazuhiro Serizawa <nserihiro@gmail.com>\nSigned-off-by: Tobias Pitters <tobias.pitters@gmail.com>\nSigned-off-by: Kathy Yu <feiyangyu@google.com>\nSigned-off-by: bjmsong <bjmsong@126.com>\nSigned-off-by: wchen61 <wchen61@foxmail.com>\nSigned-off-by: ZincCat <zincchloride@outlook.com>\nSigned-off-by: xcnick <xcnick0412@gmail.com>\nSigned-off-by: Yan Burman <yanburman@users.noreply.github.com>\nSigned-off-by: Ido Asraff <idoa@atero.ai>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: Suraj Deshmukh <surajd.service@gmail.com>\nSigned-off-by: yisheng <yi.sheng@intel.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nSigned-off-by: Ye Qi <yeq@meta.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\nSigned-off-by: Chenguang Li <757486878@qq.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nSigned-off-by: Shanshan Shen <467638484@qq.com>\nSigned-off-by: elijah <f1renze.142857@gmail.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Jiaxin Shan <seedjeffwan@gmail.com>\nCo-authored-by: Lucas Tucker <47258766+lucas-tucker@users.noreply.github.com>\nCo-authored-by: lucast2021 <lucast2021@headroyce.org>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: robertgshaw2-neuralmagic <rshaw@neuralmagic.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: AlexHe99 <alehe@amd.com>\nCo-authored-by: Chen1022 <112855051+ccjincong@users.noreply.github.com>\nCo-authored-by: ErezSC42 <erezs@ai21.com>\nCo-authored-by: Selali <selali.adobor@gmail.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Rajveer Bachkaniwala <46040700+rajveerb@users.noreply.github.com>\nCo-authored-by: hj-wei <hjwei_xd@163.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: whyiug <whyiug@hotmail.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Matthias Vogler <60004995+ayylemao@users.noreply.github.com>\nCo-authored-by: Matthias Vogler <matthias.vogler@joesecurity.org>\nCo-authored-by: John Giorgi <johnmgiorgi@gmail.com>\nCo-authored-by: sakunkun <zhou.qianjun@zte.com.cn>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Yihua Cheng <yihua98@uchicago.edu>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Lu Fang <30275821+houseroad@users.noreply.github.com>\nCo-authored-by: Kazuhiro Serizawa <nserihiro@gmail.com>\nCo-authored-by: Tobias Pitters <31857876+CloseChoice@users.noreply.github.com>\nCo-authored-by: Chunyang Wen <chunyang.wen@gmail.com>\nCo-authored-by: Kathy Yu <143133934+kathyyu-google@users.noreply.github.com>\nCo-authored-by: bjmsong <wq.songbob@gmail.com>\nCo-authored-by: bjmsong <bjmsong@126.com>\nCo-authored-by: wchen61 <wchen61@foxmail.com>\nCo-authored-by: Nathan Azrak <42650258+nathan-az@users.noreply.github.com>\nCo-authored-by: Sachin Varghese <sachin.mathew31@gmail.com>\nCo-authored-by: Aurick Qiao <aurickq@users.noreply.github.com>\nCo-authored-by: Aurick Qiao <aurick.qiao@snowflake.com>\nCo-authored-by: ZincCat <52513999+zinccat@users.noreply.github.com>\nCo-authored-by: WangErXiao <863579016@qq.com>\nCo-authored-by: Hust_YangXian <bryceyx@gmail.com>\nCo-authored-by: Alberto Ferrer <albertof@barrahome.org>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: xcnick <xcnick0412@gmail.com>\nCo-authored-by: Yan Burman <yanburman@users.noreply.github.com>\nCo-authored-by: cennn <61925104+cennn@users.noreply.github.com>\nCo-authored-by: Lancer <402430575@qq.com>\nCo-authored-by: Lancer <maruixiang6688@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Suraj Deshmukh <surajd.service@gmail.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Concurrensee <yida.wu@amd.com>\nCo-authored-by: YiSheng5 <yi.sheng@intel.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan <yuan.zhou@intel.com>\nCo-authored-by: jiangjiadi <34134495+jiangjiadi@users.noreply.github.com>\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Wallas Henrique <wallashss@users.noreply.github.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Guspan Tanadi <36249910+guspan-tanadi@users.noreply.github.com>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: Yida Wu <yidawu@amd.com>\nCo-authored-by: Charles Frye <cfrye59@gmail.com>\nCo-authored-by: minmin <rmm0811@gmail.com>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Fred Reiss <frreiss@us.ibm.com>\nCo-authored-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nCo-authored-by: shaochangxu <85155497+shaochangxu@users.noreply.github.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: sixgod <evethwillbeok@outlook.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Akshat Tripathi <Akshat.tripathi6568@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: Yangcheng Li <liyangcheng.lyc@alibaba-inc.com>\nCo-authored-by: Siyuan Li <94890248+liaoyanqing666@users.noreply.github.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: elijah <30852919+e1ijah1@users.noreply.github.com>\nCo-authored-by: Alexei-V-Ivanov-AMD <156011006+Alexei-V-Ivanov-AMD@users.noreply.github.com>\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com> rasmith pushed a commit\n        to rasmith/vllm\n      that referenced\n      this pull request Jan 30, 2025 [perf]fix current stream ( vllm-project#11870 ) ‚Ä¶ 9555dd4 Signed-off-by: youkaichao <youkaichao@gmail.com> Isotr0py pushed a commit\n        to Isotr0py/vllm\n      that referenced\n      this pull request Feb 2, 2025 [perf]fix current stream ( vllm-project#11870 ) ‚Ä¶ 2ad182f Signed-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com> mzusman pushed a commit\n        to mzusman/vllm\n      that referenced\n      this pull request Mar 12, 2025 [perf]fix current stream ( vllm-project#11870 ) ‚Ä¶ 9a981e1 Signed-off-by: youkaichao <youkaichao@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:12",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | SERVING: Serving, serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:47:12",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmarks/benchmark_latency.py --model meta-llama/Meta-Llama-3-70B --load-format dummy --enforce-eager -tp 4",
  "commit_subject": "[perf]fix current stream (#11870)",
  "commit_message": "[perf]fix current stream (#11870)\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>",
  "commit_date": "2025-01-09T07:18:21Z",
  "files_changed": [
    "vllm/distributed/device_communicators/pynccl.py",
    "vllm/distributed/parallel_state.py",
    "vllm/utils.py",
    "vllm/worker/multi_step_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 4,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 4,
    "num_hunks": 14,
    "num_edited_lines": 61,
    "num_non_test_edited_lines": 61,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/distributed/device_communicators/pynccl.py b/vllm/distributed/device_communicators/pynccl.py\nindex fda4d007c..efc599871 100644\n--- a/vllm/distributed/device_communicators/pynccl.py\n+++ b/vllm/distributed/device_communicators/pynccl.py\n@@ -10,6 +10,7 @@ from vllm.distributed.device_communicators.pynccl_wrapper import (\n     ncclRedOpTypeEnum, ncclUniqueId)\n from vllm.distributed.utils import StatelessProcessGroup\n from vllm.logger import init_logger\n+from vllm.utils import current_stream\n \n logger = init_logger(__name__)\n \n@@ -96,7 +97,7 @@ class PyNcclCommunicator:\n             self.comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                 self.world_size, self.unique_id, self.rank)\n \n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n             # A small all_reduce for warmup.\n             data = torch.zeros(1, device=device)\n             self.all_reduce(data)\n@@ -119,7 +120,7 @@ class PyNcclCommunicator:\n         out_tensor = torch.empty_like(in_tensor)\n \n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllReduce(buffer_type(in_tensor.data_ptr()),\n                                 buffer_type(out_tensor.data_ptr()),\n                                 in_tensor.numel(),\n@@ -141,7 +142,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclAllGather(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), input_tensor.numel(),\n@@ -162,7 +163,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {input_tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclReduceScatter(\n             buffer_type(input_tensor.data_ptr()),\n             buffer_type(output_tensor.data_ptr()), output_tensor.numel(),\n@@ -177,7 +178,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclSend(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), dst,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -189,7 +190,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         self.nccl.ncclRecv(buffer_type(tensor.data_ptr()), tensor.numel(),\n                            ncclDataTypeEnum.from_torch(tensor.dtype), src,\n                            self.comm, cudaStream_t(stream.cuda_stream))\n@@ -201,7 +202,7 @@ class PyNcclCommunicator:\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n         if stream is None:\n-            stream = torch.cuda.current_stream()\n+            stream = current_stream()\n         if src == self.rank:\n             sendbuff = buffer_type(tensor.data_ptr())\n             # NCCL requires the sender also to have a receive buffer\ndiff --git a/vllm/distributed/parallel_state.py b/vllm/distributed/parallel_state.py\nindex a837c1dc5..be7f16ef5 100644\n--- a/vllm/distributed/parallel_state.py\n+++ b/vllm/distributed/parallel_state.py\n@@ -357,10 +357,7 @@ class GroupCoordinator:\n             return out\n         pynccl_comm = self.pynccl_comm\n         assert pynccl_comm is not None\n-        # TODO: pynccl should not use `stream=`\n-        # it can just always use the current stream.\n-        out = pynccl_comm.all_reduce(input_,\n-                                     stream=torch.cuda.current_stream())\n+        out = pynccl_comm.all_reduce(input_)\n         if out is None:\n             # fall back to the default all-reduce using PyTorch.\n             # this usually happens during testing.\ndiff --git a/vllm/utils.py b/vllm/utils.py\nindex a92b77efd..0b0905e67 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -944,6 +944,39 @@ def find_nccl_library() -> str:\n     return so_file\n \n \n+prev_set_stream = torch.cuda.set_stream\n+\n+_current_stream = None\n+\n+\n+def _patched_set_stream(stream: torch.cuda.Stream) -> None:\n+    global _current_stream\n+    _current_stream = stream\n+    prev_set_stream(stream)\n+\n+\n+torch.cuda.set_stream = _patched_set_stream\n+\n+\n+def current_stream() -> torch.cuda.Stream:\n+    \"\"\"\n+    replace `torch.cuda.current_stream()` with `vllm.utils.current_stream()`.\n+    it turns out that `torch.cuda.current_stream()` is quite expensive,\n+    as it will construct a new stream object at each call.\n+    here we patch `torch.cuda.set_stream` to keep track of the current stream\n+    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n+\n+    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n+    from C/C++ code.\n+    \"\"\"\n+    global _current_stream\n+    if _current_stream is None:\n+        # when this function is called before any stream is set,\n+        # we return the default stream.\n+        _current_stream = torch.cuda.current_stream()\n+    return _current_stream\n+\n+\n def enable_trace_function_call_for_thread(vllm_config: \"VllmConfig\") -> None:\n     \"\"\"Set up function tracing for the current thread,\n     if enabled via the VLLM_TRACE_FUNCTION environment variable\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex a2c2cebf8..acce92349 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -14,7 +14,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n-from vllm.utils import PyObjectCache, async_tensor_h2d\n+from vllm.utils import PyObjectCache, async_tensor_h2d, current_stream\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -498,7 +498,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         #   appended sampler output from last iteration\n         #   - also maybe pythonize if CPU is ahead of GPU\n \n-        current_stream = torch.cuda.current_stream()\n+        stream = current_stream()\n         if not model_input.is_first_multi_step:\n             # Explicitly block on the previous step's forward to make sure we\n             # don't clobber any GPU tensors still in use.\n@@ -541,7 +541,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                                        num_steps=1)\n \n         # record the event for the current step so that the next step can sync\n-        model_input.record_step_event(current_stream)\n+        model_input.record_step_event(stream)\n \n         if get_pp_group().is_last_rank and self.is_driver_worker:\n             assert isinstance(output, list)\n@@ -552,7 +552,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             # event for the pythonization so that we only pythonize if the\n             # tensors are ready. May be able to be combined with the step event\n             output_ready_event = torch.cuda.Event()\n-            output_ready_event.record(current_stream)\n+            output_ready_event.record(stream)\n             if self.parallel_config.pipeline_parallel_size > 1:\n                 output[0].sampled_token_ids_cpu = output[\n                     0].sampled_token_ids.cpu()",
  "apis": [
    "vllm.distributed.device_communicators.pynccl.PyNcclCommunicator.all_reduce",
    "vllm.utils.current_stream",
    "vllm.worker.multi_step_model_runner.MultiStepModelRunner.execute_model"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/device_communicators/pynccl.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/parallel_state.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/multi_step_model_runner.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test source code files in vllm by replacing multiple invocations of torch.cuda.current_stream() with a new, cached function current_stream(), which avoids the expensive construction of new stream objects on every call. This change is aimed at improving performance by reducing overhead in stream handling. The modifications affect performance-sensitive parts of the code (device communicators and model runner) on CPU for GPU workload coordination. The changes are not mere refactoring or bug fixes, but rather an explicit optimization to bypass an expensive operation.",
  "llm_api_reason": "The commit replaces multiple calls to torch.cuda.current_stream() with a more optimized current_stream() function defined in vllm/utils.py. This change affects the methods in PyNcclCommunicator from the device communicators (all_reduce, all_gather, reduce_scatter, send, recv, broadcast, etc.) as they now call current_stream() to obtain the current CUDA stream. The change is also propagated to the GroupCoordinator‚Äôs all_reduce call and to the MultiStepModelRunner in the worker where stream-capturing and event recording is updated. Overall, the current_stream API is introduced/modified and is now used across these components to improve performance during stream retrieval."
}