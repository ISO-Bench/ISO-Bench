{
  "commit_hash": "7661e92ef85e552936195ae4b803e292b9a96776",
  "pr_url": "https://github.com/vllm-project/vllm/pull/19249",
  "pr_date": "2025-06-06",
  "timeline_text": "Copy link Collaborator jeejeelee commented Jun 6, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results Purpose Test Plan Test Result Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Done ‚Ä¶ 57ae581 Signed-off-by: Jee Jee Li <pandaleefree@gmail.com> Copy link Contributor gemini-code-assist bot commented Jun 6, 2025 Warning You have reached your daily quota limit. Please wait up to 24 hours and I will start processing your requests again! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . jeejeelee commented Jun 6, 2025 View reviewed changes vllm/model_executor/models/nemotron_h.py @@ -435,7 +444,6 @@ class NemotronHForCausalLM(nn.Module, HasInnerState, SupportsLoRA, SupportsPP, \"k_proj\", \"v_proj\", ], \"gate_up_proj\": [\"up_proj\", \"down_proj\"] Copy link Collaborator Author jeejeelee Jun 6, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It's incorrect property, delete it Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions jeejeelee commented Jun 6, 2025 View reviewed changes vllm/model_executor/models/nemotron_h.py ) -> None: super().__init__() self.up_proj = MergedColumnParallelLinear ( self.up_proj = ColumnParallelLinear ( Copy link Collaborator Author jeejeelee Jun 6, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Use ColumnParallelLinear , there's no need to use MergedColumnParallelLinear Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions jeejeelee requested a review\n  from DarkLight1337 June 6, 2025 03:52 Copy link github-actions bot commented Jun 6, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . DarkLight1337 approved these changes Jun 6, 2025 View reviewed changes Copy link Member DarkLight1337 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for simplifying! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions DarkLight1337 enabled auto-merge (squash) June 6, 2025 08:10 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jun 6, 2025 Hide details View details DarkLight1337 merged commit 7661e92 into vllm-project : main Jun 6, 2025 79 checks passed Uh oh! There was an error while loading. Please reload this page . jeejeelee deleted the fix-nemotron_h branch June 6, 2025 10:31 minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [Model] Optimize nemotron_h implementation ( vllm-project#19249 ) ‚Ä¶ 8ba4ebe Signed-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: minpeter <kali2005611@gmail.com> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [Model] Optimize nemotron_h implementation ( vllm-project#19249 ) ‚Ä¶ 80ed32a Signed-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Model] Optimize nemotron_h implementation ( vllm-project#19249 ) ‚Ä¶ 9cf44d5 Signed-off-by: Jee Jee Li <pandaleefree@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:52",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:50:52",
  "models": [
    "nvidia/Nemotron-4-340B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=nvidia/Nemotron-4-340B-Instruct --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model nvidia/Nemotron-4-340B-Instruct --dataset-name sharegpt --request-rate 1",
  "commit_subject": "[Model] Optimize nemotron_h implementation (#19249)",
  "commit_message": "[Model] Optimize nemotron_h implementation (#19249)\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>",
  "commit_date": "2025-06-06T10:05:14Z",
  "files_changed": [
    "vllm/model_executor/models/nemotron_h.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 7,
    "num_edited_lines": 24,
    "num_non_test_edited_lines": 24,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/models/nemotron_h.py b/vllm/model_executor/models/nemotron_h.py\nindex 2ef8d3115..3424efa80 100644\n--- a/vllm/model_executor/models/nemotron_h.py\n+++ b/vllm/model_executor/models/nemotron_h.py\n@@ -1,4 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n+# SPDX-FileCopyrightText: Copyright contributors to the vLLM project\n \n # Adapted from https://github.com/vllm-project/vllm/blob/94d8ec8d2bcb4ec55e33022b313c7e978edf05e1/vllm/model_executor/models/bamba.py\n # Copyright 2024 HuggingFace Inc. team. All rights reserved.\n@@ -29,7 +30,7 @@ from vllm.distributed.parallel_state import get_pp_group\n from vllm.forward_context import get_forward_context\n from vllm.model_executor.layers.activation import ReLUSquaredActivation\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n+from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -63,19 +64,22 @@ class NemotronHMLP(nn.Module):\n         config: NemotronHConfig,\n         quant_config: Optional[QuantizationConfig] = None,\n         bias: bool = False,\n+        prefix: str = \"\",\n     ) -> None:\n         super().__init__()\n-        self.up_proj = MergedColumnParallelLinear(\n+        self.up_proj = ColumnParallelLinear(\n             input_size=config.hidden_size,\n-            output_sizes=[config.intermediate_size],\n+            output_size=config.intermediate_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.up_proj\",\n         )\n         self.down_proj = RowParallelLinear(\n             input_size=config.intermediate_size,\n             output_size=config.hidden_size,\n             bias=bias,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.down_proj\",\n         )\n         self.act_fn = ReLUSquaredActivation()\n \n@@ -99,9 +103,12 @@ class NemotronHMLPDecoderLayer(nn.Module):\n         super().__init__()\n         self.config = config\n \n-        self.mixer = NemotronHMLP(config,\n-                                  quant_config=quant_config,\n-                                  bias=config.mlp_bias)\n+        self.mixer = NemotronHMLP(\n+            config,\n+            quant_config=quant_config,\n+            bias=config.mlp_bias,\n+            prefix=f\"{prefix}.mixer\",\n+        )\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -207,12 +214,14 @@ class NemotronHAttention(nn.Module):\n             self.total_num_kv_heads,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.qkv_proj\",\n         )\n         self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             config.hidden_size,\n             bias=False,\n             quant_config=quant_config,\n+            prefix=f\"{prefix}.o_proj\",\n         )\n \n         self.attn = Attention(\n@@ -253,7 +262,7 @@ class NemotronHAttentionDecoderLayer(nn.Module):\n             layer_idx,\n             cache_config,\n             quant_config,\n-            prefix,\n+            prefix=f\"{prefix}.mixer\",\n         )\n \n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -435,7 +444,6 @@ class NemotronHForCausalLM(nn.Module, HasInnerState, SupportsLoRA, SupportsPP,\n             \"k_proj\",\n             \"v_proj\",\n         ],\n-        \"gate_up_proj\": [\"up_proj\", \"down_proj\"]\n     }\n \n     # LoRA specific attributes",
  "apis": [
    "NemotronHMLP.__init__",
    "NemotronHMLPDecoderLayer.__init__",
    "NemotronHAttention.__init__",
    "NemotronHAttentionDecoderLayer.__init__",
    "NemotronHModel.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/nemotron_h.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/configs/nemotron_h.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit modifies a non-test source file in the models directory and makes non‚Äêtrivial changes: it replaces a merged linear implementation (MergedColumnParallelLinear) with a regular one (ColumnParallelLinear), adjusts the argument signature (output_sizes ‚Üí output_size), and adds a new prefix parameter to several linear layer instantiations. Although the commit message says ‚ÄúOptimize nemotron_h implementation‚Äù and the modifications seem to be aimed at improving the module‚Äôs internal instantiation (which may affect runtime efficiency), the changes are not merely cosmetic or documentation fixes, and they appear targeted at refining the performance characteristics of a key model component. Therefore, considering it is an internal, CPU-testable tweak to a high-level API, the commit qualifies as performance/optimization related.",
  "llm_api_reason": "The changes in this commit update the NemotronH model implementation to pass a new \"prefix\" parameter into various child layer constructors and replace one linear layer API. In NemotronHMLP‚Äôs constructor the MergedColumnParallelLinear call is replaced by ColumnParallelLinear with an added prefix parameter, and similarly the attention and decoder layer initializations are updated to consistently pass a formatted prefix to submodules (qkv_proj, o_proj, and the mixer in both MLP and Attention decoders). These modifications affect the initialization and forward-pass behavior of several high-level NemotronH APIs within the vLLM library."
}