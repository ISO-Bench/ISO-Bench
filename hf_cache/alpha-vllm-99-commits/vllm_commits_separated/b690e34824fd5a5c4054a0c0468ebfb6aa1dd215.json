{
  "commit_hash": "b690e34824fd5a5c4054a0c0468ebfb6aa1dd215",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21075",
  "pr_date": "2025-08-02",
  "timeline_text": "Copy link Contributor cyang49 commented Jul 16, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results (Optional) The necessary documentation update, such as updating supported_models.md and examples for a new model. Purpose This PR uses preallocated output tensor for SSM output both from decode and prefill paths, instead of allocating individual tensors and then concatenating with torch.vstack . We observed that the original approach causes unnecessary D2D copy. Test Plan Testing with benchmark_serving.py and observe the throughput change. Ideally a slight improvement should be observed Testing with lm_eval to make sure output is still correct Test Result Experiments were done on single H100-80GB. benchmark_serving.py # server\nvllm serve ibm-ai-platform/Bamba-9B-v2 --port 9998 # client\npython benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --backend vllm  --dataset-name sharegpt     --dataset-path /net/storage149/mnt/md0/ccyang/github.com/ShareGPT_V3/ShareGPT_V3_unfiltered_cleaned_split.json --ignore-eos --port 9998 Before (#1c3198b) ============ Serving Benchmark Result ============\nSuccessful requests:                     983       \nBenchmark duration (s):                  44.69     \nTotal input tokens:                      209731    \nTotal generated tokens:                  195084    \nRequest throughput (req/s):              22.00     \nOutput token throughput (tok/s):         4365.18   \nTotal Token throughput (tok/s):          9058.10 After ============ Serving Benchmark Result ============\nSuccessful requests:                     983       \nBenchmark duration (s):                  44.01     \nTotal input tokens:                      209731    \nTotal generated tokens:                  195084    \nRequest throughput (req/s):              22.34     \nOutput token throughput (tok/s):         4432.88   \nTotal Token throughput (tok/s):          9198.58 No performance degradation. lm_eval # Command\nlm_eval --model vllm  --model_args pretrained=ibm-ai-platform/Bamba-9B-v2,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95 --batch_size auto --trust_remote_code  --cache_requests true --tasks gsm8k Before (#1c3198b) vllm (pretrained=ibm-ai-platform/Bamba-9B-v2,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.4162|¬±  |0.0136|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.4132|¬±  |0.0136| After vllm (pretrained=ibm-ai-platform/Bamba-9B-v2,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.4162|¬±  |0.0136|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.4132|¬±  |0.0136| (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link github-actions bot commented Jul 16, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jul 16, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces a performance optimization by pre-allocating the SSM output tensor, which avoids an unnecessary device-to-device copy. The approach is sound and the changes are well-contained. I've identified one critical issue related to tensor sharding that would cause an assertion failure when using tensor parallelism. Addressing this should make the implementation robust. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/mamba/mamba_mixer2.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . cyang49 marked this pull request as ready for review July 16, 2025 20:19 cyang49 changed the title [Model] preallocate SSM output tensor to avoid d2d copy overhead [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead Jul 16, 2025 Copy link Member DarkLight1337 commented Jul 17, 2025 cc @tlrmchlsmth @tdoublep All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Jul 21, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @cyang49 . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 21, 2025 cyang49 force-pushed the pr_mamba2_vstack branch\n    from f9ab16e to 5f73b79 Compare July 21, 2025 14:51 mergify bot removed\n  the needs-rebase label Jul 21, 2025 cyang49 force-pushed the pr_mamba2_vstack branch\n      4 times, most recently\n    from 875c81f to 3873218 Compare July 23, 2025 15:09 tlrmchlsmth reviewed Jul 30, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This looks like a reasonable optimization. My main comment is that this leaves the interface to the mamba_ssm functions more complicated than they were before. Now they support both in-place updating and out-of-place allocation of the outputs. And we need to handle those two cases in a few different places. Could we change it to always be in-place instead? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author cyang49 commented Jul 30, 2025 This looks like a reasonable optimization. My main comment is that this leaves the interface to the mamba_ssm functions more complicated than they were before. Now they support both in-place updating and out-of-place allocation of the outputs. And we need to handle those two cases in a few different places. Could we change it to always be in-place instead? I think I kept the original logic as a fall back, but you're right, we can remove them. I will push a simplified version if it is safe to remove. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Jul 30, 2025 @tlrmchlsmth There are two other uses in plamo2.py and phi4flash.py If I make the kernel only support in-place update, they will need to be changed too. plamo2 has similar logic as mamba_mixer2, so it should work after applying similar changes phi4flash looks quite different, though. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Jul 31, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . I tried to run both plamo2 and phi4flash on main (not the PR branch) and they both failed to run. I think for now we should keep the out-of-place allocation for compatibility, because I can't check the correctness if we keep only the in-place update path. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cyang49 force-pushed the pr_mamba2_vstack branch\n    from 3873218 to b165a18 Compare July 31, 2025 16:50 cyang49 requested a review\n  from WoosukKwon as a code owner July 31, 2025 16:50 Copy link Contributor Author cyang49 commented Jul 31, 2025 Fixed models that calls the affected kernels plamo2 lm_eval --model vllm  --model_args pretrained=pfnet/plamo\n-2.1-2b-cpt,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,max_model_len=8192 --batch_size auto --trust_remote_code  --cache_re\nquests true --tasks gsm8k vllm (pretrained=pfnet/plamo-2.1-2b-cpt,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,max_model_len=8192,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.5982|¬±  |0.0135|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.5951|¬±  |0.0135| phi4flash VLLM_ATTENTION_BACKEND=DIFFERENTIAL_FLASH_ATTN lm_eval --model vllm  --model_args pretrained=microsoft/Phi-4-mini-flash-reasoning,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,enable_prefix_caching=False,enable_chunked_prefill=False,max_model_len=8192 --batch_size auto --trust_remote_code  --cache_requests true --tasks gsm8k vllm (pretrained=microsoft/Phi-4-mini-flash-reasoning,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.95,enable_prefix_caching=False,enable_chunked_prefill=False,max_model_len=8192,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.5239|¬±  |0.0138|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.4837|¬±  |0.0138| üéâ 1 nopperl reacted with hooray emoji All reactions üéâ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth approved these changes Jul 31, 2025 View reviewed changes tlrmchlsmth added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 31, 2025 tlrmchlsmth enabled auto-merge (squash) July 31, 2025 19:34 auto-merge was automatically disabled August 1, 2025 18:13 Head branch was pushed to by a user without write access cyang49 force-pushed the pr_mamba2_vstack branch\n    from b165a18 to 19651f2 Compare August 1, 2025 18:13 cyang49 added 5 commits August 1, 2025 21:13 preallocate SSM output tensor to avoid d2d copy overhead ‚Ä¶ 3cee43c Signed-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com> clean up ‚Ä¶ 6d962a5 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> keep only in-place update of output ‚Ä¶ 6035133 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> mamba2 interface changes for plamo2 ‚Ä¶ 9632f0f Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> interface change phi4flash ‚Ä¶ af5f089 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> fix CI test and mamba_mixer ‚Ä¶ 97c9a70 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> cyang49 force-pushed the pr_mamba2_vstack branch\n    from d59b61d to 97c9a70 Compare August 2, 2025 01:13 Hide details View details vllm-bot merged commit b690e34 into vllm-project : main Aug 2, 2025 39 of 45 checks passed Uh oh! There was an error while loading. Please reload this page . cyang49 deleted the pr_mamba2_vstack branch August 4, 2025 11:53 wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 8223083 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: shuw <shuw@nvidia.com> juuice-lee pushed a commit\n        to juuice-lee/vllm-moe.code\n      that referenced\n      this pull request Aug 5, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 4b81d26 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 871bde5 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ c1ce688 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: x22x22 <wadeking@qq.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 07e421d ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: x22x22 <wadeking@qq.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 4b27371 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> nopperl reviewed Aug 8, 2025 View reviewed changes vllm/model_executor/layers/mamba/ops/mamba_ssm.py @@ -206,7 +206,7 @@ def selective_state_update(state, dt_softplus=False, state_batch_indices=None, pad_slot_id=PAD_SLOT_ID, preallocated_ssm_out =None): out =None): Copy link Contributor nopperl Aug 8, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think out needs to be a required argument now, because it is not allocated within the function anymore. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author cyang49 Aug 8, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good point. Will address this in an upcoming PR Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 nopperl reacted with thumbs up emoji All reactions üëç 1 reaction jingyu-ml pushed a commit\n        to jingyu-ml/vllm\n      that referenced\n      this pull request Aug 8, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 71eb0f9 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: jingyu <jingyu@omniml.ai> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ ee9e5c1 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> noamgat pushed a commit\n        to noamgat/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ c7e2edf ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Noam Gat <noamgat@gmail.com> yyihuang pushed a commit\n        to yyihuang/vllm\n      that referenced\n      this pull request Aug 11, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 2e68882 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Avery Yingyi Huang <yingyihuang2000@outlook.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 02e862a ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 49a0a42 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> BoyuanFeng pushed a commit\n        to BoyuanFeng/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 5f66814 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Boyuan Feng <boyuan@meta.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ f79d7fa ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ d9e22d3 ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> zhewenl pushed a commit\n        to zhewenl/vllm\n      that referenced\n      this pull request Aug 28, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ 1b7d42b ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhe‚Ä¶ ‚Ä¶ e3f090e ‚Ä¶ad ( vllm-project#21075 )\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> nopperl mentioned this pull request Aug 31, 2025 [V1] v1 engine + full CUDA graph support for PLaMo2 #23998 Merged 5 tasks Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:40",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, lm_eval | PERF: throughput, throughput, throughput | SERVING: vllm serve, Serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:49:40",
  "models": [
    "ibm-ai-platform/Bamba-9B-v2",
    "microsoft/Phi-4-mini-flash-reasoning"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=ibm-ai-platform/Bamba-9B-v2,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=microsoft/Phi-4-mini-flash-reasoning,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B-v2 --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)",
  "commit_message": "[Model] Mamba2 preallocate SSM output tensor to avoid d2d copy overhead (#21075)\n\nSigned-off-by: Chih-Chieh Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>",
  "commit_date": "2025-08-02T01:59:34-07:00",
  "files_changed": [
    "tests/kernels/mamba/test_mamba_ssm.py",
    "tests/kernels/mamba/test_mamba_ssm_ssd.py",
    "vllm/model_executor/layers/mamba/mamba_mixer.py",
    "vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "vllm/model_executor/layers/mamba/ops/mamba_ssm.py",
    "vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py",
    "vllm/model_executor/layers/mamba/ops/ssd_combined.py",
    "vllm/model_executor/models/phi4flash.py",
    "vllm/model_executor/models/plamo2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 7,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 9,
    "num_hunks": 39,
    "num_edited_lines": 262,
    "num_non_test_edited_lines": 165,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/kernels/mamba/test_mamba_ssm.py b/tests/kernels/mamba/test_mamba_ssm.py\nindex 8dece26dd..4c32ae81b 100644\n--- a/tests/kernels/mamba/test_mamba_ssm.py\n+++ b/tests/kernels/mamba/test_mamba_ssm.py\n@@ -365,6 +365,7 @@ def test_selective_state_update(dim, dstate, has_z, itype):\n     batch_size = 1\n     state = torch.randn(batch_size, dim, dstate, dtype=itype, device=device)\n     x = torch.randn(batch_size, dim, device=device, dtype=itype)\n+    out = torch.empty_like(x)\n     dt = torch.randn(batch_size, dim, device=device, dtype=itype)\n     dt_bias = torch.rand(dim, device=device) - 4.0\n     A = -torch.rand(dim, dstate, device=device) - 1.0\n@@ -373,16 +374,17 @@ def test_selective_state_update(dim, dstate, has_z, itype):\n     D = torch.randn(dim, device=device)\n     z = torch.randn_like(x) if has_z else None\n     state_ref = state.detach().clone()\n-    out = selective_state_update(state,\n-                                 x,\n-                                 dt,\n-                                 A,\n-                                 B,\n-                                 C,\n-                                 D=D,\n-                                 z=z,\n-                                 dt_bias=dt_bias,\n-                                 dt_softplus=True)\n+    selective_state_update(state,\n+                           x,\n+                           dt,\n+                           A,\n+                           B,\n+                           C,\n+                           D=D,\n+                           z=z,\n+                           dt_bias=dt_bias,\n+                           dt_softplus=True,\n+                           out=out)\n     out_ref = selective_state_update_ref(state_ref,\n                                          x,\n                                          dt,\n@@ -581,6 +583,7 @@ def test_selective_state_update_with_batch_indices(with_padding, dim, dstate,\n     ],\n                                         dim=0)\n     x = torch.randn(padded_batch_size, dim, device=device, dtype=itype)\n+    out = torch.empty_like(x)\n     dt = torch.randn(padded_batch_size, dim, device=device, dtype=itype)\n     dt_bias = torch.rand(dim, device=device) - 4.0\n     A = -torch.rand(dim, dstate, device=device) - 1.0\n@@ -590,18 +593,19 @@ def test_selective_state_update_with_batch_indices(with_padding, dim, dstate,\n     z = torch.randn_like(x) if has_z else None\n     state_ref = state[state_indices, :].clone()\n     state_before = state.clone()\n-    out = selective_state_update(state,\n-                                 x,\n-                                 dt,\n-                                 A,\n-                                 B,\n-                                 C,\n-                                 D=D,\n-                                 z=z,\n-                                 dt_bias=dt_bias,\n-                                 dt_softplus=True,\n-                                 state_batch_indices=padded_state_indices,\n-                                 pad_slot_id=PAD_SLOT_ID)\n+    selective_state_update(state,\n+                           x,\n+                           dt,\n+                           A,\n+                           B,\n+                           C,\n+                           D=D,\n+                           z=z,\n+                           dt_bias=dt_bias,\n+                           dt_softplus=True,\n+                           state_batch_indices=padded_state_indices,\n+                           pad_slot_id=PAD_SLOT_ID,\n+                           out=out)\n     out_ref = selective_state_update_ref(state_ref,\n                                          x[:batch_size],\n                                          dt[:batch_size],\n@@ -665,6 +669,7 @@ def test_selective_state_update_with_heads_with_batch_indices(\n         dtype=torch.int32, device=device)\n \n     x = torch.randn(batch_size, nheads, headdim, device=device, dtype=itype)\n+    out = torch.empty_like(x)\n     if not tie_hdim:\n         dt = torch.randn(batch_size,\n                          nheads,\n@@ -691,18 +696,19 @@ def test_selective_state_update_with_heads_with_batch_indices(\n     C = torch.randn(batch_size, ngroups, dstate, device=device)\n     z = torch.randn_like(x) if has_z else None\n     state_ref = state[state_indices, :].detach().clone()\n-    out = selective_state_update(state,\n-                                 x,\n-                                 dt,\n-                                 A,\n-                                 B,\n-                                 C,\n-                                 D=D,\n-                                 z=z,\n-                                 dt_bias=dt_bias,\n-                                 dt_softplus=True,\n-                                 state_batch_indices=state_indices,\n-                                 pad_slot_id=PAD_SLOT_ID)\n+    selective_state_update(state,\n+                           x,\n+                           dt,\n+                           A,\n+                           B,\n+                           C,\n+                           D=D,\n+                           z=z,\n+                           dt_bias=dt_bias,\n+                           dt_softplus=True,\n+                           state_batch_indices=state_indices,\n+                           pad_slot_id=PAD_SLOT_ID,\n+                           out=out)\n     out_ref = selective_state_update_ref(state_ref,\n                                          x,\n                                          dt,\ndiff --git a/tests/kernels/mamba/test_mamba_ssm_ssd.py b/tests/kernels/mamba/test_mamba_ssm_ssd.py\nindex 00c1a2911..67b14a7fa 100644\n--- a/tests/kernels/mamba/test_mamba_ssm_ssd.py\n+++ b/tests/kernels/mamba/test_mamba_ssm_ssd.py\n@@ -212,15 +212,16 @@ def test_mamba_chunk_scan_single_example(d_head, n_heads, seq_len_chunk_size,\n \n     Y_min, final_state_min = ssd_minimal_discrete(X * dt.unsqueeze(-1), A * dt,\n                                                   B, C, chunk_size)\n-\n-    Y, final_state = mamba_chunk_scan_combined(X,\n-                                               dt,\n-                                               A,\n-                                               B,\n-                                               C,\n-                                               chunk_size,\n-                                               D=None,\n-                                               return_final_states=True)\n+    Y = torch.empty_like(X)\n+    final_state = mamba_chunk_scan_combined(X,\n+                                            dt,\n+                                            A,\n+                                            B,\n+                                            C,\n+                                            chunk_size,\n+                                            D=None,\n+                                            return_final_states=True,\n+                                            out=Y)\n \n     # just test the last in sequence\n     torch.testing.assert_close(Y[:, -1], Y_min[:, -1], atol=atol, rtol=rtol)\n@@ -292,7 +293,8 @@ def test_mamba_chunk_scan_cont_batch(d_head, n_heads, seq_len_chunk_size_cases,\n             _query_start_loc_to_chunk_indices_offsets(\n                 cu_seqlens, chunk_size, cu_seqlens[-1])\n \n-        Y, new_states = mamba_chunk_scan_combined(\n+        Y = torch.empty_like(X)\n+        new_states = mamba_chunk_scan_combined(\n             X,\n             dt,\n             A,\n@@ -306,6 +308,7 @@ def test_mamba_chunk_scan_cont_batch(d_head, n_heads, seq_len_chunk_size_cases,\n             chunk_offsets=chunk_offsets,\n             return_varlen_states=True,\n             initial_states=states,\n+            out=Y,\n         )\n \n         # just test the last in sequence\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer.py b/vllm/model_executor/layers/mamba/mamba_mixer.py\nindex 796c8d937..60cf3e118 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer.py\n@@ -220,7 +220,8 @@ class MambaMixer(CustomOp):\n                 has_initial_state=attn_metadata.context_lens_tensor > 0,\n                 query_start_loc=attn_metadata.query_start_loc)\n         else:\n-            scan_outputs = selective_state_update(\n+            scan_outputs = torch.empty_like(hidden_states.transpose(0, 1))\n+            selective_state_update(\n                 mamba_cache_params.ssm_state,\n                 hidden_states.transpose(0, 1),\n                 discrete_time_step.transpose(0, 1),\n@@ -231,7 +232,8 @@ class MambaMixer(CustomOp):\n                 gate.transpose(0, 1),\n                 time_proj_bias,\n                 dt_softplus=True,\n-                state_batch_indices=mamba_cache_params.state_indices_tensor)\n+                state_batch_indices=mamba_cache_params.state_indices_tensor,\n+                out=scan_outputs)\n             scan_outputs = scan_outputs.transpose(0, 1)\n \n         # 4. Final linear projection\ndiff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 36edac237..5ac9a7f9a 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -541,7 +541,6 @@ class MambaMixer2(MambaBase, CustomOp):\n         # NOTE: V0 put prefill before decode, v1 puts decode before prefill\n         # Separate prefill and decode by splitting varlen input\n         # Split along token dimension\n-        # NOTE: V0 put prefill before decode, v1 puts decode before prefill\n         if envs.VLLM_USE_V1:\n             hidden_states_B_C_d, hidden_states_B_C_p = torch.split(\n                 hidden_states_B_C[:num_actual_tokens],\n@@ -583,7 +582,28 @@ class MambaMixer2(MambaBase, CustomOp):\n                                                                1]\n                                  if has_prefill else None)\n \n-        ssd_output_list = []\n+        # Preallocate output tensor to avoid memcpy cost for merging prefill\n+        # and decode outputs\n+        preallocated_ssm_out = torch.empty(\n+            [\n+                num_prefill_tokens + num_decodes,\n+                (self.num_heads // self.tp_size) * self.head_dim\n+            ],\n+            dtype=hidden_states.dtype,\n+            device=hidden_states.device,\n+        )\n+        if envs.VLLM_USE_V1:\n+            preallocated_ssm_out_d, preallocated_ssm_out_p = torch.split(\n+                preallocated_ssm_out,\n+                [num_decodes, num_prefill_tokens],\n+                dim=0,\n+            )\n+        else:\n+            preallocated_ssm_out_p, preallocated_ssm_out_d = torch.split(\n+                preallocated_ssm_out,\n+                [num_prefill_tokens, num_decodes],\n+                dim=0,\n+            )\n \n         # Process prefill requests\n         if has_prefill:\n@@ -623,7 +643,8 @@ class MambaMixer2(MambaBase, CustomOp):\n                         has_initial_states_p[:num_prefills, None, None, None],\n                         ssm_state[state_indices_tensor_p], 0)\n \n-            scan_output, varlen_state = mamba_chunk_scan_combined(\n+            # NOTE: final output is an in-place update of out tensor\n+            varlen_state = mamba_chunk_scan_combined(\n                 hidden_states_p.view(1, num_prefill_tokens,\n                                      self.num_heads // self.tp_size,\n                                      self.head_dim),\n@@ -646,15 +667,14 @@ class MambaMixer2(MambaBase, CustomOp):\n                 return_final_states=False,\n                 dt_softplus=True,\n                 dt_limit=(0.0, float(\"inf\")),\n+                out=preallocated_ssm_out_p.view(1, num_prefill_tokens, -1,\n+                                                self.head_dim),\n             )\n \n             # update ssm states\n             # - varlen state is a (num_prefills, nheads, headdim, dstate) tensor\n             ssm_state[state_indices_tensor_p] = varlen_state\n \n-            # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n-\n         # Process decode requests\n         if has_decode:\n             # 2. Convolution sequence transformation\n@@ -684,8 +704,8 @@ class MambaMixer2(MambaBase, CustomOp):\n             # - the hidden is reshaped into (bs, num_heads, head_dim)\n             # - mamba_cache_params.ssm_state's slots will be selected\n             #   using state_indices_tensor_d\n-\n-            hidden_states_d = selective_state_update(\n+            # NOTE: final output is an in-place update of out tensor\n+            selective_state_update(\n                 ssm_state,\n                 hidden_states_d,\n                 dt_d,\n@@ -697,26 +717,16 @@ class MambaMixer2(MambaBase, CustomOp):\n                 dt_bias=dt_bias,\n                 dt_softplus=True,\n                 state_batch_indices=state_indices_tensor_d,\n+                out=preallocated_ssm_out_d.view(num_decodes, -1,\n+                                                self.head_dim),\n             )\n \n-            if envs.VLLM_USE_V1:\n-                ssd_output_list.insert(\n-                    0,\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n-            else:\n-                ssd_output_list.append(\n-                    hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                         self.head_dim))\n-\n-        # Merge prefill and decode outputs before passing to gated MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n-\n         # 4. gated MLP\n         # GatedRMSNorm internally applying SiLU to the gate\n         # SiLU is applied internally before normalization, unlike standard\n         # norm usage\n-        hidden_states = self.norm(hidden_states, gate[:num_actual_tokens])\n+        hidden_states = self.norm(preallocated_ssm_out,\n+                                  gate[:num_actual_tokens])\n \n         # 5. Final linear projection\n         output[:num_actual_tokens], _ = self.out_proj(hidden_states)\ndiff --git a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\nindex 3f67fc35a..838290a9f 100644\n--- a/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n+++ b/vllm/model_executor/layers/mamba/ops/mamba_ssm.py\n@@ -205,7 +205,8 @@ def selective_state_update(state,\n                            dt_bias=None,\n                            dt_softplus=False,\n                            state_batch_indices=None,\n-                           pad_slot_id=PAD_SLOT_ID):\n+                           pad_slot_id=PAD_SLOT_ID,\n+                           out=None):\n     \"\"\"\n     Argument:\n         state: (batch, dim, dstate) or (batch, nheads, dim, dstate)\n@@ -223,10 +224,9 @@ def selective_state_update(state,\n             for example: cache_indices = [pad_slot_id, 1, 20, pad_slot_id] \n             in this case, the kernel will not process entries at \n             indices 0 and 3\n-    Return:\n-        out: (batch, dim) or (batch, nheads, dim)\n+        out: Preallocated ssm output tensor. Assume same shape as x. \n+             In-place updated.\n     \"\"\"\n-    has_heads = state.dim() > 3\n     if state.dim() == 3:\n         state = state.unsqueeze(1)\n     if x.dim() == 2:\n@@ -245,6 +245,8 @@ def selective_state_update(state,\n         z = z.unsqueeze(1)\n     if dt_bias is not None and dt_bias.dim() == 1:\n         dt_bias = dt_bias.unsqueeze(0)\n+    if out.dim() == 2:\n+        out = out.unsqueeze(1)\n \n     _, nheads, dim, dstate = state.shape\n     batch = x.shape[0]\n@@ -264,7 +266,8 @@ def selective_state_update(state,\n         assert dt_bias.shape == (nheads, dim)\n     if state_batch_indices is not None:\n         assert state_batch_indices.shape == (batch, )\n-    out = torch.empty_like(x)\n+    assert out.shape == x.shape\n+\n     grid = lambda META: (triton.cdiv(dim, META['BLOCK_SIZE_M']), batch, nheads)\n     z_strides = ((z.stride(0), z.stride(1), z.stride(2)) if z is not None else\n                  (0, 0, 0))\n@@ -328,9 +331,6 @@ def selective_state_update(state,\n             BLOCK_SIZE_M,\n             num_warps=num_warps,\n         )\n-    if not has_heads:\n-        out = out.squeeze(1)\n-    return out\n \n \n def selective_scan_fn(u,\ndiff --git a/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py b/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\nindex 61eff0c00..fc2b3b25f 100644\n--- a/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\n+++ b/vllm/model_executor/layers/mamba/ops/ssd_chunk_scan.py\n@@ -454,6 +454,7 @@ def _chunk_scan_fwd(\n     chunk_indices=None,\n     chunk_offsets=None,\n     initial_states=None,\n+    out=None,\n ):\n     batch, seqlen, nheads, headdim = x.shape\n     _, _, nchunks, chunk_size = dt.shape\n@@ -483,20 +484,10 @@ def _chunk_scan_fwd(\n     else:\n         chunk_indices, chunk_offsets = None, None\n \n-    # Allocates output.\n-    out = torch.empty(batch,\n-                      seqlen,\n-                      nheads,\n-                      headdim,\n-                      device=x.device,\n-                      dtype=x.dtype)\n+    assert out.shape == x.shape\n+\n     if z is not None:\n-        out_x = torch.empty(batch,\n-                            seqlen,\n-                            nheads,\n-                            headdim,\n-                            device=x.device,\n-                            dtype=x.dtype)\n+        out_x = torch.empty_like(x)\n         assert out_x.stride() == out.stride()\n     else:\n         out_x = None\n@@ -579,4 +570,4 @@ def _chunk_scan_fwd(\n         IS_TRITON_22=TRITON_22,\n         HAS_INITSTATES=initial_states is not None,\n     )\n-    return out, out_x\n+    return out_x\ndiff --git a/vllm/model_executor/layers/mamba/ops/ssd_combined.py b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\nindex b121275e9..ad2853a3d 100644\n--- a/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n+++ b/vllm/model_executor/layers/mamba/ops/ssd_combined.py\n@@ -36,7 +36,8 @@ def _mamba_chunk_scan_combined_fwd(x,\n                                    chunk_offsets=None,\n                                    cu_seqlens=None,\n                                    dt_softplus=False,\n-                                   dt_limit=(0.0, float(\"inf\"))):\n+                                   dt_limit=(0.0, float(\"inf\")),\n+                                   out=None):\n     batch, seqlen, nheads, headdim = x.shape\n     _, _, ngroups, dstate = B.shape\n     assert nheads % ngroups == 0\n@@ -134,7 +135,7 @@ def _mamba_chunk_scan_combined_fwd(x,\n     # - in each (pseudo) chunk, we detect if the previous (pseudo) chunk had\n     #   a seq_idx change, in which case we take states information from\n     #   init_states.\n-    out, out_x = _chunk_scan_fwd(\n+    out_x = _chunk_scan_fwd(\n         CB,\n         x,\n         dt,\n@@ -147,9 +148,10 @@ def _mamba_chunk_scan_combined_fwd(x,\n         chunk_indices=chunk_indices,\n         chunk_offsets=chunk_offsets,\n         initial_states=initial_states,\n+        out=out,\n     )\n     if cu_seqlens is None:\n-        return out, out_x, dt, dA_cumsum, states, final_states\n+        return out_x, dt, dA_cumsum, states, final_states\n     else:\n         assert batch == 1, \"passing cu_seqlens to get the varlen states is only supported if batch dimension is 1\"\n         varlen_states = chunk_state_varlen(\n@@ -161,7 +163,7 @@ def _mamba_chunk_scan_combined_fwd(x,\n             states.squeeze(0),\n             initial_states=initial_states,\n         )\n-        return out, out_x, dt, dA_cumsum, states, final_states, varlen_states\n+        return out_x, dt, dA_cumsum, states, final_states, varlen_states\n \n \n def mamba_chunk_scan_combined(x,\n@@ -180,6 +182,7 @@ def mamba_chunk_scan_combined(x,\n                               cu_seqlens=None,\n                               dt_softplus=False,\n                               dt_limit=(0.0, float(\"inf\")),\n+                              out=None,\n                               return_final_states=False,\n                               return_varlen_states=False):\n     \"\"\"\n@@ -197,15 +200,14 @@ def mamba_chunk_scan_combined(x,\n         seq_idx: (batch, seqlen)\n         cu_seqlens: (num_sequences + 1) or None, only used if return_varlen_states is True\n         dt_softplus: Whether to apply softplus to dt\n-    Return:\n-        out: (batch, seqlen, nheads, headdim)\n+        out: Preallocated output tensor\n     \"\"\"\n \n     if not return_varlen_states:\n         cu_seqlens = None\n     else:\n         assert cu_seqlens is not None, \"cu_seqlens must be provided if return_varlen_states is True\"\n-    out, out_x, dt_out, dA_cumsum, states, final_states, *rest = _mamba_chunk_scan_combined_fwd(\n+    out_x, dt_out, dA_cumsum, states, final_states, *rest = _mamba_chunk_scan_combined_fwd(\n         x,\n         dt,\n         A,\n@@ -221,12 +223,14 @@ def mamba_chunk_scan_combined(x,\n         chunk_offsets=chunk_offsets,\n         cu_seqlens=cu_seqlens,\n         dt_softplus=dt_softplus,\n-        dt_limit=dt_limit)\n+        dt_limit=dt_limit,\n+        out=out)\n     if not return_varlen_states:\n-        return out if not return_final_states else (out, final_states)\n+        if not return_final_states:\n+            return\n+        else:\n+            return final_states\n     else:\n         varlen_states = rest[0]\n-        return (out,\n-                varlen_states) if not return_final_states else (out,\n-                                                                final_states,\n+        return (varlen_states) if not return_final_states else (final_states,\n                                                                 varlen_states)\ndiff --git a/vllm/model_executor/models/phi4flash.py b/vllm/model_executor/models/phi4flash.py\nindex a4ded2b7a..1a761d01f 100644\n--- a/vllm/model_executor/models/phi4flash.py\n+++ b/vllm/model_executor/models/phi4flash.py\n@@ -387,7 +387,8 @@ class Phi4Mamba(nn.Module):\n                 has_initial_state=attn_metadata.context_lens_tensor > 0,\n                 query_start_loc=attn_metadata.query_start_loc)\n         else:\n-            scan_outputs = selective_state_update(\n+            scan_outputs = torch.empty_like(hidden_states.transpose(0, 1))\n+            selective_state_update(\n                 mamba_cache_params.ssm_state,\n                 hidden_states.transpose(0, 1),\n                 discrete_time_step.transpose(0, 1),\n@@ -400,7 +401,8 @@ class Phi4Mamba(nn.Module):\n                 None if self.yoco_kv else gate.transpose(0, 1),\n                 time_proj_bias,\n                 dt_softplus=True,\n-                state_batch_indices=mamba_cache_params.state_indices_tensor)\n+                state_batch_indices=mamba_cache_params.state_indices_tensor,\n+                out=scan_outputs)\n             scan_outputs = scan_outputs.transpose(0, 1)\n \n         # 4. Final linear projection\ndiff --git a/vllm/model_executor/models/plamo2.py b/vllm/model_executor/models/plamo2.py\nindex 9bc577cfe..8b1df66f0 100644\n--- a/vllm/model_executor/models/plamo2.py\n+++ b/vllm/model_executor/models/plamo2.py\n@@ -257,7 +257,21 @@ class Plamo2MambaMixer(nn.Module):\n         query_start_loc_p = (attn_metadata.query_start_loc[:num_prefills + 1]\n                              if has_prefill else None)\n \n-        ssd_output_list = []\n+        # Preallocate output tensor to avoid memcpy cost for merging prefill\n+        # and decode outputs\n+        preallocated_ssm_out = torch.empty(\n+            [\n+                num_prefill_tokens + num_decodes,\n+                (self.num_heads // self.tp_size) * self.head_dim\n+            ],\n+            dtype=hidden_states.dtype,\n+            device=hidden_states.device,\n+        )\n+        preallocated_ssm_out_p, preallocated_ssm_out_d = torch.split(\n+            preallocated_ssm_out,\n+            [num_prefill_tokens, num_decodes],\n+            dim=0,\n+        )\n \n         # Process prefill requests\n         if has_prefill:\n@@ -290,7 +304,7 @@ class Plamo2MambaMixer(nn.Module):\n                 initial_states = torch.where(\n                     mamba2_metadata.has_initial_states[:, None, None, None],\n                     mamba_cache_params.ssm_state[state_indices_tensor_p], 0)\n-            scan_output, varlen_state = mamba_chunk_scan_combined(\n+            varlen_state = mamba_chunk_scan_combined(\n                 hidden_states_p.view(1, num_prefill_tokens,\n                                      self.num_heads // self.tp_size,\n                                      self.head_dim),\n@@ -312,15 +326,14 @@ class Plamo2MambaMixer(nn.Module):\n                 return_final_states=False,\n                 dt_softplus=True,\n                 dt_limit=(0.0, float(\"inf\")),\n+                out=preallocated_ssm_out_p.view(1, num_prefill_tokens, -1,\n+                                                self.head_dim),\n             )\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n             mamba_cache_params.ssm_state[state_indices_tensor_p] = varlen_state\n \n-            # - reshape\n-            ssd_output_list.append(scan_output.view(num_prefill_tokens, -1))\n-\n         # Process decode requests\n         if has_decode:\n             # 2. Convolution sequence transformation\n@@ -349,8 +362,7 @@ class Plamo2MambaMixer(nn.Module):\n             # - the hidden is reshaped into (bs, num_heads, head_dim)\n             # - mamba_cache_params.ssm_state's slots will be selected\n             #   using state_indices_tensor_d\n-\n-            hidden_states_d = selective_state_update(\n+            selective_state_update(\n                 mamba_cache_params.ssm_state,\n                 hidden_states_d,\n                 dt,\n@@ -362,17 +374,13 @@ class Plamo2MambaMixer(nn.Module):\n                 dt_bias=dt_bias,\n                 dt_softplus=True,\n                 state_batch_indices=state_indices_tensor_d,\n+                out=preallocated_ssm_out_d.view(num_decodes, -1,\n+                                                self.head_dim),\n             )\n             assert self.num_heads % self.tp_size == 0\n-            ssd_output_list.append(\n-                hidden_states_d.view(-1, (self.num_heads // self.tp_size) *\n-                                     self.head_dim))\n-\n-        # Merge prefill and decode outputs before passing to MLP\n-        hidden_states = torch.vstack(ssd_output_list)\n \n         # 4. Final linear projection\n-        out = self.out_proj(hidden_states)\n+        out = self.out_proj(preallocated_ssm_out)\n         return out",
  "apis": [
    "vllm.model_executor.layers.mamba.ops.mamba_ssm.selective_state_update",
    "vllm.model_executor.layers.mamba.ops.ssd_combined.mamba_chunk_scan_combined"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/phi4flash.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/plamo2.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes multiple changes in production files (e.g., in mamba_mixer, mamba_mixer2, phi4flash, plamo2, and lower-level ops files) by introducing preallocation of output tensors to avoid redundant device-to-device memory copies. This modification targets reducing unnecessary memcpy overhead and thus improves runtime performance. It alters the way outputs are handled in performance-critical code paths, rather than merely refactoring or bug fixing. Although test files are also modified, the main performance optimizations are implemented in non-test, production modules affecting high-level APIs that run on the CPU. Therefore, the commit meets the conditions for being performance/optimization related.",
  "llm_api_reason": "This commit modifies several low‚Äêlevel SSM functions to accept a preallocated output tensor (‚Äúout‚Äù) in order to avoid extra device-to-device copy overhead. In particular, the updates add an ‚Äúout‚Äù argument to the selective_state_update() function (in vllm/model_executor/layers/mamba/ops/mamba_ssm.py) and to the mamba_chunk_scan_combined() function (in vllm/model_executor/layers/mamba/ops/ssd_combined.py). These changes propagate to various model modules (e.g. MambaMixer2, Phi4Mamba, and Plamo2MambaMixer) that call these routines in their forward passes, as reflected in the updated tests."
}