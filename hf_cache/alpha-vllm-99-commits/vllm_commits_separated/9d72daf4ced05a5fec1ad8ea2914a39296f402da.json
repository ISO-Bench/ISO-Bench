{
  "commit_hash": "9d72daf4ced05a5fec1ad8ea2914a39296f402da",
  "pr_url": "https://github.com/vllm-project/vllm/pull/15156",
  "pr_date": "2025-03-24",
  "timeline_text": "Copy link Member njhill commented Mar 19, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Queue operations showed up when profiling high qps. Since we coalesce RequestOutput objects, we don't need to use an actual queue. This changes to merge the outputs when added rather than when removed. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions [V1][Perf] Simpler request output queues ‚Ä¶ e852802 Since we coalesce RequestOutput objects we don't need to use an actual queue.\n\nThis changes to merge the outputs when added rather than when removed.\n\nSigned-off-by: Nick Hill <nhill@redhat.com> njhill requested review from WoosukKwon , robertgshaw2-redhat , ywang96 , comaniac and alexm-redhat as code owners March 19, 2025 19:57 Copy link github-actions bot commented Mar 19, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Mar 19, 2025 njhill mentioned this pull request Mar 19, 2025 [BugFix][V1] Fix parallel sampling finishing/aborts #14512 Merged njhill added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 19, 2025 houseroad reviewed Mar 21, 2025 View reviewed changes vllm/v1/engine/output_processor.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Mar 21, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @njhill . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Mar 21, 2025 houseroad reviewed Mar 21, 2025 View reviewed changes Copy link Collaborator houseroad left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looks good to me. Wondering if we should have some e2e test? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Merge remote-tracking branch 'origin/main' into queueless-output ‚Ä¶ 8fe1e45 Signed-off-by: Nick Hill <nhill@redhat.com>\n\n# Conflicts:\n#\tvllm/v1/engine/async_llm.py\n#\tvllm/v1/engine/llm_engine.py\n#\tvllm/v1/engine/parallel_sampling.py mergify bot removed\n  the needs-rebase label Mar 21, 2025 comaniac approved these changes Mar 21, 2025 View reviewed changes Copy link Collaborator comaniac left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM. Only a nit. A unit test is definitely nice to have. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction vllm/v1/engine/output_processor.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . njhill added\n  the needs-tests Tests needed for this PR label Mar 24, 2025 robertgshaw2-redhat reviewed Mar 24, 2025 View reviewed changes vllm/v1/engine/output_processor.py else: self.output = output async def get(self) -> RequestOutput: Copy link Collaborator robertgshaw2-redhat Mar 24, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Do you think we should have an invariant that output is not None if self.ready.wait() is true? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author njhill Mar 24, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment That is the case but I'm not sure what you're suggesting to add here? self.ready.wait() just waits for the condition to be set, it can only ever return True (not even sure why it returns that rather than None ). And then we immediately check self.output again before continuing. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions robertgshaw2-redhat and others added 4 commits March 24, 2025 17:47 added unit test ‚Ä¶ 47e611d Signed-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> removed stray file ‚Ä¶ af4e13b Signed-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> updated ‚Ä¶ 7382f62 Signed-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> Merge pull request #5 from robertgshaw2-redhat/add-test ‚Ä¶ 12b2758 added unit test njhill removed\n  the needs-tests Tests needed for this PR label Mar 24, 2025 njhill added 2 commits March 24, 2025 11:18 Update docstring with more detail ‚Ä¶ 639386c Signed-off-by: Nick Hill <nhill@redhat.com> Merge remote-tracking branch 'refs/remotes/origin/main' into queueles‚Ä¶ ‚Ä¶ 4612dc5 ‚Ä¶s-output Copy link Member Author njhill commented Mar 24, 2025 Thanks for adding a test @robertgshaw2-redhat ! This should be good to merge now once the CI finishes. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . robertgshaw2-redhat enabled auto-merge (squash) March 24, 2025 19:28 Copy link Collaborator robertgshaw2-redhat commented Mar 24, 2025 Looks good to me. Wondering if we should have some e2e test? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . robertgshaw2-redhat closed this Mar 24, 2025 auto-merge was automatically disabled March 24, 2025 19:29 Pull request was closed robertgshaw2-redhat reopened this Mar 24, 2025 robertgshaw2-redhat enabled auto-merge (squash) March 24, 2025 19:30 Hide details View details robertgshaw2-redhat merged commit 9d72daf into vllm-project : main Mar 24, 2025 36 of 38 checks passed Uh oh! There was an error while loading. Please reload this page . njhill deleted the queueless-output branch March 24, 2025 22:44 erictang000 pushed a commit\n        to erictang000/vllm\n      that referenced\n      this pull request Mar 25, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ 4739656 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> wrmedford pushed a commit\n        to wrmedford/vllm\n      that referenced\n      this pull request Mar 26, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ e13c5d5 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nSigned-off-by: Wes Medford <wryanmedford@gmail.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ e5e7849 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ 6a3df39 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ 7dcaa26 Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [V1][Perf] Simpler request output queues ( vllm-project#15156 ) ‚Ä¶ 048639f Signed-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:38",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: qps, profiling | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:38",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1][Perf] Simpler request output queues (#15156)",
  "commit_message": "[V1][Perf] Simpler request output queues (#15156)\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nCo-authored-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>",
  "commit_date": "2025-03-24T22:44:08Z",
  "files_changed": [
    "tests/v1/engine/test_output_processor.py",
    "vllm/v1/engine/async_llm.py",
    "vllm/v1/engine/output_processor.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 3,
    "num_hunks": 12,
    "num_edited_lines": 171,
    "num_non_test_edited_lines": 82,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/v1/engine/test_output_processor.py b/tests/v1/engine/test_output_processor.py\nindex 388f7f45e..9ac42dbc3 100644\n--- a/tests/v1/engine/test_output_processor.py\n+++ b/tests/v1/engine/test_output_processor.py\n@@ -11,11 +11,13 @@ from tests.v1.engine.utils import (NUM_PROMPT_LOGPROBS_UNDER_TEST,\n                                    STOP_STRINGS,\n                                    DummyOutputProcessorTestVectors,\n                                    MockEngineCore)\n+from vllm.outputs import CompletionOutput, RequestOutput\n from vllm.sampling_params import RequestOutputKind, SamplingParams\n from vllm.sequence import PromptLogprobs, SampleLogprobs\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.v1.engine import EngineCoreRequest\n-from vllm.v1.engine.output_processor import OutputProcessor\n+from vllm.v1.engine.output_processor import (OutputProcessor,\n+                                             RequestOutputCollector)\n from vllm.v1.metrics.stats import IterationStats\n \n \n@@ -834,3 +836,88 @@ def test_iteration_stats(dummy_test_vectors):\n \n     assert iteration_stats.num_prompt_tokens == 0\n     assert iteration_stats.num_generation_tokens == num_active\n+\n+\n+@pytest.mark.asyncio\n+async def test_request_output_collector():\n+    NUM_REQS = 3\n+    TEXT = \"a\"\n+\n+    def make_outputs() -> list[RequestOutput]:\n+        return [\n+            RequestOutput(\n+                request_id=\"my-request-id\",\n+                prompt=None,\n+                prompt_token_ids=[1, 2, 3],\n+                prompt_logprobs=None,\n+                outputs=[\n+                    CompletionOutput(\n+                        index=0,\n+                        text=TEXT,\n+                        token_ids=[idx],\n+                        cumulative_logprob=(idx + 1 * 1.0),\n+                        logprobs=[{\n+                            \"a\": idx,\n+                            \"b\": idx\n+                        }],\n+                        finish_reason=\"length\" if\n+                        (idx == NUM_REQS - 1) else None,\n+                    )\n+                ],\n+                finished=(idx == NUM_REQS - 1),\n+            ) for idx in range(NUM_REQS)\n+        ]\n+\n+    collector = RequestOutputCollector(RequestOutputKind.DELTA)\n+\n+    # CASE 1: Put then get.\n+    outputs = make_outputs()\n+    collector.put(outputs[0])\n+    output = await collector.get()\n+    assert not collector.ready.is_set()\n+    assert collector.output is None\n+    assert output.outputs[0].text == \"a\"\n+    assert output.outputs[0].token_ids == [0]\n+\n+    # CASE 2: 2 puts then get.\n+    num_to_put = 2\n+    outputs = make_outputs()\n+    for i in range(num_to_put):\n+        collector.put(outputs[i])\n+    output = await collector.get()\n+    assert not collector.ready.is_set()\n+    assert collector.output is None\n+\n+    assert not output.finished\n+    # Text, token_ids, and logprobs should get merged.\n+    assert output.outputs[0].text == TEXT * num_to_put\n+    for tok_0, tok_1 in zip(output.outputs[0].token_ids,\n+                            list(range(num_to_put))):\n+        assert tok_0 == tok_1\n+    assert len(output.outputs[0].logprobs) == num_to_put\n+\n+    # Cumulative logprobs should be the last one.\n+    cumulative_logprob_expected = 1.0 * num_to_put\n+    assert output.outputs[0].cumulative_logprob == cumulative_logprob_expected\n+\n+    # CASE 3: Put all 3 (including a finished).\n+    num_to_put = 3\n+    outputs = make_outputs()\n+    for i in range(num_to_put):\n+        collector.put(outputs[i])\n+    output = await collector.get()\n+    assert not collector.ready.is_set()\n+    assert collector.output is None\n+\n+    assert output.finished\n+    assert output.outputs[0].finish_reason == \"length\"\n+    # Text, token_ids, and logprobs should get merged.\n+    assert output.outputs[0].text == TEXT * num_to_put\n+    for tok_0, tok_1 in zip(output.outputs[0].token_ids,\n+                            list(range(num_to_put))):\n+        assert tok_0 == tok_1\n+    assert len(output.outputs[0].logprobs) == num_to_put\n+\n+    # Cumulative logprobs should be the last one.\n+    cumulative_logprob_expected = 1.0 * num_to_put\n+    assert output.outputs[0].cumulative_logprob == cumulative_logprob_expected\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex e0169f1a4..3a6811db3 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -21,14 +21,15 @@ from vllm.lora.request import LoRARequest\n from vllm.outputs import RequestOutput\n from vllm.pooling_params import PoolingParams\n from vllm.prompt_adapter.request import PromptAdapterRequest\n-from vllm.sampling_params import RequestOutputKind, SamplingParams\n+from vllm.sampling_params import SamplingParams\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\n from vllm.usage.usage_lib import UsageContext\n from vllm.utils import Device, cdiv, kill_process_tree\n from vllm.v1.engine import EngineCoreRequest\n from vllm.v1.engine.core_client import EngineCoreClient\n-from vllm.v1.engine.output_processor import OutputProcessor\n+from vllm.v1.engine.output_processor import (OutputProcessor,\n+                                             RequestOutputCollector)\n from vllm.v1.engine.parallel_sampling import ParentRequest\n from vllm.v1.engine.processor import Processor\n from vllm.v1.executor.abstract import Executor\n@@ -176,11 +177,14 @@ class AsyncLLM(EngineClient):\n         trace_headers: Optional[Mapping[str, str]] = None,\n         prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n         priority: int = 0,\n-    ) -> asyncio.Queue[RequestOutput]:\n+    ) -> RequestOutputCollector:\n         \"\"\"Add new request to the AsyncLLM.\"\"\"\n \n-        # Create a new output queue for the request.\n-        queue: asyncio.Queue[RequestOutput] = asyncio.Queue()\n+        assert isinstance(params, SamplingParams), \\\n+            \"Pooling is not supported in V1\"\n+\n+        # Create a new output collector for the request.\n+        queue = RequestOutputCollector(output_kind=params.output_kind)\n \n         # Convert Input --> Request.\n         request = self.processor.process_inputs(request_id, prompt, params,\n@@ -189,17 +193,15 @@ class AsyncLLM(EngineClient):\n                                                 prompt_adapter_request,\n                                                 priority)\n \n-        n = params.n if isinstance(params, SamplingParams) else 1\n-\n-        if n == 1:\n+        if params.n == 1:\n             await self._add_request(request, None, 0, queue)\n             return queue\n \n         # Fan out child requests (for n>1).\n         parent_request = ParentRequest(request_id, params)\n-        for idx in range(n):\n+        for idx in range(params.n):\n             request_id, params = parent_request.get_child_info(idx)\n-            child_request = request if idx == n - 1 else copy(request)\n+            child_request = request if idx == params.n - 1 else copy(request)\n             child_request.request_id = request_id\n             child_request.sampling_params = params\n             await self._add_request(child_request, parent_request, idx, queue)\n@@ -207,7 +209,7 @@ class AsyncLLM(EngineClient):\n \n     async def _add_request(self, request: EngineCoreRequest,\n                            parent_req: Optional[ParentRequest], index: int,\n-                           queue: asyncio.Queue[RequestOutput]):\n+                           queue: RequestOutputCollector):\n \n         # Add the request to OutputProcessor (this process).\n         self.output_processor.add_request(request, parent_req, index, queue)\n@@ -272,15 +274,7 @@ class AsyncLLM(EngineClient):\n             while not finished:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if not q.empty() else await q.get()\n-\n-                # Coalesce any additional queued outputs\n-                while not q.empty():\n-                    next_out = q.get_nowait()\n-                    if sampling_params.output_kind == RequestOutputKind.DELTA:\n-                        out.add(next_out)\n-                    else:\n-                        out = next_out\n+                out = q.get_nowait() or await q.get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 12df34177..1e67bed26 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -17,6 +17,46 @@ from vllm.v1.metrics.stats import (IterationStats, LoRARequestStates,\n                                    RequestStateStats)\n \n \n+class RequestOutputCollector:\n+    \"\"\"\n+    Collects streamed RequestOutputs per individual request,\n+    for hand-off to the consuming asyncio generate task.\n+\n+    When streaming deltas, RequestOutputs are merged if the\n+    producer gets ahead of the consumer.\n+    \"\"\"\n+\n+    def __init__(self, output_kind: RequestOutputKind):\n+        self.aggregate = output_kind == RequestOutputKind.DELTA\n+        self.output: Optional[RequestOutput] = None\n+        self.ready = asyncio.Event()\n+\n+    def put(self, output: RequestOutput) -> None:\n+        if self.output is None:\n+            self.output = output\n+            self.ready.set()\n+        elif self.aggregate:\n+            # Coalesce the outputs in delta case.\n+            self.output.add(output)\n+        else:\n+            # Just replace latest in non-delta case.\n+            self.output = output\n+\n+    async def get(self) -> RequestOutput:\n+        while (output := self.output) is None:\n+            await self.ready.wait()\n+        self.output = None\n+        self.ready.clear()\n+        return output\n+\n+    def get_nowait(self) -> Optional[RequestOutput]:\n+        output = self.output\n+        if output is not None:\n+            self.output = None\n+            self.ready.clear()\n+        return output\n+\n+\n @dataclass\n class OutputProcessorOutput:\n \n@@ -39,7 +79,7 @@ class RequestState:\n         detokenizer: IncrementalDetokenizer,\n         max_tokens_param: Optional[int],\n         arrival_time: float,\n-        queue: Optional[asyncio.Queue[RequestOutput]],\n+        queue: Optional[RequestOutputCollector],\n         log_stats: bool,\n     ):\n         self.request_id = request_id\n@@ -66,7 +106,7 @@ class RequestState:\n         request: EngineCoreRequest,\n         parent_req: Optional[ParentRequest],\n         request_index: int,\n-        queue: Optional[asyncio.Queue[RequestOutput]],\n+        queue: Optional[RequestOutputCollector],\n         log_stats: bool,\n     ) -> \"RequestState\":\n         if not request.sampling_params.detokenize:\n@@ -217,7 +257,7 @@ class OutputProcessor:\n         request: EngineCoreRequest,\n         parent_req: Optional[ParentRequest] = None,\n         request_index: int = 0,\n-        queue: Optional[asyncio.Queue[RequestOutput]] = None,\n+        queue: Optional[RequestOutputCollector] = None,\n     ) -> None:\n         request_id = request.request_id\n         if request_id in self.request_states:\n@@ -300,7 +340,7 @@ class OutputProcessor:\n                     new_token_ids, finish_reason, stop_reason):\n                 if req_state.queue is not None:\n                     # AsyncLLM: put into queue for handling by generate().\n-                    req_state.queue.put_nowait(request_output)\n+                    req_state.queue.put(request_output)\n                 else:\n                     # LLMEngine: return list of RequestOutputs.\n                     request_outputs.append(request_output)",
  "apis": [
    "AsyncLLM.add_request",
    "AsyncLLM.generate",
    "RequestOutputCollector.put",
    "RequestOutputCollector.get"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/async_llm.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/output_processor.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/outputs.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/outputs.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit modifies both a test file and core engine files (async_llm.py and output_processor.py) by replacing a standard asyncio.Queue with a custom RequestOutputCollector. The changes aim to reduce unnecessary task switching under load by coalescing outputs and simplifying the queuing mechanism. The code comments explicitly mention avoiding task switching to help performance, and the commit message is tagged with ‚Äú[Perf]‚Äù. These modifications affect a high-level API (the request output processing pipeline) and are designed to yield CPU-level performance improvements without any GPU-specific changes. All conditions for a performance/optimization-related commit are met.",
  "llm_api_reason": "The commit refactors how streamed outputs are queued and merged by replacing the use of asyncio.Queue with a new RequestOutputCollector in both the AsyncLLM and OutputProcessor modules. It updates the AsyncLLM.add_request and generate methods (which now operate with RequestOutputCollector) and adds/coalesces merging logic in the RequestOutputCollector‚Äôs put and get methods. These changes affect how request outputs are collected and later merged when using delta mode."
}