{
  "commit_hash": "21d93c140d0a97af5f0c59e660cf04bd417fd424",
  "pr_url": "https://github.com/vllm-project/vllm/pull/2090",
  "pr_date": null,
  "timeline_text": "Copy link Collaborator Yard1 commented Dec 13, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR implements a more efficient parallelism scheme for the Mixtral model. Instead of sharding the layers of each expert by rank, we instead shard whole experts across ranks. This gives us several benefits: We reduce the amount of communication between ranks We do not require megablocks (meaning we can now support non-CUDA accelerators) The operations become more efficient and CUDA-graphable. In the new design, each expert will conduct a dense matrix multiplication of the whole batch, and then rows not assigned to the expert will be zeroed out before accumulation. This results in a slight inefficiency for tensor parallel sizes below the number of experts - it means that we will essentially always do the upper performance bound computation. However, we have not found this to be an issue in practice. A potential improvement would be to use a sparse/grouped GEMM kernel (at least for prefill - for decode it shouldn't matter). We have benchmarked this change and found that it lowers the e2e latency for Mixtral by 4x-5x on A100-40GB TP8 compared to the previous implementation. Furthermore, the PR refactors the Mixtral model for compatibility with Hugging Face format and safetensor weights, and adds quantization support. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üéâ 19 WoosukKwon, pcmoritz, scv119, theycallmeloki, kevinhu, esmeetu, binarycrayon, liangfu, RobPruzan, JCRPaquin, and 9 more reacted with hooray emoji üëÄ 10 luiscape, nateraw, pcmoritz, kevinhu, liangfu, 152334H, RobPruzan, pierrestock, L1aoXingyu, and bernaferrari reacted with eyes emoji All reactions üéâ 19 reactions üëÄ 10 reactions Yard1 added 3 commits December 13, 2023 14:17 Cleanup a6267bd Revert \"Update Dockerfile to build Megablocks ( vllm-project#2042 )\" ‚Ä¶ 804bccb This reverts commit 3fefe27 . Revert \"Update Dockerfile to support Mixtral ( vllm-project#2027 )\" ‚Ä¶ d96ba1c This reverts commit eb17212 . Yard1 requested review from zhuohan123 , simon-mo and WoosukKwon December 13, 2023 22:23 This was referenced Dec 13, 2023 Mixtral tokens-per-second slower than expected, 10 tps #2069 Closed Support Mixtral's safetensors weights #2041 Closed WoosukKwon linked an issue Dec 13, 2023 that may be closed by this pull request Support Mixtral's safetensors weights #2041 Closed Copy link Collaborator WoosukKwon commented Dec 13, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Hi @Yard1 , thanks for the amazing work! I've just tested the PR on examples/llm_engine_example.py and got the following results: Current main INFO 12-13 23:31:55 llm_engine.py:222] # GPU blocks: 86172, # CPU blocks: 8192\nINFO 12-13 23:31:58 llm_engine.py:649] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\nRequestOutput(request_id=0, prompt='A robot may not injure a human being', prompt_token_ids=[1, 330, 18401, 993, 459, 5891, 482, 264, 2930, 1250], prompt_logprobs=[None, {330: -9.912246704101562, 22478: -0.7872462272644043}, {18401: -8.597543716430664, 633: -3.347543478012085}, {993: -4.565238952636719, 369: -2.1902387142181396}, {459: -0.4373227059841156}, {5891: -0.4258776903152466}, {482: -3.099436753473128e-06}, {264: -0.0011317284079268575}, {2930: -0.0006484074983745813}, {1250: -0.009901456534862518}], outputs=[CompletionOutput(index=0, text=' or, through inaction, allow a human being to come to harm.\\n', token_ids=[442, 28725, 1059, 297, 1774, 28725, 1914, 264, 2930, 1250, 298, 1567, 298, 6241, 28723, 13], cumulative_logprob=-0.6244106972517329, logprobs=[{442: -0.017248855903744698}, {28725: -0.002303091809153557}, {1059: -0.0011830481234937906}, {297: -0.00041952868923544884}, {1774: -7.164221460698172e-05}, {28725: -0.0003152588615193963}, {1914: -0.0006347072194330394}, {264: -0.0005576247931458056}, {2930: -0.00010775939153973013}, {1250: -0.0015303102554753423}, {298: -0.0005830018781125546}, {1567: -0.0004058252670802176}, {298: -0.0002112165529979393}, {6241: -0.0003516055876389146}, {28723: -0.03660520166158676}, {13: -0.5618820190429688}], finish_reason=length)], finished=True)\nRequestOutput(request_id=1, prompt='To be or not to be,', prompt_token_ids=[1, 1791, 347, 442, 459, 298, 347, 28725], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' that is the question.\\nWhether ‚Äôtis nobler in the mind', token_ids=[369, 349, 272, 2996, 28723, 13, 23842, 620, 24978, 28707, 278, 7169, 1523, 297, 272, 2273], cumulative_logprob=-5.713744854774632, logprobs=None, finish_reason=length)], finished=True)\nRequestOutput(request_id=2, prompt='What is the meaning of life?', prompt_token_ids=[1, 1824, 349, 272, 5746, 302, 1411, 28804], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nThe meaning of life is the question of the purpose and significance of life', token_ids=[13, 13, 1014, 5746, 302, 1411, 349, 272, 2996, 302, 272, 6032, 304, 18309, 302, 1411], cumulative_logprob=-8.794605396687984, logprobs=None, finish_reason=length), CompletionOutput(index=3, text=' It‚Äôs a question that‚Äôs been asked by philosophers, theolog', token_ids=[661, 28809, 28713, 264, 2996, 369, 28809, 28713, 750, 2261, 486, 8829, 404, 28725, 272, 1165], cumulative_logprob=-9.33446236141026, logprobs=None, finish_reason=length)], finished=True)\nRequestOutput(request_id=3, prompt='It is only with the heart that one can see rightly', prompt_token_ids=[1, 661, 349, 865, 395, 272, 3031, 369, 624, 541, 1032, 1103, 346], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='; what is essential is invisible to the eye.\\n\\nAntoine de Saint', token_ids=[28745, 767, 349, 7974, 349, 20187, 298, 272, 5421, 28723, 13, 13, 13389, 21265, 340, 6393], cumulative_logprob=-2.537341303512221, logprobs=None, finish_reason=length), CompletionOutput(index=1, text='; what is essential is invisible to the eye. Antoine de Saint-Ex', token_ids=[28745, 767, 349, 7974, 349, 20187, 298, 272, 5421, 28723, 3821, 21265, 340, 6393, 28733, 966], cumulative_logprob=-2.979412608925486, logprobs=None, finish_reason=length), CompletionOutput(index=2, text='; what is essential is invisible to the eye. ‚Äì Antoine de Saint-', token_ids=[28745, 767, 349, 7974, 349, 20187, 298, 272, 5421, 28723, 764, 3821, 21265, 340, 6393, 28733], cumulative_logprob=-3.1470024501613807, logprobs=None, finish_reason=length)], finished=True) This PR INFO 12-13 23:20:14 llm_engine.py:222] # GPU blocks: 57756, # CPU blocks: 8192\nINFO 12-13 23:20:17 llm_engine.py:649] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%\nRequestOutput(request_id=0, prompt='A robot may not injure a human being', prompt_token_ids=[1, 330, 18401, 993, 459, 5891, 482, 264, 2930, 1250], prompt_logprobs=[None, {330: -9.617840766906738, 12: -1.7154966592788696}, {18401: -8.787067413330078, 330: -2.7558176517486572}, {993: -4.204432010650635, 349: -2.0169320106506348}, {459: -0.3415136933326721}, {5891: -1.0073399543762207, 6241: -0.5073400139808655}, {482: -1.3708974620385561e-05}, {264: -0.1135331317782402}, {2930: -0.002309514442458749}, {1250: -0.016736455261707306}], outputs=[CompletionOutput(index=0, text=', or, more importantly, a robot may not kill a human being.\\n', token_ids=[28725, 442, 28725, 680, 21485, 28725, 264, 18401, 993, 459, 4015, 264, 2930, 1250, 28723, 13], cumulative_logprob=-7.275053498335183, logprobs=[{28725: -0.16343587636947632}, {442: -0.21259483695030212}, {28725: -0.1041431725025177}, {680: -1.0776935815811157}, {21485: -0.2229764610528946}, {28725: -0.01339601818472147}, {264: -1.1102567911148071}, {18401: -0.1942392736673355}, {993: -0.3014945983886719}, {459: -0.05710757523775101}, {4015: -1.3823846578598022}, {264: -0.5338531732559204}, {2930: -0.08587013930082321}, {1250: -0.040455106645822525}, {28723: -0.33675137162208557}, {13: -1.4384008646011353}], finish_reason=length)], finished=True)\nRequestOutput(request_id=1, prompt='To be or not to be,', prompt_token_ids=[1, 1791, 347, 442, 459, 298, 347, 28725], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' that is the question of every person‚Äôs life.\\nTo live, to', token_ids=[369, 349, 272, 2996, 302, 1012, 1338, 28809, 28713, 1411, 28723, 13, 1551, 2943, 28725, 298], cumulative_logprob=-19.226570382204045, logprobs=None, finish_reason=length)], finished=True)\nRequestOutput(request_id=2, prompt='What is the meaning of life?', prompt_token_ids=[1, 1824, 349, 272, 5746, 302, 1411, 28804], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='\\n\\nThe meaning of life is the meaning of one‚Äôs life. That', token_ids=[13, 13, 1014, 5746, 302, 1411, 349, 272, 5746, 302, 624, 28809, 28713, 1411, 28723, 1725], cumulative_logprob=-16.94498591311276, logprobs=None, finish_reason=length), CompletionOutput(index=4, text='\\n\\nThis question was often asked in the ancient and modern days.\\n\\n', token_ids=[13, 13, 3260, 2996, 403, 2608, 2261, 297, 272, 9467, 304, 4638, 2202, 28723, 13, 13], cumulative_logprob=-28.903032392263412, logprobs=None, finish_reason=length)], finished=True)\nRequestOutput(request_id=3, prompt='It is only with the heart that one can see rightly', prompt_token_ids=[1, 661, 349, 865, 395, 272, 3031, 369, 624, 541, 1032, 1103, 346], prompt_logprobs=None, outputs=[CompletionOutput(index=1, text='; the\\nreasonable world does not know in unces.\\n\\nHow', token_ids=[28745, 272, 13, 14991, 522, 1526, 1235, 459, 873, 297, 521, 1377, 28723, 13, 13, 5660], cumulative_logprob=-11.229475471191108, logprobs=None, finish_reason=length), CompletionOutput(index=0, text='; the\\nreasonable world does not know in unces, what the\\n', token_ids=[28745, 272, 13, 14991, 522, 1526, 1235, 459, 873, 297, 521, 1377, 28725, 767, 272, 13], cumulative_logprob=-11.59051242750138, logprobs=None, finish_reason=length), CompletionOutput(index=2, text='; the\\nreasonable world does not know in unces.\\n\\nFor', token_ids=[28745, 272, 13, 14991, 522, 1526, 1235, 459, 873, 297, 521, 1377, 28723, 13, 13, 2565], cumulative_logprob=-11.729475471191108, logprobs=None, finish_reason=length)], finished=True) In summary, 1) the results do not match; I feel the current main's output looks more correct, and 2) There's a huge decrease in allocated the KV cache size. Does this mean that this implementation has very high memory overhead? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author Yard1 commented Dec 13, 2023 Thanks, let me check! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author Yard1 commented Dec 13, 2023 FYI we have ran MMLU and recieved extremely close results for both implementations. I feel like the divergence may be due to floating point operations, but I will see if it's possible to reduce it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member esmeetu commented Dec 14, 2023 Hi @Yard1 ,which model do you use? I tried this PR with https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 and it doesn't work. It will throw KeyError: 'tok_embeddings.weight'. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member esmeetu commented Dec 14, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Hi @Yard1 ,which model do you use? I tried this PR with https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1 and it doesn't work. It will throw KeyError: 'tok_embeddings.weight'. I found that i only download .pt weights without .safetensors. Doesn't this PR support .pt formatÔºü And Do you know how to convert pt to safetensors without download again? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author Yard1 commented Dec 14, 2023 @esmeetu There is a divergence between pt and safetensors weights uploaded to huggingface hub (they use different layer names). You can use this script to convert pt to safetensors - https://github.com/huggingface/transformers/blob/v4.36.0/src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Alternative approach aee7762 Copy link Collaborator Author Yard1 commented Dec 14, 2023 @WoosukKwon I have updated the PR using an alternative approach that should both reduce memory usage and numerical inaccuracies. PTAL! üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 added 2 commits December 13, 2023 19:25 Tweak 14f0d67 Go back to dense 02d2c04 Copy link Member esmeetu commented Dec 14, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @Yard1 Thanks. I converted .pt format weights to .bin format weights. And this PR gives me x2 speedup(6t/s -> 12t/s). Thanks for your great work! Besides i compared Humaneval score on that model. And the result(50.6) is better than main branch(49.4). Another thing, i found the GPU utilization ratio is about 80% when model running. It seems that there is more space to improve performance. üëç 4 pcmoritz, Yard1, WoosukKwon, and theycallmeloki reacted with thumbs up emoji All reactions üëç 4 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Cleanup 00ad1b7 nivibilla mentioned this pull request Dec 14, 2023 Timeline on supporting Mixtral on ROCm? #2089 Closed Copy link Collaborator WoosukKwon commented Dec 14, 2023 @Yard1 The outputs after the fix look good to me! Many thanks for the quick fix! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon self-assigned this Dec 14, 2023 WoosukKwon reviewed Dec 14, 2023 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 Thanks for submitting the PR! The code looks really great overall. I'm just wondering why we need DummyModule . Please check out my comments. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 nittaya111 reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction vllm/config.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Dockerfile Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/models/mixtral.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/models/mixtral.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/models/mixtral.py config.hidden_size, config.intermediate_size, linear_method=linear_method) if idx in self.expert_indicies else DummyModule() Copy link Collaborator WoosukKwon Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I actually didn't understand why we need DummyModule here. Could you elaborate more on this? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The whole purpose of the dummy module is so that we can discard weights for experts we do not want to load on a given rank. If you have a better way of doing that, please let me know! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator WoosukKwon Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Sorry, why can't we just use None ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Isn't that going to cause exceptions during weights loading? If not then we should definitely use None Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor liangfu Dec 14, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Instead of adding a placeholder as DummpyModule , construct self.experts as for a list of experts in local_rank? For instance, with num_local_experts=8, tp_size=4, expert_indicies=[0,1], construct self.experts with first two experts and make the ModuleList short? Since gating network is replicated, getting access to routing_weights locally in each rank should be easy, right? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It's moreso about how to make this compatible with vLLM's TP weight loading logic, which uses list indices Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator WoosukKwon Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Let's merge the PR for the release and fix this issue in another PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon mentioned this pull request Dec 14, 2023 Bump up to v0.2.5 #2095 Merged liangfu reviewed Dec 14, 2023 View reviewed changes vllm/model_executor/models/mixtral.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Yard1 added 3 commits December 13, 2023 21:54 Remove fschat 90a9fb0 Fix top_k 1b744e2 ROCM a5c7da4 WoosukKwon added 2 commits December 14, 2023 07:47 Warning for pt weights ea91f03 Fix ROCm supported model doc 39aaf15 WoosukKwon approved these changes Dec 14, 2023 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Many thanks for the great work! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 nittaya111 reacted with heart emoji üöÄ 2 Yard1 and ArthurZucker reacted with rocket emoji All reactions ‚ù§Ô∏è 1 reaction üöÄ 2 reactions Copy link Collaborator WoosukKwon commented Dec 14, 2023 @liangfu Thanks for the review! ‚ù§Ô∏è 2 Yard1 and nittaya111 reacted with heart emoji All reactions ‚ù§Ô∏è 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon merged commit 21d93c1 into vllm-project : main Dec 14, 2023 Yard1 deleted the mixtral_expert_parallelism branch December 14, 2023 08:01 This was referenced Dec 14, 2023 performance of Mixtral-8x7B inference #2098 Closed Refactor Mixtral to reuse code from MegaBlocks #2032 Closed tgale96 reviewed Dec 14, 2023 View reviewed changes vllm/model_executor/models/mixtral.py else: final_hidden_states.add_(current_hidden_states) return tensor_model_parallel_all_reduce(final_hidden_states).view( Copy link tgale96 Dec 14, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Hi! I'm curious to understand what's going on in this implementation. The PR calls this expert parallelism but it still looks like tensor parallelism to me? At least, if this is expert parallelism, I don't see any logic routing the tokens to the device that owns the expert it was assigned to? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 Dec 14, 2023 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment We run every expert on the rank and zero out the rows that were not selected to be used by the expert. We then all reduce the tensors across the ranks. This results in dense computations (and higher memory usage), but it dramatically reduces latency, especially for small batch sizes. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link tgale96 Dec 15, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Ah ok, thank you! I was confused by the \"expert parallelism\" in the PR name, which I think is a misnomer here :) The prior implementation with MegaBlocks was using a training-optimized code path. I'd expect it to be very inefficient because a) it pads each expert batch to the nearest multiple of 128 and b) dispatches to sparse matmul kernels which use tile dimensions tuned for large problems. For inference, its much better to use our grouped implementation, which avoids these pitfalls. Essentially what is in the function here . Our gather/scatter kernels handle replication for top_k>1 as well as the permutation to group tokens by expert assignment. They're also written in Triton so they should work fine on AMD. For the MLP, we dispatch to custom grouped GEMM ops, but you can also use a pure-Torch grouped MLP like what's happening in this PR to make it AMD compatible. This is the direction I'd go to improve the current implementation further, fwiw. You don't necessarily need to add MegaBlocks as a dep - most of this can be replicated without too much complexity. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 Dec 15, 2023 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the explanation! I definitely agree there is a lot of room to expand here. Looking forward to more contributions from you or the community! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link tom-doerr commented Dec 14, 2023 Of we run all experts anyway, how about using more of the results? https://www.reddit.com/r/LocalLLaMA/comments/18i2h4c/mixtral_gets_even_better_by_just_adding_an_expert/ üëç 1 NickLucche reacted with thumbs up emoji ‚ù§Ô∏è 1 nittaya111 reacted with heart emoji All reactions üëç 1 reaction ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link vibhuagrawal14 commented Dec 14, 2023 For me, the speed has increased from 11 tok/s to 30+ üöÄ üéâ 4 scv119, pcmoritz, tom-doerr, and TissueC reacted with hooray emoji ‚ù§Ô∏è 1 nittaya111 reacted with heart emoji All reactions üéâ 4 reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . 0xymoro mentioned this pull request Dec 15, 2023 Mixtral optimization from vllm NVIDIA/TensorRT-LLM#672 Closed xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Dec 18, 2023 Optimize Mixtral with expert parallelism ( vllm-project#2090 ) f49edbe timohear mentioned this pull request Feb 1, 2024 Mixtral nf4 performance 2x slower than expected huggingface/text-generation-inference#1501 Closed 4 tasks hongxiayang pushed a commit\n        to hongxiayang/vllm\n      that referenced\n      this pull request Feb 13, 2024 Optimize Mixtral with expert parallelism ( vllm-project#2090 ) bc7486b Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:30",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "LM_EVAL: MMLU, Humaneval | PERF: throughput, throughput, throughput",
  "analysis_extracted_at": "2025-09-07 17:49:30",
  "models": [
    "mistralai/Mixtral-8x7B-Instruct-v0.1"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1,tensor_parallel_size=8 --tasks mmlu --batch_size auto"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1 --tensor-parallel-size 8",
  "commit_subject": "Optimize Mixtral with expert parallelism (#2090)",
  "commit_message": "Optimize Mixtral with expert parallelism (#2090)",
  "commit_date": "2023-12-13T23:55:07-08:00",
  "files_changed": [
    "Dockerfile",
    "README.md",
    "docs/source/models/supported_models.rst",
    "vllm/config.py",
    "vllm/model_executor/models/__init__.py",
    "vllm/model_executor/models/mixtral.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 6,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 6,
    "num_hunks": 15,
    "num_edited_lines": 555,
    "num_non_test_edited_lines": 555,
    "commit_year": 2023
  },
  "diff_text": "diff --git a/Dockerfile b/Dockerfile\nindex f41753aeb..6ef03b843 100644\n--- a/Dockerfile\n+++ b/Dockerfile\n@@ -41,14 +41,6 @@ ENV NVCC_THREADS=$nvcc_threads\n \n RUN python3 setup.py build_ext --inplace\n \n-# Build the megablocks library as wheel because it doesn't publish pre-built wheels.\n-# https://github.com/stanford-futuredata/megablocks/commit/5897cd6f254b7b3edf7a708a3a3314ecb54b6f78\n-RUN apt-get install -y git && \\\n-    git clone https://github.com/stanford-futuredata/megablocks.git && \\\n-    cd megablocks && \\\n-    git checkout 5897cd6f254b7b3edf7a708a3a3314ecb54b6f78 && \\\n-    MAX_JOBS=8 NVCC_THREADS=8 python3 setup.py bdist_wheel\n-\n # image to run unit testing suite\n FROM dev AS test\n \n@@ -85,12 +77,8 @@ FROM vllm-base AS vllm-openai\n RUN --mount=type=cache,target=/root/.cache/pip \\\n     pip install accelerate\n \n-COPY vllm vllm\n COPY --from=build /workspace/vllm/*.so /workspace/vllm/\n-COPY --from=build /workspace/megablocks/dist/*.whl /tmp/\n-RUN --mount=type=cache,target=/root/.cache/pip \\\n-    pip install /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl && \\\n-    rm /tmp/megablocks-0.5.0-cp310-cp310-linux_x86_64.whl\n+COPY vllm vllm\n \n ENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n \ndiff --git a/README.md b/README.md\nindex 84cadee48..e4b3b5026 100644\n--- a/README.md\n+++ b/README.md\n@@ -72,10 +72,6 @@ Install vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/get\n ```bash\n pip install vllm\n ```\n-**NOTE:** The Mixtral model additionally requires `megablocks` which can be installed with pip or [from source](https://github.com/stanford-futuredata/megablocks):\n-```bash\n-pip install megablocks\n-```\n \n ## Getting Started\n \ndiff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst\nindex e21cdd65d..44e4fe5ea 100644\n--- a/docs/source/models/supported_models.rst\n+++ b/docs/source/models/supported_models.rst\n@@ -74,8 +74,7 @@ Otherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for in\n Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.\n \n .. note::\n-    Currently, the ROCm version of vLLM does not support Mixtral.\n-    Additionally, it only supports Mistral for context lengths up to 4096.\n+    Currently, the ROCm version of vLLM supports Mistral and Mixtral only for context lengths up to 4096.\n \n .. tip::\n     The easiest way to check if your model is supported is to run the program below:\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6bafa73c7..eb1fee0f2 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -120,14 +120,16 @@ class ModelConfig:\n             if load_format == \"auto\":\n                 load_format = \"pt\"\n \n-        # FIXME(woosuk): This is a temporary hack. Support safetensor weights.\n+        # TODO: Remove this check once HF updates the pt weights of Mixtral.\n         architectures = getattr(self.hf_config, \"architectures\", [])\n-        if \"MixtralForCausalLM\" in architectures and load_format != \"pt\":\n-            logger.info(\n-                \"Currently, only 'pt' format is supported for Mixtral. \"\n-                \"Changing the format to 'pt'. This may re-download the \"\n-                \"weights if you have downloaded the safetensor weights.\")\n-            load_format = \"pt\"\n+        if \"MixtralForCausalLM\" in architectures:\n+            if load_format == \"pt\":\n+                raise ValueError(\n+                    \"Currently, the 'pt' format is not supported for Mixtral. \"\n+                    \"Please use the 'safetensors' format instead. \")\n+            elif load_format == \"auto\":\n+                # Do not fall back to pt weights.\n+                load_format = \"safetensors\"\n \n         self.load_format = load_format\n \ndiff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 5596884f3..ab9a1636a 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -39,13 +39,15 @@ _MODELS = {\n }\n \n # Models not supported by ROCm.\n-_ROCM_UNSUPPORTED_MODELS = [\"MixtralForCausalLM\"]\n+_ROCM_UNSUPPORTED_MODELS = []\n \n # Models partially supported by ROCm.\n # Architecture -> Reason.\n _ROCM_PARTIALLY_SUPPORTED_MODELS = {\n     \"MistralForCausalLM\":\n     \"Sliding window attention is not yet supported in ROCm's flash attention\",\n+    \"MixtralForCausalLM\":\n+    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n }\n \n \ndiff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex 8e0a094c7..b11e3713f 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -31,22 +31,11 @@ import torch.nn.functional as F\n from torch import nn\n from transformers import MixtralConfig\n \n-try:\n-    import megablocks.ops as ops\n-except ImportError as e:\n-    raise ImportError(\"MegaBlocks not found. \"\n-                      \"Please install it by `pip install megablocks`.\") from e\n-try:\n-    import stk\n-except ImportError as e:\n-    raise ImportError(\n-        \"STK not found. \"\n-        \"Please install it by `pip install stanford-stk`.\") from e\n-\n from vllm.model_executor.input_metadata import InputMetadata\n from vllm.model_executor.layers.attention import PagedAttention\n from vllm.model_executor.layers.layernorm import RMSNorm\n from vllm.model_executor.layers.linear import (LinearMethodBase,\n+                                               ReplicatedLinear,\n                                                QKVParallelLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.rotary_embedding import get_rope\n@@ -66,8 +55,134 @@ from vllm.sequence import SamplerOutput\n KVCache = Tuple[torch.Tensor, torch.Tensor]\n \n \n-def promote_scalar(x: torch.Tensor) -> torch.Tensor:\n-    return x.view(1) if len(x.size()) == 0 else x\n+class MixtralMLP(nn.Module):\n+\n+    def __init__(\n+        self,\n+        num_experts: int,\n+        hidden_size: int,\n+        intermediate_size: int,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.num_experts = num_experts\n+        self.ffn_dim = intermediate_size\n+        self.hidden_dim = hidden_size\n+\n+        self.w1 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w2 = ReplicatedLinear(self.ffn_dim,\n+                                   self.hidden_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+        self.w3 = ReplicatedLinear(self.hidden_dim,\n+                                   self.ffn_dim,\n+                                   bias=False,\n+                                   linear_method=linear_method)\n+\n+        # TODO: Use vllm's SiluAndMul\n+        self.act_fn = nn.SiLU()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        w1_out, _ = self.w1(hidden_states)\n+        w1_out = self.act_fn(w1_out)\n+        w3_out, _ = self.w3(hidden_states)\n+        current_hidden_states = w1_out * w3_out\n+        current_hidden_states, _ = self.w2(current_hidden_states)\n+        return current_hidden_states\n+\n+\n+class DummyModule(nn.Module):\n+\n+    def __init__(self) -> None:\n+        super().__init__()\n+\n+        self.w1 = nn.Linear(0, 0, bias=False)\n+        self.w2 = nn.Linear(0, 0, bias=False)\n+        self.w3 = nn.Linear(0, 0, bias=False)\n+\n+        set_weight_attrs(self.w1.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w2.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+        set_weight_attrs(self.w3.weight,\n+                         {\"weight_loader\": self.dummy_weight_loader})\n+\n+    def forward(self, *args, **kwargs) -> None:\n+        raise NotImplementedError()\n+\n+    def dummy_weight_loader(self, *args, **kwargs) -> None:  # pylint: disable=unused-argument\n+        # Noop\n+        return\n+\n+\n+class MixtralMoE(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.rank = get_tensor_model_parallel_rank()\n+        self.tp_size = get_tensor_model_parallel_world_size()\n+        self.num_total_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+        if self.tp_size > self.num_total_experts:\n+            raise ValueError(\n+                f\"Tensor parallel size {self.tp_size} is greater than \"\n+                f\"the number of experts {self.num_total_experts}.\")\n+        # Split experts equally between ranks\n+        self.expert_indicies = np.array_split(range(\n+            self.num_total_experts), self.tp_size)[self.rank].tolist()\n+        if not self.expert_indicies:\n+            raise ValueError(\n+                f\"Rank {self.rank} has no experts assigned to it.\")\n+\n+        self.experts = nn.ModuleList([\n+            MixtralMLP(self.num_total_experts,\n+                       config.hidden_size,\n+                       config.intermediate_size,\n+                       linear_method=linear_method)\n+            if idx in self.expert_indicies else DummyModule()\n+            for idx in range(self.num_total_experts)\n+        ])\n+        self.gate = ReplicatedLinear(config.hidden_size,\n+                                     self.num_total_experts,\n+                                     bias=False,\n+                                     linear_method=linear_method)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+        router_logits, _ = self.gate(hidden_states)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights,\n+                                                       self.top_k,\n+                                                       dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+\n+        final_hidden_states = None\n+        for expert_idx in self.expert_indicies:\n+            expert_layer = self.experts[expert_idx]\n+            expert_mask = (selected_experts == expert_idx)\n+            expert_weights = (routing_weights * expert_mask).sum(dim=-1,\n+                                                                 keepdim=True)\n+\n+            current_hidden_states = expert_layer(hidden_states).mul_(\n+                expert_weights)\n+            if final_hidden_states is None:\n+                final_hidden_states = current_hidden_states\n+            else:\n+                final_hidden_states.add_(current_hidden_states)\n+\n+        return tensor_model_parallel_all_reduce(final_hidden_states).view(\n+            batch_size, sequence_length, hidden_dim)\n \n \n class MixtralAttention(nn.Module):\n@@ -78,6 +193,7 @@ class MixtralAttention(nn.Module):\n                  num_kv_heads: int,\n                  max_position: int = 4096 * 32,\n                  rope_theta: float = 10000,\n+                 linear_method: Optional[LinearMethodBase] = None,\n                  sliding_window: Optional[int] = None) -> None:\n         super().__init__()\n         self.hidden_size = hidden_size\n@@ -102,24 +218,26 @@ class MixtralAttention(nn.Module):\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n \n-        self.wqkv = QKVParallelLinear(\n+        self.qkv_proj = QKVParallelLinear(\n             hidden_size,\n             self.head_dim,\n             self.total_num_heads,\n             self.total_num_kv_heads,\n             bias=False,\n+            linear_method=linear_method,\n         )\n-        self.wo = RowParallelLinear(\n+        self.o_proj = RowParallelLinear(\n             self.total_num_heads * self.head_dim,\n             hidden_size,\n             bias=False,\n+            linear_method=linear_method,\n         )\n         self.rotary_emb = get_rope(\n             self.head_dim,\n             rotary_dim=self.head_dim,\n             max_position=max_position,\n             base=int(self.rope_theta),\n-            is_neox_style=False,  # weights not in HF format\n+            is_neox_style=True,\n         )\n         self.attn = PagedAttention(\n             self.num_heads,\n@@ -137,310 +255,74 @@ class MixtralAttention(nn.Module):\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:\n-        qkv, _ = self.wqkv(hidden_states)\n+        qkv, _ = self.qkv_proj(hidden_states)\n         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n         q, k = self.rotary_emb(positions, q, k)\n         k_cache, v_cache = kv_cache\n         attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,\n                                 cache_event)\n-        output, _ = self.wo(attn_output)\n+        output, _ = self.o_proj(attn_output)\n         return output\n \n \n-class BlockSparseMoE(nn.Module):\n-    \"\"\"\n-    Built on the paper and library Megablocks as described in\n-    https://arxiv.org/abs/2211.15841. This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accomodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-    \"\"\"\n-\n-    def __init__(self, hidden_dim: int, ffn_dim: int, num_experts: int,\n-                 top_k: int):\n-        super().__init__()\n-        self.hidden_dim = hidden_dim\n-        self.ffn_dim = ffn_dim\n-        self.num_experts = num_experts\n-        self.top_k = top_k\n-\n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim,\n-                              self.num_experts,\n-                              bias=False,\n-                              device=torch.cuda.current_device())\n-\n-        tp_size = get_tensor_model_parallel_world_size()\n-        assert self.ffn_dim % tp_size == 0\n-        self.ffn_dim_per_partition = self.ffn_dim // tp_size\n-        # merged expert weights, all of size  (ffn_dim * n_experts, model_dim)\n-        self.w1 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w1, {\"weight_loader\": self.moe_weight_loader})\n-        self.w2 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w2, {\"weight_loader\": self.moe_weight_loader})\n-        self.w3 = nn.Parameter(\n-            torch.empty(self.ffn_dim_per_partition * self.num_experts,\n-                        self.hidden_dim,\n-                        device=torch.cuda.current_device()))\n-        set_weight_attrs(self.w3, {\"weight_loader\": self.moe_weight_loader})\n-\n-        # Calculate the number of bits needed to represent the expert indices\n-        # so that we can pass it to radix sort.\n-        self.sort_end_bit = max(int(np.ceil(np.log2(self.num_experts))), 1)\n-        self.blocking = 128\n-        self.quantize_scatter_num_bits = -1\n-\n-        # Calculate the number of bits needed to represent the column indices\n-        # in the intermediate sparse matrix.\n-        max_column_index = (self.ffn_dim * self.num_experts) // self.blocking\n-        self.transpose_sort_end_bit = max(\n-            int(np.ceil(np.log2(max_column_index))), 1)\n-\n-    def moe_weight_loader(self, param: nn.Parameter,\n-                          loaded_weight: torch.Tensor) -> None:\n-        \"\"\"\n-        Load the weights for the MoE linear layer.\n-        \"\"\"\n-        tp_rank = get_tensor_model_parallel_rank()\n-        shard_size = self.ffn_dim_per_partition\n-        loaded_weight = loaded_weight.view(self.num_experts, self.ffn_dim, -1)\n-        loaded_weight = loaded_weight[:, shard_size * tp_rank:shard_size *\n-                                      (tp_rank + 1)]\n-        loaded_weight = loaded_weight.reshape_as(param)\n-        param.data.copy_(loaded_weight)\n-\n-    def sparse_transpose(\n-            self, size: int, row_indices,\n-            column_indices) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n-        block_columns = size[1] // self.blocking\n-\n-        # Sort row indices by column indices to get the transposed matrix's\n-        # column indices.\n-        #\n-        # NOTE: Our sort operation uses the same width indices as the input\n-        # values. To avoid overflow when we have large activation matrices\n-        # we cast to 32-bit before sorting.\n-        _, gather_indices = ops.sort(column_indices.int(),\n-                                     self.transpose_sort_end_bit)\n-\n-        # There are a constant number of blocks in every row of the sparse\n-        # matrix. A blocks offset is:\n-        #\n-        # row_index * blocks_per_row + column_index % blocks_per_row\n-        #\n-        # Once we have the block offsets ordered for transposition we can\n-        # divide by blocks_per_row to get the transposed column indices.\n-        column_indices_t = row_indices.gather(0, gather_indices.long())\n-        block_offsets_t = gather_indices.int()\n-\n-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)\n-        nnz_per_column = ops.histogram(column_indices, block_columns)\n-        nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)\n-        offsets_t = torch.cat([zero, nnz_per_column])\n-        return column_indices_t, offsets_t, block_offsets_t\n-\n-    def topology(self, x: torch.Tensor,\n-                 padded_bins: torch.Tensor) -> \"stk.Matrix\":\n-        padded_tokens, _ = x.size()\n-        assert padded_tokens % self.blocking == 0\n-        assert self.ffn_dim_per_partition % self.blocking == 0\n-\n-        # Offsets for the sparse matrix. All rows have the\n-        # same number of nonzero blocks dictated by the\n-        # dimensionality of a single expert.\n-        block_rows = padded_tokens // self.blocking\n-        blocks_per_row = self.ffn_dim_per_partition // self.blocking\n-        offsets = torch.arange(\n-            0,\n-            block_rows * blocks_per_row + 1,\n-            blocks_per_row,\n-            dtype=torch.int32,\n-            device=x.device,\n-        )\n-\n-        # Indices for the sparse matrix. The indices for\n-        # the intermediate matrix are dynamic depending\n-        # on the mapping of tokens to experts.\n-        column_indices = ops.topology(padded_bins, self.blocking, block_rows,\n-                                      blocks_per_row)\n-\n-        # TODO(tgale): This is unused. Remove the need for this in stk.\n-        # For now, use meta init to save the device memory.\n-        data = torch.empty(\n-            column_indices.numel(),\n-            self.blocking,\n-            self.blocking,\n-            dtype=x.dtype,\n-            device=\"meta\",\n-        )\n-        shape = (padded_tokens, self.ffn_dim_per_partition * self.num_experts)\n-        row_indices = stk.ops.row_indices(shape, data, offsets, column_indices)\n-        column_indices_t, offsets_t, block_offsets_t = self.sparse_transpose(\n-            shape, row_indices, column_indices)\n-        return stk.Matrix(\n-            shape,\n-            data,\n-            row_indices,\n-            column_indices,\n-            offsets,\n-            column_indices_t,\n-            offsets_t,\n-            block_offsets_t,\n-        )\n-\n-    def indices_and_padded_bins(\n-        self, selected_experts: torch.Tensor\n-    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,\n-               torch.Tensor]:\n-        # Sort the expert ids to produce the scatter/gather\n-        # indices for the permutation.\n-        selected_experts = selected_experts.int()\n-        bin_ids, indices = ops.sort(selected_experts, self.sort_end_bit)\n-\n-        # Histogram the expert ids to identify the number of\n-        # tokens routed to each expert.\n-        tokens_per_expert = ops.histogram(selected_experts, self.num_experts)\n-\n-        # Round the token counts up to the block size used in\n-        # the matrix muliplications. Caculate the starting\n-        # position of each bin.\n-        padded_tokens_per_expert = ops.round_up(tokens_per_expert,\n-                                                self.blocking)\n-        padded_bins = ops.inclusive_cumsum(padded_tokens_per_expert, 0)\n-        padded_bins = promote_scalar(padded_bins)\n-\n-        # Calculate the bin bounds for the sorted tokens.\n-        bins = ops.inclusive_cumsum(tokens_per_expert, 0)\n-        bins = promote_scalar(bins)\n-        return indices, bin_ids, bins, padded_bins, tokens_per_expert\n-\n-    @torch.inference_mode()\n-    def forward(self, x: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n-        x: (sequence_length, model_dim)\n-        gate_logits: (sequence_length, n_experts)\n-        \"\"\"\n-        # optional reshape\n-        input_shape = x.shape\n-        x = x.view(-1, input_shape[-1])\n-\n-        # gate_logits: (sequence_length, n_experts)\n-        gate_logits = self.gate(x)\n-        # all_probs: (sequence_length, n_experts) and upcast for softmax\n-        all_probs = F.softmax(gate_logits, dim=1, dtype=torch.float)\n-        # weights, selected_experts: (sequence_length, top-k)\n-        weights, selected_experts = torch.topk(all_probs, self.top_k, dim=-1)\n-        weights /= weights.sum(dim=-1, keepdim=True)\n-        weights = weights.flatten().to(x.dtype)\n-        selected_experts = selected_experts.flatten()\n-\n-        (indices, bin_ids, bins, padded_bins,\n-         _) = self.indices_and_padded_bins(selected_experts)\n-\n-        # Permute tokens and pad to prepare expert computation\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = ops.padded_gather(x, indices, bin_ids, bins, padded_bins,\n-                              self.top_k)\n-\n-        # Create the sparse matrix topology\n-        with torch.no_grad():\n-            topo = self.topology(x, padded_bins)\n-\n-        # Perform the expert computation\n-        # First Dense x Dense -> Sparse for w1 and w3,\n-        # (top_k * sequence_length + padding, ffn_dim * n_experts)\n-        x = stk.Matrix(\n-            topo.size(),\n-            F.silu(stk.ops.sdd(x, self.w1.t(), topo).data) *\n-            stk.ops.sdd(x, self.w3.t(), topo).data,\n-            topo.row_indices,\n-            topo.column_indices,\n-            topo.offsets,\n-            topo.column_indices_t,\n-            topo.offsets_t,\n-            topo.block_offsets_t,\n-        )\n-\n-        # Then Sparse x Dense -> Dense for w2\n-        # (top_k * sequence_length + padding, model_dim)\n-        x = stk.ops.dsd(x, self.w2)\n-\n-        x = tensor_model_parallel_all_reduce(x)\n-\n-        # Permute back and remove padding\n-        # (top_k * sequence_length, model_dim)\n-        x = ops.padded_scatter(\n-            x,\n-            indices,\n-            bin_ids,\n-            weights,\n-            bins,\n-            padded_bins,\n-            self.top_k,\n-            self.quantize_scatter_num_bits,\n-        )\n-        return x.view(*input_shape)\n-\n-\n class MixtralDecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         # Requires transformers > 4.32.0\n         rope_theta = getattr(config, \"rope_theta\", 10000)\n-        self.attention = MixtralAttention(\n+        self.self_attn = MixtralAttention(\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n             max_position=config.max_position_embeddings,\n             num_kv_heads=config.num_key_value_heads,\n             rope_theta=rope_theta,\n-            sliding_window=config.sliding_window)\n-        self.block_sparse_moe = BlockSparseMoE(\n-            hidden_dim=self.hidden_size,\n-            ffn_dim=config.intermediate_size,\n-            num_experts=config.num_local_experts,\n-            top_k=config.num_experts_per_tok,\n-        )\n-        self.attention_norm = RMSNorm(config.hidden_size,\n-                                      eps=config.rms_norm_eps)\n-        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+            sliding_window=config.sliding_window,\n+            linear_method=linear_method)\n+        self.block_sparse_moe = MixtralMoE(config=config,\n+                                           linear_method=linear_method)\n+        self.input_layernorm = RMSNorm(config.hidden_size,\n+                                       eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n+                                                eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         positions: torch.Tensor,\n-        x: torch.Tensor,\n+        hidden_states: torch.Tensor,\n         kv_cache: KVCache,\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n+        residual: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n-        r = self.attention(\n+        # Self Attention\n+        if residual is None:\n+            residual = hidden_states\n+            hidden_states = self.input_layernorm(hidden_states)\n+        else:\n+            hidden_states, residual = self.input_layernorm(\n+                hidden_states, residual)\n+        hidden_states = self.self_attn(\n             positions=positions,\n-            hidden_states=self.attention_norm(x),\n+            hidden_states=hidden_states,\n             kv_cache=kv_cache,\n             input_metadata=input_metadata,\n             cache_event=cache_event,\n         )\n-        h = x + r\n-        r = self.block_sparse_moe(self.ffn_norm(h))\n-        out = h + r\n-        return out\n \n+        # Fully Connected\n+        hidden_states, residual = self.post_attention_layernorm(\n+            hidden_states, residual)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n+        return hidden_states, residual\n \n-class MixtralForCausalLM(nn.Module):\n+\n+class MixtralModel(nn.Module):\n \n     def __init__(\n         self,\n@@ -448,23 +330,18 @@ class MixtralForCausalLM(nn.Module):\n         linear_method: Optional[LinearMethodBase] = None,\n     ) -> None:\n         super().__init__()\n-        self.config = config\n-        assert linear_method is None\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n-        self.tok_embeddings = VocabParallelEmbedding(\n+\n+        self.embed_tokens = VocabParallelEmbedding(\n             config.vocab_size,\n             config.hidden_size,\n         )\n-\n-        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.output = ParallelLMHead(config.vocab_size, config.hidden_size)\n-        self.sampler = Sampler(config.vocab_size)\n-\n         self.layers = nn.ModuleList([\n-            MixtralDecoderLayer(config)\n+            MixtralDecoderLayer(config, linear_method=linear_method)\n             for _ in range(config.num_hidden_layers)\n         ])\n+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -474,20 +351,42 @@ class MixtralForCausalLM(nn.Module):\n         input_metadata: InputMetadata,\n         cache_events: Optional[List[torch.cuda.Event]],\n     ) -> SamplerOutput:\n-        hidden_states = self.tok_embeddings(input_ids)\n-\n-        # forward\n+        hidden_states = self.embed_tokens(input_ids)\n+        residual = None\n         for i in range(len(self.layers)):\n             cache_event = None if cache_events is None else cache_events[i]\n             layer = self.layers[i]\n-            hidden_states = layer(\n-                positions,\n-                hidden_states,\n-                kv_caches[i],\n-                input_metadata,\n-                cache_event,\n-            )\n-        hidden_states = self.norm(hidden_states)\n+            hidden_states, residual = layer(positions, hidden_states,\n+                                            kv_caches[i], input_metadata,\n+                                            cache_event, residual)\n+        hidden_states, _ = self.norm(hidden_states, residual)\n+        return hidden_states\n+\n+\n+class MixtralForCausalLM(nn.Module):\n+\n+    def __init__(\n+        self,\n+        config: MixtralConfig,\n+        linear_method: Optional[LinearMethodBase] = None,\n+    ) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.linear_method = linear_method\n+        self.model = MixtralModel(config, linear_method)\n+        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n+        self.sampler = Sampler(config.vocab_size)\n+\n+    def forward(\n+        self,\n+        input_ids: torch.Tensor,\n+        positions: torch.Tensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n+    ) -> torch.Tensor:\n+        hidden_states = self.model(input_ids, positions, kv_caches,\n+                                   input_metadata, cache_events)\n         return hidden_states\n \n     def sample(\n@@ -495,7 +394,7 @@ class MixtralForCausalLM(nn.Module):\n         hidden_states: Optional[torch.Tensor],\n         sampling_metadata: SamplingMetadata,\n     ) -> SamplerOutput:\n-        next_tokens = self.sampler(self.output.weight, hidden_states,\n+        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens\n \n@@ -506,10 +405,11 @@ class MixtralForCausalLM(nn.Module):\n                      revision: Optional[str] = None):\n         stacked_params_mapping = [\n             # (param_name, shard_name, shard_id)\n-            (\"wqkv\", \"wq\", \"q\"),\n-            (\"wqkv\", \"wk\", \"k\"),\n-            (\"wqkv\", \"wv\", \"v\"),\n+            (\"qkv_proj\", \"q_proj\", \"q\"),\n+            (\"qkv_proj\", \"k_proj\", \"k\"),\n+            (\"qkv_proj\", \"v_proj\", \"v\"),\n         ]\n+\n         params_dict = dict(self.named_parameters())\n         for name, loaded_weight in hf_model_weights_iterator(\n                 model_name_or_path, cache_dir, load_format, revision):",
  "apis": [
    "vllm.model_executor.models.mixtral.MixtralAttention",
    "vllm.model_executor.models.mixtral.MixtralDecoderLayer",
    "vllm.model_executor.models.mixtral.MixtralModel",
    "vllm.model_executor.models.mixtral.MixtralForCausalLM",
    "vllm.model_executor.models.mixtral.MixtralMoE"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mixtral.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/config.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies several non-test source files, including Dockerfile changes, documentation updates, and comprehensive modifications in the Mixtral-related modules within the model executor. The changes are not trivial ‚Äì they completely rework the Mixtral architecture components (such as adding a new MixtralMoE module with expert parallelism, adjusting how projections and gating are handled, and replacing deprecated or less efficient implementations) in order to improve model performance. Despite the commit message using \"Optimize Mixtral with expert parallelism\", the changes are clearly performance oriented rather than simple refactoring, bug fixes, or feature additions. These modifications are designed to affect performance on the main inference APIs and are testable without requiring GPU-specific or hardware-specific contexts. Thus, this commit qualifies as performance/optimization related.",
  "llm_api_reason": "The commit removes the megablocks dependency from the Dockerfile and documentation and updates the Mixtral model implementation to support expert parallelism. In particular, it refactors key components in the Mixtral model such as the attention, decoder layer, and MoE implementations. New classes and modifications include changes to MixtralAttention (renaming ‚Äúwqkv‚Äù to ‚Äúqkv_proj‚Äù and adding a linear_method parameter), the introduction of MixtralMLP, DummyModule, and a revised MixtralMoE for expert parallelism, as well as breaking out MixtralModel and MixtralForCausalLM to reflect these changes. These modifications affect the high-level Python APIs used for the Mixtral models."
}