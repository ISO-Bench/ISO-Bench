{
  "commit_hash": "83450458339b07765b0e72a822e5fe93eeaf5258",
  "pr_url": "https://github.com/vllm-project/vllm/pull/9333",
  "pr_date": "2024-10-16",
  "timeline_text": "Copy link Collaborator LiuXiaoxuanPKU commented Oct 14, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . After benchmarking the performance of ngram in vllm, it seems that the proposal time is longer than expected. The main reason is that there are (1) CPU <-> GPU communication when building the ngram lookup table. (2) Building the ngram contains many small kernels (duration < 5 microseconds) as show below: Zoom in the propose time: The PR tries to (1) perform lookup operation on CPU. (2) trigger CPU <-> GPU communication only when there is a match in lookup. Some performance numbers on a single H100: input_len: 550, output_len: 150 I changed the prompt to try different system efficiency (which might include the number of CPU <-> GPU sync). System efficiency propose time before this PR propose time after this PR end2end latency before this PR end2end latency after this PR 0.31 4.4ms 2.2ms 6.4s 5.6s 0.63 3.3ms 1.5ms 3.8s 3.2s 0.80 2.6 ms 1.5ms 3.0s 2.6s input_len: 2048, output_len: 150 System efficiency propose time before this PR propose time after this PR end2end latency before this PR end2end latency after this PR 0.30 6.00ms 4.54ms 9.83s 9.25s 0.63 2.90ms 2.70ms 5.84s 5.45s Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 3 comaniac, trianxy, and exceedzhang reacted with thumbs up emoji üöÄ 2 mgoin and trianxy reacted with rocket emoji All reactions üëç 3 reactions üöÄ 2 reactions LiuXiaoxuanPKU added 2 commits October 13, 2024 22:45 lookup on cpu 7d631fb remove comments cc8e7a6 Copy link github-actions bot commented Oct 14, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . format 083897a comaniac approved these changes Oct 14, 2024 View reviewed changes Copy link Collaborator comaniac left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator comaniac commented Oct 14, 2024 btw does this approach still have speedup if the prompt length is much longer? I'm just thinking about the trade off between CPU-GPU sync overhead and (maybe) slower CPU computation. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author LiuXiaoxuanPKU commented Oct 14, 2024 btw does this approach still have speedup if the prompt length is much longer? I'm just thinking about the trade off between CPU-GPU sync overhead and (maybe) slower CPU computation. Yeah will do more benchmarks here All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin approved these changes Oct 14, 2024 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think this is worth considering just for the aspect of simplicity. It could even make sense to write a CPU kernel in C++ instead of trying to do it on GPU Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Oct 14, 2024 Copy link Collaborator Author LiuXiaoxuanPKU commented Oct 16, 2024 Will change the PR so that we can change the device based on the sequence length. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . switch device based on seq_len 44ae31d Hide details View details mgoin merged commit 8345045 into vllm-project : main Oct 16, 2024 53 checks passed Uh oh! There was an error while loading. Please reload this page . Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Performance][Spec Decode] Optimize ngram lookup performance ( vllm-pr‚Ä¶ ‚Ä¶ 86678fd ‚Ä¶oject#9333 )\n\nSigned-off-by: Alvant <alvasian@yandex.ru> garg-amit pushed a commit\n        to garg-amit/vllm\n      that referenced\n      this pull request Oct 28, 2024 [Performance][Spec Decode] Optimize ngram lookup performance ( vllm-pr‚Ä¶ ‚Ä¶ 10d88b1 ‚Ä¶oject#9333 )\n\nSigned-off-by: Amit Garg <mitgarg17495@gmail.com> FerdinandZhong pushed a commit\n        to FerdinandZhong/vllm\n      that referenced\n      this pull request Oct 29, 2024 [Performance][Spec Decode] Optimize ngram lookup performance ( vllm-pr‚Ä¶ ‚Ä¶ b55f889 ‚Ä¶oject#9333 )\n\nSigned-off-by: qishuai <ferdinandzhong@gmail.com> sumitd2 pushed a commit\n        to sumitd2/vllm\n      that referenced\n      this pull request Nov 14, 2024 [Performance][Spec Decode] Optimize ngram lookup performance ( vllm-pr‚Ä¶ ‚Ä¶ 1e9f47e ‚Ä¶oject#9333 )\n\nSigned-off-by: Sumit Dubey <sumit.dubey2@ibm.com> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Performance][Spec Decode] Optimize ngram lookup performance ( vllm-pr‚Ä¶ ‚Ä¶ 2a3ec7b ‚Ä¶oject#9333 )\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:54",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:54",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)",
  "commit_message": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)",
  "commit_date": "2024-10-16T13:37:45-06:00",
  "files_changed": [
    "vllm/spec_decode/ngram_worker.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 17,
    "num_non_test_edited_lines": 17,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 36e5e1774..a777e5c3f 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -67,9 +67,16 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 execute_model_req.seq_group_metadata_list):\n             seq_data = next(iter(seq_group_metadata.seq_data.values()))\n \n+            seq_len = seq_data.get_len()\n+            # When seq_len is less than 3072 (3K), we use CPU to perform\n+            # the ngram match. Otherwise, we use the device specified in\n+            # the model config (normally GPU). 3072 is a rough threshold\n+            # based on profiling on H100, and it can be adjusted based\n+            # on the actual performance on different hardware.\n+            cur_device = \"cpu\" if seq_len < 3072 else self.device\n             input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                         dtype=torch.long,\n-                                        device=self.device)\n+                                        device=cur_device)\n             input_length = seq_data.get_len()\n \n             for ngram_size in range(\n@@ -91,17 +98,15 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 # first_match includes \"values\" (bool), indicating whether\n                 # the match is found, and \"indices\", indicating the index\n                 # of the first match.\n-                # Note that \"first_match.values.item()\" triggers GPU-CPU\n-                # sync so it is a bit inefficient, but we have not found\n-                # a better way to do this.\n                 first_match = matches.max(dim=-1)\n                 if first_match.values.item():\n                     proposal_start_idx = first_match.indices.add_(ngram_size)\n                     spec_indices = (\n                         proposal_start_idx).repeat(sample_len) + torch.arange(\n-                            sample_len, device=self.device)\n+                            sample_len, device=cur_device)\n                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n-                    res = input_ids.gather(dim=-1, index=spec_indices)\n+                    res = input_ids.gather(dim=-1,\n+                                           index=spec_indices).to(self.device)\n                     token_id_list.append(res)\n                     token_prob_list.append(\n                         torch.nn.functional.one_hot(",
  "apis": [
    "None"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/ngram_proposer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a core non-test file (ngram_worker.py) altering how n-gram lookups are handled by conditionally using the CPU over the GPU for sequences shorter than 3072 tokens. It changes the device assignment logic to potentially mitigate costly GPU-CPU synchronization, which is a clear performance optimization. The changes are non-trivial and affect the underlying implementation of a primary API used for inference, making it testable on CPU without dependency on specialized GPUs. Thus, based on the performance optimization criteria, this commit qualifies as performance related.",
  "llm_api_reason": "This commit optimizes the internal n‚Äëgram matching in the speculative decoding worker by dynamically picking the device (CPU for short sequences and GPU otherwise) and ensuring that the gathered results are later transferred to the expected device. Although it improves performance for speculative decoding, it does not change any public or high‚Äëlevel API (e.g. vllm.LLM.generate or similar) exposed by vLLM."
}