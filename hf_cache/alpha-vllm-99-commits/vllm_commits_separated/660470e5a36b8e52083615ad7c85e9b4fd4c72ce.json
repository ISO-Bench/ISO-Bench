{
  "commit_hash": "660470e5a36b8e52083615ad7c85e9b4fd4c72ce",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7193",
  "pr_date": "2024-08-06",
  "timeline_text": "Copy link Contributor xiaobochen123 commented Aug 6, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Using the AutoPrefixCache, the block_manager_v2 performs worse than v1. llama-3.1-8b, H800 Test 3510 cases from mmlu dataset llm = LLM(\n Â  Â  Â   model=path,\n Â  Â  Â   tensor_parallel_size=1,\n Â  Â  Â   trust_remote_code=True,\n Â  Â  Â   gpu_memory_utilization=0.8,\n Â  Â  Â   max_num_seqs=512,\n Â  Â  Â   enable_prefix_caching=True,\n Â  Â  Â   use_v2_block_manager=XXXX,\n Â   )\nâ€‹\nsampling_params = SamplingParams(temperature=1.0, max_tokens=1)\nâ€‹\nmmlu_dataset = [...] # 3510 cases from mmlu\nâ€‹\noutputs = llm.generate(\n Â  Â  Â   sampling_params=sampling_params,\n Â  Â  Â   prompt_token_ids=mmlu_dataset,\n Â   ) The self.free_table in evictor_v2::LRUEvictor is OrderedDict class that remembers the order in which keys were first inserted. The larger timestamps will be at the end. The reason V2 slower than V1 , is that V2 will go through all the free_table, in evict. V2 has the 'update',  It breaks the order. So we can move the block to the end when update. That can keep the lowest timestamp at the start. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ğŸ‘ 4 youkaichao, jon-chuang, mgoin, and shixianc reacted with thumbs up emoji All reactions ğŸ‘ 4 reactions Copy link github-actions bot commented Aug 6, 2024 ğŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. ğŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member youkaichao commented Aug 6, 2024 thanks for the contribution! cc @cadedaniel @zhuohan123 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . xiaobochen123 force-pushed the opt_evictor branch\n    from 52379a2 to 8f387b2 Compare August 6, 2024 08:04 opt evictor-v2 performance 0856f66 xiaobochen123 force-pushed the opt_evictor branch\n    from 8f387b2 to 0856f66 Compare August 6, 2024 08:19 Yard1 mentioned this pull request Aug 6, 2024 [Performance][Core] Optimize the performance of evictor v1 and v2 by applying a priority queue and lazy deletion #7209 Merged cadedaniel approved these changes Aug 6, 2024 View reviewed changes Copy link Collaborator cadedaniel commented Aug 6, 2024 Looks good to me, although the NeuralMagic folks have better understanding of the prefix caching paths. cc @robertgshaw2-neuralmagic All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member youkaichao commented Aug 6, 2024 Looks pretty reasonable to me, and the test also passed. I will go ahead to merge this. thanks again @xiaobochen123 for the contribution! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details youkaichao merged commit 660470e into vllm-project : main Aug 6, 2024 28 checks passed Uh oh! There was an error while loading. Please reload this page . comaniac mentioned this pull request Aug 16, 2024 [MISC] Add prefix cache hit rate to metrics #7606 Merged Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Core] Optimize evictor-v2 performance ( vllm-project#7193 ) â€¦ 1ed56fb Signed-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Core] Optimize evictor-v2 performance ( vllm-project#7193 ) â€¦ ba80305 Signed-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:19",
  "has_lm_eval": true,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: mmlu, mmlu | TEST: Test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:48:19",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,tensor_parallel_size=1,gpu_memory_utilization=0.8,max_num_seqs=512,enable_prefix_caching=True,use_v2_block_manager=True --tasks mmlu --batch_size auto"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 1 --enable-prefix-caching --use-v2-block-manager",
  "commit_subject": "[Core] Optimize evictor-v2 performance (#7193)",
  "commit_message": "[Core] Optimize evictor-v2 performance (#7193)",
  "commit_date": "2024-08-06T12:34:25-07:00",
  "files_changed": [
    "vllm/core/evictor_v2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 6,
    "num_non_test_edited_lines": 6,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/core/evictor_v2.py b/vllm/core/evictor_v2.py\nindex 3dd12e2e2..5b1a208b7 100644\n--- a/vllm/core/evictor_v2.py\n+++ b/vllm/core/evictor_v2.py\n@@ -91,8 +91,9 @@ class LRUEvictor(Evictor):\n         # at the start of OrderedDict. Loop through all these blocks to\n         # find the one with maximum number of hashed tokens.\n         for _id, block in self.free_table.items():\n-            if evicted_block.last_accessed > block.last_accessed or (\n-                    evicted_block.last_accessed == block.last_accessed and\n+            if evicted_block.last_accessed < block.last_accessed:\n+                break\n+            if (evicted_block.last_accessed == block.last_accessed and\n                     evicted_block.num_hashed_tokens < block.num_hashed_tokens):\n                 evicted_block = block\n                 evicted_block_id = _id\n@@ -109,6 +110,7 @@ class LRUEvictor(Evictor):\n \n     def update(self, block_id: int, last_accessed: float):\n         self.free_table[block_id].last_accessed = last_accessed\n+        self.free_table.move_to_end(block_id)\n \n     def remove(self, block_id: int):\n         if block_id not in self.free_table:",
  "apis": [
    "None"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/api_server.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/api_server.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test file (vllm/core/evictor_v2.py) by altering the logic in the eviction loop (changing the condition from \">\" to \"<\" and reorganizing the conditional checks) and by adding a move_to_end call in the update method. These changes are non-trivial and target the core eviction algorithm's efficiency, likely to reduce unnecessary iterations or improve cache behavior. Although the commit message mentions \"Optimize evictor-v2 performance,\" the changes themselves are substantive algorithm improvements and not merely refactoring or minor fixes. They affect high-level internal performance on the CPU without specific hardware constraints. [ANSWER] YES [/ANSWER]",
  "llm_api_reason": "The commit optimizes the internal eviction logic in the LRUEvictor class (located in vllm/core/evictor_v2.py) by modifying the block selection conditions and updating the free_table ordering in the update() method. These changes are internal optimizations that do not alter any high-level or public Python APIs exposed by the repository."
}