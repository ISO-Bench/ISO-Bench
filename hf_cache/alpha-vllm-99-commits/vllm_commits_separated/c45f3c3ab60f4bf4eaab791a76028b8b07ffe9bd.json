{
  "commit_hash": "c45f3c3ab60f4bf4eaab791a76028b8b07ffe9bd",
  "pr_url": "https://github.com/vllm-project/vllm/pull/17",
  "pr_date": "2023-03-31",
  "timeline_text": "Copy link Member zhuohan123 commented Mar 31, 2023 Speed before this PR: ubuntu@ray-zhuohan-cf-head-d95da8d2-compute:~/nfs/cacheflow/cacheflow$ python benchmark/benchmark_latency.py --model facebook/opt-13b\nNamespace(batch_size=8, block_size=8, dtype='half', input_len=32, max_batch_size=2560, model='facebook/opt-13b', model_path='~/.cacheflow/model_weights', output_len=128, pipeline_parallel_size=1, seed=0, swap_space=20, tens\nor_parallel_size=1)\n2023-03-31 14:17:41,580 INFO worker.py:1535 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8266\n# GPU blocks: 1975, # CPU blocks: 3276\nWarm up step\nProfile step: 100%|██████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.18s/it]\nAvg latency: 5.184098243713379 seconds Speed after this PR: ubuntu@ray-zhuohan-cf-head-d95da8d2-compute:~/nfs/cacheflow/cacheflow$ python benchmark/benchmark_latency.py --model facebook/opt-13b\nNamespace(batch_size=8, block_size=8, dtype='half', input_len=32, max_batch_size=2560, model='facebook/opt-13b', model_path='~/.cacheflow/model_weights', output_len=128, pipeline_parallel_size=1, seed=0, swap_space=20, tensor_parallel_size=1)\n2023-03-31 15:20:04,885 INFO worker.py:1535 -- Started a local Ray instance. View the dashboard at http://127.0.0.1:8266\n# GPU blocks: 1975, # CPU blocks: 3276\nWarm up step\nProfile step: 100%|██████████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.49s/it]\nAvg latency: 3.492198626200358 seconds Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions zhuohan123 added 2 commits March 31, 2023 15:25 Optimize tensor parallel execution speed a32f244 add more files c3e6bce zhuohan123 requested a review\n  from WoosukKwon March 31, 2023 15:32 WoosukKwon approved these changes Mar 31, 2023 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Awesome! Thanks for the effort. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon reviewed Mar 31, 2023 View reviewed changes benchmark/benchmark_latency.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . nit 2bea93e zhuohan123 merged commit c45f3c3 into main Mar 31, 2023 zhuohan123 deleted the optimize-tp-speed branch June 18, 2023 07:22 shanshanpt mentioned this pull request Nov 17, 2023 Run long conetxt error : CUDA error: an illegal memory access was encountered #1700 Closed junior-zsy mentioned this pull request Nov 20, 2023 Error with 32k Long Text in chatglm2-6b-32k Model #1725 Closed hongxiayang pushed a commit\n        to hongxiayang/vllm\n      that referenced\n      this pull request Feb 13, 2024 Optimize tensor parallel execution speed ( vllm-project#17 ) ad3d36f AdrianAbeyta referenced\n      this pull request\n        in ROCm/vllm Mar 8, 2024 Merge pull request #17 from ROCm/IFU-2024-03-01-fp8-kv … b3d81e0 Rebase fp8_kv branch with upstream (3-07-2024) z103cb referenced\n      this pull request\n        in z103cb/opendatahub_vllm Apr 22, 2024 Compile kernels and fix build ( opendatahub-io#17 ) … 15076fa These Dockerfile changes:\n- Update the release stage to work with the recently refactored\n`requirements-common.txt` / `requirements-cuda.txt` split\n- Fixup the kernel compilation in the `build` stage to correctly pick up\ncuda\n- Install the kernels from this docker build rather than pulling a\nprecompiled wheel. We can swap that back once a new wheel is available\nwith the correct pytorch version + updated interfaces\n\n---------\n\nSigned-off-by: Nick Hill <nickhill@us.ibm.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com> fxmarty pushed a commit\n        to fxmarty/vllm-public\n      that referenced\n      this pull request May 31, 2024 Merge pull request vllm-project#17 from ROCm/triton-config-fix … bebcbe6 [ROCm] adding a missing triton autotune config alixiaodi mentioned this pull request Aug 2, 2024 [Bug]: #7072 Closed SpaceHunterInf mentioned this pull request Sep 30, 2024 [Bug]: Bus error (core dumped) #8974 Closed 1 task wuhuikx pushed a commit\n        to wuhuikx/vllm\n      that referenced\n      this pull request Mar 27, 2025 [Platform] add dispatch key ( vllm-project#17 ) … dd425d6 ### What this PR does / why we need it?\nAdd dispatch key for NPU, so that the log could be print correctly.\n\nNow\n```\nexecutor_base.py:110] # CPU blocks: 220478, # CPU blocks: 21845\n```\n\nAfter this pr\n```\nexecutor_base.py:110] # NPU blocks: 220478, # CPU blocks: 21845\n```\n\n### Does this PR introduce _any_ user-facing change?\nN/A\n\n### How was this patch tested?\nCI passed and log printed as above\n\nSigned-off-by: MengqingCao <cmq0113@163.com> hao-cold mentioned this pull request May 13, 2025 [Bug]: CUDA error: an illegal instruction was encountered #18045 Open 1 task markmc mentioned this pull request May 21, 2025 [Bug][Failing Test]: Distributed Comm Ops - distributed/test_shm_broadcast.py #18492 Closed 1 task zerosurplus mentioned this pull request Jun 16, 2025 [Bug]: torch.distributed.DistNetworkError: The client socket has timed out after 600000ms while trying to connect to (172.17.0.9, 46229). #19670 Open 1 task robertgshaw2-redhat added a commit\n      that referenced\n      this pull request Jul 7, 2025 Merge pull request #17 from praveingk/batching … 39e6bd1 Load balance across multiple workers xiaomofang mentioned this pull request Jul 31, 2025 [Bug]: There is an issue with speculative inference in Eagle mode, where the context length of vLLM inference is constrained by the draft model. #21986 Open 1 task zyongye pushed a commit\n        to zyongye/vllm\n      that referenced\n      this pull request Aug 5, 2025 Add TRT-LLM Attention Sink and MXFP4 MoE ( vllm-project#17 ) 78e69f6 zyongye pushed a commit\n        to zyongye/vllm\n      that referenced\n      this pull request Aug 6, 2025 Add TRT-LLM Attention Sink and MXFP4 MoE ( vllm-project#17 ) 2cc41a7 JeffreyWong20 mentioned this pull request Aug 19, 2025 [Bug]: [TPU] profiling_tpu/profiling.py example crashed when runs on vllm_tpu docker #23194 Closed 1 task Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:36",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, Profile | TEST: Test, CI",
  "analysis_extracted_at": "2025-09-07 17:49:36",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmark/benchmark_latency.py --model facebook/opt-13b",
  "commit_subject": "Optimize tensor parallel execution speed (#17)",
  "commit_message": "Optimize tensor parallel execution speed (#17)",
  "commit_date": "2023-04-01T00:51:08+08:00",
  "files_changed": [
    "benchmark/benchmark_latency.py",
    "cacheflow/parallel_utils/tensor_parallel/__init__.py",
    "cacheflow/parallel_utils/tensor_parallel/layers.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 15,
    "num_edited_lines": 390,
    "num_non_test_edited_lines": 390,
    "commit_year": 2023
  },
  "diff_text": "diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\nnew file mode 100644\nindex 000000000..a18ef98f4\n--- /dev/null\n+++ b/benchmark/benchmark_latency.py\n@@ -0,0 +1,99 @@\n+import argparse\n+import time\n+from typing import List\n+\n+from tqdm import tqdm\n+import numpy as np\n+import torch\n+\n+from cacheflow.master.simple_frontend import SimpleFrontend\n+from cacheflow.master.server import (Server, add_server_arguments,\n+                                     initialize_ray_cluster)\n+from cacheflow.sampling_params import SamplingParams\n+from cacheflow.utils import get_gpu_memory, get_cpu_memory\n+\n+\n+def main(args: argparse.Namespace):\n+    # TODO(zhuohan): Support pipeline parallelism.\n+    assert args.pipeline_parallel_size == 1, (\n+        'Pipeline parallelism is not supported yet.')\n+\n+    (num_nodes, num_devices_per_node, distributed_init_method,\n+    all_stage_devices) = (\n+        initialize_ray_cluster(\n+            address='local',\n+            pipeline_parallel_size=args.pipeline_parallel_size,\n+            tensor_parallel_size=args.tensor_parallel_size))\n+\n+    # Create a server.\n+    server = Server(\n+        model=args.model,\n+        model_path=args.model_path,\n+        pipeline_parallel_size=args.pipeline_parallel_size,\n+        tensor_parallel_size=args.tensor_parallel_size,\n+        block_size=args.block_size,\n+        dtype=args.dtype,\n+        seed=args.seed,\n+        swap_space=args.swap_space,\n+        max_batch_size=args.max_batch_size,\n+        num_nodes=num_nodes,\n+        num_devices_per_node=num_devices_per_node,\n+        distributed_init_method=distributed_init_method,\n+        all_stage_devices=all_stage_devices,\n+        gpu_memory=get_gpu_memory(),\n+        cpu_memory=get_cpu_memory(),\n+    )\n+\n+    # Create a frontend.\n+    frontend = SimpleFrontend(\n+        model_name=args.model,\n+        block_size=args.block_size,\n+    )\n+    sampling_params_dict = {\n+        'n': 1,\n+        'temperature': 0.0,\n+        'top_p': 1.0,\n+        'use_beam_search': False,\n+        'stop_token_ids': set(),\n+        'max_num_steps': args.output_len,\n+    }\n+    sampling_params = SamplingParams.from_dict(sampling_params_dict)\n+    input_token_ids = [0] * args.input_len\n+\n+    def profile_step(profile=False):\n+        if profile:\n+            torch.cuda.cudart().cudaProfilerStart()\n+        for _ in range(args.batch_size):\n+            frontend._add_query(input_token_ids, sampling_params)\n+        server.add_sequence_groups(frontend.get_inputs())\n+        start_time = time.time()\n+        while True:\n+            server.step()\n+            if not server.has_unfinished_requests():\n+                break\n+        end_time = time.time()\n+        latency = end_time - start_time\n+        if profile:\n+            torch.cuda.cudart().cudaProfilerStop()\n+        return latency\n+\n+    print(\"Warm up step\")\n+    profile_step()\n+\n+    # Benchmark.\n+    latencies = []\n+    for _ in tqdm(range(3), desc=\"Profile step\"):\n+        latencies.append(profile_step())\n+    print(f'Avg latency: {np.mean(latencies)} seconds')\n+\n+\n+if __name__ == '__main__':\n+    parser = argparse.ArgumentParser(description='CacheFlow simple server.')\n+    parser = add_server_arguments(parser)\n+    parser.add_argument('--input-len', type=int, default=32)\n+    parser.add_argument('--output-len', type=int, default=128)\n+    parser.add_argument('--batch-size', type=int, default=8)\n+    args = parser.parse_args()\n+    args.max_batch_size = max(args.max_batch_size, args.batch_size * args.input_len)\n+    print(args)\n+    main(args)\ndiff --git a/cacheflow/parallel_utils/tensor_parallel/__init__.py b/cacheflow/parallel_utils/tensor_parallel/__init__.py\nindex fba4f9abe..246f5f6f2 100644\n--- a/cacheflow/parallel_utils/tensor_parallel/__init__.py\n+++ b/cacheflow/parallel_utils/tensor_parallel/__init__.py\n@@ -6,8 +6,6 @@ from .layers import (\n     set_defaults_if_not_set_tensor_model_parallel_attributes,\n     copy_tensor_model_parallel_attributes,\n     param_is_not_tensor_parallel_duplicate,\n-    linear_with_grad_accumulation_and_async_allreduce\n-\n )\n \n from .mappings import (\n@@ -39,7 +37,6 @@ __all__ = [\n     \"set_defaults_if_not_set_tensor_model_parallel_attributes\",\n     \"copy_tensor_model_parallel_attributes\",\n     \"param_is_not_tensor_parallel_duplicate\",\n-    \"linear_with_grad_accumulation_and_async_allreduce\",\n     # mappings.py\n     \"copy_to_tensor_model_parallel_region\",\n     \"gather_from_tensor_model_parallel_region\",\ndiff --git a/cacheflow/parallel_utils/tensor_parallel/layers.py b/cacheflow/parallel_utils/tensor_parallel/layers.py\nindex 978ca04e6..f9ba8385c 100644\n--- a/cacheflow/parallel_utils/tensor_parallel/layers.py\n+++ b/cacheflow/parallel_utils/tensor_parallel/layers.py\n@@ -3,10 +3,6 @@\n # Parts of the code here are adapted from PyTorch\n # repo: https://github.com/pytorch/pytorch\n \n-import math\n-import os\n-from typing import Optional\n-import warnings\n \n import torch\n import torch.nn.functional as F\n@@ -16,31 +12,20 @@ from torch.nn.parameter import Parameter\n from cacheflow.parallel_utils.parallel_state import (\n     get_tensor_model_parallel_rank,\n     get_tensor_model_parallel_world_size,\n-    get_tensor_model_parallel_group,\n-    get_global_memory_buffer,\n )\n from .mappings import (\n     copy_to_tensor_model_parallel_region,\n     gather_from_tensor_model_parallel_region,\n-    gather_from_sequence_parallel_region,\n     reduce_from_tensor_model_parallel_region,\n     scatter_to_tensor_model_parallel_region,\n-    reduce_scatter_to_sequence_parallel_region,\n )\n \n from .random import get_cuda_rng_tracker\n from .utils import (\n     divide,\n-    split_tensor_along_last_dim,\n     VocabUtility,\n )\n \n-_grad_accum_fusion_available = True\n-try:\n-    import fused_weight_gradient_mlp_cuda\n-except ImportError:\n-    _grad_accum_fusion_available = False\n-\n _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS = {'tensor_model_parallel': False,\n                                       'partition_dim': -1,\n                                       'partition_stride': 1}\n@@ -216,202 +201,6 @@ class VocabParallelEmbedding(torch.nn.Module):\n         return output\n \n \n-class LinearWithGradAccumulationAndAsyncCommunication(torch.autograd.Function):\n-    \"\"\"See linear_with_grad_accumulation_and_async_allreduce\"\"\"\n-\n-    @staticmethod\n-    def forward(ctx, input, weight, bias, gradient_accumulation_fusion,\n-                async_grad_allreduce, sequence_parallel):\n-        ctx.save_for_backward(input, weight)\n-        ctx.use_bias = bias is not None\n-        ctx.gradient_accumulation_fusion = gradient_accumulation_fusion\n-        ctx.async_grad_allreduce = async_grad_allreduce\n-        ctx.sequence_parallel = sequence_parallel\n-\n-        if sequence_parallel:\n-            world_size = get_tensor_model_parallel_world_size()\n-            dim_size = list(input.size())\n-            dim_size[0] = dim_size[0] * world_size\n-\n-            all_gather_buffer = \\\n-                get_global_memory_buffer().get_tensor(dim_size, input.dtype, \"mpu\")\n-            torch.distributed._all_gather_base(\n-                all_gather_buffer,\n-                input,\n-                group=get_tensor_model_parallel_group())\n-            total_input = all_gather_buffer\n-        else:\n-            total_input = input\n-\n-        output = torch.matmul(total_input, weight.t())\n-        if bias is not None:\n-            output = output + bias\n-        return output\n-\n-    @staticmethod\n-    def backward(ctx, grad_output):\n-        input, weight = ctx.saved_tensors\n-        use_bias = ctx.use_bias\n-\n-        if ctx.sequence_parallel:\n-            world_size = get_tensor_model_parallel_world_size()\n-            dim_size = list(input.size())\n-            dim_size[0] = dim_size[0] * world_size\n-\n-            all_gather_buffer = \\\n-                get_global_memory_buffer().get_tensor(dim_size, input.dtype, \"mpu\")\n-            handle = torch.distributed._all_gather_base(\n-                all_gather_buffer,\n-                input,\n-                group=get_tensor_model_parallel_group(), async_op=True)\n-\n-            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n-            # gather is scheduled before the input gradient computation\n-            total_input = all_gather_buffer\n-        else:\n-            total_input = input\n-        grad_input = grad_output.matmul(weight)\n-\n-        if ctx.sequence_parallel:\n-            handle.wait()\n-\n-        # Convert the tensor shapes to 2D for execution compatibility\n-        grad_output = grad_output.view(grad_output.shape[0] * grad_output.shape[1],\n-                                       grad_output.shape[2])\n-        total_input = total_input.view(total_input.shape[0] * total_input.shape[1],\n-\t\t\t\t       total_input.shape[2])\n-\n-        if ctx.async_grad_allreduce:\n-            # Asynchronous all-reduce\n-            handle = torch.distributed.all_reduce(\n-                    grad_input, group=get_tensor_model_parallel_group(), async_op=True)\n-            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n-            # all-reduce is scheduled before the weight gradient computation\n-\n-        if ctx.sequence_parallel:\n-            assert not ctx.async_grad_allreduce\n-            dim_size = list(input.size())\n-            sub_grad_input = torch.empty(dim_size, dtype=input.dtype,\n-                                         device=torch.cuda.current_device(),\n-                                         requires_grad=False)\n-            # reduce_scatter\n-            handle = torch.distributed._reduce_scatter_base(sub_grad_input, grad_input,\n-                                                            group=get_tensor_model_parallel_group(),\n-                                                            async_op=True)\n-            # Here we rely on CUDA_DEVICE_MAX_CONNECTIONS=1 to ensure that the\n-            # reduce scatter is scheduled before the weight gradient computation\n-\n-\n-        if ctx.gradient_accumulation_fusion:\n-            if weight.main_grad.dtype == torch.float32:\n-                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp32(total_input, grad_output, weight.main_grad)\n-            elif weight.main_grad.dtype == torch.float16:\n-                fused_weight_gradient_mlp_cuda.wgrad_gemm_accum_fp16(total_input, grad_output, weight.main_grad)\n-            else:\n-                raise RuntimeError(\"Unsupported gradient type for gradient accumulation fusion\")\n-            grad_weight = None\n-        else:\n-            grad_weight = grad_output.t().matmul(total_input)\n-        grad_bias = grad_output.sum(dim=0) if use_bias else None\n-\n-        if ctx.sequence_parallel:\n-            handle.wait()\n-            return sub_grad_input, grad_weight, grad_bias, None, None, None\n-\n-        if ctx.async_grad_allreduce:\n-            handle.wait()\n-\n-        return grad_input, grad_weight, grad_bias, None, None, None\n-\n-def linear_with_grad_accumulation_and_async_allreduce(\n-    input: torch.Tensor,\n-    weight: torch.Tensor,\n-    bias: Optional[torch.Tensor],\n-    gradient_accumulation_fusion: bool,\n-    async_grad_allreduce: bool,\n-    sequence_parallel_enabled: bool,\n-) -> torch.Tensor:\n-    \"\"\"Linear layer execution with asynchronous communication and\n-    gradient accumulation fusion in backprop.\n-\n-    This has the option to accumulate the result of backprop\n-    calculation into an existing gradient buffer, preventing the need\n-    to do an additional addition kernel after the gradient\n-    calculation.\n-\n-    Additionally, the tensor parallel all reduce of the input\n-    gradients can be done asynchronously with the calculation of\n-    the weight gradients.\n-\n-    In the case of sequence parallelism, the reduce scatter of the\n-    input gradients is done asynchronously with the calcluation of the\n-    weight gradients.\n-\n-    Use of this module requires that the environment variable\n-    CUDA_DEVICE_MAX_CONNECTIONS=1. There are a few collective\n-    operations, noted in the code, that should be scheduled before\n-    compute kernels to overlap the communication with the computation,\n-    which is necessary for a speedup but not for correctness so that\n-    ordering isn't imposed by the scheduler. Setting\n-    CUDA_DEVICE_MAX_CONNECTIONS=1 forces the kernels to be scheduled\n-    in the order they are called.\n-\n-    Arguments:\n-\n-    input (torch.Tensor required): input like torch.nn.functional.linear\n-\n-    weight (torch.Tensor required): weight like torch.nn.functional.linear\n-\n-    bias (torch.Tensor optional): bias like torch.nn.functional.linear\n-\n-    gradient_accumulation_fusion (bool required): Perform the gradient\n-        accumulation fusion, requires the custom CUDA extension\n-        fused_weight_gradient_mlp_cuda module. To use\n-        gradient_accumulation_fusion you must install APEX with\n-        --cpp_ext and --cuda_ext. For example: \"pip install\n-        --global-option=\\\"--cpp_ext\\\" --global-option=\\\"--cuda_ext .\\\"\n-        \" Note that the extension requires CUDA>=11. Otherwise, you\n-        must turn off gradient accumulation fusion.\"\n-\n-    async_grad_allreduce (bool required): Do the allreduce of input\n-        gradients asyncronously with the computation of weight\n-        gradients. If sequence_parallel_enabled is True, this must be\n-        False, as no all reduce is performed.\n-\n-    sequence_parallel_enabled (bool required): Indicates that sequence\n-        parallelism is used and thus in the forward pass the input is\n-        all gathered, and the backward pass the input gradients are\n-        reduce scattered.\n-    \"\"\"\n-    args = [\n-        input,\n-        weight,\n-        bias,\n-        gradient_accumulation_fusion,\n-        async_grad_allreduce,\n-        sequence_parallel_enabled,\n-    ]\n-\n-    if not linear_with_grad_accumulation_and_async_allreduce.warned:\n-        if os.environ.get('CUDA_DEVICE_MAX_CONNECTIONS') != \"1\":\n-            if sequence_parallel_enabled:\n-                warnings.warn(\n-                    \"When using sequence parallelism it is recommended to set the \"\n-                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n-                    \"maximum speedup\")\n-                linear_with_grad_accumulation_and_async_allreduce.warned = True\n-\n-            if async_grad_allreduce:\n-                warnings.warn(\n-                    \"When using async grad allreduce it is recommended to set the \"\n-                    \"environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for \"\n-                    \"maximum speedup\")\n-                linear_with_grad_accumulation_and_async_allreduce.warned = True\n-\n-    with torch.cuda.amp.autocast(enabled=False):\n-        return LinearWithGradAccumulationAndAsyncCommunication.apply(*args)\n-linear_with_grad_accumulation_and_async_allreduce.warned = False\n-\n class ColumnParallelLinear(torch.nn.Module):\n     \"\"\"Linear layer with column parallelism.\n \n@@ -436,11 +225,8 @@ class ColumnParallelLinear(torch.nn.Module):\n         skip_bias_add: This was added to enable performance optimations where bias\n                        can be fused with other elementwise operations. we skip\n                        adding bias but instead return it.\n-        async_tensor_model_parallel_allreduce:\n         params_dtype:\n         use_cpu_initialization:\n-        gradient_accumulation_fusion:\n-        sequence_parallel_enabled:\n     \"\"\"\n \n     def __init__(self, input_size, output_size, *,\n@@ -448,12 +234,9 @@ class ColumnParallelLinear(torch.nn.Module):\n                  init_method=init.xavier_normal_, stride=1,\n                  keep_master_weight_for_test=False,\n                  skip_bias_add=False,\n-                 async_tensor_model_parallel_allreduce=True,\n                  params_dtype=None,\n                  use_cpu_initialization=False,\n                  perform_initialization=True,\n-                 gradient_accumulation_fusion=False,\n-                 sequence_parallel_enabled: bool = False,\n                  ):\n         super(ColumnParallelLinear, self).__init__()\n \n@@ -506,37 +289,6 @@ class ColumnParallelLinear(torch.nn.Module):\n         else:\n             self.register_parameter('bias', None)\n \n-        self.async_tensor_model_parallel_allreduce = (\n-                async_tensor_model_parallel_allreduce and\n-                world_size > 1)\n-        if sequence_parallel_enabled:\n-            if world_size <= 1:\n-                warnings.warn(\n-                    f\"`sequence_parallel_enabled` is set to `True`, but tensor model parallel size is {world_size}. \"\n-                    f\"Disabling sequence parallel.\"\n-                )\n-                sequence_parallel_enabled = False\n-        self.sequence_parallel_enabled = sequence_parallel_enabled\n-\n-        if gradient_accumulation_fusion:\n-            if not _grad_accum_fusion_available:\n-                raise RuntimeError(\n-                    \"ColumnParallelLinear was called with gradient_accumulation_fusion set \"\n-                    \"to True but the custom CUDA extension fused_weight_gradient_mlp_cuda \"\n-                    \"module is not found. To use gradient_accumulation_fusion you must \"\n-                    \"install APEX with --cpp_ext and --cuda_ext. For example: \"\n-                    \"pip install --global-option=\\\"--cpp_ext\\\" --global-option=\\\"--cuda_ext .\\\" \"\n-                    \"Note that the extension requires CUDA>=11. Otherwise, you must turn off \"\n-                    \"gradient accumulation fusion.\"\n-                )\n-        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n-\n-        if self.async_tensor_model_parallel_allreduce and self.sequence_parallel_enabled:\n-            raise RuntimeError(\n-                \"`async_tensor_model_parallel_allreduce` and `sequence_parallel_enabled` \"\n-                \"cannot be enabled at the same time.\"\n-            )\n-\n \n     def forward(self, input_):\n         \"\"\"Forward of ColumnParallelLinear\n@@ -550,23 +302,11 @@ class ColumnParallelLinear(torch.nn.Module):\n         \"\"\"\n         bias = self.bias if not self.skip_bias_add else None\n \n-        if self.async_tensor_model_parallel_allreduce or \\\n-                self.sequence_parallel_enabled:\n-            input_parallel = input_\n-        else:\n-            input_parallel = copy_to_tensor_model_parallel_region(input_)\n+        input_parallel = copy_to_tensor_model_parallel_region(input_)\n         # Matrix multiply.\n-        output_parallel = linear_with_grad_accumulation_and_async_allreduce(\n-            input=input_parallel,\n-            weight=self.weight,\n-            bias=bias,\n-            gradient_accumulation_fusion=self.gradient_accumulation_fusion,\n-            async_grad_allreduce=self.async_tensor_model_parallel_allreduce,\n-            sequence_parallel_enabled=self.sequence_parallel_enabled,\n-        )\n+        output_parallel = F.linear(input_parallel, self.weight, bias)\n         if self.gather_output:\n             # All-gather across the partitions.\n-            assert not self.sequence_parallel_enabled\n             output = gather_from_tensor_model_parallel_region(output_parallel)\n         else:\n             output = output_parallel\n@@ -607,8 +347,6 @@ class RowParallelLinear(torch.nn.Module):\n         params_dtype:\n         use_cpu_initialization:\n         perform_initialization:\n-        gradient_accumulation_fusion:\n-        sequence_parallel_enabled:\n     \"\"\"\n \n     def __init__(self, input_size, output_size, *,\n@@ -619,8 +357,6 @@ class RowParallelLinear(torch.nn.Module):\n                  params_dtype=None,\n                  use_cpu_initialization=False,\n                  perform_initialization=True,\n-                 gradient_accumulation_fusion=False,\n-                 sequence_parallel_enabled: bool = False,\n                  ):\n         super(RowParallelLinear, self).__init__()\n \n@@ -635,10 +371,6 @@ class RowParallelLinear(torch.nn.Module):\n         world_size = get_tensor_model_parallel_world_size()\n         self.input_size_per_partition = divide(input_size, world_size)\n         self.skip_bias_add = skip_bias_add\n-        self.gradient_accumulation_fusion = gradient_accumulation_fusion\n-        self.sequence_parallel_enabled = sequence_parallel_enabled\n-        if self.sequence_parallel_enabled and not self.input_is_parallel:\n-            raise RuntimeError(\"To enable `sequence_parallel_enabled`, `input_is_parallel` must be `True`\")\n \n         # Parameters.\n         # Note: torch.nn.functional.linear performs XA^T + b and as a result\n@@ -669,7 +401,6 @@ class RowParallelLinear(torch.nn.Module):\n                 self.bias = Parameter(torch.empty(\n                     self.output_size, device=torch.cuda.current_device(),\n                     dtype=params_dtype))\n-            setattr(self.bias, 'sequence_parallel', sequence_parallel_enabled)\n \n             # Always initialize bias to zero.\n             with torch.no_grad():\n@@ -693,23 +424,12 @@ class RowParallelLinear(torch.nn.Module):\n         if self.input_is_parallel:\n             input_parallel = input_\n         else:\n-            assert not self.sequence_parallel_enabled\n             input_parallel = scatter_to_tensor_model_parallel_region(input_)\n         # Matrix multiply.\n-        output_parallel = linear_with_grad_accumulation_and_async_allreduce(\n-            input=input_parallel,\n-            weight=self.weight,\n-            bias=None,\n-            gradient_accumulation_fusion=self.gradient_accumulation_fusion,\n-            async_grad_allreduce=False,\n-            sequence_parallel_enabled=False,\n-        )\n+        output_parallel = F.linear(input_parallel, self.weight)\n \n         # All-reduce across all the partitions.\n-        if self.sequence_parallel_enabled:\n-            output_ = reduce_scatter_to_sequence_parallel_region(output_parallel)\n-        else:\n-            output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n+        output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n         if not self.skip_bias_add:\n             output = output_ + self.bias if self.bias is not None else output_\n             output_bias = None",
  "apis": [
    "Server",
    "SimpleFrontend",
    "ColumnParallelLinear.forward",
    "RowParallelLinear.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/adapter_commons/layers.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/layers.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/benchmark_latency.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit makes non-trivial modifications to production code (e.g., changes in tensor parallel implementations) and adds a new benchmark tool, showing that its purpose is to optimize the performance of tensor parallel execution. The commit removes custom asynchronous gradient accumulation fusion and related all-reduce operations in favor of a simpler, probably faster F.linear implementation. These changes target CPU-tested tensor parallel execution speed improvements and are not just refactoring or bug fixes. Therefore, it satisfies the conditions for a performance or optimization related commit.",
  "llm_api_reason": "This commit introduces a new benchmark script that directly calls cacheflow’s Server and SimpleFrontend classes, and it refactors tensor-parallel linear layers by removing the custom asynchronous gradient accumulation function. In particular, the ColumnParallelLinear and RowParallelLinear forward methods have been updated to use a more straightforward implementation (calling F.linear directly), which affects their public Python APIs. Overall, the changes impact high-level APIs exposed by the server/front-end and the tensor-parallel layers."
}