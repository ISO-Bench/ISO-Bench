{
  "commit_hash": "0d243f2a54fbd1c56da8a571f0899c30b6aba5d9",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13577",
  "pr_date": "2025-02-20",
  "timeline_text": "Copy link Contributor divakar-amd commented Feb 20, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Found better configs when comparing with rocm fork. The PR serves 2 purposes: Update with better config setting Maintain same configs b/w upstream and rocm fork Offline-latency numbers (sec) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions update mixtral8x7B specific moe config bs perf ‚Ä¶ 44dd275 Signed-off-by: Divakar Verma <divakar.verma@amd.com> Copy link github-actions bot commented Feb 20, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . divakar-amd mentioned this pull request Feb 20, 2025 resolve configs diff for mixtral8x7B ROCm/vllm#437 Merged DarkLight1337 approved these changes Feb 20, 2025 View reviewed changes DarkLight1337 enabled auto-merge (squash) February 20, 2025 02:20 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Feb 20, 2025 Hide details View details DarkLight1337 merged commit 0d243f2 into vllm-project : main Feb 20, 2025 61 checks passed Uh oh! There was an error while loading. Please reload this page . xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Feb 20, 2025 [ROCm][MoE] mi300 mixtral8x7B perf for specific BS ( vllm-project#13577 ) ‚Ä¶ 1d993c1 Signed-off-by: Divakar Verma <divakar.verma@amd.com> Akshat-Tripathi pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Mar 3, 2025 [ROCm][MoE] mi300 mixtral8x7B perf for specific BS ( vllm-project#13577 ) ‚Ä¶ f684038 Signed-off-by: Divakar Verma <divakar.verma@amd.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [ROCm][MoE] mi300 mixtral8x7B perf for specific BS ( vllm-project#13577 ) ‚Ä¶ 2749bea Signed-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [ROCm][MoE] mi300 mixtral8x7B perf for specific BS ( vllm-project#13577 ) ‚Ä¶ 439c0ce Signed-off-by: Divakar Verma <divakar.verma@amd.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:30",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:52:30",
  "models": [
    "mistralai/Mixtral-8x7B-Instruct-v0.1"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=mistralai/Mixtral-8x7B-Instruct-v0.1 --tasks gsm8k --batch_size auto"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mixtral-8x7B-Instruct-v0.1",
  "commit_subject": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS (#13577)",
  "commit_message": "[ROCm][MoE] mi300 mixtral8x7B perf for specific BS (#13577)\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>",
  "commit_date": "2025-02-20T04:01:02Z",
  "files_changed": [
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json",
    "vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 3,
    "num_edited_lines": 10,
    "num_non_test_edited_lines": 10,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\nindex 66f9106bd..4bf775347 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=14336,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 16,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 64,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\nindex ed5b655d8..5a3f415d5 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=3584,device_name=AMD_Instinct_MI300X.json\n@@ -45,8 +45,8 @@\n     },\n     \"16\": {\n         \"BLOCK_SIZE_M\": 16,\n-        \"BLOCK_SIZE_N\": 32,\n-        \"BLOCK_SIZE_K\": 256,\n+        \"BLOCK_SIZE_N\": 64,\n+        \"BLOCK_SIZE_K\": 128,\n         \"GROUP_SIZE_M\": 1,\n         \"num_warps\": 2,\n         \"num_stages\": 2,\ndiff --git a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\nindex 822f04e33..8d7b78027 100644\n--- a/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n+++ b/vllm/model_executor/layers/fused_moe/configs/E=8,N=7168,device_name=AMD_Instinct_MI300X.json\n@@ -128,7 +128,7 @@\n         \"num_warps\": 8,\n         \"num_stages\": 2,\n         \"waves_per_eu\": 0,\n-        \"matrix_instr_nonkdim\": 32,\n+        \"matrix_instr_nonkdim\": 16,\n         \"kpack\": 2\n     },\n     \"512\": {",
  "apis": [
    "vllm.fused_moe",
    "vllm.model_executor.layers.fused_moe.get_moe_configs",
    "vllm.model_executor.layers.fused_moe.invoke_fused_moe_kernel"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/rocm_aiter_fused_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/config.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test configuration files, changing parameters like BLOCK_SIZE_N, BLOCK_SIZE_K, and matrix_instr_nonkdim in MoE layers. These changes adjust configuration values that directly influence the performance of the model executor on AMD hardware. The commit message and the parameter tuning indicate an intent to improve performance, rather than merely refactoring code, fixing bugs, or updating documentation. It directly targets performance optimization through effective parameter choices for specific hardware.",
  "llm_api_reason": "This commit only changes configuration JSON files for fused MoE kernels (found in the vllm/model_executor/layers/fused_moe/configs directory). These files contain tuning parameters (e.g. BLOCK_SIZE_N, BLOCK_SIZE_K, and matrix_instr_nonkdim) specifically adjusted for AMD MI300 models. Although no python code functions are directly modified, these configuration values affect the behavior of the fused MoE kernel when read and used by functions such as get_moe_configs, which in turn are used in fused_moe kernel invocations (for instance, via invoke_fused_moe_kernel and the top‚Äêlevel fused_moe API). Therefore, the affected high‚Äêlevel APIs are those that load and apply these configuration parameters for fused MoE execution."
}