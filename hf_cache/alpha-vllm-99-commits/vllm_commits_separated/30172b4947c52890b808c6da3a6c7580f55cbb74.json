{
  "commit_hash": "30172b4947c52890b808c6da3a6c7580f55cbb74",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13244",
  "pr_date": "2025-02-18",
  "timeline_text": "Copy link Member njhill commented Feb 13, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Move the current SamplingMetadata object to a field in the persistent batch, updated only when the batch changes rather than constructed every step Keep input_batch.req_ids sized to the number of requests in the batch, so that anywhere that iterates over it doesn't need to slice (copy) the list or keep track of the separate request count. It is still updated in-place Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction njhill requested review from WoosukKwon , robertgshaw2-redhat , ywang96 , comaniac and alexm-redhat as code owners February 13, 2025 23:29 Copy link github-actions bot commented Feb 13, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Feb 13, 2025 njhill commented Feb 13, 2025 View reviewed changes vllm/v1/worker/gpu_input_batch.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Feb 14, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @njhill . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Feb 14, 2025 [V1] Optimize handling of sampling metadata and req_ids list ‚Ä¶ 7d6ee8f - Move SamplingMetadata to a field in the persistent batch, updated only when the batch changes rather than constructed every step\n- Keep input_batch.req_ids sized to the number of requests in the batch, so that anywhere that iterates over it doesn't need to slice (copy) the list or keep track of the separate request count. It is still updated in-place\n\nSigned-off-by: Nick Hill <nhill@redhat.com> njhill force-pushed the sampler-streamline branch\n    from 2bcf20f to 7d6ee8f Compare February 14, 2025 16:27 mergify bot removed\n  the needs-rebase label Feb 14, 2025 Copy link Member Author njhill commented Feb 14, 2025 @WoosukKwon this is the first step, I am working on follow-on simplification for the penalty parameters, etc. üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon self-assigned this Feb 14, 2025 njhill added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Feb 14, 2025 Copy link Member Author njhill commented Feb 14, 2025 @WoosukKwon apologies, I am looking into the test failure. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . don't mutate \"constant\" sampling metadata tensors ‚Ä¶ 37d1f98 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Feb 14, 2025 @WoosukKwon the test failure should be fixed now... the shared apply penalties code was doing in-place unsqueezes on the sampling penalty tensors - which I think is a bad thing to do but didn't cause a problem before because we were passing new slices every step. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Feb 14, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @njhill . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Feb 14, 2025 Merge remote-tracking branch 'origin/main' into sampler-streamline ‚Ä¶ f354b07 # Conflicts:\n#\tvllm/v1/worker/gpu_input_batch.py mergify bot removed\n  the needs-rebase label Feb 15, 2025 Copy link Collaborator WoosukKwon commented Feb 15, 2025 Hi @njhill , do you mind if we merge #12193 first and review this PR? I'd like to prioritize the spec decode PR as it already got rebased many many times. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author njhill commented Feb 15, 2025 @WoosukKwon that's fine with me. ‚ù§Ô∏è 1 WoosukKwon reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . njhill added 4 commits February 14, 2025 21:49 simplify sampling metadata ‚Ä¶ 602d3b6 Signed-off-by: Nick Hill <nhill@redhat.com> Merge remote-tracking branch 'refs/remotes/origin/main' into sampler-‚Ä¶ ‚Ä¶ 80eae4e ‚Ä¶streamline\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n# Conflicts:\n#\ttests/v1/worker/test_gpu_input_batch.py\n#\tvllm/v1/sample/sampler.py group stop_token_ids with min_tokens ‚Ä¶ 57cd611 Signed-off-by: Nick Hill <nhill@redhat.com> test updates ‚Ä¶ c7e2bfd Signed-off-by: Nick Hill <nhill@redhat.com> Copy link mergify bot commented Feb 16, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @njhill . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . 5 hidden items Load more‚Ä¶ mergify bot removed\n  the needs-rebase label Feb 18, 2025 Some more small list/tuple optimizations; fix linting ‚Ä¶ d246ce5 Signed-off-by: Nick Hill <nhill@redhat.com> njhill commented Feb 18, 2025 View reviewed changes vllm/v1/request.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/core/scheduler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Member Author njhill commented Feb 18, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @WoosukKwon I have now rebased. #13360 partially overlaps with this (e,g. I simplified some of the min_tokens handling in this one but have refactored completely in the other one based on the new abstraction). But I think it would be fine to get this in first and I can rebase the other one if you're ok with that. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Small adjustment ‚Ä¶ 5e216c7 Signed-off-by: Nick Hill <nhill@redhat.com> njhill commented Feb 18, 2025 View reviewed changes vllm/v1/worker/gpu_model_runner.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . njhill commented Feb 18, 2025 View reviewed changes vllm/v1/worker/gpu_input_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator WoosukKwon commented Feb 18, 2025 @njhill I'm not sure it's worthwhile to change from [] to () . I did a microbenchmark: N = 1024 x = [] # List start = time . perf_counter () for i in range ( N ): x . append ([]) end = time . perf_counter () print ( f\"list: { ( end - start ) * 1000 :.3f } ms\" ) y = [] # Tuple start = time . perf_counter () for i in range ( N ): y . append (()) end = time . perf_counter () print ( f\"tuple: { ( end - start ) * 1000 :.3f } ms\" ) I find that adding 1024 (maximum number of requests in the batch) empty lists only takes 80-90 us. While using tuple reduces this time to 30-40 us, I think the 50 us gap (in the worst case) cannot justify the extra complexity here. When the batch size is 32, the gap becomes even smaller (7 us vs 2 us). WDYT? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Fix rejection sampler test ‚Ä¶ b2a43ba Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Feb 18, 2025 @WoosukKwon I agree it's not worth any extra complexity. Just might as well use () where it doesn't otherwise make any difference to the code. Let me check and revert where such changes were made.. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator WoosukKwon commented Feb 18, 2025 @njhill I think changing List to Sequence itself is increasing complexity? After that, we need to consider whether it's a tuple or list. I'd prefer to keep using List and [] if the performance is the only concern. üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Revert change related to list vs tuple ‚Ä¶ 2fbc6e1 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Feb 18, 2025 @WoosukKwon sure, let me revert those too. I think mostly we don't need to consider the tuple/list difference because these are args or fields that would be considered read-only. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Revert List->Sequence changes ‚Ä¶ 1b68e03 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Member Author njhill commented Feb 18, 2025 @WoosukKwon I need to fix up some of the gpu_model_runner tests, but I'll wait for your first review to make sure you are good with the changes overall before spending time on that. ‚ù§Ô∏è 1 WoosukKwon reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon reviewed Feb 18, 2025 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Amazing. Looks much cleaner! üòÑ Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/v1/worker/gpu_model_runner.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/core/scheduler.py Comment on lines +198 to +200 del request.spec_token_ids[num_scheduled_spec_tokens:] scheduled_spec_decode_tokens[request.request_id] = ( request.spec_token_ids [:num_scheduled_spec_tokens] ) request.spec_token_ids) Copy link Collaborator WoosukKwon Feb 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment What is this change for? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author njhill Feb 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It avoids creating a new list, just trims the existing one down to num_scheduled_spec_tokens , since any later spec token ids are essentially discarded anyhow. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Collaborator WoosukKwon Feb 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Got it! Maybe worth a comment. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction vllm/v1/sample/metadata.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/worker/gpu_input_batch.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/worker/gpu_input_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/worker/gpu_input_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/worker/gpu_input_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . njhill added 2 commits February 18, 2025 07:51 Address review comments ‚Ä¶ 28a17ae Signed-off-by: Nick Hill <nhill@redhat.com> Fix up gpu_model_runner tests ‚Ä¶ 9250721 Signed-off-by: Nick Hill <nhill@redhat.com> WoosukKwon approved these changes Feb 18, 2025 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Very nice simplification! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Add comment ‚Ä¶ ce3c3f4 Signed-off-by: Nick Hill <nhill@redhat.com> Hide details View details njhill merged commit 30172b4 into vllm-project : main Feb 18, 2025 44 checks passed Uh oh! There was an error while loading. Please reload this page . njhill deleted the sampler-streamline branch February 18, 2025 20:15 xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Feb 20, 2025 [V1] Optimize handling of sampling metadata and req_ids list ( vllm-pr‚Ä¶ ‚Ä¶ d54a1e9 ‚Ä¶oject#13244 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com> Akshat-Tripathi pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Mar 3, 2025 [V1] Optimize handling of sampling metadata and req_ids list ( vllm-pr‚Ä¶ ‚Ä¶ d9b7062 ‚Ä¶oject#13244 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1] Optimize handling of sampling metadata and req_ids list ( vllm-pr‚Ä¶ ‚Ä¶ be846f4 ‚Ä¶oject#13244 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [V1] Optimize handling of sampling metadata and req_ids list ( vllm-pr‚Ä¶ ‚Ä¶ ff9b783 ‚Ä¶oject#13244 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:38",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:52:38",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)",
  "commit_message": "[V1] Optimize handling of sampling metadata and req_ids list (#13244)\n\nSigned-off-by: Nick Hill <nhill@redhat.com>",
  "commit_date": "2025-02-18T12:15:33-08:00",
  "files_changed": [
    "tests/v1/sample/test_rejection_sampler.py",
    "tests/v1/sample/test_sampler.py",
    "tests/v1/worker/test_gpu_input_batch.py",
    "tests/v1/worker/test_gpu_model_runner.py",
    "vllm/model_executor/layers/utils.py",
    "vllm/v1/core/scheduler.py",
    "vllm/v1/sample/metadata.py",
    "vllm/v1/sample/ops/penalties.py",
    "vllm/v1/sample/ops/topk_topp_sampler.py",
    "vllm/v1/sample/rejection_sampler.py",
    "vllm/v1/sample/sampler.py",
    "vllm/v1/utils.py",
    "vllm/v1/worker/gpu_input_batch.py",
    "vllm/v1/worker/gpu_model_runner.py",
    "vllm/v1/worker/tpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 4,
    "num_non_test_files": 11,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 15,
    "num_hunks": 74,
    "num_edited_lines": 553,
    "num_non_test_edited_lines": 420,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 8bc33e841..3e810e525 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -26,17 +26,13 @@ def create_logits_tensor(token_ids: List[int],\n def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n     batch_size = len(spec_tokens)\n     return SamplingMetadata(\n-        temperature=0.0,\n+        temperature=torch.tensor([]),\n         all_greedy=True,\n         all_random=False,\n-        rejection_sampling=True,\n         spec_token_ids=spec_tokens,\n         top_p=None,\n         top_k=None,\n-        no_top_p=False,\n-        no_top_k=False,\n         min_p=torch.empty(batch_size, ),\n-        no_min_p=True,\n         generators={},\n         max_num_logprobs=0,\n         no_penalties=False,\n@@ -45,8 +41,7 @@ def create_sampling_metadata(spec_tokens: List[List[int]]) -> SamplingMetadata:\n         presence_penalties=torch.tensor([]),\n         repetition_penalties=torch.tensor([]),\n         output_token_ids=[],\n-        min_tokens=[],\n-        stop_token_ids=[],\n+        min_tokens={},\n         logit_bias=[None] * batch_size,\n     )\n \ndiff --git a/tests/v1/sample/test_sampler.py b/tests/v1/sample/test_sampler.py\nindex a4bd651f8..3f6301c54 100644\n--- a/tests/v1/sample/test_sampler.py\n+++ b/tests/v1/sample/test_sampler.py\n@@ -77,25 +77,20 @@ def _create_default_sampling_metadata(\n         temperature=torch.full((batch_size, ), 0.0),\n         all_greedy=True,\n         all_random=False,\n-        rejection_sampling=False,\n-        top_p=torch.empty(batch_size, ),\n-        top_k=torch.empty(batch_size, ),\n-        no_top_p=True,\n-        no_top_k=True,\n-        min_p=torch.empty(batch_size, ),\n-        no_min_p=True,\n+        top_p=None,\n+        top_k=None,\n+        min_p=None,\n         generators={},\n         max_num_logprobs=0,\n         prompt_token_ids=_create_prompt_tokens_tensor(prompt_token_ids,\n                                                       vocab_size, device),\n         output_token_ids=output_token_ids,\n-        spec_token_ids=[],\n+        spec_token_ids=None,\n         frequency_penalties=_create_penalty_tensor(batch_size, 0.0, device),\n         presence_penalties=_create_penalty_tensor(batch_size, 0.0, device),\n         repetition_penalties=_create_penalty_tensor(batch_size, 1.0, device),\n         no_penalties=True,\n-        min_tokens=[],\n-        stop_token_ids=[],\n+        min_tokens={},\n         logit_bias=[None] * batch_size,\n     )\n     return fake_sampling_metadata\n@@ -104,10 +99,10 @@ def _create_default_sampling_metadata(\n def _generate_min_token_penalties_and_stop_tokens(\n     num_output_tokens: int, batch_size: int, vocab_size: int,\n     batch_indices_for_min_token_penalty: List[int]\n-) -> Tuple[List[int], List[Set[int]]]:\n+) -> Dict[int, Tuple[int, Set[int]]]:\n     \"\"\"\n-    Generates and returns a list of minimum token penalties (`min_tokens`)\n-    and a corresponding list of stop token IDs (`stop_token_ids`) for each\n+    Generates and returns a dict of minimum token penalties and\n+    corresponding stop token IDs (`min_tokens`, `stop_token_ids`) for each\n     batch.\n \n     If a batch index is included in `batch_indices_for_min_token_penalty`,\n@@ -115,22 +110,19 @@ def _generate_min_token_penalties_and_stop_tokens(\n     and a random set of stop token IDs is created. Otherwise, a lower\n     `min_tokens` value is assigned, and the stop token IDs set is empty.\n     \"\"\"\n-    stop_token_ids: List[Set[int]] = []\n-    min_tokens: List[int] = []\n+    min_tokens: Dict[int, Tuple[int, Set[int]]] = {}\n     for index in range(batch_size):\n         if index in batch_indices_for_min_token_penalty:\n-            min_tokens.append(\n+            min_tokens[index] = (\n                 np.random.randint(num_output_tokens + 1,\n-                                  2 * num_output_tokens))\n-            stop_token_ids.append(\n+                                  2 * num_output_tokens),\n                 set(\n                     np.random.randint(0, vocab_size - 1)\n                     for _ in range(np.random.randint(0, vocab_size))))\n-\n         else:\n-            min_tokens.append(np.random.randint(0, num_output_tokens))\n-            stop_token_ids.append(set())\n-    return (min_tokens, stop_token_ids)\n+            min_tokens[index] = (np.random.randint(0,\n+                                                   num_output_tokens), set())\n+    return min_tokens\n \n \n def _create_weighted_output_token_list(\n@@ -165,7 +157,7 @@ def _create_weighted_output_token_list(\n             output_token_ids_for_batch.extend(\n                 [token_id for _ in range(index + 1)])\n         output_token_ids.append(output_token_ids_for_batch)\n-    return (output_token_ids, sorted_token_ids_in_output)\n+    return output_token_ids, sorted_token_ids_in_output\n \n \n @pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n@@ -182,17 +174,17 @@ def test_sampler_min_tokens_penalty(device: str, batch_size: int):\n         NUM_OUTPUT_TOKENS, batch_size, VOCAB_SIZE, torch.device(device))\n     batch_indices_for_min_token_penalty = np.random.randint(\n         0, batch_size - 1, size=np.random.randint(0, batch_size)).tolist()\n-    min_tokens, stop_token_ids = _generate_min_token_penalties_and_stop_tokens(\n+    min_tokens = _generate_min_token_penalties_and_stop_tokens(\n         NUM_OUTPUT_TOKENS, batch_size, VOCAB_SIZE,\n         batch_indices_for_min_token_penalty)\n     sampling_metadata.min_tokens = min_tokens\n-    sampling_metadata.stop_token_ids = stop_token_ids\n     sampler = Sampler()\n     logits = sampler.apply_penalties(fake_logits, sampling_metadata)\n     logits = logits.cpu()\n     for batch_idx in range(batch_size):\n         for token_id in range(VOCAB_SIZE):\n-            if token_id in stop_token_ids[batch_idx]:\n+            _, stop_token_ids = min_tokens.get(batch_idx, (0, set()))\n+            if token_id in stop_token_ids:\n                 assert logits[batch_idx][token_id] == -float(\"inf\")\n             else:\n                 assert logits[batch_idx][token_id] != -float(\"inf\")\ndiff --git a/tests/v1/worker/test_gpu_input_batch.py b/tests/v1/worker/test_gpu_input_batch.py\nindex c0ab356f5..cb3b3d21f 100644\n--- a/tests/v1/worker/test_gpu_input_batch.py\n+++ b/tests/v1/worker/test_gpu_input_batch.py\n@@ -1,6 +1,6 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-from typing import Dict, List, Set, Tuple\n+from typing import Dict, List, Optional, Set, Tuple\n \n import numpy as np\n import pytest\n@@ -41,7 +41,7 @@ def _remove_requests(\n     for index in req_indices_to_remove:\n         input_batch.remove_request(reqs[index].req_id)\n         req_ids_to_remove.add(reqs[index].req_id)\n-    return (req_ids_to_remove, req_indices_to_remove_list)\n+    return req_ids_to_remove, req_indices_to_remove_list\n \n \n def _construct_expected_sampling_metadata(\n@@ -64,8 +64,7 @@ def _construct_expected_sampling_metadata(\n     top_p = [0.0 for _ in range(num_reqs)]\n     min_p = [0.0 for _ in range(num_reqs)]\n     temperature = [0.0 for _ in range(num_reqs)]\n-    stop_token_ids: List[Set[int]] = [set() for _ in range(num_reqs)]\n-    min_tokens = [0 for _ in range(num_reqs)]\n+    min_tokens = {}\n     logit_bias = [None] * num_reqs\n     for req in reqs:\n         if req.req_id not in req_ids_retained:\n@@ -83,22 +82,21 @@ def _construct_expected_sampling_metadata(\n         top_p[index_in_input_batch] = req.sampling_params.top_p\n         min_p[index_in_input_batch] = req.sampling_params.min_p\n         temperature[index_in_input_batch] = req.sampling_params.temperature\n-        stop_token_ids[\n-            index_in_input_batch] = req.sampling_params.all_stop_token_ids\n-        min_tokens[index_in_input_batch] = req.sampling_params.min_tokens\n+        min_tokens[index_in_input_batch] = (\n+            req.sampling_params.min_tokens,\n+            req.sampling_params.all_stop_token_ids)\n         logit_bias[index_in_input_batch] = req.sampling_params.logit_bias\n     return SamplingMetadata(\n         temperature=torch.tensor(temperature, dtype=torch.float,\n                                  device=device),\n         all_greedy=False,\n         all_random=True,\n-        rejection_sampling=False,\n-        top_p=torch.tensor(top_p, dtype=torch.float, device=device),\n-        top_k=torch.tensor(top_k, dtype=torch.int, device=device),\n-        no_top_p=all(x == 1.0 for x in top_p),\n-        no_top_k=all(x == 0 for x in top_k),\n-        min_p=torch.tensor(min_p, dtype=torch.float, device=device),\n-        no_min_p=all(x == 0.0 for x in min_p),\n+        top_p=None if all(x == 1.0 for x in top_p) else torch.tensor(\n+            top_p, dtype=torch.float, device=device),\n+        top_k=None if all(x == 0 for x in top_k) else torch.tensor(\n+            top_k, dtype=torch.int, device=device),\n+        min_p=None if all(x == 0.0 for x in min_p) else torch.tensor(\n+            min_p, dtype=torch.float, device=device),\n         generators={},\n         max_num_logprobs=0,\n         prompt_token_ids=make_tensor_with_pad(\n@@ -117,9 +115,8 @@ def _construct_expected_sampling_metadata(\n                                           dtype=torch.float,\n                                           device=device),\n         output_token_ids=output_token_ids,\n-        spec_token_ids=[],\n+        spec_token_ids=None,\n         min_tokens=min_tokens,\n-        stop_token_ids=stop_token_ids,\n         no_penalties=(all(x == 0 for x in presence_penalties)\n                       and all(x == 0 for x in frequency_penalties)\n                       and all(x == 1 for x in repetition_penalties)),\n@@ -206,8 +203,7 @@ def test_sampling_metadata_in_input_batch(device: str, batch_size: int):\n     input_batch.condense(req_indices_to_remove)\n \n     # Generate the sampling metadata\n-    sampling_metadata = input_batch.make_sampling_metadata(\n-        req_id_output_token_ids, req_id_to_spec_token_ids={}, skip_copy=False)\n+    sampling_metadata = input_batch._make_sampling_metadata()\n \n     # Create expected output.\n     expected_sampling_metadata = _construct_expected_sampling_metadata(\n@@ -216,13 +212,16 @@ def test_sampling_metadata_in_input_batch(device: str, batch_size: int):\n         input_batch.req_id_to_index,\n         device=torch.device(device))\n \n+    def same(t1: Optional[torch.Tensor], t2: Optional[torch.Tensor]) -> bool:\n+        return (t1 is None\n+                and t2 is None) or (t1 is not None and t2 is not None\n+                                    and torch.allclose(t1, t2))\n+\n     # Assert the actual and expected output.\n     assert torch.allclose(expected_sampling_metadata.temperature,\n                           sampling_metadata.temperature)\n-    assert torch.allclose(expected_sampling_metadata.top_p,\n-                          sampling_metadata.top_p)\n-    assert torch.allclose(expected_sampling_metadata.top_k,\n-                          sampling_metadata.top_k)\n+    assert same(expected_sampling_metadata.top_p, sampling_metadata.top_p)\n+    assert same(expected_sampling_metadata.top_k, sampling_metadata.top_k)\n     assert torch.allclose(\n         expected_sampling_metadata.frequency_penalties,\n         sampling_metadata.frequency_penalties,\n@@ -240,10 +239,6 @@ def test_sampling_metadata_in_input_batch(device: str, batch_size: int):\n     assert (expected_sampling_metadata.output_token_ids ==\n             sampling_metadata.output_token_ids)\n     assert expected_sampling_metadata.min_tokens == sampling_metadata.min_tokens\n-    assert expected_sampling_metadata.stop_token_ids == \\\n-           sampling_metadata.stop_token_ids\n     assert expected_sampling_metadata.no_penalties == \\\n            sampling_metadata.no_penalties\n-    assert expected_sampling_metadata.no_top_p == sampling_metadata.no_top_p\n-    assert expected_sampling_metadata.no_top_k == sampling_metadata.no_top_k\n     assert expected_sampling_metadata.logit_bias == sampling_metadata.logit_bias\ndiff --git a/tests/v1/worker/test_gpu_model_runner.py b/tests/v1/worker/test_gpu_model_runner.py\nindex c655b0fde..973efcbf8 100644\n--- a/tests/v1/worker/test_gpu_model_runner.py\n+++ b/tests/v1/worker/test_gpu_model_runner.py\n@@ -5,6 +5,7 @@ from vllm.config import CacheConfig, ModelConfig, SchedulerConfig, VllmConfig\n from vllm.sampling_params import SamplingParams\n from vllm.v1.core.scheduler_output import (CachedRequestData, NewRequestData,\n                                            SchedulerOutput)\n+from vllm.v1.sample.metadata import SamplingMetadata\n from vllm.v1.worker.gpu_model_runner import GPUModelRunner\n \n \n@@ -82,14 +83,21 @@ def _is_req_added(model_runner, req_id: str) -> bool:\n     return req_id in model_runner.requests\n \n \n+def _is_sampling_metadata_changed(model_runner,\n+                                  sampling_metadata_before: SamplingMetadata):\n+    return model_runner.input_batch.sampling_metadata is not (\n+        sampling_metadata_before)\n+\n+\n def test_update_states_new_request(model_runner):\n     req_id = \"req_0\"\n \n     # new req\n     scheduler_output = _schedule_new_request(req_id)\n \n-    batch_changed = model_runner._update_states(scheduler_output)\n-    assert batch_changed is True\n+    metadata_before = model_runner.input_batch.sampling_metadata\n+    model_runner._update_states(scheduler_output)\n+    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n     assert _is_req_added(model_runner, req_id)\n     assert _is_req_scheduled(model_runner, req_id)\n \n@@ -117,8 +125,9 @@ def test_update_states_request_finished(model_runner):\n         free_encoder_input_ids=[],\n     )\n \n-    batch_changed = model_runner._update_states(scheduler_output)\n-    assert batch_changed is True\n+    metadata_before = model_runner.input_batch.sampling_metadata\n+    model_runner._update_states(scheduler_output)\n+    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n     assert not _is_req_added(model_runner, req_id)\n     assert not _is_req_scheduled(model_runner, req_id)\n \n@@ -142,7 +151,7 @@ def test_update_states_request_resumed(model_runner):\n         scheduled_spec_decode_tokens={},\n         scheduled_encoder_inputs={},\n         num_common_prefix_blocks=0,\n-        finished_req_ids={},\n+        finished_req_ids=set(),\n         free_encoder_input_ids=[],\n     )\n \n@@ -171,8 +180,9 @@ def test_update_states_request_resumed(model_runner):\n         free_encoder_input_ids=[],\n     )\n \n-    batch_changed = model_runner._update_states(scheduler_output)\n-    assert batch_changed is True\n+    metadata_before = model_runner.input_batch.sampling_metadata\n+    model_runner._update_states(scheduler_output)\n+    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n     assert _is_req_added(model_runner, req_id)\n     assert _is_req_scheduled(model_runner, req_id)\n \n@@ -200,8 +210,9 @@ def test_update_states_no_changes(model_runner):\n         free_encoder_input_ids=[],\n     )\n \n-    batch_changed = model_runner._update_states(scheduler_output)\n-    assert batch_changed is False\n+    metadata_before = model_runner.input_batch.sampling_metadata\n+    model_runner._update_states(scheduler_output)\n+    assert not _is_sampling_metadata_changed(model_runner, metadata_before)\n     assert _is_req_added(model_runner, req_id)\n     assert _is_req_scheduled(model_runner, req_id)\n \n@@ -233,8 +244,8 @@ def test_update_states_request_unscheduled(model_runner):\n         free_encoder_input_ids=[],\n     )\n \n-    batch_changed = model_runner._update_states(scheduler_output)\n-    assert batch_changed is True\n+    metadata_before = model_runner._update_states(scheduler_output)\n+    assert _is_sampling_metadata_changed(model_runner, metadata_before)\n \n     assert _is_req_added(model_runner, req_ids[0])\n     assert _is_req_scheduled(model_runner, req_ids[0])\ndiff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex dfe71028c..a9ef97391 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -45,7 +45,7 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                                    vocab_size, num_seqs)\n     output_bin_counts, output_mask = get_token_bin_counts_and_mask(\n         output_tokens_tensor, vocab_size, num_seqs)\n-    repetition_penalties = repetition_penalties.unsqueeze_(dim=1).repeat(\n+    repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n         1, vocab_size)\n     logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n                                       repetition_penalties, 1.0)[logits > 0]\n@@ -53,6 +53,6 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                                        repetition_penalties, 1.0)[logits <= 0]\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n-    logits -= frequency_penalties.unsqueeze_(dim=1) * output_bin_counts\n-    logits -= presence_penalties.unsqueeze_(dim=1) * output_mask\n+    logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts\n+    logits -= presence_penalties.unsqueeze(dim=1) * output_mask\n     return logits\ndiff --git a/vllm/v1/core/scheduler.py b/vllm/v1/core/scheduler.py\nindex 8f1083425..535aa644c 100644\n--- a/vllm/v1/core/scheduler.py\n+++ b/vllm/v1/core/scheduler.py\n@@ -195,8 +195,10 @@ class Scheduler:\n                                              request.num_computed_tokens -\n                                              request.num_tokens)\n                 if num_scheduled_spec_tokens > 0:\n+                    # Trim spec_token_ids list to num_scheduled_spec_tokens.\n+                    del request.spec_token_ids[num_scheduled_spec_tokens:]\n                     scheduled_spec_decode_tokens[request.request_id] = (\n-                        request.spec_token_ids[:num_scheduled_spec_tokens])\n+                        request.spec_token_ids)\n \n             # Encoder-related.\n             if encoder_inputs_to_schedule:\n@@ -567,7 +569,7 @@ class Scheduler:\n                 outputs.append(\n                     EngineCoreOutput(\n                         request_id=req_id,\n-                        new_token_ids=new_token_ids or [],\n+                        new_token_ids=new_token_ids,\n                         finish_reason=request.get_finished_reason(),\n                         new_logprobs=new_logprobs,\n                         new_prompt_logprobs_tensors=prompt_logprobs_tensors,\ndiff --git a/vllm/v1/sample/metadata.py b/vllm/v1/sample/metadata.py\nindex ea64181c0..2184a1866 100644\n--- a/vllm/v1/sample/metadata.py\n+++ b/vllm/v1/sample/metadata.py\n@@ -1,7 +1,7 @@\n # SPDX-License-Identifier: Apache-2.0\n \n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Set\n+from typing import Dict, List, Optional, Set, Tuple\n \n import torch\n \n@@ -12,15 +12,13 @@ class SamplingMetadata:\n     temperature: torch.Tensor\n     all_greedy: bool\n     all_random: bool\n-    rejection_sampling: bool\n-    spec_token_ids: List[List[int]]\n \n-    top_p: torch.Tensor\n-    top_k: torch.Tensor\n-    no_top_p: bool\n-    no_top_k: bool\n-    min_p: torch.Tensor\n-    no_min_p: bool\n+    # None when there are no speculated tokens.\n+    spec_token_ids: Optional[List[List[int]]]\n+\n+    top_p: Optional[torch.Tensor]\n+    top_k: Optional[torch.Tensor]\n+    min_p: Optional[torch.Tensor]\n \n     generators: Dict[int, torch.Generator]\n \n@@ -34,7 +32,8 @@ class SamplingMetadata:\n     repetition_penalties: torch.Tensor\n \n     output_token_ids: List[List[int]]\n-    min_tokens: List[int]\n-    stop_token_ids: List[Set[int]]\n+\n+    # req_index -> (min_tokens, stop_token_ids)\n+    min_tokens: Dict[int, Tuple[int, Set[int]]]\n \n     logit_bias: List[Optional[Dict[int, float]]]\ndiff --git a/vllm/v1/sample/ops/penalties.py b/vllm/v1/sample/ops/penalties.py\nindex ba368b44a..8d9f6529f 100644\n--- a/vllm/v1/sample/ops/penalties.py\n+++ b/vllm/v1/sample/ops/penalties.py\n@@ -1,6 +1,6 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-from typing import List, Set, Tuple\n+from typing import Dict, List, Set, Tuple\n \n import torch\n \n@@ -8,18 +8,17 @@ from vllm.model_executor.layers.utils import apply_penalties\n from vllm.utils import is_pin_memory_available, make_tensor_with_pad\n \n \n-def apply_min_token_penalties(logits: torch.Tensor,\n-                              output_token_ids: List[List[int]],\n-                              stop_token_ids: List[Set[int]],\n-                              min_tokens: List[int]) -> None:\n+def apply_min_token_penalties(\n+        logits: torch.Tensor, output_token_ids: List[List[int]],\n+        min_tokens: Dict[int, Tuple[int, Set[int]]]) -> None:\n     \"\"\"\n     Applies minimum token penalty by setting the logits of the stop tokens\n     to -inf.\n     \"\"\"\n     min_tokens_logits_to_penalize: List[Tuple[int, int]] = []\n-    for index, min_token in enumerate(min_tokens):\n+    for index, (min_token, stop_token_ids) in min_tokens.items():\n         if len(output_token_ids[index]) < min_token:\n-            for stop_token_id in stop_token_ids[index]:\n+            for stop_token_id in stop_token_ids:\n                 min_tokens_logits_to_penalize.append((index, stop_token_id))\n     if min_tokens_logits_to_penalize:\n         logits[tuple(zip(*min_tokens_logits_to_penalize))] = -float(\"inf\")\ndiff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 27431001e..78c88ad8b 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -1,6 +1,6 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-from typing import Dict\n+from typing import Dict, Optional\n \n import torch\n import torch.nn as nn\n@@ -55,13 +55,11 @@ class TopKTopPSampler(nn.Module):\n         self,\n         logits: torch.Tensor,\n         generators: Dict[int, torch.Generator],\n-        no_top_k: bool,\n-        k: torch.Tensor,\n-        no_top_p: bool,\n-        p: torch.Tensor,\n+        k: Optional[torch.Tensor],\n+        p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n         \"\"\"PyTorch-native implementation of top-k and top-p sampling.\"\"\"\n-        logits = apply_top_k_top_p(logits, no_top_k, k, no_top_p, p)\n+        logits = apply_top_k_top_p(logits, k, p)\n         probs = logits.softmax(dim=-1, dtype=torch.float32)\n         return random_sample(probs, generators)\n \n@@ -69,37 +67,33 @@ class TopKTopPSampler(nn.Module):\n         self,\n         logits: torch.Tensor,\n         generators: Dict[int, torch.Generator],\n-        no_top_k: bool,\n-        k: torch.Tensor,\n-        no_top_p: bool,\n-        p: torch.Tensor,\n+        k: Optional[torch.Tensor],\n+        p: Optional[torch.Tensor],\n     ) -> torch.Tensor:\n         \"\"\"More optimized implementation for top-k and top-p sampling.\"\"\"\n         probs = logits.softmax(dim=-1, dtype=torch.float32)\n-        if no_top_k and no_top_p:\n+        if k is None and p is None:\n             # We prefer `random_sample` over `flashinfer_sample` when sorting is\n             # not needed. This is because `random_sample` does not require\n             # CPU-GPU synchronization while `flashinfer_sample` does.\n             return random_sample(probs, generators)\n-        return flashinfer_sample(probs, no_top_k, k, no_top_p, p, generators)\n+        return flashinfer_sample(probs, k, p, generators)\n \n \n def apply_top_k_top_p(\n     logits: torch.Tensor,\n-    no_top_k: bool,\n-    k: torch.Tensor,\n-    no_top_p: bool,\n-    p: torch.Tensor,\n+    k: Optional[torch.Tensor],\n+    p: Optional[torch.Tensor],\n ) -> torch.Tensor:\n     \"\"\"Apply top-k and top-p masks to the logits.\n \n     This function sorts the logits tensor, which can be slow for large batches.\n     \"\"\"\n-    if no_top_k and no_top_p:\n+    if k is None and p is None:\n         return logits\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n-    if not no_top_k:\n+    if k is not None:\n         # Apply top-k.\n         top_k_mask = logits_sort.size(1) - k.to(torch.long)\n         # Get all the top_k values.\n@@ -107,7 +101,7 @@ def apply_top_k_top_p(\n         top_k_mask = logits_sort < top_k_mask\n         logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n \n-    if not no_top_p:\n+    if p is not None:\n         # Apply top-p.\n         probs_sort = logits_sort.softmax(dim=-1)\n         probs_sum = probs_sort.cumsum(dim=-1)\n@@ -147,10 +141,8 @@ def random_sample(\n \n def flashinfer_sample(\n     probs: torch.Tensor,\n-    no_top_k: bool,\n-    k: torch.Tensor,\n-    no_top_p: bool,\n-    p: torch.Tensor,\n+    k: Optional[torch.Tensor],\n+    p: Optional[torch.Tensor],\n     generators: Dict[int, torch.Generator],\n ) -> torch.Tensor:\n     \"\"\"Sample from the probabilities using FlashInfer.\n@@ -167,7 +159,7 @@ def flashinfer_sample(\n     does not. Call this function at the end of the forward pass to minimize\n     the synchronization overhead.\n     \"\"\"\n-    assert not (no_top_k and no_top_p)\n+    assert not (k is None and p is None)\n     max_top_k_round = 32\n     batch_size = probs.shape[0]\n     uniform_samples = torch.empty((max_top_k_round, batch_size),\n@@ -178,11 +170,11 @@ def flashinfer_sample(\n         for i, generator in generators.items():\n             uniform_samples[:, i].uniform_(generator=generator)\n \n-    if no_top_k:\n+    if k is None:\n         # Top-p only.\n         next_token_ids, success = flashinfer.sampling.top_p_sampling_from_probs(\n             probs, uniform_samples, p, deterministic=True)\n-    elif no_top_p:\n+    elif p is None:\n         # Top-k only.\n         next_token_ids, success = flashinfer.sampling.top_k_sampling_from_probs(\n             probs, uniform_samples, k, deterministic=True)\n@@ -194,9 +186,9 @@ def flashinfer_sample(\n \n     # NOTE: CPU-GPU synchronization happens here.\n     if not success.all():\n-        if not no_top_k:\n+        if k is not None:\n             probs = flashinfer.sampling.top_k_renorm_prob(probs, k)\n-        if not no_top_p:\n+        if p is not None:\n             probs = flashinfer.sampling.top_p_renorm_prob(probs, p)\n         next_token_ids = flashinfer.sampling.sampling_from_probs(\n             probs, uniform_samples[0], deterministic=True)\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex df1da8930..580ad4429 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -68,6 +68,7 @@ class RejectionSampler(nn.Module):\n         # NOTE: The following input preparationg can be moved\n         # to the model runner with a persistent manner for better\n         # performance.\n+        assert sampling_metadata.spec_token_ids is not None\n         spec_token_ids = sampling_metadata.spec_token_ids\n         max_spec_len = max(len(s) for s in spec_token_ids)\n         batch_size = len(spec_token_ids)\n@@ -119,6 +120,7 @@ class RejectionSampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> SamplerOutput:\n+        assert sampling_metadata.spec_token_ids is not None\n         spec_lens = [len(x) for x in sampling_metadata.spec_token_ids]\n         # Add 1 to include the 'bonus' token.\n         sample_lens = [x + 1 for x in spec_lens]\ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex ec6374d12..8e2533eef 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -26,7 +26,7 @@ class Sampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> SamplerOutput:\n-        if sampling_metadata.rejection_sampling:\n+        if sampling_metadata.spec_token_ids:\n             if sampling_metadata.max_num_logprobs:\n                 raise NotImplementedError(\n                     \"Rejection sampling does not support logprobs.\")\n@@ -104,16 +104,14 @@ class Sampler(nn.Module):\n         logits = self.apply_temperature(logits, sampling_metadata.temperature)\n \n         # Apply min_p.\n-        if not sampling_metadata.no_min_p:\n+        if sampling_metadata.min_p is not None:\n             logits = self.apply_min_p(logits, sampling_metadata.min_p)\n \n         # Apply top_k and/or top_p.\n         random_sampled = self.topk_topp_sampler(\n             logits,\n             sampling_metadata.generators,\n-            sampling_metadata.no_top_k,\n             sampling_metadata.top_k,\n-            sampling_metadata.no_top_p,\n             sampling_metadata.top_p,\n         )\n \n@@ -179,9 +177,10 @@ class Sampler(nn.Module):\n         logits: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> torch.Tensor:\n-        apply_min_token_penalties(logits, sampling_metadata.output_token_ids,\n-                                  sampling_metadata.stop_token_ids,\n-                                  sampling_metadata.min_tokens)\n+        if sampling_metadata.min_tokens:\n+            apply_min_token_penalties(logits,\n+                                      sampling_metadata.output_token_ids,\n+                                      sampling_metadata.min_tokens)\n         if not sampling_metadata.no_penalties:\n             assert sampling_metadata.prompt_token_ids is not None\n             logits = apply_all_penalties(\ndiff --git a/vllm/v1/utils.py b/vllm/v1/utils.py\nindex 5494542c1..5be465014 100644\n--- a/vllm/v1/utils.py\n+++ b/vllm/v1/utils.py\n@@ -188,3 +188,14 @@ def bind_kv_cache(\n     for layer_name, kv_cache in kv_caches.items():\n         # NOTE: Use list because of v0 PP virtual engine.\n         forward_context[layer_name].kv_cache = [kv_cache]\n+\n+\n+def copy_slice(from_tensor: torch.Tensor, to_tensor: torch.Tensor,\n+               length: int) -> None:\n+    \"\"\"\n+    Copy the first length elements of a tensor into another tensor in a\n+    non-blocking manner.\n+\n+    Used to copy pinned CPU tensor data to pre-allocated GPU tensors.\n+    \"\"\"\n+    to_tensor[:length].copy_(from_tensor[:length], non_blocking=True)\ndiff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex cb7411a44..ccafc325b 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -1,9 +1,8 @@\n # SPDX-License-Identifier: Apache-2.0\n-\n # Datastructures defining an input batch\n \n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple\n+from typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, cast\n \n import numpy as np\n import torch\n@@ -12,6 +11,7 @@ from vllm.lora.request import LoRARequest\n from vllm.multimodal import MultiModalKwargs\n from vllm.sampling_params import SamplingParams, SamplingType\n from vllm.v1.sample.metadata import SamplingMetadata\n+from vllm.v1.utils import copy_slice\n from vllm.v1.worker.block_table import BlockTable\n \n _SAMPLING_EPS = 1e-5\n@@ -63,7 +63,7 @@ class InputBatch:\n         self.pin_memory = pin_memory\n         self.vocab_size = vocab_size\n \n-        self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n+        self._req_ids: List[Optional[str]] = []\n         self.req_id_to_index: Dict[str, int] = {}\n \n         # TODO(woosuk): This buffer could be too large if max_model_len is big.\n@@ -171,11 +171,8 @@ class InputBatch:\n                 self.repetition_penalties_cpu_tensor.numpy()\n         self.repetition_penalties_reqs: Set[str] = set()\n \n-        self.min_tokens: List[int] = [0] * max_num_reqs\n-        self.stop_token_ids: List[Set[int]] = [\n-            set() for _ in range(max_num_reqs)\n-        ]\n-        self.prompt_token_ids: Optional[torch.Tensor] = None\n+        # req_index -> (min_tokens, stop_token_ids)\n+        self.min_tokens: Dict[int, Tuple[int, Set[int]]] = {}\n \n         # lora related\n         self.request_lora_mapping = np.zeros((self.max_num_reqs, ),\n@@ -196,6 +193,17 @@ class InputBatch:\n         self.logit_bias: List[Optional[Dict[int,\n                                             float]]] = [None] * max_num_reqs\n \n+        self.req_output_token_ids: List[Optional[List[int]]] = []\n+\n+        # This is updated each time the batch constituents change.\n+        self.sampling_metadata = self._make_sampling_metadata()\n+\n+    @property\n+    def req_ids(self) -> List[str]:\n+        # None elements should only be present transiently\n+        # while performing state updates to the batch.\n+        return cast(List[str], self._req_ids)\n+\n     def add_request(\n         self,\n         request: \"CachedRequestState\",\n@@ -206,7 +214,13 @@ class InputBatch:\n         assert req_index < self.max_num_reqs\n \n         req_id = request.req_id\n-        self.req_ids[req_index] = req_id\n+        if req_index == len(self._req_ids):\n+            self._req_ids.append(req_id)\n+            self.req_output_token_ids.append(request.output_token_ids)\n+        else:\n+            self._req_ids[req_index] = req_id\n+            self.req_output_token_ids[req_index] = request.output_token_ids\n+\n         self.req_id_to_index[req_id] = req_index\n \n         # Copy the prompt token ids and output token ids.\n@@ -255,8 +269,9 @@ class InputBatch:\n             req_index] = sampling_params.repetition_penalty\n         if sampling_params.repetition_penalty != 1.0:\n             self.repetition_penalties_reqs.add(req_id)\n-        self.min_tokens[req_index] = sampling_params.min_tokens\n-        self.stop_token_ids[req_index] = sampling_params.all_stop_token_ids\n+        if sampling_params.min_tokens:\n+            self.min_tokens[req_index] = (sampling_params.min_tokens,\n+                                          sampling_params.all_stop_token_ids)\n \n         # NOTE(woosuk): self.generators should not include the requests that\n         # do not have their own generator.\n@@ -284,16 +299,20 @@ class InputBatch:\n             self.request_lora_mapping[req_index] = 0\n \n     def remove_request(self, req_id: str) -> Optional[int]:\n+        \"\"\"This method must always be followed by a call to condense().\"\"\"\n+\n         req_index = self.req_id_to_index.pop(req_id, None)\n         if req_index is None:\n             return None\n-        self.req_ids[req_index] = None\n+        self._req_ids[req_index] = None\n+        self.req_output_token_ids[req_index] = None\n \n         self.greedy_reqs.discard(req_id)\n         self.random_reqs.discard(req_id)\n         self.top_p_reqs.discard(req_id)\n         self.top_k_reqs.discard(req_id)\n         self.min_p_reqs.discard(req_id)\n+        self.min_tokens.pop(req_index, None)\n         self.frequency_penalties_reqs.discard(req_id)\n         self.presence_penalties_reqs.discard(req_id)\n         self.repetition_penalties_reqs.discard(req_id)\n@@ -313,33 +332,17 @@ class InputBatch:\n         self.logit_bias[req_index] = None\n         return req_index\n \n-    def clear(self) -> None:\n-        self.req_ids = [None] * self.max_num_reqs\n-        self.req_id_to_index.clear()\n-        self.greedy_reqs.clear()\n-        self.random_reqs.clear()\n-        self.top_p_reqs.clear()\n-        self.top_k_reqs.clear()\n-        self.min_p_reqs.clear()\n-        self.frequency_penalties_reqs.clear()\n-        self.presence_penalties_reqs.clear()\n-        self.repetition_penalties_reqs.clear()\n-        self.generators.clear()\n-        self.num_logprobs.clear()\n-        self.num_prompt_logprobs.clear()\n-        self.request_lora_mapping.fill(0)\n-        self.lora_id_to_lora_request.clear()\n-        self.lora_id_to_request_ids.clear()\n-        self.logit_bias = [None] * self.max_num_reqs\n-\n     def condense(self, empty_req_indices: List[int]) -> None:\n-        if self.num_reqs == 0:\n+        num_reqs = self.num_reqs\n+        if num_reqs == 0:\n             # The batched states are empty.\n+            self._req_ids.clear()\n+            self.req_output_token_ids.clear()\n             return\n \n         # NOTE(woosuk): This function assumes that the empty_req_indices\n         # is sorted in descending order.\n-        last_req_index = self.num_reqs + len(empty_req_indices) - 1\n+        last_req_index = num_reqs + len(empty_req_indices) - 1\n         while empty_req_indices:\n             # Find the largest non-empty index.\n             while last_req_index in empty_req_indices:\n@@ -351,10 +354,13 @@ class InputBatch:\n                 break\n \n             # Swap the states.\n-            req_id = self.req_ids[last_req_index]\n+            req_id = self._req_ids[last_req_index]\n+            output_token_ids = self.req_output_token_ids[last_req_index]\n             assert req_id is not None\n-            self.req_ids[empty_index] = req_id\n-            self.req_ids[last_req_index] = None\n+            self._req_ids[empty_index] = req_id\n+            self._req_ids[last_req_index] = None\n+            self.req_output_token_ids[empty_index] = output_token_ids\n+            self.req_output_token_ids[last_req_index] = None\n             self.req_id_to_index[req_id] = empty_index\n \n             num_tokens = self.num_tokens[last_req_index]\n@@ -379,13 +385,14 @@ class InputBatch:\n             self.repetition_penalties_cpu[\n                 empty_index] = self.repetition_penalties_cpu[last_req_index]\n             self.min_p_cpu[empty_index] = self.min_p_cpu[last_req_index]\n-            self.min_tokens[empty_index] = self.min_tokens[last_req_index]\n-            self.stop_token_ids[empty_index] = self.stop_token_ids[\n-                last_req_index]\n             generator = self.generators.pop(last_req_index, None)\n             if generator is not None:\n                 self.generators[empty_index] = generator\n \n+            min_token = self.min_tokens.pop(last_req_index, None)\n+            if min_token is not None:\n+                self.min_tokens[empty_index] = min_token\n+\n             self.request_lora_mapping[empty_index] = self.request_lora_mapping[\n                 last_req_index]\n \n@@ -394,87 +401,71 @@ class InputBatch:\n             # Decrement last_req_index since it is now empty.\n             last_req_index -= 1\n \n-    def make_sampling_metadata(\n-        self,\n-        req_id_output_token_ids: Dict[str, List[int]],\n-        req_id_to_spec_token_ids: Dict[str, List[int]],\n-        skip_copy: bool = False,\n-    ) -> SamplingMetadata:\n-        if not skip_copy:\n-            self.temperature[:self.num_reqs].copy_(\n-                self.temperature_cpu_tensor[:self.num_reqs], non_blocking=True)\n-            self.top_p[:self.num_reqs].copy_(\n-                self.top_p_cpu_tensor[:self.num_reqs], non_blocking=True)\n-            self.top_k[:self.num_reqs].copy_(\n-                self.top_k_cpu_tensor[:self.num_reqs], non_blocking=True)\n-            self.min_p[:self.num_reqs].copy_(\n-                self.min_p_cpu_tensor[:self.num_reqs], non_blocking=True)\n-            if not self.no_penalties:\n-                # Since syncing these tensors is expensive only copy them\n-                # if necessary i.e. if there are requests which require\n-                # penalties to be applied during sampling.\n-                self.frequency_penalties[:self.num_reqs].copy_(\n-                    self.frequency_penalties_cpu_tensor[:self.num_reqs],\n-                    non_blocking=True,\n-                )\n-                self.presence_penalties[:self.num_reqs].copy_(\n-                    self.presence_penalties_cpu_tensor[:self.num_reqs],\n-                    non_blocking=True,\n-                )\n-                self.repetition_penalties[:self.num_reqs].copy_(\n-                    self.repetition_penalties_cpu_tensor[:self.num_reqs],\n-                    non_blocking=True,\n-                )\n-                # The prompt tokens are used only for applying penalties during\n-                # the sampling process. Hence copy these tensors only when\n-                # there are requests which need penalties to be applied.\n-                self.prompt_token_ids = self._make_prompt_token_ids_tensor()\n-\n-        output_token_ids: List[List[int]] = []\n-        spec_token_ids: List[List[int]] = []\n-        rejection_sampling = False\n-        for req_id in self.req_ids[:self.num_reqs]:\n-            assert req_id is not None\n-            # Currently we create a tensor for output_token_ids from scratch\n-            # at each step. However, for the penalties computation what we\n-            # need is stats about the token ids present in the output. This\n-            # stats can be maintained incrementally instead of computing it\n-            # from scratch at each step.\n-            # TODO - Replace this with incremental update to output token\n-            # statistics.\n-            output_token_ids.append(req_id_output_token_ids[req_id])\n-            req_spec_token_ids = req_id_to_spec_token_ids.get(req_id, [])\n-            spec_token_ids.append(req_spec_token_ids)\n-            if req_spec_token_ids:\n-                # If any of the requests require speculative decoding, set the\n-                # flag to True.\n-                rejection_sampling = True\n+        # Trim lists to the batch size.\n+        del self._req_ids[self.num_reqs:]\n+        del self.req_output_token_ids[self.num_reqs:]\n+\n+    def refresh_sampling_metadata(self):\n+        self.sampling_metadata = self._make_sampling_metadata()\n+\n+    def _make_sampling_metadata(self) -> SamplingMetadata:\n+        num_reqs = self.num_reqs\n+        copy_slice(self.temperature_cpu_tensor, self.temperature, num_reqs)\n+        if not self.no_top_p:\n+            copy_slice(self.top_p_cpu_tensor, self.top_p, num_reqs)\n+        if not self.no_top_k:\n+            copy_slice(self.top_k_cpu_tensor, self.top_k, num_reqs)\n+        if not self.no_min_p:\n+            copy_slice(self.min_p_cpu_tensor, self.min_p, num_reqs)\n+\n+        if not self.no_penalties:\n+            # Since syncing these tensors is expensive only copy them\n+            # if necessary i.e. if there are requests which require\n+            # penalties to be applied during sampling.\n+            copy_slice(self.frequency_penalties_cpu_tensor,\n+                       self.frequency_penalties, num_reqs)\n+            copy_slice(self.presence_penalties_cpu_tensor,\n+                       self.presence_penalties, num_reqs)\n+            copy_slice(self.repetition_penalties_cpu_tensor,\n+                       self.repetition_penalties, num_reqs)\n+\n+            # The prompt tokens are used only for applying penalties during\n+            # the sampling process. Hence copy these tensors only when\n+            # there are requests which need penalties to be applied.\n+            prompt_token_ids = self._make_prompt_token_ids_tensor()\n+        else:\n+            prompt_token_ids = None\n \n         return SamplingMetadata(\n-            temperature=self.temperature[:self.num_reqs],\n+            temperature=self.temperature[:num_reqs],\n             all_greedy=self.all_greedy,\n             all_random=self.all_random,\n-            rejection_sampling=rejection_sampling,\n-            top_p=self.top_p[:self.num_reqs],\n-            top_k=self.top_k[:self.num_reqs],\n-            min_p=self.min_p[:self.num_reqs],\n-            no_min_p=self.no_min_p,\n-            no_top_p=self.no_top_p,\n-            no_top_k=self.no_top_k,\n+            top_p=None if self.no_top_p else self.top_p[:num_reqs],\n+            top_k=None if self.no_top_k else self.top_k[:num_reqs],\n+            min_p=None if self.no_min_p else self.min_p[:num_reqs],\n             generators=self.generators,\n             max_num_logprobs=self.max_num_logprobs,\n-            prompt_token_ids=self.prompt_token_ids,\n-            frequency_penalties=self.frequency_penalties[:self.num_reqs],\n-            presence_penalties=self.presence_penalties[:self.num_reqs],\n-            repetition_penalties=self.repetition_penalties[:self.num_reqs],\n-            output_token_ids=output_token_ids,\n-            spec_token_ids=spec_token_ids,\n-            min_tokens=self.min_tokens[:self.num_reqs],\n-            stop_token_ids=self.stop_token_ids[:self.num_reqs],\n+            prompt_token_ids=prompt_token_ids,\n+            frequency_penalties=self.frequency_penalties[:num_reqs],\n+            presence_penalties=self.presence_penalties[:num_reqs],\n+            repetition_penalties=self.repetition_penalties[:num_reqs],\n+            output_token_ids=cast(List[List[int]], self.req_output_token_ids),\n+            spec_token_ids=None,\n+            min_tokens=self.min_tokens,\n             no_penalties=self.no_penalties,\n-            logit_bias=self.logit_bias[:self.num_reqs],\n+            logit_bias=self.logit_bias[:num_reqs],\n         )\n \n+    def get_sampling_metadata(\n+        self,\n+        req_id_to_spec_token_ids: Dict[str, List[int]],\n+    ) -> SamplingMetadata:\n+        # Set the new spec token ids in the cached sampling metadata.\n+        self.sampling_metadata.spec_token_ids = [\n+            req_id_to_spec_token_ids.get(req_id, []) for req_id in self.req_ids\n+        ] if req_id_to_spec_token_ids else None\n+        return self.sampling_metadata\n+\n     def _make_prompt_token_ids_tensor(self) -> torch.Tensor:\n         max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()\n         prompt_token_ids_cpu_tensor = torch.empty(\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 5754422cb..0ecc00acc 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -31,7 +31,6 @@ from vllm.v1.engine.mm_input_cache import MMInputCacheClient\n from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n                                         KVCacheSpec)\n from vllm.v1.outputs import LogprobsTensors, ModelRunnerOutput\n-from vllm.v1.sample.metadata import SamplingMetadata\n from vllm.v1.sample.rejection_sampler import INVALID_TOKEN_ID\n from vllm.v1.spec_decode.ngram_proposer import NgramProposer\n from vllm.v1.utils import bind_kv_cache\n@@ -224,16 +223,15 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                                         pin_memory=self.pin_memory)\n         self.seq_lens_np = self.seq_lens_cpu.numpy()\n \n-    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> bool:\n+    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         \"\"\"Update the cached states and the persistent batch with the scheduler\n         output.\n \n         The updated states are used by the `_prepare_inputs` function to create\n         the input GPU tensors for the model.\n \n-        Returns:\n-            True if there is a new/resumed/paused/finished request in the batch.\n-            If False, we can skip copying SamplingMetadata to the GPU.\n+        The SamplingMetadata is updated and copied to the GPU if there is a\n+        new/resumed/paused/finished request in the batch.\n         \"\"\"\n         # Remove finished requests from the cached states.\n         for req_id in scheduler_output.finished_req_ids:\n@@ -344,9 +342,12 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             num_new_tokens = (num_computed_tokens +\n                               len(req_data.new_token_ids) -\n                               req_state.num_tokens)\n-            new_token_ids = (req_data.new_token_ids[-num_new_tokens:]\n-                             if num_new_tokens > 0 else [])\n-            req_state.output_token_ids.extend(new_token_ids)\n+            if num_new_tokens == 1:\n+                # Avoid slicing list in most common case.\n+                req_state.output_token_ids.append(req_data.new_token_ids[-1])\n+            elif num_new_tokens > 0:\n+                req_state.output_token_ids.extend(\n+                    req_data.new_token_ids[-num_new_tokens:])\n             # Update the block IDs.\n             if not req_data.resumed_from_preemption:\n                 # Append the new blocks to the existing block IDs.\n@@ -380,7 +381,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             self.input_batch.num_tokens_no_spec[req_index] = end_token_index\n             # Add spec_token_ids to token_ids_cpu.\n             spec_token_ids = scheduler_output.scheduled_spec_decode_tokens.get(\n-                req_id, [])\n+                req_id, ())\n             if spec_token_ids:\n                 start_index = end_token_index\n                 end_token_index += len(spec_token_ids)\n@@ -410,7 +411,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         if removed_req_indices:\n             self.input_batch.condense(removed_req_indices)\n \n-        return batch_changed\n+        if batch_changed:\n+            self.input_batch.refresh_sampling_metadata()\n \n     def _prepare_inputs(\n         self,\n@@ -429,8 +431,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # TODO: The Python loop can be slow. Optimize.\n         num_scheduled_tokens = np.empty(num_reqs, dtype=np.int32)\n         max_num_scheduled_tokens = 0\n-        for i, req_id in zip(range(num_reqs), self.input_batch.req_ids):\n-            assert req_id is not None\n+        for i, req_id in enumerate(self.input_batch.req_ids):\n             num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n             num_scheduled_tokens[i] = num_tokens\n             max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n@@ -669,10 +670,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n     def _calc_mrope_positions(self, scheduler_output: \"SchedulerOutput\"):\n         mrope_pos_ptr = 0\n-        num_reqs = self.input_batch.num_reqs\n-        for index, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n-            assert req_id is not None\n-\n+        for index, req_id in enumerate(self.input_batch.req_ids):\n             req = self.requests[req_id]\n             assert req.mrope_positions is not None\n \n@@ -726,12 +724,11 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self,\n         scheduler_output: \"SchedulerOutput\",\n         cu_num_tokens: np.ndarray,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+    ) -> torch.Tensor:\n         # Get the number of spec decode tokens for each request.\n         num_reqs = self.input_batch.num_reqs\n         num_spec_decode_tokens = np.empty(num_reqs, dtype=np.int32)\n-        for i, req_id in zip(range(num_reqs), self.input_batch.req_ids):\n-            assert req_id is not None\n+        for i, req_id in enumerate(self.input_batch.req_ids):\n             num_spec_decode_tokens[i] = len(\n                 scheduler_output.scheduled_spec_decode_tokens.get(req_id, ()))\n \n@@ -769,22 +766,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         return torch.from_numpy(spec_decode_logits_indices).to(\n             self.device, non_blocking=True)\n \n-    def _prepare_sampling(\n-        self,\n-        batch_changed: bool,\n-        req_to_spec_token_ids: Dict[str, List[int]],\n-    ) -> SamplingMetadata:\n-        # Create the sampling metadata.\n-        req_id_output_token_ids: Dict[str, List[int]] = \\\n-            {req_id: req.output_token_ids \\\n-                for req_id, req in self.requests.items()}\n-\n-        sampling_metadata = self.input_batch.make_sampling_metadata(\n-            req_id_output_token_ids,\n-            req_to_spec_token_ids,\n-            skip_copy=not batch_changed)\n-        return sampling_metadata\n-\n     def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n         scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n         if not scheduled_encoder_inputs:\n@@ -838,9 +819,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         scheduler_output: \"SchedulerOutput\",\n     ) -> List[torch.Tensor]:\n         encoder_outputs: List[torch.Tensor] = []\n-        num_reqs = self.input_batch.num_reqs\n-        for req_id in self.input_batch.req_ids[:num_reqs]:\n-            assert req_id is not None\n+        for req_id in self.input_batch.req_ids:\n             num_scheduled_tokens = scheduler_output.num_scheduled_tokens[\n                 req_id]\n             req_state = self.requests[req_id]\n@@ -882,7 +861,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         scheduler_output: \"SchedulerOutput\",\n         intermediate_tensors: Optional[IntermediateTensors] = None,\n     ) -> Union[ModelRunnerOutput, torch.Tensor]:\n-        batch_changed = self._update_states(scheduler_output)\n+        self._update_states(scheduler_output)\n \n         if self.is_multimodal_model:\n             # Run the multimodal encoder if any.\n@@ -964,8 +943,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         logits = self.model.compute_logits(sample_hidden_states, None)\n \n         # Sample the next token and get logprobs if needed.\n-        sampling_metadata = self._prepare_sampling(\n-            batch_changed, scheduler_output.scheduled_spec_decode_tokens)\n+        sampling_metadata = self.input_batch.get_sampling_metadata(\n+            scheduler_output.scheduled_spec_decode_tokens)\n         sampler_output = self.model.sample(\n             logits=logits,\n             sampling_metadata=sampling_metadata,\n@@ -973,14 +952,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n         # TODO(woosuk): The following loop can be slow since it iterates over\n         # the requests one by one. Optimize.\n-        num_reqs = self.input_batch.num_reqs\n-        req_ids: List[str] = []\n-        # Because `input_batch.req_ids` is a list of length `max_num_reqs`,\n-        # we need to stop at `num_reqs`.\n-        # FIXME(woosuk): This is hacky. Refactor.\n-        for i, req_id in zip(range(num_reqs), self.input_batch.req_ids):\n-            assert req_id is not None\n-            req_ids.append(req_id)\n+        for i, req_id in enumerate(self.input_batch.req_ids):\n             req_state = self.requests[req_id]\n             seq_len = (req_state.num_computed_tokens +\n                        scheduler_output.num_scheduled_tokens[req_id])\n@@ -1027,7 +999,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                 valid_sampled_token_ids)\n \n         model_runner_output = ModelRunnerOutput(\n-            req_ids=req_ids,\n+            req_ids=self.input_batch.req_ids,\n             req_id_to_index=self.input_batch.req_id_to_index,\n             sampled_token_ids=valid_sampled_token_ids,\n             spec_token_ids=spec_token_ids,\n@@ -1041,19 +1013,18 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         sampled_token_ids: List[List[int]],\n     ) -> List[List[int]]:\n         # TODO(woosuk): Optimize.\n-        num_reqs = len(sampled_token_ids)\n         draft_token_ids: List[List[int]] = []\n-        for i in range(num_reqs):\n-            if len(sampled_token_ids[i]) == 0:\n+        for i, sampled_ids in enumerate(sampled_token_ids):\n+            num_sampled_ids = len(sampled_ids)\n+            if not num_sampled_ids:\n                 # Skip speculative decoding.\n                 draft_token_ids.append([])\n                 continue\n \n             # Add sampled_token_ids to token_ids_cpu.\n             start_idx = self.input_batch.num_tokens_no_spec[i]\n-            end_idx = start_idx + len(sampled_token_ids[i])\n-            self.input_batch.token_ids_cpu[\n-                i, start_idx:end_idx] = sampled_token_ids[i]\n+            end_idx = start_idx + num_sampled_ids\n+            self.input_batch.token_ids_cpu[i, start_idx:end_idx] = sampled_ids\n             drafter_output = self.drafter.propose(\n                 self.input_batch.token_ids_cpu[i, :end_idx],\n                 self.speculative_config.ngram_prompt_lookup_min,\n@@ -1204,7 +1175,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # multiplying the list, to avoid Dynamo from treating them as\n         # tensor aliasing.\n         dummy_kv_caches = [\n-            torch.tensor([], dtype=torch.float32, device=self.device)\n+            torch.tensor((), dtype=torch.float32, device=self.device)\n             for _ in range(self.num_attn_layers)\n         ]\n \ndiff --git a/vllm/v1/worker/tpu_model_runner.py b/vllm/v1/worker/tpu_model_runner.py\nindex 4ee6853ba..e60268f04 100644\n--- a/vllm/v1/worker/tpu_model_runner.py\n+++ b/vllm/v1/worker/tpu_model_runner.py\n@@ -1048,8 +1048,6 @@ def swap_positions(b: InputBatch, id_1, id_2):\n \n     b.min_tokens[id_1], b.min_tokens[id_2] = b.min_tokens[id_2], b.min_tokens[\n         id_1]\n-    b.stop_token_ids[id_1], b.stop_token_ids[id_2] = b.stop_token_ids[\n-        id_2], b.stop_token_ids[id_1]\n \n     gen_1 = b.generators.pop(id_1, None)\n     gen_2 = b.generators.pop(id_2, None)",
  "apis": [
    "vllm.v1.sample.metadata.SamplingMetadata",
    "vllm.v1.sample.sampler.Sampler",
    "vllm.v1.worker.gpu_input_batch.InputBatch",
    "vllm.v1.worker.gpu_model_runner.GPUModelRunner"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/pool/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_input_batch.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies several non-test source files (e.g., in vllm/v1/core, vllm/v1/worker, and others), and the changes are nontrivial. The modifications include handling of sampling metadata, optimizing how the req_ids list is managed, using non‚Äêblocking tensor copy (via the new copy_slice function), reducing unnecessary list slicing, and overall attempts to lower overhead in critical paths such as CPU-GPU synchronization. Although the commit message simply uses ‚ÄúOptimize‚Äù in its title, the diff shows multiple performance-related improvements affecting core functions of the runtime. These changes are designed to improve the performance of high-level APIs, are testable on CPU, and are not merely refactoring for clarity. Therefore, this commit qualifies as performance optimization related.",
  "llm_api_reason": "The commit refactors the handling of sampling metadata and request identifiers. In the sampling metadata definition (in vllm/v1/sample/metadata.py), several fields have been changed ‚Äì for example, the rejection_sampling flag is removed, spec_token_ids is now optional, and top_p, top_k, and min_p are made optional. In addition, related tests in test_sampler and test_rejection_sampler were updated to construct SamplingMetadata accordingly. Meanwhile, the InputBatch class (in vllm/v1/worker/gpu_input_batch.py) has been updated to manage the req_ids list more robustly (switching to an internal list _req_ids and providing a property for req_ids), and to refresh and generate sampling metadata via methods like _make_sampling_metadata and get_sampling_metadata. Finally, GPUModelRunner (in vllm/v1/worker/gpu_model_runner.py) has been modified to use the new sampling metadata access method, ensuring that any changes in metadata due to added/removed requests are properly propagated. Overall, these changes optimize the metadata handling across the sampling and batching subsystems."
}