{
  "commit_hash": "9badee53decb3d432dc805336abfb0eb81dfb48f",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14223",
  "pr_date": "2025-03-04",
  "timeline_text": "Copy link Member hmellor commented Mar 4, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Adds self.default_sampling_params to: OpenAIServingChat OpenAIServingCompletion OpenAIServingTranscription LLM As you can see from the benchmarks below, the performance difference is huge: vllm serve meta-llama/Llama-3.2-1B-Instruct --disable-log-requests --generation-config auto\n\npython benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json Before: ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  149.29    \nTotal input tokens:                      215196    \nTotal generated tokens:                  179873    \nRequest throughput (req/s):              6.70      \nOutput token throughput (tok/s):         1204.82   \nTotal Token throughput (tok/s):          2646.24   \n---------------Time to First Token----------------\nMean TTFT (ms):                          124792.06 \nMedian TTFT (ms):                        123725.39 \nP99 TTFT (ms):                           138387.36 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          40.52     \nMedian TPOT (ms):                        40.52     \nP99 TPOT (ms):                           67.62     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           36.56     \nMedian ITL (ms):                         37.74     \nP99 ITL (ms):                            72.37     \n================================================== After: ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  34.24     \nTotal input tokens:                      215196    \nTotal generated tokens:                  178861    \nRequest throughput (req/s):              29.21     \nOutput token throughput (tok/s):         5224.41   \nTotal Token throughput (tok/s):          11510.15  \n---------------Time to First Token----------------\nMean TTFT (ms):                          8481.82   \nMedian TTFT (ms):                        7455.52   \nP99 TTFT (ms):                           21150.72  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          37.85     \nMedian TPOT (ms):                        37.10     \nP99 TPOT (ms):                           51.13     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           35.43     \nMedian ITL (ms):                         35.88     \nP99 ITL (ms):                            72.53     \n================================================== Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions hmellor added 2 commits March 4, 2025 17:21 Prevent reads from disk at runtime when --generation-config auto is‚Ä¶ ‚Ä¶ accf38d ‚Ä¶ set\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com> Don't create a footgun ‚Ä¶ e3cd61e Signed-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com> Copy link github-actions bot commented Mar 4, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the frontend label Mar 4, 2025 mgoin requested review from njhill and robertgshaw2-redhat March 4, 2025 16:49 mgoin added\n  the performance Performance-related issues label Mar 4, 2025 mgoin changed the title Fix generation config arg Fix performance of --generation-config auto Mar 4, 2025 mgoin approved these changes Mar 4, 2025 View reviewed changes Copy link Member mgoin left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good catch, this is critical to fix as try_get_generation_config could be called for each request üòì Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 4, 2025 hmellor changed the title Fix performance of --generation-config auto Fix performance of --generation-config is not None Mar 4, 2025 Copy link Member Author hmellor commented Mar 4, 2025 Thanks for updating the title, technically --generation-config could be a file path (which would also cause this performance problem) üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Make mypy happy ‚Ä¶ 71e1cf1 Signed-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com> hmellor changed the title Fix performance of --generation-config is not None Fix performance when --generation-config is not None Mar 4, 2025 DarkLight1337 approved these changes Mar 4, 2025 View reviewed changes hmellor mentioned this pull request Mar 4, 2025 Default to generation_config from model #12622 Merged Hide details View details hmellor merged commit 9badee5 into vllm-project : main Mar 4, 2025 37 checks passed Uh oh! There was an error while loading. Please reload this page . hmellor deleted the fix-generation-config-arg branch March 4, 2025 19:59 Copy link Contributor yansh97 commented Mar 5, 2025 Very nice fix!!! Since \"--generation-config was added\", I have noticed a performance improvement when set to None, but a regression when set to \"auto\". I thought the reason is some changes in the sampling code. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Alexei-V-Ivanov-AMD added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Mar 11, 2025 Merging in the latest merge from vllm-project to ROCm ( #472 ) ‚Ä¶ a699a11 * Fix `head_dim` not existing in all model configs (Transformers backend) ( vllm-project#14141 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [V0][Metrics] Remove unimplemented `vllm:tokens_total` ( vllm-project#14134 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V0][Metrics] Deprecate some KV/prefix cache metrics ( vllm-project#14136 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V1] Simplify stats logging ( vllm-project#14082 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [WIP][[V1][Metrics] Implement max_num_generation_tokens,  request_params_n, and request_params_max_tokens metrics ( vllm-project#14055 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [Bugfix] Allow shared_experts skip quantization for DeepSeekV2/V3 ( vllm-project#14100 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Kernel] Optimize moe intermediate_cache usage ( vllm-project#13625 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Docs] Add GPTQModel ( vllm-project#14056 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\n\n* [v1] Add comments to the new ragged paged attention Pallas kernel ( vllm-project#14155 )\n\nSigned-off-by: Xiongfei Wei <isaacwxf23@gmail.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\n\n* [Model] Add support for GraniteMoeShared models ( vllm-project#13313 )\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [core] moe fp8 block quant tuning support ( vllm-project#14068 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [Misc] Remove lru_cache in NvmlCudaPlatform ( vllm-project#14156 )\n\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [core] Pass all driver env vars to ray workers unless excluded ( vllm-project#14099 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* Use math.prod instead of np.prod for trivial ops ( vllm-project#14142 )\n\n* Fix benchmark_moe.py tuning for CUDA devices ( vllm-project#14164 )\n\n* [platform] add debug logging during inferring the device type ( vllm-project#14195 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [sleep mode] error out with expandable_segments ( vllm-project#14189 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [doc] add \"Failed to infer device type\" to faq ( vllm-project#14200 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Restrict MacOS CPU detection ( vllm-project#14210 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [V1][BugFix] Fix remaining sync engine client shutdown errors/hangs ( vllm-project#13869 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [V0][Metrics] Deprecate some questionable request time metrics ( vllm-project#14135 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V1][Molmo] Fix get_multimodal_embeddings() in molmo.py ( vllm-project#14161 )\n\n* add cutlass support for blackwell fp8 gemm ( vllm-project#13798 )\n\n* [TPU][Profiler] Support start_profile/stop_profile in TPU worker ( vllm-project#13988 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\n\n* Fix performance when `--generation-config` is not `None` ( vllm-project#14223 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Frontend] Do `prompt_logprobs` clamping for chat as well as completions ( vllm-project#14225 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Docs] Update Dockerfile dependency image ( vllm-project#14215 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [v1][Metrics] Add design doc ( vllm-project#12745 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [Security] Serialize using safetensors instead of pickle in Mooncake Pipe ( vllm-project#14228 )\n\nSigned-off-by: KuntaiDu <kuntai@uchicago.edu>\n\n* Clean up unused padding_idx variables across many model definitions ( vllm-project#13240 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [ROCm] Disable a few more kernel tests that are broken on ROCm ( vllm-project#14145 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\n\n* [V1][TPU] TPU multimodal model support for ragged attention ( vllm-project#14158 )\n\nSigned-off-by: Michael Goin <mgoin64@gmail.com>\n\n* [misc] announce china meetup ( vllm-project#14248 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Moved numba from common requirements to cuda/rocm specific requirements ( vllm-project#14199 )\n\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\n\n* Disable GPTQ AllSpark kernels for CUDA Compiler < 12.0 ( vllm-project#14157 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Bugfix] Fix gptq_marlin for deepseek-v3 ( vllm-project#13750 )\n\nSigned-off-by: dangshunya <dangshunya@baichuan-inc.com>\nCo-authored-by: dangshunya <dangshunya@baichuan-inc.com>\n\n* [V1][Bugfix] Do not reset prefix caching metrics ( vllm-project#14235 )\n\n* [Model] New model support for Phi-4-multimodal-instruct ( vllm-project#14119 )\n\n* [V1] EP/TP MoE + DP Attention ( vllm-project#13931 )\n\n* [platforms] improve rocm debugging info ( vllm-project#14257 )\n\n* Temporarily disable test_awq_gemm_opcheck ( vllm-project#14251 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Frontend] Allow return_tokens_as_token_ids to be passed as a request param ( vllm-project#14066 )\n\nSigned-off-by: Benjamin Chislett <benjamin.chislett@centml.ai>\n\n* [Misc][V1] Avoid using `envs.VLLM_USE_V1` in mm processing ( vllm-project#14256 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix][V1] Fix allowed_token_ids for v1 Sampler ( vllm-project#14169 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [Doc] Update nginx guide: remove privileged from vllm container run and add target GPU ID ( vllm-project#14217 )\n\nSigned-off-by: Iacopo Poli <iacopo@lighton.ai>\n\n* [Doc] [3/N] Refer code examples for common cases in dev multimodal processor ( vllm-project#14278 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Small update for external_launcher backend docs ( vllm-project#14288 )\n\n* [V1][Frontend] Add Testing For V1 Runtime Parameters ( vllm-project#14159 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [LoRA] Remove linear hack outside transformers backend ( vllm-project#14177 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Misc] Add Qwen2MoeForCausalLM moe tuning support  ( vllm-project#14276 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* prefix_caching.md: Fixed typo ( vllm-project#14293 )\n\nSigned-off-by: Daivid Savernin-Frenk <daivid.frank@TurboNext.ai>\n\n* [Bugfix] Fix broken vision language example ( vllm-project#14292 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Docs] Add Meta Slides ( vllm-project#14297 )\n\nSigned-off-by: simon-mo <simon.mo@hey.com>\n\n* [V1][Minor] Remove obsolete FIXME comment ( vllm-project#14304 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* Deprecate `best_of` Sampling Parameter in anticipation for vLLM V1 ( vllm-project#13997 )\n\nSigned-off-by: vincent-4 <vincentzhongy+githubvincent4@gmail.com>\nSigned-off-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [V1][BugFix] Fix for mixed top_k batch ( vllm-project#14301 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n\nCo-authored-by: Ye Cao <caoye.cao@alibaba-inc.com>\n\n* [misc] Add FlashMLA as a new option of VLLM_ATTENTION_BACKEND env ( vllm-project#14267 )\n\n* [V1][Easy] Add empty allowed_token_ids in the v1 sampler test ( vllm-project#14308 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* init\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\n\n* [Bugfix] Fix DeepSeek MTP crash when using TP1ModelRunner with CUDA graph due to shape mismatch ( vllm-project#14237 )\n\nSigned-off-by: pyc96 <pychen96@gmail.com>\n\n* [Bugfix] Remove num_tokens_across_dp ( vllm-project#14302 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [BugFix] Fix prefix caching V0 MLA ( vllm-project#14255 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Ying Zhong <zhongyingmatrix@gmail.com>\n\n* [CI/Build] Use spawn multiprocessing mode for V1 test pipeline ( vllm-project#14243 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Add benchmark for DeepGEMM and vLLM Block FP8 Dense GEMM ( vllm-project#13917 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Build] Add UV_HTTP_TIMEOUT to avoid timeout during installation ( vllm-project#13850 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [BugFix] MLA + V1, illegal memory access and accuracy issues ( vllm-project#14253 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\n* [misc] Mention `ray list nodes` command to troubleshoot ray issues ( vllm-project#14318 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [Bugfix][Structured Output] Support outlines engine with reasoning outputs for DeepSeek R1 ( vllm-project#14114 )\n\n* [V1] LoRA - Enable more V1 tests ( vllm-project#14315 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Bugfix][CI] ALiBi test case in xformers multi_query_kv_attention ( vllm-project#11301 )\n\n* [Hardware] Update the flash attn tag to support Blackwell ( vllm-project#14244 )\n\n* [Model] Update Paligemma multimodal processing with PromptUpdate  ( vllm-project#14015 )\n\nSigned-off-by: Kyle Huang <kylhuang@nvidia.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [V1][VLM][Pixtral-HF] Support Pixtral-HF on V1 ( vllm-project#14275 )\n\nSigned-off-by: Linkun Chen <github@lkchen.net>\n\n* [Core] Optimizing cross-attention `QKVParallelLinear` computation ( vllm-project#12325 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: NickLucche <nick@nlucches-4xa100.c.openshift-330514.internal>\nCo-authored-by: NickLucche <nick@nlucches-4xa100.c.openshift-330514.internal>\n\n* [Frontend][Docs] Transcription API streaming ( vllm-project#13301 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [Doc] Update reasoning with stream example to use OpenAI library ( vllm-project#14077 )\n\nSigned-off-by: liuyanyi <wolfsonliu@163.com>\n\n* [Doc] Correct beam_search using in generative_models.md ( vllm-project#14363 )\n\n* [Kernel] [V1] Improved performance for V1 Triton (ROCm) backend  ( vllm-project#14152 )\n\n* [Bugfix][Core] fix abort_seq_group and memory leak when n>1 ( vllm-project#14326 )\n\nSigned-off-by: courage17340 <courage17340@163.com>\n\n* [Core] Don't use cache during multi-modal profiling ( vllm-project#14336 )\n\n* [Doc] Fix date typo in README.md ( vllm-project#14366 )\n\nSigned-off-by: Jitse Klomp <jitse.klomp@conclusionxforce.nl>\n\n* [RLHF] use worker_extension_cls for compatibility with V0 and V1 ( vllm-project#14185 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Reinstate `best_of` for V0 ( vllm-project#14356 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Adding cpu inference with VXE ISA for s390x architecture ( vllm-project#12613 )\n\nSigned-off-by: Dilip Gowda Bhagavan <dilip.bhagavan@ibm.com>\nSigned-off-by: Rishika Kedia <rishika.kedia@in.ibm.com>\nCo-authored-by: Rishika Kedia <rishika.kedia@in.ibm.com>\n\n* Add authors to license header. ( vllm-project#14371 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Burkhard Ringlein <ngl@zurich.ibm.com>\nCo-authored-by: Jan van Lunteren <jvl@zurich.ibm.com>\n\n* Fix mla prefill context performance ( vllm-project#13897 )\n\nSigned-off-by: ZhongYingMatrix <zhongyingmatrix@gmail.com>\n\n* [V1] Do not detokenize if sampling param detokenize is False ( vllm-project#14224 )\n\nSigned-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\n\n* [Distributed] Add enable_expert_parallel arg ( vllm-project#14305 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [CI/Build] Use uv python for docker rather than ppa:deadsnakes/ppa ( vllm-project#13569 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [CI] Disable spawn when running V1 Test ( vllm-project#14345 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Kernel] Add needs_fixed_stride_order tag to most GEMMs ( vllm-project#14306 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Bugfix] Fix use_direct_call condition in FusedMoE layer for  ( vllm-project#14382 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Bug] Fix Attention when ignored in by quant_method ( vllm-project#14313 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [V1][Bugfix] Standardize quantized kv cache rejection for attention backends ( vllm-project#14221 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Docs] Add nsight guide to profiling docs ( vllm-project#14298 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* cleanup boolean logic\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\n\n* [Hardware][TPU]Enable ragged paged attention kernel and resolve recompilation issue ( vllm-project#14310 )\n\nSigned-off-by: Chengji Yao <chengjiyao@google.com>\n\n* [Doc] Fix a typo ( vllm-project#14385 )\n\n* [Bugfix] Correctly call `cudaProfilerStop` in benchmarks script ( vllm-project#14183 )\n\nSigned-off-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* [Perf] Reduce MLA CPU overheads in V1 ( vllm-project#14384 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\n* [FP8] Refactor apply_fp8_linear and apply_fp8_linear_generic into an object ( vllm-project#14390 )\n\nSigned-off-by: luka <luka@neuralmagic.com>\n\n* [BugFix] Illegal Memory Access in the blockwise cutlass fp8 GEMMs ( vllm-project#14396 )\n\n* [Bugfix] Fix JambaForCausalLM LoRA  ( vllm-project#14370 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Build] Add nightly wheel fallback when latest commit wheel unavailable ( vllm-project#14358 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* OpenVINO: added CPU-like conditions ( vllm-project#14338 )\n\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\n\n* [GH] Auto-apply multi-modality label to relevant PRs ( vllm-project#14402 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* correct wrong markdown syntax ( vllm-project#14414 )\n\nSigned-off-by: vincent-pli <justdoit.pli@gmail.com>\n\n* [Bugfix] Further clean up LoRA test ( vllm-project#14422 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Bugfix] Clean up multi-modal processors ( vllm-project#14417 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] Set default value of seed to None ( vllm-project#14274 )\n\nSigned-off-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\n\n* [BUGFIX] Skip tokenization support for throughput benchmark ( vllm-project#12712 )\n\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\n\n* Fix missing `kv_caches` and `attn_metadata` in `OpenVINOCausalLM` ( vllm-project#14271 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Use the optimized block sizes after tuning the kernel. ( vllm-project#14329 )\n\n* [V1][Core] Support for Structured Outputs ( vllm-project#12388 )\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\n\n* [Doc] Update prefix_caching.md to match the example image ( vllm-project#14420 )\n\n* [Benchmarks] Make detokenization optional in benchmark scripts ( vllm-project#11697 )\n\nSigned-off-by: Jeremy Arnold <Jeremy.Arnold@amd.com>\n\n* comments\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\n\n* [Kernel] optimize performance of gptq marlin kernel when n is small ( vllm-project#14138 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\n\n* [Misc] Add Phi4-MM example ( vllm-project#14343 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [v1] torch.compile integration explanation ( vllm-project#14437 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] Eagerly remove finished requests from the batch ( vllm-project#14388 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [V1][Metrics] Fix traceback with preemptions+LoRA ( vllm-project#14220 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [Bugfix] Fix torch_xla which can't handle None seed introduced in vllm-project#14274 ( vllm-project#14459 )\n\nSigned-off-by: Yarong Mu <ymu@google.com>\n\n* [V1] Prompt logprobs + APC compatibility; prompt logprobs reqs cannot fill APC ( vllm-project#13949 )\n\n* [Bugfix][V1] Handle MLA in kv_cache_interface ( vllm-project#14462 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Revert \"[Perf] Reduce MLA CPU overheads in V1 ( vllm-project#14384 )\" ( vllm-project#14471 )\n\n* [Bugfix][Disaggregated] Add a check in send_kv_caches_and_hidden_states and fix the reshape of the KVCache ( vllm-project#14369 )\n\nSigned-off-by: Mathis Felardos <mathis@mistral.ai>\n\n* [MISC][V1] Register process killing handler only in the main thread ( vllm-project#14380 )\n\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [core] add `extra_args` to `SamplingParams` ( vllm-project#13300 )\n\nSigned-off-by: Aviv Keshet <akeshet@scaledcognition.com>\n\n* [CI/Build] refactor: set timezone of container to UTC ( vllm-project#12888 )\n\nSigned-off-by: Roger Meier <r.meier@siemens.com>\n\n* Default to `generation_config` from model ( vllm-project#12622 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Doc]add doc for Qwen models tool calling ( vllm-project#14478 )\n\nSigned-off-by: WangErXiao <863579016@qq.com>\n\n* [Doc] Added QwQ-32B to the supported models list in the reasoning out‚Ä¶ ( vllm-project#14479 )\n\nSigned-off-by: WangErXiao <863579016@qq.com>\n\n* [Bugfix] Make the deviceprofiler include LoRA memory. ( vllm-project#14469 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* Add training doc signposting to TRL ( vllm-project#14439 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Build/BugFix] Fix hopper 12.8 build ( vllm-project#14354 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Add RLHF document ( vllm-project#14482 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [CI/Build] Use a fixed seed to avoid flaky tests ( vllm-project#14480 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] TPU - Add tensor parallel support via Ray ( vllm-project#13618 )\n\nSigned-off-by: Alexander Matveev <amatveev@redhat.com>\n\n* [VLM] Add TP support for Phi-4-MM ( vllm-project#14453 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Misc] add `use_tqdm_on_load` to reduce logs ( vllm-project#14407 )\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\n\n* [V1][Core] Fix memory issue with logits & sampling ( vllm-project#13776 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [benchmarks] Add option to use unique jsonschema for each request ( vllm-project#14457 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Misc] Don't run ruff at all on 3rd party libs ( vllm-project#14493 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Move requirements into their own directory ( vllm-project#12547 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Bugfix] DeepSeek Accuracy ( vllm-project#14476 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\n* [Bugfix] Fix profiling OOM and decouple encoder multimodal profiling ( vllm-project#14361 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Update CODEOWNERS for structured output ( vllm-project#14496 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Misc] Upgrade to Python 3.9 typing for additional directories ( vllm-project#14492 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Support bad_words in sampler ( vllm-project#13376 )\n\nSigned-off-by: 22quinn <33176974+22quinn@users.noreply.github.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\n\n* Revert \"[V1][Core] Fix memory issue with logits & sampling\" ( vllm-project#14504 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Attention] Default to FlashMLA backend for MLA ( vllm-project#14451 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [V1][TPU] Remove unnecessary padding for running on TPU. ( vllm-project#14467 )\n\n* [Feat] Support chunked prefill for LMCache connector ( vllm-project#14505 )\n\nSigned-off-by: YaoJiayi <120040070@link.cuhk.edu.cn>\n\n* [Bugfix] Fix tqdm progress bar when SamplingParams.n > 1 ( vllm-project#12428 )\n\nSigned-off-by: Yuchen Yan <740987012@qq.com>\n\n* [Bugfix] Revert QKVCrossParallelLinear usage in Mllama to keep BNB quantization work ( vllm-project#14498 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][TPU] Fix the recompiling issue in logits processor after warmup ( vllm-project#14510 )\n\nSigned-off-by: Chengji Yao <chengjiyao@google.com>\n\n* [Misc] Ensure out-of-tree quantization method recognize by cli args ( vllm-project#14328 )\n\nSigned-off-by: liuyanyi <wolfsonliu@163.com>\n\n* [Bugfix] Wrong requirements path - rocm ( vllm-project#14527 )\n\nSigned-off-by: Martin Hoyer <mhoyer@redhat.com>\n\n* [Feature] Consolidate performance benchmark datasets ( vllm-project#14036 )\n\nSigned-off-by: Jennifer Zhao <7443418+JenZhao@users.noreply.github.com>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Jennifer Zhao <7443418+JenZhao@users.noreply.github.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Misc] Add log information for handle_process_request. ( vllm-project#14130 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\n\n* [Docs] Mention `model_impl` arg when explaining Transformers fallback ( vllm-project#14552 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Frontend] support image embeds ( vllm-project#13955 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\n\n* [Kernel] Add more dtype support for GGUF kernels ( vllm-project#14043 )\n\nSigned-off-by: SzymonOzog <szymon.ozog@aleph-alpha.com>\nSigned-off-by: SzymonOzog <szymon.ozog@gmail.com>\n\n* [Doc] Update PaliGemma note to a warning ( vllm-project#14565 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* V1 rocm support ( #469 )\n\n* Initial commit for V1 successfull compilation\n\n* Small improvement for linear\n\n* Small improvement for linear\n\n* making use of forward_cuda for all except ROPE in llama\n\n---------\n\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* nightly_fixed_aiter_integration_final_20250305 README update ( #470 )\n\n* nightly_fixed_aiter_integration_final_20250305 README update (perf results only)\n\n* Update Docker Manifest git hash\n\n* Update Docker Manifest and added nightly_fixed_aiter_integration_final_20250305\n\n* some more updates\n\n* Update AITER section with example\n\n* Updated AITER command with larger batch size and model name\n\n* Fixing typo\n\n* Removed --max-model-len in AITER command\n\n* Updating AITER instructions\n\n* typo\n\n* Another typo\n\n* Whitespace\n\n* modifying whats new section\n\n* Another typo\n\n---------\n\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Xiongfei Wei <isaacwxf23@gmail.com>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\nSigned-off-by: KuntaiDu <kuntai@uchicago.edu>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: Michael Goin <mgoin64@gmail.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: dangshunya <dangshunya@baichuan-inc.com>\nSigned-off-by: Benjamin Chislett <benjamin.chislett@centml.ai>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Lu Fang <lufang@fb.com>\nSigned-off-by: Iacopo Poli <iacopo@lighton.ai>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Daivid Savernin-Frenk <daivid.frank@TurboNext.ai>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: vincent-4 <vincentzhongy+githubvincent4@gmail.com>\nSigned-off-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nSigned-off-by: pyc96 <pychen96@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nSigned-off-by: Kyle Huang <kylhuang@nvidia.com>\nSigned-off-by: Linkun Chen <github@lkchen.net>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: NickLucche <nick@nlucches-4xa100.c.openshift-330514.internal>\nSigned-off-by: liuyanyi <wolfsonliu@163.com>\nSigned-off-by: courage17340 <courage17340@163.com>\nSigned-off-by: Jitse Klomp <jitse.klomp@conclusionxforce.nl>\nSigned-off-by: Dilip Gowda Bhagavan <dilip.bhagavan@ibm.com>\nSigned-off-by: Rishika Kedia <rishika.kedia@in.ibm.com>\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nSigned-off-by: ZhongYingMatrix <zhongyingmatrix@gmail.com>\nSigned-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: Chengji Yao <chengjiyao@google.com>\nSigned-off-by: luka <luka@neuralmagic.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: vincent-pli <justdoit.pli@gmail.com>\nSigned-off-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: Jeremy Arnold <Jeremy.Arnold@amd.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Yarong Mu <ymu@google.com>\nSigned-off-by: Mathis Felardos <mathis@mistral.ai>\nSigned-off-by: Aviv Keshet <akeshet@scaledcognition.com>\nSigned-off-by: Roger Meier <r.meier@siemens.com>\nSigned-off-by: WangErXiao <863579016@qq.com>\nSigned-off-by: Alexander Matveev <amatveev@redhat.com>\nSigned-off-by: 22quinn <33176974+22quinn@users.noreply.github.com>\nSigned-off-by: YaoJiayi <120040070@link.cuhk.edu.cn>\nSigned-off-by: Yuchen Yan <740987012@qq.com>\nSigned-off-by: Martin Hoyer <mhoyer@redhat.com>\nSigned-off-by: Jennifer Zhao <7443418+JenZhao@users.noreply.github.com>\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\nSigned-off-by: SzymonOzog <szymon.ozog@aleph-alpha.com>\nSigned-off-by: SzymonOzog <szymon.ozog@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: iefgnoix <isaacwxf23@gmail.com>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Zhanwen Chen <phil.zhanwen.chen@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: lkchen <github@lkchen.net>\nCo-authored-by: kushanam <42385577+kushanam@users.noreply.github.com>\nCo-authored-by: Siyuan Liu <lsiyuan@google.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: rainkert <93575312+rainkert@users.noreply.github.com>\nCo-authored-by: dangshunya <dangshunya@baichuan-inc.com>\nCo-authored-by: Congcong Chen <congcongchen@microsoft.com>\nCo-authored-by: Benjamin Chislett <benjamin.chislett@centml.ai>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Lu Fang <30275821+houseroad@users.noreply.github.com>\nCo-authored-by: Iacopo Poli <iacopo@lighton.ai>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Zhe Zhang <zhz@apache.org>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: DaividFrank <49250948+DaividFrank@users.noreply.github.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Vincent <vincentzhongy+githubvincent4@gmail.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Ye Cao <caoye.cao@alibaba-inc.com>\nCo-authored-by: Serena <yangsijia.614@bytedance.com>\nCo-authored-by: pyc96 <pychen96@gmail.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Ying Zhong <zhongyingmatrix@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Ce Gao <cegao@tensorchord.ai>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: Pavani Majety <pmajety@nvidia.com>\nCo-authored-by: kYLe <kylhuang@nvidia.com>\nCo-authored-by: NickLucche <nick@nlucches-4xa100.c.openshift-330514.internal>\nCo-authored-by: Yanyi Liu <wolfsonliu@163.com>\nCo-authored-by: Irina Yuryeva <76484191+upayuryeva@users.noreply.github.com>\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: courage17340 <courage17340@users.noreply.github.com>\nCo-authored-by: Jitse Klomp <jitse.klomp@conclusionxforce.nl>\nCo-authored-by: Dilip Gowda Bhagavan <110233170+dilipgb@users.noreply.github.com>\nCo-authored-by: Rishika Kedia <rishika.kedia@in.ibm.com>\nCo-authored-by: Burkhard Ringlein <ngl@zurich.ibm.com>\nCo-authored-by: Jan van Lunteren <jvl@zurich.ibm.com>\nCo-authored-by: Himanshu Jaju <hj@mistral.ai>\nCo-authored-by: Chengji Yao <chengjiyao@google.com>\nCo-authored-by: Daniel Li <dyli@google.com>\nCo-authored-by: Luka Govediƒç <ProExpertProg@users.noreply.github.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Peng Li <justdoit.pli@gmail.com>\nCo-authored-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Aaron Pham <contact@aarnphm.xyz>\nCo-authored-by: York-RDWang <103811994+York-RDWang@users.noreply.github.com>\nCo-authored-by: Jeremy Arnold <103538711+JArnoldAMD@users.noreply.github.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: yarongmu-google <150371854+yarongmu-google@users.noreply.github.com>\nCo-authored-by: afeldman-nm <156691304+afeldman-nm@users.noreply.github.com>\nCo-authored-by: Mathis Felardos <mathis@mistral.ai>\nCo-authored-by: Aviv Keshet <akeshet@scaledcognition.com>\nCo-authored-by: Roger Meier <r.meier@siemens.com>\nCo-authored-by: Robin <863579016@qq.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-redhat@users.noreply.github.com>\nCo-authored-by: 22quinn <33176974+22quinn@users.noreply.github.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Jiayi Yao <82156730+YaoJiayi@users.noreply.github.com>\nCo-authored-by: Yuchen Yan <50619811+yanyc428@users.noreply.github.com>\nCo-authored-by: Martin Hoyer <mhoyer@redhat.com>\nCo-authored-by: Jennifer Zhao <JenZhao@users.noreply.github.com>\nCo-authored-by: Jennifer Zhao <7443418+JenZhao@users.noreply.github.com>\nCo-authored-by: Chauncey <chaunceyjiang@gmail.com>\nCo-authored-by: Szymon O≈º√≥g <58388001+SzymonOzog@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Mcirino1 <57415822+Mcirino1@users.noreply.github.com>\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 Fix performance when --generation-config is not None ( vllm-projec‚Ä¶ ‚Ä¶ be31e4d ‚Ä¶t#14223 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 Fix performance when --generation-config is not None ( vllm-projec‚Ä¶ ‚Ä¶ b12de09 ‚Ä¶t#14223 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:18",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, Serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:52:18",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.2-1B-Instruct --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
  "commit_subject": "Fix performance when `--generation-config` is not `None` (#14223)",
  "commit_message": "Fix performance when `--generation-config` is not `None` (#14223)\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
  "commit_date": "2025-03-04T20:59:22+01:00",
  "files_changed": [
    "vllm/entrypoints/llm.py",
    "vllm/entrypoints/openai/serving_chat.py",
    "vllm/entrypoints/openai/serving_completion.py",
    "vllm/entrypoints/openai/serving_transcription.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 4,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 4,
    "num_hunks": 8,
    "num_edited_lines": 48,
    "num_non_test_edited_lines": 48,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed86..fc585ee9e 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -244,6 +244,7 @@ class LLM:\n             engine_args, usage_context=UsageContext.LLM_CLASS)\n \n         self.request_counter = Counter()\n+        self.default_sampling_params: Union[dict[str, Any], None] = None\n \n     @staticmethod\n     def get_engine_class() -> type[LLMEngine]:\n@@ -268,10 +269,11 @@ class LLM:\n             tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n \n     def get_default_sampling_params(self) -> SamplingParams:\n-        diff_sampling_param = (\n-            self.llm_engine.model_config.get_diff_sampling_param())\n-        if diff_sampling_param:\n-            return SamplingParams.from_optional(**diff_sampling_param)\n+        if self.default_sampling_params is None:\n+            self.default_sampling_params = (\n+                self.llm_engine.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n+            return SamplingParams.from_optional(**self.default_sampling_params)\n         return SamplingParams()\n \n     @overload\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0fc..f4aaee360 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -105,10 +105,11 @@ class OpenAIServingChat(OpenAIServing):\n                                 \"been registered\") from e\n \n         self.enable_prompt_tokens_details = enable_prompt_tokens_details\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = (\n+            self.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n             logger.info(\"Overwriting default chat sampling param with: %s\",\n-                        diff_sampling_param)\n+                        self.default_sampling_params)\n \n     async def create_chat_completion(\n         self,\n@@ -210,17 +211,14 @@ class OpenAIServingChat(OpenAIServing):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n-                # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n-                        default_max_tokens, default_sampling_params)\n+                        default_max_tokens, self.default_sampling_params)\n                 else:\n                     sampling_params = request.to_sampling_params(\n                         default_max_tokens,\n                         self.model_config.logits_processor_pattern,\n-                        default_sampling_params)\n+                        self.default_sampling_params)\n \n                 self._log_inputs(request_id,\n                                  request_prompts[i],\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af84f..b2ad28c0a 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -51,11 +51,12 @@ class OpenAIServingCompletion(OpenAIServing):\n                          models=models,\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = (\n+            self.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def create_completion(\n         self,\n@@ -119,17 +120,14 @@ class OpenAIServingCompletion(OpenAIServing):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n-                # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n-                        default_max_tokens, default_sampling_params)\n+                        default_max_tokens, self.default_sampling_params)\n                 else:\n                     sampling_params = request.to_sampling_params(\n                         default_max_tokens,\n                         self.model_config.logits_processor_pattern,\n-                        default_sampling_params)\n+                        self.default_sampling_params)\n \n                 request_id_item = f\"{request_id}-{i}\"\n \ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a5e..402a0bb7a 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -161,11 +161,12 @@ class OpenAIServingTranscription(OpenAIServing):\n                          request_logger=request_logger,\n                          return_tokens_as_token_ids=return_tokens_as_token_ids)\n \n-        diff_sampling_param = self.model_config.get_diff_sampling_param()\n-        if diff_sampling_param:\n+        self.default_sampling_params = (\n+            self.model_config.get_diff_sampling_param())\n+        if self.default_sampling_params:\n             logger.info(\n                 \"Overwriting default completion sampling param with: %s\",\n-                diff_sampling_param)\n+                self.default_sampling_params)\n \n     async def _preprocess_transcription(\n         self,\n@@ -273,9 +274,8 @@ class OpenAIServingTranscription(OpenAIServing):\n         try:\n             # TODO(rob): subtract len of tokenized prompt.\n             default_max_tokens = self.model_config.max_model_len\n-            default_params = self.model_config.get_diff_sampling_param()\n             sampling_params = request.to_sampling_params(\n-                default_max_tokens, default_params)\n+                default_max_tokens, self.default_sampling_params)\n \n             self._log_inputs(\n                 request_id,",
  "apis": [
    "vLLM.LLM.get_default_sampling_params",
    "OpenAIServingChat.create_chat_completion",
    "OpenAIServingCompletion.create_completion",
    "OpenAIServingTranscription._preprocess_transcription"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/llm.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/serving_chat.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/serving_completion.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies several non-test source files (llm.py, serving_chat.py, serving_completion.py, serving_transcription.py). It changes how default sampling parameters are handled by caching the result of get_diff_sampling_param() into an instance variable (default_sampling_params) rather than invoking it repeatedly. This refactoring is aimed at reducing redundant computations and thereby optimizing performance. The changes go beyond mere refactoring or comment updates, impacting the performance of core functions related to API calls on CPU without relying on GPU. Overall, the commit meets the performance/optimization criteria.",
  "llm_api_reason": "This commit introduces a caching optimization by adding a new instance variable called default_sampling_params that stores the result of self.model_config.get_diff_sampling_param(). Instead of calling the model configuration repeatedly, the methods now check if default_sampling_params is already set and reuse it. This change appears in the LLM class (in its get_default_sampling_params method) and in several OpenAIServing endpoint classes (namely, in create_chat_completion in OpenAIServingChat, create_completion in OpenAIServingCompletion, and in the transcription preprocessing in OpenAIServingTranscription), which all use the cached default_sampling_params for building sampling parameters. This improvement reduces redundant configuration lookups and should enhance performance when --generation-config is not None."
}