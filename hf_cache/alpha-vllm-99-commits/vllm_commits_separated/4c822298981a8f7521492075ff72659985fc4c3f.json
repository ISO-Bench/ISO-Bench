{
  "commit_hash": "4c822298981a8f7521492075ff72659985fc4c3f",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13365",
  "pr_date": "2025-02-18",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Feb 17, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes the N-gram matching algorithm by JIT compiling it with Numba. I've observed 20-30x speedup with large batch sizes: For ShareGPT benchmark with 5K requests, the cumulative overhead reduces from 54.3 sec to 1.9 sec, which is ~2.5% of the entire running time. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸš€ 5 njhill, ywang96, LiuXiaoxuanPKU, michaelfeil, and mgoin reacted with rocket emoji All reactions ðŸš€ 5 reactions WoosukKwon added 9 commits February 15, 2025 12:54 [V1] Get input tokens from scheduler â€¦ 8406f11 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ 0399f09 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-scheduler-input 960964a fix â€¦ c54ff6c Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-scheduler-input aa8ae69 comment â€¦ c833429 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> [V1][Spec decode] Move drafter to model runner â€¦ b42a16f Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-spec-decode 5f13604 [V1][Spec Decode] Optimize N-gram matching with Numba â€¦ 490df6d Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Copy link github-actions bot commented Feb 17, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added ci/build v1 labels Feb 17, 2025 WoosukKwon added 4 commits February 17, 2025 11:18 Merge branch 'main' into v1-spec-decode 58e0856 Merge branch 'v1-spec-decode' into v1-spec-opt 85afbe6 Merge branch 'main' into v1-spec-opt 81456ab update â€¦ c632ad4 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon marked this pull request as ready for review February 17, 2025 23:49 WoosukKwon requested review from robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners February 17, 2025 23:49 WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Feb 17, 2025 Copy link Collaborator Author WoosukKwon commented Feb 17, 2025 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . cc @LiuXiaoxuanPKU This PR is ready. Could you please take a look? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon added 4 commits February 17, 2025 15:54 minor â€¦ 524af01 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Pin numba version â€¦ ca4458d Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-spec-opt 11cceb4 Initialize drafter only for last rank â€¦ 8de56ec Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> LiuXiaoxuanPKU approved these changes Feb 18, 2025 View reviewed changes Copy link Collaborator LiuXiaoxuanPKU left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, thanks! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details WoosukKwon merged commit 4c82229 into main Feb 18, 2025 57 of 71 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the v1-spec-opt branch February 18, 2025 21:20 mgoin reviewed Feb 18, 2025 View reviewed changes requirements-common.txt @@ -1,6 +1,7 @@ psutil sentencepiece  # Required for LLaMA tokenizer. numpy < 2.0.0 numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding. Copy link Member mgoin Feb 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Shouldn't this be in requirements-cuda.txt rather than common? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author WoosukKwon Feb 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Oh I'm ok with either; I just thought it would be eventually used by others as well. Please feel free to submit a PR to move it to requirements-cuda.txt and probably requirements-rocm.txt . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor michaelfeil commented Feb 19, 2025 Very excited about this! ðŸ‘ 1 WoosukKwon reacted with thumbs up emoji All reactions ðŸ‘ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author WoosukKwon commented Feb 19, 2025 @michaelfeil Thanks! Happy to see you again :) We still have some headroom for performance: #13498 Please let us know if you are interested in working on this. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Feb 20, 2025 [V1][Spec Decode] Optimize N-gram matching with Numba ( vllm-project#1â€¦ â€¦ 0c8d213 â€¦3365 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Akshat-Tripathi pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Mar 3, 2025 [V1][Spec Decode] Optimize N-gram matching with Numba ( vllm-project#1â€¦ â€¦ 1104f29 â€¦3365 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1][Spec Decode] Optimize N-gram matching with Numba ( vllm-project#1â€¦ â€¦ 3b3b1db â€¦3365 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [V1][Spec Decode] Optimize N-gram matching with Numba ( vllm-project#1â€¦ â€¦ 0497603 â€¦3365 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:34",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: speedup | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:52:34",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)",
  "commit_message": "[V1][Spec Decode] Optimize N-gram matching with Numba (#13365)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2025-02-18T13:19:58-08:00",
  "files_changed": [
    "requirements-common.txt",
    "vllm/v1/spec_decode/ngram_proposer.py",
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 5,
    "num_edited_lines": 127,
    "num_non_test_edited_lines": 127,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/requirements-common.txt b/requirements-common.txt\nindex b7c94cbdb..c52980bc7 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -1,6 +1,7 @@\n psutil\n sentencepiece  # Required for LLaMA tokenizer.\n numpy < 2.0.0\n+numba == 0.60.0 # v0.61 doesn't support Python 3.9. Required for N-gram speculative decoding.\n requests >= 2.26.0\n tqdm\n blake3\ndiff --git a/vllm/v1/spec_decode/ngram_proposer.py b/vllm/v1/spec_decode/ngram_proposer.py\nindex 9b116e00a..33289d05d 100644\n--- a/vllm/v1/spec_decode/ngram_proposer.py\n+++ b/vllm/v1/spec_decode/ngram_proposer.py\n@@ -1,14 +1,12 @@\n # SPDX-License-Identifier: Apache-2.0\n-from typing import List, Optional\n+from typing import Optional\n \n import numpy as np\n+from numba import jit\n \n \n class NgramProposer:\n \n-    def __init__(self):\n-        pass\n-\n     def propose(\n         self,\n         context_token_ids: np.ndarray,\n@@ -21,7 +19,7 @@ class NgramProposer:\n         that match.\n         \n         Args:\n-            context_token_ids: List of token IDs representing the \n+            context_token_ids: Numpy array of token IDs representing the \n                                context sequence.\n             n: Length of the n-gram to match.\n             k: Number of tokens follow the match. If there are less \n@@ -41,66 +39,65 @@ class NgramProposer:\n               followed that pattern. Here we will return [4,2,3] because \n               we only have three tokens after the match.\n         \"\"\"\n-        # TODO: Use c++ to implement the _find_subarray_kmp to\n-        # improve the efficiency\n-        return self._find_subarray_kmp(context_token_ids, n, k)\n+        return _find_subarray_kmp(context_token_ids, n, k)\n \n-    @staticmethod\n-    def _kmp_lps_array(pattern: List[int]) -> List[int]:\n-        \"\"\"\n-        Build the lps (longest proper prefix which is also suffix) \n-        array for the pattern.\n-        \"\"\"\n-        lps = [0] * len(pattern)\n-        prev_lps = 0  # length of the previous longest prefix suffix\n-        i = 1\n \n-        while i < len(pattern):\n-            if pattern[i] == pattern[prev_lps]:\n-                prev_lps += 1\n-                lps[i] = prev_lps\n-                i += 1\n+@jit(nopython=True)\n+def _kmp_lps_array(pattern: np.ndarray) -> np.ndarray:\n+    \"\"\"\n+    Build the lps (longest proper prefix which is also suffix) \n+    array for the pattern.\n+    \"\"\"\n+    lps = np.zeros(len(pattern), dtype=np.int32)\n+    prev_lps = 0  # length of the previous longest prefix suffix\n+    i = 1\n+\n+    while i < len(pattern):\n+        if pattern[i] == pattern[prev_lps]:\n+            prev_lps += 1\n+            lps[i] = prev_lps\n+            i += 1\n+        else:\n+            if prev_lps != 0:\n+                prev_lps = lps[prev_lps - 1]\n             else:\n-                if prev_lps != 0:\n-                    prev_lps = lps[prev_lps - 1]\n-                else:\n-                    lps[i] = 0\n-                    i += 1\n+                lps[i] = 0\n+                i += 1\n+    return lps\n \n-        return lps\n \n-    @staticmethod\n-    def _find_subarray_kmp(\n-        context_token_ids: np.ndarray,\n-        n: int,\n-        k: int,\n-    ) -> Optional[np.ndarray]:\n-        context_len = context_token_ids.shape[0]\n-        assert n > 0\n+@jit(nopython=True)\n+def _find_subarray_kmp(\n+    context_token_ids: np.ndarray,\n+    n: int,\n+    k: int,\n+) -> Optional[np.ndarray]:\n+    context_len = context_token_ids.shape[0]\n+    assert n > 0\n \n-        pattern = context_token_ids[-n:]\n-        # Precompute lps array for Y\n-        lps = NgramProposer._kmp_lps_array(pattern)\n+    pattern = context_token_ids[-n:]\n+    # Precompute lps array for Y\n+    lps = _kmp_lps_array(pattern)\n \n-        i = 0\n-        j = 0\n-        # -n because the last n tokens are used as pattern\n-        while i < context_len - n:\n-            if context_token_ids[i] == pattern[j]:\n-                i += 1\n-                j += 1\n+    i = 0\n+    j = 0\n+    # -n because the last n tokens are used as pattern\n+    while i < context_len - n:\n+        if context_token_ids[i] == pattern[j]:\n+            i += 1\n+            j += 1\n \n-                # If we have matched the entire Y\n-                if j == n:\n-                    # Found pattern in context, gather the next K elements\n-                    return context_token_ids[i:i + k]\n+            # If we have matched the entire Y\n+            if j == n:\n+                # Found pattern in context, gather the next K elements\n+                return context_token_ids[i:i + k]\n+        else:\n+            # Mismatch\n+            if j != 0:\n+                # Use the lps array to avoid re-checking elements\n+                j = lps[j - 1]\n             else:\n-                # Mismatch\n-                if j != 0:\n-                    # Use the lps array to avoid re-checking elements\n-                    j = lps[j - 1]\n-                else:\n-                    i += 1\n+                i += 1\n \n-        # Y not found\n-        return None\n+    # Y not found\n+    return None\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0ecc00acc..31fe095a9 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -120,11 +120,20 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # Set up speculative decoding.\n         self.use_spec_decode = False\n         if self.speculative_config:\n+            self.use_spec_decode = True\n+\n             # TODO: find a better way to check if we are using ngram.\n             assert self.speculative_config.ngram_prompt_lookup_min, \\\n                     \"Currently, only ngram spec decode is supported in V1.\"\n-            self.drafter = NgramProposer()\n-            self.use_spec_decode = True\n+            if get_pp_group().is_last_rank:\n+                self.drafter = NgramProposer()\n+                # Trigger Numba JIT compilation for N-gram proposer.\n+                # This usually takes less than 1 second.\n+                self.drafter.propose(\n+                    np.zeros(1024, dtype=np.int32),\n+                    self.speculative_config.ngram_prompt_lookup_min,\n+                    self.speculative_config.num_speculative_tokens,\n+                )\n \n         # Request states.\n         self.requests: Dict[str, CachedRequestState] = {}",
  "apis": [
    "vllm.v1.spec_decode.ngram_proposer.NgramProposer.propose",
    "vllm.v1.worker.GPUModelRunner.__init__"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/ngram_proposer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces performance improvements to the N-gram matching functionality by applying numba's JIT compilation to critical internal functions (_kmp_lps_array and _find_subarray_kmp), which are used in the NgramProposer. It modifies source code files (vllm/v1/spec_decode/ngram_proposer.py and vllm/v1/worker/gpu_model_runner.py) and adjusts dependency requirements (requirements-common.txt) by specifying an appropriate numba version. These changes are aimed at speeding up execution performance on CPU, and they are integrated into the core logic rather than being limited to tests, bugfixes, simple refactoring, or documentation updates. Thus, this commit meets the conditions for a performance optimization commit.",
  "llm_api_reason": "This commit updates the common requirements (pinning numba to version 0.60.0) and refactors the N-gram speculative decoding implementation. In the vllm/v1/spec_decode/ngram_proposer.py module the formerly inline KMP helper methods are replaced with Numba-accelerated functions (_kmp_lps_array and _find_subarray_kmp) and the propose() method is modified to call the global jitâ€compiled functions. In addition, the GPUModelRunner initialization now triggers a dummy call to NgramProposer.propose() (thus pre-compiling the Numba code) when speculative decoding is enabled."
}