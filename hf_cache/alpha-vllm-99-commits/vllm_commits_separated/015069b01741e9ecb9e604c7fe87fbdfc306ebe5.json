{
  "commit_hash": "015069b01741e9ecb9e604c7fe87fbdfc306ebe5",
  "pr_url": "https://github.com/vllm-project/vllm/pull/17515",
  "pr_date": "2025-05-01",
  "timeline_text": "Copy link Contributor chaunceyjiang commented May 1, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . FIX #17369 (comment) Use string partition instead of regex Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions [Misc]: Optimize the Qwen3_ReasoningParser extract_reasoning_content â€¦ 493c2a8 Signed-off-by: chaunceyjiang <chaunceyjiang@gmail.com> Copy link github-actions bot commented May 1, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . chaunceyjiang changed the title [Misc]: Optimize the Qwen3_ReasoningParser extract_reasoning_content [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content May 1, 2025 Copy link Contributor Author chaunceyjiang commented May 1, 2025 /cc @gaocegege PTAL. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gaocegege reviewed May 1, 2025 View reviewed changes Copy link Contributor gaocegege left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment We could remove self.reasoning_regex Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions [Misc]: Optimize the Qwen3_ReasoningParser extract_reasoning_content â€¦ d165310 Signed-off-by: chaunceyjiang <chaunceyjiang@gmail.com> Copy link Contributor Author chaunceyjiang commented May 1, 2025 We could remove self.reasoning_regex Done. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gaocegege approved these changes May 1, 2025 View reviewed changes Copy link Contributor Author chaunceyjiang commented May 1, 2025 /cc @DarkLight1337 PTAL. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . DarkLight1337 approved these changes May 1, 2025 View reviewed changes Hide details View details vllm-bot merged commit 015069b into vllm-project : main May 1, 2025 20 of 21 checks passed Uh oh! There was an error while loading. Please reload this page . chaunceyjiang deleted the qwen3_opttimize branch May 1, 2025 10:41 radeksm pushed a commit\n        to radeksm/vllm\n      that referenced\n      this pull request May 2, 2025 [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content ( vâ€¦ â€¦ 2429b35 â€¦llm-project#17515 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content ( vâ€¦ â€¦ 69172c0 â€¦llm-project#17515 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> zzzyq pushed a commit\n        to zzzyq/vllm\n      that referenced\n      this pull request May 24, 2025 [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content ( vâ€¦ â€¦ c15fd57 â€¦llm-project#17515 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\nSigned-off-by: Yuqi Zhang <yuqizhang@google.com> minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content ( vâ€¦ â€¦ 2891c03 â€¦llm-project#17515 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\nSigned-off-by: minpeter <kali2005611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:10",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:51:10",
  "models": [
    "Qwen/Qwen3-7B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen3-7B-Instruct --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-7B-Instruct --dataset-name sharegpt --request-rate 1",
  "commit_subject": "[Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content (#17515)",
  "commit_message": "[Misc] Optimize the Qwen3_ReasoningParser extract_reasoning_content (#17515)\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>",
  "commit_date": "2025-05-01T03:29:01-07:00",
  "files_changed": [
    "vllm/reasoning/qwen3_reasoning_parser.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 3,
    "num_edited_lines": 53,
    "num_non_test_edited_lines": 53,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/reasoning/qwen3_reasoning_parser.py b/vllm/reasoning/qwen3_reasoning_parser.py\nindex f588f4016..7095034b1 100644\n--- a/vllm/reasoning/qwen3_reasoning_parser.py\n+++ b/vllm/reasoning/qwen3_reasoning_parser.py\n@@ -1,6 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n \n-import re\n from collections.abc import Sequence\n from typing import Optional, Union\n \n@@ -31,9 +30,6 @@ class Qwen3ReasoningParser(ReasoningParser):\n         self.think_start_token = \"<think>\"\n         self.think_end_token = \"</think>\"\n \n-        self.reasoning_regex = re.compile(\n-            rf\"{self.think_start_token}(.*?){self.think_end_token}\", re.DOTALL)\n-\n         if not self.model_tokenizer:\n             raise ValueError(\n                 \"The model tokenizer must be passed to the ReasoningParser \"\n@@ -121,29 +117,34 @@ class Qwen3ReasoningParser(ReasoningParser):\n     def extract_reasoning_content(\n             self, model_output: str, request: ChatCompletionRequest\n     ) -> tuple[Optional[str], Optional[str]]:\n+        \"\"\"\n+        Extract reasoning content from the model output.\n+\n+        For text <think>abc</think>xyz:\n+        - 'abc' goes to reasoning_content\n+        - 'xyz' goes to content\n \n-        # Check if the model output contains the <think> tokens.\n+        Returns:\n+            tuple[Optional[str], Optional[str]]: reasoning content and content\n+        \"\"\"\n+\n+        # Check if the model output contains the <think> and </think> tokens.\n         if (self.think_start_token not in model_output\n                 or self.think_end_token not in model_output):\n             return None, model_output\n-        else:\n-            # Use a regex to find the reasoning content\n-            reasoning_content = self.reasoning_regex.findall(model_output)[0]\n-\n-            # Remove the reasoning content from the model output\n-            # Although <think> token is always at the\n-            # beginning of the line, we cannot guarantee that the\n-            # other models will follow this convention.\n-            # Therefore, we need to add :start_index.\n-            start_index = model_output.find(self.think_start_token)\n-            if start_index != -1:\n-                end_index = start_index + len(\n-                    f\"{self.think_start_token}{reasoning_content}{self.think_end_token}\"\n-                )\n-                model_output = model_output[:start_index] + \\\n-                                model_output[end_index:]\n-\n-                if len(model_output) == 0:\n-                    return reasoning_content, None\n-\n-            return reasoning_content, model_output\n+        # Check if the <think> is present in the model output, remove it\n+        # if it is present.\n+        model_output_parts = model_output.partition(self.think_start_token)\n+        model_output = model_output_parts[2] if model_output_parts[\n+            1] else model_output_parts[0]\n+        # Check if the model output contains the </think> tokens.\n+        # If the end token is not found, return the model output as is.\n+        if self.think_end_token not in model_output:\n+            return None, model_output\n+\n+        # Extract reasoning content from the model output.\n+        reasoning_content, _, content = model_output.partition(\n+            self.think_end_token)\n+\n+        final_content = content or None\n+        return reasoning_content, final_content",
  "apis": [
    "Qwen3ReasoningParser.extract_reasoning_content"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/reasoning/qwen3_reasoning_parser.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/openai_chat_completion_with_reasoning.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/openai_chat_completion_with_reasoning_streaming.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a core (non-test) source file by changing the implementation of the extract_reasoning_content function in the Qwen3ReasoningParser. The changes remove the use of regular expressions (which can be computationally heavier) and instead leverage string partitioning to extract content, which is typically more efficient. This alteration is intended to optimize the performance of the reasoning extraction process, satisfying the criteria for performance optimization (CPU-based and amenable to testing without specialized hardware) without being merely a trivial refactor or documentation fix.",
  "llm_api_reason": "The commit removes the regex-based extraction logic from the extract_reasoning_content method of the Qwen3ReasoningParser class, optimizing how reasoning content is parsed from the modelâ€™s output by using string partitioning instead. This update improves performance while maintaining the same API interface."
}