{
  "commit_hash": "e493e48524e9e78ab33eafec6461b3940e361189",
  "pr_url": "https://github.com/vllm-project/vllm/pull/17731",
  "pr_date": "2025-05-06",
  "timeline_text": "Copy link Contributor shadeMe commented May 6, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . ParallelSampleSequenceGroup.add_request has to copy the original SamplingParams instance as many times as the number of requested samples. This is currently done with a copy.deepcopy call, which is not advisable as the the logits_processors field could contain arbitrary Python objects with expensive-to-copy state. This happens to be the case with the current guided decoding logits processors, scaling linearly with the value of SamplingParams.n and introducing a bottleneck in the hot path. A similar issue was previous identified, and SamplingParams.clone was introduced to workaround this issue - it attempts to call a clone function on each logits processor object, with the assumption that classes can implement this method to minimize the overhead by performing shallow copies when possible. However, not all existing logits processors implement this method. Nor does the ParallelSampleSequenceGroup class avail itself of the SamplingParams.clone method. This commit introduces the following changes: Modify ParallelSampleSequenceGroup.add_request to call SamplingParams.clone instead of copy.deepcopy . Update the logits processors of the guidance , outlines and xgrammar backends to expose a clone method for the efficient copying of mutable state. The lm-format-enforcer backend was left untouched as the logits processor implementation is external to vLLM. Benchmark For text generation w/t Nvidia L4, Phi-1.5, n=3 in an async setup, we see the ParallelSampleSequenceGroup.add_request call dominating the runtime during a 180 second profile (after warm-up/with in-flight requests) of the original code (anywhere b'ween 60%-86% of the total runtime depending on the backend). With the above changes, this is essentially eliminated (0.01%-0.6%). Guidance Outlines Xgrammar Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 3 Xarbirus, chaunceyjiang, and dtransposed reacted with thumbs up emoji üéâ 1 bi1101 reacted with hooray emoji All reactions üëç 3 reactions üéâ 1 reaction [Bugfix] Fix parallel sampling performance regression when guided dec‚Ä¶ ‚Ä¶ 59f7675 ‚Ä¶oding is enabled\n\n`ParallelSampleSequenceGroup.add_request` has to copy the original `SamplingParams` instance as\nmany times as the number of requested samples. This is currently done with a `copy.deepcopy` call,\nwhich is not advisable as the the `logits_processors` field could contain arbitrary Python objects\nwith expensive-to-copy state. This happens to be the case with the current guided decoding logits\nprocessors, scaling linearly with the value of `SamplingParams.n` and introducing a bottleneck in the\nhot path.\n\nA similar issue was previous identified, and `SamplingParams.clone` was introduced to workaround this\nissue - it attempts to call a `clone` function on each logits processor object, with the assumption that\nclasses can implement this method to minimize the overhead by performing shallow copies when possible.\nHowever, not all existing logits processors implement this method. Nor does the `ParallelSampleSequenceGroup`\nclass avail itself of the `SamplingParams.clone` method.\n\nThis commit introduces the following changes:\n* Modify `ParallelSampleSequenceGroup.add_request` to call `SamplingParams.clone` instead of `copy.deepcopy`.\n* Update the logits processors of the `guidance`, `outlines` and `xgrammar` backends to expose a `clone` method\n  for the efficient copying of mutable state.\n\nThe `lm-format-enforcer` backend was left untouched as the logits processor implementation is external to\nvLLM.\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com> shadeMe requested review from mgoin and russellb as code owners May 6, 2025 16:26 Copy link github-actions bot commented May 6, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the structured-output label May 6, 2025 github-project-automation bot added this to Structured Output May 6, 2025 njhill added\n  the v0 label May 6, 2025 Copy link Contributor chaunceyjiang commented May 7, 2025 @shadeMe Hi, sorry for the off-topic question‚Äîhow did you generate this performance chart? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mypy fixes ‚Ä¶ ded6280 Signed-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com> shadeMe force-pushed the v0/fix/logitsprocessor-parallel-sampling-guided-decoding-deepcopy branch\n    from 15e45cf to ded6280 Compare May 7, 2025 09:17 Copy link Contributor Author shadeMe commented May 7, 2025 @shadeMe Hi, sorry for the off-topic question‚Äîhow did you generate this performance chart? It's with the speedscope tool. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor dtransposed commented May 9, 2025 Overlap with #16349 just FYI All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented May 12, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @shadeMe . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label May 12, 2025 Merge branch 'main' into v0/fix/logitsprocessor-parallel-sampling-gui‚Ä¶ ‚Ä¶ 6172283 ‚Ä¶ded-decoding-deepcopy\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com> mergify bot removed\n  the needs-rebase label May 13, 2025 mgoin approved these changes May 16, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Seems reasonable to me, but would like @russellb or @aarnphm to confirm before merge Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label May 16, 2025 bi1101 mentioned this pull request May 16, 2025 [Bug]:Structured outputs inference often took a very long time,and eventually causing a timeout and vLLM engine crushing. #10081 Open 1 task aarnphm approved these changes May 17, 2025 View reviewed changes Copy link Collaborator aarnphm left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I'm good with this for structured outputs, and good to merge in guidance and xgrammar first before #15975 . iirc we will have to deepcopy the logit processors regardless if users use a custom logit processor? so essentially this change in sequence.py could potentially be breaking for users in V0 engine? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/guided_decoding/outlines_logits_processors.py Comment on lines +59 to +64 def clone(self) -> \"BaseLogitsProcessor\": cloned = copy.copy(self) cloned._guide = self._guide.copy() cloned._fsm_state = copy.deepcopy(self._fsm_state) return cloned Copy link Collaborator aarnphm May 17, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I would like to get #15975 in first before assigning this private attrs. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 shadeMe reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Contributor Author shadeMe commented May 19, 2025 iirc we will have to deepcopy the logit processors regardless if users use a custom logit processor? so essentially this change in sequence.py could potentially be breaking for users in V0 engine? Breaking perhaps along the same lines as the original PR that introduced the SamplingParams.clone method - this PR just brings the parallel sampling code inline with its non-parallel counterpart. We could theoretically preserve the existing behaviour while excluding the structured outputs processors, but it would result in leaky abstractions. üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge remote-tracking branch 'origin/main' into v0/fix/logitsprocesso‚Ä¶ ‚Ä¶ a17fb77 ‚Ä¶r-parallel-sampling-guided-decoding-deepcopy russellb enabled auto-merge (squash) May 19, 2025 14:18 Copy link Member russellb commented May 19, 2025 merged from main to see if that gets CI passing All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author shadeMe commented May 21, 2025 The CI failures appear to be unrelated AFAICT? The failing tests use the default n=1 and do not use structured outputs. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented May 23, 2025 Can you merge from main to fix the CI failures? üëç 1 shadeMe reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'main' into v0/fix/logitsprocessor-parallel-sampling-gui‚Ä¶ ‚Ä¶ b8b3fd7 ‚Ä¶ded-decoding-deepcopy Hide details View details vllm-bot merged commit e493e48 into vllm-project : main May 23, 2025 53 of 58 checks passed Uh oh! There was an error while loading. Please reload this page . github-project-automation bot moved this to Done in Structured Output May 23, 2025 bi1101 mentioned this pull request May 23, 2025 [Usage]: Regex Structured Output Became Very Slow #18546 Open 1 task zzzyq pushed a commit\n        to zzzyq/vllm\n      that referenced\n      this pull request May 24, 2025 [V0][Bugfix] Fix parallel sampling performance regression when guided‚Ä¶ ‚Ä¶ 3b77312 ‚Ä¶ decoding is enabled ( vllm-project#17731 )\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuqi Zhang <yuqizhang@google.com> Copy link Member DarkLight1337 commented May 24, 2025 It appears that the samplers test failure on main is caused by this PR. PTAL https://buildkite.com/vllm/ci/builds/20641/steps?jid=0196fcb9-d7f7-4ff4-ad54-260dfc784dae All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . DarkLight1337 mentioned this pull request May 24, 2025 [Bug][Failing Test]: Samplers Test - samplers/test_seeded_generate.py #18656 Closed 1 task Copy link Collaborator aarnphm commented May 24, 2025 This might have to do with deepcopy ü§î All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gshtras added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request May 27, 2025 Upstream merge 2025 05 27 ( #557 ) ‚Ä¶ 1900335 * Add files via uploadAdd fused MoE kernel tuning configs (fp8_w8a8) for DeepSeek V3/R1 on a single-node 8x NVIDIA H20 96GB setup ( vllm-project#18337 )\n\n* [Misc] Fix typo ( vllm-project#18330 )\n\n* Neuron up mistral ( vllm-project#18222 )\n\nSigned-off-by: Satyajith Chilappagari <satchill@amazon.com>\n\n* fix CUDA_check redefinition in vllm-project#17918 ( vllm-project#18287 )\n\nSigned-off-by: Lucia Fang <fanglu@fb.com>\nCo-authored-by: Lucia (Lu) Fang <fanglu@meta.com>\n\n* [neuron] fix authorization issue ( vllm-project#18364 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* [Misc] Allow `AutoWeightsLoader` to skip loading weights with specific substr in name ( vllm-project#18358 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Core] [Bugfix]: tensor parallel with prompt embeds ( vllm-project#18171 )\n\nSigned-off-by: Nan2018 <nan@protopia.ai>\nCo-authored-by: Andrew Sansom <andrew@protopia.ai>\n\n* [release] Change dockerhub username for TPU release ( vllm-project#18389 )\n\n* [Bugfix] fix adding bias twice in ipex GPTQ quantization ( vllm-project#18363 )\n\nSigned-off-by: rand-fly <randfly@outlook.com>\n\n* [doc] update env variable export ( vllm-project#18391 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Misc] Add LoRA code owner ( vllm-project#18387 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* Update cpu.txt ( vllm-project#18398 )\n\nSigned-off-by: Ê±™ÂøóÈπè <wangzhipeng628@gmail.com>\n\n* [CI] Add mteb testing to test the accuracy of the embedding model ( vllm-project#17175 )\n\n* [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text ( vllm-project#18407 )\n\nCo-authored-by: ÊùæÁÅµ <wpf272043@alibaba-inc.com>\n\n* [Misc] refactor prompt embedding examples ( vllm-project#18405 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Minor] Rename quantization nvfp4 to modelopt_fp4 ( vllm-project#18356 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Model] use AutoWeightsLoader for bloom ( vllm-project#18300 )\n\nSigned-off-by: calvin chen <120380290@qq.com>\n\n* [Kernel] update comment for KV shape in unified triton attn ( vllm-project#18099 )\n\nSigned-off-by: haochengxia <xhc_1007@163.com>\n\n* fix:Build torch wheel inline rather than picking from nightly ( vllm-project#18351 )\n\nSigned-off-by: Dilip Gowda Bhagavan <dilip.bhagavan@ibm.com>\n\n* [TPU] Re-enable the Pallas MoE kernel ( vllm-project#18025 )\n\nSigned-off-by: Michael Goin <mgoin64@gmail.com>\n\n* [Bugfix] config.head_dim is now explicitly set to None ( vllm-project#18432 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [Bug] Fix moe_sum signature ( vllm-project#18440 )\n\nSigned-off-by: Bill Nell <bnell@redhat.com>\n\n* Revert \"[Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text ( vllm-project#18407 )\" ( vllm-project#18456 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix][Failing Test] Fix nixl connector test when promt size < block size ( vllm-project#18429 )\n\nSigned-off-by: wwl2755 <wangwenlong2755@gmail.com>\n\n* [Misc] MultiConnector._connectors type ( vllm-project#18423 )\n\nSigned-off-by: nicklucche <nlucches@redhat.com>\n\n* [Frontend] deprecate `--device` arg ( vllm-project#18399 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* [V1] Fix general plugins not loaded in engine for multiproc ( vllm-project#18326 )\n\nSigned-off-by: Yong Hoon Shin <yhshin@meta.com>\n\n* [Misc] refactor disaggregated-prefill-v1 example ( vllm-project#18474 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Bugfix][Failing Test] Fix test_events.py ( vllm-project#18460 )\n\nSigned-off-by: rabi <ramishra@redhat.com>\n\n* [MODEL] FalconH1 ( vllm-project#18406 )\n\nSigned-off-by: dhia.rhaiem <dhia.rhaiem@tii.ae>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Ilyas Chahed <ilyas.chahed@tii.ae>\nCo-authored-by: Jingwei Zuo <jingwei.zuo@tii.ae>\n\n* [Doc] fix arg docstring in linear layers ( vllm-project#18410 )\n\nSigned-off-by: giantcroc <1204449533@qq.com>\n\n* [Bugfix] Reduce moe_sum test size to avoid OOM ( vllm-project#18484 )\n\nSigned-off-by: Bill Nell <bnell@redhat.com>\n\n* [Build] fix Dockerfile shell ( vllm-project#18402 )\n\n* [Misc] Update deprecation message for `--enable-reasoning` ( vllm-project#18404 )\n\n* [ROCm][Kernel][V1] Enable AMD Radeon GPU Custom Paged Attention on v1 ( vllm-project#17004 )\n\nSigned-off-by: Hosang Yoon <hosang.yoon@amd.com>\n\n* Remove incorrect env value\n\n* Revert \"[v1] Support multiple KV cache groups in GPU model runner ( vllm-project#17945 ) ( vllm-project#18459 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [FEAT][ROCm] Upgrade AITER MLA v1 backend ( vllm-project#18338 )\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Luka Govediƒç <ProExpertProg@users.noreply.github.com>\n\n* [Bugfix] Consistent ascii handling in tool parsers ( vllm-project#17704 )\n\nSigned-off-by: Sebastian Sch√∂nnenbeck <sebastian.schoennenbeck@comma-soft.com>\n\n* [FalconH1] Fix output dtype in RMSNorm fallback path for Falcon-H1 (e.g. 0.5B) ( vllm-project#18500 )\n\nSigned-off-by: dhia.rhaiem <dhia.rhaiem@tii.ae>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Ilyas Chahed <ilyas.chahed@tii.ae>\nCo-authored-by: Jingwei Zuo <jingwei.zuo@tii.ae>\n\n* [MISC] update project urls in pyproject.toml ( vllm-project#18519 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [CI] Fix race condition with StatelessProcessGroup.barrier ( vllm-project#18506 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Intialize io_thread_pool attribute in the beginning. ( vllm-project#18331 )\n\nSigned-off-by: rabi <ramishra@redhat.com>\n\n* [Bugfix] Inconsistent token calculation compared to HF in llava family ( vllm-project#18479 )\n\nSigned-off-by: jaycha <jaycha@ncsoft.com>\n\n* [BugFix][DP] Send DP wave completion only from `dp_rank==0` ( vllm-project#18502 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: kourosh hakhamaneshi <kourosh@anyscale.com>\n\n* [Bugfix][Model] Make Olmo2Model weight loading return loaded weights ( vllm-project#18504 )\n\nSigned-off-by: Shane A <shanea@allenai.org>\n\n* [Bugfix] Fix LoRA test ( vllm-project#18518 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Doc] Fix invalid JSON in example args ( vllm-project#18527 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Neuron] Update Dockerfile.neuron to use latest neuron release (2.23) ( vllm-project#18512 )\n\nSigned-off-by: Satyajith Chilappagari <satchill@amazon.com>\n\n* Update default neuron config for speculation ( vllm-project#18274 )\n\nSigned-off-by: Elaine Zhao <elaineyz@amazon.com>\nCo-authored-by: Shashwat Srijan <sssrijan@amazon.com>\nCo-authored-by: Aakash Shetty <sheaak@amazon.com>\n\n* Order sequence ids + config update to support specifying custom quantization layers ( vllm-project#18279 )\n\nSigned-off-by: Elaine Zhao <elaineyz@amazon.com>\nCo-authored-by: Tailin Pan <tailinpa@amazon.com>\nCo-authored-by: Rishabh Rajesh <rishyraj@amazon.com>\nCo-authored-by: Yishan McNabb <yishanm@amazon.com>\nCo-authored-by: Patrick Lange <patlange@amazon.com>\nCo-authored-by: Maxwell Goldberg <mgld@amazon.com>\nCo-authored-by: Aakash Shetty <sheaak@amazon.com>\n\n* [Bugfix] Fix MRoPE Errors in the Qwen-VL Model When Processing Pure Text ( vllm-project#18526 )\n\nCo-authored-by: ÊùæÁÅµ <wpf272043@alibaba-inc.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Add kwargs to RequestOutput __init__ to be forward compatible ( vllm-project#18513 )\n\nSigned-off-by: Linkun <github@lkchen.net>\n\n* [CI/Build] Update bamba test model location ( vllm-project#18544 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Doc] Support --stream arg in openai_completion_client.py script ( vllm-project#18388 )\n\nSigned-off-by: googs1025 <googs1025@gmail.com>\n\n* [Bugfix] Use random hidden states in dummy sampler run ( vllm-project#18543 )\n\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\n\n* [Doc] Add stream flag for chat completion example ( vllm-project#18524 )\n\nSigned-off-by: calvin chen <120380290@qq.com>\n\n* [BugFix][CPU] Fix x86 SHM distributed module initialization ( vllm-project#18536 )\n\nSigned-off-by: jiang.li <jiang1.li@intel.com>\n\n* [Misc] improve Automatic Prefix Caching example ( vllm-project#18554 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Misc] Call `ndarray.tobytes()` directly instead of `ndarray.data.tobytes()` ( vllm-project#18347 )\n\nSigned-off-by: Lukas Geiger <lukas.geiger94@gmail.com>\n\n* [Bugfix] make `test_openai_schema.py` pass ( vllm-project#18224 )\n\nSigned-off-by: David Xia <david@davidxia.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Platform] Move platform check to right place ( vllm-project#18470 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Compile][Platform] Make PiecewiseBackend pluggable and extendable ( vllm-project#18076 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [Build/CI] Fix CUDA 11.8 build ( vllm-project#17679 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Tool] Add NIXL installation script ( vllm-project#18172 )\n\nSigned-off-by: Linkun <github@lkchen.net>\n\n* [V1][Spec Decode][Bugfix] Load quantize weights for EAGLE ( vllm-project#18290 )\n\n* [Frontend][Bug Fix] Update llama4 pythonic jinja template and llama4_pythonic parser ( vllm-project#17917 )\n\nSigned-off-by: Kai Wu <kaiwu@meta.com>\n\n* [Frontend] [Core] Add Tensorizer support for V1, LoRA adapter serialization and deserialization ( vllm-project#17926 )\n\nSigned-off-by: Sanger Steel <sangersteel@gmail.com>\n\n* [AMD] [P/D] Compute num gpus for ROCm correctly in run_accuracy_test.sh ( vllm-project#18568 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* Re-submit: Fix: Proper RGBA -> RGB conversion for PIL images. ( vllm-project#18569 )\n\nSigned-off-by: Chenheli Hua <huachenheli@outlook.com>\n\n* [V1][Spec Decoding] Use model_loader.get_model() to load models ( vllm-project#18273 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* Enable hybrid attention models for Transformers backend ( vllm-project#18494 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Misc] refactor: simplify input validation and num_requests handling in _convert_v1_inputs ( vllm-project#18482 )\n\nSigned-off-by: googs1025 <googs1025@gmail.com>\n\n* [BugFix] Increase TP execute_model timeout ( vllm-project#18558 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [Bugfix] Set `KVTransferConfig.engine_id` in post_init ( vllm-project#18576 )\n\nSigned-off-by: Linkun Chen <github@lkchen.net>\n\n* [Spec Decode] Make EAGLE3 draft token ID mapping optional ( vllm-project#18488 )\n\nSigned-off-by: Benjamin Chislett <benjamin.chislett@centml.ai>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Neuron] Remove bypass on EAGLEConfig and add a test ( vllm-project#18514 )\n\nSigned-off-by: Elaine Zhao <elaineyz@amazon.com>\n\n* [Bugfix][Benchmarks] Fix a benchmark of deepspeed-mii backend to use api_key ( vllm-project#17291 )\n\nSigned-off-by: Teruaki Ishizaki <teruaki.ishizaki@ntt.com>\n\n* [Misc] Replace `cuda` hard code with `current_platform` ( vllm-project#16983 )\n\nSigned-off-by: shen-shanshan <467638484@qq.com>\n\n* [Hardware] correct method signatures for HPU,ROCm,XPU ( vllm-project#18551 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal ( vllm-project#18034 )\n\nSigned-off-by: Ronald Xu <ronaldxu@amazon.com>\n\n* [Feature]Add async tensor parallelism using compilation pass ( vllm-project#17882 )\n\nSigned-off-by: cascade812 <cascade812@outlook.com>\n\n* [Doc] Update quickstart and install for cu128 using `--torch-backend=auto` ( vllm-project#18505 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Feature][V1]: suupports cached_tokens in response usage ( vllm-project#18149 )\n\nCo-authored-by: simon-mo <xmo@berkeley.edu>\n\n* [Bugfix] Add half type support in reshape_and_cache_cpu_impl on x86 cpu platform ( vllm-project#18430 )\n\nSigned-off-by: Yuqi Zhang <yuqizhang@google.com>\nCo-authored-by: Yuqi Zhang <yuqizhang@google.com>\n\n* Migrate docs from Sphinx to MkDocs ( vllm-project#18145 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Revert \"[V1] [Bugfix] eagle bugfix and enable correct lm_head for multimodal ( vllm-project#18034 )\" ( vllm-project#18600 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix][Model] Fix baichuan model loader for tp ( vllm-project#18597 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled ( vllm-project#17731 )\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\n\n* Add myself as docs code owner ( vllm-project#18605 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Hardware][CPU] Update intel_extension_for_pytorch 2.7.0 and move to `requirements/cpu.txt`  ( vllm-project#18542 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* [CI] fix kv_cache_type argument ( vllm-project#18594 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [Doc] Fix indent of contributing to vllm ( vllm-project#18611 )\n\nSigned-off-by: Zerohertz <ohg3417@gmail.com>\n\n* Replace `{func}` with mkdocs style links ( vllm-project#18610 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [CI/Build] Fix V1 flag being set in entrypoints tests ( vllm-project#18598 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Fix examples with code blocks in docs ( vllm-project#18609 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Bugfix] Fix transformers model impl ignored for mixtral quant ( vllm-project#18602 )\n\nSigned-off-by: Tristan Leclercq <tristanleclercq@gmail.com>\n\n* Include private attributes in API documentation ( vllm-project#18614 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Misc] add Haystack integration ( vllm-project#18601 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Bugfix][Build/CI] Fixup CUDA compiler version check for CUDA_SUPPORTED_ARCHS ( vllm-project#18579 )\n\n* [Doc] Fix markdown list indentation for MkDocs rendering ( vllm-project#18620 )\n\nSigned-off-by: Zerohertz <ohg3417@gmail.com>\n\n* [Doc] Use a different color for the announcement ( vllm-project#18616 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Refactor pplx init logic to make it modular (prepare for deepep) ( vllm-project#18200 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Fix figures in design doc ( vllm-project#18612 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Docs] Change mkdocs to not use directory urls ( vllm-project#18622 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [v1] Redo \"Support multiple KV cache groups in GPU model runner ( vllm-project#17945 )\" ( vllm-project#18593 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Doc] fix list formatting ( vllm-project#18624 )\n\nSigned-off-by: David Xia <david@davidxia.com>\n\n* [Doc] Fix top-level API links/docs ( vllm-project#18621 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Avoid documenting dynamic / internal modules ( vllm-project#18626 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Fix broken links and unlinked docs, add shortcuts to home sidebar ( vllm-project#18627 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Support Deepseek MTP ( vllm-project#18435 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: YaoJiayi <120040070@link.cuhk.edu.cn>\nCo-authored-by: Rui Qiao <ruisearch42@gmail.com>\n\n* Use prebuilt FlashInfer x86_64 PyTorch 2.7 CUDA 12.8 wheel for CI ( vllm-project#18537 )\n\nSigned-off-by: Huy Do <huydhn@gmail.com>\n\n* [CI] Enable test_initialization to run on V1 ( vllm-project#16736 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Doc] Update references to doc files ( vllm-project#18637 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ModelOpt] Introduce VLLM_MAX_TOKENS_PER_EXPERT_FP4_MOE env var to control blockscale tensor allocation ( vllm-project#18160 )\n\nSigned-off-by: Pavani Majety <pmajety@nvidia.com>\n\n* [Bugfix] Migrate to REGEX Library to prevent catastrophic backtracking ( vllm-project#18454 )\n\nSigned-off-by: Crucifixion-Fxl <xmufxl@gmail.com>\nCo-authored-by: Crucifixion-Fxl <xmufxl@gmail.com>\n\n* [Bugfix][Nixl] Fix Preemption Bug ( vllm-project#18631 )\n\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\n\n* config.py: Clarify that only local GGUF checkpoints are supported. ( vllm-project#18623 )\n\nSigned-off-by: Mathieu Bordere <mathieu@letmetweakit.com>\n\n* FIX MOE issue in AutoRound format ( vllm-project#18586 )\n\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>\n\n* [V1][Spec Decode] Small refactors to improve eagle bookkeeping performance ( vllm-project#18424 )\n\nSigned-off-by: qizixi <qizixi@meta.com>\n\n* [Frontend] improve vllm serve --help display ( vllm-project#18643 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Model] Add support for Qwen2.5-Omni-7B-AWQ (Qwen2_5OmniForConditionalGeneration) ( vllm-project#18647 )\n\n* [V1][Spec Decode] Support multi-layer eagle draft model ( vllm-project#18030 )\n\nSigned-off-by: qizixi <qizixi@meta.com>\n\n* [Doc] Update README links, mark external links ( vllm-project#18635 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [MISC][pre-commit] Add pre-commit check for triton import ( vllm-project#17716 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [Doc] Fix indentation problems in V0 Paged Attention docs ( vllm-project#18659 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Add community links ( vllm-project#18657 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] use AutoWeightsLoader for gpt2 ( vllm-project#18625 )\n\nSigned-off-by: zt2370 <ztang2370@gmail.com>\n\n* [Doc] Reorganize user guide ( vllm-project#18661 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build] `chmod +x` to `cleanup_pr_body.sh` ( vllm-project#18650 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [MISC] typo fix and clean import ( vllm-project#18664 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [BugFix] Fix import error for fused_moe ( vllm-project#18642 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [CI] enforce import regex instead of re ( vllm-project#18665 )\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\n\n* fix(regression): clone from reference items ( vllm-project#18662 )\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\n\n* [CI/Build] fix permission denied issue ( vllm-project#18645 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [BugFix][Spec Decode] Improve Prefix Caching Logic in Speculative Decoding ( vllm-project#18668 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1] Fix _pickle.PicklingError: Can't pickle <class 'transformers_modules.deepseek-ai.DeepSeek-V2-Lite... ( vllm-project#18640 )\n\nSigned-off-by: Seiji Eicher <seiji@anyscale.com>\n\n* [MISC] correct signature for LoaderFunction ( vllm-project#18670 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [Misc]Replace `cuda` hard code with `current_platform` in Ray ( vllm-project#14668 )\n\nSigned-off-by: noemotiovon <757486878@qq.com>\n\n* [Misc][ModelScope] Change to use runtime VLLM_USE_MODELSCOPE ( vllm-project#18655 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [VLM] Initialize video input support for InternVL models ( vllm-project#18499 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* Speed up the `kernels/quantization/` tests ( vllm-project#18669 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [BUGFIX] catch subclass first for try...except ( vllm-project#18672 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [Misc] Reduce logs on startup ( vllm-project#18649 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [doc] fix broken links ( vllm-project#18671 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [doc] improve readability ( vllm-project#18675 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Bugfix] Fix cpu usage and cache hit stats reporting on cpu environment ( vllm-project#18674 )\n\nSigned-off-by: zzzyq <zhangyuqi94@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [CI/build] fix no regex ( vllm-project#18676 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Misc] small improve ( vllm-project#18680 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Bugfix] Fix profiling dummy data for Pixtral ( vllm-project#18677 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Core][Multimodal] Convert PIL Image to array without data copy when hashing ( vllm-project#18682 )\n\nSigned-off-by: Lukas Geiger <lukas.geiger94@gmail.com>\n\n* [CI/Build][Doc] Update `gte-Qwen2-1.5B-instruct` usage ( vllm-project#18683 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Misc] Fixed the abnormally high TTFT issue in the PD disaggregation example ( vllm-project#18644 )\n\nSigned-off-by: zhaohaidao <zhaohaidao2008@hotmail.com>\nSigned-off-by: zhaohaiyuan <zhaohaiyuan@xiaohongshu.com>\nCo-authored-by: zhaohaiyuan <zhaohaiyuan@xiaohongshu.com>\n\n* refactor: simplify request handler, use positive condition check for handler assignment ( vllm-project#18690 )\n\nSigned-off-by: googs1025 <googs1025@gmail.com>\n\n* [Bugfix] Fix the lm_head in gpt_bigcode in lora mode ( vllm-project#6357 )\n\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Max de Bayser <maxdebayser@gmail.com>\n\n* [CI] add missing argument ( vllm-project#18694 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [GH] Add issue template for reporting CI failures ( vllm-project#18696 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Fix issue template format ( vllm-project#18699 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix Mistral-format models with sliding window ( vllm-project#18693 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build] Replace `math.isclose` with `pytest.approx` ( vllm-project#18703 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI] fix dump_input for str type ( vllm-project#18697 )\n\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\n\n* [Model] Add support for YARN in NemotronNAS models ( vllm-project#18427 )\n\nSigned-off-by: Nave Assaf <nassaf@nvidia.com>\n\n* [CI/Build] Split pooling and generation extended language models tests in CI ( vllm-project#18705 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][Intel-Gaudi] [CI/Build] Add tensor parallel size = 2 test to HPU CI ( vllm-project#18709 )\n\nSigned-off-by: Lukasz Durejko <ldurejko@habana.ai>\n\n* [Misc] add AutoGen integration ( vllm-project#18712 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Bugfix]: handle hf-xet CAS error when loading Qwen3 weights in vLLM ( vllm-project#18701 )\n\n* [Doc] Improve API docs ( vllm-project#18713 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Move examples and further reorganize user guide ( vllm-project#18666 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix Llama GGUF initialization ( vllm-project#18717 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1][Sampler] Improve performance of FlashInfer sampling by sampling logits instead of probs ( vllm-project#18608 )\n\n* Convert `examples` to `ruff-format` ( vllm-project#18400 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Model][Gemma3] Simplify image input validation ( vllm-project#18710 )\n\nSigned-off-by: Lukas Geiger <lukas.geiger94@gmail.com>\n\n* [Misc] improve web section group title display ( vllm-project#18684 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [V1][Quantization] Add CUDA graph compatible v1 GGUF support ( vllm-project#18646 )\n\nSigned-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Model][Gemma3] Cast image pixel values already on CPU ( vllm-project#18732 )\n\nSigned-off-by: Lukas Geiger <lukas.geiger94@gmail.com>\n\n* [FEAT] [ROCm] Upgrade AITER Fused MoE kernels. ( vllm-project#18271 )\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* [Doc] Update OOT model docs ( vllm-project#18742 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Doc] Update reproducibility doc and example ( vllm-project#18741 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] improve docs ( vllm-project#18734 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* feat(rocm-support): support mamba2 on rocm ( vllm-project#18565 )\n\nSigned-off-by: Islam Almersawi <islam.almersawi@openinnovation.ai>\nCo-authored-by: Islam Almersawi <islam.almersawi@openinnovation.ai>\n\n* [Hardware][Intel-Gaudi] [CI/Build] Fix multiple containers using the same name in run-hpu-test.sh ( vllm-project#18752 )\n\nSigned-off-by: Lukasz Durejko <ldurejko@habana.ai>\n\n* [Doc] cleanup deprecated flag for doc ( vllm-project#18715 )\n\nSigned-off-by: calvin chen <120380290@qq.com>\n\n* Minor fix about MooncakeStoreConnector ( vllm-project#18721 )\n\nSigned-off-by: baoloongmao <baoloongmao@tencent.com>\n\n* [Build] fix cpu build missing libtbbmalloc.so ( vllm-project#18744 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* [BUG FIX] minicpm ( vllm-project#18739 )\n\nSigned-off-by: huangyuxiang03 <huangyx0321@gmail.com>\nCo-authored-by: huangyuxiang03 <huangyx0321@gmail.com>\n\n* [Doc]  Convert Sphinx directives ( `{class}`, `{meth}`, `{attr}`, ...) to MkDocs format for better documentation linking ( vllm-project#18663 )\n\nSigned-off-by: Zerohertz <ohg3417@gmail.com>\n\n* [CI/Build] Remove imports of built-in `re` ( vllm-project#18750 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1][Metrics] Add API for accessing in-memory Prometheus metrics ( vllm-project#17010 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* Disable prefix cache by default for benchmark ( vllm-project#18639 )\n\nSigned-off-by: cascade812 <cascade812@outlook.com>\n\n* optimize get_kv_cache_torch_dtype ( vllm-project#18531 )\n\nSigned-off-by: idellzheng <idellzheng@tencent.com>\n\n* [Core] Automatically cast multi-modal input dtype ( vllm-project#18756 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Mistral tool calling when content is list ( vllm-project#18729 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n---------\n\nSigned-off-by: Satyajith Chilappagari <satchill@amazon.com>\nSigned-off-by: Lucia Fang <fanglu@fb.com>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Nan2018 <nan@protopia.ai>\nSigned-off-by: rand-fly <randfly@outlook.com>\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Ê±™ÂøóÈπè <wangzhipeng628@gmail.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: calvin chen <120380290@qq.com>\nSigned-off-by: haochengxia <xhc_1007@163.com>\nSigned-off-by: Dilip Gowda Bhagavan <dilip.bhagavan@ibm.com>\nSigned-off-by: Michael Goin <mgoin64@gmail.com>\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nSigned-off-by: Bill Nell <bnell@redhat.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: wwl2755 <wangwenlong2755@gmail.com>\nSigned-off-by: nicklucche <nlucches@redhat.com>\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: Yong Hoon Shin <yhshin@meta.com>\nSigned-off-by: rabi <ramishra@redhat.com>\nSigned-off-by: dhia.rhaiem <dhia.rhaiem@tii.ae>\nSigned-off-by: giantcroc <1204449533@qq.com>\nSigned-off-by: Hosang Yoon <hosang.yoon@amd.com>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nSigned-off-by: Sebastian Sch√∂nnenbeck <sebastian.schoennenbeck@comma-soft.com>\nSigned-off-by: Andy Xie <andy.xning@gmail.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: jaycha <jaycha@ncsoft.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Shane A <shanea@allenai.org>\nSigned-off-by: Elaine Zhao <elaineyz@amazon.com>\nSigned-off-by: Linkun <github@lkchen.net>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: googs1025 <googs1025@gmail.com>\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: jiang.li <jiang1.li@intel.com>\nSigned-off-by: Lukas Geiger <lukas.geiger94@gmail.com>\nSigned-off-by: David Xia <david@davidxia.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Tyler Michael Smith <tysmith@redhat.com>\nSigned-off-by: Kai Wu <kaiwu@meta.com>\nSigned-off-by: Sanger Steel <sangersteel@gmail.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Chenheli Hua <huachenheli@outlook.com>\nSigned-off-by: Linkun Chen <github@lkchen.net>\nSigned-off-by: Benjamin Chislett <benjamin.chislett@centml.ai>\nSigned-off-by: Teruaki Ishizaki <teruaki.ishizaki@ntt.com>\nSigned-off-by: shen-shanshan <467638484@qq.com>\nSigned-off-by: Ronald Xu <ronaldxu@amazon.com>\nSigned-off-by: cascade812 <cascade812@outlook.com>\nSigned-off-by: Yuqi Zhang <yuqizhang@google.com>\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: Zerohertz <ohg3417@gmail.com>\nSigned-off-by: Tristan Leclercq <tristanleclercq@gmail.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: YaoJiayi <120040070@link.cuhk.edu.cn>\nSigned-off-by: Huy Do <huydhn@gmail.com>\nSigned-off-by: Pavani Majety <pmajety@nvidia.com>\nSigned-off-by: Crucifixion-Fxl <xmufxl@gmail.com>\nSigned-off-by: rshaw@neuralmagic.com <robertgshaw2@gmail.com>\nSigned-off-by: Mathieu Bordere <mathieu@letmetweakit.com>\nSigned-off-by: wenhuach21 <wenhua.cheng@intel.com>\nSigned-off-by: qizixi <qizixi@meta.com>\nSigned-off-by: zt2370 <ztang2370@gmail.com>\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Seiji Eicher <seiji@anyscale.com>\nSigned-off-by: noemotiovon <757486878@qq.com>\nSigned-off-by: zzzyq <zhangyuqi94@gmail.com>\nSigned-off-by: zhaohaidao <zhaohaidao2008@hotmail.com>\nSigned-off-by: zhaohaiyuan <zhaohaiyuan@xiaohongshu.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Max de Bayser <maxdebayser@gmail.com>\nSigned-off-by: Nave Assaf <nassaf@nvidia.com>\nSigned-off-by: Lukasz Durejko <ldurejko@habana.ai>\nSigned-off-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nSigned-off-by: Islam Almersawi <islam.almersawi@openinnovation.ai>\nSigned-off-by: baoloongmao <baoloongmao@tencent.com>\nSigned-off-by: huangyuxiang03 <huangyx0321@gmail.com>\nSigned-off-by: idellzheng <idellzheng@tencent.com>\nCo-authored-by: sunyicode0012 <116338547+sunyicode0012@users.noreply.github.com>\nCo-authored-by: Gong Shufan <2624542821@qq.com>\nCo-authored-by: Satyajith Chilappagari <satchill@amazon.com>\nCo-authored-by: Lucia Fang <116399278+luccafong@users.noreply.github.com>\nCo-authored-by: Lucia (Lu) Fang <fanglu@meta.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Nan Qin <nan@protopia.ai>\nCo-authored-by: Andrew Sansom <andrew@protopia.ai>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Random Fly <renfei8@live.cn>\nCo-authored-by: Reid <61492567+reidliu41@users.noreply.github.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Ê±™ÂøóÈπè <wangzhipeng628@gmail.com>\nCo-authored-by: wang.yuqi <noooop@126.com>\nCo-authored-by: ÁáÉ <wulipc@163.com>\nCo-authored-by: ÊùæÁÅµ <wpf272043@alibaba-inc.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Calvin Chen <45745657+calvin0327@users.noreply.github.com>\nCo-authored-by: Percy <xhc_1007@163.com>\nCo-authored-by: Dilip Gowda Bhagavan <110233170+dilipgb@users.noreply.github.com>\nCo-authored-by: bnellnm <49004751+bnellnm@users.noreply.github.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: wwl2755 <wangwenlong2755@gmail.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: Yong Hoon Shin <48474650+sarckk@users.noreply.github.com>\nCo-authored-by: Rabi Mishra <ramishra@redhat.com>\nCo-authored-by: Dhia Eddine Rhaiem <163106757+dhiaEddineRhaiem@users.noreply.github.com>\nCo-authored-by: younesbelkada <younesbelkada@gmail.com>\nCo-authored-by: Ilyas Chahed <ilyas.chahed@tii.ae>\nCo-authored-by: Jingwei Zuo <jingwei.zuo@tii.ae>\nCo-authored-by: GiantCroc <1204449533@qq.com>\nCo-authored-by: Hyogeun Oh (Ïò§Ìö®Í∑º) <ohg3417@gmail.com>\nCo-authored-by: Hosang <156028780+hyoon1@users.noreply.github.com>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Luka Govediƒç <ProExpertProg@users.noreply.github.com>\nCo-authored-by: Sebastian Schoennenbeck <sebastian.schoennenbeck@comma-soft.com>\nCo-authored-by: Ning Xie <andy.xning@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: youngrok cha <line0930@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: kourosh hakhamaneshi <kourosh@anyscale.com>\nCo-authored-by: Shane A <shanea@allenai.org>\nCo-authored-by: aws-elaineyz <elaineyz@amazon.com>\nCo-authored-by: Shashwat Srijan <sssrijan@amazon.com>\nCo-authored-by: Aakash Shetty <sheaak@amazon.com>\nCo-authored-by: Tailin Pan <tailinpa@amazon.com>\nCo-authored-by: Rishabh Rajesh <rishyraj@amazon.com>\nCo-authored-by: Yishan McNabb <yishanm@amazon.com>\nCo-authored-by: Patrick Lange <patlange@amazon.com>\nCo-authored-by: Maxwell Goldberg <mgld@amazon.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: lkchen <github@lkchen.net>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: CYJiang <86391540+googs1025@users.noreply.github.com>\nCo-authored-by: Bowen Wang <abmfy@icloud.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Lukas Geiger <lukas.geiger94@gmail.com>\nCo-authored-by: David Xia <david@davidxia.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Ekagra Ranjan <3116519+ekagra-ranjan@users.noreply.github.com>\nCo-authored-by: Kai Wu <kaiwu@meta.com>\nCo-authored-by: Sanger Steel <sangersteel@gmail.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Chenheli Hua <huachenheli@outlook.com>\nCo-authored-by: Benjamin Chislett <chislett.ben@gmail.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Teruaki Ishizaki <tell.ishi@gmail.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: RonaldBXu <72748153+RonaldBXu@users.noreply.github.com>\nCo-authored-by: cascade <cascade812@outlook.com>\nCo-authored-by: Chauncey <chaunceyjiang@gmail.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: Yuqi Zhang <zhangyuqi94@gmail.com>\nCo-authored-by: Yuqi Zhang <yuqizhang@google.com>\nCo-authored-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: Tristan Leclercq <49700633+tristanleclercq@users.noreply.github.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Jiayi Yao <82156730+YaoJiayi@users.noreply.github.com>\nCo-authored-by: Rui Qiao <ruisearch42@gmail.com>\nCo-authored-by: Huy Do <huydhn@gmail.com>\nCo-authored-by: Pavani Majety <pmajety@nvidia.com>\nCo-authored-by: Feng XiaoLong <79261065+Crucifixion-Fxl@users.noreply.github.com>\nCo-authored-by: Crucifixion-Fxl <xmufxl@gmail.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: Mathieu Border√© <mathieu@bordere.org>\nCo-authored-by: Wenhua Cheng <wenhua.cheng@intel.com>\nCo-authored-by: qizixi <22851944+zixi-qi@users.noreply.github.com>\nCo-authored-by: Yuanhao WU <Nalkey@users.noreply.github.com>\nCo-authored-by: ztang2370 <ztang2370@gmail.com>\nCo-authored-by: Aaron Pham <contact@aarnphm.xyz>\nCo-authored-by: Seiji Eicher <58963096+eicherseiji@users.noreply.github.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: AlexZhao <zhaohaidao2008@hotmail.com>\nCo-authored-by: zhaohaiyuan <zhaohaiyuan@xiaohongshu.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Naveassaf <55059536+Naveassaf@users.noreply.github.com>\nCo-authored-by: ≈Åukasz Durejko <lukasz.durejko@intel.com>\nCo-authored-by: dylan <xuhao296@qq.com>\nCo-authored-by: almersawi <43927639+almersawi@users.noreply.github.com>\nCo-authored-by: Islam Almersawi <islam.almersawi@openinnovation.ai>\nCo-authored-by: ≈Åukasz Durejko <ldurejko@habana.ai>\nCo-authored-by: maobaolong <baoloongmao@tencent.com>\nCo-authored-by: Shawn Huang <57223022+huangyuxiang03@users.noreply.github.com>\nCo-authored-by: huangyuxiang03 <huangyx0321@gmail.com>\nCo-authored-by: chunxiaozheng <55471457+chunxiaozheng@users.noreply.github.com> minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [V0][Bugfix] Fix parallel sampling performance regression when guided‚Ä¶ ‚Ä¶ ac503be ‚Ä¶ decoding is enabled ( vllm-project#17731 )\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: minpeter <kali2005611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:04",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, profile, profiling | SERVING: vllm serve, serve, Frontend | TEST: test, test, Test",
  "analysis_extracted_at": "2025-09-07 17:51:04",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled (#17731)",
  "commit_message": "[V0][Bugfix] Fix parallel sampling performance regression when guided decoding is enabled (#17731)\n\nSigned-off-by: Madeesh Kannan <shadeMe@users.noreply.github.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>",
  "commit_date": "2025-05-23T03:38:23-07:00",
  "files_changed": [
    "vllm/model_executor/guided_decoding/guidance_logits_processors.py",
    "vllm/model_executor/guided_decoding/outlines_logits_processors.py",
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py",
    "vllm/sequence.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 4,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 4,
    "num_hunks": 9,
    "num_edited_lines": 48,
    "num_non_test_edited_lines": 48,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/guided_decoding/guidance_logits_processors.py b/vllm/model_executor/guided_decoding/guidance_logits_processors.py\nindex 4b45c272a..e17df68b4 100644\n--- a/vllm/model_executor/guided_decoding/guidance_logits_processors.py\n+++ b/vllm/model_executor/guided_decoding/guidance_logits_processors.py\n@@ -1,4 +1,5 @@\n # SPDX-License-Identifier: Apache-2.0\n+import copy\n import os\n from typing import Any\n \n@@ -34,9 +35,24 @@ class GuidanceLogitsProcessor:\n         self.grammar = grammar\n         self.tokenizer = tokenizer\n         self.tokenizer_name = tokenizer.name_or_path\n+        self.ll_tokenizer = None\n+        self.ll_matcher = None\n+        self.bitmask = None\n         self.new_sampling = False\n         self.initialized = False\n \n+    def clone(self) -> \"GuidanceLogitsProcessor\":\n+        cloned = copy.copy(self)\n+        if self.initialized:\n+            cloned.ll_matcher = llguidance.LLMatcher(\n+                self.ll_tokenizer,  # type: ignore[assignment]\n+                self.grammar,\n+                log_level=int(os.environ.get(\"LLGUIDANCE_LOG_LEVEL\", \"1\")),\n+            )\n+            self.bitmask = llguidance.torch.allocate_token_bitmask(\n+                1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]\n+        return cloned\n+\n     def _initialize(self):\n         if self.initialized:\n             return\n@@ -56,7 +72,7 @@ class GuidanceLogitsProcessor:\n \n         # create reusable bitmask\n         self.bitmask = llguidance.torch.allocate_token_bitmask(\n-            1, self.ll_tokenizer.vocab_size)\n+            1, self.ll_tokenizer.vocab_size)  # type: ignore[attr-defined]\n \n         self.initialized = True\n \n@@ -70,15 +86,17 @@ class GuidanceLogitsProcessor:\n         self._initialize()\n \n         if self.new_sampling and len(input_ids) > 0:\n-            self.ll_matcher.consume_token(input_ids[-1])\n-            err = self.ll_matcher.get_error()\n+            self.ll_matcher.consume_token(  # type: ignore[attr-defined]\n+                input_ids[-1])\n+            err = self.ll_matcher.get_error()  # type: ignore[attr-defined]\n             if err:\n                 logger.warning(\"Error in LLMatcher: %s\", err)\n \n         llguidance.torch.fill_next_token_bitmask(self.ll_matcher, self.bitmask,\n                                                  0)\n         llguidance.torch.apply_token_bitmask_inplace(\n-            scores, self.bitmask.to(scores.device))\n+            scores,\n+            self.bitmask.to(scores.device))  # type: ignore[attr-defined]\n \n         self.new_sampling = True\n \ndiff --git a/vllm/model_executor/guided_decoding/outlines_logits_processors.py b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\nindex 8ae7c7b6b..6986b6554 100644\n--- a/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n+++ b/vllm/model_executor/guided_decoding/outlines_logits_processors.py\n@@ -56,6 +56,12 @@ class BaseLogitsProcessor:\n         self._fsm_state: defaultdict[int, Union[int,\n                                                 CFGState]] = defaultdict(int)\n \n+    def clone(self) -> \"BaseLogitsProcessor\":\n+        cloned = copy.copy(self)\n+        cloned._guide = self._guide.copy()\n+        cloned._fsm_state = copy.deepcopy(self._fsm_state)\n+        return cloned\n+\n     def __call__(self, input_ids: list[int],\n                  scores: torch.Tensor) -> torch.Tensor:\n         \"\"\"Use the FSM to bias the logits before sampling the next token.\"\"\"\n@@ -218,6 +224,12 @@ class CFGLogitsProcessor(BaseLogitsProcessor):\n                          reasoner)\n         self._guide = self._guide.copy()\n \n+    def clone(self) -> \"CFGLogitsProcessor\":\n+        cloned = copy.copy(self)\n+        cloned._fsm_state = copy.deepcopy(self._fsm_state)\n+        cloned._guide = self._guide.copy()\n+        return cloned\n+\n \n @lru_cache(maxsize=32)\n def _adapt_tokenizer(tokenizer: PreTrainedTokenizerBase):\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 8e40da4b3..7ca7bab81 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -302,8 +302,9 @@ class XGrammarLogitsProcessor:\n     prefilled: bool = field(default=False)\n \n     def __post_init__(self):\n-        self.tokenizer_info = self.config.tokenizer_info(\n-            self.config.tokenizer_data)\n+        if self.tokenizer_info is None:\n+            self.tokenizer_info = self.config.tokenizer_info(\n+                self.config.tokenizer_data)\n \n     def __getstate__(self) -> dict[str, Any]:\n         return {'config': self.config, 'reasoner': self.reasoner}\n@@ -400,7 +401,8 @@ class XGrammarLogitsProcessor:\n     def clone(self) -> XGrammarLogitsProcessor:\n         \"\"\"Create a new instance with shared compiled grammar\n           but separate state\"\"\"\n-        new_processor = XGrammarLogitsProcessor(self.config, self.reasoner)\n+        new_processor = XGrammarLogitsProcessor(self.config, self.reasoner,\n+                                                None, self.tokenizer_info)\n \n         # Share the compiled grammar context (immutable after compilation)\n         new_processor.ctx = self.ctx\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex f5f9c56a7..f3dfd32d9 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1494,7 +1494,7 @@ class ParallelSampleSequenceGroup(SequenceGroupBase):\n         for i in range(original_params.n):\n             request_id_i = f\"{request_id}_parallel_sample_{i}\"\n             group.seq_id_to_index[request_id_i] = i\n-            params = copy.deepcopy(original_params)\n+            params = params.clone()\n             params.n = 1\n             if params.seed is not None:\n                 params.seed += i",
  "apis": [
    "GuidanceLogitsProcessor.clone",
    "BaseLogitsProcessor.clone",
    "CFGLogitsProcessor.clone",
    "XGrammarLogitsProcessor.clone",
    "SamplingParams.clone"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/sequence.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit addresses a parallel sampling performance regression when guided decoding is enabled. It modifies multiple non-test source files by changing how objects are cloned (e.g., replacing a deepcopy with a clone method in sequence.py) and updating related functions in guided decoding modules. Although the commit message includes a bugfix tag, it specifically refers to a performance regression, and the changes aim at improving the execution speed of guided decoding under parallel sampling. The modifications are non-trivial and affect internal/high-level APIs without being tied to GPU-specific optimizations. Therefore, the commit meets the criteria for being performance-related.",
  "llm_api_reason": "The commit adds new ‚Äúclone‚Äù methods to multiple guided decoding logits processor classes. In guidance_logits_processors.py, a clone() is introduced to copy a GuidanceLogitsProcessor instance while reinitializing certain internal state. Similarly, in outlines_logits_processors.py both BaseLogitsProcessor and CFGLogitsProcessor receive new clone() implementations, and in xgrammar_decoding.py the clone() method of XGrammarLogitsProcessor is modified to pass tokenizer_info. Additionally, in sequence.py the usage of deep-copying of sampling parameters is replaced by invoking a clone() method, affecting the SamplingParams API."
}