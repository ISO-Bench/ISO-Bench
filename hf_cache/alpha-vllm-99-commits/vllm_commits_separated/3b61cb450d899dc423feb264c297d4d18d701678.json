{
  "commit_hash": "3b61cb450d899dc423feb264c297d4d18d701678",
  "pr_url": "https://github.com/vllm-project/vllm/pull/10989",
  "pr_date": "2024-12-09",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Dec 8, 2024 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR reduces the CPU ops in V1 flash-attn: two slice ops for key and value by slightly modifying the reshape_and_cache_flash op. Also, it uses kv_cache.unbind(0) instead of kv_cache[0] and kv_cache[1] , to reduce the number of ops. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëÄ 1 tlrmchlsmth reacted with eyes emoji All reactions üëÄ 1 reaction WoosukKwon added 6 commits December 4, 2024 21:06 tmp ‚Ä¶ d34c4a8 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor ‚Ä¶ 14e2f77 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix ‚Ä¶ fc025ec Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-cache-opt 001ad42 minor ‚Ä¶ 194fa9e Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> comment ‚Ä¶ 269901d Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon requested review from robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners December 8, 2024 11:02 Copy link github-actions bot commented Dec 8, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Dec 8, 2024 Hide details View details WoosukKwon merged commit 3b61cb4 into main Dec 9, 2024 90 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the v1-cache-opt branch December 9, 2024 20:38 sleepwalker2017 pushed a commit\n        to sleepwalker2017/vllm\n      that referenced\n      this pull request Dec 13, 2024 [V1] Further reduce CPU overheads in flash-attn ( vllm-project#10989 ) ‚Ä¶ 0ad90dd Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:34",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:34",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1] Further reduce CPU overheads in flash-attn (#10989)",
  "commit_message": "[V1] Further reduce CPU overheads in flash-attn (#10989)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2024-12-09T12:38:46-08:00",
  "files_changed": [
    "csrc/cache_kernels.cu",
    "vllm/v1/attention/backends/flash_attn.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 2,
    "num_edited_lines": 35,
    "num_non_test_edited_lines": 35,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 1be806bbf..8a95279f9 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -307,10 +307,20 @@ void reshape_and_cache_flash(\n     torch::Tensor& key_cache,  // [num_blocks, block_size, num_heads, head_size]\n     torch::Tensor&\n         value_cache,  // [num_blocks, block_size, num_heads, head_size]\n-    torch::Tensor& slot_mapping,  // [num_tokens]\n+    torch::Tensor& slot_mapping,  // [num_tokens] or [num_actual_tokens]\n     const std::string& kv_cache_dtype, const double k_scale,\n     const double v_scale) {\n-  int num_tokens = key.size(0);\n+  // NOTE(woosuk): In vLLM V1, key.size(0) can be different from\n+  // slot_mapping.size(0) because of padding for CUDA graphs.\n+  // In vLLM V0, key.size(0) is always equal to slot_mapping.size(0) because\n+  // both include padding.\n+  // In vLLM V1, however, key.size(0) can be larger than slot_mapping.size(0)\n+  // since key includes padding for CUDA graphs, while slot_mapping does not.\n+  // In this case, slot_mapping.size(0) represents the actual number of tokens\n+  // before padding.\n+  // For compatibility with both cases, we use slot_mapping.size(0) as the\n+  // number of tokens.\n+  int num_tokens = slot_mapping.size(0);\n   int num_heads = key.size(1);\n   int head_size = key.size(2);\n   int block_size = key_cache.size(1);\ndiff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex d37989055..251a103e6 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -138,14 +138,25 @@ class FlashAttentionImpl(AttentionImpl):\n             # Profiling run.\n             return output\n \n-        num_actual_tokens = attn_metadata.num_actual_tokens\n+        # IMPORTANT!\n+        # NOTE(woosuk): With piece-wise CUDA graphs, this method is executed in\n+        # eager-mode PyTorch. Thus, we need to be careful about any CPU overhead\n+        # in this method. For example, `view` and `slice` (or `[:n]`) operations\n+        # are surprisingly slow even in the case they do not invoke any GPU ops.\n+        # Minimize the PyTorch ops in this method as much as possible.\n+        # Whenever making a change in this method, please benchmark the\n+        # performance to make sure it does not introduce any overhead.\n \n+        num_actual_tokens = attn_metadata.num_actual_tokens\n         # Reshape the input keys and values and store them in the cache.\n-        key_cache = kv_cache[0]\n-        value_cache = kv_cache[1]\n+        # NOTE(woosuk): Here, key and value are padded while slot_mapping is\n+        # not padded. However, we don't need to do key[:num_actual_tokens] and\n+        # value[:num_actual_tokens] because the reshape_and_cache_flash op uses\n+        # the slot_mapping's shape to determine the number of actual tokens.\n+        key_cache, value_cache = kv_cache.unbind(0)\n         torch.ops._C_cache_ops.reshape_and_cache_flash(\n-            key[:num_actual_tokens],\n-            value[:num_actual_tokens],\n+            key,\n+            value,\n             key_cache,\n             value_cache,\n             attn_metadata.slot_mapping,",
  "apis": [
    "vllm.v1.attention.backends.flash_attn.FlashAttentionImpl.forward",
    "torch.ops._C_cache_ops.reshape_and_cache_flash"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/flash_attn.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/flash_attn.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/_custom_ops.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies two source code files (cache_kernels.cu and flash_attn.py) which are non-test files. It focuses on reducing CPU overhead in critical methods by adjusting how the number of tokens is computed and how operations are executed (e.g., using slot_mapping.size() for token count to avoid unnecessary padding-related overhead and making changes in the flash attention implementation to reduce operations like slicing/viewing that can be slow in eager-mode PyTorch). These modifications are performance-specific and aimed at optimizing CPU performance without changing the underlying functionality. Although the commit comments mention benchmarking and careful performance considerations rather than explicitly stating ‚Äúperformance‚Äù everywhere, the changes clearly target optimizing the performance of a high-level API, impacting CPU efficiency. Therefore, this commit meets the performance/optimization-related criteria.",
  "llm_api_reason": "This commit revises the flash‚Äêattention caching mechanism by changing the way the number of tokens is computed in the low-level CUDA kernel used for reshaping and caching. In the C++ kernel, it now uses slot_mapping.size(0) instead of key.size(0) to determine the token count since in vLLM V1 the key tensor may include extra padding for CUDA graphs. In the Python layer ‚Äì in the FlashAttentionImpl in the v1 backend ‚Äì additional comments regarding eager-mode execution and minimization of CPU overhead were added and the code now calls kv_cache.unbind(0) instead of directly indexing key and value. Thus, the affected high‚Äêlevel Python APIs include the flash‚Äêattention forward method in the V1 backend and the underlying op call wrapped in torch.ops._C_cache_ops.reshape_and_cache_flash."
}