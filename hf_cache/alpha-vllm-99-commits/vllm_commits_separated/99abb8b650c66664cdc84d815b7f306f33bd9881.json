{
  "commit_hash": "99abb8b650c66664cdc84d815b7f306f33bd9881",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14930",
  "pr_date": "2025-03-18",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Mar 17, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes the rejection sampler in #13933 with custom Triton kernels. By using the Triton kernels, the PR brings the following benefits: Now we use the flattened shape [num_tokens, vocab_size] for the logits tensors, instead of [batch_size, max_spec_len, vocab_size] . This reduces the GPU memory usage a lot. Zero synchronization between CPU and GPU. Remove inefficient data movement (i.e., a bunch of cat , gather , etc.) (Arguably) easier-to-read code Performance benchmark: Llama 3.1 8B, ShareGPT, 1xH100, temperature 0.1 SD config: --speculative-model \"[ngram]\" --ngram_prompt_lookup_min 5 --ngram-prompt-lookup-max 5 --num_speculative_tokens 3 Throughput (reqs/s) main (w/o SD) 51.49 main (w/ SD) 54.41 This PR (w/ SD) 64.16 25% throughput increase compared to main w/o SD, and 18% increase compared to main w/ SD. Accuracy benchmark: GSM8K, Llama 3.1 8B Instruct, 5 shots Temperature Exact match w/o SD 0.0 75.7 1.0 50.9 w/ SD 0.0 75.9 1.0 51.8 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸŽ‰ 4 robertgshaw2-redhat, LiuXiaoxuanPKU, MARD1NO, and mlinmg reacted with hooray emoji All reactions ðŸŽ‰ 4 reactions WoosukKwon added 30 commits March 14, 2025 20:41 tmp â€¦ c09ae5e Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ e3f3513 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix shape â€¦ be535aa Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ be950c7 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 1fee177 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Add parse_outputs â€¦ d30970e Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 32fefa1 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 4a93973 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ f2455fd Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> kernel â€¦ fbba0ff Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> kernel â€¦ 255d1ee Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ 22c9515 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> comment â€¦ c631935 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 566caea Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ c427ffd Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ d896f41 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ cb8e699 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ c0bcf5a Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ ae3d7fc Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ 412e2f4 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> remove â€¦ df66124 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> opt â€¦ 704da77 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 4f95ca9 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> opt softmax & fix recompilation â€¦ 803c9de Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> minor â€¦ 9cc9349 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> remove envs â€¦ 2b69e51 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-opt-rej d374d59 Merge branch 'main' into v1-opt-rej d4a6437 fix â€¦ 75e93aa Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix â€¦ 5a86ff3 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> 24 hidden items Load moreâ€¦ WoosukKwon added 6 commits March 17, 2025 10:12 fix test â€¦ 8b7a398 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into v1-opt-rej b303722 Merge branch 'main' into v1-opt-rej a0440c8 comment â€¦ 40f334a Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> comment â€¦ 6935bfd Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix shape mismatch â€¦ 0baa33e Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> LiuXiaoxuanPKU reviewed Mar 18, 2025 View reviewed changes Copy link Collaborator LiuXiaoxuanPKU left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Finished the rejection_sampler.py, will continue other files tonight Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/v1/sample/rejection_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/sample/rejection_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/sample/rejection_sampler.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . LiuXiaoxuanPKU reviewed Mar 18, 2025 View reviewed changes vllm/v1/sample/rejection_sampler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . WoosukKwon added 4 commits March 18, 2025 12:17 Merge branch 'main' into v1-opt-rej 459b2fa fix docstrings â€¦ aaf2316 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> fix dtype â€¦ 531068e Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> add comment â€¦ 69c88b8 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon requested a review\n  from LiuXiaoxuanPKU March 18, 2025 19:29 LiuXiaoxuanPKU approved these changes Mar 18, 2025 View reviewed changes Copy link Collaborator LiuXiaoxuanPKU left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, thanks! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details WoosukKwon merged commit 99abb8b into main Mar 18, 2025 29 of 32 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the v1-opt-rej branch March 18, 2025 21:31 youkaichao reviewed Mar 19, 2025 View reviewed changes vllm/v1/sample/ops/utils.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . CXIAAAAA mentioned this pull request Mar 19, 2025 [Feature]: Add likaixin/InstructCoder as spec decode benchmark dataset option #14045 Closed 1 task This was referenced Mar 21, 2025 [Bug]: v1 speculate decoding NgramProposer experiences service exceptions during stress testing #14742 Closed add last slot for the invalid_token in greedy rejection sampler, specdec #14519 Closed WoosukKwon mentioned this pull request Apr 2, 2025 [Bug]: [V1][SpecDec] RuntimeError: CUDA error: an illegal memory access was encountered #13673 Closed 1 task lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels ( vllâ€¦ â€¦ f928001 â€¦m-project#14930 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels ( vllâ€¦ â€¦ 0e57658 â€¦m-project#14930 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels ( vllâ€¦ â€¦ 08577f8 â€¦m-project#14930 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> mmyxym reviewed Aug 5, 2025 View reviewed changes vllm/v1/sample/rejection_sampler.py GREEDY_TEMPERATURE: tl.constexpr = -1 # Maximum number of speculative draft tokens allowed per request in a single # step. This value is chosen to be large enough to handle typical use cases. MAX_SPEC_LEN = 32 Copy link mmyxym Aug 5, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Hi @WoosukKwon , is there any limitation MAX_SPEC_LEN should be 32? Can it be larger? Thanks. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author WoosukKwon Aug 28, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @mmyxym There's no blocker to make it 64. Everything should work if you just change the number. I just thought 32 would be enough for all practical use cases. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mergify bot added\n  the speculative-decoding label Aug 5, 2025 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:49",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: GSM8K | PERF: Throughput, throughput | TEST: test, test, testing",
  "analysis_extracted_at": "2025-09-07 17:51:49",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 1000",
  "commit_subject": "[V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)",
  "commit_message": "[V1][Spec Decode] Optimize Rejection Sampler with Triton Kernels (#14930)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2025-03-18T14:31:54-07:00",
  "files_changed": [
    "tests/v1/sample/test_rejection_sampler.py",
    "vllm/envs.py",
    "vllm/v1/outputs.py",
    "vllm/v1/sample/ops/utils.py",
    "vllm/v1/sample/rejection_sampler.py",
    "vllm/v1/spec_decode/metadata.py",
    "vllm/v1/spec_decode/utils.py",
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 7,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 8,
    "num_hunks": 34,
    "num_edited_lines": 1329,
    "num_non_test_edited_lines": 1098,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a40b..8c423e367 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -6,20 +6,23 @@ import torch\n import torch.nn.functional as F\n \n from vllm.v1.sample.metadata import SamplingMetadata\n-from vllm.v1.sample.rejection_sampler import INVALID_TOKEN_ID, RejectionSampler\n+from vllm.v1.sample.rejection_sampler import (PLACEHOLDER_TOKEN_ID,\n+                                              RejectionSampler)\n+from vllm.v1.spec_decode.metadata import SpecDecodeMetadata\n \n-DEVICE = \"cpu\"\n+DEVICE = \"cuda\"\n \n \n @pytest.fixture\n-def sampler():\n+def rejection_sampler():\n     return RejectionSampler()\n \n \n-def create_logits_tensor(token_ids: list[list[int]],\n+def create_logits_tensor(output_token_ids: list[list[int]],\n                          vocab_size: int = 100) -> torch.Tensor:\n     \"\"\"Helper function to create logits tensor that \n        will produce desired token ids on argmax\"\"\"\n+    token_ids = [tokens[:-1] for tokens in output_token_ids]\n     num_total_tokens = sum(len(tokens) for tokens in token_ids)\n     logits = torch.full((num_total_tokens, vocab_size), -100.0, device=DEVICE)\n     start_loc = 0\n@@ -31,15 +34,22 @@ def create_logits_tensor(token_ids: list[list[int]],\n \n \n def create_sampling_metadata(\n-        all_greedy: bool,\n-        generators: Optional[dict[int, Any]] = None) -> SamplingMetadata:\n+    all_greedy: bool,\n+    temperature: Optional[torch.Tensor] = None,\n+    generators: Optional[dict[int, Any]] = None,\n+) -> SamplingMetadata:\n     \"\"\"Create a v1 sampling metadata object with all_greedy set \n         to the given value. Either all greedy or all random sampling \n         is used.\n     \"\"\"\n     generators = generators or {}\n+    if all_greedy:\n+        temperature = None\n+    else:\n+        assert temperature is not None\n+\n     return SamplingMetadata(\n-        temperature=torch.tensor([]),\n+        temperature=temperature,\n         all_greedy=all_greedy,\n         all_random=not all_greedy,\n         top_p=None,\n@@ -61,7 +71,7 @@ def create_sampling_metadata(\n \n \n ########################### Tests for Greedy Sampling ###################\n-def test_perfect_match(sampler):\n+def test_perfect_match(rejection_sampler):\n     \"\"\"Test when output tokens perfectly match speculated tokens\"\"\"\n     spec_tokens = [[1, 2, 3]]\n     output_tokens = [[1, 2, 3, 4]]  # 4 is the bonus token\n@@ -70,15 +80,23 @@ def test_perfect_match(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n     expected = torch.tensor([[1, 2, 3, 4]],\n                             dtype=torch.int,\n                             device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_early_mismatch(sampler):\n+def test_early_mismatch(rejection_sampler):\n     \"\"\"Test when there's an early mismatch in tokens\"\"\"\n     spec_tokens = [[1, 2, 3]]\n     output_tokens = [[1, 5, 3, 4]]  # Mismatch at position 1\n@@ -87,15 +105,25 @@ def test_early_mismatch(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n-    expected = torch.tensor([[1, 5, INVALID_TOKEN_ID, INVALID_TOKEN_ID]],\n-                            dtype=torch.int,\n-                            device=logits.device)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n+    expected = torch.tensor(\n+        [[1, 5, PLACEHOLDER_TOKEN_ID, PLACEHOLDER_TOKEN_ID]],\n+        dtype=torch.int,\n+        device=logits.device,\n+    )\n     assert torch.equal(output, expected)\n \n \n-def test_multiple_sequences(sampler):\n+def test_multiple_sequences(rejection_sampler):\n     \"\"\"Test handling multiple sequences of speculated tokens\"\"\"\n     spec_tokens = [[1, 2], [3]]\n     output_tokens = [[1, 2, 5], [3,\n@@ -105,15 +133,23 @@ def test_multiple_sequences(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor(\n         [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n-    expected = torch.tensor([[1, 2, 5], [3, 4, INVALID_TOKEN_ID]],\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n+    expected = torch.tensor([[1, 2, 5], [3, 4, PLACEHOLDER_TOKEN_ID]],\n                             dtype=torch.int,\n                             device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_single_token_sequence(sampler):\n+def test_single_token_sequence(rejection_sampler):\n     \"\"\"Test handling sequences with single token\"\"\"\n     spec_tokens = [[1]]\n     output_tokens = [[1, 2]]  # Single token with bonus token 2\n@@ -122,13 +158,21 @@ def test_single_token_sequence(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n     expected = torch.tensor([[1, 2]], dtype=torch.int, device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_empty_sequence(sampler):\n+def test_empty_sequence(rejection_sampler):\n     \"\"\"Test handling empty sequence of speculated tokens\"\"\"\n     spec_tokens: list[list[int]] = [[]]\n     output_tokens = [[5]]  # Just the bonus token\n@@ -137,13 +181,21 @@ def test_empty_sequence(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([output_tokens[0][-1]],\n                                       device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n     expected = torch.tensor([[5]], dtype=torch.int, device=logits.device)\n     assert torch.equal(output, expected)\n \n \n-def test_multiple_mismatches(sampler):\n+def test_multiple_mismatches(rejection_sampler):\n     \"\"\"Test handling multiple sequences with mismatches\"\"\"\n     spec_tokens = [[1, 2, 3], [4, 5, 6]]\n     output_tokens = [[1, 2, 7, 6], [4, 8, 6,\n@@ -153,12 +205,22 @@ def test_multiple_mismatches(sampler):\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor(\n         [output_tokens[0][-1], output_tokens[1][-1]], device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n-    expected = torch.tensor([[1, 2, 7, INVALID_TOKEN_ID],\n-                             [4, 8, INVALID_TOKEN_ID, INVALID_TOKEN_ID]],\n-                            dtype=torch.int,\n-                            device=logits.device)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n+    expected = torch.tensor(\n+        [[1, 2, 7, PLACEHOLDER_TOKEN_ID],\n+         [4, 8, PLACEHOLDER_TOKEN_ID, PLACEHOLDER_TOKEN_ID]],\n+        dtype=torch.int,\n+        device=logits.device,\n+    )\n     assert torch.equal(output, expected)\n \n \n@@ -166,18 +228,27 @@ def test_multiple_mismatches(sampler):\n     \"spec_tokens,output_tokens,expected\",\n     [\n         ([[1, 2]], [[1, 2, 3]], [[1, 2, 3]]),  # Perfect match with bonus\n-        ([[1]], [[2, 3]], [[2, INVALID_TOKEN_ID]]),  # First mismatch\n+        ([[1]], [[2, 3]], [[2, PLACEHOLDER_TOKEN_ID]]),  # First mismatch\n         ([[1, 2], [3, 4]], [[1, 5, 6], [3, 4, 7]],\n-         [[1, 5, INVALID_TOKEN_ID], [3, 4, 7]]),  # Mixed matches\n+         [[1, 5, PLACEHOLDER_TOKEN_ID], [3, 4, 7]]),  # Mixed matches\n     ])\n-def test_parametrized_cases(sampler, spec_tokens, output_tokens, expected):\n+def test_parametrized_cases(rejection_sampler, spec_tokens, output_tokens,\n+                            expected):\n     \"\"\"Parametrized test for various matching scenarios\"\"\"\n     metadata = create_sampling_metadata(all_greedy=True)\n     logits = create_logits_tensor(output_tokens)\n     bonus_token_tensor = torch.tensor([tokens[-1] for tokens in output_tokens],\n                                       device=logits.device)\n-\n-    output = sampler(spec_tokens, None, bonus_token_tensor, logits, metadata)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(spec_tokens,\n+                                                         device=logits.device)\n+\n+    output = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=None,\n+        target_logits=logits,\n+        bonus_token_ids=bonus_token_tensor,\n+        sampling_metadata=metadata,\n+    )\n     expected_tensor = torch.tensor(expected,\n                                    dtype=torch.int,\n                                    device=logits.device)\n@@ -190,21 +261,31 @@ def test_parametrized_cases(sampler, spec_tokens, output_tokens, expected):\n @pytest.mark.parametrize(\"batch_size\", [1, 4, 8])\n @pytest.mark.parametrize(\"frac_seeded\", [0.0, 0.5])\n @pytest.mark.parametrize(\"n_rep\", [20])\n-def test_deterministic_when_seeded(sampler, k: int, vocab_size: int,\n-                                   batch_size: int, frac_seeded: float,\n-                                   n_rep: int):\n-    draft_probs = torch.rand(batch_size, k, vocab_size, dtype=torch.float32)\n-    target_probs = torch.rand(batch_size * (k + 1),\n-                              vocab_size,\n-                              dtype=torch.float32)\n+def test_deterministic_when_seeded(\n+    rejection_sampler,\n+    k: int,\n+    vocab_size: int,\n+    batch_size: int,\n+    frac_seeded: float,\n+    n_rep: int,\n+):\n+    num_tokens = batch_size * k\n+    draft_probs = torch.rand(num_tokens,\n+                             vocab_size,\n+                             dtype=torch.float32,\n+                             device=DEVICE)\n+    draft_probs = F.softmax(draft_probs, dim=-1)\n+    target_logits = torch.rand_like(draft_probs)\n     bonus_token_ids = torch.randint(low=0,\n                                     high=vocab_size,\n                                     size=(batch_size, 1),\n-                                    dtype=torch.int64)\n+                                    dtype=torch.int64,\n+                                    device=DEVICE)\n     draft_token_ids = torch.randint(low=0,\n                                     high=vocab_size,\n                                     size=(batch_size, k),\n-                                    dtype=torch.int64)\n+                                    dtype=torch.int64,\n+                                    device=DEVICE)\n \n     seeded_mask = torch.rand(batch_size, dtype=torch.float32) <= frac_seeded\n \n@@ -215,10 +296,21 @@ def test_deterministic_when_seeded(sampler, k: int, vocab_size: int,\n             for i in range(batch_size) if seeded_mask[i]\n         }\n \n+        temperature = torch.ones(batch_size,\n+                                 dtype=torch.float32,\n+                                 device=DEVICE)\n         sampling_metadata = create_sampling_metadata(all_greedy=False,\n+                                                     temperature=temperature,\n                                                      generators=seeded_seqs)\n-        rep_result = sampler(draft_token_ids.tolist(), draft_probs,\n-                             bonus_token_ids, target_probs, sampling_metadata)\n+        spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n+            draft_token_ids.tolist(), device=DEVICE)\n+        rep_result = rejection_sampler(\n+            spec_decode_metadata,\n+            draft_probs=draft_probs,\n+            target_logits=target_logits,\n+            bonus_token_ids=bonus_token_ids,\n+            sampling_metadata=sampling_metadata,\n+        )\n \n         results.append(rep_result)\n \n@@ -257,10 +349,10 @@ def test_rejection_sampling_approximates_target_distribution():\n     num_reference_probs = 100\n \n     # Prepare draft, target, and reference probability distributions\n-    draft_probs, target_probs = (F.softmax(\n-        torch.rand(vocab_size, dtype=torch.float32),\n-        dim=-1,\n-    ) for _ in range(2))\n+    draft_probs = F.softmax(torch.rand(vocab_size, dtype=torch.float32),\n+                            dim=-1)\n+    target_logits = torch.rand(vocab_size, dtype=torch.float32)\n+    target_probs = F.softmax(target_logits, dim=-1)\n     reference_probs = F.softmax(\n         torch.rand(num_reference_probs, vocab_size, dtype=torch.float32),\n         dim=-1,\n@@ -273,7 +365,7 @@ def test_rejection_sampling_approximates_target_distribution():\n     for num_samples in sample_sizes:\n         # Sample using rejection sampling.\n         rej_sample_probs = estimate_rejection_sampling_pdf(\n-            draft_probs, target_probs, k, vocab_size, num_samples)\n+            draft_probs, target_logits, k, vocab_size, num_samples)\n         rej_sample_probs = rej_sample_probs.to(DEVICE)\n \n         # Average distance from reference probs.\n@@ -313,7 +405,7 @@ def get_ratio_first_to_last(elements: list[float]) -> float:\n \n def estimate_rejection_sampling_pdf(\n     draft_probs: torch.Tensor,\n-    target_probs: torch.Tensor,\n+    target_logits: torch.Tensor,\n     k: int,\n     vocab_size: int,\n     num_samples: int,\n@@ -323,35 +415,44 @@ def estimate_rejection_sampling_pdf(\n \n     Args:\n         draft_probs: Draft probability distribution.\n-        target_probs: Target probability distribution.\n+        target_logits: Target logits.\n         num_samples: Number of samples to draw.\n \n     Returns:\n         Estimated probability distribution of the output tokens.\n     \"\"\"\n-    sampler = RejectionSampler()\n-    # Repeat draft probs num_samples times.\n+    rejection_sampler = RejectionSampler()\n+    num_tokens = num_samples * k\n+    # Repeat draft probs num_samples * k times.\n     draft_probs = draft_probs.reshape(1, 1,\n                                       vocab_size).repeat(num_samples, k, 1)\n \n-    # Repeat target probs num_samples * (k + 1) times.\n-    target_probs = target_probs.reshape(1, 1, vocab_size).repeat(\n-        num_samples, k + 1, 1).reshape(num_samples * (k + 1), vocab_size)\n+    # Repeat target probs num_tokens times.\n+    target_logits = target_logits.reshape(1, vocab_size).repeat(num_tokens, 1)\n \n     # Randomly sample draft token ids from draft probs.\n     draft_token_ids = torch.multinomial(draft_probs[:, 0, :],\n                                         num_samples=k,\n                                         replacement=True).reshape(\n                                             num_samples, k)\n+    draft_probs = draft_probs.view(num_tokens, vocab_size)\n \n     # Bonus tokens not used but required.\n     bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n                                   device=DEVICE).repeat(num_samples, 1)\n \n-    sampling_metadata = create_sampling_metadata(all_greedy=False)\n-    output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\n-                               bonus_token_ids, target_probs,\n-                               sampling_metadata)\n+    temperature = torch.ones(num_samples, dtype=torch.float32, device=DEVICE)\n+    sampling_metadata = create_sampling_metadata(all_greedy=False,\n+                                                 temperature=temperature)\n+    spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n+        draft_token_ids.tolist(), device=bonus_token_ids.device)\n+    output_token_ids = rejection_sampler(\n+        spec_decode_metadata,\n+        draft_probs=draft_probs,\n+        target_logits=target_logits,\n+        bonus_token_ids=bonus_token_ids,\n+        sampling_metadata=sampling_metadata,\n+    )\n     output_token_ids = output_token_ids[:, :-1].flatten()\n \n     hist = torch.histogram(output_token_ids.to(dtype=torch.float,\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex bf214f314..b2937462a 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -35,7 +35,6 @@ if TYPE_CHECKING:\n     VLLM_TRACE_FUNCTION: int = 0\n     VLLM_ATTENTION_BACKEND: Optional[str] = None\n     VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None\n-    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False\n     VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False\n     VLLM_PP_LAYER_PARTITION: Optional[str] = None\n     VLLM_CPU_KVCACHE_SPACE: int = 0\ndiff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex edae654b5..6f4641717 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -46,7 +46,7 @@ class SamplerOutput:\n     # [num_reqs, max_num_generated_tokens]\n     # Different requests can have different number of generated tokens.\n     # All requests are padded to max_num_generated_tokens.\n-    # INVALID_TOKEN_ID (-1 by default) is used for padding.\n+    # PLACEHOLDER_TOKEN_ID (-1 by default) is used for padding.\n     sampled_token_ids: torch.Tensor\n     logprobs_tensors: Optional[LogprobsTensors]\n \ndiff --git a/vllm/v1/sample/ops/utils.py b/vllm/v1/sample/ops/utils.py\nnew file mode 100644\nindex 000000000..a54e20603\n--- /dev/null\n+++ b/vllm/v1/sample/ops/utils.py\n@@ -0,0 +1,30 @@\n+# SPDX-License-Identifier: Apache-2.0\n+from typing import Union\n+\n+import torch\n+\n+\n+def compiled_softmax(\n+    logits: torch.Tensor,\n+    temperature: Union[float, torch.Tensor] = 1.0,\n+) -> torch.Tensor:\n+    \"\"\"Faster softmax kernel generated by torch.compile.\n+\n+    Args:\n+        logits: [n, vocab_size]\n+        temperature: [n] or float\n+    \"\"\"\n+    # NOTE(woosuk): Avoid recompilation by marking the first dim as dynamic.\n+    torch._dynamo.mark_dynamic(logits, index=0)\n+    if isinstance(temperature, torch.Tensor):\n+        torch._dynamo.mark_dynamic(temperature, index=0)\n+    return _softmax(logits, temperature)\n+\n+\n+@torch.compile\n+def _softmax(\n+    logits: torch.Tensor,\n+    temperature: Union[float, torch.Tensor],\n+) -> torch.Tensor:\n+    logits = logits / temperature\n+    return torch.softmax(logits, dim=-1, dtype=torch.float32)\ndiff --git a/vllm/v1/sample/rejection_sampler.py b/vllm/v1/sample/rejection_sampler.py\nindex 5601c62e9..6284ae4b4 100644\n--- a/vllm/v1/sample/rejection_sampler.py\n+++ b/vllm/v1/sample/rejection_sampler.py\n@@ -3,25 +3,32 @@ from typing import Optional\n \n import torch\n import torch.nn as nn\n-from torch.nn.utils.rnn import pad_sequence\n+import triton\n+import triton.language as tl\n \n from vllm.logger import init_logger\n from vllm.v1.sample.metadata import SamplingMetadata\n-from vllm.v1.spec_decode.utils import random_sample\n+from vllm.v1.sample.ops.utils import compiled_softmax\n+from vllm.v1.spec_decode.metadata import SpecDecodeMetadata\n \n logger = init_logger(__name__)\n-INVALID_TOKEN_ID = -1\n+\n+PLACEHOLDER_TOKEN_ID: tl.constexpr = -1\n+GREEDY_TEMPERATURE: tl.constexpr = -1\n+# Maximum number of speculative draft tokens allowed per request in a single\n+# step. This value is chosen to be large enough to handle typical use cases.\n+MAX_SPEC_LEN = 32\n \n \n class RejectionSampler(nn.Module):\n     \"\"\"\n-    The implementation strictly follows the algorithm described in \n+    The implementation strictly follows the algorithm described in\n         https://arxiv.org/abs/2211.17192.\n     However, we want to clarify the terminology used in the implementation:\n-    accepted tokens: tokens that are accepted based on the relationship \n+    accepted tokens: tokens that are accepted based on the relationship\n             between the \"raw\" draft and target probabilities.\n     recovered tokens: tokens that are sampled based on the adjusted probability\n-        distribution, which is derived from both the draft and target \n+        distribution, which is derived from both the draft and target\n         probabilities.\n     bonus tokens:\n         If all proposed tokens are accepted, the bonus token is added to the\n@@ -31,48 +38,42 @@ class RejectionSampler(nn.Module):\n         sampling process. For example, we can use top_p, top_k sampling for\n         bonus tokens, while spec decode does not support these sampling\n         strategies.\n-    output tokens: \n-        Tokens are finally generated with the rejection sampler. \n+    output tokens:\n+        Tokens are finally generated with the rejection sampler.\n         output tokens = accepted tokens + recovered tokens + bonus tokens\n     \"\"\"\n \n-    def __init__(self):\n-        super().__init__()\n-\n     def forward(\n         self,\n-        draft_token_ids: list[list[int]],\n+        metadata: SpecDecodeMetadata,\n+        # [num_tokens, vocab_size]\n         draft_probs: Optional[torch.Tensor],\n-        bonus_token_ids_tensor: torch.Tensor,  # [batch_size, 1]\n-        target_probs: torch.Tensor,  # [num_total_tokens, vocab_size]\n+        # [num_tokens, vocab_size]\n+        target_logits: torch.Tensor,\n+        # [batch_size, 1]\n+        bonus_token_ids: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n     ) -> torch.Tensor:\n         '''\n         Args:\n-            draft_token_ids (List[List[int]]):\n-                A 2D list of token IDs for each request in the batch. \n-                Each request might have different number of draft tokens. \n-                It may also contain empty lists for requests that have \n-                no draft tokens.\n+            metadata:\n+                Metadata for spec decoding.\n             draft_probs (Optional[torch.Tensor]):\n                 Probability distribution for the draft tokens. Shape is\n-                [batch_size, max_spec_len, vocab_size]. Can be None if \n-                probabilities are not provided, which is the case for\n-                ngram spec decode.\n+                [num_tokens, vocab_size]. Can be None if probabilities are\n+                not provided, which is the case for ngram spec decode.\n+            target_logits (torch.Tensor):\n+                Target model's logits probability distribution.\n+                Shape is [num_tokens, vocab_size]. Here, probabilities from\n+                different requests are flattened into a single tensor because\n+                this is the shape of the output logits.\n             bonus_token_ids_tensor (torch.Tensor):\n-                A tensor containing bonus tokens. Shape is [batch_size, 1]. \n-                Bonus tokens are added to the end of the sequence if all \n-                proposed tokens are accepted. We generate the bonus tokens \n-                outside of the rejection sampler with the default sampling \n-                strategy. It allows for more flexibility in the sampling \n+                A tensor containing bonus tokens. Shape is [batch_size, 1].\n+                Bonus tokens are added to the end of the sequence if all\n+                proposed tokens are accepted. We generate the bonus tokens\n+                outside of the rejection sampler with the default sampling\n+                strategy. It allows for more flexibility in the sampling\n                 process such as top_p, top_k sampling.\n-            target_probs (torch.Tensor):\n-                Target model probability distribution.\n-                Shape is [num_total_tokens, vocab_size]. num_total_tokens \n-                is the total number of tokens from all requests. Here, \n-                probabilities from different requests are flattened into\n-                a single tensor because this is the shape of the output \n-                logits.\n             sampling_metadata (SamplingMetadata):\n                 Additional metadata needed for sampling, such as temperature,\n                 top-k/top-p parameters, or other relevant information.\n@@ -80,268 +81,481 @@ class RejectionSampler(nn.Module):\n             output_token_ids (torch.Tensor):\n                 A tensor containing the final output token IDs.\n         '''\n-\n-        # NOTE: The following input preparationg can be moved\n-        # to the model runner with a persistent manner for better\n-        # performance.\n-        # Convert draft token IDs to a tensor, split by sample_lens, then pad.\n-        draft_token_ids = [\n-            torch.tensor(x, dtype=int, device='cpu') for x in draft_token_ids\n-        ]\n-        draft_token_ids_tensor = pad_sequence(draft_token_ids,\n-                                              batch_first=True,\n-                                              padding_value=INVALID_TOKEN_ID)\n-\n-        # NOTE: CPU <-> GPU synchronization happens here.\n-        draft_token_ids_tensor = draft_token_ids_tensor.to(target_probs.device)\n-\n-        # Create one-hot tensor for draft token ids.\n-        # This is used for ngram where we don't have draft_probs.\n-        if draft_probs is None and not sampling_metadata.all_greedy:\n-            vocab_size = target_probs.size(-1)\n-            draft_probs = _create_greedy_token_probs(draft_token_ids_tensor,\n-                                                     vocab_size,\n-                                                     target_probs.device)\n-        sample_lens = [len(x) + 1 for x in draft_token_ids]\n-        target_probs = _convert_2d_probs(target_probs, sample_lens)\n-\n-        return self.forward_native(draft_token_ids_tensor, draft_probs,\n-                                   bonus_token_ids_tensor, target_probs,\n-                                   sampling_metadata)\n-\n-    # TODO: The following method can be optimized for better performance.\n-    def forward_native(\n-        self,\n-        draft_token_ids_tensor: torch.Tensor,\n-        # [batch_size, max_spec_len, vocab_size]\n-        draft_probs: Optional[torch.Tensor],\n-        bonus_token_ids_tensor: torch.Tensor,\n-        # [batch_size, max_spec_len + 1, vocab_size]\n-        target_probs: torch.Tensor,\n-        sampling_metadata: SamplingMetadata,\n-    ) -> torch.Tensor:\n-        # Add 1 to include the 'bonus' token.\n-        if sampling_metadata.all_greedy:\n-            # Produce a mask that remains 1 (True) until the first\n-            # mismatch (cumprod turns 0 after a mismatch).\n-            target_token_ids_tensor = target_probs.argmax(dim=-1)\n-            accept_mask = (target_token_ids_tensor[:, :-1] ==\n-                           draft_token_ids_tensor).cumprod(dim=1)\n-\n-            # Identify valid positions (non-padding).\n-            valid_mask = target_token_ids_tensor != INVALID_TOKEN_ID\n-            # Generate mask with bonus token.\n-            generate_mask = torch.cat([\n-                accept_mask,\n-                torch.zeros(accept_mask.size(0), 1, device=accept_mask.device)\n-            ],\n-                                      dim=1).to(torch.bool) & valid_mask\n-            zeros_mask = (generate_mask == 0)\n-            first_zero_idx = zeros_mask.float().argmax(dim=1)\n-            # Figure out which rows actually contain at least one zero.\n-            rows_with_zero = zeros_mask.any(dim=1)\n-            # Use indexing to set the first zero in each of those rows to 1.\n-            generate_mask[rows_with_zero, first_zero_idx[rows_with_zero]] = 1\n-\n-            output_token_ids = target_token_ids_tensor\n-            output_token_ids[~generate_mask] = INVALID_TOKEN_ID\n-        else:\n-            # Reference: https://arxiv.org/pdf/2211.17192\n-            # 1. Extract the probabilities of the draft tokens.\n-            # [batch_size, max_spec_len]\n-            batch_size = draft_token_ids_tensor.size(0)\n-            max_spec_len = draft_token_ids_tensor.size(1)\n-            invalid_idx = draft_token_ids_tensor == INVALID_TOKEN_ID\n-            draft_token_ids_tensor[invalid_idx] = 0\n-            assert draft_probs is not None\n-            draft_token_probs = draft_probs.gather(\n-                dim=-1, index=draft_token_ids_tensor.unsqueeze(-1)).squeeze(-1)\n-            target_token_probs = target_probs.gather(\n-                dim=-1, index=draft_token_ids_tensor.unsqueeze(-1)).squeeze(-1)\n-            # Force the probabilities of invalid tokens to inf\n-            # so that they are not accepted.\n-            draft_token_probs[invalid_idx] = float('inf')\n-\n-            # 2. Generate uniform samples.\n-            # [batch_size, max_spec_len + 1]\n-            uniform_samples = _create_uniform_samples(\n-                sampling_metadata.generators, batch_size, max_spec_len,\n-                target_probs.device)\n-\n-            # 3. Accept or reject the samples.\n-            # [batch_size, max_spec_len]\n-            # If the draft token probabilities are 0, set them to the smallest\n-            # positive normal value representable by float32.\n-            safe_draft_probs = torch.where(draft_token_probs > 0,\n-                                           draft_token_probs,\n-                                           torch.finfo(torch.float32).tiny)\n-            accepted = uniform_samples <= target_token_probs / safe_draft_probs\n-            accept_mask = accepted.cumprod(dim=1)\n-            # Set the token ids to the draft token ids if accepted, otherwise\n-            # set them to INVALID_TOKEN_ID.\n-            accepted_token_ids = (draft_token_ids_tensor * accept_mask +\n-                                  INVALID_TOKEN_ID * (1 - accept_mask))\n-\n-            # 4. Adjust the distribution for the recovered tokens.\n-            # Clamp the bonus probabilities to the smallest positive normal\n-            # value representable by float32.\n-            bonus_prob = torch.clamp(target_probs[:, :-1, :] - draft_probs,\n-                                     min=torch.finfo(torch.float32).tiny)\n-            normalized_bonus_prob = bonus_prob / bonus_prob.sum(dim=-1,\n-                                                                keepdim=True)\n-\n-            # 5. Sample recovered token ids.\n-            recovered_token_ids = random_sample(\n-                normalized_bonus_prob,\n-                sampling_metadata.generators).reshape(batch_size, max_spec_len)\n-\n-            # 6. Get the final output token ids.\n-            # output_token_ids = accepted_token_ids +\n-            #                    recovered_token_ids +\n-            #                    bonus_token_id\n-            recovered_bonus_token_ids = torch.cat(\n-                [recovered_token_ids, bonus_token_ids_tensor], dim=1)\n-            # Generate mask with bonus tokens.\n-            generate_mask = torch.cat([\n-                accept_mask,\n-                torch.zeros(batch_size, 1, device=accept_mask.device)\n-            ],\n-                                      dim=1).to(torch.bool)\n-            zeros_mask = (generate_mask == 0)\n-            first_zero_idx = zeros_mask.float().argmax(dim=1)\n-            output_token_ids = torch.cat([\n-                accepted_token_ids,\n-                torch.full((batch_size, 1),\n-                           fill_value=INVALID_TOKEN_ID,\n-                           device=accept_mask.device)\n-            ],\n-                                         dim=1)\n-            output_token_ids[torch.arange(batch_size),\n-                             first_zero_idx] = recovered_bonus_token_ids[\n-                                 torch.arange(batch_size), first_zero_idx]\n-\n+        assert metadata.max_spec_len <= MAX_SPEC_LEN\n+        # [num_tokens, vocab_size]\n+        target_probs = compute_probs(\n+            target_logits,\n+            metadata.cu_num_draft_tokens,\n+            sampling_metadata,\n+        )\n+\n+        output_token_ids = rejection_sample(\n+            metadata.draft_token_ids,\n+            metadata.num_draft_tokens,\n+            metadata.max_spec_len,\n+            metadata.cu_num_draft_tokens,\n+            draft_probs,\n+            target_probs,\n+            bonus_token_ids,\n+            sampling_metadata,\n+        )\n         return output_token_ids\n \n-    def compute_probs(self, logits: torch.Tensor,\n-                      sampling_metadata: SamplingMetadata,\n-                      sample_lens: list[int]) -> torch.Tensor:\n-        \"\"\"\n-        Compute probability distribution from logits based on sampling metadata.\n-    \n-        This function applies temperature scaling to the logits and converts \n-        them to probabilities using softmax. Note that division by \n-        temperature is not performed inplace to preserve the original logits \n-        tensor, which will be used by the original sampler to get bonus tokens.\n-        \n-        Args:\n-            logits: Input logits tensor to be converted to probabilities\n-            sampling_metadata: Metadata containing sampling parameters such \n-                    as temperature and whether greedy sampling is used\n-            sample_lens: List of sample lengths used for repeating \n-                    temperature values\n-            \n-        Returns:\n-            torch.Tensor: Probability distribution (softmax of scaled logits) \n-                    if non-greedy sampling is used, otherwise returns the \n-                    original logits\n-        \"\"\"\n+    @staticmethod\n+    def parse_output(\n+        output_token_ids: torch.Tensor,\n+        vocab_size: int,\n+    ) -> list[list[int]]:\n+        output_token_ids_np = output_token_ids.cpu().numpy()\n+        # Create mask for valid tokens.\n+        valid_mask = ((output_token_ids_np != PLACEHOLDER_TOKEN_ID) &\n+                      (output_token_ids_np < vocab_size))\n+        outputs = [\n+            row[valid_mask[i]].tolist()\n+            for i, row in enumerate(output_token_ids_np)\n+        ]\n+        return outputs\n+\n+\n+def rejection_sample(\n+    # [num_tokens]\n+    draft_token_ids: torch.Tensor,\n+    # [batch_size]\n+    num_draft_tokens: list[int],\n+    max_spec_len: int,\n+    # [batch_size]\n+    cu_num_draft_tokens: torch.Tensor,\n+    # [num_tokens, vocab_size]\n+    draft_probs: Optional[torch.Tensor],\n+    # [num_tokens, vocab_size]\n+    target_probs: torch.Tensor,\n+    # [batch_size, 1]\n+    bonus_token_ids: torch.Tensor,\n+    sampling_metadata: SamplingMetadata,\n+) -> torch.Tensor:\n+    assert draft_token_ids.ndim == 1\n+    assert draft_probs is None or draft_probs.ndim == 2\n+    assert cu_num_draft_tokens.ndim == 1\n+    assert target_probs.ndim == 2\n+\n+    batch_size = len(num_draft_tokens)\n+    num_tokens = draft_token_ids.shape[0]\n+    vocab_size = target_probs.shape[-1]\n+    device = target_probs.device\n+    assert draft_token_ids.is_contiguous()\n+    assert draft_probs is None or draft_probs.is_contiguous()\n+    assert target_probs.is_contiguous()\n+    assert bonus_token_ids.is_contiguous()\n+    assert target_probs.shape == (num_tokens, vocab_size)\n+\n+    # Create output buffer.\n+    output_token_ids = torch.empty(\n+        (batch_size, max_spec_len + 1),\n+        dtype=torch.int32,  # Consistent with SamplerOutput.sampled_token_ids.\n+        device=device,\n+    )\n+    output_token_ids.fill_(PLACEHOLDER_TOKEN_ID)\n+\n+    if sampling_metadata.all_greedy:\n+        is_greedy = None\n+    else:\n+        is_greedy = sampling_metadata.temperature == GREEDY_TEMPERATURE\n+    if not sampling_metadata.all_random:\n+        # Rejection sampling for greedy sampling requests.\n+        target_argmax = target_probs.argmax(dim=-1)\n+        rejection_greedy_sample_kernel[(batch_size, )](\n+            output_token_ids,\n+            cu_num_draft_tokens,\n+            draft_token_ids,\n+            target_argmax,\n+            bonus_token_ids,\n+            is_greedy,\n+            max_spec_len,\n+            num_warps=1,\n+        )\n         if sampling_metadata.all_greedy:\n-            return logits\n-        assert sampling_metadata.temperature is not None\n-        # We should optimize the following code as\n-        # it will cause CPU -> GPU synchronization.\n-        temperature = torch.repeat_interleave(\n-            sampling_metadata.temperature,\n-            torch.tensor(sample_lens,\n-                         device=sampling_metadata.temperature.device))\n-        temperature = temperature.unsqueeze(dim=1)\n-        logits = logits / temperature\n-        return logits.softmax(dim=-1, dtype=torch.float32)\n-\n-\n-def _create_greedy_token_probs(\n-    token_ids: torch.Tensor,\n-    vocab_size: int,\n-    out_device: torch.device,\n+            return output_token_ids\n+\n+    # Generate uniform probabilities for rejection sampling.\n+    # [num_tokens]\n+    uniform_probs = generate_uniform_probs(\n+        num_tokens,\n+        num_draft_tokens,\n+        sampling_metadata.generators,\n+        device,\n+    )\n+\n+    # Sample recovered tokens for each position.\n+    # [num_tokens]\n+    recovered_token_ids = sample_recovered_tokens(\n+        max_spec_len,\n+        num_draft_tokens,\n+        cu_num_draft_tokens,\n+        draft_token_ids,\n+        draft_probs,\n+        target_probs,\n+        sampling_metadata,\n+        device,\n+    )\n+\n+    # Rejection sampling for random sampling requests.\n+    rejection_random_sample_kernel[(batch_size, )](\n+        output_token_ids,\n+        cu_num_draft_tokens,\n+        draft_token_ids,\n+        draft_probs,\n+        target_probs,\n+        bonus_token_ids,\n+        recovered_token_ids,\n+        uniform_probs,\n+        is_greedy,\n+        max_spec_len,\n+        vocab_size,\n+        IS_NGRAM=draft_probs is None,\n+        num_warps=1,\n+    )\n+    return output_token_ids\n+\n+\n+def compute_probs(\n+    logits: torch.Tensor,  # [num_tokens, vocab_size]\n+    cu_num_draft_tokens: torch.Tensor,  # [batch_size]\n+    sampling_metadata: SamplingMetadata,\n ) -> torch.Tensor:\n-    batch_size, num_tokens = token_ids.shape\n-\n-    token_probs = torch.zeros(batch_size,\n-                              num_tokens,\n-                              vocab_size,\n-                              dtype=torch.float,\n-                              device=out_device)\n-\n-    # Ignore INVALID_TOKEN_ID.\n-    valid_mask = (token_ids != INVALID_TOKEN_ID)\n-    valid_indices = token_ids.clone()\n-    valid_indices[~valid_mask] = 0\n-\n-    token_probs.scatter_(dim=2,\n-                         index=valid_indices.unsqueeze(-1),\n-                         src=valid_mask.unsqueeze(-1).float())\n-\n-    return token_probs\n-\n-\n-def _convert_2d_probs(\n-        probs: torch.Tensor,  # [num_total_tokens, vocab_size]\n-        sample_lens: list[int]) -> torch.Tensor:\n+    \"\"\"Compute probability distribution from logits based on sampling metadata.\n+\n+    This function applies temperature scaling to the logits and converts\n+    them to probabilities using softmax. For greedy decoding, it returns\n+    the original logits.\n+\n+    Args:\n+        logits: Input logits tensor to be converted to probabilities.\n+        cu_num_draft_tokens: Cumulative number of draft tokens.\n+        sampling_metadata: Metadata containing sampling parameters such as\n+            temperature and whether greedy sampling is used.\n+\n+    Returns:\n+        torch.Tensor: Probability distribution (softmax of scaled logits)\n+            if non-greedy sampling is used, otherwise returns the\n+            original logits.\n     \"\"\"\n-        Converts a 2D tensor of probabilities to a 3D tensor with padding.\n-        [num_total_tokens, vocab_size] -> \n-            [batch_size, max_spec_len + 1, vocab_size]\n+    assert logits.ndim == 2\n+    assert cu_num_draft_tokens.ndim == 1\n+    if sampling_metadata.all_greedy:\n+        return logits\n+\n+    num_tokens = logits.shape[0]\n+    batch_size = cu_num_draft_tokens.shape[0]\n+    expanded_temperature = torch.empty(\n+        (num_tokens, 1),\n+        dtype=torch.float32,\n+        device=logits.device,\n+    )\n+    expand_kernel[(batch_size, )](\n+        expanded_temperature,\n+        sampling_metadata.temperature,\n+        cu_num_draft_tokens,\n+        GREEDY_TEMPERATURE,  # replace_from\n+        1,  # replace_to\n+        MAX_NUM_TOKENS=MAX_SPEC_LEN,\n+        num_warps=1,\n+    )\n+    output_prob = compiled_softmax(logits, expanded_temperature)\n+    return output_prob\n+\n+\n+def generate_uniform_probs(\n+    num_tokens: int,\n+    num_draft_tokens: list[int],\n+    generators: dict[int, torch.Generator],\n+    device: torch.device,\n+) -> torch.Tensor:\n     \"\"\"\n-    cumulative_lens = torch.cumsum(torch.tensor(sample_lens,\n-                                                device=probs.device),\n-                                   dim=0)\n-    split_indices = cumulative_lens[:-1].tolist()  # Exclude last index\n-\n-    # Split into chunks without loops\n-    chunks = torch.tensor_split(probs, split_indices, dim=0)\n-\n-    # Pad all sequences to maximum length\n-    padded_probs = pad_sequence(chunks, batch_first=True, padding_value=0.0)\n-    return padded_probs\n-\n-\n-def _create_uniform_samples(seeded_seqs: dict[int, torch.Generator],\n-                            batch_size: int, k: int,\n-                            device: torch.device) -> torch.Tensor:\n+    Generates a batch of uniform random samples, with optional seeding\n+    if available.\n+\n+    This method creates a tensor of shape `(num_tokens, )` filled\n+    with uniform random values in the range [0, 1). If `generators` is provided,\n+    the requests with their own seeds will use the provided `torch.Generator`\n+    for reproducibility. The samples for the other requests will be generated\n+    without a seed.\n+\n+    Args:\n+        num_tokens : int\n+            Total number of tokens.\n+        num_draft_tokens : List[List[int]]\n+            Number of draft tokens per request.\n+        generators : Optional[Dict[int, torch.Generator]]\n+            A dictionary mapping indices in the batch to\n+            `torch.Generator` objects.\n+        device : torch.device\n+            The device on which to allocate the tensor.\n+    Returns:\n+        uniform_rand : torch.Tensor\n+            A tensor of shape `(num_tokens, )` containing uniform\n+            random values in the range [0, 1).\n     \"\"\"\n-        Generates a batch of uniform random samples, with optional seeding \n-        for specific sequences.\n-\n-        This method creates a tensor of shape `(batch_size, k)` filled \n-        with uniform random values in the range [0, 1). If `seeded_seqs` \n-        is provided, the sequences corresponding to specific indices \n-        will be generated using the provided `torch.Generator` for \n-        reproducibility. The other sequences will be generated without \n-        a seed.\n-\n-        Args:\n-            seeded_seqs : Optional[Dict[int, torch.Generator]]\n-                A dictionary mapping indices in the batch to \n-                `torch.Generator` objects.\n-            batch_size : int\n-                The number of sequences to generate.\n-            k : int\n-                The number of random samples per sequence.\n-            device : torch.device\n-                The device on which to allocate the tensor.\n-\n-        Returns:\n-            uniform_rand : torch.Tensor\n-                A tensor of shape `(batch_size, k)` containing uniform \n-                random values in the range [0, 1).\n-        \"\"\"\n-\n-    uniform_rand = torch.rand(batch_size,\n-                              k,\n-                              dtype=torch.float32,\n-                              device=device)\n-    # Apply seeded generators only where needed\n-    if seeded_seqs:\n-        for idx, generator in seeded_seqs.items():\n-            uniform_rand[idx].uniform_(0, 1, generator=generator)\n-    return uniform_rand\n+    uniform_probs = torch.rand(\n+        (num_tokens, ),\n+        dtype=torch.float32,\n+        device=device,\n+    )\n+    start_idx = 0\n+    for req_idx, n in enumerate(num_draft_tokens):\n+        # Do not generate random numbers for requests with no draft tokens.\n+        # This can be important for reproducibility.\n+        if n == 0:\n+            continue\n+        end_idx = start_idx + n\n+        generator = generators.get(req_idx)\n+        if generator is not None:\n+            uniform_probs[start_idx:end_idx].uniform_(generator=generator)\n+        start_idx = end_idx\n+    return uniform_probs\n+\n+\n+def sample_recovered_tokens(\n+    max_spec_len: int,\n+    num_draft_tokens: list[int],\n+    # [batch_size]\n+    cu_num_draft_tokens: torch.Tensor,\n+    # [num_tokens]\n+    draft_token_ids: torch.Tensor,\n+    # [num_tokens, vocab_size]\n+    draft_probs: Optional[torch.Tensor],\n+    # [num_tokens, vocab_size]\n+    target_probs: torch.Tensor,\n+    sampling_metadata: SamplingMetadata,\n+    device: torch.device,\n+) -> torch.Tensor:\n+    # NOTE(woosuk): Create only one distribution for each request.\n+    batch_size = len(num_draft_tokens)\n+    vocab_size = target_probs.shape[-1]\n+    q = torch.empty(\n+        (batch_size, vocab_size),\n+        dtype=torch.float32,\n+        device=device,\n+    )\n+    q.exponential_()\n+    for i, generator in sampling_metadata.generators.items():\n+        # Do not generate random numbers for requests with no draft tokens.\n+        # This can be important for reproducibility.\n+        if num_draft_tokens[i] > 0:\n+            q[i].exponential_(generator=generator)\n+\n+    recovered_token_ids = torch.empty_like(draft_token_ids)\n+    sample_recovered_tokens_kernel[(batch_size, max_spec_len)](\n+        recovered_token_ids,\n+        cu_num_draft_tokens,\n+        draft_token_ids,\n+        draft_probs,\n+        target_probs,\n+        q,\n+        vocab_size,\n+        triton.next_power_of_2(vocab_size),\n+        IS_NGRAM=draft_probs is None,\n+    )\n+    return recovered_token_ids\n+\n+\n+# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.\n+@triton.jit(do_not_specialize=[\"max_spec_len\"])\n+def rejection_greedy_sample_kernel(\n+    output_token_ids_ptr,  # [batch_size, max_spec_len + 1]\n+    cu_num_draft_tokens_ptr,  # [batch_size]\n+    draft_token_ids_ptr,  # [num_tokens]\n+    target_argmax_ptr,  # [num_tokens]\n+    bonus_token_ids_ptr,  # [batch_size]\n+    is_greedy_ptr,  # [batch_size] or None\n+    max_spec_len,\n+):\n+    req_idx = tl.program_id(0)\n+    # FIXME(woosuk): Because is_greedy_ptr is not None at profiling run,\n+    # re-compilation may happen during runtime when is_greedy_ptr is None.\n+    if is_greedy_ptr is None:\n+        is_greedy = True\n+    else:\n+        is_greedy = tl.load(is_greedy_ptr + req_idx)\n+    if not is_greedy:\n+        # Early exit for non-greedy sampling requests.\n+        return\n+\n+    if req_idx == 0:\n+        start_idx = 0\n+    else:\n+        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)\n+    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)\n+    num_draft_tokens = end_idx - start_idx\n+\n+    rejected = False\n+    for pos in range(num_draft_tokens):\n+        if not rejected:\n+            draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)\n+            target_argmax_id = tl.load(target_argmax_ptr + start_idx + pos)\n+            tl.store(output_token_ids_ptr + req_idx * (max_spec_len + 1) + pos,\n+                     target_argmax_id)\n+            if draft_token_id != target_argmax_id:\n+                # Reject.\n+                rejected = True\n+\n+    if not rejected:\n+        # If all tokens are accepted, append the bonus token.\n+        bonus_token_id = tl.load(bonus_token_ids_ptr + req_idx)\n+        tl.store(\n+            output_token_ids_ptr + req_idx * (max_spec_len + 1) +\n+            num_draft_tokens, bonus_token_id)\n+\n+\n+# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.\n+@triton.jit(do_not_specialize=[\"max_spec_len\"])\n+def rejection_random_sample_kernel(\n+    output_token_ids_ptr,  # [batch_size, max_spec_len + 1]\n+    cu_num_draft_tokens_ptr,  # [batch_size]\n+    draft_token_ids_ptr,  # [num_tokens]\n+    draft_probs_ptr,  # [num_tokens, vocab_size] or None\n+    target_probs_ptr,  # [num_tokens, vocab_size]\n+    bonus_token_ids_ptr,  # [batch_size]\n+    recovered_token_ids_ptr,  # [num_tokens]\n+    uniform_probs_ptr,  # [num_tokens]\n+    is_greedy_ptr,  # [batch_size]\n+    max_spec_len,\n+    vocab_size,\n+    IS_NGRAM: tl.constexpr,\n+):\n+    req_idx = tl.program_id(0)\n+    is_greedy = tl.load(is_greedy_ptr + req_idx)\n+    if is_greedy:\n+        # Early exit for greedy sampling requests.\n+        return\n+\n+    if req_idx == 0:\n+        start_idx = 0\n+    else:\n+        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)\n+    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)\n+    num_draft_tokens = end_idx - start_idx\n+\n+    rejected = False\n+    for pos in range(num_draft_tokens):\n+        if not rejected:\n+            draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)\n+            if IS_NGRAM:\n+                draft_prob = 1\n+            else:\n+                draft_prob = tl.load(draft_probs_ptr +\n+                                     (start_idx + pos) * vocab_size +\n+                                     draft_token_id)\n+            target_prob = tl.load(target_probs_ptr +\n+                                  (start_idx + pos) * vocab_size +\n+                                  draft_token_id)\n+            uniform_prob = tl.load(uniform_probs_ptr + start_idx + pos)\n+            # NOTE(woosuk): While the draft probability should never be 0,\n+            # we check it to avoid NaNs. If it happens to be 0, we reject.\n+            if draft_prob > 0 and target_prob / draft_prob >= uniform_prob:\n+                # Accept.\n+                token_id = draft_token_id\n+            else:\n+                # Reject. Use recovered token.\n+                rejected = True\n+                token_id = tl.load(recovered_token_ids_ptr + start_idx + pos)\n+            tl.store(output_token_ids_ptr + req_idx * (max_spec_len + 1) + pos,\n+                     token_id)\n+\n+    if not rejected:\n+        # If all tokens are accepted, append the bonus token.\n+        bonus_token_id = tl.load(bonus_token_ids_ptr + req_idx)\n+        tl.store(\n+            output_token_ids_ptr + req_idx * (max_spec_len + 1) +\n+            num_draft_tokens, bonus_token_id)\n+\n+\n+# NOTE(woosuk): Avoid specialization to prevent unnecessary recompilation.\n+@triton.jit(do_not_specialize=[\"replace_from\", \"replace_to\"])\n+def expand_kernel(\n+    output_ptr,  # [num_tokens]\n+    input_ptr,  # [batch_size]\n+    cu_num_tokens_ptr,  # [batch_size]\n+    replace_from,\n+    replace_to,\n+    MAX_NUM_TOKENS: tl.constexpr,\n+):\n+    req_idx = tl.program_id(0)\n+    if req_idx == 0:  # noqa: SIM108\n+        start_idx = 0\n+    else:\n+        start_idx = tl.load(cu_num_tokens_ptr + req_idx - 1)\n+    end_idx = tl.load(cu_num_tokens_ptr + req_idx)\n+    num_tokens = end_idx - start_idx\n+\n+    src_val = tl.load(input_ptr + req_idx)\n+    src_val = tl.where(src_val == replace_from, replace_to, src_val)\n+    offset = tl.arange(0, MAX_NUM_TOKENS)\n+    tl.store(output_ptr + start_idx + offset,\n+             src_val,\n+             mask=offset < num_tokens)\n+\n+\n+@triton.jit\n+def sample_recovered_tokens_kernel(\n+    output_token_ids_ptr,  # [num_tokens]\n+    cu_num_draft_tokens_ptr,  # [batch_size]\n+    draft_token_ids_ptr,  # [num_tokens]\n+    draft_probs_ptr,  # [num_tokens, vocab_size] or None\n+    target_probs_ptr,  # [num_tokens, vocab_size]\n+    q_ptr,  # [batch_size, vocab_size]\n+    vocab_size,\n+    PADDED_VOCAB_SIZE: tl.constexpr,\n+    IS_NGRAM: tl.constexpr,\n+):\n+    req_idx = tl.program_id(0)\n+    if req_idx == 0:\n+        start_idx = 0\n+    else:\n+        start_idx = tl.load(cu_num_draft_tokens_ptr + req_idx - 1)\n+    end_idx = tl.load(cu_num_draft_tokens_ptr + req_idx)\n+    num_draft_tokens = end_idx - start_idx\n+\n+    # Early exit for out-of-range positions.\n+    pos = tl.program_id(1)\n+    if pos >= num_draft_tokens:\n+        return\n+\n+    vocab_offset = tl.arange(0, PADDED_VOCAB_SIZE)\n+    if IS_NGRAM:\n+        draft_token_id = tl.load(draft_token_ids_ptr + start_idx + pos)\n+        orig_prob = tl.load(target_probs_ptr + (start_idx + pos) * vocab_size +\n+                            draft_token_id)\n+        # Temporarily zero out the probability of the draft token.\n+        # This is essentially the same as target_prob - draft_prob, except that\n+        # n-gram does not have draft_prob. We regard it as 1.\n+        tl.store(\n+            target_probs_ptr + (start_idx + pos) * vocab_size + draft_token_id,\n+            0)\n+        prob = tl.load(target_probs_ptr + (start_idx + pos) * vocab_size +\n+                       vocab_offset,\n+                       mask=vocab_offset < vocab_size,\n+                       other=0)\n+    else:\n+        draft_prob = tl.load(draft_probs_ptr + (start_idx + pos) * vocab_size +\n+                             vocab_offset,\n+                             mask=vocab_offset < vocab_size,\n+                             other=0)\n+        target_prob = tl.load(target_probs_ptr +\n+                              (start_idx + pos) * vocab_size + vocab_offset,\n+                              mask=vocab_offset < vocab_size,\n+                              other=0)\n+        prob = tl.maximum(target_prob - draft_prob, 0)\n+        # NOTE(woosuk): We don't need `prob = prob / tl.sum(prob)` here because\n+        # `tl.argmax` will select the maximum value.\n+\n+    q = tl.load(q_ptr + req_idx * vocab_size + vocab_offset,\n+                mask=vocab_offset < vocab_size,\n+                other=float(\"-inf\"))\n+    recovered_id = tl.argmax(prob / q, axis=-1)\n+    tl.store(output_token_ids_ptr + start_idx + pos, recovered_id)\n+\n+    if IS_NGRAM:\n+        # Restore the original probability.\n+        tl.store(\n+            target_probs_ptr + (start_idx + pos) * vocab_size + draft_token_id,\n+            orig_prob)\ndiff --git a/vllm/v1/spec_decode/metadata.py b/vllm/v1/spec_decode/metadata.py\nnew file mode 100644\nindex 000000000..1cf650d5f\n--- /dev/null\n+++ b/vllm/v1/spec_decode/metadata.py\n@@ -0,0 +1,61 @@\n+# SPDX-License-Identifier: Apache-2.0\n+from dataclasses import dataclass\n+\n+import numpy as np\n+import torch\n+\n+\n+@dataclass\n+class SpecDecodeMetadata:\n+\n+    # [num_tokens]\n+    draft_token_ids: torch.Tensor\n+    # [batch_size]\n+    num_draft_tokens: list[int]\n+    # [batch_size]\n+    cu_num_draft_tokens: torch.Tensor\n+    # [num_tokens]\n+    target_logits_indices: torch.Tensor\n+    # [batch_size]\n+    bonus_logits_indices: torch.Tensor\n+    # [num_tokens + batch_size]\n+    logits_indices: torch.Tensor\n+\n+    def __post_init__(self):\n+        self.max_spec_len = max(self.num_draft_tokens)\n+\n+    @classmethod\n+    def make_dummy(\n+        cls,\n+        draft_token_ids: list[list[int]],\n+        device: torch.device,\n+    ) -> \"SpecDecodeMetadata\":\n+        batch_size = len(draft_token_ids)\n+        num_draft_tokens = [len(ids) for ids in draft_token_ids]\n+        flattened_draft_token_ids = sum(draft_token_ids, [])\n+        num_tokens = len(flattened_draft_token_ids)\n+\n+        draft_token_ids_tensor = torch.tensor(flattened_draft_token_ids,\n+                                              dtype=torch.int32,\n+                                              device=device)\n+        cu_num_draft_tokens = np.cumsum(num_draft_tokens, dtype=np.int32)\n+        cu_num_draft_tokens_tensor = torch.from_numpy(cu_num_draft_tokens).to(\n+            device)\n+\n+        target_logits_indices = torch.zeros(num_tokens,\n+                                            dtype=torch.int32,\n+                                            device=device)\n+        bonus_logits_indices = torch.zeros(batch_size,\n+                                           dtype=torch.int32,\n+                                           device=device)\n+        logits_indices = torch.zeros(num_tokens + batch_size,\n+                                     dtype=torch.int32,\n+                                     device=device)\n+        return cls(\n+            draft_token_ids=draft_token_ids_tensor,\n+            num_draft_tokens=num_draft_tokens,\n+            cu_num_draft_tokens=cu_num_draft_tokens_tensor,\n+            target_logits_indices=target_logits_indices,\n+            bonus_logits_indices=bonus_logits_indices,\n+            logits_indices=logits_indices,\n+        )\ndiff --git a/vllm/v1/spec_decode/utils.py b/vllm/v1/spec_decode/utils.py\nindex 584140136..d5329ef7b 100644\n--- a/vllm/v1/spec_decode/utils.py\n+++ b/vllm/v1/spec_decode/utils.py\n@@ -1,5 +1,4 @@\n # SPDX-License-Identifier: Apache-2.0\n-from vllm.v1.sample.ops.topk_topp_sampler import random_sample  # noqa\n from vllm.v1.worker.gpu_input_batch import InputBatch\n \n \ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 66015382b..657333c6d 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -34,7 +34,8 @@ from vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n from vllm.v1.outputs import (EMPTY_MODEL_RUNNER_OUTPUT, LogprobsTensors,\n                              ModelRunnerOutput)\n from vllm.v1.sample.metadata import SamplingMetadata\n-from vllm.v1.sample.rejection_sampler import INVALID_TOKEN_ID, RejectionSampler\n+from vllm.v1.sample.rejection_sampler import RejectionSampler\n+from vllm.v1.spec_decode.metadata import SpecDecodeMetadata\n from vllm.v1.spec_decode.ngram_proposer import NgramProposer\n from vllm.v1.spec_decode.utils import is_spec_decode_supported\n from vllm.v1.utils import bind_kv_cache\n@@ -149,7 +150,6 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.use_spec_decode = False\n         if self.speculative_config:\n             self.use_spec_decode = True\n-            self.rejection_sampler = RejectionSampler()\n             # TODO: find a better way to check if we are using ngram.\n             assert self.speculative_config.ngram_prompt_lookup_min, \\\n                     \"Currently, only ngram spec decode is supported in V1.\"\n@@ -162,6 +162,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     self.speculative_config.ngram_prompt_lookup_min,\n                     self.speculative_config.num_speculative_tokens,\n                 )\n+                self.rejection_sampler = RejectionSampler()\n \n         # Request states.\n         self.requests: dict[str, CachedRequestState] = {}\n@@ -452,7 +453,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n     def _prepare_inputs(\n         self,\n         scheduler_output: \"SchedulerOutput\",\n-    ) -> tuple[FlashAttentionMetadata, torch.Tensor]:\n+    ) -> tuple[FlashAttentionMetadata, torch.Tensor,\n+               Optional[SpecDecodeMetadata]]:\n         total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n         assert total_num_scheduled_tokens > 0\n         num_reqs = self.input_batch.num_reqs\n@@ -577,22 +579,33 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n         use_spec_decode = len(\n             scheduler_output.scheduled_spec_decode_tokens) > 0\n-        if use_spec_decode:\n-            logits_indices = self._calc_spec_decode_metadata(\n-                scheduler_output, cu_num_tokens)\n-        else:\n+        if not use_spec_decode:\n             # NOTE(woosuk): Due to chunked prefills, the batch may contain\n             # partial requests. While we should not sample any token\n             # from these partial requests, we do so for simplicity.\n             # We will ignore the sampled tokens from the partial requests.\n             # TODO: Support prompt logprobs.\n             logits_indices = attn_metadata.query_start_loc[1:] - 1\n+            spec_decode_metadata = None\n+        else:\n+            # Get the number of draft tokens for each request.\n+            # Iterate over the dictionary rather than all requests since not all\n+            # requests have draft tokens.\n+            num_draft_tokens = np.zeros(num_reqs, dtype=np.int32)\n+            for req_id, draft_token_ids in (\n+                    scheduler_output.scheduled_spec_decode_tokens.items()):\n+                req_idx = self.input_batch.req_id_to_index[req_id]\n+                num_draft_tokens[req_idx] = len(draft_token_ids)\n+\n+            spec_decode_metadata = self._calc_spec_decode_metadata(\n+                num_draft_tokens, cu_num_tokens)\n+            logits_indices = spec_decode_metadata.logits_indices\n \n         # Hot-Swap lora model\n         if self.lora_config:\n             self.set_active_loras(self.input_batch, num_scheduled_tokens)\n \n-        return attn_metadata, logits_indices\n+        return attn_metadata, logits_indices, spec_decode_metadata\n \n     def _compute_cascade_attn_prefix_len(\n         self,\n@@ -732,50 +745,79 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n     def _calc_spec_decode_metadata(\n         self,\n-        scheduler_output: \"SchedulerOutput\",\n-        cu_num_tokens: np.ndarray,\n-    ) -> torch.Tensor:\n-        # Get the number of spec decode tokens for each request.\n-        num_reqs = self.input_batch.num_reqs\n-        num_spec_decode_tokens = np.empty(num_reqs, dtype=np.int32)\n-        for i, req_id in enumerate(self.input_batch.req_ids):\n-            num_spec_decode_tokens[i] = len(\n-                scheduler_output.scheduled_spec_decode_tokens.get(req_id, ()))\n-\n-        # Get spec decode logits indices.\n-        # E.g.,   num_scheduled_tokens: [4, 100, 3,   100, 2]\n-        #         cu_num_tokens:        [4, 104, 107, 207, 209]\n-        #         num_spec_tokens_list: [3, 0,   2,   0,   1]\n-        #         num_sampled_tokens:   [4, 1,   3,   1,   2]\n-        #         spec_decode_logits_indices:\n-        #                 [0, 1, 2, 3, 103, 104, 105, 106, 206, 207, 208]\n-        num_sampled_tokens = num_spec_decode_tokens + 1\n-        # logits_start_loc: [0, 103, 104, 206, 207]\n-        logits_start_loc = cu_num_tokens - num_sampled_tokens\n-        # [0, 103, 104, 206, 207] ->\n-        #               [0, 0, 0, 0, 103, 104, 104, 104, 206, 207, 207]\n-        logits_start_loc = np.repeat(logits_start_loc, num_sampled_tokens)\n-        # The following three lines:\n-        # [4, 1,   3,   1,   2] -> [0, 1, 2, 3, 0, 0, 1, 2, 0, 0, 1]\n-        # Step 1. [4, 1, 3, 1, 2] -> [4, 5, 8, 9, 11]\n-        cu_num_sampled_tokens = np.cumsum(num_sampled_tokens)\n-        # Step 2. [4, 5, 8, 9, 11] -> [0, 4, 5, 8, 9]\n-        #         -> [0, 0, 0, 0, 4, 5, 5, 5, 8, 9, 9]\n-        cumsums_sampled_offsets = np.repeat(\n-            cu_num_sampled_tokens - num_sampled_tokens, num_sampled_tokens)\n-        # Step 3.  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-        #       -  [0, 0, 0, 0, 4, 5, 5, 5, 8, 9, 9]\n-        #      -> [0, 1, 2, 3, 0, 0, 1, 2, 0, 0, 1]\n-        total_num_sampled_tokens = num_sampled_tokens.sum()\n-        sampled_arange = (self.arange_np[:total_num_sampled_tokens] -\n-                          cumsums_sampled_offsets)\n-\n-        # [0, 0, 0, 0, 103, 104, 104, 104, 206, 207, 207] ->\n-        # [0, 1, 2, 3, 103, 104, 105, 106, 206, 207, 208]\n-        spec_decode_logits_indices = logits_start_loc + sampled_arange\n-        return torch.from_numpy(spec_decode_logits_indices).to(\n+        num_draft_tokens: np.ndarray,\n+        cu_num_scheduled_tokens: np.ndarray,\n+    ) -> SpecDecodeMetadata:\n+        # Inputs:\n+        # cu_num_scheduled_tokens:  [  4, 104, 107, 207, 209]\n+        # num_draft_tokens:         [  3,   0,   2,   0,   1]\n+        # Outputs:\n+        # cu_num_draft_tokens:      [  3,   3,   5,   5,   6]\n+        # logits_indices:           [  0,   1,   2,   3, 103, 104, 105, 106,\n+        #                            206, 207, 208]\n+        # target_logits_indices:    [  0,   1,   2,   5,   6,   9]\n+        # bonus_logits_indices:     [  3,   4,   7,   8,  10]\n+\n+        # Compute the logits indices.\n+        # [4, 1, 3, 1, 2]\n+        num_sampled_tokens = num_draft_tokens + 1\n+        # Step 1. [4, 5, 8, 9, 11]\n+        cu_num_sampled_tokens = np.cumsum(num_sampled_tokens, dtype=np.int32)\n+        total_num_sampled_tokens = cu_num_sampled_tokens[-1]\n+        # Step 2. [0, 0, 0, 0, 4, 5, 5, 5, 8, 9, 9]\n+        cumsums_offsets = np.repeat(cu_num_sampled_tokens - num_sampled_tokens,\n+                                    num_sampled_tokens)\n+        # Step 3. [0, 1, 2, 3, 0, 0, 1, 2, 0, 0, 1]\n+        arange = self.arange_np[:total_num_sampled_tokens] - cumsums_offsets\n+        # Step 4. [0, 0, 0, 0, 103, 104, 104, 104, 206, 207, 207]\n+        logits_indices = np.repeat(\n+            cu_num_scheduled_tokens - num_sampled_tokens, num_sampled_tokens)\n+        # Step 5. [0, 1, 2, 3, 103, 104, 105, 106, 206, 207, 208]\n+        logits_indices += arange\n+\n+        # Compute the bonus logits indices.\n+        bonus_logits_indices = cu_num_sampled_tokens - 1\n+\n+        # Compute the draft logits indices.\n+        # [3, 3, 5, 5, 6]\n+        cu_num_draft_tokens = np.cumsum(num_draft_tokens, dtype=np.int32)\n+        total_num_draft_tokens = cu_num_draft_tokens[-1]\n+        # [0, 0, 0, 3, 3, 5]\n+        cumsums_offsets = np.repeat(cu_num_draft_tokens - num_draft_tokens,\n+                                    num_draft_tokens)\n+        # [0, 1, 2, 0, 1, 0]\n+        arange = self.arange_np[:total_num_draft_tokens] - cumsums_offsets\n+        # [0, 0, 0, 5, 5, 9]\n+        target_logits_indices = np.repeat(\n+            cu_num_sampled_tokens - num_sampled_tokens, num_draft_tokens)\n+        # [0, 1, 2, 5, 6, 9]\n+        target_logits_indices += arange\n+\n+        # TODO: Optimize the CPU -> GPU copy.\n+        cu_num_draft_tokens = torch.from_numpy(cu_num_draft_tokens).to(\n+            self.device, non_blocking=True)\n+        logits_indices = torch.from_numpy(logits_indices).to(self.device,\n+                                                             non_blocking=True)\n+        target_logits_indices = torch.from_numpy(target_logits_indices).to(\n+            self.device, non_blocking=True)\n+        bonus_logits_indices = torch.from_numpy(bonus_logits_indices).to(\n             self.device, non_blocking=True)\n \n+        # Compute the draft token ids.\n+        # draft_token_indices:      [  1,   2,   3, 105, 106, 208]\n+        draft_token_ids = self.input_ids[logits_indices]\n+        draft_token_ids = draft_token_ids[target_logits_indices + 1]\n+\n+        metadata = SpecDecodeMetadata(\n+            draft_token_ids=draft_token_ids,\n+            num_draft_tokens=num_draft_tokens.tolist(),\n+            cu_num_draft_tokens=cu_num_draft_tokens,\n+            target_logits_indices=target_logits_indices,\n+            bonus_logits_indices=bonus_logits_indices,\n+            logits_indices=logits_indices,\n+        )\n+        return metadata\n+\n     def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n         scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n         if not scheduled_encoder_inputs:\n@@ -931,7 +973,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             encoder_outputs = []\n \n         # Prepare the decoder inputs.\n-        attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\n+        attn_metadata, logits_indices, spec_decode_metadata = (\n+            self._prepare_inputs(scheduler_output))\n         num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n         if (self.use_cuda_graph\n                 and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n@@ -1006,31 +1049,29 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n \n         # Sample the next token and get logprobs if needed.\n         sampling_metadata = self.input_batch.sampling_metadata\n-        if not self.use_spec_decode:\n+        if spec_decode_metadata is None:\n             sampler_output = self.model.sample(\n                 logits=logits,\n                 sampling_metadata=sampling_metadata,\n             )\n         else:\n-            draft_token_ids = [\n-                scheduler_output.scheduled_spec_decode_tokens.get(req_id, [])\n-                for req_id in self.input_batch.req_ids\n-            ]\n-            sample_lens = [len(tokens) + 1 for tokens in draft_token_ids]\n-            recover_logits_idx = np.cumsum(sample_lens) - 1\n-            target_probs = self.rejection_sampler.compute_probs(\n-                logits, sampling_metadata, sample_lens)\n+            # TODO(woosuk): Optimize the memory usage.\n+            bonus_logits = logits[spec_decode_metadata.bonus_logits_indices]\n             sampler_output = self.model.sample(\n-                logits=logits[recover_logits_idx, :],\n+                logits=bonus_logits,\n                 sampling_metadata=sampling_metadata,\n             )\n             bonus_token_ids = sampler_output.sampled_token_ids\n+\n+            # TODO(woosuk): Optimize the memory usage.\n+            target_logits = logits[spec_decode_metadata.target_logits_indices]\n             output_token_ids = self.rejection_sampler(\n-                draft_token_ids,\n+                spec_decode_metadata,\n                 None,  # draft_probs\n+                target_logits,\n                 bonus_token_ids,\n-                target_probs,\n-                sampling_metadata)\n+                sampling_metadata,\n+            )\n             sampler_output.sampled_token_ids = output_token_ids\n \n         # TODO(woosuk): The following loop can be slow since it iterates over\n@@ -1066,13 +1107,8 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n             valid_sampled_token_ids = sampled_token_ids.tolist()\n         else:\n             # Includes spec decode tokens.\n-            valid_mask = sampled_token_ids != INVALID_TOKEN_ID\n-            gen_lens = valid_mask.sum(dim=1).tolist()\n-            # TODO(woosuk): Optimize this.\n-            valid_sampled_token_ids = [\n-                seq.tolist()\n-                for seq in sampled_token_ids[valid_mask].split(gen_lens)\n-            ]\n+            valid_sampled_token_ids = self.rejection_sampler.parse_output(\n+                sampled_token_ids, self.input_batch.vocab_size)\n \n         if not self.use_spec_decode:\n             spec_token_ids = None\n@@ -1316,6 +1352,33 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                     \"initializing the engine.\") from e\n             else:\n                 raise e\n+        if self.use_spec_decode:\n+            draft_token_ids = [[0] for _ in range(num_reqs)]\n+            dummy_spec_decode_metadata = SpecDecodeMetadata.make_dummy(\n+                draft_token_ids, self.device)\n+\n+            num_tokens = sum(len(ids) for ids in draft_token_ids)\n+            # draft_probs = torch.randn(\n+            #     num_tokens, logits.shape[-1], device=self.device,\n+            #     dtype=logits.dtype)\n+            draft_probs = None\n+            target_logits = torch.randn(num_tokens,\n+                                        logits.shape[-1],\n+                                        device=self.device,\n+                                        dtype=logits.dtype)\n+            # NOTE(woosuk): Here, we should use int32 because the sampler uses\n+            # int32 for bonus_token_ids. If the dtype mismatches, re-compilation\n+            # will occur at runtime.\n+            bonus_token_ids = torch.zeros(num_reqs,\n+                                          device=self.device,\n+                                          dtype=torch.int32)\n+            self.rejection_sampler(\n+                dummy_spec_decode_metadata,\n+                draft_probs,\n+                target_logits,\n+                bonus_token_ids,\n+                dummy_metadata,\n+            )\n         return sampler_output\n \n     def profile_run(self) -> None:",
  "apis": [
    "RejectionSampler.forward",
    "RejectionSampler.parse_output",
    "SpecDecodeMetadata.make_dummy",
    "compiled_softmax"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/rejection_sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/pool/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_model_runner.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes extensive changes to the core sampling implementation, particularly the RejectionSampler in the source code (non-test files) by introducing optimizations via Triton kernels and a compiled softmax function. These changes aim to improve the performance of the sampler by leveraging low-level kernel optimizations on the CPU. The modifications not only refactor the code but significantly alter the mechanism for performing rejection sampling, which is a performance-critical part of the system. The changes affect high-level APIs in the repository and they are testable on CPU. Despite modifications in test files, the source code responsible for performance is clearly optimized through these changes.",
  "llm_api_reason": "This commit reworks how rejection sampling is performed in spec decode. In particular, it changes the RejectionSampler API to expect a SpecDecodeMetadata instance and target logits rather than separate draft tokens and probability tensors. Several Triton kernels are added to accelerate the sampling process (e.g. rejection_greedy_sample_kernel, rejection_random_sample_kernel, expand_kernel, and sample_recovered_tokens_kernel), and a new compiled softmax utility (compiled_softmax) is introduced under vllm.sample.ops.utils. Additionally, a new SpecDecodeMetadata class with a make_dummy classmethod is provided to support dummy spec decode metadata. These changes affect the topâ€‘level Python APIs for rejection sampling (RejectionSampler.forward and its parse_output helper), the SpecDecodeMetadata.make_dummy factory method, and the compiled_softmax function used to compute the scaled softmax probabilities."
}