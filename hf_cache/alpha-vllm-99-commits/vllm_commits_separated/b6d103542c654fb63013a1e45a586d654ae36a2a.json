{
  "commit_hash": "b6d103542c654fb63013a1e45a586d654ae36a2a",
  "pr_url": "https://github.com/vllm-project/vllm/pull/3662",
  "pr_date": "2024-03-30",
  "timeline_text": "Copy link Contributor mawong-amd commented Mar 27, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR primarily creates optimized specializations of fused_add_rms_norm_kernel, used in many layernorms. It also includes a slightly optimized version of blockReduceSum/warpReduceSum which slightly reduce the number of shuffles done when the max block size is <=512 and known at compile time. It is observed that fused_add_rms_norm is memory latency bound under many scenarios. The optimized implementation primarily derives its benefits by Coalescing global memory transactions into larger operations, which reduces the number of stalls that need to be hidden. This is achieved by (implicitly) unrolling both of the for loops through the use of a vector struct. Using a smaller block size when the number of blocks dispatched is large, which allows more blocks to simultaneously fit onto execution units and hence improves latency hiding. The same ideas contained here can be applied to other relatively simple kernels which should be memory bound (e.g. some activation kernels). More performance numbers can be provided as they become available or if requested. The existing test suite appears sufficient, but additional tests can be created on request. Some examples of the speed up, as obtained by profiling via benchmark_latency on Llama2-70B (hidden size 8192), FP16, TP = 1, on MI300X: (input_len = output_len = batch_size = 128): Prefill improves to 305 ms from 440 ms. (input_len = 2048, output_len = 128, batch_size = 1): Prefill improves to 41 ms from 88 ms. For both cases above, decode improves to 7 ms from 11 ms. Another optimization attempted was the use of shared memory, which effectively converts a global memory load into a shared memory load/store pair per item. While this improves performance when applied to baseline, it was not observed to improve performance on top of the current optimizations. BEFORE SUBMITTING, PLEASE READ THE CHECKLIST BELOW AND FILL IN THE DESCRIPTION ABOVE PR Checklist (Click to Expand) Thank you for your contribution to vLLM! Before submitting the pull request, please ensure the PR meets the following criteria. This helps vLLM maintain the code quality and improve the efficiency of the review process. PR Title and Classification Only specific types of PRs will be reviewed. The PR title is prefixed appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [CI/Build] for build or continuous integration improvements. [Doc] for documentation fixes and improvements. [Model] for adding a new model or improving an existing model. Model name should appear in the title. [Frontend] For changes on the vLLM frontend (e.g., OpenAI API server, LLM class, etc.) [Kernel] for changes affecting CUDA kernels or other compute kernels. [Core] for changes in the core vLLM logic (e.g., LLMEngine , AsyncLLMEngine , Scheduler , etc.) [Hardware][Vendor] for hardware-specific changes. Vendor name should appear in the prefix (e.g., [Hardware][AMD] ). [Misc] for PRs that do not fit the above categories. Please use this sparingly. Note: If the PR spans more than one category, please include all relevant prefixes. Code Quality The PR need to meet the following code quality standards: We adhere to Google Python style guide and Google C++ style guide . Pass all linter checks. Please use format.sh to format your code. The code need to be well-documented to ensure future contributors can easily understand the code. Include sufficient tests to ensure the project to stay correct and robust. This includes both unit tests and integration tests. Please add documentation to docs/source/ if the PR modifies the user-facing behaviors of vLLM. It helps vLLM user understand and utilize the new features or changes. Notes for Large Changes Please keep the changes as concise as possible. For major architectural changes (>500 LOC excluding kernel/data/config/test), we would expect a GitHub issue (RFC) discussing the technical design and justification. Otherwise, we will tag it with rfc-required and might not go through the PR. What to Expect for the Reviews The goal of the vLLM team is to be a transparent reviewing machine . We would like to make the review process transparent and efficient and make sure no contributor feel confused or frustrated. However, the vLLM team is small, so we need to prioritize some PRs over others. Here is what you can expect from the review process: After the PR is submitted, the PR will be assigned to a reviewer. Every reviewer will pick up the PRs based on their expertise and availability. After the PR is assigned, the reviewer will provide status update every 2-3 days. If the PR is not reviewed within 7 days, please feel free to ping the reviewer or the vLLM team. After the review, the reviewer will put an action-required label on the PR if there are changes required. The contributor should address the comments and ping the reviewer to re-review the PR. Please respond to all comments within a reasonable time frame. If a comment isn't clear or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Thank You Finally, thank you for taking the time to read these guidelines and for your interest in contributing to vLLM. Your contributions make vLLM a great tool for everyone! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 2 cadedaniel and WoosukKwon reacted with thumbs up emoji All reactions üëç 2 reactions WoosukKwon self-assigned this Mar 28, 2024 WoosukKwon reviewed Mar 28, 2024 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @mawong-amd Thanks for submitting the PR! This optimization seems to be necessary for MI300x GPUs. Unfortunately, I didn't see noticeable e2e performance boost for A100 GPUs. Is this expected? Also, I'm a bit worried about whether the new kernels keep the semantics of the current kernels. Could you double check? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions csrc/reduction_utils.cuh Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . csrc/layernorm_kernels.cu Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . csrc/layernorm_kernels.cu Comment on lines +252 to +253 scalar_t z = input[blockIdx.x * hidden_size + idx]; z += residual[blockIdx.x * hidden_size + idx]; float x = (float) z; Copy link Collaborator WoosukKwon Mar 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Doesn't this change the semantics of the kernel since we do the addition in FP16/BF16 instead of FP32? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author mawong-amd Mar 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It does in theory, however I've not noticed any observable effects from doing the addition in lower precision so far (even the logprobs of generated sequences are identical). In terms of a possible increase in rounding error, this is likely still negligible compared to typical errors incurred during the reduction phase and in the approximate rsqrt. The benefit of doing the addition in FP16/BF16 is that it can be implemented as a packed operation. But this step shouldn't be a bottleneck in any case. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator WoosukKwon Mar 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I see, makes sense. Thanks for the explanation! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions cmake/utils.cmake Comment on lines +103 to +107 list(REMOVE_ITEM GPU_FLAGS \"-D__CUDA_NO_HALF_OPERATORS__\" \"-D__CUDA_NO_HALF_CONVERSIONS__\" \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\" \"-D__CUDA_NO_HALF2_OPERATORS__\") Copy link Collaborator WoosukKwon Mar 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Can this affect other CUDA kernels? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author mawong-amd Mar 28, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It could, but I haven't noticed any side effects and neither have the tests. The existing defines seem to originate from Torch's default defines as a legacy item and it's not clear to me if there's a good reason to retain them nowadays (e.g. seems like the recently added Punica extension similarly disables these defines). If this is a concern, we could either limit the scope of removing these defines to this file or use free functions instead of operators (e.g. __hadd/__hadd2 for __half/__half2 operator+). But this increases code bloat and non-portability even further: the current implementation is already compromised to an extent by the (deficient) headers provided by CUDA/HIP (neither __hadd/__hadd2 as free functions or \"heterogeneous\" operators like float2::operator*(float) are consistently implemented in CUDA, while conversion operators/constructors are not consistently implemented by both). Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator WoosukKwon Mar 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Got it. Thanks for the explanation! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon added\n  the action-required label Mar 28, 2024 WoosukKwon removed their assignment Mar 28, 2024 mawong-amd changed the title [Kernel] Layernorm performance optimization [WIP][Kernel] Layernorm performance optimization Mar 28, 2024 mawong-amd changed the title [WIP][Kernel] Layernorm performance optimization [Kernel] Layernorm performance optimization Mar 28, 2024 Copy link Contributor Author mawong-amd commented Mar 28, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @mawong-amd Thanks for submitting the PR! This optimization seems to be necessary for MI300x GPUs. Unfortunately, I didn't see noticeable e2e performance boost for A100 GPUs. Is this expected? Also, I'm a bit worried about whether the new kernels keep the semantics of the current kernels. Could you double check? Hi, I managed to run a few performance tests on H100 last night and also observed that there was no speed up. I looked at the PTX and SASS assembly and NVCC was not fusing the loads/stores as expected. It appears NVCC needs to know these global memory ops are aligned on a 16 byte boundary to unlock the full 128-bit coalesced op; I've added this alignment requirement to the vector struct and now I'm observing similar speedups on H100. Preliminary numbers I'm seeing on H100 are: (input_len = output_len = batch_size = 128): Prefill improves to 92 ms from 178 ms. (input_len = 2048, output_len = 128, batch_size = 1): Prefill improves to 45 ms from 84 ms. For both cases above, decode improves to 3 ms from 8 ms. One \"drawback\" of this change is we can now only enable optimizations when the hidden_size is a multiple of 8 and the tensor pointers are aligned on a 16 byte boundary. But these conditions should be met essentially all the time. As for the changed semantics, I'll discuss it in the relevant review comment thread. Thanks! üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mawong-amd added 6 commits March 30, 2024 03:30 Layernorm optimizations: ‚Ä¶ aac1754 Bulk conversions (packed halfs into half2, using vectors of half2);\nblock and warp reduce with AMD wavesize 64 (vs 32);\nusing smaller block sizes for improved block occupancy on CUs\n\nUse larger block sizes for decode; optimize warp and block reduce fully\n\nRefactor vector to use half to maintain same alignment as c10::Half; move packed logic into member functions\n\nAdd a few missing unroll directives\n\nFix blockReduce stall caused by warp divergence on CUDA (vLLM uses universal masks)\n\nRefactor vector type to enable optimizations for bf16\n\nRe-apply the blockReduceSum fix for warp divergence\n\nHotfix: Disable BF16 opts due to ROCm 5.7 incompatibility\n\nRemove redundant inline specifiers; preparing for upstream Disable no half conv flags for CUDA d2f681a Add more hidden sizes (including non-multiples of 8) to test 5128836 Enforce 16 byte alignment for CUDA vectorized mem ops c0e37f6 Add back explicit cast to T in reduction_utils 677e045 Style tweak a1bbdc4 mawong-amd force-pushed the layernorm2upstream branch\n    from 4f94b87 to a1bbdc4 Compare March 30, 2024 04:03 Copy link Contributor Author mawong-amd commented Mar 30, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Quick update on end-to-end runtime numbers. With the latest changes, I'm seeing small but observable improvements on H100. Specifically, on the latency benchmark (50 iters on each test): (input_len = output_len = batch_size = 128): Improves to 11.463s from 11.658s. [1.7% improvement] (input_len = 2048, output_len = 128, batch_size = 1): Improves to 4.261s from 4.362s. [2.3% improvement] üëç 1 WoosukKwon reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mawong-amd requested a review\n  from WoosukKwon March 30, 2024 16:08 WoosukKwon added rocm Related to AMD ROCm and removed action-required labels Mar 30, 2024 WoosukKwon self-assigned this Mar 30, 2024 WoosukKwon approved these changes Mar 30, 2024 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @mawong-amd LGTM! Thanks for the optimization! Didn't know that RMSNorm can affect the performance this much. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon merged commit b6d1035 into vllm-project : main Mar 30, 2024 Copy link Member youkaichao commented Apr 1, 2024 I realized that this pr breaks cuda 11.8 support because of the usage of __half2 etc. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author mawong-amd commented Apr 1, 2024 I realized that this pr breaks cuda 11.8 support because of the usage of __half2 etc. I think we can hotfix in a define guard to enable these optimizations only when the cuda version is > 11.8. Let me prepare a diff that does that. üëç 1 youkaichao reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author mawong-amd commented Apr 1, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . EDIT: Hotfix created as the following PR #3782 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member youkaichao commented Apr 1, 2024 @mawong-amd Can you send a PR to land that patch? üöÄ 1 mawong-amd reacted with rocket emoji All reactions üöÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mawong-amd mentioned this pull request Apr 1, 2024 [Hotfix][CI/Build][Kernel] CUDA 11.8 does not support layernorm optimizations #3782 Merged dtrifiro mentioned this pull request May 15, 2024 bump ubi base image tag opendatahub-io/vllm#24 Merged Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:12",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | SERVING: API server, OpenAI API server, Frontend | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:49:12",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Kernel] Layernorm performance optimization (#3662)",
  "commit_message": "[Kernel] Layernorm performance optimization (#3662)",
  "commit_date": "2024-03-30T14:26:38-07:00",
  "files_changed": [
    "cmake/utils.cmake",
    "csrc/layernorm_kernels.cu",
    "csrc/reduction_utils.cuh",
    "tests/kernels/test_layernorm.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 4,
    "num_hunks": 8,
    "num_edited_lines": 332,
    "num_non_test_edited_lines": 329,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/cmake/utils.cmake b/cmake/utils.cmake\nindex 6bf5d5130..c7d3d8538 100644\n--- a/cmake/utils.cmake\n+++ b/cmake/utils.cmake\n@@ -100,6 +100,11 @@ function (get_torch_gpu_compiler_flags OUT_GPU_FLAGS GPU_LANG)\n \n     if (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n       list(APPEND GPU_FLAGS \"-DENABLE_FP8_E5M2\")\n+      list(REMOVE_ITEM GPU_FLAGS\n+        \"-D__CUDA_NO_HALF_OPERATORS__\"\n+        \"-D__CUDA_NO_HALF_CONVERSIONS__\"\n+        \"-D__CUDA_NO_BFLOAT16_CONVERSIONS__\"\n+        \"-D__CUDA_NO_HALF2_OPERATORS__\")\n     endif()\n \n   elseif(${GPU_LANG} STREQUAL \"HIP\")\ndiff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex 6d34d014c..ea30fa274 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -4,6 +4,16 @@\n \n #include \"dispatch_utils.h\"\n #include \"reduction_utils.cuh\"\n+#ifndef USE_ROCM\n+  #include <cuda_bf16.h>\n+  #include <cuda_fp16.h>\n+#else\n+  #include <hip/hip_bf16.h>\n+  #include <hip/hip_fp16.h>\n+\n+  using __nv_bfloat16 = __hip_bfloat16;\n+  using __nv_bfloat162 = __hip_bfloat162;\n+#endif\n \n namespace vllm {\n \n@@ -35,9 +45,199 @@ __global__ void rms_norm_kernel(\n   }\n }\n \n-// TODO: Further optimize this kernel.\n-template<typename scalar_t>\n-__global__ void fused_add_rms_norm_kernel(\n+\n+/* Converter structs for the conversion from torch types to HIP/CUDA types,\n+   and the associated type conversions within HIP/CUDA. These helpers need\n+   to be implemented for now because the relevant type conversion\n+   operators/constructors are not consistently implemented by HIP/CUDA, so\n+   a generic conversion via type casts cannot be implemented.\n+\n+   Each struct should have the member static constexpr bool `exists`:\n+   If false, the optimized kernel is not used for the corresponding torch type.\n+   If true, the struct should be fully defined as shown in the examples below. \n+ */\n+template<typename torch_type>\n+struct _typeConvert { static constexpr bool exists = false; };\n+\n+template<>\n+struct _typeConvert<c10::Half> {\n+  static constexpr bool exists = true;\n+  using hip_type = __half;\n+  using packed_hip_type = __half2;\n+\n+  __device__ static inline float convert(hip_type x) { return __half2float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __half22float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2half_rn(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22half2_rn(x); }\n+};\n+\n+#if defined(__CUDA_ARCH__) && __CUDA_ARCH__ >= 800\n+// CUDA_ARCH < 800 does not have BF16 support\n+// TODO: Add in ROCm support once public headers handle bf16 maturely\n+template<>\n+struct _typeConvert<c10::BFloat16> {\n+  static constexpr bool exists = true;\n+  using hip_type = __nv_bfloat16;\n+  using packed_hip_type = __nv_bfloat162;\n+\n+  __device__ static inline float convert(hip_type x) { return __bfloat162float(x); }\n+  __device__ static inline float2 convert(packed_hip_type x) { return __bfloat1622float2(x); }\n+  __device__ static inline hip_type convert(float x) { return __float2bfloat16(x); }\n+  __device__ static inline packed_hip_type convert(float2 x) { return __float22bfloat162_rn(x); }\n+};\n+#endif\n+\n+\n+/* Vector POD struct to generate vectorized and packed FP16/BF16 ops\n+   for appropriate specializations of fused_add_rms_norm_kernel.\n+   Only functions that are necessary in that kernel are implemented.\n+   Alignment to 16 bytes is required to use 128-bit global memory ops.\n+ */\n+template<typename scalar_t, int width>\n+struct alignas(16) _f16Vec {\n+  /* Not theoretically necessary that width is a power of 2 but should \n+     almost always be the case for optimization purposes */ \n+  static_assert(width > 0 && (width & (width - 1)) == 0,\n+                \"Width is not a positive power of 2!\");\n+  using Converter = _typeConvert<scalar_t>;\n+  using T1 = typename Converter::hip_type;\n+  using T2 = typename Converter::packed_hip_type;\n+  T1 data[width];\n+\n+  __device__ _f16Vec& operator+=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp += T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] += other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const _f16Vec<scalar_t, width>& other) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        T2 temp{data[i], data[i+1]};\n+        temp *= T2{other.data[i], other.data[i+1]};\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i)\n+        data[i] *= other.data[i];\n+    }\n+    return *this;\n+  }\n+\n+  __device__ _f16Vec& operator*=(const float scale) {\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 temp_f = Converter::convert(T2{data[i], data[i+1]});\n+        temp_f.x *= scale;\n+        temp_f.y *= scale;\n+        T2 temp = Converter::convert(temp_f);\n+        data[i] = temp.x;\n+        data[i+1] = temp.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float temp = Converter::convert(data[i]) * scale;\n+        data[i] = Converter::convert(temp);\n+      }\n+    }\n+    return *this;\n+  }\n+\n+  __device__ float sum_squares() const {\n+    float result = 0.0f;\n+    if constexpr (width % 2 == 0) {\n+      #pragma unroll\n+      for (int i = 0; i < width; i += 2) {\n+        float2 z = Converter::convert(T2{data[i], data[i+1]});\n+        result += z.x * z.x + z.y * z.y;\n+      }\n+    } else {\n+      #pragma unroll\n+      for (int i = 0; i < width; ++i) {\n+        float x = Converter::convert(data[i]);\n+        result += x * x;\n+      }\n+    }\n+    return result;\n+  }\n+};\n+\n+/* Function specialization in the case of FP16/BF16 tensors.\n+   Additional optimizations we can make in this case are\n+   packed and vectorized operations, which help with the\n+   memory latency bottleneck. */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width > 0) && _typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n+  scalar_t* __restrict__ input,           // [..., hidden_size]\n+  scalar_t* __restrict__ residual,        // [..., hidden_size]\n+  const scalar_t* __restrict__ weight,    // [hidden_size]\n+  const float epsilon,\n+  const int num_tokens,\n+  const int hidden_size) {\n+  // Sanity checks on our vector struct and type-punned pointer arithmetic\n+  static_assert(std::is_pod_v<_f16Vec<scalar_t, width>>);\n+  static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);\n+\n+  const int vec_hidden_size = hidden_size / width;\n+  __shared__ float s_variance;\n+  float variance = 0.0f;\n+  /* These and the argument pointers are all declared `restrict` as they are\n+     not aliased in practice. Argument pointers should not be dereferenced\n+     in this kernel as that would be undefined behavior */\n+  auto* __restrict__ input_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(input);\n+  auto* __restrict__ residual_v = reinterpret_cast<_f16Vec<scalar_t, width>*>(residual);\n+  auto* __restrict__ weight_v = reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = input_v[id];\n+    temp += residual_v[id];\n+    variance += temp.sum_squares();\n+    residual_v[id] = temp;\n+  }\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n+  if (threadIdx.x == 0) {\n+    s_variance = rsqrtf(variance / hidden_size + epsilon);\n+  }\n+  __syncthreads();\n+\n+  for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int id = blockIdx.x * vec_hidden_size + idx;\n+    _f16Vec<scalar_t, width> temp = residual_v[id];\n+    temp *= s_variance;\n+    temp *= weight_v[idx];\n+    input_v[id] = temp;\n+  }\n+}\n+\n+\n+/* Generic fused_add_rms_norm_kernel\n+   The width field is not used here but necessary for other specializations.\n+ */\n+template<typename scalar_t, int width>\n+__global__ std::enable_if_t<\n+  (width == 0) || !_typeConvert<scalar_t>::exists> fused_add_rms_norm_kernel(\n   scalar_t* __restrict__ input,           // [..., hidden_size]\n   scalar_t* __restrict__ residual,        // [..., hidden_size]\n   const scalar_t* __restrict__ weight,    // [hidden_size]\n@@ -48,12 +248,17 @@ __global__ void fused_add_rms_norm_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float) input[blockIdx.x * hidden_size + idx];\n-    x += (float) residual[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[blockIdx.x * hidden_size + idx];\n+    z += residual[blockIdx.x * hidden_size + idx];\n+    float x = (float) z;\n     variance += x * x;\n-    residual[blockIdx.x * hidden_size + idx] = (scalar_t) x;\n+    residual[blockIdx.x * hidden_size + idx] = z;\n   }\n-  variance = blockReduceSum<float>(variance);\n+  /* Keep the following if-else block in sync with the\n+     calculation of max_block_size in fused_add_rms_norm */ \n+  if (num_tokens < 256) {\n+    variance = blockReduceSum<float, 1024>(variance);\n+  } else variance = blockReduceSum<float, 256>(variance);\n   if (threadIdx.x == 0) {\n     s_variance = rsqrtf(variance / hidden_size + epsilon);\n   }\n@@ -93,6 +298,21 @@ void rms_norm(\n     });\n }\n \n+#define LAUNCH_FUSED_ADD_RMS_NORM(width)              \\\n+  VLLM_DISPATCH_FLOATING_TYPES(                       \\\n+    input.scalar_type(),                              \\\n+    \"fused_add_rms_norm_kernel\",                      \\\n+    [&] {                                             \\\n+      vllm::fused_add_rms_norm_kernel                 \\\n+      <scalar_t, width><<<grid, block, 0, stream>>>(  \\\n+        input.data_ptr<scalar_t>(),                   \\\n+        residual.data_ptr<scalar_t>(),                \\\n+        weight.data_ptr<scalar_t>(),                  \\\n+        epsilon,                                      \\\n+        num_tokens,                                   \\\n+        hidden_size);                                 \\\n+    });\n+\n void fused_add_rms_norm(\n   torch::Tensor& input,    // [..., hidden_size]\n   torch::Tensor& residual, // [..., hidden_size]\n@@ -102,19 +322,29 @@ void fused_add_rms_norm(\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(hidden_size, 1024));\n+  /* This kernel is memory-latency bound in many scenarios.\n+     When num_tokens is large, a smaller block size allows\n+     for increased block occupancy on CUs and better latency\n+     hiding on global mem ops. */\n+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n+  dim3 block(std::min(hidden_size, max_block_size));\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-  VLLM_DISPATCH_FLOATING_TYPES(\n-    input.scalar_type(),\n-    \"fused_add_rms_norm_kernel\",\n-    [&] {\n-      vllm::fused_add_rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        input.data_ptr<scalar_t>(),\n-        residual.data_ptr<scalar_t>(),\n-        weight.data_ptr<scalar_t>(),\n-        epsilon,\n-        num_tokens,\n-        hidden_size);\n-    });\n+  /*If the tensor types are FP16/BF16, try to use the optimized kernel\n+    with packed + vectorized ops.\n+    Max optimization is achieved with a width-8 vector of FP16/BF16s\n+    since we can load at most 128 bits at once in a global memory op.\n+    However, this requires each tensor's data to be aligned to 16\n+    bytes.\n+   */\n+  auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());\n+  auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());\n+  auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());\n+  bool ptrs_are_aligned = inp_ptr % 16 == 0 && res_ptr % 16 == 0 \\\n+                          && wt_ptr % 16 == 0;\n+  if (ptrs_are_aligned && hidden_size % 8 == 0) {\n+    LAUNCH_FUSED_ADD_RMS_NORM(8);\n+  } else {\n+    LAUNCH_FUSED_ADD_RMS_NORM(0);\n+  }\n }\ndiff --git a/csrc/reduction_utils.cuh b/csrc/reduction_utils.cuh\nindex c25464e86..bb5171f85 100644\n--- a/csrc/reduction_utils.cuh\n+++ b/csrc/reduction_utils.cuh\n@@ -20,43 +20,45 @@\n #include \"cuda_compat.h\"\n \n namespace vllm {\n-\n-template<typename T>\n+template<typename T, int numLanes = WARP_SIZE>\n __inline__ __device__ T warpReduceSum(T val) {\n-#pragma unroll\n-  for (int mask = WARP_SIZE/2; mask > 0; mask >>= 1)\n+  static_assert(numLanes > 0 && (numLanes & (numLanes - 1)) == 0,\n+                \"numLanes is not a positive power of 2!\");\n+  static_assert(numLanes <= WARP_SIZE);\n+  #pragma unroll\n+  for (int mask = numLanes >> 1; mask > 0; mask >>= 1)\n     val += VLLM_SHFL_XOR_SYNC(val, mask);\n   return val;\n }\n \n-__inline__ __device__ constexpr int _calculateLaneMask(int warp_size) {\n-  return warp_size - 1;\n-}\n-\n-__inline__ __device__ constexpr int _calculateWidShift(int warp_size) {\n-  return 5 + (warp_size >> 6);\n+// Helper function to return the next largest power of 2\n+static constexpr int _nextPow2(unsigned int num) {\n+  if (num <= 1) return num;\n+  return 1 << (CHAR_BIT * sizeof(num) - __builtin_clz(num - 1));\n }\n \n /* Calculate the sum of all elements in a block */\n-template<typename T>\n+template<typename T, int maxBlockSize = 1024>\n __inline__ __device__ T blockReduceSum(T val) {\n-  static __shared__ T shared[WARP_SIZE];\n-  constexpr auto LANE_MASK = _calculateLaneMask(WARP_SIZE);\n-  constexpr auto WID_SHIFT = _calculateWidShift(WARP_SIZE);\n-  int lane = threadIdx.x & LANE_MASK;\n-  int wid = threadIdx.x >> WID_SHIFT;\n-\n-  val = warpReduceSum<T>(val);\n-\n-  if (lane == 0)\n-    shared[wid] = val;\n+  static_assert(maxBlockSize <= 1024);\n+  if constexpr (maxBlockSize > WARP_SIZE) {\n+    val = warpReduceSum<T>(val);\n+    // Calculates max number of lanes that need to participate in the last warpReduce\n+    constexpr int maxActiveLanes = (maxBlockSize + WARP_SIZE - 1) / WARP_SIZE;\n+    static __shared__ T shared[maxActiveLanes];\n+    int lane = threadIdx.x % WARP_SIZE;\n+    int wid = threadIdx.x / WARP_SIZE;\n+    if (lane == 0)\n+      shared[wid] = val;\n \n-  __syncthreads();\n+    __syncthreads();\n \n-  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent\n-  // blockDim.x is not divided by 32\n-  val = (threadIdx.x < (blockDim.x / (WARP_SIZE * 1.0f))) ? shared[lane] : (T)(0.0f);\n-  val = warpReduceSum<T>(val);\n+    val = (threadIdx.x < blockDim.x / float(WARP_SIZE)) ? shared[lane] : (T)(0.0f);\n+    val = warpReduceSum<T, _nextPow2(maxActiveLanes)>(val);\n+  } else {\n+    // A single warpReduce is equal to blockReduce\n+    val = warpReduceSum<T, _nextPow2(maxBlockSize)>(val);\n+  }\n   return val;\n }\n \ndiff --git a/tests/kernels/test_layernorm.py b/tests/kernels/test_layernorm.py\nindex b1e3c1a7f..210d59e4f 100644\n--- a/tests/kernels/test_layernorm.py\n+++ b/tests/kernels/test_layernorm.py\n@@ -5,7 +5,8 @@ from vllm.model_executor.layers.layernorm import RMSNorm\n \n DTYPES = [torch.half, torch.bfloat16, torch.float]\n NUM_TOKENS = [7, 83, 4096]  # Arbitrary values for testing\n-HIDDEN_SIZES = [768, 5120, 8192]  # Arbitrary values for testing\n+HIDDEN_SIZES = [768, 769, 770, 771, 5120, 5124, 5125, 5126, 8192,\n+                8199]  # Arbitrary values for testing\n ADD_RESIDUAL = [False, True]\n SEEDS = [0]\n CUDA_DEVICES = [",
  "apis": [
    "RMSNorm.forward_cuda"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/layernorm.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/_custom_ops.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes non-trivial modifications to several source files (cmake and C++ CUDA source files) that implement a new vectorized and packed version of a layer normalization kernel. The changes include optimized memory-latency handling in the kernel launch configuration, new conversion structs and specialized kernels for FP16/BF16 data types, and adjustments in reduction utilities. These modifications are aimed at improving kernel performance on CPU-accessible GPUs (the kernels themselves run on CUDA hardware but affect performance optimizations testable without GPU-specific heavy dependencies). Therefore, the commit clearly addresses performance optimization rather than merely refactoring, fixing bugs, or adding non-performance features.",
  "llm_api_reason": "This commit makes performance optimizations in the layer normalization CUDA kernels by improving the fused_add_rms_norm_kernel implementation (including adding optimizations for FP16/BF16 types using vectorized and packed operations) as well as tuning the reduction functions. These low‚Äêlevel kernel changes directly benefit the CUDA path invoked by the Python API for RMSNorm (e.g. in its forward_cuda method). Hence, the affected Python API is that of the RMSNorm op when using the CUDA implementation."
}