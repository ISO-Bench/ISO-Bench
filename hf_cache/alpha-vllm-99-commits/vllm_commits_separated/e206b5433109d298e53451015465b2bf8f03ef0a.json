{
  "commit_hash": "e206b5433109d298e53451015465b2bf8f03ef0a",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13837",
  "pr_date": "2025-02-25",
  "timeline_text": "Copy link Contributor sethkimmel3 commented Feb 25, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . The deepcopy introduced in #11637 adds a lot of overhead when adding a large number of requests to an llm_engine . This adds a more efficient method of copying the XGrammarLogitsProcessor data structure to remove that overhead. cc: @mgoin @aarnphm Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link github-actions bot commented Feb 25, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the structured-output label Feb 25, 2025 aarnphm reviewed Feb 25, 2025 View reviewed changes vllm/model_executor/guided_decoding/xgrammar_decoding.py Outdated Comment on lines 362 to 364 if hasattr(self, 'token_bitmask') and self.token_bitmask is not None: new_processor.token_bitmask = xgr.allocate_token_bitmask( self.batch_size, self.config.vocab_size) Copy link Collaborator aarnphm Feb 25, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment should it be Suggested change if hasattr ( self , 'token_bitmask' ) and self . token_bitmask is not None : new_processor . token_bitmask = xgr . allocate_token_bitmask ( self . batch_size , self . config . vocab_size ) if hasattr ( self , 'token_bitmask' ) and self . token_bitmask is not None : new_processor . token_bitmask = self . token_bitmask Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions aarnphm approved these changes Feb 25, 2025 View reviewed changes Copy link Collaborator aarnphm left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment one tiny comment, if it passes the tests then LGTM. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator aarnphm commented Feb 25, 2025 @sethkimmel3 there are a few pre-commit problem can you fix this? thanks. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sethkimmel3 added 5 commits February 25, 2025 10:43 clone test â€¦ 4f8265e Signed-off-by: Seth Kimmel <seth.kimmel3@gmail.com> replace deepcopy â€¦ fbe5acf Signed-off-by: Seth Kimmel <seth.kimmel3@gmail.com> ruff and small tweak â€¦ bf10cbc Signed-off-by: Seth Kimmel <seth.kimmel3@gmail.com> update â€¦ 2c1a699 Signed-off-by: Seth Kimmel <seth.kimmel3@gmail.com> lint â€¦ 11b4114 Signed-off-by: Seth Kimmel <seth.kimmel3@gmail.com> sethkimmel3 force-pushed the clone-test branch\n    from a19541b to 11b4114 Compare February 25, 2025 18:43 Copy link Collaborator aarnphm commented Feb 25, 2025 I cant update the title, but can you make it to [v0][Core] Use shared context to avoid copy overhead for offline engine otherwise I think this should be ready to bring out of draft All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sethkimmel3 changed the title Replace xgrammar deepcopy [v0][Core] Use shared context to avoid copy overhead for offline engine Feb 25, 2025 sethkimmel3 marked this pull request as ready for review February 25, 2025 18:49 sethkimmel3 requested a review\n  from mgoin as a code owner February 25, 2025 18:49 Copy link Contributor Author sethkimmel3 commented Feb 25, 2025 Done and done @aarnphm ! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin changed the title [v0][Core] Use shared context to avoid copy overhead for offline engine [v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine Feb 25, 2025 mgoin approved these changes Feb 25, 2025 View reviewed changes mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Feb 25, 2025 Copy link Collaborator aarnphm commented Feb 25, 2025 Thanks. Once all PR pass we can merge this All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details DarkLight1337 merged commit e206b54 into vllm-project : main Feb 26, 2025 56 of 58 checks passed Uh oh! There was an error while loading. Please reload this page . Akshat-Tripathi pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Mar 3, 2025 [v0][Core] Use xgrammar shared context to avoid copy overhead for offâ€¦ â€¦ 77ca08e â€¦line engine ( vllm-project#13837 )\n\nSigned-off-by: Seth Kimmel <seth.kimmel3@gmail.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [v0][Core] Use xgrammar shared context to avoid copy overhead for offâ€¦ â€¦ c2d7cba â€¦line engine ( vllm-project#13837 )\n\nSigned-off-by: Seth Kimmel <seth.kimmel3@gmail.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [v0][Core] Use xgrammar shared context to avoid copy overhead for offâ€¦ â€¦ f4c2054 â€¦line engine ( vllm-project#13837 )\n\nSigned-off-by: Seth Kimmel <seth.kimmel3@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:25",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:52:25",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine (#13837)",
  "commit_message": "[v0][Core] Use xgrammar shared context to avoid copy overhead for offline engine (#13837)\n\nSigned-off-by: Seth Kimmel <seth.kimmel3@gmail.com>",
  "commit_date": "2025-02-26T14:58:24+08:00",
  "files_changed": [
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 26,
    "num_non_test_edited_lines": 26,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 329b03a57..e6ba7f5ec 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -3,7 +3,6 @@\n # noqa: UP007\n from __future__ import annotations\n \n-import copy\n import json\n import re\n from dataclasses import dataclass, field\n@@ -348,5 +347,26 @@ class XGrammarLogitsProcessor:\n         return scores\n \n     def clone(self) -> XGrammarLogitsProcessor:\n-        \"\"\"Deepcopy due to per-sequence state in the matchers\"\"\"\n-        return copy.deepcopy(self)\n+        \"\"\"Create a new instance with shared compiled grammar\n+          but separate state\"\"\"\n+        new_processor = XGrammarLogitsProcessor(self.config)\n+\n+        # Share the compiled grammar context (immutable after compilation)\n+        new_processor.ctx = self.ctx\n+\n+        # Create fresh matchers for the new sequence\n+        if self.ctx is not None:\n+            new_processor.matchers = [\n+                xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n+            ]\n+\n+        # Create a new token bitmask with the same size\n+        if hasattr(self, 'token_bitmask') and self.token_bitmask is not None:\n+            new_processor.token_bitmask = self.token_bitmask\n+\n+        # Copy simple attributes\n+        new_processor.batch_size = self.batch_size\n+        # Reset prefilled state for new sequence\n+        new_processor.prefilled = False\n+\n+        return new_processor",
  "apis": [
    "vllm.model_executor.guided_decoding.xgrammar_decoding.XGrammarLogitsProcessor.clone"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/llm.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a source code file (a non-test file) by replacing a deep copy operation with a more efficient instance creation that uses a shared compiled grammar context. This reduces copy overhead, which is a performance optimization. The change goes beyond refactoring or bug fixes, impacting the performance of the decoding process in a CPU-testable manner.",
  "llm_api_reason": "The commit refactors the clone() method in the XGrammarLogitsProcessor class so that instead of performing a full deep copy (and the overhead that comes with it), it now creates a new instance while sharing the compiled grammar context and reinitializing the sequence-specific state (matchers, prefilled flags, and token bitmask). This change optimizes cloning for offline engine inference without altering the external behavior of the processor."
}