{
  "commit_hash": "6d0734c562e759fdb7076d762222b3881e62ab1f",
  "pr_url": "https://github.com/vllm-project/vllm/pull/20645",
  "pr_date": "2025-07-19",
  "timeline_text": "Copy link Contributor kaixih commented Jul 8, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . For this PR , Flashinfer introduces a new backend for block-wise scaled FP8. This PR adds support for that backend. cc. @kushanam @wenscarl @pavanimajety Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions kaixih requested review from mgoin , robertgshaw2-redhat and tlrmchlsmth as code owners July 8, 2025 21:54 Copy link github-actions bot commented Jul 8, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jul 8, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @kaixih , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request integrates a new Flashinfer backend to support block-wise scaled FP8 quantization for Mixture-of-Experts (MoE) layers. The primary goal is to leverage Flashinfer's optimized kernels to potentially enhance the performance of FP8 MoE computations within vLLM, introducing a specialized execution path for compatible configurations. Highlights Flashinfer Integration : Introduced flashinfer.fused_moe as a new dependency and added a flashinfer_moe_enabled flag within the Fp8LinearMethod class to conditionally activate the new backend. Weight Preprocessing for Compatibility : Implemented a new helper function _swap_w13_to_w31 and integrated its application during the process_weights_after_loading phase. This is crucial for reordering weights ( w13_weight and w13_weight_scale_inv ) to ensure compatibility with Flashinfer's expected activation layout. Conditional MoE Execution Path : Modified the apply method in Fp8LinearMethod to introduce a new execution branch. When flashinfer_moe_enabled is true, vLLM's default expert selection logic is bypassed, and the call is delegated to Flashinfer's fused_moe.trtllm_fp8_block_scale_moe for the core FP8 MoE computation. Backend-Specific Constraints and Input Handling : Added several assertions within the Flashinfer MoE execution path to enforce specific requirements, such as block_quant , renormalize , use_grouped_topk , sigmoid scoring, silu activation, and specific constraints on top_k , global_num_experts , and num_expert_group . Additionally, input hidden states are now quantized using per_token_group_quant_fp8 before being passed to the Flashinfer kernel. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with üëç and üëé on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . ‚Ü© Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jul 8, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces support for a new Flashinfer MoE backend for block-wise scaled FP8 quantization, primarily in vllm/model_executor/layers/quantization/fp8.py . A critical bug exists in weight processing, where swapped weights are not persisted. Additionally, assert statements should be replaced with proper error handling, and the new backend should be made configurable. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/fp8.py Outdated Comment on lines 656 to 707 w13_weight = _swap_w13_to_w31(layer.w13_weight.data) w13_weight_scale_inv = _swap_w13_to_w31(layer.w13_weight_scale_inv.data) w2_weight = layer.w2_weight.data w2_weight_scale_inv = layer.w2_weight_scale_inv.data Copy link Contributor gemini-code-assist bot Jul 8, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The swapped weights and scales obtained from _swap_w13_to_w31 are assigned to local variables but not persisted back to the layer 's parameters. This will cause the apply method to use the original, un-swapped weights, leading to incorrect results. Ensure the processed weights are copied back to the layer's parameters using param.data.copy_(swapped_tensor) . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/fp8.py Outdated Comment on lines 937 to 1011 assert self.block_quant assert (renormalize and use_grouped_topk and scoring_func == 'sigmoid' and custom_routing_function == None) assert top_k <= global_num_experts assert top_k <= 8 assert topk_group <= 4 assert global_num_experts > num_expert_group assert global_num_experts % num_expert_group == 0 assert global_num_experts % 4 == 0 assert top_k < (topk_group * global_num_experts / num_expert_group) assert activation == \"silu\" Copy link Contributor gemini-code-assist bot Jul 8, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Replace assert statements with explicit if conditions and raise ValueError or NotImplementedError exceptions. This ensures that these checks are always active, even when assertions are disabled in production environments. if not self . block_quant : raise ValueError ( \"Flashinfer MoE backend requires block quantization.\" ) if not ( renormalize and use_grouped_topk and scoring_func == 'sigmoid' and custom_routing_function is None ): raise NotImplementedError ( \"Flashinfer MoE backend has specific routing requirements: \" \"renormalize=True, use_grouped_topk=True, scoring_func='sigmoid', \" \"and no custom_routing_function.\" ) if not top_k <= 8 : raise ValueError ( f\"Unsupported top_k= { top_k } , must be <= 8.\" ) if not topk_group <= 4 : raise ValueError ( f\"Unsupported topk_group= { topk_group } , must be <= 4.\" ) if not ( global_num_experts > num_expert_group and global_num_experts % num_expert_group == 0 and global_num_experts % 4 == 0 ): raise ValueError ( f\"Unsupported expert grouping: { global_num_experts = } , { num_expert_group = } \" ) if not top_k < ( topk_group * global_num_experts / num_expert_group ): raise ValueError ( \"top_k must be less than (topk_group * num_experts_per_group).\" ) if activation != \"silu\" : raise NotImplementedError ( f\"Flashinfer MoE backend only supports 'silu' activation, but got { activation } .\" ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/fp8.py Outdated @@ -449,6 +454,7 @@ def __init__(self, quant_config: Fp8Config): self.quant_config = quant_config self.block_quant = self.quant_config.weight_block_size is not None self.flashinfer_moe_enabled = True Copy link Contributor gemini-code-assist bot Jul 8, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Consider making flashinfer_moe_enabled configurable via an environment variable or model configuration parameter to facilitate debugging and performance comparisons. This would allow users to easily switch between backends. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author kaixih commented Jul 10, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Here is the lm_eval results after enabling the new moe backend: Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ‚Üë 0.968 ¬± 0.0079 strict-match 5 exact_match ‚Üë 0.962 ¬± 0.0086 To repro: pip install lm_eval[api]==0.4.8 export VLLM_WORKER_MULTIPROC_METHOD= \" spawn \" export VLLM_USE_V1= \" 1 \" export VLLM_USE_STANDALONE_COMPILE= \" 0 \" export VLLM_USE_FLASHINFER_MOE_FP8= \" 1 \" model_dir= < your ckpts of DeepSeek-R1- 0528> model_args= \" model= ${model_dir} ,pretrained= ${model_dir} ,trust_remote_code=True,tensor_parallel_size=8,enable_expert_parallel=True,enforce_eager=False,max_model_len=2048 \" lm_eval --model vllm --model_args $model_args --gen_kwargs temperature=0.0 --limit 500 --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size 200 üëç 2 mgoin and pavanimajety reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Jul 11, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @kaixih . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 11, 2025 kaixih force-pushed the kaixih/flashinfer-moe-bs-fp8 branch\n      2 times, most recently\n    from 6229f18 to 567d6ae Compare July 11, 2025 23:17 mergify bot removed\n  the needs-rebase label Jul 11, 2025 kaixih force-pushed the kaixih/flashinfer-moe-bs-fp8 branch\n    from 567d6ae to 85ccae5 Compare July 11, 2025 23:32 support flashinfer moe blockscale fp8 ‚Ä¶ 644d108 Signed-off-by: kaixih <kaixih@nvidia.com> kaixih force-pushed the kaixih/flashinfer-moe-bs-fp8 branch\n    from 85ccae5 to 644d108 Compare July 11, 2025 23:58 Minor ‚Ä¶ 44d86bb Signed-off-by: kaixih <kaixih@nvidia.com> Copy link Contributor Author kaixih commented Jul 12, 2025 These kernels are primarily beneficial in low-latency scenarios, so I also ran some latency benchmarks. The results are shown below. The flashinfer kernels can bring ~32% perf improvement for a DSR1 model on 8xB200 GPUs. # default:\nAvg latency: 22.061138840367253 seconds\n# flashinfer:\nAvg latency: 15.51937770833271 seconds To repro: export VLLM_WORKER_MULTIPROC_METHOD= \" spawn \" export VLLM_USE_V1= \" 1 \" export VLLM_USE_STANDALONE_COMPILE= \" 0 \" export VLLM_USE_FLASHINFER_MOE_FP8= \" 0 \" # or \"1\" for flashinfer model_dir= < your ckpts of DeepSeek-R1- 0528> python benchmarks/benchmark_latency.py --model= $model_dir --output-len=1024 --tensor-parallel-size=8 --enable-expert-parallel --input-len=128 --trust_remote_code --max-model-len=2048 --batch-size=1 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . kaixih changed the title [Draft] Add Flashinfer MoE blockscale fp8 backend [NVIDIA] Add Flashinfer MoE blockscale fp8 backend Jul 12, 2025 pavanimajety reviewed Jul 13, 2025 View reviewed changes vllm/model_executor/layers/fused_moe/fused_moe.py Outdated Comment on lines 1067 to 1068 def flashinfer_fused_moe_fp8(router_logits: torch.Tensor, e_score_correction_bias: torch.Tensor, Copy link Contributor pavanimajety Jul 13, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Use flashinfer_fused_moe_blockscale_fp8 to differentiate between other moe variants in FI Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor pavanimajety Jul 13, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment also add assert fi_fused_moe is not None Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author kaixih Jul 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Done. Thx. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions kaixih added 2 commits July 14, 2025 16:42 Address comments ‚Ä¶ a2b14c6 Signed-off-by: kaixih <kaixih@nvidia.com> Formatting ‚Ä¶ aa634a6 Signed-off-by: kaixih <kaixih@nvidia.com> mgoin changed the title [NVIDIA] Add Flashinfer MoE blockscale fp8 backend [NVIDIA] Add Flashinfer MoE blockscale fp8 backend for low latency Jul 16, 2025 Update API ‚Ä¶ 7ce56eb Signed-off-by: kaixih <kaixih@nvidia.com> Copy link Contributor Author kaixih commented Jul 16, 2025 I‚Äôve just updated the API call sites to accommodate the latest FlashInfer changes, which are recommended for improved robustness. I‚Äôd suggest testing the code with the ToT version of flashinfer or any release after 0.2.8. üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented Jul 18, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @kaixih . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 18, 2025 mgoin added 2 commits July 18, 2025 09:32 Merge branch 'main' into kaixih/flashinfer-moe-bs-fp8 ‚Ä¶ 8f6aa2f Signed-off-by: mgoin <mgoin64@gmail.com> Refactor to use flashinfer wrapper for lazy import ‚Ä¶ 2e61e91 Signed-off-by: mgoin <mgoin64@gmail.com> mgoin reviewed Jul 18, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Is it right that TP is not supported, only EP? I think we should assert if so I see this error with TP RuntimeError: Worker failed with error 'vllm::flashinfer_fused_moe_blockscale_fp8() Expected a value of type 'int' for argument 'num_expert_group' but instead found type 'NoneType'.\nPosition: 9\nValue: None\nDeclaration: vllm::flashinfer_fused_moe_blockscale_fp8(Tensor router_logits, Tensor e_score_correction_bias, Tensor x, Tensor w13_weight, Tensor w13_weight_scale_inv, Tensor w2_weight, Tensor w2_weight_scale_inv, SymInt global_num_experts, SymInt top_k, SymInt num_expert_group, SymInt topk_group, SymInt intermediate_size_per_partition, SymInt expert_offset, SymInt local_num_experts, SymInt[] block_shape, float routed_scaling=1., SymInt tile_tokens_dim=8, SymInt routing_method_type=2) -> Tensor\nCast error details: Unable to cast Python instance of type <class 'NoneType'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)', please check the stack trace above for the root cause Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/quantization/fp8.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/layers/quantization/fp8.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/layers/fused_moe/fused_moe.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . mergify bot removed\n  the needs-rebase label Jul 18, 2025 kaixih added 2 commits July 18, 2025 19:45 Address comments ‚Ä¶ 79ef02e Signed-off-by: kaixih <kaixih@nvidia.com> Format ‚Ä¶ 44b0d24 Signed-off-by: kaixih <kaixih@nvidia.com> mgoin added performance Performance-related issues ready ONLY add when PR is ready to merge/full CI is needed labels Jul 18, 2025 mgoin changed the title [NVIDIA] Add Flashinfer MoE blockscale fp8 backend for low latency [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency Jul 18, 2025 Copy link Contributor Author kaixih commented Jul 18, 2025 Is it right that TP is not supported, only EP? I think we should assert if so I think it supports it. I did a quick check and it looked good. Can you double check what is in your num_expert_group ? Are you testing a DS model? Here is what I used for quick test and you can turn on/off the enable_expert_parallel . All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author kaixih commented Jul 18, 2025 Also checked accuracy with TP=8: INFO:lm_eval.loggers.evaluation_tracker:Output path not provided, skipping saving results aggregated vllm (model=/model/models--deepseek-ai--DeepSeek-R1-0528/snapshots/4236a6af538feda4548eca9ab308586007567f52/,pretrained=/model/models--deepseek-ai--DeepSeek-R1-0528/snapshots/4236a6af538feda4548eca9ab308586007567f52/,trust_remote_code=True,tensor_parallel_size=8,enable_expert_parallel=False,enforce_eager=False,max_model_len=2048,trust_remote_code=True), gen_kwargs: (temperature=0.0), limit: 500.0, num_fewshot: 5, batch_size: 200 Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ‚Üë 0.964 ¬± 0.0083 strict-match 5 exact_match ‚Üë 0.958 ¬± 0.0090 To repro: export VLLM_WORKER_MULTIPROC_METHOD=\"spawn\"\nexport VLLM_USE_V1=\"1\"\nexport VLLM_USE_STANDALONE_COMPILE=\"0\"\nexport VLLM_USE_FLASHINFER_MOE_FP8=\"1\"\nmodel_dir=\"/model/models--deepseek-ai--DeepSeek-R1-0528/snapshots/4236a6af538feda4548eca9ab308586007567f52/\"\nmodel_args=\"model=${model_dir},pretrained=${model_dir},trust_remote_code=True,tensor_parallel_size=8,enable_expert_parallel=False,enforce_eager=False,max_model_len=2048\"\nlm_eval --model vllm --model_args $model_args --gen_kwargs temperature=0.0 --limit 500 --trust_remote_code --tasks gsm8k --num_fewshot 5 --batch_size 200 üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin approved these changes Jul 18, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, thank you! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin enabled auto-merge (squash) July 18, 2025 21:22 Hide details View details vllm-bot merged commit 6d0734c into vllm-project : main Jul 19, 2025 80 of 83 checks passed Uh oh! There was an error while loading. Please reload this page . hj-mistral pushed a commit\n        to hj-mistral/vllm\n      that referenced\n      this pull request Jul 19, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ d195bb6 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Himanshu Jaju <hj@mistral.ai> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ ac5c103 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 2919908 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 71dd173 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 36f7621 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 268cfab ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 392b3e9 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 0b6eb26 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 51d92ce ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ 4970555 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 27, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ d42a70b ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low late‚Ä¶ ‚Ä¶ fd638e0 ‚Ä¶ncy ( vllm-project#20645 )\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:29",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, lm_eval | PERF: latency, latency, latency | TEST: test, test, testing",
  "analysis_extracted_at": "2025-09-07 17:50:29",
  "models": [
    "mistralai/Mistral-7B-Instruct-v0.3",
    "deepseek-ai/DeepSeek-R1"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.3,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-R1,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)",
  "commit_message": "[NVIDIA] Add SM100 Flashinfer MoE blockscale fp8 backend for low latency (#20645)\n\nSigned-off-by: kaixih <kaixih@nvidia.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>",
  "commit_date": "2025-07-19T02:33:01-07:00",
  "files_changed": [
    "vllm/envs.py",
    "vllm/model_executor/layers/fused_moe/config.py",
    "vllm/model_executor/layers/fused_moe/fused_moe.py",
    "vllm/model_executor/layers/quantization/fp8.py",
    "vllm/model_executor/layers/quantization/modelopt.py",
    "vllm/utils/flashinfer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 6,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 6,
    "num_hunks": 16,
    "num_edited_lines": 218,
    "num_non_test_edited_lines": 218,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 261cc7855..0896ae3a9 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -119,7 +119,8 @@ if TYPE_CHECKING:\n     VLLM_TPU_BUCKET_PADDING_GAP: int = 0\n     VLLM_TPU_MOST_MODEL_LEN: Optional[int] = None\n     VLLM_USE_DEEP_GEMM: bool = False\n-    VLLM_USE_FLASHINFER_MOE: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP8: bool = False\n+    VLLM_USE_FLASHINFER_MOE_FP4: bool = False\n     VLLM_XGRAMMAR_CACHE_MB: int = 0\n     VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256\n     VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False\n@@ -854,9 +855,13 @@ environment_variables: dict[str, Callable[[], Any]] = {\n     \"VLLM_USE_DEEP_GEMM\":\n     lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"0\"))),\n \n+    # Allow use of FlashInfer MoE kernels for fused moe ops.\n+    \"VLLM_USE_FLASHINFER_MOE_FP8\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP8\", \"0\"))),\n+\n     # Allow use of FlashInfer CUTLASS kernels for fused moe ops.\n-    \"VLLM_USE_FLASHINFER_MOE\":\n-    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE\", \"0\"))),\n+    \"VLLM_USE_FLASHINFER_MOE_FP4\":\n+    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_MOE_FP4\", \"0\"))),\n \n     # Control the cache sized used by the xgrammar compiler. The default\n     # of 512 MB should be enough for roughly 1000 JSON schemas.\ndiff --git a/vllm/model_executor/layers/fused_moe/config.py b/vllm/model_executor/layers/fused_moe/config.py\nindex 9bebb6a65..51c421bd2 100644\n--- a/vllm/model_executor/layers/fused_moe/config.py\n+++ b/vllm/model_executor/layers/fused_moe/config.py\n@@ -191,7 +191,7 @@ class FusedMoEParallelConfig:\n \n     @property\n     def use_flashinfer_cutlass_kernels(self):\n-        return (envs.VLLM_USE_FLASHINFER_MOE\n+        return (envs.VLLM_USE_FLASHINFER_MOE_FP4\n                 and has_flashinfer_cutlass_fused_moe())\n \n     @staticmethod\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex aec5d7b25..c412f695a 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -28,7 +28,7 @@ from vllm.model_executor.layers.fused_moe.prepare_finalize import (\n from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (\n     TopKWeightAndReduceNoOP)\n from vllm.model_executor.layers.fused_moe.utils import (\n-    _resize_cache, moe_kernel_quantize_input)\n+    _resize_cache, moe_kernel_quantize_input, per_token_group_quant_fp8)\n from vllm.model_executor.layers.quantization.utils.mxfp4_utils import (\n     dequant_mxfp4)\n from vllm.platforms import current_platform\n@@ -1061,6 +1061,104 @@ direct_register_custom_op(\n )\n \n \n+def next_positive_power_of_2(x: int) -> int:\n+    if x < 1:\n+        return 1\n+    return 1 << (x - 1).bit_length()\n+\n+\n+def _get_tile_tokens_dim(num_tokens, top_k, num_experts):\n+    # Guess tokens per expert assuming perfect expert distribution first.\n+    num_tokens_per_expert = (num_tokens * top_k) // num_experts\n+    # And pad the number to the next power of 2.\n+    tile_tokens_dim = next_positive_power_of_2(num_tokens_per_expert)\n+    # Cap to 8-64 tokens per CTA tile as it's the range supported by the kernel.\n+    tile_tokens_dim = min(max(tile_tokens_dim, 8), 64)\n+    return tile_tokens_dim\n+\n+\n+def flashinfer_fused_moe_blockscale_fp8(\n+        routing_logits: torch.Tensor,\n+        routing_bias: torch.Tensor,\n+        x: torch.Tensor,\n+        w13_weight: torch.Tensor,\n+        w13_weight_scale_inv: torch.Tensor,\n+        w2_weight: torch.Tensor,\n+        w2_weight_scale_inv: torch.Tensor,\n+        global_num_experts: int,\n+        top_k: int,\n+        num_expert_group: int,\n+        topk_group: int,\n+        intermediate_size: int,\n+        expert_offset: int,\n+        local_num_experts: int,\n+        block_shape: list[int],\n+        routed_scaling: float = 1.0) -> torch.Tensor:\n+    from vllm.utils.flashinfer import flashinfer_trtllm_fp8_block_scale_moe\n+    assert top_k <= global_num_experts\n+    assert top_k <= 8\n+    assert topk_group <= 4\n+    assert global_num_experts > num_expert_group\n+    assert global_num_experts % num_expert_group == 0\n+    assert global_num_experts % 4 == 0\n+    assert top_k < (topk_group * global_num_experts / num_expert_group)\n+    assert block_shape == [128, 128]\n+\n+    a_q, a_sf = per_token_group_quant_fp8(x, block_shape[1])\n+    # NOTE: scales of hidden states have to be transposed!\n+    a_sf_t = a_sf.t().contiguous()\n+    return flashinfer_trtllm_fp8_block_scale_moe(\n+        routing_logits=routing_logits,\n+        routing_bias=routing_bias,\n+        hidden_states=a_q,\n+        hidden_states_scale=a_sf_t,\n+        gemm1_weights=w13_weight,\n+        gemm1_weights_scale=w13_weight_scale_inv,\n+        gemm2_weights=w2_weight,\n+        gemm2_weights_scale=w2_weight_scale_inv,\n+        num_experts=global_num_experts,\n+        top_k=top_k,\n+        n_group=num_expert_group,\n+        topk_group=topk_group,\n+        intermediate_size=intermediate_size,\n+        local_expert_offset=expert_offset,\n+        local_num_experts=local_num_experts,\n+        routed_scaling_factor=routed_scaling,\n+        tile_tokens_dim=_get_tile_tokens_dim(x.shape[0], top_k,\n+                                             global_num_experts),\n+        routing_method_type=2,  # DeepSeek-styled routing method\n+    )\n+\n+\n+def flashinfer_fused_moe_blockscale_fp8_fake(\n+        routing_logits: torch.Tensor,\n+        routing_bias: torch.Tensor,\n+        x: torch.Tensor,\n+        w13_weight: torch.Tensor,\n+        w13_weight_scale_inv: torch.Tensor,\n+        w2_weight: torch.Tensor,\n+        w2_weight_scale_inv: torch.Tensor,\n+        global_num_experts: int,\n+        top_k: int,\n+        num_expert_group: int,\n+        topk_group: int,\n+        intermediate_size: int,\n+        expert_offset: int,\n+        local_num_experts: int,\n+        block_shape: list[int],\n+        routed_scaling: float = 1.0) -> torch.Tensor:\n+    return torch.empty_like(x)\n+\n+\n+direct_register_custom_op(\n+    op_name=\"flashinfer_fused_moe_blockscale_fp8\",\n+    op_func=flashinfer_fused_moe_blockscale_fp8,\n+    mutates_args=[],\n+    fake_impl=flashinfer_fused_moe_blockscale_fp8_fake,\n+    tags=(torch.Tag.needs_fixed_stride_order, ),\n+)\n+\n+\n def outplace_fused_experts(\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex 824dfe15a..35d7545d8 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -43,6 +43,7 @@ from vllm.platforms import current_platform\n from vllm.scalar_type import scalar_types\n from vllm.utils import has_deep_gemm\n from vllm.utils.deep_gemm import is_blackwell_deep_gemm_used\n+from vllm.utils.flashinfer import has_flashinfer_moe\n \n if TYPE_CHECKING:\n     from vllm.model_executor.models.utils import WeightsMapper\n@@ -52,6 +53,11 @@ ACTIVATION_SCHEMES = [\"static\", \"dynamic\"]\n logger = init_logger(__name__)\n \n \n+def _swap_w13_to_w31(x: torch.Tensor) -> torch.Tensor:\n+    return x.reshape(-1, 2, x.shape[-2] // 2,\n+                     x.shape[-1]).flip(dims=[1]).reshape(x.shape)\n+\n+\n def _is_col_major(x: torch.Tensor) -> bool:\n     assert x.dim() == 3\n     b, m, n = x.shape\n@@ -473,6 +479,11 @@ class Fp8MoEMethod(FusedMoEMethodBase):\n         self.quant_config = quant_config\n         self.block_quant = self.quant_config.weight_block_size is not None\n \n+        self.flashinfer_moe_enabled = False\n+        if envs.VLLM_USE_FLASHINFER_MOE_FP8 and has_flashinfer_moe():\n+            logger.info_once(\n+                \"Using FlashInfer MoE FP8 kernels for Fp8MoEMethod.\")\n+            self.flashinfer_moe_enabled = True\n         # For GPUs that lack FP8 hardware support, we can leverage the Marlin\n         # kernel for fast weight-only FP8 quantization\n         self.use_marlin = (not current_platform.has_device_capability(89)\n@@ -674,6 +685,14 @@ class Fp8MoEMethod(FusedMoEMethodBase):\n                     normalize_e4m3fn_to_e4m3fnuz(\n                         layer.w2_weight, layer.w2_weight_scale_inv,\n                         layer.w2_input_scale)\n+            elif self.flashinfer_moe_enabled:\n+                # NOTE: weights have to be swapped since the activation is\n+                # applied on different half for flashinfer vs vllm\n+                w13_weight = _swap_w13_to_w31(layer.w13_weight.data)\n+                w13_weight_scale_inv = _swap_w13_to_w31(\n+                    layer.w13_weight_scale_inv.data)\n+                w2_weight = layer.w2_weight.data\n+                w2_weight_scale_inv = layer.w2_weight_scale_inv.data\n             else:\n                 w13_weight = layer.w13_weight.data\n                 w13_weight_scale_inv = layer.w13_weight_scale_inv.data\n@@ -915,25 +934,25 @@ class Fp8MoEMethod(FusedMoEMethodBase):\n             assert logical_to_physical_map is not None\n             assert logical_replica_count is not None\n             assert isinstance(layer, FusedMoE)\n-\n-        topk_weights, topk_ids = FusedMoE.select_experts(\n-            hidden_states=x,\n-            router_logits=router_logits,\n-            use_grouped_topk=use_grouped_topk,\n-            top_k=top_k,\n-            renormalize=renormalize,\n-            topk_group=topk_group,\n-            num_expert_group=num_expert_group,\n-            custom_routing_function=custom_routing_function,\n-            scoring_func=scoring_func,\n-            e_score_correction_bias=e_score_correction_bias,\n-            indices_type=self.topk_indices_dtype,\n-            enable_eplb=enable_eplb,\n-            expert_map=expert_map,\n-            expert_load_view=expert_load_view,\n-            logical_to_physical_map=logical_to_physical_map,\n-            logical_replica_count=logical_replica_count,\n-        )\n+        if not self.flashinfer_moe_enabled:\n+            topk_weights, topk_ids = FusedMoE.select_experts(\n+                hidden_states=x,\n+                router_logits=router_logits,\n+                use_grouped_topk=use_grouped_topk,\n+                top_k=top_k,\n+                renormalize=renormalize,\n+                topk_group=topk_group,\n+                num_expert_group=num_expert_group,\n+                custom_routing_function=custom_routing_function,\n+                scoring_func=scoring_func,\n+                e_score_correction_bias=e_score_correction_bias,\n+                indices_type=self.topk_indices_dtype,\n+                enable_eplb=enable_eplb,\n+                expert_map=expert_map,\n+                expert_load_view=expert_load_view,\n+                logical_to_physical_map=logical_to_physical_map,\n+                logical_replica_count=logical_replica_count,\n+            )\n \n         if self.rocm_aiter_moe_enabled:\n             from vllm.model_executor.layers.fused_moe.rocm_aiter_fused_moe import (  # noqa: E501\n@@ -971,6 +990,31 @@ class Fp8MoEMethod(FusedMoEMethodBase):\n                 apply_router_weight_on_input=apply_router_weight_on_input,\n                 global_num_experts=global_num_experts,\n                 expert_map=expert_map)\n+        elif self.flashinfer_moe_enabled:\n+            # Currently only work with DS models\n+            assert self.block_quant\n+            assert (renormalize and use_grouped_topk\n+                    and scoring_func == 'sigmoid'\n+                    and custom_routing_function is None)\n+            assert activation == \"silu\"\n+            return torch.ops.vllm.flashinfer_fused_moe_blockscale_fp8(\n+                routing_logits=router_logits.to(torch.float32),\n+                routing_bias=e_score_correction_bias,\n+                x=x,\n+                w13_weight=layer.w13_weight,\n+                w13_weight_scale_inv=layer.w13_weight_scale_inv,\n+                w2_weight=layer.w2_weight,\n+                w2_weight_scale_inv=layer.w2_weight_scale_inv,\n+                global_num_experts=global_num_experts,\n+                top_k=top_k,\n+                num_expert_group=num_expert_group,\n+                topk_group=topk_group,\n+                intermediate_size=layer.intermediate_size_per_partition,\n+                expert_offset=layer.ep_rank * layer.local_num_experts,\n+                local_num_experts=layer.local_num_experts,\n+                block_shape=self.quant_config.weight_block_size,\n+                routed_scaling=1.0,\n+            )\n         else:\n             return self.fused_experts(\n                 hidden_states=x,\ndiff --git a/vllm/model_executor/layers/quantization/modelopt.py b/vllm/model_executor/layers/quantization/modelopt.py\nindex 3807899fc..20def70d1 100644\n--- a/vllm/model_executor/layers/quantization/modelopt.py\n+++ b/vllm/model_executor/layers/quantization/modelopt.py\n@@ -721,7 +721,7 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n         self.use_marlin = False\n         self.allow_flashinfer_cutlass = False\n \n-        if envs.VLLM_USE_FLASHINFER_MOE:\n+        if envs.VLLM_USE_FLASHINFER_MOE_FP4:\n             if self.cutlass_nvfp4_supported and current_platform.is_cuda() \\\n                and current_platform.is_device_capability(100):\n                 logger.info_once(\n@@ -800,10 +800,9 @@ class ModelOptNvFp4FusedMoE(FusedMoEMethodBase):\n             assert moe.dp_size > 1\n             logger.debug_once(\"Using CutlassExpertsFp4\")\n             # Currently CutlassExpertsFp4 doesn't support DP\n-            raise ValueError(\n-                \"CutlassExpertsFp4 doesn't support DP. \"\n-                \"Use flashinfer CUTLASS FusedMoE(VLLM_USE_FLASHINFER_MOE)\"\n-                \" backend instead.\")\n+            raise ValueError(\"CutlassExpertsFp4 doesn't support DP. \"\n+                             \"Use flashinfer CUTLASS FusedMoE backend instead \"\n+                             \"(set VLLM_USE_FLASHINFER_MOE_FP4=1)\")\n \n         return experts\n \ndiff --git a/vllm/utils/flashinfer.py b/vllm/utils/flashinfer.py\nindex dbd2dc393..fd8b384a6 100644\n--- a/vllm/utils/flashinfer.py\n+++ b/vllm/utils/flashinfer.py\n@@ -64,6 +64,8 @@ def _lazy_import_wrapper(module_name: str,\n \n \n # Create lazy wrappers for each function\n+flashinfer_trtllm_fp8_block_scale_moe = _lazy_import_wrapper(\n+    \"flashinfer.fused_moe\", \"trtllm_fp8_block_scale_moe\")\n flashinfer_cutlass_fused_moe = _lazy_import_wrapper(\"flashinfer.fused_moe\",\n                                                     \"cutlass_fused_moe\")\n fp4_quantize = _lazy_import_wrapper(\"flashinfer\", \"fp4_quantize\")\n@@ -77,10 +79,16 @@ autotune = _lazy_import_wrapper(\n     fallback_fn=lambda *args, **kwargs: contextlib.nullcontext())\n \n \n+@functools.cache\n+def has_flashinfer_moe() -> bool:\n+    \"\"\"Return ``True`` if FlashInfer MoE module is available.\"\"\"\n+    return importlib.util.find_spec(\"flashinfer.fused_moe\") is not None\n+\n+\n @functools.cache\n def has_flashinfer_cutlass_fused_moe() -> bool:\n     \"\"\"Return ``True`` if FlashInfer CUTLASS fused MoE is available.\"\"\"\n-    if not has_flashinfer():\n+    if not has_flashinfer_moe():\n         return False\n \n     # Check if all required functions are available\n@@ -99,9 +107,11 @@ def has_flashinfer_cutlass_fused_moe() -> bool:\n \n __all__ = [\n     \"has_flashinfer\",\n-    \"has_flashinfer_cutlass_fused_moe\",\n+    \"flashinfer_trtllm_fp8_block_scale_moe\",\n     \"flashinfer_cutlass_fused_moe\",\n     \"fp4_quantize\",\n     \"fp4_swizzle_blockscale\",\n     \"autotune\",\n+    \"has_flashinfer_moe\",\n+    \"has_flashinfer_cutlass_fused_moe\",\n ]",
  "apis": [
    "vllm.model_executor.layers.fused_moe.fused_moe.flashinfer_fused_moe_blockscale_fp8",
    "vllm.model_executor.layers.fused_moe.config.FusedMoEParallelConfig.use_flashinfer_cutlass_kernels",
    "vllm.model_executor.layers.quantization.fp8.Fp8MoEMethod.apply",
    "vllm.utils.flashinfer.has_flashinfer_moe"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/envs.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/fp8.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies several non-test source files in the vllm codebase, including configuration files and quantization implementations, to add new backend functionality: FlashInfer MoE blockscale fp8 kernels. This change introduces new environment variables and selects new optimized code paths that target low latency operation on CPU (not GPU-specific) for fused MoE operations. The changes are non-trivial and focused on performance improvements (reducing latency), rather than simple refactoring, bug fixes, or standard feature additions. Hence, the modifications are performance/optimization related.",
  "llm_api_reason": "This commit adds support for new FlashInfer MoE backends by introducing two new environment flags (VLLM_USE_FLASHINFER_MOE_FP8 and VLLM_USE_FLASHINFER_MOE_FP4) in the environment module. It then updates the fused MoE layer by adding a new function (flashinfer_fused_moe_blockscale_fp8) and a helper to compute tile dimensions. In the FP8 quantization code for MoE, the Fp8MoEMethod.apply branch now calls the appropriate FlashInfer kernel when flashinfer MoE is enabled. Finally, a new utility (has_flashinfer_moe) is added in the flashinfer module to detect if the FlashInfer MoE module is present. These changes affect the public/fused MoE API endpoints used in FP8 MoE inference and configuration."
}