{
  "commit_hash": "b9986454fe8ba80e2a109d069397b6b59aae658b",
  "pr_url": "https://github.com/vllm-project/vllm/pull/12570",
  "pr_date": null,
  "timeline_text": "Copy link Contributor srikanthsrnvs commented Jan 30, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Fix to AWQ quant loading of the new R1 model The new optimized MoE kernels for a large number of experts moe_wn16 uses AWQ quant which requires the attention layers to be in 16bit The current merge has broken this, and the get_quant_method must return None for it to work correctly again Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions srikanthsrnvs requested review from mgoin , robertgshaw2-redhat and tlrmchlsmth as code owners January 30, 2025 04:43 Copy link github-actions bot commented Jan 30, 2025 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin approved these changes Jan 31, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thank you, makes sense! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added quantization ready ONLY add when PR is ready to merge/full CI is needed labels Jan 31, 2025 srikanthsrnvs and others added 23 commits February 3, 2025 03:14 Fix for attention layers to remain unquantized during moe_wn16 quant ‚Ä¶ ‚Ä¶ 483b60c ‚Ä¶method\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> Set ?device={device} when changing tab in installation guides ( vllm‚Ä¶ ‚Ä¶ 915fdce ‚Ä¶-project#12560 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Misc] fix typo: add missing space in lora adapter error message ( vll‚Ä¶ ‚Ä¶ d689505 ‚Ä¶m-project#12564 )\n\nSigned-off-by: Beim <beim2015@outlook.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Kernel] Triton Configs for Fp8 Block Quantization ( vllm-project#11589 ) ‚Ä¶ 689bd19 Signed-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [CPU][PPC] Updated torch, torchvision, torchaudio dependencies ( vllm-‚Ä¶ ‚Ä¶ f7a4e12 ‚Ä¶project#12555 )\n\nSigned-off-by: npanpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [V1][Log] Add max request concurrency log to V1 ( vllm-project#12569 ) ‚Ä¶ 95b49be Signed-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Kernel] Update cutlass_scaled_mm to support 2d group (blockwise) s‚Ä¶ ‚Ä¶ b0d7288 ‚Ä¶caling ( vllm-project#11868 )\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [ROCm][AMD][Model] llama 3.2 support upstreaming ( vllm-project#12421 ) ‚Ä¶ 9813962 Signed-off-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Attention] MLA decode optimizations ( vllm-project#12528 ) ‚Ä¶ 897c8c2 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Bugfix] Gracefully handle huggingface hub http error ( vllm-project#1‚Ä¶ ‚Ä¶ c4795ce ‚Ä¶2571 )\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> Format ‚Ä¶ a5e6700 Signed-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> Add favicon to docs ( vllm-project#12611 ) ‚Ä¶ 1ce860b Signed-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [BugFix] Fix Torch.Compile For DeepSeek ( vllm-project#12594 ) ‚Ä¶ bc9d831 Co-authored-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Git] Automatically sign-off commits ( vllm-project#12595 ) ‚Ä¶ 22b918d It's very annoying when I forgot to add `-s` in `git commit` to\nsign-off, because I then need to `git rebase HEAD~1 --signoff` and `git\npush -f` to fix the DCO. This PR adds a hook to sign off commits\nautomatically when `-s` is missing to solve this problem. The only\nchange from the user side is now users have to install 2 hooks, so\ninstead of just\n\n```\npre-commit install\n```\n\nNow we need to\n\n```\npre-commit install --hook-type pre-commit --hook-type commit-msg\n```\n\nNote that even if users still only install the pre-commit hook, they\nwon't get any error in `git commit`. Just the sign-off hook won't run.\n\ncc @hmellor @youkaichao ---------\n\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Docs][V1] Prefix caching design ( vllm-project#12598 ) ‚Ä¶ 00df0e4 - Create v1 design document section in docs.\n- Add prefix caching design doc. @WoosukKwon @ywang96 ---------\n\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [v1][Bugfix] Add extra_keys to block_hash for prefix caching ( vllm-pr‚Ä¶ ‚Ä¶ 44fa70d ‚Ä¶oject#12603 )\n\nThis pr adds extra key to block hash, to generate different hash value\nfor two blocks with the same token string but different extra_keys in\ntheir parent blocks. For example, it can generate different hash value\nfor the second block of the following two requests:\n```python\nrequest1 = make_request(\n        request_id=0,\n        prompt_token_ids=[_ for _ in range(6)],\n        mm_positions=[{\n            \"offset\": 0,\n            \"length\": 3\n        }, {\n            \"offset\": 3,\n            \"length\": 3\n        }],\n        mm_hashes=[\"hash1\", \"hash2\"],\n    )\n    request2 = make_request(\n        request_id=1,\n        prompt_token_ids=[_ for _ in range(6)],\n        mm_positions=[{\n            \"offset\": 0,\n            \"length\": 3\n        }, {\n            \"offset\": 3,\n            \"length\": 3\n        }],\n        mm_hashes=[\"hash3\", \"hash2\"],\n    )\n```\n\n---------\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [release] Add input step to ask for Release version ( vllm-project#12631 ) ‚Ä¶ fdd86fb Instead of having to create a new build with release version put in as\nenv var.\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Bugfix] Revert MoE Triton Config Default ( vllm-project#12629 ) ‚Ä¶ c4a7c26 SUMMARY:\n* previous PR for pulling in block configs also changed defaults\n( https://github.com/vllm-project/vllm/pull/11589/files ) for FP8\n* this broke L4 MoE since there was not enough SHM for the default\nconfiguration\n* this reverts the non-block example to the default\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Kernel][Quantization] Integrate block-quantized CUTLASS kernels for ‚Ä¶ ‚Ä¶ e7c98c6 ‚Ä¶DeepSeekV3 ( vllm-project#12587 )\n\nIntegrates the block-quantized kernels introduced in vllm-project#11868 for use in linear\nlayers.\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ d27e55d ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Doc] Improve installation signposting ( vllm-project#12575 ) ‚Ä¶ bece70b - Make device tab names more explicit\n- Add comprehensive list of devices to https://docs.vllm.ai/en/latest/getting_started/installation/index.html - Add `attention` blocks to the intro of all devices that don't have\npre-built wheels/images\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [Doc] int4 w4a16 example ( vllm-project#12585 ) ‚Ä¶ 6b7e433 Based on a request by @mgoin , with @kylesayrs we have added an example\ndoc for int4 w4a16 quantization, following the pre-existing int8 w8a8\nquantization example and the example available in\n[`llm-compressor`]( https://github.com/vllm-project/llm-compressor/blob/main/examples/quantization_w4a16/llama3_example.py )\n\nFIX #n/a (no issue created) @kylesayrs and I have discussed a couple additional improvements for the\nquantization docs. We will revisit at a later date, possibly including:\n- A section for \"choosing the correct quantization scheme/ compression\ntechnique\"\n- Additional vision or audio calibration datasets\n\n---------\n\nSigned-off-by: Brian Dellabetta <bdellabe@redhat.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> [V1] Bugfix: Validate Model Input Length ( vllm-project#12600 ) ‚Ä¶ fd9060b SUMMARY:\n* avoid crashing the engine when we get an input longer than\nmax_model_len FIX vllm-project#12567 (*link existing issues this PR will resolve*)\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> 18 hidden items Load more‚Ä¶ srikanthsrnvs requested review from LiuXiaoxuanPKU , KuntaiDu , DarkLight1337 , ywang96 and zhuohan123 as code owners February 3, 2025 03:15 mergify bot added documentation Improvements or additions to documentation ci/build frontend structured-output speculative-decoding labels Feb 3, 2025 Copy link mergify bot commented Feb 3, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @srikanthsrnvs . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added v1 needs-rebase labels Feb 3, 2025 Merge branch 'main' into fix-moe-wna16-attention 8b5a0ea mergify bot removed\n  the needs-rebase label Feb 3, 2025 unused imports 9d09ec0 DarkLight1337 enabled auto-merge (squash) February 3, 2025 05:11 Copy link Contributor Author srikanthsrnvs commented Feb 3, 2025 Anyone know why the Docker image building fails? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented Feb 3, 2025 Not sure. It's also a problem on main so it's not related to this PR. We will force-merge if necessary. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . youkaichao disabled auto-merge February 3, 2025 05:46 Hide details View details youkaichao merged commit b998645 into vllm-project : main Feb 3, 2025 24 of 38 checks passed Uh oh! There was an error while loading. Please reload this page . sahelib25 pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Feb 3, 2025 Fix for attention layers to remain unquantized during moe_wn16 quant ( v‚Ä¶ ‚Ä¶ 576c903 ‚Ä¶llm-project#12570 )\n\nFix to AWQ quant loading of the new R1 model\n\nThe new optimized MoE kernels for a large number of experts `moe_wn16`\nuses AWQ quant which requires the attention layers to be in 16bit\n\nThe current merge has broken this, and the `get_quant_method` must\nreturn None for it to work correctly again\n\n---------\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Beim <beim2015@outlook.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: npanpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Brian Dellabetta <bdellabe@redhat.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Shawn Du <shawnd200@outlook.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Beim <805908499@qq.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Ryan Nguyen <96593302+xpbowler@users.noreply.github.com>\nCo-authored-by: Brian Dellabetta <brian-dellabetta@users.noreply.github.com>\nCo-authored-by: fade_away <1028552010@qq.com>\nCo-authored-by: weilong.yu <weilong.yu@shopee.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Eldar Kurtic <eldarkurtic314@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Shawn Du <shawnd200@outlook.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 Fix for attention layers to remain unquantized during moe_wn16 quant ( v‚Ä¶ ‚Ä¶ e145287 ‚Ä¶llm-project#12570 )\n\nFix to AWQ quant loading of the new R1 model\n\nThe new optimized MoE kernels for a large number of experts `moe_wn16`\nuses AWQ quant which requires the attention layers to be in 16bit\n\nThe current merge has broken this, and the `get_quant_method` must\nreturn None for it to work correctly again\n\n---------\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Beim <beim2015@outlook.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: npanpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Brian Dellabetta <bdellabe@redhat.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Shawn Du <shawnd200@outlook.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Beim <805908499@qq.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Ryan Nguyen <96593302+xpbowler@users.noreply.github.com>\nCo-authored-by: Brian Dellabetta <brian-dellabetta@users.noreply.github.com>\nCo-authored-by: fade_away <1028552010@qq.com>\nCo-authored-by: weilong.yu <weilong.yu@shopee.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Eldar Kurtic <eldarkurtic314@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Shawn Du <shawnd200@outlook.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:46",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: optimization, optimization, profile | SERVING: frontend | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:52:46",
  "models": [
    "deepseek-ai/DeepSeek-R1"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-R1 --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1 --dataset-name sharegpt --num-prompts 100",
  "commit_subject": "Fix for attention layers to remain unquantized during moe_wn16 quant (#12570)",
  "commit_message": "Fix for attention layers to remain unquantized during moe_wn16 quant (#12570)\n\nFix to AWQ quant loading of the new R1 model\n\nThe new optimized MoE kernels for a large number of experts `moe_wn16`\nuses AWQ quant which requires the attention layers to be in 16bit\n\nThe current merge has broken this, and the `get_quant_method` must\nreturn None for it to work correctly again\n\n---------\n\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Beim <beim2015@outlook.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: npanpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: simon-mo <xmo@berkeley.edu>\nSigned-off-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Brian Dellabetta <bdellabe@redhat.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Shawn Du <shawnd200@outlook.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Beim <805908499@qq.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: simon-mo <xmo@berkeley.edu>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Ryan Nguyen <96593302+xpbowler@users.noreply.github.com>\nCo-authored-by: Brian Dellabetta <brian-dellabetta@users.noreply.github.com>\nCo-authored-by: fade_away <1028552010@qq.com>\nCo-authored-by: weilong.yu <weilong.yu@shopee.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Eldar Kurtic <eldarkurtic314@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Vicente Herrera <vicenteherrera@vicenteherrera.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Shawn Du <shawnd200@outlook.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>",
  "commit_date": "2025-02-03T13:46:19+08:00",
  "files_changed": [
    "vllm/model_executor/layers/quantization/moe_wna16.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 3,
    "num_edited_lines": 10,
    "num_non_test_edited_lines": 10,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py\nindex 1ae765a22..56fa597e2 100644\n--- a/vllm/model_executor/layers/quantization/moe_wna16.py\n+++ b/vllm/model_executor/layers/quantization/moe_wna16.py\n@@ -7,7 +7,8 @@ import torch\n from vllm.distributed import get_tensor_model_parallel_rank, get_tp_group\n from vllm.model_executor.layers.fused_moe.layer import (\n     FusedMoE, FusedMoEMethodBase, FusedMoeWeightScaleSupported)\n-from vllm.model_executor.layers.linear import UnquantizedLinearMethod\n+from vllm.model_executor.layers.linear import (LinearBase,\n+                                               UnquantizedLinearMethod)\n from vllm.model_executor.layers.quantization.awq import AWQConfig\n from vllm.model_executor.layers.quantization.awq_marlin import AWQMarlinConfig\n from vllm.model_executor.layers.quantization.base_config import (\n@@ -125,9 +126,7 @@ class MoeWNA16Config(QuantizationConfig):\n                          prefix: str) -> Optional[\"QuantizeMethodBase\"]:\n         if is_layer_skipped_quant(prefix, self.modules_to_not_convert):\n             return UnquantizedLinearMethod()\n-        elif isinstance(layer, FusedMoE):\n-            return MoeWNA16Method(self)\n-        else:\n+        elif isinstance(layer, LinearBase):\n             if self.linear_quant_method == \"gptq\":\n                 if self.use_marlin:\n                     return GPTQMarlinConfig.from_config(\n@@ -144,6 +143,9 @@ class MoeWNA16Config(QuantizationConfig):\n                         self.full_config).get_quant_method(layer, prefix)\n             else:\n                 raise ValueError(\"moe_wna16 only support gptq and awq.\")\n+        elif isinstance(layer, FusedMoE):\n+            return MoeWNA16Method(self)\n+        return None\n \n \n def is_layer_skipped_quant(prefix: str, modules_to_not_convert: List[str]):",
  "apis": [
    "vllm.model_executor.layers.quantization.moe_wna16.MoeWNA16Config.get_quant_method"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/moe_wna16.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/linear.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/layer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/layer.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The patch modifies a source code file (not a test or documentation file) in a non-trivial way by changing the logic in the quantization configuration. Although the commit message describes a ‚ÄúFix for attention layers‚Äù it explains that this change is needed so that the new optimized MoE kernels can function correctly. This indicates that the fix is directly tied to performance optimization‚Äîensuring that attention layers are not inadvertently quantized and thus can run in highly optimized 16-bit mode for large expert models. The change helps to restore the intended performance behavior by aligning the quantization logic with the performance requirements of the optimized kernels. Therefore, it qualifies as a performance/optimization-related commit.",
  "llm_api_reason": "The commit reorders the type checks in the get_quant_method function of the MoeWNA16 quantization configuration. Its behavior now first handles layers derived from LinearBase (applying GPTQ/AWQ quantization as appropriate), then ‚Äì if the layer is a FusedMoE ‚Äì returns the MoeWNA16Method, and finally returns None for other layer types (thereby leaving attention layers unquantized). This change fixes the AWQ quant loading for the new R1 model by ensuring that unsupported layers (such as attention) remain in 16‚Äêbit mode rather than being quantized."
}