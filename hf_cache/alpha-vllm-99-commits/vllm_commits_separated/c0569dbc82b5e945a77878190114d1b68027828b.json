{
  "commit_hash": "c0569dbc82b5e945a77878190114d1b68027828b",
  "pr_url": "https://github.com/vllm-project/vllm/pull/20725",
  "pr_date": "2025-07-14",
  "timeline_text": "Copy link Contributor varun-sundar-rabindranath commented Jul 10, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Purpose Perform weight-application and reduction inside the TritonExperts and DeepGemmExperts. This helps save memory. For example please refer to #20228 Changes: Add topk_weights and apply_router_weight_on_input args to FusedMoEPermuteExpertsUnpermute::apply functions - so the implementations can perform topk-weight application if they wish to. Adjust workspace reuse in TritonExperts and DeepGemmExperts to accommodate weight-application and reduction. Test Plan pytest : pytest -s tests/kernels/moe/test_modular_kernel_combinations.py e2e tests: Using TritonOrDeepGemmExperts VLLM_ALL2ALL_BACKEND=\"deepep_high_throughput\" VLLM_USE_DEEP_GEMM=1  canhazgpu run -g 2 --  vllm serve Qwen/Qwen3-30B-A3B-FP8  --trust-remote-code --enable-expert-parallel --data-parallel-size 2 --port 9010 Using only TritonExperts VLLM_ALL2ALL_BACKEND=\"deepep_high_throughput\" VLLM_USE_DEEP_GEMM=0  canhazgpu run -g 2 --  vllm serve Qwen/Qwen3-30B-A3B-FP8  --trust-remote-code --enable-expert-parallel --data-parallel-size 2 --port 9010 lm-eval command : lm_eval --model local-completions --tasks gsm8k --model_args model=Qwen/Qwen3-30B-A3B-FP8,base_url=http://127.0.0.1:9010/v1/completions,num_concurrent=30,max_retries=3 --limit 100 Test Result |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  | 0.86|¬±  |0.0349|\n|     |       |strict-match    |     5|exact_match|‚Üë  | 0.92|¬±  |0.0273| |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  | 0.80|¬±  |0.0402|\n|     |       |strict-match    |     5|exact_match|‚Üë  | 0.91|¬±  |0.0288| (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions varun-sundar-rabindranath requested review from tlrmchlsmth and WoosukKwon as code owners July 10, 2025 00:41 Copy link github-actions bot commented Jul 10, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . varun-sundar-rabindranath marked this pull request as draft July 10, 2025 00:42 Copy link mergify bot commented Jul 10, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @varun-sundar-rabindranath . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 10, 2025 gemini-code-assist bot reviewed Jul 10, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @varun-sundar-rabindranath , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request enhances the FusedMoEPermuteExpertsUnpermute module by enabling weight application and reduction within the fused operation. This is achieved through the introduction of the TopKWeightAndReduce abstraction, which allows implementations to specify how the finalize() method should behave. The PR also standardizes weight application and reduction implementations, improving code organization and maintainability. Highlights MoE Reduction : Adds the ability to perform MoE reduction within the FusedMoEPermuteExpertsUnpermute operation, allowing for memory footprint reduction. TopKWeightAndReduce Abstraction : Introduces the TopKWeightAndReduce abstraction to standardize weight application and reduction implementations, providing flexibility in the finalize() method. Standardization : Consolidates all weight-application and reduction implementations into a single location for better maintainability. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with üëç and üëé on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . ‚Ü© Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jul 10, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces a significant and well-designed refactoring to make MoE kernels more modular. By abstracting the weight application and reduction logic into a WeightAndReduce class, it allows different FusedMoEPermuteExpertsUnpermute implementations to either perform this step themselves or delegate it to the finalize stage. This is a great improvement for code clarity, reusability, and will help in reducing memory footprint as intended. The changes are well-implemented across the affected files. My feedback focuses on a few areas where code can be made more concise and consistent with the established API contracts. These are minor points in an otherwise excellent PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/layers/fused_moe/deep_gemm_moe.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . varun-sundar-rabindranath force-pushed the varun/experts-reduce branch\n    from e797a42 to 27306fa Compare July 10, 2025 00:51 mergify bot removed\n  the needs-rebase label Jul 10, 2025 varun-sundar-rabindranath force-pushed the varun/experts-reduce branch\n    from 27306fa to 3f1d2da Compare July 10, 2025 19:55 Copy link mergify bot commented Jul 10, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @varun-sundar-rabindranath . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 10, 2025 varun-sundar-rabindranath force-pushed the varun/experts-reduce branch\n    from 3d3003a to 4389c7a Compare July 11, 2025 01:36 mergify bot removed\n  the needs-rebase label Jul 11, 2025 varun-sundar-rabindranath changed the title [Misc] Modular Kernel : Add ability to MoE reduce in FusedMoEPermuteExpertsUnpermute [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts Jul 11, 2025 varun-sundar-rabindranath marked this pull request as ready for review July 11, 2025 02:42 varun-sundar-rabindranath commented Jul 11, 2025 View reviewed changes vllm/model_executor/layers/fused_moe/deep_gemm_moe.py (M_sum, N // 2)) mm2_out = _resize_cache(workspace2, (M_sum, K)) mm2_out = _resize_cache(workspace13, (M_sum, K)) perm_out = _resize_cache(workspace2, (M * num_topk, K)) Copy link Contributor Author varun-sundar-rabindranath Jul 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment rearrage how workspaces are used to make space for perm_out - note that perm_out cannot use workspace13 as workspace13 may be used as the output tensor ( vllm/vllm/model_executor/layers/fused_moe/modular_kernel.py Line 486\n      in 5923ab9 fused_out = _resize_cache ( workspace13 , fused_out_shape ) ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions varun-sundar-rabindranath commented Jul 11, 2025 View reviewed changes vllm/model_executor/layers/fused_moe/fused_moe.py (num_tokens * top_k_num, N // 2)) intermediate_cache3 = _resize_cache(workspace2, (num_tokens, top_k_num, K)) Copy link Contributor Author varun-sundar-rabindranath Jul 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment rearrage how workspaces are used to make space for intermediate_cache3 - note that intermediate_cache3 cannot use workspace13 as workspace13 may be used as the output tensor Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link mergify bot commented Jul 11, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @varun-sundar-rabindranath . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 11, 2025 varun-sundar-rabindranath force-pushed the varun/experts-reduce branch\n    from 4389c7a to c5fd979 Compare July 11, 2025 16:56 mergify bot removed\n  the needs-rebase label Jul 11, 2025 This was referenced Jul 12, 2025 [Kernels][Misc] DeepGemm High-Throughput Optimizations #20228 Closed [Kernel] DeepGemm MoE : Integrate triton permute / unpermute kernels #20903 Merged tlrmchlsmth approved these changes Jul 14, 2025 View reviewed changes vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 14, 2025 Varun Sundar Rabindranath added 3 commits July 14, 2025 16:10 do reduction in experts ‚Ä¶ c9f2001 Signed-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com> fix workspace overallocation ‚Ä¶ 4d7e07b Signed-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com> TritonExperts opt ‚Ä¶ 2961f53 Signed-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com> varun-sundar-rabindranath force-pushed the varun/experts-reduce branch\n    from e369637 to 2961f53 Compare July 14, 2025 16:13 Copy link Collaborator tlrmchlsmth commented Jul 14, 2025 Confirm that without this PR, I cannot run a full sequence length DeepSeekV3 across 16 H200s and with it I see: GPU KV cache size: 236,736 tokens üéâ 1 robertgshaw2-redhat reacted with hooray emoji All reactions üéâ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth enabled auto-merge (squash) July 14, 2025 18:04 Hide details View details tlrmchlsmth merged commit c0569db into vllm-project : main Jul 14, 2025 68 checks passed Uh oh! There was an error while loading. Please reload this page . py-andy-c pushed a commit\n        to py-andy-c/vllm\n      that referenced\n      this pull request Jul 14, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 5dfb1a9 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> py-andy-c pushed a commit\n        to py-andy-c/vllm\n      that referenced\n      this pull request Jul 14, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ d9b727c ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> patrickvonplaten pushed a commit\n        to patrickvonplaten/vllm\n      that referenced\n      this pull request Jul 15, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 8595ba0 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Patrick von Platen <patrick.v.platen@gmail.com> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 8150275 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 0bee6a6 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 3eba418 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 813b32a ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 8e72fe1 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 98a3732 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 1bb105e ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 7d7f94b ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 6c7acc9 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 27, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ a202b30 ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts &‚Ä¶ ‚Ä¶ 9737a2e ‚Ä¶ DeepGemmExperts ( vllm-project#20725 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:41",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm-eval, lm_eval, gsm8k | PERF: Throughput, improvement | SERVING: vllm serve, vllm serve, serve | TEST: Test, Test, test",
  "analysis_extracted_at": "2025-09-07 17:50:41",
  "models": [
    "Qwen/Qwen3-30B-A3B-FP8"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen3-30B-A3B-FP8,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)",
  "commit_message": "[Misc] ModularKernel : Perform WeightAndReduce inside TritonExperts & DeepGemmExperts (#20725)\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>",
  "commit_date": "2025-07-14T19:47:16Z",
  "files_changed": [
    "vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py",
    "vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py",
    "vllm/model_executor/layers/fused_moe/cutlass_moe.py",
    "vllm/model_executor/layers/fused_moe/deep_gemm_moe.py",
    "vllm/model_executor/layers/fused_moe/fused_batched_moe.py",
    "vllm/model_executor/layers/fused_moe/fused_moe.py",
    "vllm/model_executor/layers/fused_moe/modular_kernel.py",
    "vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py",
    "vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 9,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 9,
    "num_hunks": 39,
    "num_edited_lines": 360,
    "num_non_test_edited_lines": 360,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\nindex 70a580b9c..0b3943292 100644\n--- a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\n@@ -260,6 +260,7 @@ class BatchedDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\n         w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         activation: str,\n         global_num_experts: int,\n@@ -273,6 +274,7 @@ class BatchedDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         workspace13: torch.Tensor,\n         workspace2: torch.Tensor,\n         expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ):\n         assert expert_tokens_meta is not None\n         expert_num_tokens = expert_tokens_meta.expert_num_tokens\ndiff --git a/vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py\nindex 41faced58..12df9bb34 100644\n--- a/vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py\n@@ -129,30 +129,22 @@ class BatchedTritonOrDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             return self.batched_triton_experts.workspace_shapes(\n                 a, aq, M, N, K, topk, global_num_experts, local_num_experts)\n \n-    def apply(\n-        self,\n-        output: torch.Tensor,\n-        hidden_states: torch.Tensor,\n-        w1: torch.Tensor,\n-        w2: torch.Tensor,\n-        topk_ids: torch.Tensor,\n-        activation: str,\n-        global_num_experts: int,\n-        expert_map: Optional[torch.Tensor],\n-        w1_scale: Optional[torch.Tensor],\n-        w2_scale: Optional[torch.Tensor],\n-        w1_zp: Optional[torch.Tensor],\n-        w2_zp: Optional[torch.Tensor],\n-        a1q_scale: Optional[torch.Tensor],\n-        a2_scale: Optional[torch.Tensor],\n-        workspace13: torch.Tensor,\n-        workspace2: torch.Tensor,\n-        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n-    ):\n+    def apply(self, output: torch.Tensor, hidden_states: torch.Tensor,\n+              w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor,\n+              topk_ids: torch.Tensor, activation: str, global_num_experts: int,\n+              expert_map: Optional[torch.Tensor],\n+              w1_scale: Optional[torch.Tensor],\n+              w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor],\n+              w2_zp: Optional[torch.Tensor], a1q_scale: Optional[torch.Tensor],\n+              a2_scale: Optional[torch.Tensor], workspace13: torch.Tensor,\n+              workspace2: torch.Tensor,\n+              expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+              apply_router_weight_on_input: bool):\n         experts = (self.batched_deep_gemm_experts\n                    if self.allow_deep_gemm else self.batched_triton_experts)\n         assert experts is not None\n-        experts.apply(output, hidden_states, w1, w2, topk_ids, activation,\n-                      global_num_experts, expert_map, w1_scale, w2_scale,\n-                      w1_zp, w2_zp, a1q_scale, a2_scale, workspace13,\n-                      workspace2, expert_tokens_meta)\n+        experts.apply(output, hidden_states, w1, w2, topk_weights, topk_ids,\n+                      activation, global_num_experts, expert_map, w1_scale,\n+                      w2_scale, w1_zp, w2_zp, a1q_scale, a2_scale, workspace13,\n+                      workspace2, expert_tokens_meta,\n+                      apply_router_weight_on_input)\ndiff --git a/vllm/model_executor/layers/fused_moe/cutlass_moe.py b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\nindex d6a30e342..e479f1b40 100644\n--- a/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/cutlass_moe.py\n@@ -291,26 +291,17 @@ class CutlassExpertsFp8(mk.FusedMoEPermuteExpertsUnpermute):\n         return (workspace1, workspace2, output,\n                 self.out_dtype if self.out_dtype is not None else a.dtype)\n \n-    def apply(\n-        self,\n-        output: torch.Tensor,\n-        hidden_states: torch.Tensor,\n-        w1: torch.Tensor,\n-        w2: torch.Tensor,\n-        topk_ids: torch.Tensor,\n-        activation: str,\n-        global_num_experts: int,\n-        expert_map: Optional[torch.Tensor],\n-        w1_scale: Optional[torch.Tensor],\n-        w2_scale: Optional[torch.Tensor],\n-        w1_zp: Optional[torch.Tensor],\n-        w2_zp: Optional[torch.Tensor],\n-        a1q_scale: Optional[torch.Tensor],\n-        a2_scale: Optional[torch.Tensor],\n-        workspace13: torch.Tensor,\n-        workspace2: torch.Tensor,\n-        expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n-    ):\n+    def apply(self, output: torch.Tensor, hidden_states: torch.Tensor,\n+              w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor,\n+              topk_ids: torch.Tensor, activation: str, global_num_experts: int,\n+              expert_map: Optional[torch.Tensor],\n+              w1_scale: Optional[torch.Tensor],\n+              w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor],\n+              w2_zp: Optional[torch.Tensor], a1q_scale: Optional[torch.Tensor],\n+              a2_scale: Optional[torch.Tensor], workspace13: torch.Tensor,\n+              workspace2: torch.Tensor,\n+              expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+              apply_router_weight_on_input: bool):\n         assert w1_zp is None, \"w1_zp is not supported in CUTLASS MoE\"\n         assert w2_zp is None, \"w2_zp is not supported in CUTLASS MoE\"\n \ndiff --git a/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py\nindex b1107a1f4..cc5e7cf57 100644\n--- a/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py\n@@ -13,7 +13,7 @@ from vllm.model_executor.layers.fused_moe.moe_permute_unpermute import (\n from vllm.model_executor.layers.fused_moe.prepare_finalize import (\n     MoEPrepareAndFinalizeNoEP)\n from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (\n-    TopKWeightAndReduceDelegate)\n+    TopKWeightAndReduceContiguous, TopKWeightAndReduceNoOP)\n from vllm.model_executor.layers.fused_moe.utils import _resize_cache\n from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n     per_token_group_quant_fp8)\n@@ -90,8 +90,7 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         return True\n \n     def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:\n-        # Let PrepareAndFinalize::finalize() decide the impl.\n-        return TopKWeightAndReduceDelegate()\n+        return TopKWeightAndReduceNoOP()\n \n     def workspace_shapes(\n         self, a: torch.Tensor, aq: torch.Tensor, M: int, N: int, K: int,\n@@ -104,9 +103,9 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         block_m = self.block_shape[0]\n         M_sum = (M * topk) + num_experts * (block_m - 1)\n         M_sum = round_up(M_sum, block_m)\n-        workspace1 = (M_sum, max(N * 2, K))\n+        workspace1 = (M_sum, max(N // 2, K))\n         workspace2 = (M_sum, max(N, K))\n-        output = (M, topk, K)\n+        output = (M, K)\n         return (workspace1, workspace2, output, a.dtype)\n \n     def apply(\n@@ -115,6 +114,7 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\n         w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         activation: str,\n         global_num_experts: int,\n@@ -128,11 +128,14 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         workspace13: torch.Tensor,\n         workspace2: torch.Tensor,\n         expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ):\n         assert self.block_shape is not None\n \n         a1q = hidden_states\n         _, N, K = w1.size()\n+        M, _ = output.size()\n+        num_topk = topk_ids.size(1)\n \n         if global_num_experts == -1:\n             global_num_experts = w1.size(0)\n@@ -159,11 +162,12 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         # Note: M_sum is different than the pre-permuted shape of a1q.\n         M_sum = a1q.size(0)\n \n-        mm1_out = _resize_cache(workspace13, (M_sum, N))\n-        act_out = _resize_cache(workspace2, (M_sum, N // 2))\n-        quant_out = _resize_cache(workspace13.view(dtype=torch.float8_e4m3fn),\n+        mm1_out = _resize_cache(workspace2, (M_sum, N))\n+        act_out = _resize_cache(workspace13, (M_sum, N // 2))\n+        quant_out = _resize_cache(workspace2.view(dtype=torch.float8_e4m3fn),\n                                   (M_sum, N // 2))\n-        mm2_out = _resize_cache(workspace2, (M_sum, K))\n+        mm2_out = _resize_cache(workspace13, (M_sum, K))\n+        perm_out = _resize_cache(workspace2, (M * num_topk, K))\n \n         m_grouped_fp8_gemm_nt_contiguous((a1q, a1q_scale), (w1, w1_scale),\n                                          mm1_out, expert_ids)\n@@ -179,7 +183,14 @@ class DeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         m_grouped_fp8_gemm_nt_contiguous((a2q, a2q_scale), (w2, w2_scale),\n                                          mm2_out, expert_ids)\n \n-        torch.index_select(mm2_out, 0, inv_perm, out=output.view((-1, K)))\n+        torch.index_select(mm2_out, 0, inv_perm, out=perm_out)\n+\n+        TopKWeightAndReduceContiguous().apply(\n+            output=output,\n+            fused_expert_output=perm_out,\n+            topk_weights=topk_weights,\n+            topk_ids=topk_ids,\n+            apply_router_weight_on_input=apply_router_weight_on_input)\n \n \n def deep_gemm_moe_fp8(\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\nindex 61247e930..b311ef1ac 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py\n@@ -696,15 +696,16 @@ class NaiveBatchedExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             return t.to(f32) * group_broadcast(scale, t.shape)\n \n     def apply(self, output: torch.Tensor, hidden_states: torch.Tensor,\n-              w1: torch.Tensor, w2: torch.Tensor, topk_ids: torch.Tensor,\n-              activation: str, global_num_experts: int,\n+              w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor,\n+              topk_ids: torch.Tensor, activation: str, global_num_experts: int,\n               expert_map: Optional[torch.Tensor],\n               w1_scale: Optional[torch.Tensor],\n               w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor],\n               w2_zp: Optional[torch.Tensor], a1q_scale: Optional[torch.Tensor],\n               a2_scale: Optional[torch.Tensor], workspace13: torch.Tensor,\n               workspace2: torch.Tensor,\n-              expert_tokens_meta: Optional[mk.ExpertTokensMetadata]):\n+              expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+              apply_router_weight_on_input: bool):\n         assert hidden_states.dim() == 3\n         assert expert_tokens_meta is not None\n         expert_num_tokens = expert_tokens_meta.expert_num_tokens\n@@ -899,15 +900,16 @@ class BatchedTritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         return (workspace13, workspace2, output, a.dtype)\n \n     def apply(self, output: torch.Tensor, hidden_states: torch.Tensor,\n-              w1: torch.Tensor, w2: torch.Tensor, topk_ids: torch.Tensor,\n-              activation: str, global_num_experts: int,\n+              w1: torch.Tensor, w2: torch.Tensor, topk_weights: torch.Tensor,\n+              topk_ids: torch.Tensor, activation: str, global_num_experts: int,\n               expert_map: Optional[torch.Tensor],\n               w1_scale: Optional[torch.Tensor],\n               w2_scale: Optional[torch.Tensor], w1_zp: Optional[torch.Tensor],\n               w2_zp: Optional[torch.Tensor], a1q_scale: Optional[torch.Tensor],\n               a2_scale: Optional[torch.Tensor], workspace13: torch.Tensor,\n               workspace2: torch.Tensor,\n-              expert_tokens_meta: Optional[mk.ExpertTokensMetadata]):\n+              expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+              apply_router_weight_on_input: bool):\n         # Check constraints.\n         if self.use_int4_w4a16:\n             assert hidden_states.size(-1) // 2 == w1.size(2), (\ndiff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py\nindex 6a9767fc6..f0bffc7da 100644\n--- a/vllm/model_executor/layers/fused_moe/fused_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py\n@@ -26,7 +26,7 @@ from vllm.model_executor.layers.fused_moe.moe_align_block_size import (\n from vllm.model_executor.layers.fused_moe.prepare_finalize import (\n     MoEPrepareAndFinalizeNoEP)\n from vllm.model_executor.layers.fused_moe.topk_weight_and_reduce import (\n-    TopKWeightAndReduceDelegate)\n+    TopKWeightAndReduceNoOP)\n from vllm.model_executor.layers.fused_moe.utils import (\n     _resize_cache, moe_kernel_quantize_input)\n from vllm.model_executor.layers.quantization.utils.mxfp4_utils import (\n@@ -1606,8 +1606,7 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         return True\n \n     def finalize_weight_and_reduce_impl(self) -> mk.TopKWeightAndReduce:\n-        # Let PrepareAndFinalize::finalize() decide the impl.\n-        return TopKWeightAndReduceDelegate()\n+        return TopKWeightAndReduceNoOP()\n \n     def workspace_shapes(\n         self,\n@@ -1620,9 +1619,9 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         global_num_experts: int,\n         local_num_experts: int,\n     ) -> tuple[tuple[int, ...], tuple[int, ...], tuple[int, ...], torch.dtype]:\n-        workspace1 = (M, topk, max(N * 2, K))\n-        workspace2 = (M, topk, N)\n-        output = (M, topk, K)\n+        workspace1 = (M, topk, max(N // 2, K))\n+        workspace2 = (M, topk, max(N, K))\n+        output = (M, K)\n         return (workspace1, workspace2, output, a.dtype)\n \n     def apply(\n@@ -1631,6 +1630,7 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\n         w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         activation: str,\n         global_num_experts: int,\n@@ -1644,6 +1644,7 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         workspace13: torch.Tensor,\n         workspace2: torch.Tensor,\n         expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ):\n         # Check constraints.\n         if self.use_int4_w4a16:\n@@ -1696,37 +1697,39 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             raise ValueError(\n                 f\"Unsupported compute_type: {hidden_states.dtype}\")\n \n-        # We can reuse the memory between these because by the time we need\n-        # cache3, we're done with cache1\n-        intermediate_cache1 = _resize_cache(workspace13,\n+        # Note that the output tensor might be in workspace1\n+        intermediate_cache1 = _resize_cache(workspace2,\n                                             (num_tokens, top_k_num, N))\n-        intermediate_cache2 = _resize_cache(workspace2,\n+        intermediate_cache2 = _resize_cache(workspace13,\n                                             (num_tokens * top_k_num, N // 2))\n+        intermediate_cache3 = _resize_cache(workspace2,\n+                                            (num_tokens, top_k_num, K))\n \n         sorted_token_ids, expert_ids, num_tokens_post_padded = (\n             moe_align_block_size(topk_ids, config['BLOCK_SIZE_M'],\n                                  global_num_experts, expert_map))\n \n-        invoke_fused_moe_kernel(hidden_states,\n-                                w1,\n-                                intermediate_cache1,\n-                                a1q_scale,\n-                                w1_scale,\n-                                w1_zp,\n-                                None,\n-                                sorted_token_ids,\n-                                expert_ids,\n-                                num_tokens_post_padded,\n-                                False,\n-                                top_k_num,\n-                                config,\n-                                compute_type=compute_type,\n-                                use_fp8_w8a8=self.use_fp8_w8a8,\n-                                use_int8_w8a8=self.use_int8_w8a8,\n-                                use_int8_w8a16=self.use_int8_w8a16,\n-                                use_int4_w4a16=self.use_int4_w4a16,\n-                                per_channel_quant=self.per_act_token_quant,\n-                                block_shape=self.block_shape)\n+        invoke_fused_moe_kernel(\n+            hidden_states,\n+            w1,\n+            intermediate_cache1,\n+            a1q_scale,\n+            w1_scale,\n+            w1_zp,\n+            None,  # topk_weights\n+            sorted_token_ids,\n+            expert_ids,\n+            num_tokens_post_padded,\n+            False,  # mul_routed_weights\n+            top_k_num,\n+            config,\n+            compute_type=compute_type,\n+            use_fp8_w8a8=self.use_fp8_w8a8,\n+            use_int8_w8a8=self.use_int8_w8a8,\n+            use_int8_w8a16=self.use_int8_w8a16,\n+            use_int4_w4a16=self.use_int4_w4a16,\n+            per_channel_quant=self.per_act_token_quant,\n+            block_shape=self.block_shape)\n \n         self.activation(activation, intermediate_cache2,\n                         intermediate_cache1.view(-1, N))\n@@ -1739,15 +1742,15 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n \n         invoke_fused_moe_kernel(qintermediate_cache2,\n                                 w2,\n-                                output,\n+                                intermediate_cache3,\n                                 a2q_scale,\n                                 w2_scale,\n                                 w2_zp,\n-                                None,\n+                                topk_weights,\n                                 sorted_token_ids,\n                                 expert_ids,\n                                 num_tokens_post_padded,\n-                                False,\n+                                not apply_router_weight_on_input,\n                                 1,\n                                 config,\n                                 compute_type=compute_type,\n@@ -1758,6 +1761,8 @@ class TritonExperts(mk.FusedMoEPermuteExpertsUnpermute):\n                                 per_channel_quant=self.per_act_token_quant,\n                                 block_shape=self.block_shape)\n \n+        ops.moe_sum(intermediate_cache3, output)\n+\n \n def modular_triton_fused_moe(\n     use_fp8_w8a8: bool,\ndiff --git a/vllm/model_executor/layers/fused_moe/modular_kernel.py b/vllm/model_executor/layers/fused_moe/modular_kernel.py\nindex d0d8c7d6f..028eee241 100644\n--- a/vllm/model_executor/layers/fused_moe/modular_kernel.py\n+++ b/vllm/model_executor/layers/fused_moe/modular_kernel.py\n@@ -360,6 +360,7 @@ class FusedMoEPermuteExpertsUnpermute(ABC):\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\n         w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         activation: str,\n         global_num_experts: int,\n@@ -373,6 +374,7 @@ class FusedMoEPermuteExpertsUnpermute(ABC):\n         workspace13: torch.Tensor,\n         workspace2: torch.Tensor,\n         expert_tokens_meta: Optional[ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ):\n         \"\"\"\n         This function computes the intermediate result of a Mixture of Experts\n@@ -384,6 +386,8 @@ class FusedMoEPermuteExpertsUnpermute(ABC):\n           layer.\n         - w1 (torch.Tensor): The first set of expert weights.\n         - w2 (torch.Tensor): The second set of expert weights.\n+        - topk_weights: A map of row to expert weights. Some implementations\n+          choose to do weight application. \n         - topk_ids (torch.Tensor): A map of row to expert id.\n         - activation (str): The activation function to apply after the first\n           MoE layer.\n@@ -409,6 +413,9 @@ class FusedMoEPermuteExpertsUnpermute(ABC):\n           ExpertTokensMetadata object containing gpu/cpu tensors\n           as big as the number of local experts with the information about the\n           number of tokens assigned to each local expert.\n+        - apply_router_weight_on_input: True if router weights are already\n+          applied on the input. This is relevant if the implementation\n+          chooses to do weight application.\n         \"\"\"\n         raise NotImplementedError\n \n@@ -452,17 +459,21 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 f\"{fused_experts.__class__.__name__}.\"\n                 f\"{fused_experts.activation_formats[0]}\")\n \n-    def _do_fused_experts(\n-            self, fused_out: Optional[torch.Tensor], a1: torch.Tensor,\n-            a1q: torch.Tensor, w1: torch.Tensor, w2: torch.Tensor,\n-            topk_ids: torch.Tensor, activation: str, global_num_experts: int,\n-            local_num_experts: int, expert_map: Optional[torch.Tensor],\n-            w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor],\n-            w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor],\n-            a1q_scale: Optional[torch.Tensor],\n-            a2_scale: Optional[torch.Tensor],\n-            expert_tokens_meta: Optional[ExpertTokensMetadata]\n-    ) -> torch.Tensor:\n+    def _do_fused_experts(self, fused_out: Optional[torch.Tensor],\n+                          a1: torch.Tensor, a1q: torch.Tensor,\n+                          w1: torch.Tensor, w2: torch.Tensor,\n+                          topk_weights: torch.Tensor, topk_ids: torch.Tensor,\n+                          activation: str, global_num_experts: int,\n+                          local_num_experts: int,\n+                          expert_map: Optional[torch.Tensor],\n+                          w1_scale: Optional[torch.Tensor],\n+                          w2_scale: Optional[torch.Tensor],\n+                          w1_zp: Optional[torch.Tensor],\n+                          w2_zp: Optional[torch.Tensor],\n+                          a1q_scale: Optional[torch.Tensor],\n+                          a2_scale: Optional[torch.Tensor],\n+                          expert_tokens_meta: Optional[ExpertTokensMetadata],\n+                          apply_router_weight_on_input: bool) -> torch.Tensor:\n \n         _, M, N, K, top_k = _moe_problem_size(a1q, w1, w2, topk_ids)\n \n@@ -485,36 +496,49 @@ class FusedMoEModularKernel(torch.nn.Module):\n             # reuse workspace13 for the output\n             fused_out = _resize_cache(workspace13, fused_out_shape)\n \n-        self.fused_experts.apply(fused_out,\n-                                 a1q,\n-                                 w1,\n-                                 w2,\n-                                 topk_ids=topk_ids,\n-                                 activation=activation,\n-                                 global_num_experts=global_num_experts,\n-                                 expert_map=expert_map,\n-                                 w1_scale=w1_scale,\n-                                 w2_scale=w2_scale,\n-                                 w1_zp=w1_zp,\n-                                 w2_zp=w2_zp,\n-                                 a1q_scale=a1q_scale,\n-                                 a2_scale=a2_scale,\n-                                 workspace13=workspace13,\n-                                 workspace2=workspace2,\n-                                 expert_tokens_meta=expert_tokens_meta)\n+        self.fused_experts.apply(\n+            fused_out,\n+            a1q,\n+            w1,\n+            w2,\n+            topk_weights=topk_weights,\n+            topk_ids=topk_ids,\n+            activation=activation,\n+            global_num_experts=global_num_experts,\n+            expert_map=expert_map,\n+            w1_scale=w1_scale,\n+            w2_scale=w2_scale,\n+            w1_zp=w1_zp,\n+            w2_zp=w2_zp,\n+            a1q_scale=a1q_scale,\n+            a2_scale=a2_scale,\n+            workspace13=workspace13,\n+            workspace2=workspace2,\n+            expert_tokens_meta=expert_tokens_meta,\n+            apply_router_weight_on_input=apply_router_weight_on_input)\n \n         return fused_out\n \n     def _maybe_chunk_fused_experts(\n-            self, a1: torch.Tensor, a1q: torch.Tensor, w1: torch.Tensor,\n-            w2: torch.Tensor, topk_ids: torch.Tensor, activation: str,\n-            global_num_experts: int, local_num_experts: int,\n-            expert_map: Optional[torch.Tensor],\n-            w1_scale: Optional[torch.Tensor], w2_scale: Optional[torch.Tensor],\n-            w1_zp: Optional[torch.Tensor], w2_zp: Optional[torch.Tensor],\n-            a1q_scale: Optional[torch.Tensor],\n-            a2_scale: Optional[torch.Tensor],\n-            expert_tokens_meta: Optional[ExpertTokensMetadata]\n+        self,\n+        a1: torch.Tensor,\n+        a1q: torch.Tensor,\n+        w1: torch.Tensor,\n+        w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n+        topk_ids: torch.Tensor,\n+        activation: str,\n+        global_num_experts: int,\n+        local_num_experts: int,\n+        expert_map: Optional[torch.Tensor],\n+        w1_scale: Optional[torch.Tensor],\n+        w2_scale: Optional[torch.Tensor],\n+        w1_zp: Optional[torch.Tensor],\n+        w2_zp: Optional[torch.Tensor],\n+        a1q_scale: Optional[torch.Tensor],\n+        a2_scale: Optional[torch.Tensor],\n+        expert_tokens_meta: Optional[ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ) -> torch.Tensor:\n \n         _, M, N, K, top_k = _moe_problem_size(a1q, w1, w2, topk_ids)\n@@ -529,6 +553,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 a1q=a1q,\n                 w1=w1,\n                 w2=w2,\n+                topk_weights=topk_weights,\n                 topk_ids=topk_ids,\n                 activation=activation,\n                 global_num_experts=global_num_experts,\n@@ -540,7 +565,8 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 w2_zp=w2_zp,\n                 a1q_scale=a1q_scale,\n                 a2_scale=a2_scale,\n-                expert_tokens_meta=expert_tokens_meta)\n+                expert_tokens_meta=expert_tokens_meta,\n+                apply_router_weight_on_input=apply_router_weight_on_input)\n \n         # Chunking required case\n         assert num_chunks > 1\n@@ -557,11 +583,12 @@ class FusedMoEModularKernel(torch.nn.Module):\n         def slice_input_tensors(\n             chunk_idx: int\n         ) -> tuple[torch.Tensor, Optional[torch.Tensor],\n-                   Optional[torch.Tensor], torch.Tensor]:\n+                   Optional[torch.Tensor], torch.Tensor, torch.Tensor]:\n             s = chunk_idx * CHUNK_SIZE\n             e = min(s + CHUNK_SIZE, M)\n             return (a1q[s:e], _chunk_scales(a1q_scale, s, e),\n-                    _chunk_scales(a2_scale, s, e), topk_ids[s:e])\n+                    _chunk_scales(a2_scale, s,\n+                                  e), topk_ids[s:e], topk_weights[s:e])\n \n         def slice_output_tensor(chunk_idx: int) -> torch.Tensor:\n             assert fused_out.size(0) % M == 0, (\n@@ -594,7 +621,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 expert_num_tokens_cpu=c_expert_num_tokens_cpu)\n \n         for chunk_idx in range(num_chunks):\n-            c_a1q, c_a1q_scale, c_a2_scale, c_topk_ids = (\n+            c_a1q, c_a1q_scale, c_a2_scale, c_topk_ids, c_topk_weights = (\n                 slice_input_tensors(chunk_idx))\n \n             c_expert_tokens_meta = None\n@@ -603,23 +630,26 @@ class FusedMoEModularKernel(torch.nn.Module):\n                     expert_tokens_meta, c_topk_ids, local_num_experts,\n                     expert_map)\n \n-            self._do_fused_experts(fused_out=slice_output_tensor(chunk_idx),\n-                                   a1=a1,\n-                                   a1q=c_a1q,\n-                                   w1=w1,\n-                                   w2=w2,\n-                                   topk_ids=c_topk_ids,\n-                                   activation=activation,\n-                                   global_num_experts=global_num_experts,\n-                                   local_num_experts=local_num_experts,\n-                                   expert_map=expert_map,\n-                                   w1_scale=w1_scale,\n-                                   w2_scale=w2_scale,\n-                                   w1_zp=w1_zp,\n-                                   w2_zp=w2_zp,\n-                                   a1q_scale=c_a1q_scale,\n-                                   a2_scale=c_a2_scale,\n-                                   expert_tokens_meta=c_expert_tokens_meta)\n+            self._do_fused_experts(\n+                fused_out=slice_output_tensor(chunk_idx),\n+                a1=a1,\n+                a1q=c_a1q,\n+                w1=w1,\n+                w2=w2,\n+                topk_weights=c_topk_weights,\n+                topk_ids=c_topk_ids,\n+                activation=activation,\n+                global_num_experts=global_num_experts,\n+                local_num_experts=local_num_experts,\n+                expert_map=expert_map,\n+                w1_scale=w1_scale,\n+                w2_scale=w2_scale,\n+                w1_zp=w1_zp,\n+                w2_zp=w2_zp,\n+                a1q_scale=c_a1q_scale,\n+                a2_scale=c_a2_scale,\n+                expert_tokens_meta=c_expert_tokens_meta,\n+                apply_router_weight_on_input=apply_router_weight_on_input)\n \n         return fused_out\n \n@@ -719,6 +749,7 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 a1q=a1q,\n                 w1=w1,\n                 w2=w2,\n+                topk_weights=topk_weights,\n                 topk_ids=topk_ids,\n                 activation=activation,\n                 global_num_experts=global_num_experts,\n@@ -730,7 +761,8 @@ class FusedMoEModularKernel(torch.nn.Module):\n                 w2_zp=w2_zp,\n                 a1q_scale=a1q_scale,\n                 a2_scale=a2_scale,\n-                expert_tokens_meta=expert_tokens_meta)\n+                expert_tokens_meta=expert_tokens_meta,\n+                apply_router_weight_on_input=apply_router_weight_on_input)\n \n         self.prepare_finalize.finalize(\n             output, fused_out, topk_weights, topk_ids,\ndiff --git a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\nindex 9a5315b8b..fb398eec1 100644\n--- a/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n+++ b/vllm/model_executor/layers/fused_moe/topk_weight_and_reduce.py\n@@ -48,11 +48,18 @@ class TopKWeightAndReduceNoOP(mk.TopKWeightAndReduce):\n               fused_expert_output: torch.Tensor, topk_weights: torch.Tensor,\n               topk_ids: torch.Tensor,\n               apply_router_weight_on_input: bool) -> torch.Tensor:\n-        # Relax this if an explicit copy is necessary. Note that,\n-        # if a copy is employed we have to make sure that the\n-        # tensors don't overlap\n-        assert output is None\n-        return fused_expert_output\n+        # Weight application and reduction operations are already done.\n+        if output is None:\n+            return fused_expert_output\n+\n+        # MoEPrepareAndFinalizeNoEP needs the output to be in the `output`\n+        # tensor.\n+        assert output.size() == fused_expert_output.size(), (\n+            \"output shape is expected to match the fused_expert_output shape. \"\n+            f\"But got output={output.size()}, \"\n+            f\"used_expert_output={fused_expert_output.size()}\")\n+        output.copy_(fused_expert_output, non_blocking=True)\n+        return output\n \n \n class TopKWeightAndReduceContiguous(mk.TopKWeightAndReduce):\ndiff --git a/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py\nindex fefe74cc4..2f35c19b7 100644\n--- a/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py\n@@ -122,6 +122,7 @@ class TritonOrDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         hidden_states: torch.Tensor,\n         w1: torch.Tensor,\n         w2: torch.Tensor,\n+        topk_weights: torch.Tensor,\n         topk_ids: torch.Tensor,\n         activation: str,\n         global_num_experts: int,\n@@ -135,6 +136,7 @@ class TritonOrDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n         workspace13: torch.Tensor,\n         workspace2: torch.Tensor,\n         expert_tokens_meta: Optional[mk.ExpertTokensMetadata],\n+        apply_router_weight_on_input: bool,\n     ):\n         use_deep_gemm = (self.allow_deep_gemm\n                          and (_valid_deep_gemm(hidden_states, w1, w2)\n@@ -148,6 +150,7 @@ class TritonOrDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             hidden_states,\n             w1,\n             w2,\n+            topk_weights,\n             topk_ids,\n             activation,\n             global_num_experts,\n@@ -161,4 +164,5 @@ class TritonOrDeepGemmExperts(mk.FusedMoEPermuteExpertsUnpermute):\n             workspace13,\n             workspace2,\n             expert_tokens_meta,\n+            apply_router_weight_on_input,\n         )",
  "apis": [
    "FusedMoEModularKernel.forward",
    "FusedMoEPermuteExpertsUnpermute.apply",
    "DeepGemmExperts.apply",
    "TritonExperts.apply",
    "TopKWeightAndReduceNoOP.apply"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/modular_kernel.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/triton_deep_gemm_moe.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit makes non-trivial modifications to core source files (non-test code) by updating the internal Mixture of Experts (MoE) kernel logic. It adds new parameters (topk_weights and apply_router_weight_on_input) and changes workspace sizing and tensor indexing to incorporate weight-and-reduce operations directly inside high-performance components like TritonExperts and DeepGemmExperts. These modifications target a more efficient data handling strategy and kernel operation, which are likely aimed at optimizing performance on CPU workloads. The changes are not superficial refactoring or documentation fixes but adjustments to the computational kernels that are integral to performance.",
  "llm_api_reason": "This commit modifies several methods responsible for executing the fused MoE kernel. In particular, it adds new parameters (topk_weights and apply_router_weight_on_input) to the method signatures of various apply functions in implementations of the abstract API FusedMoEPermuteExpertsUnpermute, including those in DeepGemmExperts, TritonExperts, CutlassExpertsFp8, BatchedDeepGemmExperts, BatchedTritonOrDeepGemmExperts, and even in the FusedMoEModularKernel‚Äôs internal methods. It also updates the TopKWeightAndReduce implementations accordingly. These changes affect the downstream behavior of the weight application and reduction step in the MoE inference pipeline."
}