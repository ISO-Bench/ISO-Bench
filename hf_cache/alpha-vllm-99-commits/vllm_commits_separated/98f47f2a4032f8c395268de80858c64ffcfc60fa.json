{
  "commit_hash": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
  "pr_url": "https://github.com/vllm-project/vllm/pull/10733",
  "pr_date": "2024-11-28",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Nov 28, 2024 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . With piece-wise CUDA graphs, we have to make sure that the attention custom op causes minimal CPU overheads. This PR made a few changes to optimize the CPU overheads in the FlashAttention custom op: We directly use torch.ops.vllm_flash_attn_c.varlen_fwd rather than flash_attn_varlen_func , since FlashAttnFunc which inherits torch.autograd.Function causes unnecessary overheads. We move the reshapes and shape check logics to outside of the custom op, so that they can be done at the CUDA graph capture time. Results of python benchmarks/benchmark_latency.py (opt-125m) on a single H100 GPU: V1 main: 227 ms V1 this PR: 192 ms V0 + 8-step: 130 ms Next step: further reduce the unnecessary CPU ops inside the FlashAttention op. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸ‘ 1 mgoin reacted with thumbs up emoji All reactions ðŸ‘ 1 reaction WoosukKwon requested review from robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners November 28, 2024 04:05 Copy link github-actions bot commented Nov 28, 2024 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Nov 28, 2024 Copy link Member youkaichao commented Nov 28, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . well, I think I forgot to update the v1 flash attention file, after #10558 , you don't need the torch.ops.vllm.unified_v1_flash_attention call. nvm All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . youkaichao reviewed Nov 28, 2024 View reviewed changes vllm/v1/attention/backends/flash_attn.py Outdated @@ -203,23 +209,31 @@ def unified_v1_flash_attention( v_scale, ) attn_output = flash_attn_varlen_func( Copy link Member youkaichao Nov 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment can you also update the corresponding v0 code? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tlrmchlsmth approved these changes Nov 28, 2024 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looking at profile results on #9856 , this saves about 60Âµs off of the CPU time spent in each flash attention call (approx 300Âµs -> 240Âµs) Thanks! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸš€ 2 mgoin and WoosukKwon reacted with rocket emoji All reactions ðŸš€ 2 reactions mgoin approved these changes Nov 28, 2024 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM with Kaichao's comment, thanks for quickly improving this. The failing test is due to neuralmagic/Phi-3-medium-128k-instruct-quantized.w4a16 and unrelated Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Re â€¦ 456980b Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon force-pushed the v1-flash-opt branch\n    from e4f8b06 to 456980b Compare November 28, 2024 16:45 Copy link Collaborator Author WoosukKwon commented Nov 28, 2024 @youkaichao @mgoin As we merged vllm-project/flash-attention#30 , we don't have to directly use torch.ops.vllm_flash_attn_c.varlen_fwd . We can just use flash_attn_varlen_func as we currently do. Both V0 and V1 already gets the benefits after vllm-project/flash-attention#30 . ðŸ‘€ 2 mgoin and youkaichao reacted with eyes emoji All reactions ðŸ‘€ 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author WoosukKwon commented Nov 28, 2024 One weird phenomenon I found is that V1 has a spike in latency: Avg latency: 0.20093455887205589 seconds\n10% percentile latency: 0.1931818482640665 seconds\n25% percentile latency: 0.19354040725738741 seconds\n50% percentile latency: 0.19391279752017 seconds\n75% percentile latency: 0.19426249974640086 seconds\n90% percentile latency: 0.1961068181961309 seconds\n99% percentile latency: 0.3368887884780999 seconds This is highly reproducible on my dev machine. Can this be because of Python gc or something like that? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details WoosukKwon merged commit 98f47f2 into main Nov 28, 2024 15 of 18 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the v1-flash-opt branch November 28, 2024 17:01 Copy link Collaborator robertgshaw2-redhat commented Nov 29, 2024 One weird phenomenon I found is that V1 has a spike in latency: Avg latency: 0.20093455887205589 seconds\n10% percentile latency: 0.1931818482640665 seconds\n25% percentile latency: 0.19354040725738741 seconds\n50% percentile latency: 0.19391279752017 seconds\n75% percentile latency: 0.19426249974640086 seconds\n90% percentile latency: 0.1961068181961309 seconds\n99% percentile latency: 0.3368887884780999 seconds This is highly reproducible on my dev machine. Can this be because of Python gc or something like that? Itâ€™s probably the prefix caching â€¦ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator comaniac commented Nov 29, 2024 Hmm but benchmark_latency.py does sample each prompts separately: https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py#L36 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator comaniac commented Nov 29, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Hmm but benchmark_latency.py does sample each prompts separately: https://github.com/vllm-project/vllm/blob/main/benchmarks/benchmark_latency.py#L36 Just found that it has a warmup phase. It's still possible due to prefix caching if all prompts are cached then. Suggest to explicitly disable prefix caching to double check. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author WoosukKwon commented Dec 1, 2024 @comaniac @robertgshaw2-neuralmagic You're right. The latency becomes stable when prefix caching is turned off. Avg latency: 0.1945609479948568 seconds\n10% percentile latency: 0.19310778125654907 seconds\n25% percentile latency: 0.19390572598786093 seconds\n50% percentile latency: 0.19475348049309105 seconds\n75% percentile latency: 0.195164829317946 seconds\n90% percentile latency: 0.19570096801035106 seconds\n99% percentile latency: 0.1962820820847992 seconds All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . afeldman-nm pushed a commit\n        to neuralmagic/vllm\n      that referenced\n      this pull request Dec 2, 2024 [V1] Optimize the CPU overheads in FlashAttention custom op ( vllm-proâ€¦ â€¦ bc6637c â€¦ject#10733 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Andrew Feldman <afeldman@neuralmagic.com> sleepwalker2017 pushed a commit\n        to sleepwalker2017/vllm\n      that referenced\n      this pull request Dec 13, 2024 [V1] Optimize the CPU overheads in FlashAttention custom op ( vllm-proâ€¦ â€¦ 17b4a20 â€¦ject#10733 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> anko-intel pushed a commit\n        to HabanaAI/vllm-fork\n      that referenced\n      this pull request Feb 12, 2025 [V1] Optimize the CPU overheads in FlashAttention custom op ( vllm-proâ€¦ â€¦ 34de378 â€¦ject#10733 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:41",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:47:41",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmarks/benchmark_latency.py",
  "commit_subject": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)",
  "commit_message": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2024-11-28T09:01:02-08:00",
  "files_changed": [
    "vllm/v1/attention/backends/flash_attn.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 4,
    "num_edited_lines": 17,
    "num_non_test_edited_lines": 17,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 5f8535eaa..e618edf7d 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -135,6 +135,13 @@ class FlashAttentionImpl(AttentionImpl):\n         assert k_scale == 1.0 and v_scale == 1.0, (\n             \"key/v_scale is not supported in FlashAttention.\")\n \n+        # Reshape the query, key, and value tensors.\n+        # NOTE(woosuk): We do this outside the custom op to minimize the CPU\n+        # overheads from the non-CUDA-graph regions.\n+        query = query.view(-1, self.num_heads, self.head_size)\n+        key = key.view(-1, self.num_kv_heads, self.head_size)\n+        value = value.view(-1, self.num_kv_heads, self.head_size)\n+\n         output = torch.empty_like(query)\n         torch.ops.vllm.unified_v1_flash_attention(\n             output,\n@@ -153,7 +160,7 @@ class FlashAttentionImpl(AttentionImpl):\n             self.alibi_slopes,\n             self.logits_soft_cap,\n         )\n-        return output\n+        return output.view(-1, self.num_heads * self.head_size)\n \n \n def unified_v1_flash_attention(\n@@ -184,11 +191,6 @@ def unified_v1_flash_attention(\n     attn_metadata: FlashAttentionMetadata = current_metadata\n     num_actual_tokens = attn_metadata.num_actual_tokens\n \n-    # Reshape the query, key, and value tensors.\n-    query = query.view(-1, num_heads, head_size)\n-    key = key.view(-1, num_kv_heads, head_size)\n-    value = value.view(-1, num_kv_heads, head_size)\n-\n     # Reshape the input keys and values and store them in the cache.\n     key_cache = kv_cache[0]\n     value_cache = kv_cache[1]\n@@ -218,8 +220,7 @@ def unified_v1_flash_attention(\n         block_table=attn_metadata.block_table,\n         softcap=logits_soft_cap,\n     )\n-    attn_output = attn_output.view(num_actual_tokens, -1)\n-    # TODO(woosuk): Optimize this.\n+    # TODO(woosuk): Remove this unnecessary copy.\n     output[:num_actual_tokens].copy_(attn_output)",
  "apis": [
    "vllm.v1.attention.backends.flash_attn.FlashAttentionImpl.forward",
    "vllm.v1.attention.backends.flash_attn.unified_v1_flash_attention"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/flash_attn.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/flash_attn.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/_custom_ops.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/csrc/torch_bindings.cpp",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/csrc/cpu/torch_bindings.cpp",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/csrc/rocm/torch_bindings.cpp",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/csrc/moe/torch_bindings.cpp"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file (flash_attn.py) and contains non-trivial code changesâ€”specifically the reshaping of query, key, and value tensors outside the custom opâ€”to reduce CPU overhead in the FlashAttention implementation. The commit message explicitly refers to CPU overhead optimization, and the changes aim to minimize unnecessary CPU operations (e.g., rearranging tensor views), which directly impacts performance. These modifications are intended to improve runtime efficiency rather than fix bugs, perform refactoring, or add new features. Overall, these changes align with performance-related optimization criteria for CPU workloads.",
  "llm_api_reason": "The commit moves tensor reshaping for the query, key, and value out of the custom op in the FlashAttentionImpl.forward method to reduce CPU overhead, and it adjusts the final returned tensor's shape. Additionally, the internal unified_v1_flash_attention function no longer performs its own reshaping, indicating a change in how tensor dimensions are managed in the attention computation."
}