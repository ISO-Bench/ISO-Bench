{
  "commit_hash": "296f927f2493908984707354e3cc5d7b2e41650b",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14857",
  "pr_date": "2025-03-21",
  "timeline_text": "Copy link Contributor cyang49 commented Mar 15, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This is a re-attempt to fix mamba2's excessive memory copies. The previous solution failed due to difference in semantics when indexing tensor with tensor.  This new solution directly utilizes indexing with state_indices_tensor to create tensor views and simplified the code without over-engineering. FIX #14778 The results from benchmark_serving on single H100-80GB GPU (Actually I found high variance of throughput numbers from consecutive tests of the same code base when using this benchmark. Not sure if this is meaningful to report? @njhill @tlrmchlsmth ) Benchmark serving main ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  291.76    \nTotal input tokens:                      215201    \nTotal generated tokens:                  198343    \nRequest throughput (req/s):              3.43      \nOutput token throughput (tok/s):         679.81    \nTotal Token throughput (tok/s):          1417.39   \n---------------Time to First Token----------------\nMean TTFT (ms):                          108636.82 \nMedian TTFT (ms):                        96115.48  \nP99 TTFT (ms):                           276325.38 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          409.48    \nMedian TPOT (ms):                        427.24    \nP99 TPOT (ms):                           655.84    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           352.50    \nMedian ITL (ms):                         606.12    \nP99 ITL (ms):                            969.64    \n================================================== Benchmark serving with this PR ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  252.11    \nTotal input tokens:                      215201    \nTotal generated tokens:                  198343    \nRequest throughput (req/s):              3.97      \nOutput token throughput (tok/s):         786.73    \nTotal Token throughput (tok/s):          1640.33   \n---------------Time to First Token----------------\nMean TTFT (ms):                          97161.98  \nMedian TTFT (ms):                        94360.96  \nP99 TTFT (ms):                           237572.12 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          355.17    \nMedian TPOT (ms):                        381.49    \nP99 TPOT (ms):                           548.15    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           306.68    \nMedian ITL (ms):                         501.06    \nP99 ITL (ms):                            750.59    \n================================================== lm-eval main |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  | 0.22|Â±  |0.0416|\n|     |       |strict-match    |     5|exact_match|â†‘  | 0.32|Â±  |0.0469| lm-eval with this PR |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  | 0.22|Â±  |0.0416|\n|     |       |strict-match    |     5|exact_match|â†‘  | 0.32|Â±  |0.0469| cc @fabianlim @yury-tokpanov Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸ‘ 2 fabianlim and yury-tokpanov reacted with thumbs up emoji All reactions ðŸ‘ 2 reactions Copy link github-actions bot commented Mar 15, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 15, 2025 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Experiment to test semantics of calling zero_() on indexed tensor >>> import torch\n>>> x = torch.randperm(10)\n>>> y = torch.randperm(10)\n>>> x\ntensor([5, 4, 6, 2, 0, 3, 8, 9, 7, 1])\n>>> y\ntensor([6, 8, 3, 2, 7, 5, 9, 4, 1, 0])\n>>> x[y<5].zero_()\ntensor([0, 0, 0, 0, 0])\n>>> x\ntensor([5, 4, 6, 2, 0, 3, 8, 9, 7, 1])\n>>> x[y<5] = 0\n>>> x\ntensor([5, 4, 0, 0, 0, 3, 8, 0, 0, 0]) From this experiment, It seems that zero_() wouldn't give the right results? The zero init code should be the following instead? This would be index_put_ if has_initial_states is not None and torch.any(\n                    has_initial_states):\n                zero_init_indices = mamba_cache_params.state_indices_tensor[\n                    ~has_initial_states]\n                mamba_cache_params.ssm_state[zero_init_indices] = 0\n                initial_states = mamba_cache_params.ssm_state[\n                    mamba_cache_params.state_indices_tensor] Another reference states: The copy is performed right away â€“ but note the exception to this (mentioned in the quoted documentation) when you are assigning to an indexed tensor. lm-eval results with this change |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  | 0.22|Â±  |0.0416|\n|     |       |strict-match    |     5|exact_match|â†‘  | 0.32|Â±  |0.0469| benchmark results with this change ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  250.68    \nTotal input tokens:                      215201    \nTotal generated tokens:                  198343    \nRequest throughput (req/s):              3.99      \nOutput token throughput (tok/s):         791.23    \nTotal Token throughput (tok/s):          1649.71   \n---------------Time to First Token----------------\nMean TTFT (ms):                          95232.94  \nMedian TTFT (ms):                        85040.17  \nP99 TTFT (ms):                           231833.63 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          337.99    \nMedian TPOT (ms):                        351.17    \nP99 TPOT (ms):                           522.21    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           292.12    \nMedian ITL (ms):                         494.48    \nP99 ITL (ms):                            730.20    \n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . njhill approved these changes Mar 17, 2025 View reviewed changes Copy link Member njhill left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, thanks @cyang49 ! I've run into similar issue with in-place updates in the past Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸš€ 1 cyang49 reacted with rocket emoji All reactions ðŸš€ 1 reaction Copy link Contributor yury-tokpanov commented Mar 18, 2025 how are you deploying your model's server? Seems like Bamba config lacks max model length, so vllm picks up something really big and enables chunked prefill, which is slow. Just setting --max-model-len 4096 is enough to disable chunked prefill: vllm serve ibm-ai-platform/Bamba-9B --dtype float16 --gpu-memory-utilization 0.9 --max-model-len 4096 . Without chunked prefill, I'm getting much better and more stable numbers for serving metrics. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth approved these changes Mar 20, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM and confirmed the gsm8k results on my end this time Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tlrmchlsmth added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 20, 2025 tlrmchlsmth enabled auto-merge (squash) March 20, 2025 15:15 Copy link Contributor Author cyang49 commented Mar 20, 2025 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Build failed.. will try rebasing main [2025-03-20T15:54:50Z] FAILED tool_use/test_chat_completions.py::test_chat_completion_with_tools[granite-3.0-8b] - AssertionError: assert 'Of course! H...p everything!' == 'Of course! H...p everything!'\n[2025-03-20T15:54:50Z]\n[2025-03-20T15:54:50Z]   - Of course! Here's a joke for you: Why don't scientists trust atoms? Because they make up everything!\n[2025-03-20T15:54:50Z]   + Of course! Here's a joke for you:\n[2025-03-20T15:54:50Z]   +\n[2025-03-20T15:54:50Z]   + Why don't scientists trust atoms?\n[2025-03-20T15:54:50Z]   +\n[2025-03-20T15:54:50Z]   + Because they make up everything! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cyang49 added 2 commits March 20, 2025 16:10 simplify and optimize mamba2 code that caused flurry of memcpys â€¦ d0a7427 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> Use assignment instead of zero_ on indexed ssm_state â€¦ 2807c52 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> auto-merge was automatically disabled March 20, 2025 20:10 Head branch was pushed to by a user without write access cyang49 force-pushed the pr_mamba2_mem_fix branch\n    from 0f41a64 to 2807c52 Compare March 20, 2025 20:10 tlrmchlsmth enabled auto-merge (squash) March 20, 2025 20:32 Copy link Collaborator tlrmchlsmth commented Mar 20, 2025 Ok! If it fails again, let's take a look at the failures and force merge if unrelated (feel free to ping me on this @cyang49 ) All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 20, 2025 @tlrmchlsmth failed again on V1 test of Qwen.. :( All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details vllm-bot merged commit 296f927 into vllm-project : main Mar 21, 2025 32 of 35 checks passed Uh oh! There was an error while loading. Please reload this page . cyang49 deleted the pr_mamba2_mem_fix branch March 24, 2025 22:00 erictang000 pushed a commit\n        to erictang000/vllm\n      that referenced\n      this pull request Mar 25, 2025 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecâ€¦ â€¦ 93fab96 â€¦essary Memory Copies ( vllm-project#14857 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecâ€¦ â€¦ 0dbd3df â€¦essary Memory Copies ( vllm-project#14857 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecâ€¦ â€¦ 252cff0 â€¦essary Memory Copies ( vllm-project#14857 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecâ€¦ â€¦ 97055ac â€¦essary Memory Copies ( vllm-project#14857 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:42",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm-eval, lm-eval, lm-eval | PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:42",
  "models": [
    "ibm-ai-platform/Bamba-9B"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=ibm-ai-platform/Bamba-9B,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14857)",
  "commit_message": "[Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14857)\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>",
  "commit_date": "2025-03-20T19:21:08-07:00",
  "files_changed": [
    "vllm/model_executor/layers/mamba/mamba_mixer2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 13,
    "num_non_test_edited_lines": 13,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex fec6d6112..d7a45bc51 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -470,10 +470,11 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-            if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+            if has_initial_states is not None and torch.any(\n+                    has_initial_states):\n+                zero_init_indices = mamba_cache_params.state_indices_tensor[\n+                    ~has_initial_states]\n+                mamba_cache_params.ssm_state[zero_init_indices] = 0\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -499,8 +500,8 @@ class MambaMixer2(CustomOp):\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            mamba_cache_params.ssm_state[\n+                mamba_cache_params.state_indices_tensor] = varlen_state\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)",
  "apis": [
    "MambaMixer2.forward_cuda",
    "mamba_mixer2"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba_cache.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test file (mamba_mixer2.py) and changes the implementation of state initialization and update by replacing Python loops with tensor indexing assignments, likely reducing unnecessary memory copy operations. The commit message also hints at performance improvements (\"Fixing Flurry of Unnecessary Memory Copies\"). These changes affect the underlying operations (memory copy, state updates) that can impact the performance of high-level APIs. The modifications are non-trivial code changes intended to optimize runtime performance on CPU, not bug fixes or simple refactoring. Therefore, the commit meets the criteria for being performance/optimization related.",
  "llm_api_reason": "The commit refactors parts of the prefill branch in the MambaMixer2 custom op implementation to avoid iterative memory copy operations by replacing Python loops with vectorized tensor slicing assignments. This change affects the internal behavior of the forward_cuda method of the MambaMixer2 class and the registered mamba_mixer2 op function that dispatches to it."
}