{
  "commit_hash": "67da5720d4ed2aa1f615ec812031f4f3753b3f62",
  "pr_url": "https://github.com/vllm-project/vllm/pull/17973",
  "pr_date": "2025-05-16",
  "timeline_text": "Copy link Contributor vadiklyutiy commented May 12, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Description of Problem In Qwen2.5-VL rotary position embedding constant tensors creates in the beginning of model's forward . Before this PR there were a mix of CPU and GPU tensors and (small) data pieces transferred back and forward to device. Profile looked like below pink tmp is begining of Qwen2_5_VisionTransformer.forward() before main transformer started. Solution This PR: makes a refactoring and put all tensors necessary to create constant mrope data to CPU (similar to how it works for mrope for language (part of) models) regroup calculation by grid_thw line and cache results Now profile looks like below Performance results Run Qwen2.5-3B-VL on H100 with following command line vllm serve Qwen/Qwen2.5-VL-3B-Instruct --disable-log-requests --max-num-seqs 1024  --block-size 16 --max-num-batched-tokens 2048 Construction of constant mrope tensors itself speeded up 5+ times . E2E measured with https://github.com/CentML/flexible-inference-bench fib benchmark -rps 50 --input-token-distribution uniform 250 300 --output-token-distribution uniform 150 250 --num-of-imgs-per-req 1 --img-ratios-per-req 512x512 -n 1000 --base-url http://localhost:8000 --endpoint v1/chat/completions --backend openai-chat The above runs 1000 requests, 50 reqs/sec, every request has one 512x512 image. Measured average reqs/s. Made 11 runs and took median Before: 25.99 reqs/s After: 26.63 req/s Speed up: 2.46% Correctness Run lm_eval with chartqa and mmmu lm_eval --model vllm-vlm --model_args \"pretrained=Qwen/Qwen2.5-VL-3B-Instruct,model=Qwen/Qwen2.5-VL-3B-Instruct\"  --tasks mmmu_val,chartqa  --batch_size 32 --apply_chat_template Before |                 Tasks                 |Version|Filter|n-shot|     Metric      |   |Value |   |Stderr|\n|---------------------------------------|------:|------|-----:|-----------------|---|-----:|---|-----:|\n|chartqa                                |      0|none  |     0|anywhere_accuracy|‚Üë  |0.8072|¬±  |0.0079|\n|                                       |       |none  |     0|exact_match      |‚Üë  |0.5712|¬±  |0.0099|\n|                                       |       |none  |     0|relaxed_accuracy |‚Üë  |0.8040|¬±  |0.0079|\n\n|             Groups             |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n|--------------------------------|------:|------|------|------|---|-----:|---|-----:|\n|mmmu_val                        |      0|none  |      |acc   |‚Üë  |0.4567|¬±  |0.0159|\n| - Art and Design               |      0|none  |      |acc   |‚Üë  |0.5583|¬±  |0.0437|\n| - Business                     |      0|none  |      |acc   |‚Üë  |0.3733|¬±  |0.0395|\n| - Health and Medicine          |      0|none  |      |acc   |‚Üë  |0.5267|¬±  |0.0406|\n| - Humanities and Social Science|      0|none  |      |acc   |‚Üë  |0.7000|¬±  |0.0412|\n| - Science                      |      0|none  |      |acc   |‚Üë  |0.3267|¬±  |0.0386|\n| - Tech and Engineering         |      0|none  |      |acc   |‚Üë  |0.3619|¬±  |0.0326| After |                 Tasks                 |Version|Filter|n-shot|     Metric      |   |Value |   |Stderr|\n|---------------------------------------|------:|------|-----:|-----------------|---|-----:|---|-----:|\n|chartqa                                |      0|none  |     0|anywhere_accuracy|‚Üë  |0.8032|¬±  |0.0080|\n|                                       |       |none  |     0|exact_match      |‚Üë  |0.5756|¬±  |0.0099|\n|                                       |       |none  |     0|relaxed_accuracy |‚Üë  |0.8016|¬±  |0.0080|\n\n|             Groups             |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n|--------------------------------|------:|------|------|------|---|-----:|---|-----:|\n|mmmu_val                        |      0|none  |      |acc   |‚Üë  |0.4544|¬±  |0.0159|\n| - Art and Design               |      0|none  |      |acc   |‚Üë  |0.5583|¬±  |0.0443|\n| - Business                     |      0|none  |      |acc   |‚Üë  |0.3733|¬±  |0.0395|\n| - Health and Medicine          |      0|none  |      |acc   |‚Üë  |0.5067|¬±  |0.0407|\n| - Humanities and Social Science|      0|none  |      |acc   |‚Üë  |0.7083|¬±  |0.0411|\n| - Science                      |      0|none  |      |acc   |‚Üë  |0.3267|¬±  |0.0386|\n| - Tech and Engineering         |      0|none  |      |acc   |‚Üë  |0.3619|¬±  |0.0327| Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Speed up Qwen2.5-VL model by speed up rotary position embedding const‚Ä¶ ‚Ä¶ 7eec475 ‚Ä¶ Tensors creation\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> Copy link github-actions bot commented May 12, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . pre-commit fixes ‚Ä¶ ab81b1d Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> simon-mo approved these changes May 14, 2025 View reviewed changes Copy link Collaborator simon-mo left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment thank you for the optimization, please run a mmmu or chartqa evaluation to verify the correctness of the changes. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 vadiklyutiy reacted with thumbs up emoji All reactions üëç 1 reaction vllm/model_executor/models/qwen2_5_vl.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/models/qwen2_5_vl.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . revome unnecessary coments ‚Ä¶ 5d0b6e2 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> Copy link Contributor Author vadiklyutiy commented May 15, 2025 thank you for the optimization, please run a mmmu or chartqa evaluation to verify the correctness of the changes. I added to description results of mmmu and chartqa \"before\" and \"after\" üëç 1 simon-mo reacted with thumbs up emoji üöÄ 1 simon-mo reacted with rocket emoji All reactions üëç 1 reaction üöÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . simon-mo enabled auto-merge (squash) May 15, 2025 01:10 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label May 15, 2025 WoosukKwon disabled auto-merge May 15, 2025 01:37 Copy link Collaborator WoosukKwon commented May 15, 2025 @imkero Could you please take a final look? I'm not sure if this overlaps with #14684 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link mergify bot commented May 15, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @vadiklyutiy . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label May 15, 2025 Merge branch 'main' into rope-const-creation-speedup 20808fa mergify bot removed\n  the needs-rebase label May 16, 2025 Copy link Collaborator WoosukKwon commented May 16, 2025 @vadiklyutiy QQ: Why does this PR change the accuracy (though the diff is small)? I thought the PR doesn't change the computation at all. Can we somehow strictly match the accuracy? I'm a bit careful about this because we've seen a few bugs regarding m-rope. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator simon-mo commented May 16, 2025 @WoosukKwon these tests are not deterministic due to temperature, I read values and apply the stderr; seems no change to accuracy to me. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor imkero commented May 16, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . The idea of this PR is similar to #14684 . And it is verified by both #14684 and this PR that such approach will gain some performance improvement. If the inference result slightly changed in this PR, maybe we should compare the generated m-rope pos seq and window_index seq  output with those generated by main branch. Also check if we are testing with greedy decoding. By the way I suggest that we can keep image_grid_thw and video_grid_thw in CPU all the time by modifying vllm/multimodal/inputs.py::MultiModalKwargs::as_kwargs (here vLLM move all mm data to device by default, and still needed to move them back to host later) @staticmethod\n  def as_kwargs(\n      batched_inputs: BatchedTensorInputs,\n      *,\n      device: torch.types.Device,\n  ) -> BatchedTensorInputs:\n      json_inputs = cast(JSONTree[torch.Tensor], batched_inputs) + # keep Qwen2/2.5-VL's image_grid_thw and video_grid_thw in cpu + image_grid_thw = None + video_grid_thw = None + if isinstance(json_inputs, dict): + image_grid_thw = json_inputs.pop(\"image_grid_thw\", None) + video_grid_thw = json_inputs.pop(\"video_grid_thw\", None) json_mapped = json_map_leaves(\n          lambda x: x.to(device, non_blocking=True),\n          json_inputs,\n      ) + if image_grid_thw is not None: + json_mapped[\"image_grid_thw\"] = image_grid_thw  # type: ignore + if video_grid_thw is not None: + json_mapped[\"video_grid_thw\"] = video_grid_thw  # type: ignore return cast(BatchedTensorInputs, json_mapped) All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator WoosukKwon commented May 16, 2025 @simon-mo @imkero Thanks for the explanation. Ok let's merge this PR for v0.9.0 and further improve it with @imkero 's suggestion All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details WoosukKwon merged commit 67da572 into vllm-project : main May 16, 2025 65 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author vadiklyutiy commented May 16, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @WoosukKwon As @simon-mo said lm_eval isn't deterministic. To dispel doubts in correctness I wrote the following test that compare \"before\" and \"after\" implementations. In test I took Qwen2_5_VisionTransformer before and after and copy to test. Clean both to calculate only rotary_pos_emb , window_index , cu_window_seqlens , and cu_seqlens . Test takes arbitrary grid_thw , run both version and compare results. Test accept following args --samples number of different grid to test --max-t max value of t --max-h max value of h --max-w max value of w --max-images - len(grid_thw) The following runs successfully passed: $ python test_qwen25_vl_transformer.py --mass-test --samples 10000 --max-t 50 --max-h 100 --max-w 100 --max-images 5 $python test_qwen25_vl_transformer.py --mass-test --samples 10000 --max-t 100 --max-h 250 --max-w 250 --max-images 10 Hope that resolved worries about correctness Test source import torch import torch . nn as nn import torch . nn . functional as F from functools import lru_cache import argparse import numpy as np import random import tqdm import sys class TestFailureException ( Exception ): \"\"\"Exception raised when the test results don't match between old and new implementations.\"\"\" pass class Qwen2_5_VisionRotaryEmbedding ( nn . Module ): def __init__ ( self , dim : int , theta : float = 10000.0 ) -> None : super (). __init__ () self . dim = dim self . theta = theta inv_freq = 1.0 / ( theta ** ( torch . arange ( 0 , dim , 2 , dtype = torch . float , device = 'cpu' ) / dim )) self . register_buffer ( \"inv_freq\" , inv_freq , persistent = False ) self . _seq_len_cached = 0 self . _freqs_cached = None def update_freqs_cache ( self , seqlen : int ) -> None : if seqlen > self . _seq_len_cached : seqlen *= 2 self . _seq_len_cached = seqlen self . inv_freq = 1.0 / ( self . theta ** ( torch . arange ( 0 , self . dim , 2 , dtype = torch . float , device = self . inv_freq . device ) / self . dim )) seq = torch . arange ( seqlen , device = self . inv_freq . device , dtype = self . inv_freq . dtype ) freqs = torch . outer ( seq , self . inv_freq ) self . _freqs_cached = freqs def forward ( self , seqlen : int ) -> torch . Tensor : self . update_freqs_cache ( seqlen ) return self . _freqs_cached [: seqlen ] class Qwen2_5_VisionTransformer_New ( nn . Module ): def __init__ ( self , hidden_size = 1152 , num_heads = 16 , window_size = 32 , patch_size = 14 , spatial_merge_size = 2 , fullatt_block_indexes = [ 0 , 1 , 2 , 3 , 8 , 9 , 10 , 11 , 16 , 17 , 18 , 19 , 24 , 25 , 26 , 27 ],\n    ) -> None : super (). __init__ () self . hidden_size = hidden_size self . num_heads = num_heads self . window_size = window_size self . patch_size = patch_size self . spatial_merge_size = spatial_merge_size self . fullatt_block_indexes = fullatt_block_indexes self . spatial_merge_unit = self . spatial_merge_size ** 2 head_dim = self . hidden_size // self . num_heads self . rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding ( head_dim // 2 ) @ property def dtype ( self ) -> torch . dtype : return torch . float32 @ property def device ( self ) -> torch . device : return torch . device ( 'cpu' ) def rotary_pos_emb_thw ( self , t , h , w ): hpos_ids = torch . arange ( h ). unsqueeze ( 1 ). expand ( - 1 , w ) wpos_ids = torch . arange ( w ). unsqueeze ( 0 ). expand ( h , - 1 ) hpos_ids = hpos_ids . reshape ( h // self . spatial_merge_size , self . spatial_merge_size , w // self . spatial_merge_size , self . spatial_merge_size ,\n        ). permute ( 0 , 2 , 1 , 3 ). flatten () wpos_ids = wpos_ids . reshape ( h // self . spatial_merge_size , self . spatial_merge_size , w // self . spatial_merge_size , self . spatial_merge_size ,\n        ). permute ( 0 , 2 , 1 , 3 ). flatten () pos_ids = torch . stack ([ hpos_ids , wpos_ids ], dim = - 1 ). repeat ( t , 1 ) max_size = max ( h , w ) rotary_pos_emb_full = self . rotary_pos_emb ( max_size ) rotary_pos_emb = rotary_pos_emb_full [ pos_ids ]. flatten ( 1 ) rotary_pos_emb = rotary_pos_emb . reshape ( rotary_pos_emb . shape [ 0 ] // self . spatial_merge_unit , self . spatial_merge_unit , - 1 ) return rotary_pos_emb def get_window_index_thw ( self , grid_t , grid_h , grid_w ): vit_merger_window_size = ( self . window_size // self . spatial_merge_size // self . patch_size ) llm_grid_h = grid_h // self . spatial_merge_size llm_grid_w = grid_w // self . spatial_merge_size index = torch . arange ( grid_t * llm_grid_h * llm_grid_w ). reshape ( grid_t , llm_grid_h , llm_grid_w ) pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size num_windows_h = ( llm_grid_h + pad_h ) // vit_merger_window_size num_windows_w = ( llm_grid_w + pad_w ) // vit_merger_window_size index_padded = F . pad ( index , ( 0 , pad_w , 0 , pad_h ), 'constant' , - 100 ) index_padded = index_padded . reshape ( grid_t , num_windows_h , vit_merger_window_size , num_windows_w , vit_merger_window_size ) index_padded = index_padded . permute ( 0 , 1 , 3 , 2 , 4 ). reshape ( grid_t , num_windows_h * num_windows_w , vit_merger_window_size , vit_merger_window_size ) seqlens = ( index_padded != - 100 ). sum ([ 2 , 3 ]). reshape ( - 1 ) index_padded = index_padded . reshape ( - 1 ) index_new = index_padded [ index_padded != - 100 ] cu_seqlens_tmp = seqlens . cumsum ( 0 ) * self . spatial_merge_unit cu_seqlens_tmp = cu_seqlens_tmp . to ( dtype = torch . int32 ) cu_seqlens_tmp = torch . unique_consecutive ( cu_seqlens_tmp ) return index_new , cu_seqlens_tmp @ lru_cache ( maxsize = 1024 ) # noqa: B019 def get_rope_by_thw ( self , t , h , w ): window_index_thw , cu_seqlens_window_thw = self . get_window_index_thw ( t , h , w ) rotary_pos_emb_thw = self . rotary_pos_emb_thw ( t , h , w ) rotary_pos_emb_thw = rotary_pos_emb_thw [ window_index_thw , :, :] rotary_pos_emb_thw = rotary_pos_emb_thw . flatten ( start_dim = 0 , end_dim = 1 ) cu_seqlens_thw = torch . repeat_interleave ( torch . tensor ([ h * w ], dtype = torch . int32 ), t ) return ( rotary_pos_emb_thw , window_index_thw , cu_seqlens_window_thw , cu_seqlens_thw ) def process_grid_thw ( self , grid_thw ): rotary_pos_emb = [] window_index = [] cu_window_seqlens = [ torch . tensor ([ 0 ], dtype = torch . int32 )] cu_seqlens = [] window_index_id = 0 cu_window_seqlens_last = 0 for t , h , w in grid_thw : t , h , w = int ( t ), int ( h ), int ( w ) llm_h = h // self . spatial_merge_size llm_w = w // self . spatial_merge_size ( rotary_pos_emb_thw , window_index_thw , cu_seqlens_window_thw , cu_seqlens_thw ,\n            ) = self . get_rope_by_thw ( t , h , w ) window_index . append ( window_index_thw + window_index_id ) window_index_id += ( t * llm_h * llm_w ) cu_seqlens_window_thw = ( cu_seqlens_window_thw + cu_window_seqlens_last ) cu_window_seqlens_last = cu_seqlens_window_thw [ - 1 ] cu_window_seqlens . append ( cu_seqlens_window_thw ) rotary_pos_emb . append ( rotary_pos_emb_thw ) cu_seqlens . append ( cu_seqlens_thw ) rotary_pos_emb = torch . cat ( rotary_pos_emb ) window_index = torch . cat ( window_index ) cu_window_seqlens = torch . cat ( cu_window_seqlens ) cu_window_seqlens = torch . unique_consecutive ( cu_window_seqlens ) cu_seqlens = torch . cat ( cu_seqlens ) cu_seqlens = torch . cumsum ( cu_seqlens , dim = 0 , dtype = torch . int32 ) cu_seqlens = F . pad ( cu_seqlens , ( 1 , 0 ), \"constant\" , 0 ) return rotary_pos_emb , window_index , cu_window_seqlens , cu_seqlens class Qwen2_5_VisionTransformer_Old ( nn . Module ): def __init__ ( self , hidden_size = 1152 , num_heads = 16 , window_size = 32 , patch_size = 14 , spatial_merge_size = 2 , fullatt_block_indexes = [ 0 , 1 , 2 , 3 , 8 , 9 , 10 , 11 , 16 , 17 , 18 , 19 , 24 , 25 , 26 , 27 ],\n    ) -> None : super (). __init__ () self . hidden_size = hidden_size self . num_heads = num_heads self . window_size = window_size self . patch_size = patch_size self . spatial_merge_size = spatial_merge_size self . fullatt_block_indexes = fullatt_block_indexes self . spatial_merge_unit = self . spatial_merge_size ** 2 head_dim = self . hidden_size // self . num_heads self . rotary_pos_emb = Qwen2_5_VisionRotaryEmbedding ( head_dim // 2 ) @ property def dtype ( self ) -> torch . dtype : return torch . float32 @ property def device ( self ) -> torch . device : return torch . device ( 'cpu' ) def rot_pos_emb ( self , grid_thw : torch . Tensor ) -> torch . Tensor : pos_ids = [] for t , h , w in grid_thw : hpos_ids = torch . arange ( h ). unsqueeze ( 1 ). expand ( - 1 , w ) wpos_ids = torch . arange ( w ). unsqueeze ( 0 ). expand ( h , - 1 ) hpos_ids = hpos_ids . reshape ( h // self . spatial_merge_size , self . spatial_merge_size , w // self . spatial_merge_size , self . spatial_merge_size ,\n            ). permute ( 0 , 2 , 1 , 3 ). flatten () wpos_ids = wpos_ids . reshape ( h // self . spatial_merge_size , self . spatial_merge_size , w // self . spatial_merge_size , self . spatial_merge_size ,\n            ). permute ( 0 , 2 , 1 , 3 ). flatten () pos_ids . append ( torch . stack ([ hpos_ids , wpos_ids ], dim = - 1 ). repeat ( t , 1 )) pos_ids = torch . cat ( pos_ids , dim = 0 ) max_grid_size = grid_thw [:, 1 :]. max () rotary_pos_emb_full = self . rotary_pos_emb ( max_grid_size ) rotary_pos_emb = rotary_pos_emb_full [ pos_ids ]. flatten ( 1 ) return rotary_pos_emb def get_window_index ( self , grid_thw ): window_index : list = [] cu_window_seqlens : list = [ 0 ] window_index_id = 0 vit_merger_window_size = ( self . window_size // self . spatial_merge_size // self . patch_size ) for grid_t , grid_h , grid_w in grid_thw : llm_grid_h = grid_h // self . spatial_merge_size llm_grid_w = grid_w // self . spatial_merge_size index = torch . arange ( grid_t * llm_grid_h * llm_grid_w ). reshape ( grid_t , llm_grid_h , llm_grid_w ) pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size num_windows_h = ( llm_grid_h + pad_h ) // vit_merger_window_size num_windows_w = ( llm_grid_w + pad_w ) // vit_merger_window_size index_padded = F . pad ( index , ( 0 , pad_w , 0 , pad_h ), 'constant' , - 100 ) index_padded = index_padded . reshape ( grid_t , num_windows_h , vit_merger_window_size , num_windows_w , vit_merger_window_size ) index_padded = index_padded . permute ( 0 , 1 , 3 , 2 , 4 ). reshape ( grid_t , num_windows_h * num_windows_w , vit_merger_window_size , vit_merger_window_size ) seqlens = ( index_padded != - 100 ). sum ([ 2 , 3 ]). reshape ( - 1 ) index_padded = index_padded . reshape ( - 1 ) index_new = index_padded [ index_padded != - 100 ] window_index . append ( index_new + window_index_id ) cu_seqlens_tmp = seqlens . cumsum ( 0 ) * self . spatial_merge_unit + cu_window_seqlens [ - 1 ] cu_window_seqlens . extend ( cu_seqlens_tmp . tolist ()) window_index_id += ( grid_t * llm_grid_h * llm_grid_w ). item () window_index = torch . cat ( window_index , dim = 0 ) return window_index , cu_window_seqlens def compute_attn_mask_seqlen ( self , cu_seqlens : torch . Tensor ,\n    ) -> tuple [ None , None ]: return None , None def process_grid_thw ( self , grid_thw_list ): # Convert list to tensor for compatibility with old model grid_thw = torch . tensor ( grid_thw_list , dtype = torch . int32 ) # Compute positional embeddings rotary_pos_emb = self . rot_pos_emb ( grid_thw ) # Compute window indices and seqlens window_index , cu_window_seqlens = self . get_window_index ( grid_thw ) cu_window_seqlens = torch . tensor ( cu_window_seqlens , device = window_index . device , dtype = torch . int32 ) cu_window_seqlens = torch . unique_consecutive ( cu_window_seqlens ) # Compute sequence lengths cu_seqlens = torch . repeat_interleave ( grid_thw [:, 1 ] * grid_thw [:, 2 ], grid_thw [:, 0 ]). cumsum ( dim = 0 , dtype = torch . int32 ) cu_seqlens = F . pad ( cu_seqlens , ( 1 , 0 ), \"constant\" , 0 ) return rotary_pos_emb , window_index , cu_window_seqlens , cu_seqlens def tensor_equals ( t1 , t2 , name = None , rtol = 1e-5 , atol = 1e-5 ): if t1 . shape != t2 . shape : if name : print ( f\"‚úó { name } shapes differ: { t1 . shape } vs { t2 . shape } \" ) return False equal = torch . allclose ( t1 , t2 , rtol = rtol , atol = atol ) if not equal : # Find the positions where they differ diff_mask = ~ torch . isclose ( t1 , t2 , rtol = rtol , atol = atol ) if diff_mask . sum () > 0 : diff_pos = diff_mask . nonzero () first_diff = diff_pos [ 0 ]. tolist () t1_val = t1 [ tuple ( first_diff )] t2_val = t2 [ tuple ( first_diff )] if name : print ( f\"‚úó { name } values differ at { first_diff } : { t1_val } vs { t2_val } \" ) print ( f\"Total number of different values: { diff_mask . sum (). item () } / { t1 . numel () } \" ) else : if name : print ( f\"‚úó { name } values differ but couldn't identify position\" ) # Print some stats about the differences if name and t1 . numel () < 100 : print ( f\"Old: { t1 . flatten (). tolist () } \" ) print ( f\"New: { t2 . flatten (). tolist () } \" ) return False if name : print ( f\"‚úì { name } matched\" ) return True def run_test ( grid_thw , verbose = True ): # Create models new_model = Qwen2_5_VisionTransformer_New () old_model = Qwen2_5_VisionTransformer_Old () if verbose : print ( \" \\n Testing with grid_thw:\" , grid_thw ) # Test the new model rotary_pos_emb_new , window_index_new , cu_window_seqlens_new , cu_seqlens_new = new_model . process_grid_thw ( grid_thw ) if verbose : print ( \" \\n New model outputs:\" ) print ( f\"rotary_pos_emb shape: { rotary_pos_emb_new . shape } \" ) print ( f\"window_index shape: { window_index_new . shape } \" ) print ( f\"cu_window_seqlens shape: { cu_window_seqlens_new . shape } \" ) print ( f\"cu_seqlens shape: { cu_seqlens_new . shape } \" ) # Test the old model rotary_pos_emb_old , window_index_old , cu_window_seqlens_old , cu_seqlens_old = old_model . process_grid_thw ( grid_thw ) if verbose : print ( \" \\n Old model outputs:\" ) print ( f\"rotary_pos_emb shape: { rotary_pos_emb_old . shape } \" ) print ( f\"window_index shape: { window_index_old . shape } \" ) print ( f\"cu_window_seqlens shape: { cu_window_seqlens_old . shape } \" ) print ( f\"cu_seqlens shape: { cu_seqlens_old . shape } \" ) # Compare outputs if verbose : print ( \" \\n Comparing outputs:\" ) match_rotary = tensor_equals ( rotary_pos_emb_old , rotary_pos_emb_new , \"rotary_pos_emb\" if verbose else None ) match_window = tensor_equals ( window_index_old , window_index_new , \"window_index\" if verbose else None ) match_cu_window = tensor_equals ( cu_window_seqlens_old , cu_window_seqlens_new , \"cu_window_seqlens\" if verbose else None ) match_cu_seq = tensor_equals ( cu_seqlens_old , cu_seqlens_new , \"cu_seqlens\" if verbose else None ) all_match = match_rotary and match_window and match_cu_window and match_cu_seq if verbose : print ( f\" \\n All outputs match: { all_match } \" ) if not all_match : error_msg = f\"Test failed for grid_thw= { grid_thw } : Outputs between old and new implementations do not match\" raise TestFailureException ( error_msg ) return all_match def run_mass_test ( t_range = ( 1 , 50 ), h_range = ( 1 , 250 ), w_range = ( 1 , 250 ), num_samples = 100 , max_images_per_sample = 1 , seed = 42 ): \"\"\" Run mass testing by sampling grid_thw configurations from the specified ranges. Args: t_range: Tuple of (min_t, max_t) h_range: Tuple of (min_h, max_h) w_range: Tuple of (min_w, max_w) num_samples: Number of random samples to test max_images_per_sample: Maximum number of images per sample seed: Random seed for reproducibility \"\"\" random . seed ( seed ) # Ensure minimum h and w values are at least 2 (spatial_merge_size) # This is required by the model architecture min_t = max ( 1 , t_range [ 0 ]) min_h = max ( 2 , h_range [ 0 ]) # Minimum must be at least spatial_merge_size min_w = max ( 2 , w_range [ 0 ]) # Minimum must be at least spatial_merge_size max_t = t_range [ 1 ] max_h = h_range [ 1 ] max_w = w_range [ 1 ] t_range = ( min_t , max_t ) h_range = ( min_h , max_h ) w_range = ( min_w , max_w ) print ( f\"Running mass testing with { num_samples } samples\" ) print ( f\"T range: { t_range } \" ) print ( f\"H range: { h_range } \" ) print ( f\"W range: { w_range } \" ) print ( f\"Max images per sample: { max_images_per_sample } \" ) # Include edge cases edge_cases = [ # Smallest valid values [[ min_t , min_h , min_w ]], # Largest values [[ max_t , max_h , max_w ]], # Min t, max h, w [[ min_t , max_h , max_w ]], # Max t, min h, w [[ max_t , min_h , min_w ]], # Mixed values [[ min_t , max_h , min_w ]],\n        [[ max_t , min_h , max_w ]], # Values divisible by window_size/spatial_merge_size/patch_size [[ min_t , 16 , 16 ]], # 16 = 32/2/1 (window_size/spatial_merge_size/1) [[ min_t , 32 , 32 ]], # 32 = 32/2/0.5 (window_size/spatial_merge_size/0.5) ] # Add multi-image edge cases if max_images_per_sample > 1 if max_images_per_sample > 1 : multi_image_edge_cases = [ # Multiple small images [[ min_t , min_h , min_w ], [ min_t , min_h , min_w ]], # One small, one large [[ min_t , min_h , min_w ], [ max_t , max_h , max_w ]], # Maximum number of images with varied sizes [[ min_t , min_h , min_w ]] * max_images_per_sample ,\n        ] edge_cases . extend ( multi_image_edge_cases ) # Test edge cases first print ( \" \\n Testing edge cases:\" ) for i , grid_thw in enumerate ( edge_cases ): try : print ( f\"Edge case { i + 1 } / { len ( edge_cases ) } : { grid_thw } \" ) run_test ( grid_thw , verbose = False ) print ( f\"‚úì Edge case { i + 1 } passed\" ) except TestFailureException as e : print ( f\" \\n ERROR: { e } \" ) return False except Exception as e : print ( f\" \\n Unexpected error for grid_thw= { grid_thw } : { e } \" ) print ( f\"Exception details: { type ( e ). __name__ } : { e } \" ) return False # Generate random samples for the mass test samples = [] for _ in range ( num_samples ): # Decide how many images to include in this sample num_images = random . randint ( 1 , max_images_per_sample ) # Generate grid_thw for each image sample = [] for _ in range ( num_images ): t = random . randint ( min_t , max_t ) h = random . randint ( min_h , max_h ) w = random . randint ( min_h , max_w ) # Ensure h and w are multiples of spatial_merge_size (2) h = ( h // 2 ) * 2 w = ( w // 2 ) * 2 if h == 0 : h = 2 if w == 0 : w = 2 sample . append ([ t , h , w ]) samples . append ( sample ) # Run the mass test with a progress bar print ( f\" \\n Running { num_samples } random samples:\" ) progress_bar = tqdm . tqdm ( total = num_samples ) for i , grid_thw in enumerate ( samples ): try : run_test ( grid_thw , verbose = False ) progress_bar . update ( 1 ) except TestFailureException as e : progress_bar . close () print ( f\" \\n ERROR at sample { i + 1 } / { num_samples } : { e } \" ) return False except Exception as e : progress_bar . close () print ( f\" \\n Unexpected error at sample { i + 1 } / { num_samples } for grid_thw= { grid_thw } : { e } \" ) print ( f\"Exception details: { type ( e ). __name__ } : { e } \" ) return False progress_bar . close () print ( f\" \\n All { num_samples } samples passed successfully!\" ) return True if __name__ == \"__main__\" : parser = argparse . ArgumentParser ( description = 'Test Qwen2.5-VL Vision Transformer' ) parser . add_argument ( '--grid_t' , type = int , default = 1 , help = 'Grid size T' ) parser . add_argument ( '--grid_h' , type = int , default = 36 , help = 'Grid size H' ) parser . add_argument ( '--grid_w' , type = int , default = 36 , help = 'Grid size W' ) parser . add_argument ( '--multiple' , action = 'store_true' , help = 'Test with multiple images' ) parser . add_argument ( '--large' , action = 'store_true' , help = 'Test with many high-resolution images' ) parser . add_argument ( '--mass-test' , action = 'store_true' , help = 'Run mass testing with many grid configurations' ) parser . add_argument ( '--samples' , type = int , default = 100 , help = 'Number of samples for mass testing' ) parser . add_argument ( '--seed' , type = int , default = 42 , help = 'Random seed for mass testing' ) parser . add_argument ( '--max-t' , type = int , default = 50 , help = 'Maximum T value for mass testing' ) parser . add_argument ( '--max-h' , type = int , default = 250 , help = 'Maximum H value for mass testing' ) parser . add_argument ( '--max-w' , type = int , default = 250 , help = 'Maximum W value for mass testing' ) parser . add_argument ( '--max-images' , type = int , default = 1 , help = 'Maximum number of images per sample for mass testing' ) args = parser . parse_args () if args . mass_test : success = run_mass_test ( t_range = ( 1 , args . max_t ), h_range = ( 1 , args . max_h ), w_range = ( 1 , args . max_w ), num_samples = args . samples , max_images_per_sample = args . max_images , seed = args . seed ) sys . exit ( 0 if success else 1 ) else : if args . large : # Test with a large number of high-resolution images/videos grid_thw = [\n                [ 1 , 224 , 224 ], # High-res image 1 [ 1 , 112 , 112 ], # Medium-res image [ 4 , 96 , 96 ], # Video 1 [ 1 , 168 , 168 ], # Another image [ 2 , 128 , 224 ], # Video 2 [ 1 , 224 , 224 ], # High-res image 2 [ 3 , 64 , 128 ], # Video 3 [ 1 , 96 , 96 ], # Small image [ 6 , 64 , 64 ], # Longer video [ 1 , 192 , 192 ] # Another image ] print ( \"Testing with large dataset (many high-resolution images/videos)\" ) elif args . multiple : # Test with multiple images grid_thw = [\n                [ 1 , 36 , 36 ], # First image [ 2 , 48 , 64 ], # Second image (video) [ 1 , 24 , 24 ] # Third image ] print ( \"Testing with multiple images\" ) else : # Test with a single image grid_thw = [[ args . grid_t , args . grid_h , args . grid_w ]] try : # Run correctness test run_test ( grid_thw ) print ( \" \\n Test completed successfully!\" ) except TestFailureException as e : print ( f\" \\n ERROR: { e } \" ) sys . exit ( 1 ) # Exit with error code üëç 1 WoosukKwon reacted with thumbs up emoji ‚ù§Ô∏è 1 WoosukKwon reacted with heart emoji All reactions üëç 1 reaction ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zzzyq pushed a commit\n        to zzzyq/vllm\n      that referenced\n      this pull request May 24, 2025 [PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding ( ‚Ä¶ 92d9cdb vllm-project#17973 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Yuqi Zhang <yuqizhang@google.com> mergify bot added\n  the qwen Related to Qwen models label Jun 19, 2025 minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding ( ‚Ä¶ 65b6ec6 vllm-project#17973 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: minpeter <kali2005611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:07",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, lm_eval | PERF: req/s, optimization, optimization | SERVING: vllm serve, serve | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:07",
  "models": [
    "Qwen/Qwen2.5-7B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,dtype=float16 --tasks hellaswag,arc_challenge --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-7B-Instruct --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding (#17973)",
  "commit_message": "[PERF] Speed up Qwen2.5-VL model by speed up rotary position embedding (#17973)\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>",
  "commit_date": "2025-05-15T23:31:02-07:00",
  "files_changed": [
    "vllm/model_executor/models/qwen2_5_vl.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 7,
    "num_edited_lines": 204,
    "num_non_test_edited_lines": 204,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex 5904ad1f1..68dd07820 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -25,7 +25,7 @@\n # limitations under the License.\n \"\"\"Inference-only Qwen2.5-VL model compatible with HuggingFace weights.\"\"\"\n from collections.abc import Iterable, Mapping\n-from functools import partial\n+from functools import lru_cache, partial\n from typing import Callable, Literal, Optional, TypedDict, Union\n \n import torch\n@@ -478,8 +478,8 @@ class Qwen2_5_VisionRotaryEmbedding(nn.Module):\n         super().__init__()\n         self.dim = dim\n         self.theta = theta\n-        inv_freq = 1.0 / (theta\n-                          **(torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        inv_freq = 1.0 / (theta**(\n+            torch.arange(0, dim, 2, dtype=torch.float, device='cpu') / dim))\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self._seq_len_cached = 0\n         self._freqs_cached = None\n@@ -520,7 +520,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n         self.hidden_size = vision_config.hidden_size\n         self.num_heads = vision_config.num_heads\n \n-        # args for get_window_index\n+        # args for get_window_index_thw\n         self.window_size = vision_config.window_size\n         self.patch_size = vision_config.patch_size\n         self.spatial_merge_size = vision_config.spatial_merge_size\n@@ -567,65 +567,71 @@ class Qwen2_5_VisionTransformer(nn.Module):\n     def device(self) -> torch.device:\n         return self.patch_embed.proj.weight.device\n \n-    def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n-        pos_ids = []\n-        for t, h, w in grid_thw:\n-            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n-            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n-            hpos_ids = hpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            wpos_ids = wpos_ids.reshape(\n-                h // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-                w // self.spatial_merge_size,\n-                self.spatial_merge_size,\n-            ).permute(0, 2, 1, 3).flatten()\n-            pos_ids.append(\n-                torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n-        pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n-        rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n+    def rotary_pos_emb_thw(self, t, h, w):\n+        hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n+        wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+        hpos_ids = hpos_ids.reshape(\n+            h // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+            w // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+        ).permute(0, 2, 1, 3).flatten()\n+        wpos_ids = wpos_ids.reshape(\n+            h // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+            w // self.spatial_merge_size,\n+            self.spatial_merge_size,\n+        ).permute(0, 2, 1, 3).flatten()\n+        pos_ids = torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1)\n+        max_size = max(h, w)\n+        rotary_pos_emb_full = self.rotary_pos_emb(max_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+        rotary_pos_emb = rotary_pos_emb.reshape(\n+            rotary_pos_emb.shape[0] // self.spatial_merge_unit,\n+            self.spatial_merge_unit, -1)\n+\n         return rotary_pos_emb\n \n-    def get_window_index(self, grid_thw):\n-        window_index: list = []\n-        cu_window_seqlens: list = [0]\n-        window_index_id = 0\n+    def get_window_index_thw(self, grid_t, grid_h, grid_w):\n         vit_merger_window_size = (self.window_size //\n                                   self.spatial_merge_size // self.patch_size)\n \n-        for grid_t, grid_h, grid_w in grid_thw:\n-            llm_grid_h = grid_h // self.spatial_merge_size\n-            llm_grid_w = grid_w // self.spatial_merge_size\n-            index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(\n-                grid_t, llm_grid_h, llm_grid_w)\n-            pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size\n-            pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size\n-            num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size\n-            num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size\n-            index_padded = F.pad(index, (0, pad_w, 0, pad_h), 'constant', -100)\n-            index_padded = index_padded.reshape(grid_t, num_windows_h,\n-                                                vit_merger_window_size,\n-                                                num_windows_w,\n-                                                vit_merger_window_size)\n-            index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(\n-                grid_t, num_windows_h * num_windows_w, vit_merger_window_size,\n-                vit_merger_window_size)\n-            seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)\n-            index_padded = index_padded.reshape(-1)\n-            index_new = index_padded[index_padded != -100]\n-            window_index.append(index_new + window_index_id)\n-            cu_seqlens_tmp = seqlens.cumsum(\n-                0) * self.spatial_merge_unit + cu_window_seqlens[-1]\n-            cu_window_seqlens.extend(cu_seqlens_tmp.tolist())\n-            window_index_id += (grid_t * llm_grid_h * llm_grid_w).item()\n-        window_index = torch.cat(window_index, dim=0)\n-        return window_index, cu_window_seqlens\n+        llm_grid_h = grid_h // self.spatial_merge_size\n+        llm_grid_w = grid_w // self.spatial_merge_size\n+        index = torch.arange(grid_t * llm_grid_h * llm_grid_w).reshape(\n+            grid_t, llm_grid_h, llm_grid_w)\n+        pad_h = vit_merger_window_size - llm_grid_h % vit_merger_window_size\n+        pad_w = vit_merger_window_size - llm_grid_w % vit_merger_window_size\n+        num_windows_h = (llm_grid_h + pad_h) // vit_merger_window_size\n+        num_windows_w = (llm_grid_w + pad_w) // vit_merger_window_size\n+        index_padded = F.pad(index, (0, pad_w, 0, pad_h), 'constant', -100)\n+        index_padded = index_padded.reshape(grid_t, num_windows_h,\n+                                            vit_merger_window_size,\n+                                            num_windows_w,\n+                                            vit_merger_window_size)\n+        index_padded = index_padded.permute(0, 1, 3, 2, 4).reshape(\n+            grid_t, num_windows_h * num_windows_w, vit_merger_window_size,\n+            vit_merger_window_size)\n+        seqlens = (index_padded != -100).sum([2, 3]).reshape(-1)\n+        index_padded = index_padded.reshape(-1)\n+        index_new = index_padded[index_padded != -100]\n+        cu_seqlens_tmp = seqlens.cumsum(0) * self.spatial_merge_unit\n+        cu_seqlens_tmp = cu_seqlens_tmp.to(dtype=torch.int32)\n+        cu_seqlens_tmp = torch.unique_consecutive(cu_seqlens_tmp)\n+\n+        return index_new, cu_seqlens_tmp\n+\n+    @lru_cache(maxsize=1024)  # noqa: B019\n+    def get_rope_by_thw(self, t, h, w):\n+        window_index_thw, cu_seqlens_window_thw = self.get_window_index_thw(\n+            t, h, w)\n+        rotary_pos_emb_thw = self.rotary_pos_emb_thw(t, h, w)\n+        rotary_pos_emb_thw = rotary_pos_emb_thw[window_index_thw, :, :]\n+        rotary_pos_emb_thw = rotary_pos_emb_thw.flatten(start_dim=0, end_dim=1)\n+        cu_seqlens_thw = torch.repeat_interleave(\n+            torch.tensor([h * w], dtype=torch.int32), t)\n+        return (rotary_pos_emb_thw, window_index_thw, cu_seqlens_window_thw,\n+                cu_seqlens_thw)\n \n     def compute_attn_mask_seqlen(\n         self,\n@@ -641,45 +647,74 @@ class Qwen2_5_VisionTransformer(nn.Module):\n     def forward(\n         self,\n         x: torch.Tensor,\n-        grid_thw: torch.Tensor,\n+        grid_thw: list[list[int]],\n     ) -> torch.Tensor:\n         # patchify\n+        seq_len, _ = x.size()\n+        rotary_pos_emb = []\n+        window_index: list = []\n+        cu_window_seqlens: list = [torch.tensor([0], dtype=torch.int32)]\n+        cu_seqlens: list = []\n+\n         hidden_states = x.to(device=self.device, dtype=self.dtype)\n         hidden_states = self.patch_embed(hidden_states)\n \n-        # compute position embedding\n-        rotary_pos_emb = self.rot_pos_emb(grid_thw)\n+        window_index_id = 0\n+        cu_window_seqlens_last = 0\n+        for t, h, w in grid_thw:\n+            t, h, w = int(t), int(h), int(w)\n+            llm_h = h // self.spatial_merge_size\n+            llm_w = w // self.spatial_merge_size\n+\n+            (\n+                rotary_pos_emb_thw,\n+                window_index_thw,\n+                cu_seqlens_window_thw,\n+                cu_seqlens_thw,\n+            ) = self.get_rope_by_thw(t, h, w)\n+\n+            window_index.append(window_index_thw + window_index_id)\n+            window_index_id += (t * llm_h * llm_w)\n+\n+            cu_seqlens_window_thw = (cu_seqlens_window_thw +\n+                                     cu_window_seqlens_last)\n+            cu_window_seqlens_last = cu_seqlens_window_thw[-1]\n+            cu_window_seqlens.append(cu_seqlens_window_thw)\n \n-        # windows attention\n-        window_index, cu_window_seqlens = self.get_window_index(grid_thw)\n-        cu_window_seqlens = torch.tensor(\n-            cu_window_seqlens,\n-            device=hidden_states.device,\n-            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32)\n+            rotary_pos_emb.append(rotary_pos_emb_thw)\n+\n+            cu_seqlens.append(cu_seqlens_thw)\n+\n+        rotary_pos_emb = torch.cat(rotary_pos_emb)\n+        window_index = torch.cat(window_index)\n+        cu_window_seqlens = torch.cat(cu_window_seqlens)\n         cu_window_seqlens = torch.unique_consecutive(cu_window_seqlens)\n-        seq_len, _ = hidden_states.size()\n-        hidden_states = hidden_states.reshape(\n-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n-        hidden_states = hidden_states[window_index, :, :]\n-        hidden_states = hidden_states.reshape(seq_len, -1)\n-        rotary_pos_emb = rotary_pos_emb.reshape(\n-            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n-        rotary_pos_emb = rotary_pos_emb[window_index, :, :]\n-        rotary_pos_emb = rotary_pos_emb.reshape(seq_len, -1)\n-        # compute cu_seqlens\n-        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2],\n-                                             grid_thw[:, 0]).cumsum(\n-                                                 dim=0, dtype=torch.int32)\n+        cu_seqlens = torch.cat(cu_seqlens)\n+        cu_seqlens = torch.cumsum(cu_seqlens, dim=0, dtype=torch.int32)\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), \"constant\", 0)\n \n         # transformers\n-        hidden_states = hidden_states.unsqueeze(1)\n-\n         # pre-compute seqlens for window/full attn to reduce cuMemcpy operations\n         max_seqlen_full, seqlens_full = self.compute_attn_mask_seqlen(\n             cu_seqlens)\n         max_seqlen_window, seqlens_window = self.compute_attn_mask_seqlen(\n             cu_window_seqlens)\n+\n+        cu_seqlens = cu_seqlens.to(device=self.device, non_blocking=True)\n+        cu_window_seqlens = cu_window_seqlens.to(device=self.device,\n+                                                 non_blocking=True)\n+        rotary_pos_emb = rotary_pos_emb.to(device=self.device,\n+                                           non_blocking=True)\n+        window_index = window_index.to(device=hidden_states.device,\n+                                       non_blocking=True)\n+\n+        hidden_states = hidden_states.reshape(\n+            seq_len // self.spatial_merge_unit, self.spatial_merge_unit, -1)\n+        hidden_states = hidden_states[window_index, :, :]\n+        hidden_states = hidden_states.reshape(seq_len, -1)\n+\n+        hidden_states = hidden_states.unsqueeze(1)\n+\n         for layer_num, blk in enumerate(self.blocks):\n             if layer_num in self.fullatt_block_indexes:\n                 cu_seqlens_now = cu_seqlens\n@@ -932,12 +967,13 @@ class Qwen2_5_VLForConditionalGeneration(nn.Module, SupportsMultiModal,\n \n         grid_thw = image_input[\"image_grid_thw\"]\n         assert grid_thw.ndim == 2\n+        grid_thw_list = grid_thw.tolist()\n \n         if image_input[\"type\"] == \"image_embeds\":\n             image_embeds = image_input[\"image_embeds\"].type(self.visual.dtype)\n         else:\n             pixel_values = image_input[\"pixel_values\"].type(self.visual.dtype)\n-            image_embeds = self.visual(pixel_values, grid_thw=grid_thw)\n+            image_embeds = self.visual(pixel_values, grid_thw=grid_thw_list)\n \n         # Split concatenated embeddings for each image item.\n         merge_size = self.visual.spatial_merge_size\n@@ -951,13 +987,15 @@ class Qwen2_5_VLForConditionalGeneration(nn.Module, SupportsMultiModal,\n \n         grid_thw = video_input[\"video_grid_thw\"]\n         assert grid_thw.ndim == 2\n+        grid_thw_list = grid_thw.tolist()\n \n         if video_input[\"type\"] == \"video_embeds\":\n             video_embeds = video_input[\"video_embeds\"].type(self.visual.dtype)\n         else:\n             pixel_values_videos = video_input[\"pixel_values_videos\"].type(\n                 self.visual.dtype)\n-            video_embeds = self.visual(pixel_values_videos, grid_thw=grid_thw)\n+            video_embeds = self.visual(pixel_values_videos,\n+                                       grid_thw=grid_thw_list)\n \n         # Split concatenated embeddings for each video item.\n         merge_size = self.visual.spatial_merge_size",
  "apis": [
    "Qwen2_5_VisionTransformer.forward",
    "Qwen2_5_VisionTransformer.get_window_index_thw",
    "Qwen2_5_VisionTransformer.get_rope_by_thw",
    "Qwen2_5_VLForConditionalGeneration.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/qwen2_5_vl.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/llm.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file and makes non-trivial changes to the computation of rotary position embeddings, window indexing, and sequence length handling. It introduces caching via lru_cache and restructures the functions to streamline and speed up these computations, which directly impacts the performance of the model's inference. The improvements target CPU-based operations and are not just refactoring or bug fixes; they optimize the performance of a high-level API in the model. Therefore, the commit satisfies the conditions for being performance or optimization related.",
  "llm_api_reason": "This commit refactors the rotary position embedding functionality for the Qwen2.5-VL model. It replaces the old ‚Äúrot_pos_emb‚Äù function with a new ‚Äúrotary_pos_emb_thw‚Äù method, renames and revises ‚Äúget_window_index‚Äù to ‚Äúget_window_index_thw‚Äù, and introduces a cached ‚Äúget_rope_by_thw‚Äù to precompute rotary embeddings. The forward pass in the vision transformer is adjusted to expect grid parameters as a list (improving both performance and clarity), and the multimodal generation routine now passes these grid lists to the visual encoder. Overall, the changes aim to speed up inference while ensuring correct positional embedding computation for image/video inputs."
}