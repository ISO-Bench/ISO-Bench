{
  "commit_hash": "89a84b0bb7b30706a02836234a94493ea8f780bf",
  "pr_url": "https://github.com/vllm-project/vllm/pull/6779",
  "pr_date": "2024-07-25",
  "timeline_text": "Copy link Contributor peng1999 commented Jul 25, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Using array.array in SequenceData greatly improves performance of make_tensor_with_pad in Sampler. Micro-benchmark using 1024 input length and 2048 batch size shows a great latency improvment (79ms to 22ms): Before: After: End-to-end test on qwen-1.5-0.5b model also shows improvement on throughput: main: Processed prompts: 100%|‚ñà‚ñà‚ñà| 2048/2048 [01:22<00:00, 24.76it/s, est. speed input: 25352.26 toks/s, output: 3165.44 toks/s] This PR: Processed prompts: 100%|‚ñà‚ñà‚ñà| 2048/2048 [01:09<00:00, 29.44it/s, est. speed input: 30150.97 toks/s, output: 3764.60 toks/s] BEFORE SUBMITTING, PLEASE READ THE CHECKLIST BELOW AND FILL IN THE DESCRIPTION ABOVE PR Checklist (Click to Expand) Thank you for your contribution to vLLM! Before submitting the pull request, please ensure the PR meets the following criteria. This helps vLLM maintain the code quality and improve the efficiency of the review process. PR Title and Classification Only specific types of PRs will be reviewed. The PR title is prefixed appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [CI/Build] for build or continuous integration improvements. [Doc] for documentation fixes and improvements. [Model] for adding a new model or improving an existing model. Model name should appear in the title. [Frontend] For changes on the vLLM frontend (e.g., OpenAI API server, LLM class, etc.) [Kernel] for changes affecting CUDA kernels or other compute kernels. [Core] for changes in the core vLLM logic (e.g., LLMEngine , AsyncLLMEngine , Scheduler , etc.) [Hardware][Vendor] for hardware-specific changes. Vendor name should appear in the prefix (e.g., [Hardware][AMD] ). [Misc] for PRs that do not fit the above categories. Please use this sparingly. Note: If the PR spans more than one category, please include all relevant prefixes. Code Quality The PR need to meet the following code quality standards: We adhere to Google Python style guide and Google C++ style guide . Pass all linter checks. Please use format.sh to format your code. The code need to be well-documented to ensure future contributors can easily understand the code. Include sufficient tests to ensure the project to stay correct and robust. This includes both unit tests and integration tests. Please add documentation to docs/source/ if the PR modifies the user-facing behaviors of vLLM. It helps vLLM user understand and utilize the new features or changes. Notes for Large Changes Please keep the changes as concise as possible. For major architectural changes (>500 LOC excluding kernel/data/config/test), we would expect a GitHub issue (RFC) discussing the technical design and justification. Otherwise, we will tag it with rfc-required and might not go through the PR. What to Expect for the Reviews The goal of the vLLM team is to be a transparent reviewing machine . We would like to make the review process transparent and efficient and make sure no contributor feel confused or frustrated. However, the vLLM team is small, so we need to prioritize some PRs over others. Here is what you can expect from the review process: After the PR is submitted, the PR will be assigned to a reviewer. Every reviewer will pick up the PRs based on their expertise and availability. After the PR is assigned, the reviewer will provide status update every 2-3 days. If the PR is not reviewed within 7 days, please feel free to ping the reviewer or the vLLM team. After the review, the reviewer will put an action-required label on the PR if there are changes required. The contributor should address the comments and ping the reviewer to re-review the PR. Please respond to all comments within a reasonable time frame. If a comment isn't clear or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Thank You Finally, thank you for taking the time to read these guidelines and for your interest in contributing to vLLM. Your contributions make vLLM a great tool for everyone! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üöÄ 8 casper-hansen, mgoin, robertgshaw2-redhat, LiuXiaoxuanPKU, comaniac, akai-shuuichi, Xu-Chen, and Shang-QY reacted with rocket emoji All reactions üöÄ 8 reactions Use array to speedup padding d2ab931 Copy link github-actions bot commented Jul 25, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . reformat code d9c591e peng1999 changed the title Use array to speedup padding [Core] Use array to speedup padding Jul 25, 2024 Copy link Contributor casper-hansen commented Jul 25, 2024 Nice to see an 18% speedup from this optimization. Is it mainly for small models? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin reviewed Jul 25, 2024 View reviewed changes vllm/model_executor/sampling_metadata.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 25, 2024 Copy link Contributor Author peng1999 commented Jul 25, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Is it mainly for small models? Yes. This PR is for small models and large batch sizes. The from_sampling_metadata function, optimized by this PR, primarily runs on the CPU and is independent of logists. Therefore, it can overlap with the GPU work of model inference. It will only be on the critical path if its execution time exceeds that of model inference, which occurs with smaller models. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin approved these changes Jul 25, 2024 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM any concerns @youkaichao ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor daquexian commented Jul 25, 2024 Great PR! Would you mind sharing what tool you used to get this image, is it nsight system? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . youkaichao reviewed Jul 25, 2024 View reviewed changes vllm/sequence.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author peng1999 commented Jul 26, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Great PR! Would you mind sharing what tool you used to get this image, is it nsight system? Yes. The blue spans are recorded using NVTX. üëç 1 daquexian reacted with thumbs up emoji ‚ù§Ô∏è 1 daquexian reacted with heart emoji All reactions üëç 1 reaction ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . youkaichao approved these changes Jul 26, 2024 View reviewed changes Copy link Member youkaichao left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the great job!  Please merge the latest main to pass the tests. I once tried to replace the whole prompt/output tokens to numpy array, but it involves changing too much code, so I gave it up due to limited bandwidth. It's good to see this speedup with a self-contained change. cc @alexm-neuralmagic if you are planning to change the underlying data structure in block managers. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Merge remote-tracking branch 'upstream/main' into opt-array fb63840 Hide details View details youkaichao merged commit 89a84b0 into vllm-project : main Jul 26, 2024 72 checks passed Uh oh! There was an error while loading. Please reload this page . peng1999 deleted the opt-array branch July 30, 2024 09:58 dtrifiro mentioned this pull request Aug 5, 2024 Sync with upstream@v0.5.4-7-g9118217f opendatahub-io/vllm#120 Closed Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Core] Use array to speedup padding ( vllm-project#6779 ) ‚Ä¶ 62afef0 Signed-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Core] Use array to speedup padding ( vllm-project#6779 ) ‚Ä¶ 3f840ef Signed-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:26",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: throughput, latency, optimization | SERVING: API server, OpenAI API server, Frontend | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:48:26",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Core] Use array to speedup padding (#6779)",
  "commit_message": "[Core] Use array to speedup padding (#6779)",
  "commit_date": "2024-07-25T21:31:31-07:00",
  "files_changed": [
    "vllm/model_executor/layers/sampler.py",
    "vllm/model_executor/sampling_metadata.py",
    "vllm/sequence.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 9,
    "num_edited_lines": 46,
    "num_non_test_edited_lines": 46,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 5c376797a..121458f81 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -220,7 +220,7 @@ def _apply_min_tokens_penalty(\n             seqs_to_penalize: List[int] = []\n             for j, seq_id in enumerate(seq_ids):\n                 seq_data = seq_group.seq_data[seq_id]\n-                if len(seq_data.output_token_ids) < min_tokens:\n+                if len(seq_data.output_token_ids_array) < min_tokens:\n                     seqs_to_penalize.append(j)\n \n             if seqs_to_penalize:\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 390b5d173..27b37a9d5 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -1,4 +1,5 @@\n import random\n+from array import array\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple\n \n@@ -329,8 +330,8 @@ class SamplingTensors:\n             user-defined seed for each sequence.\n         extra_entropy: extra entropy to use when generating seeds.\n         \"\"\"\n-        prompt_tokens: List[List[int]] = []\n-        output_tokens: List[List[int]] = []\n+        prompt_tokens: List[array] = []\n+        output_tokens: List[array] = []\n         top_ks: List[int] = []\n         temperatures: List[float] = []\n         top_ps: List[float] = []\n@@ -432,13 +433,15 @@ class SamplingTensors:\n                 if (seq_group.is_prompt\n                         and sampling_params.prompt_logprobs is not None):\n                     prefill_len = len(seq_group.prompt_logprob_indices)\n-                    prompt_tokens.extend([] for _ in range(prefill_len))\n-                    output_tokens.extend([] for _ in range(prefill_len))\n+                    prompt_tokens.extend(\n+                        array('l') for _ in range(prefill_len))\n+                    output_tokens.extend(\n+                        array('l') for _ in range(prefill_len))\n                 if seq_group.do_sample:\n                     for seq_id in seq_ids:\n                         seq_data = seq_group.seq_data[seq_id]\n-                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n-                        output_tokens.append(list(seq_data.output_token_ids))\n+                        prompt_tokens.append(seq_data.prompt_token_ids_array)\n+                        output_tokens.append(seq_data.output_token_ids_array)\n \n         sampling_tensors = SamplingTensors.from_lists(\n             temperatures, top_ps, top_ks, min_ps, presence_penalties,\n@@ -454,9 +457,9 @@ class SamplingTensors:\n                    frequency_penalties: List[float],\n                    repetition_penalties: List[float],\n                    sampling_seeds: List[int], sample_indices: List[int],\n-                   prompt_tokens: List[List[int]],\n-                   output_tokens: List[List[int]], vocab_size: int,\n-                   extra_seeds_to_generate: int, device: torch.device,\n+                   prompt_tokens: List[array], output_tokens: List[array],\n+                   vocab_size: int, extra_seeds_to_generate: int,\n+                   device: torch.device,\n                    dtype: torch.dtype) -> \"SamplingTensors\":\n         # Note that the performance will be very bad without\n         # pinned memory.\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 0cd4c7e71..72821ecea 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -3,6 +3,7 @@ import copy\n import enum\n import math\n from abc import ABC, abstractmethod\n+from array import array\n from collections import defaultdict\n from dataclasses import dataclass, field\n from typing import (TYPE_CHECKING, Dict, List, Mapping, Optional, Set, Tuple,\n@@ -119,10 +120,10 @@ class SequenceData:\n         prompt_token_ids: List[int],\n         output_token_ids: Optional[List[int]] = None,\n     ) -> None:\n-        self._prompt_token_ids: List[int] = list(prompt_token_ids)\n+        self._prompt_token_ids = array('l', prompt_token_ids)\n         self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n-        self._output_token_ids: List[int] = (\n-            list(output_token_ids) if output_token_ids is not None else [])\n+        self._output_token_ids = array(\n+            'l', output_token_ids if output_token_ids is not None else [])\n \n         self.cumulative_logprob = 0.0\n         # The number of tokens that are computed (that run against the model).\n@@ -132,8 +133,8 @@ class SequenceData:\n         self._update_cached_all_tokens()\n \n     def _update_cached_all_tokens(self):\n-        self._cached_all_token_ids: List[int] = (self._prompt_token_ids +\n-                                                 self._output_token_ids)\n+        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n+                                                     self._output_token_ids)\n \n     @property\n     def prompt_token_ids(self) -> Tuple[int, ...]:\n@@ -141,19 +142,27 @@ class SequenceData:\n \n     @prompt_token_ids.setter\n     def prompt_token_ids(self, new_prompt_token_ids) -> None:\n-        self._prompt_token_ids = list(new_prompt_token_ids)\n+        self._prompt_token_ids = array('l', new_prompt_token_ids)\n         self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n         self._update_cached_all_tokens()\n \n+    @property\n+    def prompt_token_ids_array(self) -> array:\n+        return self._prompt_token_ids\n+\n     @property\n     def output_token_ids(self) -> Tuple[int, ...]:\n         return tuple(self._output_token_ids)\n \n     @output_token_ids.setter\n     def output_token_ids(self, new_output_token_ids) -> None:\n-        self._output_token_ids = list(new_output_token_ids)\n+        self._output_token_ids = array('l', new_output_token_ids)\n         self._update_cached_all_tokens()\n \n+    @property\n+    def output_token_ids_array(self) -> array:\n+        return self._output_token_ids\n+\n     def append_token_id(self, token_id: int, logprob: float) -> None:\n         self._output_token_ids.append(token_id)\n         self._cached_all_token_ids.append(token_id)",
  "apis": [
    "vllm.sequence.SequenceData.prompt_token_ids_array",
    "vllm.sequence.SequenceData.output_token_ids_array",
    "vllm.model_executor.sampling_metadata.SamplingTensors.from_lists"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/sequence.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/sampling_metadata.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/sampler.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies several non-test source files (sampler.py, sampling_metadata.py, sequence.py) and replaces list-based token id storage and appending with the use of Python's built-in \"array\" type. This change likely reduces memory overhead and may provide speed improvements when padding or concatenating sequences, thus improving the performance of core operations. The commit message \"[Core] Use array to speedup padding (#6779)\" aligns with the technical modifications, which are non-trivial and affect high-level API performance. These changes are aimed at CPU performance improvement and are testable without specialized GPU or hardware requirements. Therefore, the commit qualifies as a performance or optimization related change.",
  "llm_api_reason": "The commit updates several files to change the underlying type used for storing token ID lists from Python lists to array objects. In the sequence module, the initialization and setters for prompt and output token IDs now convert to arrays (‚Äúl‚Äù type) and additional properties (prompt_token_ids_array and output_token_ids_array) are provided for access. In the sampling metadata module, the expected types for prompt_tokens and output_tokens are changed from List[List[int]] to List[array] (again ‚Äúl‚Äù type) to optimize padding operations. Also, in the sampler layer, the logic now uses the new output_token_ids_array property rather than output_token_ids. These changes affect the public APIs related to token ID access and the API for constructing sampling tensors from lists of token IDs via arrays."
}