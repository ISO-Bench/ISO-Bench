{
  "commit_hash": "58eee5f2e05b74eb2cb1a3bbda9c04df4805e4cc",
  "pr_url": "https://github.com/vllm-project/vllm/pull/20000",
  "pr_date": "2025-08-02",
  "timeline_text": "Copy link Contributor vadiklyutiy commented Jun 24, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. Purpose to_py_obj is called inside transformers tokenizer.decode . In our case to_py_obj convert list to list what is useless. Avoid to_py_obj to speedup decoding. Test Plan Change is really short and trivial. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions avoid useless list-to-list conversion ‚Ä¶ b74bbba Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> Copy link github-actions bot commented Jun 24, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jun 24, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @vadiklyutiy , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request introduces a performance optimization to the token decoding process within the tokenizer utility. By intelligently selecting the most efficient decoding method available, it aims to eliminate redundant data conversions and improve the speed of token decoding. Highlights Performance Optimization : Modified the decode_tokens function in vllm/transformers_utils/tokenizer.py to prioritize using the _decode method of the tokenizer if it exists, falling back to the standard decode method otherwise. This change aims to bypass an unnecessary list-to-list conversion ( to_py_obj ) that can occur during token decoding, thereby improving performance. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with üëç and üëé on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . ‚Ü© Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jun 24, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review The pull request modifies the decode_tokens function in vllm/transformers_utils/tokenizer.py to use the _decode method of the tokenizer if it exists, falling back to the decode method if it doesn't. This is done to potentially speed up decoding by avoiding unnecessary list-to-list conversions. The change is concise and seems reasonable. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/transformers_utils/tokenizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . 22quinn reviewed Jun 24, 2025 View reviewed changes Copy link Collaborator 22quinn left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Interesting. This is the code pointer for anyone interested. I feel this kind of optimization is better done in huggingface. I dig a bit and found there was already some discussion and optimization in huggingface/transformers#36885 Have you measured the speedup for this PR? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 22quinn added\n  the performance Performance-related issues label Jun 24, 2025 Copy link Contributor Author vadiklyutiy commented Jun 24, 2025 Interesting. This is the code pointer for anyone interested. I feel this kind of optimization is better done in huggingface. I dig a bit and found there was already some discussion and optimization in huggingface/transformers#36885 Have you measured the speedup for this PR? @22quinn you are right. This change from my backlog and I did it some time ago. I measured performance without patch to HF you mentioned and that saw a lot of to_py_obj calls for every list element. I will check performance improvement on the latest version. Maybe after HF patch performance improvement too minor to worry about it. Thank you for pointing this out. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator ProExpertProg commented Jun 26, 2025 Congrats on #20000 ! üòÑ 1 22quinn reacted with laugh emoji All reactions üòÑ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details vllm-bot merged commit 58eee5f into vllm-project : main Aug 2, 2025 15 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented Aug 2, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Oops accidentally merged this PR, feel free to revert if there's a problem with it All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author vadiklyutiy commented Aug 3, 2025 @DarkLight1337 Should I create PR to revert it? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented Aug 3, 2025 Is this change still relevant? If not then yeah let's revert All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author vadiklyutiy commented Aug 3, 2025 Ok, let's me collect up to date numbers. Mentioned above merge to transformers improved performance but not fully - there is still some overhead. With specific numbers we can decide. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 35f1408 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: shuw <shuw@nvidia.com> juuice-lee pushed a commit\n        to juuice-lee/vllm-moe.code\n      that referenced\n      this pull request Aug 5, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 9b76219 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ fc6cbb1 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 8cb05d1 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: x22x22 <wadeking@qq.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ fa14d61 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: x22x22 <wadeking@qq.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 91186e5 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> jingyu-ml pushed a commit\n        to jingyu-ml/vllm\n      that referenced\n      this pull request Aug 8, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 6e204de ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: jingyu <jingyu@omniml.ai> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 2d6070c ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> noamgat pushed a commit\n        to noamgat/vllm\n      that referenced\n      this pull request Aug 9, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 2349d3d ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Noam Gat <noamgat@gmail.com> yyihuang pushed a commit\n        to yyihuang/vllm\n      that referenced\n      this pull request Aug 11, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 5372242 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Avery Yingyi Huang <yingyihuang2000@outlook.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 66782d4 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 4b814e9 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> BoyuanFeng pushed a commit\n        to BoyuanFeng/vllm\n      that referenced\n      this pull request Aug 14, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 8ffd112 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Boyuan Feng <boyuan@meta.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 8fb256d ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> Copy link Contributor Author vadiklyutiy commented Aug 22, 2025 @DarkLight1337 Sorry for late reply. Ran Qwen-2.5-VL-3B with high load on latest main with and without this PR. decode_token itself speed up is sufficient - 28%. But after transformers optimizations we don't spend a lot of time in it. E2E improving is tiny - around 0.2%. Please let me know what do you think. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented Aug 22, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . OK, let's revert this PR then. Thanks for investgating this! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . DarkLight1337 added a commit\n        to DarkLight1337/vllm\n      that referenced\n      this pull request Aug 22, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ ec8ebfe ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\"\n\nThis reverts commit 58eee5f .\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> DarkLight1337 mentioned this pull request Aug 22, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)\" #23396 Merged 4 tasks Isotr0py pushed a commit\n      that referenced\n      this pull request Aug 23, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ b4e9fd8 ‚Ä¶ist-to-list conversion ( #20000 )\" ( #23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> FFFfff1FFFfff pushed a commit\n        to FFFfff1FFFfff/my_vllm\n      that referenced\n      this pull request Aug 25, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ cb92141 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: FFFfff1FFFfff <yifanli0919@gmail.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ f902dce ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 622bd37 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> juuice-lee pushed a commit\n        to juuice-lee/vllm-moe.code\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 84c70d4 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> xiao-llm pushed a commit\n        to xiao-llm/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ dd95e26 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Xiao Yu <xiao.yu@amd.com> xiao-llm pushed a commit\n        to xiao-llm/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 2b472fc ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Xiao Yu <xiao.yu@amd.com> zhewenl pushed a commit\n        to zhewenl/vllm\n      that referenced\n      this pull request Aug 28, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ cd0e40b ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> zhewenl pushed a commit\n        to zhewenl/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ fbaa487 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> dumb0002 pushed a commit\n        to dumb0002/vllm\n      that referenced\n      this pull request Aug 28, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ f30ac74 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [PERF] Use faster way of decode in tokenizer: avoid useless list-to-l‚Ä¶ ‚Ä¶ 04627e3 ‚Ä¶ist conversion ( vllm-project#20000 )\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> 2015aroras pushed a commit\n        to 2015aroras/vllm\n      that referenced\n      this pull request Aug 29, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 38f7e84 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> mengxingkongzhouhan pushed a commit\n        to mengxingkongzhouhan/vllm\n      that referenced\n      this pull request Aug 30, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 4eec518 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> mengxingkongzhouhan pushed a commit\n        to mengxingkongzhouhan/vllm\n      that referenced\n      this pull request Aug 30, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 1f5ccee ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> mengxingkongzhouhan pushed a commit\n        to mengxingkongzhouhan/vllm\n      that referenced\n      this pull request Aug 30, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ b20b3e1 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> mengxingkongzhouhan pushed a commit\n        to mengxingkongzhouhan/vllm\n      that referenced\n      this pull request Aug 30, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ fe798f2 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> mengxingkongzhouhan pushed a commit\n        to mengxingkongzhouhan/vllm\n      that referenced\n      this pull request Aug 30, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 4f2a849 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> nopperl pushed a commit\n        to pfnet/vllm\n      that referenced\n      this pull request Sep 3, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 5a917a8 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> 842974287 pushed a commit\n        to 842974287/vllm\n      that referenced\n      this pull request Sep 3, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 81e37d6 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Shiyan Deng <dsy842974287@meta.com> zhewenl pushed a commit\n        to zhewenl/vllm\n      that referenced\n      this pull request Sep 3, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 1046c1c ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk> ekagra-ranjan pushed a commit\n        to ekagra-ranjan/vllm\n      that referenced\n      this pull request Sep 4, 2025 Revert \"[PERF] Use faster way of decode in tokenizer: avoid useless l‚Ä¶ ‚Ä¶ 4f93bc2 ‚Ä¶ist-to-list conversion ( vllm-project#20000 )\" ( vllm-project#23396 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Ekagra Ranjan <3116519+ekagra-ranjan@users.noreply.github.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:44",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: optimization, Optimization, optimization | TEST: test, test, Test",
  "analysis_extracted_at": "2025-09-07 17:49:44",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)",
  "commit_message": "[PERF] Use faster way of decode in tokenizer: avoid useless list-to-list conversion (#20000)\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>",
  "commit_date": "2025-08-02T01:43:52-07:00",
  "files_changed": [
    "vllm/transformers_utils/tokenizer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 7,
    "num_non_test_edited_lines": 7,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 24ddd35ab..6a31a4198 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -50,11 +50,12 @@ def decode_tokens(\n     `skip_special_tokens=None` means to use the backend's default\n     settings.\n     \"\"\"\n+    decode_method = getattr(tokenizer, \"_decode\", tokenizer.decode)\n     if skip_special_tokens is not None:\n-        return tokenizer.decode(token_ids,\n-                                skip_special_tokens=skip_special_tokens)\n+        return decode_method(token_ids,\n+                             skip_special_tokens=skip_special_tokens)\n \n-    return tokenizer.decode(token_ids)\n+    return decode_method(token_ids)\n \n \n def encode_tokens(",
  "apis": [
    "vllm.transformers_utils.tokenizer.decode_tokens"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer_base.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer_group.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file (vllm/transformers_utils/tokenizer.py) and changes the decode_tokens function to use a potentially faster internal method (_decode) over the default decode. This modification directly impacts the performance of the tokenizer's decode functionality, avoiding unnecessary list-to-list conversion as indicated by the commit message. The change is non-trivial, focuses on optimizing performance of a high-level API, and is applicable in general CPU execution contexts.",
  "llm_api_reason": "The commit optimizes the decode_tokens function in the tokenizer module by using getattr to check for a faster \"_decode\" method. This update directly affects the public API decode_tokens, ensuring that downstream users benefit from improved performance in token decoding."
}