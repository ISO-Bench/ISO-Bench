{
  "commit_hash": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
  "pr_url": "https://github.com/vllm-project/vllm/pull/3357",
  "pr_date": null,
  "timeline_text": "Copy link Contributor ElizaWszola commented Mar 12, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . The performance of block allocator went down after implementing automatic prefix caching, even when running with prefix caching disabled. This pr brings back parts of the old code and regains some of the lost performance in the scenario with disabled prefix caching. Benchmarked with: python benchmark_throughput_cache.py --backend vllm --model huggyllama/llama-7b --dataset ../data/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 2000 Performance before introducing automatic prefix caching (commit baee28c ): Throughput: 10.37 requests/s, 5062.42 tokens/s Throughput: 10.46 requests/s, 5102.27 tokens/s Throughput: 10.47 requests/s, 5107.30 tokens/s Throughput: 10.48 requests/s, 5113.97 tokens/s Throughput: 10.53 requests/s, 5137.21 tokens/s Throughput: 10.54 requests/s, 5145.38 tokens/s Throughput: 10.56 requests/s, 5153.24 tokens/s Throughput: 10.57 requests/s, 5157.54 tokens/s Throughput: 10.63 requests/s, 5187.32 tokens/s Throughput: 10.65 requests/s, 5198.19 tokens/s Performance after introducing changes in this PR to commit ce4f5a2 : Throughput: 10.40 requests/s, 5076.05 tokens/s Throughput: 10.53 requests/s, 5137.97 tokens/s Throughput: 10.57 requests/s, 5156.04 tokens/s Throughput: 10.60 requests/s, 5173.07 tokens/s Throughput: 10.61 requests/s, 5177.02 tokens/s Throughput: 10.62 requests/s, 5179.91 tokens/s Throughput: 10.63 requests/s, 5186.06 tokens/s Throughput: 10.63 requests/s, 5186.63 tokens/s Throughput: 10.64 requests/s, 5193.72 tokens/s Throughput: 10.67 requests/s, 5207.76 tokens/s (OLD) Benchmark results (10 runs each): Performance before introducing automatic prefix caching (commit baee28c ): Throughput: 10.15 requests/s, 4909.50 tokens/s Throughput: 10.17 requests/s, 4918.22 tokens/s Throughput: 10.20 requests/s, 4936.93 tokens/s Throughput: 10.23 requests/s, 4949.76 tokens/s Throughput: 10.22 requests/s, 4945.64 tokens/s Throughput: 10.27 requests/s, 4967.08 tokens/s Throughput: 10.28 requests/s, 4971.52 tokens/s Throughput: 10.29 requests/s, 4980.92 tokens/s Throughput: 10.29 requests/s, 4976.94 tokens/s Throughput: 10.30 requests/s, 4982.69 tokens/s Performance after introducing automatic prefix caching (commit ce4f5a2 ): Throughput: 9.91 requests/s, 4795.14 tokens/s Throughput: 9.98 requests/s, 4830.01 tokens/s Throughput: 9.99 requests/s, 4832.00 tokens/s Throughput: 10.00 requests/s, 4839.62 tokens/s Throughput: 10.03 requests/s, 4851.13 tokens/s Throughput: 10.06 requests/s, 4868.87 tokens/s Throughput: 10.07 requests/s, 4873.87 tokens/s Throughput: 10.07 requests/s, 4872.51 tokens/s Throughput: 10.08 requests/s, 4876.18 tokens/s Throughput: 10.08 requests/s, 4877.26 tokens/s Performance after introducing changes in this PR to commit ce4f5a2 : Throughput: 10.07 requests/s, 4873.42 tokens/s Throughput: 10.17 requests/s, 4919.84 tokens/s Throughput: 10.18 requests/s, 4923.71 tokens/s Throughput: 10.18 requests/s, 4925.56 tokens/s Throughput: 10.19 requests/s, 4928.09 tokens/s Throughput: 10.20 requests/s, 4937.20 tokens/s Throughput: 10.21 requests/s, 4942.21 tokens/s Throughput: 10.21 requests/s, 4938.38 tokens/s Throughput: 10.21 requests/s, 4940.22 tokens/s Throughput: 10.22 requests/s, 4946.95 tokens/s Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸ‘ 2 cadedaniel and rkooo567 reacted with thumbs up emoji All reactions ðŸ‘ 2 reactions ElizaWszola added 8 commits March 6, 2024 13:10 Auto prefix performace fixes 2d2f5bb Small change to no-prefix-caching hashing 9468ce8 Pre-allocate token block list in no-cache scenario 83cd6ed Refactor block manager 4dd06e5 Clean up evictor, fix 20b7db8 Sage's feedback 690cc5e Merge branch 'upstream-main' into auto-prefix-perf 6e50143 format evictor 723e56b Copy link Member zhuohan123 commented Mar 12, 2024 cc @cadedaniel ðŸ‘ 1 cadedaniel reacted with thumbs up emoji All reactions ðŸ‘ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Fix tests fc9aebb cadedaniel reviewed Mar 13, 2024 View reviewed changes Copy link Collaborator cadedaniel left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the PR! I am concerned that our test coverage of the block manager is not sufficient to allow for refactors w/o good tests. There's a few branches in this PR that are only for prefix caching, which adds a lot of complexity. Could you comment on what causes the performance degradation / improvement? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions cadedaniel self-assigned this Mar 13, 2024 zhuohan123 self-assigned this Mar 14, 2024 zhuohan123 reviewed Mar 14, 2024 View reviewed changes Copy link Member zhuohan123 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Some random small comments. Will review in more detail! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager.py Outdated def free(self, block: PhysicalTokenBlock) -> None: pass @abstractproperty Copy link Member zhuohan123 Mar 13, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Should be abstract_method Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . ElizaWszola and others added 3 commits March 14, 2024 14:23 Update vllm/core/block_manager.py â€¦ c2f74ef Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Update vllm/core/block_manager.py â€¦ 17ffc2d Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Update vllm/core/block_manager.py â€¦ c383bac Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Copy link Contributor Author ElizaWszola commented Mar 14, 2024 @cadedaniel I can think up some tests to add. Is there anything that you would like to be tested specifically? As for the performance gap that still exists, I'm not sure about it because the non-cached codepath is currently very similar to what had been there before the original auto prefix commit. I'm still poking around. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Feedback, one more small modification eaa1fb3 Copy link Contributor Author ElizaWszola commented Mar 14, 2024 Good news, I've found a small bug and redid some of the benchmarks: the performance looks similar to the old one, but I'd be happy if more people can verify. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'upstream-main' into auto-prefix-perf 29f9414 ElizaWszola mentioned this pull request Mar 15, 2024 [PREFIX CACHING FOLLOW UP] OrderedDict-based evictor #3431 Merged ElizaWszola changed the title A bunch of fixes to block allocator performance when automatic prefix caching is disabled [PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled Mar 15, 2024 AllenDou reviewed Mar 18, 2024 View reviewed changes vllm/core/evictor.py if block.num_hashed_tokens == highest_num_hashed_tokens: if (block.last_accessed < evicted_block.last_accessed or block.last_accessed == evicted_block.last_accessed and block.num_hashed_tokens > evicted_block.num_hashed_tokens): Copy link Contributor AllenDou Mar 18, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I have also optimized the evictor LRU, but after learning more about evictors, I feel that LRU is unnecessary as it is not as efficient as the random policy. So, in my opinion, LRU policy should be removed. cc @cadedaniel Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author ElizaWszola Mar 18, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The changes in this PR improve LRU evictor efficiency marginally. I'm ok with removing them from this PR, especially when a better way to improve LRU evictor efficiency (bringing it to the level roughly on par with random evictor for the tested cases) is implemented here: #3431 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions zhuohan123 approved these changes Mar 19, 2024 View reviewed changes Copy link Member zhuohan123 left a comment â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Thanks for the fix and left some small comments. Regarding @cadedaniel 's comment on tests, let's discuss more offline together and figure out what tests we need to write. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager.py else: # Set the reference counts of the token blocks. block.ref_count = seq_group.num_seqs() elif self.enable_caching: Copy link Member zhuohan123 Mar 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Does prefix caching work with sliding window now? Should we explicitly check somewhere that if we enable caching, sliding window should not be enabled. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author ElizaWszola Mar 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The prefix caching functionality is simply not used when we have sliding windows. We have specific checks for that in different places in the code. Putting it in a more central place sounds like a better idea though, and less confusing. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . zhuohan123 added\n  the action-required label Mar 19, 2024 ElizaWszola and others added 4 commits March 19, 2024 13:06 Update vllm/core/block_manager.py â€¦ 65b8213 Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Update vllm/core/block_manager.py â€¦ 1fc91bb Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Update vllm/core/block_manager.py â€¦ e39ae06 Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> Update vllm/core/block_manager.py â€¦ af1285f Co-authored-by: Zhuohan Li <zhuohan123@gmail.com> ElizaWszola added 2 commits March 19, 2024 08:38 format, disallow sliding window with prefix caching 6c96014 Merge branch 'upstream-main' into auto-prefix-perf c4b69ab Copy link Member zhuohan123 commented Mar 19, 2024 @ElizaWszola Please let me know when this PR is ready to be merged! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhuohan123 approved these changes Mar 20, 2024 View reviewed changes Copy link Member zhuohan123 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Thanks for the fix! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions zhuohan123 enabled auto-merge (squash) March 20, 2024 07:11 zhuohan123 disabled auto-merge March 20, 2024 07:11 zhuohan123 merged commit 9474e89 into vllm-project : main Mar 20, 2024 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:25",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: Throughput, Throughput, Throughput | TEST: test",
  "analysis_extracted_at": "2025-09-07 17:49:25",
  "models": [
    "huggyllama/llama-7b"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmark_throughput_cache.py --backend vllm --model huggyllama/llama-7b --dataset ../data/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 2000",
  "commit_subject": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)",
  "commit_message": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)\n\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>",
  "commit_date": "2024-03-20T00:11:11-07:00",
  "files_changed": [
    "tests/core/test_block_manager.py",
    "tests/prefix_caching/test_prefix_caching.py",
    "vllm/core/block_manager.py",
    "vllm/core/evictor.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 4,
    "num_hunks": 25,
    "num_edited_lines": 286,
    "num_non_test_edited_lines": 260,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex 44ac05a14..9473a33f0 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -4,7 +4,7 @@ from typing import List\n \n from vllm import SamplingParams\n from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager import (BlockAllocator, BlockSpaceManager,\n+from vllm.core.block_manager import (UncachedBlockAllocator, BlockSpaceManager,\n                                      AllocStatus)\n from vllm.utils import Device\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus, Logprob\n@@ -15,7 +15,8 @@ from .utils import create_dummy_prompt\n def test_block_allocator_allocate():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     num_free = num_cpu_blocks\n@@ -24,7 +25,7 @@ def test_block_allocator_allocate():\n         block = cpu_allocator.allocate()\n         num_free -= 1\n \n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n     with pytest.raises(ValueError):\n@@ -34,14 +35,15 @@ def test_block_allocator_allocate():\n def test_block_allocator_free():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     blocks: List[PhysicalTokenBlock] = []\n     for _ in range(num_cpu_blocks):\n         block = cpu_allocator.allocate()\n         blocks.append(block)\n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n \n     # Free all allocated cpu blocks.\n     num_free = 0\n@@ -49,7 +51,7 @@ def test_block_allocator_free():\n     for block in blocks:\n         cpu_allocator.free(block)\n         num_free += 1\n-        assert block.block_hash in cpu_allocator.evictor\n+        assert block in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n         with pytest.raises(ValueError):\ndiff --git a/tests/prefix_caching/test_prefix_caching.py b/tests/prefix_caching/test_prefix_caching.py\nindex c83551c36..cb61aac39 100644\n--- a/tests/prefix_caching/test_prefix_caching.py\n+++ b/tests/prefix_caching/test_prefix_caching.py\n@@ -4,7 +4,7 @@ Run `pytest tests/prefix_caching/test_prefix_caching.py`.\n \"\"\"\n import pytest\n \n-from vllm.core.block_manager import BlockAllocator\n+from vllm.core.block_manager import CachedBlockAllocator\n from vllm.utils import Device\n \n \n@@ -15,10 +15,7 @@ def test_block_allocator(\n     num_blocks: int,\n ):\n     block_hash = 1\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n \n     # Allocate two PysicalTokenBlocks with the same hash and check\n     # that they are the same PhysicalTokenBlock\n@@ -45,10 +42,7 @@ def test_block_allocator(\n @pytest.mark.parametrize(\"num_blocks\", [16])\n def test_eviction(num_blocks: int, ):\n     block_size = 16\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n     blocks = []\n \n     for i in range(num_blocks):\ndiff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nindex 8b089a565..ad9b557fd 100644\n--- a/vllm/core/block_manager.py\n+++ b/vllm/core/block_manager.py\n@@ -3,6 +3,7 @@ import enum\n from itertools import count, takewhile\n from os.path import commonprefix\n from typing import Dict, List, Optional, Set, Tuple\n+from abc import ABC, abstractmethod\n \n from vllm.block import BlockTable, PhysicalTokenBlock\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n@@ -10,7 +11,7 @@ from vllm.utils import Device\n from vllm.core.evictor import Evictor, EvictionPolicy, make_evictor\n \n \n-class BlockAllocator:\n+class BlockAllocatorBase(ABC):\n     \"\"\"Manages free physical token blocks for a device.\n \n     The allocator maintains a list of free blocks and allocates a block when\n@@ -18,23 +19,57 @@ class BlockAllocator:\n     the reference count becomes zero, the block is added back to the free list.\n     \"\"\"\n \n+    @abstractmethod\n     def __init__(self,\n                  device: Device,\n                  block_size: int,\n                  num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n-                 enable_caching: bool = False) -> None:\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n+        pass\n+\n+    @abstractmethod\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        pass\n+\n+    @abstractmethod\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        pass\n+\n+    @abstractmethod\n+    def get_num_free_blocks(self) -> int:\n+        pass\n+\n+    @abstractmethod\n+    def contains_block(self, block_hash: int) -> bool:\n+        pass\n+\n+    @abstractmethod\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        pass\n+\n+\n+class CachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 device: Device,\n+                 block_size: int,\n+                 num_blocks: int,\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n         self.device = device\n         self.block_size = block_size\n         self.num_blocks = num_blocks\n-        self.enable_caching = enable_caching\n \n         self.current_num_blocks = 0\n         self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n \n-        # Switch over to FIFO eviction when caching is disabled\n-        if not self.enable_caching:\n-            eviction_policy = EvictionPolicy.FIFO\n         self.evictor: Evictor = make_evictor(eviction_policy)\n \n         self.default_hash_ctr = count()\n@@ -57,13 +92,6 @@ class BlockAllocator:\n     def allocate(self,\n                  block_hash: Optional[int] = None,\n                  num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        # If caching is disabled, just allocate a new block and return it\n-        if not self.enable_caching:\n-            block = self.allocate_block(next(self.default_hash_ctr),\n-                                        num_hashed_tokens)\n-            block.ref_count += 1\n-            return block\n-\n         if block_hash is None:\n             block_hash = next(self.default_hash_ctr)\n         if block_hash in self.evictor:\n@@ -90,9 +118,8 @@ class BlockAllocator:\n             assert block.block_hash not in self.evictor\n             self.evictor.add(block)\n \n-            # If caching is enabled, remove the block from the cached_blocks\n-            if self.enable_caching:\n-                del self.cached_blocks[block.block_hash]\n+            # Remove the block from the cached_blocks\n+            del self.cached_blocks[block.block_hash]\n \n     def get_num_free_blocks(self) -> int:\n         return (self.num_blocks - self.current_num_blocks +\n@@ -102,14 +129,68 @@ class BlockAllocator:\n         return block_hash in self.cached_blocks or block_hash in self.evictor\n \n     def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        # If caching is enabled, update the hash of block and the\n-        # cached_blocks dictionary.\n-        if self.enable_caching:\n-            assert not self.contains_block(block_hash)\n-            old_hash = block.block_hash\n-            block.block_hash = block_hash\n-            del self.cached_blocks[old_hash]\n-            self.cached_blocks[block_hash] = block\n+        # Update the hash of block and the cached_blocks dictionary.\n+        assert not self.contains_block(block_hash)\n+        old_hash = block.block_hash\n+        block.block_hash = block_hash\n+        del self.cached_blocks[old_hash]\n+        self.cached_blocks[block_hash] = block\n+\n+\n+class UncachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        device: Device,\n+        block_size: int,\n+        num_blocks: int,\n+    ) -> None:\n+        self.device = device\n+        self.block_size = block_size\n+        self.num_blocks = num_blocks\n+\n+        # Initialize the free blocks.\n+        self.free_blocks: BlockTable = []\n+        for i in range(num_blocks):\n+            block = PhysicalTokenBlock(device=device,\n+                                       block_number=i,\n+                                       block_size=block_size,\n+                                       block_hash=-1,\n+                                       num_hashed_tokens=0)\n+            self.free_blocks.append(block)\n+\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        if not self.free_blocks:\n+            raise ValueError(\"Out of memory! No free blocks are available.\")\n+        block = self.free_blocks.pop()\n+        block.ref_count = 1\n+        return block\n+\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        if block.ref_count == 0:\n+            raise ValueError(f\"Double free! {block} is already freed.\")\n+        block.ref_count -= 1\n+        if block.ref_count == 0:\n+            self.free_blocks.append(block)\n+\n+    def get_num_free_blocks(self) -> int:\n+        return len(self.free_blocks)\n+\n+    def contains_block(self, block_hash: int) -> bool:\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n+\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n \n \n class AllocStatus(enum.Enum):\n@@ -142,6 +223,10 @@ class BlockSpaceManager:\n         self.num_total_gpu_blocks = num_gpu_blocks\n         self.num_total_cpu_blocks = num_cpu_blocks\n \n+        if enable_caching and sliding_window is not None:\n+            raise NotImplementedError(\n+                \"Sliding window is not allowed with prefix caching enabled!\")\n+\n         self.block_sliding_window = None\n         if sliding_window is not None:\n             assert sliding_window % block_size == 0, (sliding_window,\n@@ -154,14 +239,17 @@ class BlockSpaceManager:\n         self.enable_caching = enable_caching\n \n         self.watermark_blocks = int(watermark * num_gpu_blocks)\n-        self.gpu_allocator = BlockAllocator(Device.GPU,\n-                                            block_size,\n-                                            num_gpu_blocks,\n-                                            enable_caching=enable_caching)\n-        self.cpu_allocator = BlockAllocator(Device.CPU,\n-                                            block_size,\n-                                            num_cpu_blocks,\n-                                            enable_caching=enable_caching)\n+\n+        if self.enable_caching:\n+            self.gpu_allocator = CachedBlockAllocator(Device.GPU, block_size,\n+                                                      num_gpu_blocks)\n+            self.cpu_allocator = CachedBlockAllocator(Device.CPU, block_size,\n+                                                      num_cpu_blocks)\n+        else:\n+            self.gpu_allocator = UncachedBlockAllocator(\n+                Device.GPU, block_size, num_gpu_blocks)\n+            self.cpu_allocator = UncachedBlockAllocator(\n+                Device.CPU, block_size, num_cpu_blocks)\n         # Mapping: seq_id -> BlockTable.\n         self.block_tables: Dict[int, BlockTable] = {}\n \n@@ -198,10 +286,16 @@ class BlockSpaceManager:\n             if (self.block_sliding_window is not None\n                     and logical_idx >= self.block_sliding_window):\n                 block = block_table[logical_idx % self.block_sliding_window]\n-            else:\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n+            elif self.enable_caching:\n                 block = self.gpu_allocator.allocate(\n                     seq.hash_of_block(logical_idx),\n                     seq.num_hashed_tokens_of_block(logical_idx))\n+            else:\n+                block = self.gpu_allocator.allocate()\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n             block_table.append(block)\n \n         # Assign the block table for each sequence.\n@@ -220,8 +314,10 @@ class BlockSpaceManager:\n         seq: Sequence,\n         last_block: PhysicalTokenBlock,\n     ) -> PhysicalTokenBlock:\n-        # Compute a new hash for the block so that it can be shared by\n-        # other Sequences\n+        assert self.enable_caching\n+\n+        # Compute a new hash for the block so that it can be shared by other\n+        # Sequences\n         new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n \n         # if new_hash is already in the cached table, then free last_block\n@@ -254,6 +350,8 @@ class BlockSpaceManager:\n         self,\n         seq: Sequence,\n     ) -> PhysicalTokenBlock:\n+        if not self.enable_caching:\n+            return self.gpu_allocator.allocate()\n         block_hash: Optional[int] = None\n         if (self._is_last_block_full(seq)):\n             block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n@@ -293,10 +391,12 @@ class BlockSpaceManager:\n         assert last_block.device == Device.GPU\n         if last_block.ref_count == 1:\n             # Not shared with other sequences. Appendable.\n-            # If the last block is now complete, promote it to a full block so\n-            # that it can be shared\n-            new_block = self._maybe_promote_last_block(seq, last_block)\n-            block_table[-1] = new_block\n+            if self.enable_caching:\n+                # If the last block is now complete, we may reuse an old block\n+                # to save memory.\n+                maybe_new_block = self._maybe_promote_last_block(\n+                    seq, last_block)\n+                block_table[-1] = maybe_new_block\n             return None\n         else:\n             # The last block is shared with other sequences.\n@@ -440,9 +540,12 @@ class BlockSpaceManager:\n         seq: Sequence,\n         access_time: float,\n     ) -> None:\n-        block_table = self.block_tables[seq.seq_id]\n-        for block in block_table:\n-            block.last_accessed = access_time\n+        if self.enable_caching:\n+            # Update the last accessed time of all the blocks accessed\n+            # in this step.\n+            block_table = self.block_tables[seq.seq_id]\n+            for block in block_table:\n+                block.last_accessed = access_time\n \n     def compute_full_blocks_in_seq(self, seq: Sequence):\n         if seq.seq_id not in self.block_tables:\ndiff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex 1d81f5a97..9f401cba3 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,5 +1,5 @@\n import enum\n-from typing import Dict, List, Optional\n+from typing import Dict\n from abc import ABC, abstractmethod, abstractproperty\n \n from vllm.block import PhysicalTokenBlock\n@@ -10,7 +10,6 @@ class EvictionPolicy(enum.Enum):\n        Evictor subclass.\n     \"\"\"\n     LRU = enum.auto()\n-    FIFO = enum.auto()\n \n \n class Evictor(ABC):\n@@ -66,37 +65,18 @@ class LRUEvictor(Evictor):\n \n     # TODO: The performance of this evict function can be optimized further.\n     def evict(self) -> PhysicalTokenBlock:\n-        free_blocks: List[PhysicalTokenBlock] = list(self.free_table.values())\n-        if len(free_blocks) == 0:\n+        if len(self.free_table) == 0:\n             raise ValueError(\"No usable cache memory left\")\n+        free_blocks = self.free_table.values()\n \n-        # Find lowest timestamp\n-        lowest_timestamp = free_blocks[0].last_accessed\n-        for block in free_blocks:\n-            if block.last_accessed < lowest_timestamp:\n-                lowest_timestamp = block.last_accessed\n+        # Get evicted block\n+        evicted_block: PhysicalTokenBlock = next(iter(free_blocks))\n \n-        # Find all blocks with the lowest timestamp\n-        least_recent: List[PhysicalTokenBlock] = []\n         for block in free_blocks:\n-            if block.last_accessed == lowest_timestamp:\n-                least_recent.append(block)\n-\n-        # Find highest prefix count per block\n-        highest_num_hashed_tokens = 0\n-        for block in least_recent:\n-            if block.num_hashed_tokens > highest_num_hashed_tokens:\n-                highest_num_hashed_tokens = block.num_hashed_tokens\n-\n-        evicted_block: Optional[PhysicalTokenBlock] = None\n-\n-        # Find the first block with the lowest timestamp\n-        for block in least_recent:\n-            if block.num_hashed_tokens == highest_num_hashed_tokens:\n+            if (block.last_accessed < evicted_block.last_accessed\n+                    or block.last_accessed == evicted_block.last_accessed and\n+                    block.num_hashed_tokens > evicted_block.num_hashed_tokens):\n                 evicted_block = block\n-                break\n-\n-        assert evicted_block is not None\n \n         del self.free_table[evicted_block.block_hash]\n \n@@ -119,43 +99,8 @@ class LRUEvictor(Evictor):\n         return len(self.free_table)\n \n \n-class RandomEvictor(Evictor):\n-    \"\"\"Evicts in a first-in-first-out order\"\"\"\n-\n-    def __init__(self):\n-        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n-\n-    def __contains__(self, block_hash: int) -> bool:\n-        return block_hash in self.free_table\n-\n-    def evict(self) -> PhysicalTokenBlock:\n-        if len(self.free_table) == 0:\n-            raise ValueError(\"No usable cache memory left\")\n-        evicted_block = next(iter(self.free_table.values()))\n-        evicted_block.computed = False\n-        del self.free_table[evicted_block.block_hash]\n-        return evicted_block\n-\n-    def add(self, block: PhysicalTokenBlock):\n-        self.free_table[block.block_hash] = block\n-\n-    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n-        if block_hash not in self.free_table:\n-            raise ValueError(\n-                \"Attempting to remove block that's not in the evictor\")\n-        block: PhysicalTokenBlock = self.free_table[block_hash]\n-        del self.free_table[block_hash]\n-        return block\n-\n-    @property\n-    def num_blocks(self) -> int:\n-        return len(self.free_table)\n-\n-\n def make_evictor(eviction_policy: EvictionPolicy) -> Evictor:\n     if eviction_policy == EvictionPolicy.LRU:\n         return LRUEvictor()\n-    elif eviction_policy == EvictionPolicy.FIFO:\n-        return RandomEvictor()\n     else:\n         raise ValueError(f\"Unknown cache eviction policy: {eviction_policy}\")",
  "apis": [
    "vllm.core.block_manager.UncachedBlockAllocator",
    "vllm.core.block_manager.CachedBlockAllocator",
    "vllm.core.block_manager.BlockSpaceManager"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block_manager.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/prefix_caching_block.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/offline_inference/automatic_prefix_caching.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes modifications in both test and non-test source filesâ€”in particular, in files under vllm/core, including the block_allocator and evictor implementations. The changes include introducing distinct classes for cached and uncached block allocation by adding UncachedBlockAllocator and CachedBlockAllocator, refactoring how blocks are allocated, freed, and managed, and simplifying the eviction logic. Although the commit message mentions \"prefix caching\" and patch title hints at performance-related fixes, the changes clearly improve block allocation efficiency and memory management rather than merely renaming functions or fixing bugs in a trivial fashion. The modifications affect core APIs that handle CPU-based operations, and they appear to target performance optimization of allocation and eviction workflows. Therefore, the commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "The commit refactors the block allocator system. It replaces the old BlockAllocator with two distinct implementations â€“ one for cached allocation (CachedBlockAllocator) and one for uncached allocation (UncachedBlockAllocator) â€“ and updates the BlockSpaceManager to instantiate the appropriate allocator based on whether prefix caching is enabled. Test files were updated to import and use these new allocator classes and verify correct behavior, such as checking free blocks instead of block hashes. Overall, this change affects the public Python APIs related to block allocation and management used by vLLM."
}