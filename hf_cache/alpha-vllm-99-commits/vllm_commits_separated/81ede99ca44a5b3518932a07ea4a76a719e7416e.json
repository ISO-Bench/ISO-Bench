{
  "commit_hash": "81ede99ca44a5b3518932a07ea4a76a719e7416e",
  "pr_url": "https://github.com/vllm-project/vllm/pull/8704",
  "pr_date": "2024-10-17",
  "timeline_text": "Copy link Collaborator KuntaiDu commented Sep 22, 2024 This PR deprecates block manager v1 and makes block manager v2 the default to simplify the code path. This is supported by this benchmark , where block manager v2 is <2% slower than block manager v1 on Llama 8B when no prefix hit, and has significant speedup upon full prefix hit. Summary of changes: Leave --use-v2-block-manager in the EngineArgs for compatibility Remove use_v2_block_manager flag in all tests and configs (except during initialization), so that the value change of use-v2-block-manager has no effect on vLLM behavior. BEFORE SUBMITTING, PLEASE READ THE CHECKLIST BELOW AND FILL IN THE DESCRIPTION ABOVE PR Checklist (Click to Expand) Thank you for your contribution to vLLM! Before submitting the pull request, please ensure the PR meets the following criteria. This helps vLLM maintain the code quality and improve the efficiency of the review process. PR Title and Classification Only specific types of PRs will be reviewed. The PR title is prefixed appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [CI/Build] for build or continuous integration improvements. [Doc] for documentation fixes and improvements. [Model] for adding a new model or improving an existing model. Model name should appear in the title. [Frontend] For changes on the vLLM frontend (e.g., OpenAI API server, LLM class, etc.) [Kernel] for changes affecting CUDA kernels or other compute kernels. [Core] for changes in the core vLLM logic (e.g., LLMEngine , AsyncLLMEngine , Scheduler , etc.) [Hardware][Vendor] for hardware-specific changes. Vendor name should appear in the prefix (e.g., [Hardware][AMD] ). [Misc] for PRs that do not fit the above categories. Please use this sparingly. Note: If the PR spans more than one category, please include all relevant prefixes. Code Quality The PR need to meet the following code quality standards: We adhere to Google Python style guide and Google C++ style guide . Pass all linter checks. Please use format.sh to format your code. The code need to be well-documented to ensure future contributors can easily understand the code. Include sufficient tests to ensure the project to stay correct and robust. This includes both unit tests and integration tests. Please add documentation to docs/source/ if the PR modifies the user-facing behaviors of vLLM. It helps vLLM user understand and utilize the new features or changes. Adding or changing kernels Each custom kernel needs a schema and one or more implementations to be registered with PyTorch. Make sure custom ops are registered following PyTorch guidelines: Custom C++ and CUDA Operators and The Custom Operators Manual Custom operations that return Tensors require meta-functions. Meta-functions should be implemented and registered in python so that dynamic dims can be handled automatically. See above documents for a description of meta-functions. Use torch.libary.opcheck() to test the function registration and meta-function for any registered ops.  See tests/kernels for examples. When changing the C++ signature of an existing op, the schema must be updated to reflect the changes. If a new custom type is needed, see the following document: Custom Class Support in PT2 . Notes for Large Changes Please keep the changes as concise as possible. For major architectural changes (>500 LOC excluding kernel/data/config/test), we would expect a GitHub issue (RFC) discussing the technical design and justification. Otherwise, we will tag it with rfc-required and might not go through the PR. What to Expect for the Reviews The goal of the vLLM team is to be a transparent reviewing machine . We would like to make the review process transparent and efficient and make sure no contributor feel confused or frustrated. However, the vLLM team is small, so we need to prioritize some PRs over others. Here is what you can expect from the review process: After the PR is submitted, the PR will be assigned to a reviewer. Every reviewer will pick up the PRs based on their expertise and availability. After the PR is assigned, the reviewer will provide status update every 2-3 days. If the PR is not reviewed within 7 days, please feel free to ping the reviewer or the vLLM team. After the review, the reviewer will put an action-required label on the PR if there are changes required. The contributor should address the comments and ping the reviewer to re-review the PR. Please respond to all comments within a reasonable time frame. If a comment isn't clear or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Thank You Finally, thank you for taking the time to read these guidelines and for your interest in contributing to vLLM. Your contributions make vLLM a great tool for everyone! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üéâ 1 cadedaniel reacted with hooray emoji All reactions üéâ 1 reaction KuntaiDu added 7 commits September 20, 2024 05:09 remove block_manager_v1 and rename block_manager_v2 to block_manager 53cac04 remove block manager v2 related args f199d95 move the version name of block manager from v2 to main da0f9e3 remove flags that set use-v2-block-manager 59ee8fb remove v2 block manager d12ced7 remove warnings with blockmanagerv1 3203112 remove block manager v2 45d35ba Copy link github-actions bot commented Sep 22, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator comaniac commented Sep 22, 2024 FYI: @sroy745 has #8678 verifying the functional correctness. Could you folks coordinate on this? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author KuntaiDu commented Sep 22, 2024 Sure! This PR will be a draft PR until @sroy745 verifies all the tests. I will also talk to @sroy745 and see if I can help. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . KuntaiDu marked this pull request as draft September 22, 2024 06:55 Copy link Collaborator comaniac commented Sep 22, 2024 Sure! This PR will be a draft PR until @sroy745 verifies all the tests. I will also talk to @sroy745 and see if I can help. Thanks! @sroy745 has identified some failed tests and is fixing them. We could have a tracking issue and work together on fixing them. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sroy745 commented Sep 22, 2024 Sure! This PR will be a draft PR until @sroy745 verifies all the tests. I will also talk to @sroy745 and see if I can help. Thanks! @sroy745 has identified some failed tests and is fixing them. We could have a tracking issue and work together on fixing them. I filed #8718 to track the unit test failures. I am currently looking at the test_scheduler.py failures. üëç 2 comaniac and KuntaiDu reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . KuntaiDu added 3 commits September 25, 2024 16:07 Merge branch 'main' into kuntai-remove-blockmngerv1 ba12509 remove use_v2_block_manager in Speculative decoding config 479104c make format checker happy 17ccfd6 KuntaiDu added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Sep 25, 2024 Copy link Collaborator Author KuntaiDu commented Sep 25, 2024 Add ready to trigger full set of CI and see which test fails All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sroy745 commented Sep 26, 2024 fyi I have one pr in flight #8824 which fixes the last of the know test failures that I found earlier. üëç 1 KuntaiDu reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . KuntaiDu added 6 commits September 26, 2024 01:59 remove v2 block manager flag c97fdac bug fix: change BlockManager to MainBlockManager 25584f7 fix wrong parameters in test, and remove the check for blockmanagerv1 1149e40 remove best_of 2 --- beam search is deprecated now a0e9e36 make ruff happy 243a8bd make yapf happy c95e720 KuntaiDu marked this pull request as ready for review September 26, 2024 03:07 KuntaiDu added 3 commits September 26, 2024 03:08 empty change to trigger CI 4afa3a3 ok 95231af Merge branch 'vllm-project:main' into kuntai-remove-blockmngerv1 46410be Isotr0py mentioned this pull request Sep 29, 2024 [Core][VLM] Add support for prefix caching for multi-modal models #8348 Closed 52 hidden items Load more‚Ä¶ KuntaiDu added 4 commits October 16, 2024 06:51 make format checker happy 2e5f091 Make yapf happy ccf9362 Remove the corresponding test for \"CachedBlockAllocator\", which is on‚Ä¶ ‚Ä¶ fe7ea69 ‚Ä¶ly for block manager v1. Make ruff happy 3b7005b KuntaiDu requested a review\n  from comaniac October 16, 2024 19:53 Copy link Collaborator Author KuntaiDu commented Oct 16, 2024 @comaniac I fixed merge conflicts and removed some unnecessary flags and functions for block manager v1. PTAL All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac approved these changes Oct 16, 2024 View reviewed changes Copy link Collaborator comaniac left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Otherwise LGTM. Also cc @sroy745 for review. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions .buildkite/test-pipeline.yaml Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tests/core/block/e2e/test_correctness.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tests/core/block/e2e/test_correctness.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tests/core/block/e2e/test_correctness.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/arg_utils.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . sroy745 reviewed Oct 17, 2024 View reviewed changes Copy link Collaborator sroy745 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the pr!! LGTM. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/core/block/e2e/test_correctness.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tests/core/block/e2e/test_correctness.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/arg_utils.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . KuntaiDu added 5 commits October 17, 2024 02:59 adjust test name and doc string to avoid reusing v1 and v2 in tes‚Ä¶ ‚Ä¶ 178c260 ‚Ä¶t name remove \"v2\" in the test name 4ae3567 Adjust docstrings for --use-v2-block-manager 70be1de further adjust the doc string --- use \"block manager v1\" and \"block m‚Ä¶ ‚Ä¶ 755fec3 ‚Ä¶anager v2\" in engine args doc string as it is more familiar for people. Merge branch 'main' into kuntai-remove-blockmngerv1 405f415 Hide details View details KuntaiDu merged commit 81ede99 into vllm-project : main Oct 17, 2024 77 checks passed Uh oh! There was an error while loading. Please reload this page . KuntaiDu deleted the kuntai-remove-blockmngerv1 branch October 17, 2024 16:38 KuntaiDu restored the kuntai-remove-blockmngerv1 branch October 17, 2024 16:43 KuntaiDu deleted the kuntai-remove-blockmngerv1 branch October 17, 2024 16:43 DarkLight1337 mentioned this pull request Oct 17, 2024 [Misc] Remove commit id file #9470 Merged KuntaiDu mentioned this pull request Oct 22, 2024 [Core] Remove evictor_v1 #9572 Merged saienduri mentioned this pull request Oct 24, 2024 update block_manager usage in setup_cython ROCm/vllm#243 Merged Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Core] Deprecating block manager v1 and make block manager v2 default ( ‚Ä¶ ‚Ä¶ 7cd2f07 ‚Ä¶vllm-project#8704 )\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).\n\nSigned-off-by: Alvant <alvasian@yandex.ru> garg-amit pushed a commit\n        to garg-amit/vllm\n      that referenced\n      this pull request Oct 28, 2024 [Core] Deprecating block manager v1 and make block manager v2 default ( ‚Ä¶ ‚Ä¶ fdd67ee ‚Ä¶vllm-project#8704 )\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).\n\nSigned-off-by: Amit Garg <mitgarg17495@gmail.com> FerdinandZhong pushed a commit\n        to FerdinandZhong/vllm\n      that referenced\n      this pull request Oct 29, 2024 [Core] Deprecating block manager v1 and make block manager v2 default ( ‚Ä¶ ‚Ä¶ c086d36 ‚Ä¶vllm-project#8704 )\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).\n\nSigned-off-by: qishuai <ferdinandzhong@gmail.com> sumitd2 pushed a commit\n        to sumitd2/vllm\n      that referenced\n      this pull request Nov 14, 2024 [Core] Deprecating block manager v1 and make block manager v2 default ( ‚Ä¶ ‚Ä¶ 8e864ff ‚Ä¶vllm-project#8704 )\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).\n\nSigned-off-by: Sumit Dubey <sumit.dubey2@ibm.com> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Core] Deprecating block manager v1 and make block manager v2 default ( ‚Ä¶ ‚Ä¶ f09498c ‚Ä¶vllm-project#8704 )\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:50",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: speedup | SERVING: API server, OpenAI API server, Frontend | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:47:50",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Core] Deprecating block manager v1 and make block manager v2 default (#8704)",
  "commit_message": "[Core] Deprecating block manager v1 and make block manager v2 default (#8704)\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).",
  "commit_date": "2024-10-17T11:38:15-05:00",
  "files_changed": [
    ".buildkite/test-pipeline.yaml",
    "benchmarks/benchmark_latency.py",
    "benchmarks/benchmark_prefix_caching.py",
    "benchmarks/benchmark_throughput.py",
    "benchmarks/overheads/benchmark_hashing.py",
    "docs/source/models/spec_decode.rst",
    "examples/offline_inference_mlpspeculator.py",
    "tests/basic_correctness/test_chunked_prefill.py",
    "tests/core/block/e2e/test_correctness.py",
    "tests/core/block/e2e/test_correctness_sliding_window.py",
    "tests/core/block/test_block_manager.py",
    "tests/core/test_block_manager.py",
    "tests/core/test_chunked_prefill_scheduler.py",
    "tests/core/test_num_computed_tokens_update.py",
    "tests/core/test_scheduler.py",
    "tests/metrics/test_metrics.py",
    "tests/multi_step/test_correctness_async_llm.py",
    "tests/multi_step/test_correctness_llm.py",
    "tests/prefix_caching/test_prefix_caching.py",
    "tests/spec_decode/e2e/test_compatibility.py",
    "tests/spec_decode/e2e/test_eagle_correctness.py",
    "tests/spec_decode/e2e/test_integration.py",
    "tests/spec_decode/e2e/test_integration_dist_tp2.py",
    "tests/spec_decode/e2e/test_integration_dist_tp4.py",
    "tests/spec_decode/e2e/test_logprobs.py",
    "tests/spec_decode/e2e/test_medusa_correctness.py",
    "tests/spec_decode/e2e/test_mlp_correctness.py",
    "tests/spec_decode/e2e/test_multistep_correctness.py",
    "tests/spec_decode/e2e/test_ngram_correctness.py",
    "tests/spec_decode/e2e/test_seed.py",
    "tests/utils.py",
    "vllm/attention/backends/flash_attn.py",
    "vllm/attention/backends/flashinfer.py",
    "vllm/attention/backends/utils.py",
    "vllm/commit_id.py",
    "vllm/config.py",
    "vllm/core/block/utils.py",
    "vllm/core/block_manager.py",
    "vllm/core/block_manager_v1.py",
    "vllm/core/interfaces.py",
    "vllm/core/scheduler.py",
    "vllm/engine/arg_utils.py",
    "vllm/engine/llm_engine.py",
    "vllm/envs.py",
    "vllm/worker/model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 24,
    "num_non_test_files": 21,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 45,
    "num_hunks": 207,
    "num_edited_lines": 2315,
    "num_non_test_edited_lines": 952,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 398fdc5f0..d2324d7ce 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -77,8 +77,8 @@ steps:\n   - vllm/\n   - tests/basic_correctness/test_chunked_prefill\n   commands:\n-  - VLLM_ATTENTION_BACKEND=XFORMERS VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n-  - VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py\n \n - label: Core Test # 10min\n   mirror_hardwares: [amd]\n@@ -88,11 +88,7 @@ steps:\n   - vllm/distributed\n   - tests/core\n   commands:\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core/test_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/test_chunked_prefill_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n-  - pytest -v -s core --ignore=core/block/e2e/test_correctness.py --ignore=core/test_scheduler.py --ignore=core/test_chunked_prefill_scheduler.py --ignore=core/block/e2e/test_correctness.py --ignore=core/block/e2e/test_correctness_sliding_window.py\n+  - pytest -v -s core\n \n - label: Entrypoints Test # 40min\n   working_dir: \"/vllm-workspace/tests\"\n@@ -192,8 +188,7 @@ steps:\n   - vllm/\n   - tests/prefix_caching\n   commands:\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s prefix_caching/test_prefix_caching.py\n-    - pytest -v -s prefix_caching --ignore=prefix_caching/test_prefix_caching.py\n+    - pytest -v -s prefix_caching\n \n - label: Samplers Test # 36min\n   source_file_dependencies:\n@@ -217,8 +212,7 @@ steps:\n   - tests/spec_decode\n   commands:\n     - pytest -v -s spec_decode/e2e/test_multistep_correctness.py\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s spec_decode/e2e/test_compatibility.py\n-    - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py --ignore=spec_decode/e2e/test_compatibility.py\n+    - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py\n \n - label: LoRA Test %N # 15min each\n   mirror_hardwares: [amd]\n@@ -405,7 +399,7 @@ steps:\n   - pytest -v -s ./compile/test_basic_correctness.py\n   - pytest -v -s ./compile/test_wrapper.py\n   - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py | grep -q 'Same node test passed'\n-  - TARGET_TEST_SUITE=L4 VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest basic_correctness/ -v -s -m distributed_2_gpus\n+  - TARGET_TEST_SUITE=L4 pytest basic_correctness/ -v -s -m distributed_2_gpus\n   # Avoid importing model tests that cause CUDA reinitialization error\n   - pytest models/encoder_decoder/language/test_bart.py -v -s -m distributed_2_gpus\n   - pytest models/encoder_decoder/vision_language/test_broadcast.py -v -s -m distributed_2_gpus\ndiff --git a/benchmarks/benchmark_latency.py b/benchmarks/benchmark_latency.py\nindex 79a48b2a1..ea1a7788f 100644\n--- a/benchmarks/benchmark_latency.py\n+++ b/benchmarks/benchmark_latency.py\n@@ -38,7 +38,6 @@ def main(args: argparse.Namespace):\n         quantization_param_path=args.quantization_param_path,\n         device=args.device,\n         ray_workers_use_nsight=args.ray_workers_use_nsight,\n-        use_v2_block_manager=args.use_v2_block_manager,\n         enable_chunked_prefill=args.enable_chunked_prefill,\n         download_dir=args.download_dir,\n         block_size=args.block_size,\n@@ -221,9 +220,6 @@ if __name__ == '__main__':\n     parser.add_argument(\"--enable-prefix-caching\",\n                         action='store_true',\n                         help=\"Enable automatic prefix caching\")\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager)\n     parser.add_argument(\n         \"--ray-workers-use-nsight\",\n         action='store_true',\ndiff --git a/benchmarks/benchmark_prefix_caching.py b/benchmarks/benchmark_prefix_caching.py\nindex f14092d34..a354358e4 100644\n--- a/benchmarks/benchmark_prefix_caching.py\n+++ b/benchmarks/benchmark_prefix_caching.py\n@@ -33,7 +33,6 @@ from typing import List, Optional, Tuple\n from transformers import PreTrainedTokenizerBase\n \n from vllm import LLM, SamplingParams\n-from vllm.engine.arg_utils import EngineArgs\n from vllm.utils import FlexibleArgumentParser\n \n try:\n@@ -134,7 +133,6 @@ def main(args):\n               tokenizer_mode='auto',\n               trust_remote_code=True,\n               enforce_eager=True,\n-              use_v2_block_manager=args.use_v2_block_manager,\n               tensor_parallel_size=args.tensor_parallel_size,\n               enable_prefix_caching=args.enable_prefix_caching)\n \n@@ -176,10 +174,6 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help='Use BlockSpaceMangerV2')\n     parser.add_argument('--num-prompts',\n                         type=int,\n                         default=1,\ndiff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py\nindex b7bc2a640..e26706af6 100644\n--- a/benchmarks/benchmark_throughput.py\n+++ b/benchmarks/benchmark_throughput.py\n@@ -86,7 +86,6 @@ def run_vllm(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -113,7 +112,6 @@ def run_vllm(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n     )\n \n@@ -176,7 +174,6 @@ async def run_vllm_async(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -204,7 +201,6 @@ async def run_vllm_async(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n         worker_use_ray=False,\n         disable_log_requests=True,\n@@ -341,8 +337,7 @@ def main(args: argparse.Namespace):\n             args.enable_prefix_caching, args.enable_chunked_prefill,\n             args.max_num_batched_tokens, args.distributed_executor_backend,\n             args.gpu_memory_utilization, args.num_scheduler_steps,\n-            args.use_v2_block_manager, args.download_dir, args.load_format,\n-            args.disable_async_output_proc\n+            args.download_dir, args.load_format, args.disable_async_output_proc\n         ]\n \n         if args.async_engine:\n@@ -471,10 +466,6 @@ if __name__ == \"__main__\":\n         type=int,\n         default=1,\n         help=\"Maximum number of forward steps per scheduler call.\")\n-    parser.add_argument(\"--use-v2-block-manager\",\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help=\"Enable block manager v2.\")\n     parser.add_argument(\n         \"--enable-prefix-caching\",\n         action='store_true',\ndiff --git a/benchmarks/overheads/benchmark_hashing.py b/benchmarks/overheads/benchmark_hashing.py\nindex 203699e9a..d16d6f9fb 100644\n--- a/benchmarks/overheads/benchmark_hashing.py\n+++ b/benchmarks/overheads/benchmark_hashing.py\n@@ -16,7 +16,6 @@ def main(args):\n         enforce_eager=True,\n         enable_prefix_caching=True,\n         tensor_parallel_size=args.tensor_parallel_size,\n-        use_v2_block_manager=args.use_v2_block_manager,\n     )\n \n     sampling_params = SamplingParams(temperature=0, max_tokens=args.output_len)\n@@ -56,8 +55,5 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        help='Use BlockSpaceMangerV2')\n     args = parser.parse_args()\n     main(args)\ndiff --git a/docs/source/models/spec_decode.rst b/docs/source/models/spec_decode.rst\nindex 0dc9cb383..b02c80aeb 100644\n--- a/docs/source/models/spec_decode.rst\n+++ b/docs/source/models/spec_decode.rst\n@@ -30,7 +30,6 @@ The following code configures vLLM in an offline mode to use speculative decodin\n         tensor_parallel_size=1,\n         speculative_model=\"facebook/opt-125m\",\n         num_speculative_tokens=5,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -104,7 +103,6 @@ matching n-grams in the prompt. For more information read `this thread. <https:/\n         speculative_model=\"[ngram]\",\n         num_speculative_tokens=5,\n         ngram_prompt_lookup_max=4,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -135,7 +133,6 @@ For more information see `this blog <https://pytorch.org/blog/hitchhikers-guide-\n         tensor_parallel_size=4,\n         speculative_model=\"ibm-fms/llama3-70b-accelerator\",\n         speculative_draft_tensor_parallel_size=1,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \ndiff --git a/examples/offline_inference_mlpspeculator.py b/examples/offline_inference_mlpspeculator.py\nindex 5dec4a76a..8f0eb65e4 100644\n--- a/examples/offline_inference_mlpspeculator.py\n+++ b/examples/offline_inference_mlpspeculator.py\n@@ -50,8 +50,6 @@ if __name__ == \"__main__\":\n     llm = LLM(\n         model=\"meta-llama/Llama-2-13b-chat-hf\",\n         speculative_model=\"ibm-fms/llama-13b-accelerator\",\n-        # These are currently required for MLPSpeculator decoding\n-        use_v2_block_manager=True,\n     )\n \n     print(\"With speculation\")\ndiff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex e8819688c..c3e3835af 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -12,7 +12,7 @@ from contextlib import nullcontext\n import pytest\n \n from ..models.utils import check_logprobs_close, check_outputs_equal\n-from ..utils import check_deprecated_block_manager_usage, multi_gpu_test\n+from ..utils import multi_gpu_test\n \n MODELS = [\n     \"facebook/opt-125m\",\n@@ -20,12 +20,6 @@ MODELS = [\n ]\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/basic_correctness/test_chunked_prefill.py')\n-\n-\n @pytest.mark.parametrize(\"model\", MODELS)\n @pytest.mark.parametrize(\"dtype\", [\"half\"])\n @pytest.mark.parametrize(\"max_tokens\", [32])\n@@ -197,7 +191,6 @@ def test_models_with_fp8_kv_cache(\n @pytest.mark.parametrize(\"max_tokens\", [16])\n @pytest.mark.parametrize(\"enforce_eager\", [False])\n @pytest.mark.parametrize(\"chunk_size\", [30, 32])\n-@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n # NOTE: Increasing this in this suite will fail CI because we currently cannot\n # reset distributed env properly. Use a value > 1 just when you test.\n @pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n@@ -206,7 +199,6 @@ def test_with_prefix_caching(\n     max_tokens: int,\n     enforce_eager: bool,\n     chunk_size: int,\n-    use_v2_block_manager: bool,\n     tensor_parallel_size: int,\n ) -> None:\n     \"\"\"\n@@ -234,7 +226,6 @@ def test_with_prefix_caching(\n                 enable_chunked_prefill=True,\n                 enable_prefix_caching=enable,\n                 tensor_parallel_size=tensor_parallel_size,\n-                use_v2_block_manager=use_v2_block_manager,\n                 enforce_eager=enforce_eager,\n                 max_num_seqs=max_num_seqs,\n         ) as vllm_model:\ndiff --git a/tests/core/block/e2e/test_correctness.py b/tests/core/block/e2e/test_correctness.py\nindex b3f626714..86502f613 100644\n--- a/tests/core/block/e2e/test_correctness.py\n+++ b/tests/core/block/e2e/test_correctness.py\n@@ -2,18 +2,11 @@ from itertools import cycle\n \n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import SamplingParams\n \n from .conftest import get_token_ids_from_llm_generator\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/block/e2e/test_correctness.py')\n-\n-\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n@@ -28,32 +21,32 @@ def check_deprecated_block_manager():\n         \"num_gpu_blocks_override\": 5 * (64 + 1),\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"swap\"\n }, {\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"recompute\"\n }])\n @pytest.mark.parametrize(\"batch_size\", [10])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n-                                               test_llm_generator, batch_size):\n-    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n-    when there is preemption.\n+def test_block_manager_with_preemption(baseline_llm_generator,\n+                                       test_llm_generator, batch_size):\n+    \"\"\"Verify block manager produces same outputs even when there is preemption.\n \n     This constructs two LLM, each with limited number of GPU blocks. The limit\n     is decided such that as the sequences in the batch grow, sequences must be\n     preempted and removed from cache.\n \n     If the output token ids are equivalent, then we have confidence that the KV\n-    cache is not corrupted in the v2 block manager.\n+    cache is not corrupted.\n \n     NOTE: We want a significant number of generated tokens so that any incorrect\n     KV mapping has time to build up error.\n+\n+    NOTE(Kuntai): Though we have removed block manager v1, this test is still\n+    useful as it asserts the behavior of block manager v2 (now it is called \n+    SelfAttnBlockSpaceManager) is the same when swapping / preemption, so we  \n+    keep this test.\n     \"\"\"\n     output_len = 1024\n     temperature = 0.0\n@@ -77,11 +70,9 @@ def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids from block manager v1')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids from block manager v2')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -104,9 +95,6 @@ def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n \n         # skip cuda graph creation for fast test.\n         \"enforce_eager\": True,\n-\n-        # Lookahead scheduling only supported in v2 block manager.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -218,26 +206,22 @@ def test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n                              \"max_num_seqs\": 10,\n                          }])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [\n-    {\n-        \"use_v2_block_manager\": False,\n-    },\n+    {},\n ])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [\n     {\n-        \"use_v2_block_manager\": True,\n         \"num_lookahead_slots\": 0,\n     },\n     {\n-        \"use_v2_block_manager\": True,\n         \"num_lookahead_slots\": 5,\n     },\n ])\n @pytest.mark.parametrize(\"batch_size\", [4])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n-                                          test_llm_generator, batch_size):\n-    \"\"\"Verify that chunked prefill works with BlockManagerV2, with and without\n-    lookahead scheduling.\n+def test_chunked_prefill_block_manager(baseline_llm_generator,\n+                                       test_llm_generator, batch_size):\n+    \"\"\"Verify that chunked prefill works with SelfAttnBlockSpaceManager, \n+    with and without lookahead scheduling.\n     \"\"\"\n     output_len = 32\n     temperature = 0.0\n@@ -258,11 +242,11 @@ def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids with BlockManagerV1')\n+    print('Getting token ids with BlockManager')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids with BlockManagerV2')\n+    print('Getting token ids with BlockManager, with lookahead slots.')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -290,32 +274,32 @@ def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n         \"enable_prefix_caching\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"swap\"\n }, {\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"recompute\"\n }])\n @pytest.mark.parametrize(\"batch_size\", [10])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n+def test_block_manager_prefix_caching_enabled_with_preemption(\n         baseline_llm_generator, test_llm_generator, batch_size):\n-    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n-    when there is preemption.\n+    \"\"\"Verify block manager produces same outputs even when there is preemption.\n \n     This constructs two LLM, each with limited number of GPU blocks. The limit\n     is decided such that as the sequences in the batch grow, sequences must be\n     preempted and removed from cache.\n \n     If the output token ids are equivalent, then we have confidence that the KV\n-    cache is not corrupted in the v2 block manager.\n+    cache is not corrupted.\n \n     NOTE: We want a significant number of generated tokens so that any incorrect\n     KV mapping has time to build up error.\n+\n+    NOTE(Kuntai): Though we have removed block manager v1, this test is still\n+    useful as it asserts the behavior of block manager v2 (now it is called \n+    SelfAttnBlockSpaceManager) is the same when swapping / preemption, so we  \n+    keep this test.\n     \"\"\"\n     output_len = 1024\n     temperature = 0.0\n@@ -339,11 +323,11 @@ def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n         temperature=temperature,\n     )\n \n-    print('Getting token ids from block manager v1')\n+    print('Getting token ids from block manager')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids from block manager v2')\n+    print('Getting token ids from block manager, with preemption')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -366,9 +350,6 @@ def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n         # Allow only 5 sequences of ~1024 tokens in worst case.\n         \"block_size\": 16,\n         \"num_gpu_blocks_override\": 5 * (64 + 1),\n-\n-        # Test APC in v2 block\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n@@ -444,9 +425,6 @@ def test_auto_prefix_caching_with_preemption(baseline_llm_generator,\n         \"max_model_len\": 48,\n         \"block_size\": 16,\n         \"num_gpu_blocks_override\": 3,\n-\n-        # Test APC in v2 block\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\ndiff --git a/tests/core/block/e2e/test_correctness_sliding_window.py b/tests/core/block/e2e/test_correctness_sliding_window.py\nindex 731131984..9320a9ef6 100644\n--- a/tests/core/block/e2e/test_correctness_sliding_window.py\n+++ b/tests/core/block/e2e/test_correctness_sliding_window.py\n@@ -3,7 +3,6 @@ from typing import List\n \n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import LLM, SamplingParams\n \n from .conftest import get_text_from_llm_generator\n@@ -13,12 +12,6 @@ MODEL = \"bigcode/starcoder2-3b\"\n BLOCK_SIZE = 16\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/block/e2e/test_correctness_sliding_window.py')\n-\n-\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n@@ -31,10 +24,8 @@ def check_deprecated_block_manager():\n         \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"use_v2_block_manager\": True}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n+@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"batch_size\", [5])\n @pytest.mark.parametrize(\"seed\", [1])\n def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n@@ -55,7 +46,6 @@ def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n \n     prompts, answer, indices = prep_prompts(batch_size)\n \n-    print('Getting token ids from block manager v1')\n     baseline_texts = get_text_from_llm_generator(baseline_llm_generator,\n                                                  prompts,\n                                                  sampling_params,\n@@ -91,10 +81,7 @@ def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n         \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n-    \"enable_chunked_prefill\": True\n-}])\n+@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"enable_chunked_prefill\": True}])\n @pytest.mark.parametrize(\"batch_size\", [5])\n @pytest.mark.parametrize(\"seed\", [1])\n def test_sliding_window_chunked_prefill(test_llm_generator, batch_size, seed):\ndiff --git a/tests/core/block/test_block_manager_v2.py b/tests/core/block/test_block_manager.py\nsimilarity index 91%\nrename from tests/core/block/test_block_manager_v2.py\nrename to tests/core/block/test_block_manager.py\nindex e67883367..cfd749ad5 100644\n--- a/tests/core/block/test_block_manager_v2.py\n+++ b/tests/core/block/test_block_manager.py\n@@ -2,7 +2,7 @@ import pytest\n \n from vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                                    STR_NOT_IMPL_ENC_DEC_SWA)\n-from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n+from vllm.core.block_manager import SelfAttnBlockSpaceManager\n from vllm.core.interfaces import AllocStatus\n from vllm.sequence import Logprob, SequenceStatus\n from vllm.utils import chunk_list\n@@ -17,7 +17,7 @@ from ..utils import (create_dummy_prompt, create_seq_group,\n @pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\n def test_can_allocate_seq_group(block_size: int, num_seqs_per_group: int,\n                                 num_gpu_blocks: int, watermark: float):\n-    block_manager = BlockSpaceManagerV2(\n+    block_manager = SelfAttnBlockSpaceManager(\n         block_size=block_size,\n         num_gpu_blocks=num_gpu_blocks,\n         num_cpu_blocks=1024,\n@@ -63,7 +63,7 @@ def test_can_allocate_seq_group_encoder_decoder(block_size: int,\n                                                 num_seqs_per_group: int,\n                                                 num_gpu_blocks: int,\n                                                 watermark: float):\n-    block_manager = BlockSpaceManagerV2(\n+    block_manager = SelfAttnBlockSpaceManager(\n         block_size=block_size,\n         num_gpu_blocks=num_gpu_blocks,\n         num_cpu_blocks=1024,\n@@ -117,16 +117,16 @@ def test_can_allocate_encoder_decoder_fails_with_swa(block_size: int,\n     '''\n     SWA short for Sliding Window Attention.\n \n-    At time of writing block manager v2 does not support SWA.\n+    At time of writing block manager does not support SWA.\n \n-    However even when SWA is implemented for block manager v2,\n+    However even when SWA is implemented for block manager,\n     there will still most likely be a separate workstream required\n     to enable SWA for encoder/decoder models.\n \n     Therefore this test enforces that one of the following cases\n     hold true:\n-    1. Block manager v2 does not support SWA at all (true at time of writing)\n-    2. Block manager v2 fails with NotImplementError when SWA is enabled\n+    1. Block manager does not support SWA at all (true at time of writing)\n+    2. Block manager fails with NotImplementError when SWA is enabled\n        AND a SequenceGroup with an encoder sequence (i.e. in support of an\n        encoder/decoder model) is passed into can_allocate() as an argument\n \n@@ -135,7 +135,7 @@ def test_can_allocate_encoder_decoder_fails_with_swa(block_size: int,\n     '''\n \n     with pytest.raises((NotImplementedError, AssertionError)) as exc_info:\n-        block_manager = BlockSpaceManagerV2(\n+        block_manager = SelfAttnBlockSpaceManager(\n             block_size=block_size,\n             num_gpu_blocks=num_gpu_blocks,\n             num_cpu_blocks=1024,\n@@ -158,7 +158,7 @@ def test_can_allocate_encoder_decoder_fails_with_swa(block_size: int,\n         block_manager.can_allocate(seq_group)\n \n     # Assert that either\n-    # 1. Block manager v2 constructor fails with assertion that sliding window\n+    # 1. Block manager constructor fails with assertion that sliding window\n     #    is not yet supported (most likely near-term outcome at time of\n     #    writing), or\n     # 2. can_allocate() fails with NotImplementedError due to combination of\n@@ -177,7 +177,7 @@ def test_can_allocate_encoder_decoder_fails_with_prefix_cache(\n         block_size: int, num_seqs_per_group: int, num_gpu_blocks: int,\n         watermark: float):\n \n-    block_manager = BlockSpaceManagerV2(\n+    block_manager = SelfAttnBlockSpaceManager(\n         block_size=block_size,\n         num_gpu_blocks=num_gpu_blocks,\n         num_cpu_blocks=1024,\n@@ -217,7 +217,7 @@ def test_append_slots(block_size, prompt_len, num_slots_to_append,\n \n     num_gpu_blocks = 1024\n     watermark = 0.1\n-    block_manager = BlockSpaceManagerV2(\n+    block_manager = SelfAttnBlockSpaceManager(\n         block_size=block_size,\n         num_gpu_blocks=num_gpu_blocks,\n         num_cpu_blocks=0,\n@@ -269,14 +269,15 @@ def test_swap(block_size, num_cpu_blocks, num_gpu_blocks, num_lookahead_slots,\n     \"\"\"Verify blocks number on src/desc device is correct after swapping in/out\n         sequence group (not missing or extra blocks).\n     \"\"\"\n-    block_manager = BlockSpaceManagerV2(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=enable_caching)\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n     prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n     prompt.status = SequenceStatus.WAITING\n     block_manager.allocate(seq_group)\n+\n     # Emulate a forward pass by appending a single token.\n     # The block manager then knows how many unprocessed\n     # tokens will be written in the next forward pass.\n@@ -321,11 +322,11 @@ def test_can_swap(block_size, num_gpu_blocks, num_lookahead_slots,\n         can be swapped in/out.\n     \"\"\"\n     num_cpu_blocks = num_gpu_blocks\n-    block_manager = BlockSpaceManagerV2(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=enable_caching)\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n     prompt, seq_group = create_dummy_prompt(\n         \"1\", prompt_length=(num_gpu_blocks - 1) * block_size - 1)\n     prompt.status = SequenceStatus.WAITING\n@@ -382,11 +383,11 @@ def test_swap_in_infeasible(num_lookahead_slots, enable_caching):\n     block_size = 8\n     num_cpu_blocks = 1\n     num_gpu_blocks = 1\n-    block_manager = BlockSpaceManagerV2(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=enable_caching)\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n     prompt_length = block_size - 3\n     assert prompt_length > 0\n     prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=prompt_length)\n@@ -434,7 +435,7 @@ def test_sliding_window(block_size, prompt_len, num_slots_to_append,\n \n     num_gpu_blocks = 1024\n     watermark = 0.1\n-    block_manager = BlockSpaceManagerV2(\n+    block_manager = SelfAttnBlockSpaceManager(\n         block_size=block_size,\n         num_gpu_blocks=num_gpu_blocks,\n         num_cpu_blocks=0,\n@@ -474,7 +475,7 @@ def test_sliding_window(block_size, prompt_len, num_slots_to_append,\n     seq.data.update_num_computed_tokens(prompt_len)\n     check_used(num_blocks(prompt_len))\n \n-    # this is how we compute it in BlockSpaceManagerV2.__init__\n+    # this is how we compute it in SelfAttnBlockSpaceManager.__init__\n     sliding_blocks = (sliding_window // block_size) + 2\n     # plus one block for null block\n     sliding_blocks += 1\ndiff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\ndeleted file mode 100644\nindex 2ee9f2082..000000000\n--- a/tests/core/test_block_manager.py\n+++ /dev/null\n@@ -1,637 +0,0 @@\n-import time\n-from collections import defaultdict\n-from typing import List\n-\n-import pytest\n-\n-from vllm import SamplingParams\n-from vllm.block import PhysicalTokenBlock\n-from vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n-                                   STR_NOT_IMPL_ENC_DEC_SWA)\n-from vllm.core.block_manager_v1 import (BlockSpaceManagerV1,\n-                                        UncachedBlockAllocator)\n-from vllm.core.interfaces import AllocStatus\n-from vllm.sequence import Logprob, Sequence, SequenceGroup, SequenceStatus\n-from vllm.utils import Device\n-\n-from .utils import create_dummy_prompt, create_dummy_prompt_encoder_decoder\n-\n-\n-def test_block_allocator_allocate():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n-                                           num_cpu_blocks)\n-\n-    # Allocate all available cpu blocks.\n-    num_free = num_cpu_blocks\n-    assert cpu_allocator.get_num_free_blocks() == num_free\n-    for _ in range(num_cpu_blocks):\n-        block = cpu_allocator.allocate()\n-        num_free -= 1\n-\n-        assert block not in cpu_allocator.free_blocks\n-        assert cpu_allocator.get_num_free_blocks() == num_free\n-\n-    with pytest.raises(ValueError):\n-        cpu_allocator.allocate()\n-\n-\n-def test_block_allocator_free():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n-                                           num_cpu_blocks)\n-\n-    # Allocate all available cpu blocks.\n-    blocks: List[PhysicalTokenBlock] = []\n-    for _ in range(num_cpu_blocks):\n-        block = cpu_allocator.allocate()\n-        blocks.append(block)\n-        assert block not in cpu_allocator.free_blocks\n-\n-    # Free all allocated cpu blocks.\n-    num_free = 0\n-    assert cpu_allocator.get_num_free_blocks() == num_free\n-    for block in blocks:\n-        cpu_allocator.free(block)\n-        num_free += 1\n-        assert block in cpu_allocator.free_blocks\n-        assert cpu_allocator.get_num_free_blocks() == num_free\n-\n-        with pytest.raises(ValueError):\n-            cpu_allocator.free(block)\n-\n-\n-def test_allocate():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    for i in range(num_gpu_blocks):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    # Use watermark to reserve one gpu block.\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=1 / num_gpu_blocks)\n-    for i in range(num_gpu_blocks - 1):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-\n-def test_allocate_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_req_per_seq_group = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    for i in range(num_gpu_blocks // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            str(i),\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    # Use watermark to reserve one gpu block.\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=1 / num_gpu_blocks)\n-    for i in range((num_gpu_blocks - 1) // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            str(i),\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-\n-def test_allocate_encoder_decoder_fails_with_swa():\n-    # SWA short for sliding window attention\n-\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        sliding_window=5)  # swa\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-        \"0\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-\n-    # Assert that can_allocate() fails due to SWA\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.can_allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n-\n-    # Assert that allocate() fails due to SWA\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n-\n-\n-def test_allocate_encoder_decoder_fails_with_prefix_caching():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=True)  # Prefix cache\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-        \"0\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-\n-    # Assert that can_allocate() fails due to prefix caching\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.can_allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n-\n-    # Assert that allocate() fails due to prefix caching\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n-\n-\n-def test_append_slot_single_seq():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate single seq to gpu block.\n-    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Nothing to append. Sequence has no new logical blocks.\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert not block_manager.append_slots(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks == after_blocks\n-\n-    # Add block_size number of new tokens and append slot.\n-    for i in range(block_size):\n-        token_id = i + 5\n-        prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert not block_manager.append_slots(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks - after_blocks == 1\n-\n-\n-def test_append_slot_cow():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size=block_size,\n-                                        num_cpu_blocks=num_cpu_blocks,\n-                                        num_gpu_blocks=num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate prompt to gpu block. There is one slot left in the block.\n-    prompt = Sequence(seq_id=1,\n-                      inputs={\n-                          \"prompt\": \"one two three\",\n-                          \"prompt_token_ids\": [1, 2, 3],\n-                      },\n-                      block_size=block_size)\n-\n-    # Fork the sequence, such that a COW will be required when we append a new\n-    # token id.\n-    child = prompt.fork(new_seq_id=2)\n-\n-    # Allocate space for the sequence group.\n-    seq_group = SequenceGroup(request_id=\"1\",\n-                              seqs=[prompt, child],\n-                              arrival_time=time.time(),\n-                              sampling_params=SamplingParams())\n-    block_manager.allocate(seq_group)\n-\n-    # Fork and append a new token id. We expect a COW to be scheduled.\n-    token_id = 4\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.fork(prompt, child)\n-\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-\n-    cows = block_manager.append_slots(child)\n-    assert cows\n-    dict_cows = defaultdict(list)\n-    for src_block, dst_block in cows:\n-        dict_cows[src_block].append(dst_block)\n-    for src_block, dst_blocks in dict_cows.items():\n-        assert src_block not in dst_blocks\n-\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks - after_blocks == 1\n-\n-\n-def test_fork():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\",\n-                                            block_size - 1,\n-                                            block_size=block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Fork prompt and copy block tables.\n-    child = prompt.fork(2)\n-    block_manager.fork(prompt, child)\n-    assert block_manager.get_block_table(\n-        prompt) == block_manager.get_block_table(child)\n-    token_id = 4\n-    # Append token to child. Block is shared so copy on write occurs.\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(child)\n-    assert block_manager.get_block_table(\n-        prompt) != block_manager.get_block_table(child)\n-\n-\n-def test_swap():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n-    prompt.status = SequenceStatus.WAITING\n-    block_manager.allocate(seq_group)\n-\n-    # Emulate a forward pass by appending a single token.\n-    # The block manager then knows how many unprocessed\n-    # tokens will be written in the next forward pass.\n-    token_id = 0\n-    prompt.status = SequenceStatus.RUNNING\n-    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    # Swap seq group from GPU -> CPU.\n-    gpu_blocks = block_manager.get_block_table(prompt)\n-    assert block_manager.can_swap_out(seq_group)\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_out(seq_group)\n-    assert [x[0] for x in mapping] == gpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n-    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n-    prompt.status = SequenceStatus.SWAPPED\n-\n-    # Swap seq group from CPU -> GPU.\n-    cpu_blocks = block_manager.get_block_table(prompt)\n-    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_in(seq_group)\n-    assert [x[0] for x in mapping] == cpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n-    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n-\n-\n-def test_swap_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    decoder_prompt, encoder_prompt, seq_group = \\\n-        create_dummy_prompt_encoder_decoder(\n-        \"1\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-    decoder_prompt.status = SequenceStatus.WAITING\n-    encoder_prompt.status = SequenceStatus.WAITING\n-    block_manager.allocate(seq_group)\n-\n-    # Emulate a forward pass by appending a single token.\n-    # The block manager then knows how many unprocessed\n-    # tokens will be written in the next forward pass.\n-    token_id = 0\n-    decoder_prompt.status = SequenceStatus.RUNNING\n-    decoder_prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    # Swap encoder/decoder seq group from GPU -> CPU.\n-    decoder_gpu_blocks = block_manager.get_block_table(decoder_prompt)\n-    cross_gpu_blocks = block_manager.get_cross_block_table(seq_group)\n-    gpu_blocks = decoder_gpu_blocks + cross_gpu_blocks\n-    assert block_manager.can_swap_out(seq_group)\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_out(seq_group)\n-    assert [x[0] for x in mapping] == gpu_blocks\n-    #assert list(mapping.keys()) == gpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n-    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n-    decoder_prompt.status = SequenceStatus.SWAPPED\n-\n-    # Swap encoder/decoder seq group from CPU -> GPU.\n-    decoder_cpu_blocks = block_manager.get_block_table(decoder_prompt)\n-    cross_cpu_blocks = block_manager.get_cross_block_table(seq_group)\n-    cpu_blocks = decoder_cpu_blocks + cross_cpu_blocks\n-    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_in(seq_group)\n-    assert [x[0] for x in mapping] == cpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n-    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n-\n-\n-def test_free():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Free allocated seq.\n-    prompt_blocks = len(block_manager.get_block_table(prompt))\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    block_manager.free(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert after_blocks == before_blocks + prompt_blocks\n-\n-    # Block table for freed seq is deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(prompt)\n-\n-\n-def test_free_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    decoder_prompt, encoder_prompt, seq_group = \\\n-        create_dummy_prompt_encoder_decoder(\n-        \"1\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Free allocated seq.\n-    decoder_prompt_blocks = len(block_manager.get_block_table(decoder_prompt))\n-    encoder_prompt_blocks = len(block_manager.get_cross_block_table(seq_group))\n-    prompt_blocks = decoder_prompt_blocks + encoder_prompt_blocks\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    block_manager.free(decoder_prompt)\n-    block_manager.free_cross(seq_group)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert after_blocks == before_blocks + prompt_blocks\n-\n-    # Block table for freed encoder & decoder seq's are deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(decoder_prompt)\n-\n-    # Block table for freed encoder & decoder seq's are deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(encoder_prompt)\n-\n-\n-def test_reset():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same seq group on all available gpu blocks.\n-    original_blocks = block_manager.get_num_free_gpu_blocks()\n-    for i in range(num_gpu_blocks):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        block_manager.allocate(seq_group)\n-    assert block_manager.get_num_free_gpu_blocks() == 0\n-\n-    # Resetting block manager frees all allocated blocks.\n-    block_manager.reset()\n-    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n-\n-\n-def test_reset_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_req_per_seq_group = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same seq group on all available gpu blocks.\n-    original_blocks = block_manager.get_num_free_gpu_blocks()\n-    for i in range(num_gpu_blocks // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            f\"{i}\",\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        block_manager.allocate(seq_group)\n-    assert block_manager.get_num_free_gpu_blocks() == 0\n-\n-    # Resetting block manager frees all allocated blocks.\n-    block_manager.reset()\n-    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n-\n-\n-def test_sliding_window_multi_seq():\n-    \"\"\"\n-    Tests that memory allocation and deallocation is handled\n-    correctly with multiple sequences that exceed the sliding\n-    window's capacity.\n-    \"\"\"\n-    block_size = 1\n-    num_cpu_blocks = 8\n-    num_gpu_blocks = 8\n-    sliding_window = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        sliding_window=sliding_window,\n-                                        watermark=0)\n-\n-    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n-\n-    parent = Sequence(seq_id=1,\n-                      inputs={\n-                          \"prompt\": \"one two three\",\n-                          \"prompt_token_ids\": [0, 1, 2],\n-                      },\n-                      block_size=block_size)\n-    seq_group = SequenceGroup(request_id=\"1\",\n-                              seqs=[parent],\n-                              arrival_time=time.time(),\n-                              sampling_params=SamplingParams(),\n-                              lora_request=None)\n-    block_manager.allocate(seq_group)\n-\n-    # assert the number of blocks allocated is correct\n-    # the parent seq has len 3, but since sliding_window is 2,\n-    # we will use at most 2 blocks\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # Fork prompt and copy block tables.\n-    child = parent.fork(2)\n-    block_manager.fork(parent, child)\n-\n-    # assert the number of blocks allocated is correct\n-    # forking does not increase memory consumption\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # assert both parent and child share all blocks\n-    assert block_manager.get_block_table(\n-        parent) == block_manager.get_block_table(child)\n-\n-    token_id = 4\n-    # Append token to child. Block is shared so copy on write occurs.\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(child)\n-\n-    # assert the number of blocks allocated is correct\n-    # we will use now one block more. Each seq will use 2 blocks,\n-    # but only one can be shared\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window - 1\n-\n-    token_id = 5\n-    parent.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(parent)\n-\n-    # assert the number of blocks allocated is correct\n-    # no change, because both sequences are still just sharing one block\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window - 1\n-\n-    block_table_parent = block_manager.get_block_table(parent)\n-    block_table_child = block_manager.get_block_table(child)\n-\n-    assert block_table_parent != block_table_child\n-\n-    # assert both blocks are sharing the second-last block\n-    assert block_table_parent[-2] == block_table_child[-2]\n-\n-    # now let's clean up...\n-    block_manager.free(parent)\n-\n-    # assert the number of blocks allocated is correct\n-    # We have freed one seq, reducing the ref count of two blocks by one.\n-    # One of the two was only used by the parent seq, so this is now free.\n-    # The child seq still consumes sliding_window blocks\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # free all blocks\n-    block_manager.free(child)\n-\n-    # assert all blocks are free now\n-    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n-\n-\n-def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n-    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n-    should only mark a chunk of blocks as computed instead of all blocks.\n-    \"\"\"\n-\n-    block_size = 4\n-    num_cpu_blocks = 0\n-    num_gpu_blocks = 16\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_gpu_blocks,\n-                                        num_cpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=True)\n-\n-    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n-    prompt_length = block_size * num_gpu_blocks - 1\n-\n-    # Allocate (reserve) all blocks.\n-    _, seq_group = create_dummy_prompt(\"0\",\n-                                       prompt_length,\n-                                       block_size=block_size)\n-    block_manager.allocate(seq_group)\n-    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n-\n-    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n-    token_chunk_size = int(block_size * 2.5)\n-    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n-    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n-    assert len(computed_blocks) == 2\n-\n-    # Actual computed tokens.\n-    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n-\n-    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n-    token_chunk_size = int(block_size * 4.5)\n-    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n-    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n-    assert len(computed_blocks) == 7\ndiff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex c9495fd50..f97caa06f 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -8,7 +8,6 @@ from vllm.core.interfaces import AllocStatus\n from vllm.core.scheduler import Scheduler\n from vllm.sequence import Logprob, SequenceGroup\n \n-from ..utils import check_deprecated_block_manager_usage\n from .utils import create_dummy_prompt\n \n \n@@ -28,25 +27,16 @@ def schedule_and_update_computed_tokens(scheduler):\n     return metas, out\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/test_chunked_prefill_scheduler.py')\n-\n-\n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_simple(use_v2_block_manager: bool):\n+def test_simple():\n     \"\"\"Verify basic scheduling works.\"\"\"\n     block_size = 4\n     num_seq_group = 4\n     max_model_len = 16\n     max_num_batched_tokens = 64\n-    scheduler_config = SchedulerConfig(\n-        max_num_batched_tokens,\n-        num_seq_group,\n-        max_model_len,\n-        enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n+                                       num_seq_group,\n+                                       max_model_len,\n+                                       enable_chunked_prefill=True)\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -81,8 +71,7 @@ def test_simple(use_v2_block_manager: bool):\n     assert len(seq_group_meta) == num_seq_group\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunk(use_v2_block_manager: bool):\n+def test_chunk():\n     \"\"\"Verify prefills are chunked properly.\"\"\"\n     block_size = 4\n     max_seqs = 60\n@@ -93,7 +82,7 @@ def test_chunk(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 32\n     cache_config.num_gpu_blocks = 32\n@@ -131,8 +120,7 @@ def test_chunk(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 57\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_complex(use_v2_block_manager: bool):\n+def test_complex():\n     block_size = 4\n     max_seqs = 60\n     max_model_len = 80\n@@ -142,7 +130,7 @@ def test_complex(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 64\n     cache_config.num_gpu_blocks = 64\n@@ -201,8 +189,7 @@ def test_complex(use_v2_block_manager: bool):\n     assert running[2].is_prefill()\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_maximal_decoding(use_v2_block_manager: bool):\n+def test_maximal_decoding():\n     \"\"\"Verify decoding requests are prioritized.\"\"\"\n     block_size = 4\n     max_seqs = 2\n@@ -213,7 +200,7 @@ def test_maximal_decoding(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -295,8 +282,7 @@ def test_maximal_decoding(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 2\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prompt_limit(use_v2_block_manager: bool):\n+def test_prompt_limit():\n     \"\"\"Verify max_num_batched_tokens < max_model_len is possible.\"\"\"\n     block_size = 4\n     max_seqs = 32\n@@ -307,7 +293,7 @@ def test_prompt_limit(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -330,8 +316,7 @@ def test_prompt_limit(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 32\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prompt_limit_exceed(use_v2_block_manager: bool):\n+def test_prompt_limit_exceed():\n     block_size = 4\n     max_seqs = 64\n     max_model_len = 32\n@@ -356,8 +341,7 @@ def test_prompt_limit_exceed(use_v2_block_manager: bool):\n     assert out.ignored_seq_groups[0] == seq_group\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_swap(use_v2_block_manager: bool):\n+def test_swap():\n     \"\"\"Verify swapping works with chunked prefill requests\"\"\"\n     block_size = 4\n     max_seqs = 30\n@@ -368,7 +352,7 @@ def test_swap(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -414,8 +398,7 @@ def test_swap(use_v2_block_manager: bool):\n     assert out.blocks_to_swap_out == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n+def test_running_prefill_prioritized_over_swap():\n     block_size = 4\n     max_seqs = 30\n     max_model_len = 200\n@@ -425,7 +408,7 @@ def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 32\n     cache_config.num_gpu_blocks = 32\n@@ -508,8 +491,7 @@ def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n     assert out.blocks_to_swap_out == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n+def test_chunked_prefill_preempt():\n     \"\"\"Verify preempt works with chunked prefill requests\"\"\"\n     block_size = 4\n     max_seqs = 30\n@@ -520,7 +502,7 @@ def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -575,8 +557,7 @@ def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == max_num_batched_tokens\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n+def test_chunked_prefill_max_seqs():\n     block_size = 4\n     max_seqs = 2\n     max_model_len = 80\n@@ -586,7 +567,7 @@ def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 128\n     cache_config.num_gpu_blocks = 128\n@@ -629,8 +610,7 @@ def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n     assert not running[1].is_prefill()\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_perfix_caching(use_v2_block_manager: bool):\n+def test_perfix_caching():\n     \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n     block_size = 4\n     max_seqs = 10\n@@ -641,7 +621,7 @@ def test_perfix_caching(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size,\n                                1.0,\n                                1,\ndiff --git a/tests/core/test_num_computed_tokens_update.py b/tests/core/test_num_computed_tokens_update.py\nindex f3ec24e7b..bd4accab7 100644\n--- a/tests/core/test_num_computed_tokens_update.py\n+++ b/tests/core/test_num_computed_tokens_update.py\n@@ -31,7 +31,6 @@ def test_num_computed_tokens_update(num_scheduler_steps: int,\n     # Make a vllm engine\n     runner = VllmRunner(model_name=MODEL,\n                         gpu_memory_utilization=0.7,\n-                        use_v2_block_manager=True,\n                         num_scheduler_steps=num_scheduler_steps,\n                         enable_chunked_prefill=enable_chunked_prefill,\n                         enforce_eager=enforce_eager)\ndiff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 5cdf743a4..defa6c1bd 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -3,7 +3,7 @@ from collections import deque\n from typing import List, Set, Tuple\n from unittest.mock import MagicMock\n \n-import pytest\n+import pytest  # noqa\n from torch import Use  # noqa\n \n from vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\n@@ -12,23 +12,18 @@ from vllm.core.scheduler import Scheduler, SchedulingBudget\n from vllm.lora.request import LoRARequest\n from vllm.sequence import SequenceGroup, SequenceStatus\n \n-from ..utils import check_deprecated_block_manager_usage\n from .utils import (append_new_token, append_new_token_seq_group,\n                     create_dummy_prompt, get_sequence_groups,\n                     schedule_and_update_computed_tokens)\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        \"tests/core/test_chunked_prefill_scheduler.py\")\n-\n-\n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_add_seq_group(use_v2_block_manager: bool):\n+def test_scheduler_add_seq_group():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n-        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n+        100,\n+        64,\n+        1,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, cache_dtype=\"auto\")\n     cache_config.num_cpu_blocks = 4\n     cache_config.num_gpu_blocks = 4\n@@ -44,11 +39,13 @@ def test_scheduler_add_seq_group(use_v2_block_manager: bool):\n         assert scheduler.get_num_unfinished_seq_groups() == i + 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_abort_seq_group(use_v2_block_manager: bool):\n+def test_scheduler_abort_seq_group():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n-        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n+        100,\n+        64,\n+        1,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 4\n     cache_config.num_gpu_blocks = 4\n@@ -68,8 +65,7 @@ def test_scheduler_abort_seq_group(use_v2_block_manager: bool):\n     assert scheduler.get_num_unfinished_seq_groups() == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n+def test_scheduler_schedule_simple():\n     block_size = 4\n     num_seq_group = 4\n     max_model_len = 16\n@@ -77,7 +73,7 @@ def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n         64,\n         num_seq_group,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -112,8 +108,7 @@ def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n     append_new_token(out, 1)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n+def test_scheduler_prefill_prioritized():\n     \"\"\"Verify running batched tokens are not applied to prefill requests.\"\"\"\n     block_size = 4\n     max_model_len = 30\n@@ -122,7 +117,7 @@ def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n         max_batched_num_tokens,\n         2,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -146,12 +141,14 @@ def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n     assert get_sequence_groups(out) == [seq_group_b]\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_schedule_preempt_abort(use_v2_block_manager: bool):\n+def test_scheduler_schedule_preempt_abort():\n     block_size = 4\n     max_model_len = 16\n     scheduler_config = SchedulerConfig(\n-        64, 2, max_model_len, use_v2_block_manager=use_v2_block_manager)\n+        64,\n+        2,\n+        max_model_len,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 2\n     cache_config.num_gpu_blocks = 2\n@@ -201,8 +198,7 @@ def test_scheduler_schedule_preempt_abort(use_v2_block_manager: bool):\n     assert scheduler.get_num_unfinished_seq_groups() == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_max_seqs(use_v2_block_manager: bool):\n+def test_scheduler_max_seqs():\n     block_size = 4\n     num_seq_group = 4\n     max_seq_group = 2\n@@ -211,7 +207,7 @@ def test_scheduler_max_seqs(use_v2_block_manager: bool):\n         64,\n         max_seq_group,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -249,15 +245,14 @@ def test_scheduler_max_seqs(use_v2_block_manager: bool):\n     assert set(get_sequence_groups(out)) == set([all_seq_groups[1]])\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_delay_factor(use_v2_block_manager: bool):\n+def test_scheduler_delay_factor():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n         100,\n         64,\n         16,\n         delay_factor=0.5,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -294,12 +289,10 @@ def test_scheduler_delay_factor(use_v2_block_manager: bool):\n     append_new_token(out, 1)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_swapped_out_prioritized(use_v2_block_manager: bool):\n+def test_swapped_out_prioritized():\n     block_size = 4\n     scheduler = initialize_scheduler(max_num_seqs=6,\n                                      block_size=block_size,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     # best_of=2 * 3 == 6 sequences.\n@@ -351,7 +344,6 @@ def initialize_scheduler(\n     max_token_budget=1000,\n     max_model_len=1000,\n     lora_config=None,\n-    use_v2_block_manager=False,\n     block_size=4,\n     num_cpu_blocks=8,\n     num_gpu_blocks=8,\n@@ -361,7 +353,7 @@ def initialize_scheduler(\n         max_token_budget,\n         max_num_seqs,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = num_cpu_blocks\n     cache_config.num_gpu_blocks = num_gpu_blocks\n@@ -386,15 +378,12 @@ def add_token_budget(budget: SchedulingBudget,\n     budget.add_num_seqs(mock_seq_group.request_id, num_curr_seqs)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_prompt_len(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_prompt_len():\n     \"\"\"\n     Test prompt longer than max_prompt_len is aborted.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(max_model_len=30,\n-                                     use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size)\n+    scheduler = initialize_scheduler(max_model_len=30, block_size=block_size)\n     _, seq_group = create_dummy_prompt(\"0\",\n                                        prompt_length=60,\n                                        block_size=block_size)\n@@ -409,14 +398,12 @@ def test_prefill_schedule_max_prompt_len(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n+def test_prefill_schedule_token_budget():\n     \"\"\"\n     Test token budget respected.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     budget = create_token_budget(token_budget=0)\n@@ -446,8 +433,7 @@ def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 1\n \n     # Test when current_batched_tokens respected.\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=16,\n                                      num_gpu_blocks=16)\n     budget = create_token_budget(token_budget=60)\n@@ -474,14 +460,12 @@ def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_seqs(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_seqs():\n     \"\"\"\n     Test max seq respected.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     budget = create_token_budget(max_num_seqs=2)\n@@ -515,15 +499,13 @@ def test_prefill_schedule_max_seqs(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_lora(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_lora():\n     \"\"\"\n     Test max lora is respected and prioritized.\n     \"\"\"\n     block_size = 4\n     lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n     scheduler = initialize_scheduler(lora_config=lora_config,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n@@ -570,14 +552,12 @@ def test_prefill_schedule_max_lora(use_v2_block_manager: bool):\n     assert budget.num_batched_tokens == 60\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_no_block_manager_capacity(use_v2_block_manager):\n+def test_prefill_schedule_no_block_manager_capacity():\n     \"\"\"\n     Test sequence cannot be scheduled due to block manager has no capacity.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_gpu_blocks=128,\n                                      num_cpu_blocks=128)\n     budget = create_token_budget()\n@@ -614,14 +594,12 @@ def test_prefill_schedule_no_block_manager_capacity(use_v2_block_manager):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_decode_schedule_preempted(use_v2_block_manager: bool):\n+def test_decode_schedule_preempted():\n     \"\"\"\n     Test decodes cannot be scheduled and preempted.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     curr_loras = None\n@@ -660,14 +638,12 @@ def test_decode_schedule_preempted(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_decode_swap_beam_search(use_v2_block_manager: bool):\n+def test_decode_swap_beam_search():\n     \"\"\"\n     Test best_of > 1 swap out blocks\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_gpu_blocks=64,\n                                      num_cpu_blocks=64)\n     curr_loras = None\n@@ -716,14 +692,12 @@ def test_decode_swap_beam_search(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_decode_blocks_to_copy_update(use_v2_block_manager: bool):\n+def test_schedule_decode_blocks_to_copy_update():\n     \"\"\"\n     Verify blocks_to_copy is updated.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=4,\n+    scheduler = initialize_scheduler(block_size=4,\n                                      num_cpu_blocks=16,\n                                      num_gpu_blocks=16)\n     _, seq_group = create_dummy_prompt(\"1\",\n@@ -754,11 +728,9 @@ def test_schedule_decode_blocks_to_copy_update(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == [(2, 3)]\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_simple(use_v2_block_manager: bool):\n+def test_schedule_swapped_simple():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size)\n+    scheduler = initialize_scheduler(block_size=block_size)\n     curr_loras = None\n     blocks_to_swap_out: List[Tuple[int, int]] = []\n     _, seq_group = create_dummy_prompt(\"1\",\n@@ -785,11 +757,9 @@ def test_schedule_swapped_simple(use_v2_block_manager: bool):\n     assert blocks_to_swap_out == blocks_to_swap_in_reverse\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_token_budget(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_token_budget():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -822,11 +792,9 @@ def test_schedule_swapped_max_token_budget(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_seqs(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_seqs():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     curr_loras = None\n@@ -859,12 +827,10 @@ def test_schedule_swapped_max_seqs(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_loras(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_loras():\n     block_size = 4\n     lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n     scheduler = initialize_scheduler(lora_config=lora_config,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n@@ -894,11 +860,9 @@ def test_schedule_swapped_max_loras(use_v2_block_manager: bool):\n     assert len(curr_loras) == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_cannot_swap_in(use_v2_block_manager: bool):\n+def test_schedule_swapped_cannot_swap_in():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -927,11 +891,9 @@ def test_schedule_swapped_cannot_swap_in(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_infeasible_swap(use_v2_block_manager: bool):\n+def test_infeasible_swap():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -961,11 +923,9 @@ def test_infeasible_swap(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_blocks_to_copy(use_v2_block_manager: bool):\n+def test_schedule_swapped_blocks_to_copy():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\ndiff --git a/tests/metrics/test_metrics.py b/tests/metrics/test_metrics.py\nindex f1003221a..8798ff078 100644\n--- a/tests/metrics/test_metrics.py\n+++ b/tests/metrics/test_metrics.py\n@@ -185,13 +185,14 @@ def test_metric_spec_decode(\n ) -> None:\n     k = 5\n \n-    with vllm_runner(model,\n-                     dtype=dtype,\n-                     disable_log_stats=False,\n-                     gpu_memory_utilization=0.4,\n-                     speculative_model=model,\n-                     num_speculative_tokens=k,\n-                     use_v2_block_manager=True) as vllm_model:\n+    with vllm_runner(\n+            model,\n+            dtype=dtype,\n+            disable_log_stats=False,\n+            gpu_memory_utilization=0.4,\n+            speculative_model=model,\n+            num_speculative_tokens=k,\n+    ) as vllm_model:\n \n         # Force log interval to be 0 to catch all metrics.\n         stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n@@ -242,7 +243,6 @@ def test_metric_spec_decode_interval(\n                              gpu_memory_utilization=0.4,\n                              speculative_model=model,\n                              num_speculative_tokens=k,\n-                             use_v2_block_manager=True,\n                              enforce_eager=True)\n \n     engine = LLMEngine.from_engine_args(engine_args)\ndiff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex 000c923ef..7203d635c 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -17,7 +17,6 @@ NUM_PROMPTS = [10]\n \n DEFAULT_SERVER_ARGS: List[str] = [\n     \"--disable-log-requests\",\n-    \"--use-v2-block-manager\",\n     \"--worker-use-ray\",\n     \"--gpu-memory-utilization\",\n     \"0.85\",\ndiff --git a/tests/multi_step/test_correctness_llm.py b/tests/multi_step/test_correctness_llm.py\nindex f45428675..cc1fd1925 100644\n--- a/tests/multi_step/test_correctness_llm.py\n+++ b/tests/multi_step/test_correctness_llm.py\n@@ -76,7 +76,6 @@ def test_multi_step_llm(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             enable_chunked_prefill=enable_chunked_prefill,\n             num_scheduler_steps=num_scheduler_steps,\n     ) as vllm_model:\n@@ -169,7 +168,6 @@ def test_multi_step_llm_w_prompt_logprobs(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             num_scheduler_steps=num_scheduler_steps,\n     ) as vllm_model:\n         vllm_outputs = vllm_model.generate_greedy_logprobs(\n@@ -305,7 +303,6 @@ def test_multi_step_llm_chunked_prefill_prefix_cache(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             num_scheduler_steps=num_scheduler_steps,\n             max_model_len=48,\n             max_num_batched_tokens=48,\n@@ -324,7 +321,6 @@ def test_multi_step_llm_chunked_prefill_prefix_cache(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             enable_chunked_prefill=True,\n             enable_prefix_caching=True,\n             num_scheduler_steps=num_scheduler_steps,\ndiff --git a/tests/prefix_caching/test_prefix_caching.py b/tests/prefix_caching/test_prefix_caching.py\nindex 88437425f..366b030ea 100644\n--- a/tests/prefix_caching/test_prefix_caching.py\n+++ b/tests/prefix_caching/test_prefix_caching.py\n@@ -2,15 +2,9 @@\n \n Run `pytest tests/prefix_caching/test_prefix_caching.py`.\n \"\"\"\n-from typing import List\n-\n import pytest\n \n from tests.kernels.utils import override_backend_env_variable\n-from tests.utils import check_deprecated_block_manager_usage\n-from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager_v1 import CachedBlockAllocator\n-from vllm.utils import Device\n \n from ..models.utils import check_outputs_equal\n \n@@ -19,92 +13,11 @@ MODELS = [\n ]\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/prefix_caching/test_prefix_caching.py')\n-\n-\n-@pytest.mark.parametrize(\"block_size\", [16])\n-@pytest.mark.parametrize(\"num_blocks\", [16])\n-def test_block_allocator(\n-    block_size: int,\n-    num_blocks: int,\n-):\n-    block_hash = 1\n-    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n-\n-    # Allocate two PysicalTokenBlocks with the same hash and check\n-    # that they are the same PhysicalTokenBlock\n-    first_block = block_allocator.allocate(block_hash, 0)\n-    second_block = block_allocator.allocate(block_hash, 0)\n-    assert (first_block == second_block)\n-    assert (second_block.ref_count == 2)\n-\n-    # Check metric: 1 hit of 2 queries\n-    assert block_allocator.get_prefix_cache_hit_rate() == 0.5\n-\n-    # Free the first_block and confirm that the ref_count is correctly\n-    # decremented on the second block\n-    block_allocator.free(first_block)\n-    assert (second_block.ref_count == 1)\n-\n-    # Free the second block\n-    block_allocator.free(second_block)\n-\n-    # Reallocate the first block and confirm that, even after the block\n-    # had its ref_count go to 0, we still get the same block back\n-    first_block = block_allocator.allocate(block_hash, 0)\n-    assert (first_block == second_block)\n-    assert (first_block.block_hash == block_hash)\n-\n-    # Allocate one more time to get 3/4 hit rate for easy checking\n-    block_allocator.allocate(block_hash, 0)\n-    assert block_allocator.get_prefix_cache_hit_rate() == 0.75\n-\n-\n-@pytest.mark.parametrize(\"num_blocks\", [16])\n-def test_eviction(num_blocks: int, ):\n-    block_size = 16\n-    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n-    blocks: List[PhysicalTokenBlock] = []\n-\n-    for i in range(num_blocks):\n-        # use i as the block_hash\n-        blocks.append(block_allocator.allocate(i, 0))\n-\n-    #Free all blocks\n-    for block in blocks:\n-        block_allocator.free(block)\n-\n-    # Allocate a new block and confirm that it's the first block freed.\n-    # I.E The Least Recently Used block\n-    new_block_hash = block_size\n-    new_block = block_allocator.allocate(new_block_hash, 0)\n-    assert (new_block == blocks[0])\n-    assert (new_block.block_hash == new_block_hash)\n-\n-    # Reallocate the second in blocks to remove it from the free list\n-    realloc_block_hash = 1\n-    realloc_block = block_allocator.allocate(realloc_block_hash, 0)\n-    assert (realloc_block == blocks[realloc_block_hash])\n-    assert (realloc_block.block_hash == realloc_block_hash)\n-\n-    # Allocate a new block and confirm that it's not the realloc_block,\n-    # since the realloc_block shouldn't be in the free list\n-    new_block_hash = block_size + 1\n-    new_block = block_allocator.allocate(new_block_hash, 0)\n-    assert (realloc_block != new_block)\n-    assert (new_block.block_hash == new_block_hash)\n-    assert (new_block.block_number == 2)\n-\n-\n @pytest.mark.parametrize(\"model\", MODELS)\n @pytest.mark.parametrize(\"backend\", [\"FLASH_ATTN\", \"FLASHINFER\", \"XFORMERS\"])\n @pytest.mark.parametrize(\"dtype\", [\"half\"])\n @pytest.mark.parametrize(\"max_tokens\", [5])\n @pytest.mark.parametrize(\"cached_position\", [0, 1])\n-@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n def test_mixed_requests(\n     hf_runner,\n     vllm_runner,\n@@ -114,7 +27,6 @@ def test_mixed_requests(\n     dtype: str,\n     max_tokens: int,\n     cached_position: int,\n-    use_v2_block_manager: bool,\n     monkeypatch,\n ) -> None:\n     \"\"\"\n@@ -132,7 +44,6 @@ def test_mixed_requests(\n             model,\n             dtype=dtype,\n             enable_prefix_caching=True,\n-            use_v2_block_manager=use_v2_block_manager,\n     ) as vllm_model:\n         # Run the first prompt so the cache is populated\n         vllm_outputs = vllm_model.generate_greedy([cached_prompt], max_tokens)\ndiff --git a/tests/spec_decode/e2e/test_compatibility.py b/tests/spec_decode/e2e/test_compatibility.py\nindex 69ea81cff..629074188 100644\n--- a/tests/spec_decode/e2e/test_compatibility.py\n+++ b/tests/spec_decode/e2e/test_compatibility.py\n@@ -1,27 +1,15 @@\n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import SamplingParams\n \n from .conftest import get_output_from_llm_generator\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/spec_decode/e2e/test_compatibility.py')\n-\n-\n-@pytest.mark.parametrize(\n-    \"common_llm_kwargs\",\n-    [{\n-        \"model\": \"JackFram/llama-68m\",\n-        \"speculative_model\": \"JackFram/llama-68m\",\n-        \"num_speculative_tokens\": 5,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n-    }])\n+@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n+    \"model\": \"JackFram/llama-68m\",\n+    \"speculative_model\": \"JackFram/llama-68m\",\n+    \"num_speculative_tokens\": 5,\n+}])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n         \"enable_chunked_prefill\": True,\n@@ -51,16 +39,11 @@ def test_spec_decode_xfail_chunked_prefill(test_llm_generator):\n                                       sampling_params)\n \n \n-@pytest.mark.parametrize(\n-    \"common_llm_kwargs\",\n-    [{\n-        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n-        \"speculative_model\": \"JackFram/llama-68m\",\n-        \"num_speculative_tokens\": 5,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n-    }])\n+@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n+    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n+    \"speculative_model\": \"JackFram/llama-68m\",\n+    \"num_speculative_tokens\": 5,\n+}])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n     [\n@@ -101,34 +84,3 @@ def test_spec_decode_xfail_spec_max_model_len(test_llm_generator):\n     with pytest.raises(ValueError, match=\"cannot be larger than\"):\n         get_output_from_llm_generator(test_llm_generator, prompts,\n                                       sampling_params)\n-\n-\n-@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n-    \"model\": \"JackFram/llama-68m\",\n-    \"speculative_model\": \"JackFram/llama-68m\",\n-    \"num_speculative_tokens\": 5,\n-    \"use_v2_block_manager\": False,\n-}])\n-@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"seed\", [1])\n-def test_spec_decode_xfail_block_manager_v1(test_llm_generator):\n-    \"\"\"Verify that speculative decoding with block manager v1 fails.\n-    \"\"\"\n-    output_len = 128\n-    temperature = 0.0\n-\n-    prompts = [\n-        \"Hello, my name is\",\n-    ]\n-\n-    sampling_params = SamplingParams(\n-        max_tokens=output_len,\n-        ignore_eos=True,\n-        temperature=temperature,\n-    )\n-\n-    with pytest.raises(ValueError,\n-                       match=\"Speculative decoding requires usage of the V2\"):\n-        get_output_from_llm_generator(test_llm_generator, prompts,\n-                                      sampling_params)\ndiff --git a/tests/spec_decode/e2e/test_eagle_correctness.py b/tests/spec_decode/e2e/test_eagle_correctness.py\nindex d7ca8815e..5bc70de9d 100644\n--- a/tests/spec_decode/e2e/test_eagle_correctness.py\n+++ b/tests/spec_decode/e2e/test_eagle_correctness.py\n@@ -43,9 +43,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -86,9 +83,6 @@ def test_eagle_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -143,9 +137,6 @@ def test_eagle_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n     [{\n         \"enforce_eager\": False,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -191,9 +182,6 @@ def test_eagle_e2e_greedy_correctness_cuda_graph(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -235,9 +223,6 @@ def test_eagle_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -283,9 +268,6 @@ def test_eagle_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \ndiff --git a/tests/spec_decode/e2e/test_integration.py b/tests/spec_decode/e2e/test_integration.py\nindex d04e31268..b89e58497 100644\n--- a/tests/spec_decode/e2e/test_integration.py\n+++ b/tests/spec_decode/e2e/test_integration.py\n@@ -12,8 +12,6 @@ MAIN_MODEL = \"JackFram/llama-68m\"\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n \n         # Verify equality when cuda graphs allowed.\n         \"enforce_eager\": False,\n@@ -57,9 +55,6 @@ def test_spec_decode_cuda_graph(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -111,9 +106,6 @@ def test_speculative_model_quantization_config(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": \"JackFram/llama-68m\",\n         \"num_speculative_tokens\": 3,\n     }])\ndiff --git a/tests/spec_decode/e2e/test_integration_dist_tp2.py b/tests/spec_decode/e2e/test_integration_dist_tp2.py\nindex 679a6ded9..b829d1a5b 100644\n--- a/tests/spec_decode/e2e/test_integration_dist_tp2.py\n+++ b/tests/spec_decode/e2e/test_integration_dist_tp2.py\n@@ -17,9 +17,6 @@ from .conftest import run_equality_correctness_test_tp\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"2\"\n     ]])\n@@ -74,9 +71,6 @@ def test_target_model_tp_gt_1(common_llm_kwargs, per_test_common_llm_kwargs,\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use_v2_block_manager\",\n         \"--tensor_parallel_size\",\n         \"2\",\n \ndiff --git a/tests/spec_decode/e2e/test_integration_dist_tp4.py b/tests/spec_decode/e2e/test_integration_dist_tp4.py\nindex 3f7c5d749..555aef992 100644\n--- a/tests/spec_decode/e2e/test_integration_dist_tp4.py\n+++ b/tests/spec_decode/e2e/test_integration_dist_tp4.py\n@@ -19,9 +19,6 @@ SPEC_MODEL = \"JackFram/llama-68m\"\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce_eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"4\",\n     ]])\n@@ -71,9 +68,6 @@ def test_draft_model_tp_lt_target_model_tp4(common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"4\",\n     ]])\ndiff --git a/tests/spec_decode/e2e/test_logprobs.py b/tests/spec_decode/e2e/test_logprobs.py\nindex b7d54991e..4cfca8b78 100644\n--- a/tests/spec_decode/e2e/test_logprobs.py\n+++ b/tests/spec_decode/e2e/test_logprobs.py\n@@ -14,9 +14,6 @@ from .conftest import run_equality_correctness_test\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -67,9 +64,6 @@ def test_logprobs_equality(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -119,9 +113,6 @@ def test_logprobs_different_k(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -173,9 +164,6 @@ def test_logprobs_when_skip_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -251,8 +239,6 @@ def test_logprobs_temp_1(vllm_runner, common_llm_kwargs,\n         \"model_name\": \"JackFram/llama-160m\",\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\ndiff --git a/tests/spec_decode/e2e/test_medusa_correctness.py b/tests/spec_decode/e2e/test_medusa_correctness.py\nindex 0b36e712a..b8965606b 100644\n--- a/tests/spec_decode/e2e/test_medusa_correctness.py\n+++ b/tests/spec_decode/e2e/test_medusa_correctness.py\n@@ -45,9 +45,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -93,9 +90,6 @@ def test_medusa_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -151,9 +145,6 @@ def test_medusa_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n     [{\n         \"enforce_eager\": False,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -204,9 +195,6 @@ def test_medusa_e2e_greedy_correctness_cuda_graph(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -253,9 +241,6 @@ def test_medusa_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -306,9 +291,6 @@ def test_medusa_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -356,9 +338,6 @@ def test_medusa_disable_queue(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \ndiff --git a/tests/spec_decode/e2e/test_mlp_correctness.py b/tests/spec_decode/e2e/test_mlp_correctness.py\nindex 52b48a33c..5ecc0d4e9 100644\n--- a/tests/spec_decode/e2e/test_mlp_correctness.py\n+++ b/tests/spec_decode/e2e/test_mlp_correctness.py\n@@ -47,9 +47,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -94,9 +91,6 @@ def test_mlp_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -149,9 +143,6 @@ def test_mlp_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -195,9 +186,6 @@ def test_mlp_e2e_acceptance_rate(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -258,9 +246,6 @@ def test_mlp_e2e_seeded_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -311,9 +296,6 @@ def test_mlp_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -366,9 +348,6 @@ def test_mlp_e2e_greedy_correctness_with_padding(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -419,9 +398,6 @@ def test_mlp_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -469,9 +445,6 @@ def test_mlp_disable_queue(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": SPEC_MODEL,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\ndiff --git a/tests/spec_decode/e2e/test_multistep_correctness.py b/tests/spec_decode/e2e/test_multistep_correctness.py\nindex df6f12d57..5f240d42d 100644\n--- a/tests/spec_decode/e2e/test_multistep_correctness.py\n+++ b/tests/spec_decode/e2e/test_multistep_correctness.py\n@@ -55,9 +55,6 @@ from .conftest import (get_output_from_llm_generator,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -124,9 +121,6 @@ def test_spec_decode_e2e_with_detokenization(test_llm_generator,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -190,9 +184,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_bs1(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -246,9 +237,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs(\n     [{\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -303,9 +291,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs_diff_output_len(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -353,9 +338,6 @@ def test_spec_decode_e2e_greedy_correctness_real_model_bs1(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -404,9 +386,6 @@ def test_spec_decode_e2e_greedy_correctness_real_model_large_bs(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -454,9 +433,6 @@ def test_spec_decode_e2e_greedy_correctness_with_preemption(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -514,9 +490,6 @@ def test_spec_decode_different_block_size(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -570,9 +543,6 @@ def test_skip_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -611,9 +581,6 @@ def test_disable_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -660,9 +627,6 @@ def test_many_k(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\ndiff --git a/tests/spec_decode/e2e/test_ngram_correctness.py b/tests/spec_decode/e2e/test_ngram_correctness.py\nindex 586245938..31bedad48 100644\n--- a/tests/spec_decode/e2e/test_ngram_correctness.py\n+++ b/tests/spec_decode/e2e/test_ngram_correctness.py\n@@ -35,9 +35,6 @@ from .conftest import run_equality_correctness_test\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -82,9 +79,6 @@ def test_ngram_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -145,9 +139,6 @@ def test_ngram_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -195,9 +186,6 @@ def test_ngram_e2e_greedy_correctness_with_preemption(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -254,9 +242,6 @@ def test_ngram_different_k(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -303,7 +288,6 @@ def test_ngram_disable_queue(vllm_runner, common_llm_kwargs,\n         \"enforce_eager\": True,\n \n         # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": \"[ngram]\",\n         \"num_speculative_tokens\": 5,\n         \"ngram_prompt_lookup_max\": 3,\ndiff --git a/tests/spec_decode/e2e/test_seed.py b/tests/spec_decode/e2e/test_seed.py\nindex b17013216..e42cf416b 100644\n--- a/tests/spec_decode/e2e/test_seed.py\n+++ b/tests/spec_decode/e2e/test_seed.py\n@@ -17,9 +17,6 @@ SPEC_MODEL = \"JackFram/llama-160m\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # speculative model\n         \"speculative_model\": \"JackFram/llama-160m\",\n \ndiff --git a/tests/utils.py b/tests/utils.py\nindex 924465057..115cab806 100644\n--- a/tests/utils.py\n+++ b/tests/utils.py\n@@ -678,12 +678,3 @@ def get_client_text_logprob_generations(\n     return [(text_generations, text,\n              (None if x.logprobs is None else x.logprobs.top_logprobs))\n             for completion in completions for x in completion.choices]\n-\n-\n-def check_deprecated_block_manager_usage(test_name: str):\n-    assert envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1 is True, (\n-        f\"To allow the use of deprecated BlockSpaceManagerV1, set the \"\n-        f\"environment variable VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. \"\n-        f\"You can run the tests with: \"\n-        f\"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest {test_name}`\"  #noqa\n-    )\ndiff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py\nindex 8457bde06..d54dbdcb1 100644\n--- a/vllm/attention/backends/flash_attn.py\n+++ b/vllm/attention/backends/flash_attn.py\n@@ -305,8 +305,6 @@ class FlashAttentionMetadataBuilder(\n         self.runner = input_builder.runner\n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n     def _add_seq_group(\n             self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n@@ -355,9 +353,9 @@ class FlashAttentionMetadataBuilder(\n \n             # Compute slot mapping.\n             is_profile_run = is_block_tables_empty(block_tables)\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)\ndiff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py\nindex ba9b2d043..dd9a0fb9d 100644\n--- a/vllm/attention/backends/flashinfer.py\n+++ b/vllm/attention/backends/flashinfer.py\n@@ -475,8 +475,6 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n \n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n         # Please follow https://docs.flashinfer.ai/tutorials/kv_layout.html#page-layout\n         # for the precise definition of the following fields.\n@@ -542,9 +540,9 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             is_profile_run = is_block_tables_empty(block_tables)\n \n             # Compute slot mapping.\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)\ndiff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py\nindex 53e3a53ba..358a223e7 100644\n--- a/vllm/attention/backends/utils.py\n+++ b/vllm/attention/backends/utils.py\n@@ -38,18 +38,12 @@ def is_block_tables_empty(block_tables: Union[None, Dict]):\n \n \n def compute_slot_mapping_start_idx(is_prompt: bool, query_len: int,\n-                                   context_len: int, sliding_window: int,\n-                                   use_v2_block_manager: bool):\n+                                   context_len: int, sliding_window: int):\n     \"\"\"\n     Compute the start index of slot mapping.\n     \"\"\"\n     start_idx = 0\n     if is_prompt and sliding_window is not None:\n-        assert use_v2_block_manager or context_len == 0, (\n-            \"Prefix caching is currently not supported with \"\n-            \"sliding window attention in V1 block manager\")\n-        # When prefill, we use it to not write slots to kv cache\n-        # to save memory.\n         start_idx = max(0, query_len - sliding_window)\n     return start_idx\n \n@@ -138,8 +132,6 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n \n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n     def _add_seq_group(\n             self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n@@ -180,9 +172,9 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n \n             # Compute slot mapping.\n             is_profile_run = is_block_tables_empty(block_tables)\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)\ndiff --git a/vllm/commit_id.py b/vllm/commit_id.py\nnew file mode 100644\nindex 000000000..d857066f1\n--- /dev/null\n+++ b/vllm/commit_id.py\n@@ -0,0 +1 @@\n+__commit__ = \"93ec62b8556e279d2c050bdc1c3247831bd39466\"\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 2e98923a3..4533fb017 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -949,7 +949,6 @@ class SchedulerConfig:\n             iteration.\n         max_model_len: Maximum length of a sequence (including prompt\n             and generated text).\n-        use_v2_block_manager: Whether to use the BlockSpaceManagerV2 or not.\n         num_lookahead_slots: The number of slots to allocate per sequence per\n             step, beyond the known token ids. This is used in speculative\n             decoding to store KV activations of tokens which may or may not be\n@@ -976,7 +975,6 @@ class SchedulerConfig:\n                  max_num_batched_tokens: Optional[int],\n                  max_num_seqs: int,\n                  max_model_len: int,\n-                 use_v2_block_manager: bool = True,\n                  num_lookahead_slots: int = 0,\n                  delay_factor: float = 0.0,\n                  enable_chunked_prefill: bool = False,\n@@ -1026,7 +1024,6 @@ class SchedulerConfig:\n \n         self.max_num_seqs = max_num_seqs\n         self.max_model_len = max_model_len\n-        self.use_v2_block_manager = use_v2_block_manager\n         self.num_lookahead_slots = num_lookahead_slots\n         self.delay_factor = delay_factor\n         self.chunked_prefill_enabled = enable_chunked_prefill\n@@ -1067,18 +1064,6 @@ class SchedulerConfig:\n                 f\"({self.num_scheduler_steps}) must be greater than or \"\n                 \"equal to 1.\")\n \n-        if (not self.use_v2_block_manager \\\n-            and not envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1):\n-            raise ValueError(\n-                \"The use of BlockSpaceManagerV1 is deprecated and will \"\n-                \"be removed in a future release. Please switch to \"\n-                \"BlockSpaceManagerV2 by setting --use-v2-block-manager to \"\n-                \"True. If you wish to suppress this error temporarily, \"\n-                \"you can set the environment variable \"\n-                \"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. If your use \"\n-                \"case is not supported in BlockSpaceManagerV2, please \"\n-                \"file an issue with detailed information.\")\n-\n     @property\n     def is_multi_step(self) -> bool:\n         return self.num_scheduler_steps > 1\n@@ -1137,7 +1122,6 @@ class SpeculativeConfig:\n         speculative_disable_mqa_scorer: Optional[bool],\n         speculative_max_model_len: Optional[int],\n         enable_chunked_prefill: bool,\n-        use_v2_block_manager: bool,\n         disable_log_stats: bool,\n         speculative_disable_by_batch_size: Optional[int],\n         ngram_prompt_lookup_max: Optional[int],\n@@ -1178,9 +1162,6 @@ class SpeculativeConfig:\n             enable_chunked_prefill (bool): Whether vLLM is configured to use\n                 chunked prefill or not. Used for raising an error since its not\n                 yet compatible with spec decode.\n-            use_v2_block_manager (bool): Whether vLLM is configured to use the\n-                v2 block manager or not. Used for raising an error since the v2\n-                block manager is required with spec decode.\n             speculative_disable_by_batch_size (Optional[int]): Disable\n                 speculative decoding for new incoming requests when the number\n                 of enqueue requests  is larger than this value, if provided.\n@@ -1231,11 +1212,6 @@ class SpeculativeConfig:\n                 \"Speculative decoding and chunked prefill are \"\n                 f\"currently mutually exclusive ({enable_chunked_prefill=}).\")\n \n-        if not use_v2_block_manager:\n-            raise ValueError(\n-                \"Speculative decoding requires usage of the V2 \"\n-                \"block manager. Enable it with --use-v2-block-manager.\")\n-\n         # TODO: The user should be able to specify revision/max model len\n         # for the draft model. It is not currently supported.\n         draft_revision = None\ndiff --git a/vllm/core/block/utils.py b/vllm/core/block/utils.py\nindex 28839437c..1c6578e4c 100644\n--- a/vllm/core/block/utils.py\n+++ b/vllm/core/block/utils.py\n@@ -4,28 +4,6 @@ from vllm.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                         STR_NOT_IMPL_ENC_DEC_SWA)\n \n \n-def _get_block_mgr_sliding_window_attr(block_mgr):\n-    '''\n-    BlockManagerV1 and BlockManagerV2 have slightly different\n-    members related to sliding window attention (SWA). This\n-    function extracts the appropriate member to use for determining\n-    whether SWA is enabled.\n-\n-    Arguments:\n-\n-    * block_mgr: BlockManagerV1 or BlockManagerV2 instance\n-    '''\n-\n-    if hasattr(block_mgr, 'block_sliding_window'):\n-        return block_mgr.block_sliding_window\n-    if hasattr(block_mgr, 'max_block_sliding_window'):\n-        return block_mgr.max_block_sliding_window\n-\n-    raise AttributeError(\"Block manager instance has neither \" + \\\n-                         \"block_sliding_window nor \" + \\\n-                         \"max_block_sliding_window attributes.\")\n-\n-\n def check_no_caching_or_swa_for_blockmgr_encdec(\n         block_mgr, seq_group: SequenceGroup) -> None:\n     '''\n@@ -41,7 +19,7 @@ def check_no_caching_or_swa_for_blockmgr_encdec(\n     '''\n \n     if seq_group.is_encoder_decoder():\n-        if _get_block_mgr_sliding_window_attr(block_mgr) is not None:\n+        if block_mgr.max_block_sliding_window is not None:\n             raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_SWA)\n \n         if block_mgr.enable_caching:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager.py\nsimilarity index 99%\nrename from vllm/core/block_manager_v2.py\nrename to vllm/core/block_manager.py\nindex cb047c832..61ed7afba 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager.py\n@@ -17,7 +17,7 @@ SeqId = int\n EncoderSeqId = str\n \n \n-class BlockSpaceManagerV2(BlockSpaceManager):\n+class SelfAttnBlockSpaceManager(BlockSpaceManager):\n     \"\"\"BlockSpaceManager which manages the allocation of KV cache.\n \n     It owns responsibility for allocation, swapping, allocating memory for\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\ndeleted file mode 100644\nindex 8bc0ce2bc..000000000\n--- a/vllm/core/block_manager_v1.py\n+++ /dev/null\n@@ -1,743 +0,0 @@\n-\"\"\"A block manager that manages token blocks.\"\"\"\n-import math\n-from abc import ABC, abstractmethod\n-from itertools import count, takewhile\n-from os.path import commonprefix\n-from typing import Dict, List, Optional\n-from typing import Sequence as GenericSequence\n-from typing import Set, Tuple\n-\n-from vllm.block import BlockTable, PhysicalTokenBlock\n-from vllm.core.block.common import CacheMetricData\n-from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\n-from vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\n-from vllm.core.interfaces import AllocStatus, BlockSpaceManager\n-from vllm.logger import init_logger\n-from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n-from vllm.utils import Device\n-\n-logger = init_logger(__name__)\n-\n-\n-class BlockAllocatorBase(ABC):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    @abstractmethod\n-    def __init__(self,\n-                 device: Device,\n-                 block_size: int,\n-                 num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n-        pass\n-\n-    @abstractmethod\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        pass\n-\n-    @abstractmethod\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        pass\n-\n-    @abstractmethod\n-    def get_num_free_blocks(self) -> int:\n-        pass\n-\n-    @abstractmethod\n-    def get_num_total_blocks(self) -> int:\n-        pass\n-\n-    @abstractmethod\n-    def contains_block(self, block_hash: int) -> bool:\n-        pass\n-\n-    @abstractmethod\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        pass\n-\n-    @abstractmethod\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n-        pass\n-\n-\n-class CachedBlockAllocator(BlockAllocatorBase):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 device: Device,\n-                 block_size: int,\n-                 num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n-        self.device = device\n-        self.block_size = block_size\n-        self.num_blocks = num_blocks\n-\n-        self.current_num_blocks = 0\n-        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n-\n-        self.evictor: Evictor = make_evictor(eviction_policy)\n-\n-        self.default_hash_ctr = count()\n-\n-        self.cache_metric_data = CacheMetricData()\n-\n-    def allocate_block(self, block_hash: int,\n-                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n-        if self.current_num_blocks == self.num_blocks:\n-            block = self.evictor.evict()\n-            block.block_hash = block_hash\n-            block.num_hashed_tokens = num_hashed_tokens\n-            return block\n-        block = PhysicalTokenBlock(device=self.device,\n-                                   block_number=self.current_num_blocks,\n-                                   block_size=self.block_size,\n-                                   block_hash=block_hash,\n-                                   num_hashed_tokens=num_hashed_tokens)\n-        self.current_num_blocks += 1\n-        return block\n-\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        if block_hash is None:\n-            block_hash = next(self.default_hash_ctr)\n-\n-        if block_hash in self.evictor:\n-            assert block_hash not in self.cached_blocks\n-            block = self.evictor.remove(block_hash)\n-            assert block.ref_count == 0\n-            self.cached_blocks[block_hash] = block\n-\n-        if block_hash in self.cached_blocks:\n-            self.cache_metric_data.query(hit=True)\n-        else:\n-            self.cache_metric_data.query(hit=False)\n-            self.cached_blocks[block_hash] = self.allocate_block(\n-                block_hash, num_hashed_tokens)\n-        block = self.cached_blocks[block_hash]\n-        assert block.block_hash == block_hash\n-        block.ref_count += 1\n-        return block\n-\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        if block.ref_count == 0:\n-            raise ValueError(f\"Double free! {block} is already freed.\")\n-        block.ref_count -= 1\n-        if block.ref_count == 0:\n-            assert block.block_hash not in self.evictor\n-            self.evictor.add(block)\n-\n-            # Remove the block from the cached_blocks\n-            del self.cached_blocks[block.block_hash]\n-\n-    def get_num_free_blocks(self) -> int:\n-        return (self.num_blocks - self.current_num_blocks +\n-                self.evictor.num_blocks)\n-\n-    def get_num_total_blocks(self) -> int:\n-        return self.num_blocks\n-\n-    def contains_block(self, block_hash: int) -> bool:\n-        return block_hash in self.cached_blocks or block_hash in self.evictor\n-\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        # Update the hash of block and the cached_blocks dictionary.\n-        assert not self.contains_block(block_hash)\n-        old_hash = block.block_hash\n-        block.block_hash = block_hash\n-        del self.cached_blocks[old_hash]\n-        self.cached_blocks[block_hash] = block\n-\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        return self.cache_metric_data.get_hit_rate()\n-\n-\n-class UncachedBlockAllocator(BlockAllocatorBase):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        device: Device,\n-        block_size: int,\n-        num_blocks: int,\n-    ) -> None:\n-        self.device = device\n-        self.block_size = block_size\n-        self.num_blocks = num_blocks\n-\n-        # Initialize the free blocks.\n-        self.free_blocks: List[PhysicalTokenBlock] = []\n-        for i in range(num_blocks):\n-            block = PhysicalTokenBlock(device=device,\n-                                       block_number=i,\n-                                       block_size=block_size,\n-                                       block_hash=-1,\n-                                       num_hashed_tokens=0)\n-            self.free_blocks.append(block)\n-\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        if not self.free_blocks:\n-            raise ValueError(\"Out of memory! No free blocks are available.\")\n-        block = self.free_blocks.pop()\n-        block.ref_count = 1\n-        return block\n-\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        if block.ref_count == 0:\n-            raise ValueError(f\"Double free! {block} is already freed.\")\n-        block.ref_count -= 1\n-        if block.ref_count == 0:\n-            self.free_blocks.append(block)\n-\n-    def get_num_free_blocks(self) -> int:\n-        return len(self.free_blocks)\n-\n-    def get_num_total_blocks(self) -> int:\n-        return self.num_blocks\n-\n-    def contains_block(self, block_hash: int) -> bool:\n-        raise NotImplementedError(\n-            \"Invalid codepath for uncached block allocator.\")\n-\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        raise NotImplementedError(\n-            \"Invalid codepath for uncached block allocator.\")\n-\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        return -1\n-\n-\n-class BlockSpaceManagerV1(BlockSpaceManager):\n-    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n-\n-    def __init__(\n-        self,\n-        block_size: int,\n-        num_gpu_blocks: int,\n-        num_cpu_blocks: int,\n-        watermark: float = 0.01,\n-        sliding_window: Optional[int] = None,\n-        enable_caching: bool = False,\n-    ) -> None:\n-        self.block_size = block_size\n-        self.num_total_gpu_blocks = num_gpu_blocks\n-        self.num_total_cpu_blocks = num_cpu_blocks\n-\n-        if enable_caching and sliding_window is not None:\n-            raise NotImplementedError(\n-                \"Sliding window is not allowed with prefix caching enabled!\")\n-\n-        self.block_sliding_window = None\n-        if sliding_window is not None:\n-            # Round up to nearest block size to regularize sliding window\n-            # allocation sizes.\n-            self.block_sliding_window = math.ceil(sliding_window / block_size)\n-\n-        self.watermark = watermark\n-        assert watermark >= 0.0\n-\n-        self.enable_caching = enable_caching\n-\n-        self.watermark_blocks = int(watermark * num_gpu_blocks)\n-\n-        if self.enable_caching:\n-            logger.info(\"Automatic prefix caching is enabled.\")\n-            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n-                Device.GPU, block_size, num_gpu_blocks)\n-            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n-                Device.CPU, block_size, num_cpu_blocks)\n-        else:\n-            self.gpu_allocator = UncachedBlockAllocator(\n-                Device.GPU, block_size, num_gpu_blocks)\n-            self.cpu_allocator = UncachedBlockAllocator(\n-                Device.CPU, block_size, num_cpu_blocks)\n-        # Mapping: seq_id -> BlockTable.\n-        self.block_tables: Dict[int, BlockTable] = {}\n-\n-        # Mapping: req_id -> BlockTable\n-        # Note that each SequenceGroup has a unique\n-        # request ID\n-        self.cross_block_tables: Dict[str, BlockTable] = {}\n-\n-    def _get_seq_num_required_blocks(self, seq: Optional[Sequence]) -> int:\n-        return 0 if seq is None else seq.n_blocks\n-\n-    def can_allocate(self,\n-                     seq_group: SequenceGroup,\n-                     num_lookahead_slots: int = 0) -> AllocStatus:\n-        # FIXME(woosuk): Here we assume that all sequences in the group share\n-        # the same prompt. This may not be true for preempted sequences.\n-\n-        assert (num_lookahead_slots == 0\n-                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n-\n-        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n-\n-        self_num_required_blocks = self._get_seq_num_required_blocks(\n-            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n-        cross_num_required_blocks = self._get_seq_num_required_blocks(\n-            seq_group.get_encoder_seq())\n-        num_required_blocks = self_num_required_blocks + \\\n-                              cross_num_required_blocks\n-\n-        if self.block_sliding_window is not None:\n-\n-            num_required_blocks = min(num_required_blocks,\n-                                      self.block_sliding_window)\n-        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n-\n-        # Use watermark to avoid frequent cache eviction.\n-        if (self.num_total_gpu_blocks - num_required_blocks <\n-                self.watermark_blocks):\n-            return AllocStatus.NEVER\n-        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n-            return AllocStatus.OK\n-        else:\n-            return AllocStatus.LATER\n-\n-    def _allocate_sequence(self, \\\n-                           seq: Optional[Sequence], \\\n-                           ref_count: int, \\\n-                           is_encoder_decoder: bool = True) -> BlockTable:\n-        # Allocate new physical token blocks that will store the prompt tokens.\n-        num_prompt_blocks = self._get_seq_num_required_blocks(seq)\n-\n-        block_table: BlockTable = BlockTable()\n-        assert seq is not None\n-        for logical_idx in range(num_prompt_blocks):\n-            if (self.block_sliding_window is not None\n-                    and logical_idx >= self.block_sliding_window):\n-                block = block_table[logical_idx % self.block_sliding_window]\n-                # Set the reference counts of the token blocks.\n-                block.ref_count = ref_count\n-            elif not is_encoder_decoder and self.enable_caching:\n-                block = self.gpu_allocator.allocate(\n-                    seq.hash_of_block(logical_idx),\n-                    seq.num_hashed_tokens_of_block(logical_idx))\n-            else:\n-                block = self.gpu_allocator.allocate()\n-                # Set the reference counts of the token blocks.\n-                block.ref_count = ref_count\n-            block_table.append(block)\n-\n-        return block_table\n-\n-    def allocate(self, seq_group: SequenceGroup) -> None:\n-        is_encoder_decoder = seq_group.is_encoder_decoder()\n-        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n-\n-        # Allocate decoder sequences\n-        #\n-        # NOTE: Here we assume that all sequences in the group have the same\n-        # decoder prompt.\n-        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n-        seq = wait_seqs[0]\n-        block_table: BlockTable = \\\n-            self._allocate_sequence(seq,\n-                                    seq_group.num_seqs(),\n-                                    is_encoder_decoder)\n-\n-        # Assign the self-attention block tables for each sequence.\n-        if len(wait_seqs) == 1:\n-            self.block_tables[seq.seq_id] = block_table\n-        else:\n-            for seq in wait_seqs:\n-                self.block_tables[seq.seq_id] = block_table.copy()\n-\n-        # Allocate encoder sequence\n-        if is_encoder_decoder:\n-            # A SequenceGroup has only a single encoder sequence (at most),\n-            # thus allocate with a ref count of 1\n-            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n-                                                  1, is_encoder_decoder)\n-            # Assign the cross-attention block table for the SequenceGroup.\n-            self.cross_block_tables[seq_group.request_id] = block_table\n-\n-    def can_append_slots(self,\n-                         seq_group: SequenceGroup,\n-                         num_lookahead_slots: int = 0) -> bool:\n-        assert (num_lookahead_slots == 0\n-                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n-\n-        # Simple heuristic: If there is at least one free block\n-        # for each sequence, we can append.\n-        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n-        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n-        return num_seqs <= num_free_gpu_blocks\n-\n-    def _promote_last_block(\n-        self,\n-        seq: Sequence,\n-        last_block: PhysicalTokenBlock,\n-    ) -> PhysicalTokenBlock:\n-        assert self.enable_caching\n-\n-        # Compute a new hash for the block so that it can be shared by other\n-        # Sequences\n-        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n-\n-        # if new_hash is already in the cached table, then free last_block\n-        # and return the cached version\n-        if self.gpu_allocator.contains_block(new_hash):\n-            self.gpu_allocator.free(last_block)\n-            return self.gpu_allocator.allocate(new_hash)\n-        else:\n-            self.gpu_allocator.update_hash(new_hash, last_block)\n-            return last_block\n-\n-    def _is_last_block_full(\n-        self,\n-        seq: Sequence,\n-    ) -> bool:\n-        token_ids_len = seq.data.get_len()\n-        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n-\n-    def _maybe_promote_last_block(\n-        self,\n-        seq: Sequence,\n-        last_block: PhysicalTokenBlock,\n-    ) -> PhysicalTokenBlock:\n-        if self._is_last_block_full(seq):\n-            return self._promote_last_block(seq, last_block)\n-        else:\n-            return last_block\n-\n-    def _allocate_last_physical_block(\n-        self,\n-        seq: Sequence,\n-    ) -> PhysicalTokenBlock:\n-        # Called before a new block is appended.\n-        # This is in charge of allocating a new physical block (to be appended).\n-\n-        # None if the last block is not full. Otherwise, we set it to the\n-        # content hash.\n-        if not self.enable_caching:\n-            return self.gpu_allocator.allocate()\n-        block_hash: Optional[int] = None\n-        n_blocks = seq.n_blocks\n-        if (self._is_last_block_full(seq)):\n-            block_hash = seq.hash_of_block(n_blocks - 1)\n-        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n-\n-        # num_hashed_tokens is used to compute future hashes\n-        # (e.g. in the hashing function, it is used to ask the sequence for\n-        # prefix tokens)\n-        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n-\n-        # If the block_hash is None, then the block is not full.\n-        # If the block is not full, then we expect it to have a refcount of 1.\n-        if block_hash is None:\n-            assert new_block.ref_count == 1\n-        return new_block\n-\n-    def append_slots(\n-        self,\n-        seq: Sequence,\n-        num_lookahead_slots: int = 0,\n-    ) -> List[Tuple[int, int]]:\n-        \"\"\"Allocate a physical slot for a new token.\"\"\"\n-        n_blocks = seq.n_blocks\n-        block_table = self.block_tables[seq.seq_id]\n-        # If we need to allocate a new physical block\n-        if len(block_table) < n_blocks:\n-            # Currently this code only supports adding one physical block\n-            assert len(block_table) == n_blocks - 1\n-\n-            if (self.block_sliding_window\n-                    and len(block_table) >= self.block_sliding_window):\n-                # reuse a block\n-                block_table.append(block_table[len(block_table) %\n-                                               self.block_sliding_window])\n-            else:\n-                # The sequence hash a new logical block.\n-                # Allocate a new physical block.\n-                new_block = self._allocate_last_physical_block(seq)\n-                block_table.append(new_block)\n-                return []\n-\n-        # We want to append the token to the last physical block.\n-        last_block = block_table[-1]\n-        assert last_block.device == Device.GPU\n-        if last_block.ref_count == 1:\n-            # Not shared with other sequences. Appendable.\n-            if self.enable_caching:\n-                # If the last block is now complete, we may reuse an old block\n-                # to save memory.\n-                maybe_new_block = self._maybe_promote_last_block(\n-                    seq, last_block)\n-                block_table[-1] = maybe_new_block\n-            return []\n-        else:\n-            # The last block is shared with other sequences.\n-            # Copy on Write: Allocate a new block and copy the tokens.\n-            new_block = self._allocate_last_physical_block(seq)\n-\n-            block_table[-1] = new_block\n-            self.gpu_allocator.free(last_block)\n-            return [(last_block.block_number, new_block.block_number)]\n-\n-    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n-        # NOTE: fork does not allocate a new physical block.\n-        # Thus, it is always safe from OOM.\n-        if parent_seq.seq_id not in self.block_tables:\n-            # Parent sequence has either been freed or never existed.\n-            return\n-        src_block_table = self.block_tables[parent_seq.seq_id]\n-        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n-\n-        # When using a sliding window, blocks will be eventually reused.\n-        # In this case the block tables will contain repeated blocks.\n-        # When forking, we must make sure that each block's `ref_count`\n-        # is only incremented by one, so we deduplicate them by wrapping\n-        # them in a set.\n-        for block in set(src_block_table):\n-            block.ref_count += 1\n-\n-    def _get_physical_blocks(\n-            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n-\n-        # NOTE: Here, we assume that the physical blocks are only shared by\n-        # the sequences in the same group.\n-        request_id = seq_group.request_id\n-        blocks: Set[PhysicalTokenBlock] = set()\n-        for seq in seq_group.get_seqs():\n-            if seq.is_finished():\n-                continue\n-            blocks.update(self.block_tables[seq.seq_id])\n-        # Cross-attention blocks\n-        if seq_group.is_encoder_decoder():\n-            blocks.update(self.cross_block_tables[request_id])\n-        return list(blocks)\n-\n-    def can_swap_in(self,\n-                    seq_group: SequenceGroup,\n-                    num_lookahead_slots: int = 0) -> AllocStatus:\n-        assert (num_lookahead_slots == 0\n-                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n-\n-        blocks = self._get_physical_blocks(seq_group)\n-        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n-        if seq_group.is_encoder_decoder():\n-            num_swapped_seqs += 1\n-        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n-        # NOTE: Conservatively, we assume that every sequence will allocate\n-        # at least one free block right after the swap-in.\n-        # NOTE: This should match the logic in can_append_slot().\n-        num_required_blocks = len(blocks) + num_swapped_seqs\n-        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n-            return AllocStatus.NEVER\n-        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n-            return AllocStatus.OK\n-        else:\n-            return AllocStatus.LATER\n-\n-    def _swap_block_table(\n-            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n-            dest_allocator: BlockAllocatorBase,\n-            mapping: Dict[PhysicalTokenBlock,\n-                          PhysicalTokenBlock]) -> BlockTable:\n-        new_block_table: BlockTable = BlockTable()\n-\n-        for from_block in block_table:\n-            if from_block in mapping:\n-                to_block = mapping[from_block]\n-                to_block.ref_count += 1\n-            else:\n-                to_block = dest_allocator.allocate(\n-                    from_block.block_hash, from_block.num_hashed_tokens)\n-                mapping[from_block] = to_block\n-            new_block_table.append(to_block)\n-            # Free the source block swapped in to destination.\n-            src_allocator.free(from_block)\n-\n-        return new_block_table\n-\n-    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n-\n-        request_id = seq_group.request_id\n-\n-        # CPU block -> GPU block.\n-        # dict is efficient in lookup `if cpu_block in mapping`\n-        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n-        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n-            self.block_tables[seq.seq_id] = \\\n-                self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.cpu_allocator, self.gpu_allocator,\n-                                       mapping)\n-\n-        if seq_group.is_encoder_decoder():\n-            self.cross_block_tables[request_id] = \\\n-                self._swap_block_table(self.cross_block_tables[request_id],\n-                                       self.cpu_allocator,\n-                                       self.gpu_allocator,\n-                                       mapping)\n-\n-        return [(cpu_block.block_number, gpu_block.block_number)\n-                for cpu_block, gpu_block in mapping.items()]\n-\n-    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n-        blocks = self._get_physical_blocks(seq_group)\n-        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n-\n-    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n-        request_id = seq_group.request_id\n-\n-        # GPU block -> CPU block.\n-        # dict is efficient in lookup `if gpu_block in mapping`\n-        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n-        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n-            self.block_tables[seq.seq_id] = \\\n-                self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.gpu_allocator, self.cpu_allocator,\n-                                       mapping)\n-\n-        if seq_group.is_encoder_decoder():\n-            self.cross_block_tables[request_id] = \\\n-                self._swap_block_table(self.cross_block_tables[request_id],\n-                                       self.gpu_allocator,\n-                                       self.cpu_allocator,\n-                                       mapping)\n-\n-        return [(cpu_block.block_number, gpu_block.block_number)\n-                for cpu_block, gpu_block in mapping.items()]\n-\n-    def _free_block_table(self, block_table: BlockTable) -> None:\n-        # when using a sliding window, each seq will only use up\n-        # to `self.block_sliding_window` blocks. When freeing\n-        # the block table, we must make sure to not free blocks more\n-        # than once. If no sliding window is used, there is no block\n-        # reuse in the block table, so we must free all blocks.\n-        blocks_to_free = (block_table[-self.block_sliding_window:]\n-                          if self.block_sliding_window is not None else\n-                          block_table)\n-        for block in set(blocks_to_free):\n-            if block.device == Device.GPU:\n-                self.gpu_allocator.free(block)\n-            else:\n-                self.cpu_allocator.free(block)\n-\n-    def free(self, seq: Sequence) -> None:\n-        if seq.seq_id not in self.block_tables:\n-            # Already freed or haven't been scheduled yet.\n-            return\n-        block_table = self.block_tables[seq.seq_id]\n-        self._free_block_table(block_table)\n-        del self.block_tables[seq.seq_id]\n-\n-    def free_cross(self, seq_group: SequenceGroup) -> None:\n-        if seq_group.request_id not in self.cross_block_tables:\n-            # Already freed or hasn't ben scheduled yet.\n-            return\n-        block_table = self.cross_block_tables[seq_group.request_id]\n-        self._free_block_table(block_table)\n-        del self.cross_block_tables[seq_group.request_id]\n-\n-    def reset(self) -> None:\n-        # Free decoder block tables\n-        for block_table in self.block_tables.values():\n-            self._free_block_table(block_table)\n-        self.block_tables.clear()\n-        # Free cross-attention block tables\n-        for block_table in self.cross_block_tables.values():\n-            self._free_block_table(block_table)\n-        self.cross_block_tables.clear()\n-\n-    def get_block_table(self, seq: Sequence) -> List[int]:\n-        return self.block_tables[seq.seq_id].ids()\n-\n-    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n-        block_table = self.cross_block_tables[seq_group.request_id]\n-        return [block.block_number for block in block_table]\n-\n-    def get_num_free_gpu_blocks(self) -> int:\n-        return self.gpu_allocator.get_num_free_blocks()\n-\n-    def get_num_free_cpu_blocks(self) -> int:\n-        return self.cpu_allocator.get_num_free_blocks()\n-\n-    def access_all_blocks_in_seq(\n-        self,\n-        seq: Sequence,\n-        access_time: float,\n-    ) -> None:\n-        if self.enable_caching:\n-            # Update the last accessed time of all the blocks accessed\n-            # in this step.\n-            block_table = self.block_tables[seq.seq_id]\n-            for block in block_table:\n-                block.last_accessed = access_time\n-\n-    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n-        if seq.seq_id not in self.block_tables:\n-            return\n-\n-        # When chunked prefill is enabled, the computed full blocks\n-        # should be calculated based on the number of computed tokens.\n-        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n-                               token_chunk_size)\n-        computed_full_blocks = max_computed_tokens // self.block_size\n-\n-        block_table = self.block_tables[seq.seq_id]\n-        if computed_full_blocks == 0:\n-            return\n-        for i in reversed(range(computed_full_blocks)):\n-            if block_table[i].computed:\n-                break\n-            block_table[i].computed = True\n-\n-    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n-        if seq.seq_id not in self.block_tables:\n-            return []\n-        block_table = self.block_tables[seq.seq_id]\n-        # NOTE We exclude the last block to avoid the case where the entire\n-        # prompt is cached. This would cause erroneous behavior in model\n-        # runner.\n-        return [\n-            b.block_number\n-            for b in takewhile(lambda b: b.computed, block_table[:-1])\n-        ]\n-\n-    def get_common_computed_block_ids(\n-            self, seqs: List[Sequence]) -> GenericSequence[int]:\n-        \"\"\"Return the block ids that are common for a given sequence group.\n-\n-        Used in prefill (can skip prefill of some blocks).\n-        \"\"\"\n-        # Can return non-empty result only with prefix caching enabled.\n-        if not self.enable_caching:\n-            return []\n-\n-        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n-        return commonprefix([ids for ids in ids_list if ids != []])\n-\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n-                                token_chunk_size: int):\n-        if self.enable_caching:\n-            for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n-\n-    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n-        if device == Device.GPU:\n-            return self.gpu_allocator.get_prefix_cache_hit_rate()\n-        if device == Device.CPU:\n-            return self.cpu_allocator.get_prefix_cache_hit_rate()\n-        raise ValueError(f\"Invalid device: {device}\")\ndiff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex 9e1d1b02f..9501a516b 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -28,13 +28,9 @@ class BlockSpaceManager(ABC):\n     def get_block_space_manager_class(version: str):\n         version = version.lower()\n \n-        if version == \"v1\":\n-            from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n-            return BlockSpaceManagerV1\n-\n-        if version == \"v2\":\n-            from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n-            return BlockSpaceManagerV2\n+        if version == \"selfattn\":\n+            from vllm.core.block_manager import SelfAttnBlockSpaceManager\n+            return SelfAttnBlockSpaceManager\n \n         if version == \"placeholder\":\n             from vllm.core.placeholder_block_space_manager import (\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex e7eaaf122..f0c8e6bab 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -312,9 +312,7 @@ class Scheduler:\n         # LoRAs. This should be improved in the future.\n         self.lora_config = lora_config\n \n-        version = \"v1\"\n-        if self.scheduler_config.use_v2_block_manager:\n-            version = \"v2\"\n+        version = \"selfattn\"\n         if (self.scheduler_config.embedding_mode\n                 or self.cache_config.is_attention_free):\n             version = \"placeholder\"\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 1ce9e6200..41963dcb1 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -373,12 +373,13 @@ class EngineArgs:\n                             action='store_true',\n                             help='Disables sliding window, '\n                             'capping to sliding window size')\n-        parser.add_argument(\n-            '--use-v2-block-manager',\n-            default=EngineArgs.use_v2_block_manager,\n-            action='store_true',\n-            help='Use BlockSpaceMangerV2. By default this is set to True. '\n-            'Set to False to use BlockSpaceManagerV1')\n+        parser.add_argument('--use-v2-block-manager',\n+                            action='store_true',\n+                            help='[DEPRECATED] block manager v1 has been '\n+                            'removed and SelfAttnBlockSpaceManager (i.e. '\n+                            'block manager v2) is now the default. '\n+                            'Setting this flag to True or False'\n+                            ' has no effect on vLLM behavior.')\n         parser.add_argument(\n             '--num-lookahead-slots',\n             type=int,\n@@ -969,12 +970,6 @@ class EngineArgs:\n                 \"in low performance due to small KV cache space. Consider \"\n                 \"setting --max-model-len to a smaller value.\", max_model_len)\n \n-        if self.num_scheduler_steps > 1 and not self.use_v2_block_manager:\n-            self.use_v2_block_manager = True\n-            logger.warning(\n-                \"Enabled BlockSpaceManagerV2 because it is \"\n-                \"required for multi-step (--num-scheduler-steps > 1)\")\n-\n         speculative_config = SpeculativeConfig.maybe_create_spec_config(\n             target_model_config=model_config,\n             target_parallel_config=parallel_config,\n@@ -990,7 +985,6 @@ class EngineArgs:\n             speculative_disable_by_batch_size,\n             speculative_max_model_len=self.speculative_max_model_len,\n             enable_chunked_prefill=self.enable_chunked_prefill,\n-            use_v2_block_manager=self.use_v2_block_manager,\n             disable_log_stats=self.disable_log_stats,\n             ngram_prompt_lookup_max=self.ngram_prompt_lookup_max,\n             ngram_prompt_lookup_min=self.ngram_prompt_lookup_min,\n@@ -1021,11 +1015,20 @@ class EngineArgs:\n             if speculative_config is None \\\n             else speculative_config.num_lookahead_slots\n \n+        if not self.use_v2_block_manager:\n+            logger.warning(\n+                \"[DEPRECATED] Block manager v1 has been removed, \"\n+                \"and setting --use-v2-block-manager to True or False has \"\n+                \"no effect on vLLM behavior. Please remove \"\n+                \"--use-v2-block-manager in your engine argument. \"\n+                \"If your use case is not supported by \"\n+                \"SelfAttnBlockSpaceManager (i.e. block manager v2),\"\n+                \" please file an issue with detailed information.\")\n+\n         scheduler_config = SchedulerConfig(\n             max_num_batched_tokens=self.max_num_batched_tokens,\n             max_num_seqs=self.max_num_seqs,\n             max_model_len=model_config.max_model_len,\n-            use_v2_block_manager=self.use_v2_block_manager,\n             num_lookahead_slots=num_lookahead_slots,\n             delay_factor=self.scheduler_delay_factor,\n             enable_chunked_prefill=self.enable_chunked_prefill,\n@@ -1081,13 +1084,6 @@ class EngineArgs:\n             or \"all\" in detailed_trace_modules,\n         )\n \n-        if (model_config.get_sliding_window() is not None\n-                and scheduler_config.chunked_prefill_enabled\n-                and not scheduler_config.use_v2_block_manager):\n-            raise ValueError(\n-                \"Chunked prefill is not supported with sliding window. \"\n-                \"Set --disable-sliding-window to disable sliding window.\")\n-\n         return EngineConfig(\n             model_config=model_config,\n             cache_config=cache_config,\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex a570d096d..61c21887e 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -247,7 +247,7 @@ class LLMEngine:\n             \"enforce_eager=%s, kv_cache_dtype=%s, \"\n             \"quantization_param_path=%s, device_config=%s, \"\n             \"decoding_config=%r, observability_config=%r, \"\n-            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n+            \"seed=%d, served_model_name=%s, \"\n             \"num_scheduler_steps=%d, chunked_prefill_enabled=%s \"\n             \"multi_step_stream_outputs=%s, enable_prefix_caching=%s, \"\n             \"use_async_output_proc=%s, use_cached_outputs=%s, \"\n@@ -280,7 +280,6 @@ class LLMEngine:\n             observability_config,\n             model_config.seed,\n             model_config.served_model_name,\n-            scheduler_config.use_v2_block_manager,\n             scheduler_config.num_scheduler_steps,\n             scheduler_config.chunked_prefill_enabled,\n             scheduler_config.multi_step_stream_outputs,\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 45a999961..2d283fae2 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -64,7 +64,6 @@ if TYPE_CHECKING:\n     VLLM_USE_TRITON_AWQ: bool = False\n     VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n     VLLM_SKIP_P2P_CHECK: bool = False\n-    VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1: bool = False\n     VLLM_TORCH_COMPILE_LEVEL: int = 0\n     VLLM_DISABLED_KERNELS: List[str] = []\n \n@@ -427,11 +426,6 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     \"VLLM_SKIP_P2P_CHECK\":\n     lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n \n-    # If set, allowing the use of deprecated block manager V1\n-    \"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\":\n-    lambda: os.environ.get(\"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\", \"0\"\n-                           ) == \"1\",\n-\n     # List of quantization kernels that should be disabled, used for testing\n     # and performance comparisons. Currently only affects MPLinearKernel\n     # selection\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 36753b858..a82956985 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -574,17 +574,12 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             # paged attn. We can remove it if we make paged attn kernel\n             # to properly handle slinding window attn.\n             curr_sliding_window_block = self.sliding_window_blocks\n-            if self.scheduler_config.use_v2_block_manager:\n-                # number of elements in last block\n-                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n-                sliding_seq_len = min(\n-                    inter_data.seq_lens[seq_idx],\n-                    self.block_aligned_sliding_window + suff_len)\n-                if suff_len > 0:\n-                    curr_sliding_window_block += 1\n-            else:\n-                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n-                                      self.sliding_window)\n+            # number of elements in last block\n+            suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n+            sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n+                                  self.block_aligned_sliding_window + suff_len)\n+            if suff_len > 0:\n+                curr_sliding_window_block += 1\n \n         inter_data.curr_sliding_window_blocks[\n             seq_idx] = curr_sliding_window_block",
  "apis": [
    "vllm.engine.arg_utils.EngineArgs",
    "vllm.core.block_manager.SelfAttnBlockSpaceManager"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/llm.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block_manager.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit removes flags and code paths associated with the deprecated block manager v1 and makes block manager v2 the default. The v2 block manager is described in the message as having \"much higher performance on prefix caching,\" implying a direct performance optimization improvement. Although many of the changes update test and benchmark files (as well as documentation and examples), they also affect non-test source code (benchmarks, core tests, docs) and simplify code paths to exclusively use the optimized block manager. This change is aimed at optimizing the performance (specifically for prefix caching) of the system on the CPU, not just a simple refactoring or bug fix. Therefore, the commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "This commit removes the legacy flag and configuration for enabling the v1 block manager and cleans up all invocations that depended on the ‚Äúuse_v2_block_manager‚Äù parameter. In essence, the codebase now defaults to using the newer block manager implementation (v2), specifically via the SelfAttnBlockSpaceManager, and the EngineArgs configuration no longer accepts or passes the deprecated flag. These changes affect test pipelines, benchmark scripts, and test cases that used to toggle between block manager versions."
}