{
  "commit_hash": "3476ed0809ec91a3457da0cb90543133a4f4b519",
  "pr_url": "https://github.com/vllm-project/vllm/pull/5602",
  "pr_date": "2024-07-02",
  "timeline_text": "Copy link Collaborator alexm-redhat commented Jun 17, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes block_manager_v2 python logic to make it comparable to block_manager_v1. The goal is to enable block_manager_v2 by default as part of the spec decode project. The issues optimized are: Python Block object allocations/deallocations are expensive on the hot-path of iterative batching, so a block pool is used to cache block objects. Any string/list duplication should be avoided, especially for token id lists Modified Prefix Caching Block/Allocator to avoid any full traversals of block_ids by using dynamic/incremental style computations Redid the way access all blocks updates timestamps by deferring the actual updates to free(..) of sequences Here is initial performance comparison for both standard and prefix-cache enabled runs: Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 cadedaniel reacted with heart emoji üöÄ 5 cadedaniel, mgoin, robertgshaw2-redhat, zhuohan123, and CatherineSue reacted with rocket emoji All reactions ‚ù§Ô∏è 1 reaction üöÄ 5 reactions robertgshaw2-redhat requested a review\n  from cadedaniel June 17, 2024 14:58 alexm-redhat marked this pull request as draft June 17, 2024 15:02 cadedaniel reviewed Jun 18, 2024 View reviewed changes vllm/sequence.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block_manager_v2.py block_ids = self.block_tables[seq.seq_id].physical_block_ids assert all(b is not None for b in block_ids) Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment can we keep these in for correctness? can have a flag strict_mode which checks these only in testing / not in production Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I have added \"assert block_id is not None\" checks into BlockList so the invariant of \"assert all(b is not None for b in block_ids)\" is always kept. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment awesome Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/naive_block.py Outdated block_size: int, block_id: Optional[int] = None): # Please keep sync with the __init__() # (Calling __init__() directly raises linter errors) Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Can we ignore the linter error instead of duplicating code ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This actually works! Thanks for the suggestion Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated if block_token_ids: blocks.extend( self._allocator.allocate_immutable_group( Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: we can name it allocate_immutable_blocks to reduce new concepts. can also rename the bs=1 path to be allocate_immutable_block so contrast is clear. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good idea, renamed the functions as you proposed. In addition renamed allocate_mutable => allocate_mutable_block Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated Comment on lines 143 to 196 blocks = self. _blocks [self._num_full_slots // self._block_size:] blocks = self. blocks [self._num_full_slots // self._block_size:] Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment is this working? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Yeah, this invokes the property blocks(..) and it returns self._blocks.list() Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment oh gotcha Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 3 hidden conversations Load more‚Ä¶ vllm/core/block/naive_block.py Outdated token_ids=token_ids, block_size=block_size, block_id=physical_block_id) block.block_pool_id = block_pool_id Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment can we avoid extending the block API for this optimization? we can keep a mapping of object address to block pool id in this class Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Yeah, just replaced with simple class member Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 cadedaniel reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction vllm/core/block/naive_block.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block/naive_block.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block/naive_block.py Outdated assert block.block_id is not None self._free_block_id(block.block_id) block.block_id = None def free(self, block: Block) -> None: Copy link Collaborator cadedaniel Jun 17, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: for readability, have this invoke free_block_id instead of _free_block_id Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good catch, modified to invoke free_block_id directly Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/cpu_gpu_block_allocator.py Outdated @@ -149,6 +169,17 @@ def allocate_immutable(self, prev_block: Optional[Block], return self._allocators[device].allocate_immutable( prev_block, token_ids) def free_block_id(self, block: Block) -> None: Copy link Collaborator cadedaniel Jun 18, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I ran out of time to review today. Can you help me understand why we need a new API for this // if there's no way to combine free_block and free_block_id ? ideally we have one way of freeing Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The issue is that inside cow_block_if_not_appendable(..) (in common.py) we decrement ref count for the block_id for this block, and then in the caller, we reuse the same block object while assigning to its block_id the newly allocated block id (self._block_id = (self._allocator.cow_block_if_not_appendable(..)). Same happens in prefix caching inside _free_block_id_for_block(..) when we promote a naive block to the immutable (prefix block) => we call return self._hashless_allocator.free_block_id(block), and at the caller reuse the same block object. Without the block pool a free() was simply setting block.block_id = None, but with block pool, free(..) is actually releasing the block itself, so the second free_block_id() is behaving more similar to block.block_id = None Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 19, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I will try to restructure the code a bit, so that we don't have the free_block_id. Will keep you posted about this issue. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Sounds good. It sounds like my original design should have had more thought on the distinction between Python block objects and block ids themselves. It's OK if we have some suboptimality given that, but also hope you're able to find a simple solution :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 20, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I was able to refactor the code so that only free() is used at all places. I think it is a good change since it forces an explicit free/alloc calls for block objects, and this avoids potential memory leaks (due to previous separation between the block_id and block - currently they are \"more fused\"). The main things I needed to change is CoW and promote_to_immutable (in prefix-caching). The change moves these two functions to the allocator level (outside of the block itself), since these functions free-and-reallocate a new block, which needs to be updated in the associated lists in block_table.py. To make this cleaner, I added a function in block_table.py that is called \"append_token_ids_and_update_allocator\". In addition, I redid the free() procedure of prefix-caching since it was a bit complicated, by separating the two main cases there: (1) immutable/promoted block and (2) mutable/hashless block. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 20, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I have verified performance it is even a little better now. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 20, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Also, I have squashed the relevant commits to \"refactor code so that only free() is used\" so it will be easier to see the changes I did only for this change. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Collaborator cadedaniel commented Jun 18, 2024 Great work btw! thanks! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Jun 18, 2024 Updated the PR with performance fixes for prefix-caching block_manager_v2. The table above is updated with new numbers for both standard run and prefix-cache enabled run. üéâ 1 cadedaniel reacted with hooray emoji All reactions üéâ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Jun 18, 2024 Will start addressing review comments and cleaning up the PR üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 reviewed Jun 18, 2024 View reviewed changes vllm/core/block/block_table.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link hibukipanim commented Jun 19, 2024 As the PR touches prefix caching and preparing v2-block-manager to be default, I was curious to see if the PR might resolve this correctness issue: #5543 (comment) . and you might be interested to know that when running with this branch (commit c1f650fa7f162eb48763d8eeb70081986379f7e1) with --enable-prefix-caching --use-v2-block-manager , the snippet in the linked issue crashes the server with: ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] Engine background task failed ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] Traceback ( most recent call last ): ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 40 , in _raise_exception_on_finish ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] task . result () ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 521 , in run_engine_loop ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] has_requests_in_progress = await asyncio . wait_for ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/lib/python3.10/asyncio/tasks.py\" , line 445 , in wait_for ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return fut . result () ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 495 , in engine_step ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] request_outputs = await self . engine . step_async () ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 226 , in step_async ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] output = await self . model_executor . execute_model_async ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/executor/gpu_executor.py\" , line 117 , in execute_model_async ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] output = await make_async ( self . driver_worker . execute_model ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/lib/python3.10/concurrent/futures/thread.py\" , line 58 , in run ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] result = self . fn ( * self . args , ** self . kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\" , line 115 , in decorate_context ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return func ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/worker/worker.py\" , line 272 , in execute_model ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] output = self . model_runner . execute_model ( seq_group_metadata_list , ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\" , line 115 , in decorate_context ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return func ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/worker/model_runner.py\" , line 736 , in execute_model ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] hidden_states = model_executable ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . _call_impl ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return forward_call ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 371 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] hidden_states = self . model ( input_ids , positions , kv_caches , ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . _call_impl ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return forward_call ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 288 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] hidden_states , residual = layer ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . _call_impl ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return forward_call ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 227 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] hidden_states = self . self_attn ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . _call_impl ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return forward_call ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 161 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] attn_output = self . attn ( q , k , v , kv_cache , attn_metadata ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . _call_impl ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return forward_call ( * args , ** kwargs ) ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/attention/layer.py\" , line 89 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return self . impl . forward ( query , key , value , kv_cache , attn_metadata , ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/workspace/nm-vllm/vllm/attention/backends/flash_attn.py\" , line 338 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] flash_attn_varlen_func ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 1099 , in flash_attn_varlen_func ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return FlashAttnVarlenFunc . apply ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\" , line 598 , in apply ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] return super (). apply ( * args , ** kwargs ) # type: ignore[misc] ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 596 , in forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] out , q , k , v , out_padded , softmax_lse , S_dmask , rng_state = _flash_attn_varlen_forward ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 88 , in _flash_attn_varlen_forward ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] out , q , k , v , out_padded , softmax_lse , S_dmask , rng_state = flash_attn_cuda . varlen_fwd ( ERROR 06 - 19 07 : 45 : 58 async_llm_engine . py : 45 ] RuntimeError : out must have shape ( total_q , num_heads , head_size_og ) Exception in callback functools . partial ( < function _raise_exception_on_finish at 0x7f2bc22f4160 > , error_callback = < bound method AsyncLLMEngine . _error_callback of < vllm . engine . async_llm_engine . AsyncLLMEngine object at 0x7f2bb73e0910 >> ) handle : < Handle functools . partial ( < function _raise_exception_on_finish at 0x7f2bc22f4160 > , error_callback = < bound method AsyncLLMEngine . _error_callback of < vllm . engine . async_llm_engine . AsyncLLMEngine object at 0x7f2bb73e0910 >> ) > Traceback ( most recent call last ): File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 40 , in _raise_exception_on_finish task . result () File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 521 , in run_engine_loop has_requests_in_progress = await asyncio . wait_for ( File \"/usr/lib/python3.10/asyncio/tasks.py\" , line 445 , in wait_for return fut . result () File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 495 , in engine_step request_outputs = await self . engine . step_async () File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 226 , in step_async output = await self . model_executor . execute_model_async ( File \"/workspace/nm-vllm/vllm/executor/gpu_executor.py\" , line 117 , in execute_model_async output = await make_async ( self . driver_worker . execute_model File \"/usr/lib/python3.10/concurrent/futures/thread.py\" , line 58 , in run result = self . fn ( * self . args , ** self . kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\" , line 115 , in decorate_context return func ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/worker/worker.py\" , line 272 , in execute_model output = self . model_runner . execute_model ( seq_group_metadata_list , File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\" , line 115 , in decorate_context return func ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/worker/model_runner.py\" , line 736 , in execute_model hidden_states = model_executable ( File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl return self . _call_impl ( * args , ** kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl return forward_call ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 371 , in forward hidden_states = self . model ( input_ids , positions , kv_caches , File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl return self . _call_impl ( * args , ** kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl return forward_call ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 288 , in forward hidden_states , residual = layer ( File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl return self . _call_impl ( * args , ** kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl return forward_call ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 227 , in forward hidden_states = self . self_attn ( File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl return self . _call_impl ( * args , ** kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl return forward_call ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/model_executor/models/llama.py\" , line 161 , in forward attn_output = self . attn ( q , k , v , kv_cache , attn_metadata ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1532 , in _wrapped_call_impl return self . _call_impl ( * args , ** kwargs ) File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\" , line 1541 , in _call_impl return forward_call ( * args , ** kwargs ) File \"/workspace/nm-vllm/vllm/attention/layer.py\" , line 89 , in forward return self . impl . forward ( query , key , value , kv_cache , attn_metadata , File \"/workspace/nm-vllm/vllm/attention/backends/flash_attn.py\" , line 338 , in forward flash_attn_varlen_func ( File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 1099 , in flash_attn_varlen_func return FlashAttnVarlenFunc . apply ( File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\" , line 598 , in apply return super (). apply ( * args , ** kwargs ) # type: ignore[misc] File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 596 , in forward out , q , k , v , out_padded , softmax_lse , S_dmask , rng_state = _flash_attn_varlen_forward ( File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\" , line 88 , in _flash_attn_varlen_forward out , q , k , v , out_padded , softmax_lse , S_dmask , rng_state = flash_attn_cuda . varlen_fwd ( RuntimeError : out must have shape ( total_q , num_heads , head_size_og ) The above exception was the direct cause of the following exception : Traceback ( most recent call last ): File \"uvloop/cbhandles.pyx\" , line 63 , in uvloop . loop . Handle . _run File \"/workspace/nm-vllm/vllm/engine/async_llm_engine.py\" , line 47 , in _raise_exception_on_finish raise AsyncEngineDeadError ( vllm . engine . async_llm_engine . AsyncEngineDeadError : Task finished unexpectedly . This should never happen ! Please open an issue on Github . See stack trace above for the actual cause . INFO 06 - 19 07 : 45 : 58 async_llm_engine . py : 158 ] Aborted request cmpl - 4 ce91102896f49d598ec6313f9629a10 - 0. INFO : 172.17 .0 . 1 : 47640 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error ERROR : Exception in ASGI application All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Jun 19, 2024 @hibukipanim thanks for pointing this issue, I will check ‚ù§Ô∏è 1 hibukipanim reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . alexm-redhat marked this pull request as ready for review June 19, 2024 19:03 alexm-redhat force-pushed the block_manager_v2_perf branch\n      2 times, most recently\n    from 0148b6e to e08d643 Compare June 20, 2024 21:34 Yard1 reviewed Jun 21, 2024 View reviewed changes vllm/sequence.py Outdated @property def prompt_token_ids(self) -> List[int]: return self._prompt_token_ids Copy link Collaborator Yard1 Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think we should return a tuple/shallow copy so that this and also output_token_ids doesn't get modified by mistake (and thus bypass _update_cached_all_tokens ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment yeah, what happens if someone modifies the prompt token ids / output token ids list? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good catch, changed the return types to be tuples. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I have changed the approach here to protect accesses to prompt_token_ids and output_token_ids. Now, it uses a class MonitoredList that records a timestamp of the last update, and based on that, the cached all tokens is updated. I did in this way to avoid changing all usages of the prompt/output token ids due to tuple change and also it avoids unnecessary copies of list => tuples which are also expensive. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 Yard1 reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Collaborator Author alexm-redhat Jun 29, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 found out that there is actually an issue with the deserialization with ray, so I have removed this and made the prompt/output token_ids accessors return tuples. It introduces a conversion for the output_token_ids to tuple but it seems not to be bad and the performance is still good. To make it work, I have propagated the tuple type upward in the vllm software stack, since we don't expect seq_data users to use these accessors to change data (but only via the append_token() function) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel commented Jun 21, 2024 ok looking All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cadedaniel reviewed Jun 26, 2024 View reviewed changes Copy link Collaborator cadedaniel left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment most comments are nits. big question is the design change around CoW/promotion (I think it's actually a bad design change). let's schedule some time to go over this sync as I think it will be faster than back and forth. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions examples/offline_inference.py Outdated Comment on lines 14 to 16 llm = LLM(model=\"facebook/opt-125m\") llm = LLM(model=\"facebook/opt-125m\", use_v2_block_manager=True, enable_prefix_caching=True) Copy link Collaborator cadedaniel Jun 20, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Let's leave this out for now Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 26, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment good catch, removed Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/core/block/test_prefix_caching_block.py first_chain = TestPrefixCachingBlockAllocator.create_immutable_chain( block_size=block_size, token_ids=token_ids, allocator=allocator, ) # mark all blocks in first chain as computed allocator.mark_blocks_as_computed(blocks) Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment TODO(cade) see why this api is no longer required Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated from vllm.utils import Device, cdiv, chunk_list # This class is an optimization to allow fast-access to physical block ids Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Let's write this as a docstring Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment suggest writing also how it achieves the optimization (can write docstrings for individual functions but it's more tedious) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated from vllm.utils import Device, cdiv, chunk_list # This class is an optimization to allow fast-access to physical block ids class BlockList: Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: would be great to have basic unit tests for this helper Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated from vllm.utils import Device, cdiv, chunk_list # This class is an optimization to allow fast-access to physical block ids class BlockList: Copy link Collaborator cadedaniel Jun 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: I have preference for putting helper methods/functions below the main class of the file, so the file can be read top-down Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment moved to block/common.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 15 hidden conversations Load more‚Ä¶ vllm/core/block_manager_v2.py Outdated Comment on lines 103 to 104 self._cached_computed_seq_blocks: Dict[SeqId, List[int]] = {} self._seq_last_access: Dict[SeqId, float] = {} Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment what's the motivation for raising these to BlockManger level? we should keep things simple at this layer unless there's good reason not to Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment There was a significant overhead in these function calls, since they traversed the full block lists. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Can we modify the API such that it allows caching the result // we don't have to traverse the full block lists? Two downsides: we expose more complexity in this layer than is necessary (this is tech debt we can live with, if it's too hard) we make it harder for other block managers to use prefix caching (we may have a block manager which specializes for another type, e.g. the newer models which use sliding window + normal attention). Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This is a good idea. I have refactored this logic out to two classes: ComputedBlocksTracker and LastAccessBlocksTracker so it will be easier to port the logic to other places. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager_v2.py Outdated Comment on lines 239 to 240 # TODO: Ask Cade how it may be possible to have # allocated block id inside the evictor Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment let's go over this Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager_v2.py block_ids = self.block_tables[seq.seq_id].physical_block_ids assert all(b is not None for b in block_ids) Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment awesome Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager_v2.py Outdated @@ -274,6 +285,43 @@ def mark_blocks_as_computed(self, seq_group: SequenceGroup): # So this function is useless for block_v2. pass def get_and_update_computed_block_ids(self, seqs): Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment docstring / typing Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/sequence.py Outdated @property def prompt_token_ids(self) -> List[int]: return self._prompt_token_ids Copy link Collaborator cadedaniel Jun 25, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment yeah, what happens if someone modifies the prompt token ids / output token ids list? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions alexm-redhat commented Jun 27, 2024 View reviewed changes Copy link Collaborator Author alexm-redhat left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Updated the PR with addressed review comments from Cade and Yard1. I have moved the CoW and Promo functionality back to the block and ensured that there is no new _free_block_id() interface to minimize interface changes. Also, I had moved the code a bit inside the prefix-caching allocator to make it more readable and easier to maintain. Verified that performance is still good, for both standard and prefix-cached runs. TODO: Fixing tests now Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/naive_block.py Outdated Comment on lines 12 to 19 # Used to pre-allocate block objects, in order to avoid excessive python # object allocations/deallocations. # The pool starts from \"pool_size\" objects and will increase to more objects # if necessary # # Note that multiple block objects may point to the same physical block id, # which is why this pool is needed, so that it will be easier to support # prefix caching and more complicated sharing of physical blocks. Copy link Collaborator Author alexm-redhat Jun 26, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Added docstring and moved BlockPool class to block/common.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py @@ -19,6 +19,28 @@ _DEFAULT_LAST_ACCESSED_TIME = -1 class BlockTracker: Copy link Collaborator Author alexm-redhat Jun 26, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated from vllm.utils import Device, cdiv, chunk_list # This class is an optimization to allow fast-access to physical block ids class BlockList: Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment moved to block/common.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated return self._block_ids def append_token_ids_and_update_allocator( Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Removed this function in favor of moving this logic back into block class Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated block: Block, token_ids: List[int], allocator: DeviceAwareBlockAllocator) -> Block: new_block = allocator.cow_block_if_not_appendable(block) if new_block: Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Removed Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 7 hidden conversations Load more‚Ä¶ vllm/sequence.py Outdated @property def prompt_token_ids(self) -> List[int]: return self._prompt_token_ids Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good catch, changed the return types to be tuples. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated from vllm.utils import Device, cdiv, chunk_list # This class is an optimization to allow fast-access to physical block ids Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py elif block_id in self.evictor: self.evictor.update(block_id, now) else: raise ValueError( \"Mark block as accessed which is not belonged to GPU\") def mark_blocks_as_computed(self, block_ids: List[int]) -> None: \"\"\"Mark blocks as computed , used in prefix caching.\"\"\" raise NotImplementedError(\"Marking as computed is incremental\") Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment For prefix caching, a block is \"computed\" when it is full, so it is possible to use the block.content_hash as the indicator for computed or not computed without the need from the scheduler to explicitly state it. Which is why the original implementation was not doing anything for that case, and this function was never called. I simply replaced the code with an error exception just to make sure it is indeed not used. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py Outdated self._update_num_token_ids() def _update_num_token_ids(self): Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager_v2.py Outdated Comment on lines 103 to 104 self._cached_computed_seq_blocks: Dict[SeqId, List[int]] = {} self._seq_last_access: Dict[SeqId, float] = {} Copy link Collaborator Author alexm-redhat Jun 27, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment There was a significant overhead in these function calls, since they traversed the full block lists. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member DarkLight1337 commented Jun 28, 2024 To speed up the CI queue for #5905 , I've cancelled the distributed tests for the latest CI run in this PR since they won't pass anyway until #5905 has been merged. Please merge main into your branch after that happens so that the CI can pass once again. üëç 2 cadedaniel and alexm-redhat reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cadedaniel reviewed Jun 28, 2024 View reviewed changes vllm/core/block/block_table.py Outdated self._num_full_slots = len(token_ids) def update(self, blocks): Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: typing Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py elif block_id in self.evictor: self.evictor.update(block_id, now) else: raise ValueError( \"Mark block as accessed which is not belonged to GPU\") def mark_blocks_as_computed(self, block_ids: List[int]) -> None: \"\"\"Mark blocks as computed , used in prefix caching.\"\"\" raise NotImplementedError(\"Marking as computed is incremental\") Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Sounds good. let's delete the API? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/common.py allocator=self._allocator, block_id=None)) def increase_pool(self): Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: docstrings on public methods Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment mark_blocks_as_computed still used in block_manager_v1 added docstring Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction vllm/core/block/block_table.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block/cpu_gpu_block_allocator.py Outdated Comment on lines 298 to 328 raise NotImplementedError device = Device.GPU return self._allocators[device].promote_to_immutable_block(block) Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment do we need this implementation and cow_block_if_not_appendable ? technically, vLLM does not support modification of block content for CPU-based allocators Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I assume this method is only invoked when appending tokens Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment yeah Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment add some comment when it's used? (I think they should be removed but seems I miss a case) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment You actually right, this is cpu-gpu allocator, so it is not doing the actual CoW or promo, since it is done only by the specific Naive or Prefix allocators, and they have these functions define via the base class BlockAllocator. Good catch! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction vllm/core/block/cpu_gpu_block_allocator.py Outdated Comment on lines 376 to 379 if self._proxy.token_ids: return len(self._proxy.token_ids) else: return 0 Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Did you see my comment about token_ids being optional? It adds more complexity to the API, and leaks abstraction details here and other places that need to check if it's None before deciding behavior. If we want a no-op token id List for the undefined blocks, we can have a class which implements List and always returns 0 for len / raises NotImplemented for anything that writes. that way we don't have Optional / no branches checking for it everywhere Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 29, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I was able to remove the Optional from token_ids. Now it is the same as before. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 cadedaniel reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction vllm/core/block_manager_v2.py Outdated Comment on lines 103 to 104 self._cached_computed_seq_blocks: Dict[SeqId, List[int]] = {} self._seq_last_access: Dict[SeqId, float] = {} Copy link Collaborator cadedaniel Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Can we modify the API such that it allows caching the result // we don't have to traverse the full block lists? Two downsides: we expose more complexity in this layer than is necessary (this is tech debt we can live with, if it's too hard) we make it harder for other block managers to use prefix caching (we may have a block manager which specializes for another type, e.g. the newer models which use sliding window + normal attention). Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Yard1 reviewed Jun 28, 2024 View reviewed changes vllm/core/block_manager_v2.py Outdated Comment on lines 315 to 326 self._cached_computed_seq_blocks[seq_id] = computed_block_ids else: computed_block_ids = self._cached_computed_seq_blocks[seq_id] if len(computed_block_ids) < len(block_ids): # Incremental init for seq_id => Look only at the new blocks computed_block_ids = self.block_allocator.get_computed_block_ids(  # noqa: E501 computed_block_ids, block_ids) self._cached_computed_seq_blocks[ seq_id] = computed_block_ids else: # Cache HIT assert len(computed_block_ids) == len(block_ids) Copy link Collaborator Yard1 Jun 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This will still result in constant recomputation in the worst case. I think we can do the following: After the first run, if len(computed_block_ids) != len(block_ids) , we know that we will never add any extra blocks to computed_block_ids (since we'd have a gap otherwise). Therefore, we should save that as a boolean in the cache alongside the computed block ids In the subsequent runs, if the seq_id is present in cache, but the boolean is False, we just return the cached computed block ids without calling get_computed_block_ids . Otherwise, if the boolean is true, we call get_computed_block_ids for the new blocks and save in cache, with the len(computed_block_ids) == len(block_ids) boolean. let me know if this makes sense? I may be missing something here. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Yard1 Jun 28, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Here's the suggested change: def _get_and_update_computed_block_ids ( self , seqs ): \"\"\"Handles caching of per-sequence computed block ids. When a sequence appears for the first time, it traverses all of the blocks and detects the prefix of blocks that is computed. On the subsequent times, it only traverses the new blocks that were added and updates the already recorded prefix of blocks with the newly computed blocks. \"\"\" ret = [] for seq in seqs : seq_id = seq . seq_id # Get block ids of this sequence, while not considering the # last block block_ids = self . block_tables [ seq_id ]. physical_block_ids [: - 1 ] # Here we cache the detection of computed_block_ids for seq_id. # Since computed_block_ids form a prefix of block_ids, # the first time we see seq_id, we detect computed_block_ids # fully and store them in the cache. In the next times we see # seq_id, we detect computed_block_ids incrementally, by looking # only at the new blocks that come after the cached # computed_block_ids if seq_id not in self . _cached_computed_seq_blocks : # First time init for seq_id => Detect fully computed_block_ids = self . block_allocator . get_computed_block_ids ( # noqa: E501 [], block_ids ) self . _cached_computed_seq_blocks [ seq_id ] = ( computed_block_ids , len ( computed_block_ids ) >= len ( block_ids ) - 1 ) else : computed_block_ids , should_continue_adding = self . _cached_computed_seq_blocks [ seq_id ] if should_continue_adding : if len ( computed_block_ids ) < len ( block_ids ): # Incremental init for seq_id => Look only at the new blocks computed_block_ids = self . block_allocator . get_computed_block_ids ( # noqa: E501 computed_block_ids , block_ids ) self . _cached_computed_seq_blocks [ seq_id ] = ( computed_block_ids , len ( computed_block_ids ) >= len ( block_ids ) - 1 ) else : # Cache HIT assert len ( computed_block_ids ) == len ( block_ids ) ret . append ( computed_block_ids ) return ret Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 cadedaniel reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Copy link Collaborator Author alexm-redhat Jun 29, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 and I discussed this in more detail and this is a really good suggestion that should help with performance. Will add this to the algorithm. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jun 30, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 Added your idea inside. All works. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions alexm-redhat force-pushed the block_manager_v2_perf branch\n    from 0cd4aae to ac9cbdc Compare June 30, 2024 11:50 Copy link Collaborator Author alexm-redhat commented Jun 30, 2024 @cadedaniel @Yard1 I have addressed the review comments, the PR is ready for a pass. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . alexm-redhat added 9 commits July 1, 2024 00:02 Optimize block_manager_v2 so it becomes the default 007b32d cleanups ea94e85 refactor code so that only free() is used e21c410 prefix_caching: refactor self._blocks to tracked blocks b5872d2 format 54d76ba cpu bug fix 0aecdb2 fixes d649055 fixes 92550b0 fix immutable promotion 4100268 23 hidden items Load more‚Ä¶ alexm-redhat added 14 commits July 1, 2024 00:02 Refactor back token_ids based on Cade comments. b74d834 use tuples for seq_data prompt/output token_ids 179542b sync 7c0ce65 fix 4dd957e fix tests 325226f fix tests 29e9683 add Antoni's idea for improving caching of computed block ids by usin‚Ä¶ ‚Ä¶ c36f353 ‚Ä¶g the gap detection Based on Cade comment, refactored the seq last_access and cached comp‚Ä¶ ‚Ä¶ d0b2ef9 ‚Ä¶uted blocks dicts to be encapsulated inside classes instead of simply embedded in block_manager_v2 cleanup bd65468 Cade's comments 3064208 fix test 2236d5e fix fork_seq 4ea6938 ping 82b31e8 ping2 3f1c2a1 alexm-redhat force-pushed the block_manager_v2_perf branch\n    from 6854308 to 3f1c2a1 Compare July 1, 2024 00:03 cadedaniel mentioned this pull request Jul 1, 2024 [misc][optimization] optimize data structure in allocator #5968 Closed cadedaniel reviewed Jul 1, 2024 View reviewed changes Copy link Collaborator cadedaniel left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment small comments only, let's go! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated block.append_token_ids(token_block) self._blocks[idx] = block  # Refresh the cached block_id Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment is this still necessary? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I redid the code so it is hidden inside the BlockList (by adding append_token_ids(block_idx, tokens) api func) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üéâ 1 cadedaniel reacted with hooray emoji All reactions üéâ 1 reaction vllm/core/block/block_table.py Outdated Comment on lines 301 to 303 cur_token_ids = block.token_ids if cur_token_ids is not None: token_ids.extend(cur_token_ids) Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Remove check now that it can't be None? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good catch! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/block_table.py Outdated Comment on lines 308 to 309 if not self._is_allocated: return 0 Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: I think we don't need this branch anymore. if it's not allocated, self.blocks will be empty Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Nice, removed Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/common.py Comment on lines +129 to +130 assert src_block_id is not None assert trg_block_id is not None Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: a little weird that we check a non-Optional is not None. but my guess it's due to python typing weakness... can ignore Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I changed the type to Optional[BlockId], I think it makes more sense Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction vllm/core/block/cpu_gpu_block_allocator.py Outdated Comment on lines 298 to 328 raise NotImplementedError device = Device.GPU return self._allocators[device].promote_to_immutable_block(block) Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment add some comment when it's used? (I think they should be removed but seems I miss a case) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions 1 hidden conversation Load more‚Ä¶ vllm/core/block/interfaces.py Outdated pass @abstractmethod def promote_to_immutable_block(self, block: Block) -> BlockId: \"\"\"NOTE: This should not be used besides Block\"\"\" Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment suggest keeping the NOTE in Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Added Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py Comment on lines +315 to +321 \"\"\"Decrements the refcount of the block. The block may be in two possible states: (1) immutable/cached or (2) mutable/hashless. In the first case, the refcount is decremented directly and the block may be possibly added to the evictor. In other case, hashless allocator free(..) with keep_block_object=True is called to only free the block id (since the block object may be reused by the caller) \"\"\" Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment love this :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py Outdated @@ -658,6 +801,7 @@ def content_hash(self) -> Optional[int]: if prev_block_hash is None and not is_first_block: return None assert len(self.token_ids) > 0 Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: do we need this assert given if not self.is_full ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment You right, removed Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py Comment on lines +850 to +851 Note that currently, for a given sequence, we also skip the last block id for caching purposes, to avoid caching of a full sequence Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment does this work with lookahead scheduling (where potenially >1 block is modified in single step)? don't have to fix now but in the future we want speculative decoding x prefix caching to work Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think it should work since the blocks that are used for appending or speculative tokens won't be marked as computed, so they won't go into the common cache prefix. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 cadedaniel reacted with thumbs up emoji All reactions üëç 1 reaction vllm/core/block/prefix_caching_block.py Comment on lines +918 to +921 class LastAccessBlocksTracker: \"\"\"Manages the last access time of the tracked sequences, in order to allow an efficient update of allocator's block last access times \"\"\" Copy link Collaborator cadedaniel Jul 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment ‚ù§Ô∏è Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions alexm-redhat added 2 commits July 1, 2024 15:03 Cade's comments 2ff442d more Cade commants 3322f8c Copy link Collaborator Author alexm-redhat commented Jul 1, 2024 @cadedaniel fixed the nits, thanks for catching these issues! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cadedaniel approved these changes Jul 2, 2024 View reviewed changes Copy link Collaborator cadedaniel left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the excellent contribution! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions cadedaniel merged commit 3476ed0 into vllm-project : main Jul 2, 2024 kzawora-intel added a commit\n        to HabanaAI/vllm-fork\n      that referenced\n      this pull request Jul 2, 2024 habana_main rebase ( #71 ) ‚Ä¶ 5e1a565 * [Hardware][Intel] Optimize CPU backend and add more performance tips ( vllm-project#4971 )\n\nCo-authored-by: Jianan Gu <jianan.gu@intel.com>\n\n* [Docs] Add 4th meetup slides ( vllm-project#5509 )\n\n* [Misc] Add vLLM version getter to utils ( vllm-project#5098 )\n\n* [CI/Build] Simplify OpenAI server setup in tests ( vllm-project#5100 )\n\n* [Doc] Update LLaVA docs ( vllm-project#5437 )\n\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Kernel] Factor out epilogues from cutlass kernels ( vllm-project#5391 )\n\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: zifeitong <zifei.tong@parasail.io>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\n\n* [MISC] Remove FP8 warning ( vllm-project#5472 )\n\nCo-authored-by: Philipp Moritz <pcmoritz@gmail.com>\n\n* Seperate dev requirements into lint and test ( vllm-project#5474 )\n\n* Revert \"[Core] Remove unnecessary copies in flash attn backend\" ( vllm-project#5478 )\n\n* [misc] fix format.sh ( vllm-project#5511 )\n\n* [CI/Build] Disable test_fp8.py ( vllm-project#5508 )\n\n* [Kernel] Disable CUTLASS kernels for fp8 ( vllm-project#5505 )\n\n* Add `cuda_device_count_stateless` ( vllm-project#5473 )\n\n* [Hardware][Intel] Support CPU inference with AVX2 ISA ( vllm-project#5452 )\n\n* [Misc] Fix arg names in quantizer script ( vllm-project#5507 )\n\n* bump version to v0.5.0.post1 ( vllm-project#5522 )\n\n* [CI/Build][Misc] Add CI that benchmarks vllm performance on those PRs with `perf-benchmarks` label ( vllm-project#5073 )\n\nCo-authored-by: simon-mo <simon.mo@hey.com>\n\n* [CI/Build] Disable LLaVA-NeXT CPU test ( vllm-project#5529 )\n\n* [Kernel] Fix CUTLASS 3.x custom broadcast load epilogue ( vllm-project#5516 )\n\n* [Misc] Fix arg names ( vllm-project#5524 )\n\n* [ Misc ] Rs/compressed tensors cleanup ( vllm-project#5432 )\n\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\n\n* [Kernel] Suppress mma.sp warning on CUDA 12.5 and later ( vllm-project#5401 )\n\n* [mis] fix flaky test of test_cuda_device_count_stateless ( vllm-project#5546 )\n\n* [Core] Remove duplicate processing in async engine ( vllm-project#5525 )\n\n* [misc][distributed] fix benign error in `is_in_the_same_node` ( vllm-project#5512 )\n\n* [Docs] Add ZhenFund as a Sponsor ( vllm-project#5548 )\n\n* [Doc] Update documentation on Tensorizer ( vllm-project#5471 )\n\n* [Bugfix] Enable loading FP8 checkpoints for gpt_bigcode models  ( vllm-project#5460 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Bugfix] Fix typo in Pallas backend ( vllm-project#5558 )\n\n* [Core][Distributed] improve p2p cache generation ( vllm-project#5528 )\n\n* Add ccache to amd ( vllm-project#5555 )\n\n* [Core][Bugfix]: fix prefix caching for blockv2 ( vllm-project#5364 )\n\nSigned-off-by: Lei Wen <wenlei03@qiyi.com>\nCo-authored-by: Lei Wen <wenlei03@qiyi.com>\n\n* [mypy] Enable type checking for test directory ( vllm-project#5017 )\n\n* [CI/Build] Test both text and token IDs in batched OpenAI Completions API ( vllm-project#5568 )\n\n* [misc] Do not allow to use lora with chunked prefill. ( vllm-project#5538 )\n\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* add gptq_marlin test for bug report vllm-project#5088 ( vllm-project#5145 )\n\n* [BugFix] Don't start a Ray cluster when not using Ray ( vllm-project#5570 )\n\n* [Fix] Correct OpenAI batch response format ( vllm-project#5554 )\n\n* Add basic correctness 2 GPU tests to 4 GPU pipeline ( vllm-project#5518 )\n\n* [CI][BugFix] Flip is_quant_method_supported condition ( vllm-project#5577 )\n\n* [build][misc] limit numpy version ( vllm-project#5582 )\n\n* [Doc] add debugging tips for crash and multi-node debugging ( vllm-project#5581 )\n\n* Fix w8a8 benchmark and add Llama-3-8B ( vllm-project#5562 )\n\n* [Model] Rename Phi3 rope scaling type ( vllm-project#5595 )\n\n* Correct alignment in the seq_len diagram. ( vllm-project#5592 )\n\nCo-authored-by: Liqian Chen <liqian.chen@deeplang.ai>\n\n* [Kernel] `compressed-tensors` marlin 24 support ( vllm-project#5435 )\n\n* [Misc] use AutoTokenizer for benchmark serving when vLLM not installed ( vllm-project#5588 )\n\n* [Hardware][Intel GPU] Add Intel GPU(XPU) inference backend ( vllm-project#3814 )\n\nCo-authored-by: Jiang Li <jiang1.li@intel.com>\nCo-authored-by: Abhilash Majumder <abhilash.majumder@intel.com>\nCo-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>\n\n* [CI/BUILD] Support non-AVX512 vLLM building and testing ( vllm-project#5574 )\n\n* [CI] the readability of benchmarking and prepare for dashboard ( vllm-project#5571 )\n\n[CI] Improve the readability of performance benchmarking results and prepare for upcoming performance dashboard ( vllm-project#5571 )\n\n* [bugfix][distributed] fix 16 gpus local rank arrangement ( vllm-project#5604 )\n\n* [Optimization] use a pool to reuse LogicalTokenBlock.token_ids ( vllm-project#5584 )\n\n* [Bugfix] Fix KV head calculation for MPT models when using GQA ( vllm-project#5142 )\n\n* [Fix] Use utf-8 encoding in entrypoints/openai/run_batch.py ( vllm-project#5606 )\n\n* [Speculative Decoding 1/2 ] Add typical acceptance sampling as one of the sampling techniques in the verifier ( vllm-project#5131 )\n\n* [Model] Initialize Phi-3-vision support ( vllm-project#4986 )\n\n* [Kernel] Add punica dimensions for Granite 13b ( vllm-project#5559 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\n\n* [misc][typo] fix typo ( vllm-project#5620 )\n\n* [Misc] Fix typo ( vllm-project#5618 )\n\n* [CI] Avoid naming different metrics with the same name in performance benchmark ( vllm-project#5615 )\n\n* [bugfix][distributed] improve p2p capability test ( vllm-project#5612 )\n\n[bugfix][distributed] do not error if two processes do not agree on p2p capability ( vllm-project#5612 )\n\n* [Misc] Remove import from transformers logging ( vllm-project#5625 )\n\n* [CI/Build][Misc] Update Pytest Marker for VLMs ( vllm-project#5623 )\n\n* [ci] Deprecate original CI template ( vllm-project#5624 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [Misc] Add OpenTelemetry support ( vllm-project#4687 )\n\nThis PR adds basic support for OpenTelemetry distributed tracing.\nIt includes changes to enable tracing functionality and improve monitoring capabilities.\n\nI've also added a markdown with print-screens to guide users how to use this feature. You can find it here\n\n* [Misc] Add channel-wise quantization support for w8a8 dynamic per token activation quantization ( vllm-project#5542 )\n\n* [ci] Setup Release pipeline and build release wheels with cache ( vllm-project#5610 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [Model] LoRA support added for command-r ( vllm-project#5178 )\n\n* [Bugfix] Fix for inconsistent behaviour related to sampling and repetition penalties  ( vllm-project#5639 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Doc] Added cerebrium as Integration option ( vllm-project#5553 )\n\n* [Bugfix] Fix CUDA version check for mma warning suppression ( vllm-project#5642 )\n\n* [Bugfix] Fix w8a8 benchmarks for int8 case ( vllm-project#5643 )\n\n* [Bugfix] Fix Phi-3 Long RoPE scaling implementation ( vllm-project#5628 )\n\n* [Bugfix] Added test for sampling repetition penalty bug. ( vllm-project#5659 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Bugfix][CI/Build][AMD][ROCm]Fixed the cmake build bug which generate garbage on certain devices ( vllm-project#5641 )\n\n* [misc][distributed] use 127.0.0.1 for single-node ( vllm-project#5619 )\n\n* [Model] Add FP8 kv cache for Qwen2 ( vllm-project#5656 )\n\n* [Bugfix] Fix sampling_params passed incorrectly in Phi3v example ( vllm-project#5684 )\n\n* [Misc]Add param max-model-len in benchmark_latency.py ( vllm-project#5629 )\n\n* [CI/Build] Add tqdm to dependencies ( vllm-project#5680 )\n\n* [ci] Add A100 queue into AWS CI template ( vllm-project#5648 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [Frontend][Bugfix] Fix preemption_mode -> preemption-mode for CLI arg in arg_utils.py ( vllm-project#5688 )\n\n* [ci][distributed] add tests for custom allreduce ( vllm-project#5689 )\n\n* [Bugfix] AsyncLLMEngine hangs with asyncio.run ( vllm-project#5654 )\n\n* [Doc] Update docker references ( vllm-project#5614 )\n\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\n\n* [Misc] Add per channel support for static activation quantization; update w8a8 schemes to share base classes ( vllm-project#5650 )\n\n* [ci] Limit num gpus if specified for A100 ( vllm-project#5694 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [Misc] Improve conftest ( vllm-project#5681 )\n\n* [Bugfix][Doc] FIx Duplicate Explicit Target Name Errors ( vllm-project#5703 )\n\n* [Kernel] Update Cutlass int8 kernel configs for SM90 ( vllm-project#5514 )\n\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Model] Port over CLIPVisionModel for VLMs ( vllm-project#5591 )\n\n* [Kernel] Update Cutlass int8 kernel configs for SM80 ( vllm-project#5275 )\n\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Bugfix] Fix the CUDA version check for FP8 support in the CUTLASS kernels ( vllm-project#5715 )\n\n* [Frontend] Add FlexibleArgumentParser to support both underscore and dash in names ( vllm-project#5718 )\n\n* [distributed][misc] use fork by default for mp ( vllm-project#5669 )\n\n* [Model] MLPSpeculator speculative decoding support ( vllm-project#4947 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: Davis Wertheimer <Davis.Wertheimer@ibm.com>\n\n* [Kernel] Add punica dimension for Qwen2 LoRA ( vllm-project#5441 )\n\n* [BugFix] Fix test_phi3v.py ( vllm-project#5725 )\n\n* [Bugfix] Add  fully sharded layer for QKVParallelLinearWithLora ( vllm-project#5665 )\n\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com>\n\n* [Core][Distributed] add shm broadcast ( vllm-project#5399 )\n\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [Kernel][CPU] Add Quick `gelu` to CPU ( vllm-project#5717 )\n\n* [Doc] Documentation on supported hardware for quantization methods ( vllm-project#5745 )\n\n* [BugFix] exclude version 1.15.0 for modelscope ( vllm-project#5668 )\n\n* [ci][test] fix ca test in main ( vllm-project#5746 )\n\n* [LoRA] Add support for pinning lora adapters in the LRU cache ( vllm-project#5603 )\n\n* [CI][Hardware][Intel GPU] add Intel GPU(XPU) ci pipeline ( vllm-project#5616 )\n\n* [Model] Support Qwen-VL and Qwen-VL-Chat models with text-only inputs ( vllm-project#5710 )\n\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Misc] Remove vllm-project#4789 workaround left in vllm/entrypoints/openai/run_batch.py ( vllm-project#5756 )\n\n* [Bugfix] Fix pin_lora error in TPU executor ( vllm-project#5760 )\n\n* [Docs][TPU] Add installation tip for TPU ( vllm-project#5761 )\n\n* [core][distributed] improve shared memory broadcast ( vllm-project#5754 )\n\n* [BugFix] [Kernel] Add Cutlass2x fallback kernels ( vllm-project#5744 )\n\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Distributed] Add send and recv helpers ( vllm-project#5719 )\n\n* [Bugfix] Add phi3v resize for dynamic shape and fix torchvision requirement ( vllm-project#5772 )\n\n* [doc][faq] add warning to download models for every nodes ( vllm-project#5783 )\n\n* post-rebase api adjustments\n\n* [Doc] Add \"Suggest edit\" button to doc pages ( vllm-project#5789 )\n\n* [Doc] Add Phi-3-medium to list of supported models ( vllm-project#5788 )\n\n* [Bugfix] Fix FlexibleArgumentParser replaces _ with - for actual args ( vllm-project#5795 )\n\n* [ci] Remove aws template ( vllm-project#5757 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [Doc] Add notice about breaking changes to VLMs ( vllm-project#5818 )\n\n* [Speculative Decoding] Support draft model on different tensor-parallel size than target model ( vllm-project#5414 )\n\n* add pin_lora to habana components\n\n* add WA for model loader\n\n* fix api mismatches with ray\n\n* tensor parallel fixes\n\n* workers cpu alignment fix\n\n* [Misc] Remove useless code in cpu_worker ( vllm-project#5824 )\n\n* prefill/decode metadata fixes\n\n* [Core] Add fault tolerance for `RayTokenizerGroupPool` ( vllm-project#5748 )\n\n* re-enable attn metadata trimming\n\n* worker_use_ray fix\n\n* [doc][distributed] add both gloo and nccl tests ( vllm-project#5834 )\n\n* [CI/Build] Add unit testing for FlexibleArgumentParser ( vllm-project#5798 )\n\n* [Misc] Update `w4a16` `compressed-tensors` support to include `w8a16` ( vllm-project#5794 )\n\n* [Hardware][TPU] Refactor TPU backend ( vllm-project#5831 )\n\n* [Hardware][AMD][CI/Build][Doc] Upgrade to ROCm 6.1, Dockerfile improvements, test fixes ( vllm-project#5422 )\n\n* [Hardware][TPU] Raise errors for unsupported sampling params ( vllm-project#5850 )\n\n* [CI/Build] Add E2E tests for MLPSpeculator ( vllm-project#5791 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Bugfix] Fix assertion in NeuronExecutor ( vllm-project#5841 )\n\n* [Core] Refactor Worker and ModelRunner to consolidate control plane communication ( vllm-project#5408 )\n\nSigned-off-by: Stephanie Wang <swang@cs.berkeley.edu>\nSigned-off-by: Stephanie <swang@anyscale.com>\nCo-authored-by: Stephanie <swang@anyscale.com>\n\n* [Misc][Doc] Add Example of using OpenAI Server with VLM ( vllm-project#5832 )\n\n* [bugfix][distributed] fix shm broadcast when the queue size is full ( vllm-project#5801 )\n\n* [Bugfix] Fix embedding to support 2D inputs ( vllm-project#5829 )\n\n* [Bugfix][TPU] Fix KV cache size calculation ( vllm-project#5860 )\n\n* [CI/Build] Refactor image test assets ( vllm-project#5821 )\n\n* [Kernel] Adding bias epilogue support for `cutlass_scaled_mm` ( vllm-project#5560 )\n\nCo-authored-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nCo-authored-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Frontend] Add tokenize/detokenize endpoints ( vllm-project#5054 )\n\n* [Hardware][TPU] Support parallel sampling & Swapping ( vllm-project#5855 )\n\n* [Bugfix][TPU] Fix CPU cache allocation ( vllm-project#5869 )\n\n* Support CPU inference with VSX PowerPC ISA ( vllm-project#5652 )\n\n* [doc] update usage of env var to avoid conflict ( vllm-project#5873 )\n\n* [Misc] Add example for LLaVA-NeXT ( vllm-project#5879 )\n\n* [BugFix] Fix cuda graph for MLPSpeculator ( vllm-project#5875 )\n\nCo-authored-by: Abhinav Goyal <abhinav.goyal@flipkart.com>\n\n* [Doc] Add note about context length in Phi-3-Vision example ( vllm-project#5887 )\n\n* [VLM][Bugfix] Make sure that `multi_modal_kwargs` is broadcasted properly ( vllm-project#5880 )\n\nSigned-off-by: Xiaowei Jiang <xwjiang2010@gmail.com>\n\n* [Model] Add base class for LoRA-supported models ( vllm-project#5018 )\n\n* [Bugfix] Fix img_sizes Parsing in Phi3-Vision ( vllm-project#5888 )\n\n* [CI/Build] [1/3] Reorganize entrypoints tests ( vllm-project#5526 )\n\n* add collective crash WA\n\n* add comment to the weird mark_step\n\n* [Model][Bugfix] Implicit model flags and reenable Phi-3-Vision ( vllm-project#5896 )\n\n* [doc][misc] add note for Kubernetes users ( vllm-project#5916 )\n\n* [BugFix] Fix `MLPSpeculator` handling of `num_speculative_tokens` ( vllm-project#5876 )\n\n* [BugFix] Fix `min_tokens` behaviour for multiple eos tokens ( vllm-project#5849 )\n\n* [CI/Build] Fix Args for `_get_logits_warper` in Sampler Test ( vllm-project#5922 )\n\n* [Model] Add Gemma 2 ( vllm-project#5908 )\n\n* [core][misc] remove logical block ( vllm-project#5882 )\n\n* [Kernel][ROCm][AMD] fused_moe Triton configs v2 for mi300X ( vllm-project#5932 )\n\n* [Hardware][TPU] Optimize KV cache swapping ( vllm-project#5878 )\n\n* [VLM][BugFix] Make sure that `multi_modal_kwargs` can broadcast properly with ring buffer. ( vllm-project#5905 )\n\nSigned-off-by: Xiaowei Jiang <xwjiang2010@gmail.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix][Hardware][Intel CPU] Fix unpassed multi_modal_kwargs for CPU runner ( vllm-project#5956 )\n\n* [Core] Registry for processing model inputs ( vllm-project#5214 )\n\nCo-authored-by: ywang96 <ywang@roblox.com>\n\n* Unmark fused_moe config json file as executable ( vllm-project#5960 )\n\n* [Hardware][Intel] OpenVINO vLLM backend ( vllm-project#5379 )\n\n* [Bugfix] Better error message for MLPSpeculator when `num_speculative_tokens` is set too high ( vllm-project#5894 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [CI/Build] [2/3] Reorganize entrypoints tests ( vllm-project#5904 )\n\n* [Distributed] Make it clear that % should not be in tensor dict keys. ( vllm-project#5927 )\n\nSigned-off-by: Xiaowei Jiang <xwjiang2010@gmail.com>\n\n* [Spec Decode] Introduce DraftModelRunner ( vllm-project#5799 )\n\n* [Bugfix] Fix compute datatype for cutlass 3.x epilogues ( vllm-project#5931 )\n\n* [ Misc ] Remove `fp8_shard_indexer` from Col/Row Parallel Linear (Simplify Weight Loading) ( vllm-project#5928 )\n\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* [ Bugfix ] Enabling Loading Models With Fused QKV/MLP on Disk with FP8 ( vllm-project#5921 )\n\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* Support Deepseek-V2 ( vllm-project#4650 )\n\nCo-authored-by: Philipp Moritz <pcmoritz@gmail.com>\n\n* [Bugfix] Only add `Attention.kv_scale` if kv cache quantization is enabled ( vllm-project#5936 )\n\n* Unmark more files as executable ( vllm-project#5962 )\n\n* [Bugfix] Fix Engine Failing After Invalid Request - AsyncEngineDeadError ( vllm-project#5963 )\n\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* [Kernel] Flashinfer for prefill & decode, with Cudagraph support for decode ( vllm-project#4628 )\n\nCo-authored-by: LiuXiaoxuanPKU <llilyliupku@gmail.com>, bong-furiosa <bongwon.jang@furiosa.ai>\n\n* [Bugfix][TPU] Fix TPU sampler output ( vllm-project#5978 )\n\n* [Bugfix][TPU] Fix pad slot id ( vllm-project#5977 )\n\n* [Bugfix] fix missing last itl in openai completions benchmark ( vllm-project#5926 )\n\n* [Misc] Extend vLLM Metrics logging API ( vllm-project#5925 )\n\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com>\n\n* [Kernel] Add punica dimensions for Granite 3b and 8b ( vllm-project#5930 )\n\nSigned-off-by: Joe Runde <joe@joerun.de>\n\n* [Bugfix] Fix precisions in Gemma 1 ( vllm-project#5913 )\n\n* [Misc] Update Phi-3-Vision Example ( vllm-project#5981 )\n\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Bugfix] Support `eos_token_id` from `config.json` ( vllm-project#5954 )\n\n* [Core] Optimize `SequenceStatus.is_finished` by switching to IntEnum ( vllm-project#5974 )\n\n* [Kernel] Raise an exception in MoE kernel if the batch size is larger then 65k ( vllm-project#5939 )\n\n* [ CI/Build ] Added E2E Test For Compressed Tensors ( vllm-project#5839 )\n\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* [CI/Build] Add TP test for vision models ( vllm-project#5892 )\n\n* [ CI/Build ] LM Eval Harness Based CI Testing ( vllm-project#5838 )\n\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* [Bugfix][CI/Build][Hardware][AMD] Install matching torchvision to fix AMD tests ( vllm-project#5949 )\n\n* [CI/Build] Temporarily Remove Phi3-Vision from TP Test ( vllm-project#5989 )\n\n* [CI/Build] Reuse code for checking output consistency ( vllm-project#5988 )\n\n* [CI/Build] [3/3] Reorganize entrypoints tests ( vllm-project#5966 )\n\n* [ci][distributed] fix device count call\n\n[ci][distributed] fix some cuda init that makes it necessary to use spawn ( vllm-project#5991 )\n\n* [Frontend]: Support base64 embedding ( vllm-project#5935 )\n\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Lora] Use safetensor keys instead of adapter_config.json to find unexpected modules.  ( vllm-project#5909 )\n\nCo-authored-by: sang <sangcho@anyscale.com>\n\n* [ CI ] Temporarily Disable Large LM-Eval Tests ( vllm-project#6005 )\n\nCo-authored-by: rshaw@neuralmagic.com <rshaw@neuralmagic>\n\n* [Misc] Fix `get_min_capability` ( vllm-project#5971 )\n\n* [ Misc ] Refactor w8a8 to use `process_weights_after_load` (Simplify Weight Loading) ( vllm-project#5940 )\n\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\n\n* [misc][cuda] use nvml to avoid accidentally cuda initialization ( vllm-project#6007 )\n\n* [Speculative Decoding 2/2 ] Integrate typical acceptance sampler into Spec Decode Worker ( vllm-project#5348 )\n\n* Revert test changes\n\n* cleanup\n\n* llm engine cleanup\n\n* utils.py cleanup\n\n* custom ops refactor\n\n* move xops to ops\n\n* remove vllm/hpu/attn_bias.py\n\n* whitespace fix\n\n* revert accidental changes in rmsnorm\n\n* Fix hpugraph hashing\n\n* add trim_attn_metadata comment\n\n* fix prompt bucketing:\n\n* [ CI ] Re-enable Large Model LM Eval ( vllm-project#6031 )\n\n* [doc][misc] remove deprecated api server in doc ( vllm-project#6037 )\n\n* [Misc] update benchmark backend for scalellm ( vllm-project#6018 )\n\n* [doc][misc] further lower visibility of simple api server ( vllm-project#6041 )\n\nCo-authored-by: Simon Mo <simon.mo@hey.com>\n\n* [Bugfix] Use RayActorError for older versions of Ray in  RayTokenizerGroupPool ( vllm-project#6039 )\n\n* [Bugfix] adding chunking mechanism to fused_moe to handle large inputs ( vllm-project#6029 )\n\n* add FAQ doc under 'serving' ( vllm-project#5946 )\n\n* [Bugfix][Doc] Fix Doc Formatting ( vllm-project#6048 )\n\n* [Bugfix] Add explicit `end_forward` calls to flashinfer ( vllm-project#6044 )\n\n* [BugFix] Ensure worker model loop is always stopped at the right time ( vllm-project#5987 )\n\n* [Frontend] Relax api url assertion for openai benchmarking ( vllm-project#6046 )\n\n* [Model] Changes to MLPSpeculator to support tie_weights and input_scale ( vllm-project#5965 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Joshua Rosenkranz <jmrosenk@us.ibm.com>\n\n* [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  ( vllm-project#5602 )\n\n* [Frontend] Add template related params to request ( vllm-project#5709 )\n\n* [VLM] Remove `image_input_type` from VLM config ( vllm-project#5852 )\n\nSigned-off-by: Xiaowei Jiang <xwjiang2010@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [Doc] Reinstate doc dependencies ( vllm-project#6061 )\n\n* guard model loader wa for hpu\n\n---------\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nSigned-off-by: Lei Wen <wenlei03@qiyi.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: kevin <kevin@anyscale.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Stephanie Wang <swang@cs.berkeley.edu>\nSigned-off-by: Stephanie <swang@anyscale.com>\nSigned-off-by: Xiaowei Jiang <xwjiang2010@gmail.com>\nSigned-off-by: Joe Runde <joe@joerun.de>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Jianan Gu <jianan.gu@intel.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: zifeitong <zifei.tong@parasail.io>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Philipp Moritz <pcmoritz@gmail.com>\nCo-authored-by: Antoni Baum <antoni.baum@protonmail.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Allen.Dou <allen.dou@hotmail.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\nCo-authored-by: Sanger Steel <sangersteel@gmail.com>\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: leiwen83 <leiwen83@users.noreply.github.com>\nCo-authored-by: Lei Wen <wenlei03@qiyi.com>\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: Amit Garg <gargamit@microsoft.com>\nCo-authored-by: Charles Riggins <liqianchen123@foxmail.com>\nCo-authored-by: Liqian Chen <liqian.chen@deeplang.ai>\nCo-authored-by: zhyncs <me@zhyncs.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: Abhilash Majumder <abhilash.majumder@intel.com>\nCo-authored-by: Abhilash Majumder <30946547+abhilash1910@users.noreply.github.com>\nCo-authored-by: Bruce Fontaine <bruce@2.7182.net>\nCo-authored-by: zifeitong <zifeitong@gmail.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Joe Runde <joe@joerun.de>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Ronen Schaffer <ronen.schaffer@ibm.com>\nCo-authored-by: sergey-tinkoff <167607910+sergey-tinkoff@users.noreply.github.com>\nCo-authored-by: milo157 <43028253+milo157@users.noreply.github.com>\nCo-authored-by: Shukant Pal <SukantK2002@outlook.com>\nCo-authored-by: Hongxia Yang <62075498+hongxiayang@users.noreply.github.com>\nCo-authored-by: DearPlanet <junsong.zhang2021.work@outlook.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Joshua Rosenkranz <joshua.rosenkranz@gmail.com>\nCo-authored-by: Davis Wertheimer <Davis.Wertheimer@ibm.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Jee Li <pandaleefree@163.com>\nCo-authored-by: rohithkrn <rohith.nallamaddi@gmail.com>\nCo-authored-by: Murali Andoorveedu <37849411+andoorve@users.noreply.github.com>\nCo-authored-by: Woo-Yeon Lee <wooyeonlee0@gmail.com>\nCo-authored-by: Matt Wong <156021403+mawong-amd@users.noreply.github.com>\nCo-authored-by: aws-patlange <90803007+aws-patlange@users.noreply.github.com>\nCo-authored-by: Stephanie Wang <swang@cs.berkeley.edu>\nCo-authored-by: Stephanie <swang@anyscale.com>\nCo-authored-by: Luka Govediƒç <ProExpertProg@users.noreply.github.com>\nCo-authored-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nCo-authored-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: sasha0552 <admin@sasha0552.org>\nCo-authored-by: Chip Kerchner <49959681+ChipKerchner@users.noreply.github.com>\nCo-authored-by: Abhinav Goyal <abhinav.goyal@flipkart.com>\nCo-authored-by: xwjiang2010 <87673679+xwjiang2010@users.noreply.github.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic>\nCo-authored-by: wangding zeng <155410488+zwd003@users.noreply.github.com>\nCo-authored-by: Lily Liu <lilyliupku@gmail.com>\nCo-authored-by: LiuXiaoxuanPKU <llilyliupku@gmail.com>, bong-furiosa <bongwon.jang@furiosa.ai>\nCo-authored-by: mcalman <68564154+mcalman@users.noreply.github.com>\nCo-authored-by: William Lin <SolitaryThinker@users.noreply.github.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: llmpros <10524065+llmpros@users.noreply.github.com>\nCo-authored-by: sang <sangcho@anyscale.com>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: James Whedbee <jamesw@telnyx.com>\nCo-authored-by: Joshua Rosenkranz <jmrosenk@us.ibm.com>\nCo-authored-by: danieljannai21 <100521221+danieljannai21@users.noreply.github.com> CatherineSue reviewed Jul 2, 2024 View reviewed changes vllm/core/block/prefix_caching_block.py from os.path import commonprefix from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple from vllm.core.block.common import (CopyOnWriteTracker, get_all_blocks_recursively) from vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device from vllm.core.block.naive_block import NaiveBlock, NaiveBlockAllocator from vllm.core.block.naive_block import (BlockPool, NaiveBlock, Copy link Contributor CatherineSue Jul 2, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment qq: Why import BlockPool from vllm.core.block.naive_block instead of vllm.core.block.common ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Jul 2, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Forgot to change it. It was originally in naive_block. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 CatherineSue reacted with thumbs up emoji All reactions üëç 1 reaction prashantgupta24 pushed a commit\n        to opendatahub-io/vllm\n      that referenced\n      this pull request Jul 3, 2024 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ 549c660 ‚Ä¶ult) ( vllm-project#5602 ) robertgshaw2-redhat pushed a commit\n        to neuralmagic/nm-vllm\n      that referenced\n      this pull request Jul 7, 2024 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ 77f588c ‚Ä¶ult) ( vllm-project#5602 ) xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Jul 8, 2024 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ efecae2 ‚Ä¶ult) ( vllm-project#5602 ) xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Jul 24, 2024 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ efceec4 ‚Ä¶ult) ( vllm-project#5602 ) Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ 4795da5 ‚Ä¶ult) ( vllm-project#5602 )\n\nSigned-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 defa‚Ä¶ ‚Ä¶ a93ba05 ‚Ä¶ult) ( vllm-project#5602 )\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:40",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: LM-Eval | PERF: itl, benchmark serving, optimization | SERVING: serving, serving, api server | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:48:40",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct",
    "Qwen/Qwen2.5-7B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,dtype=float16 --tasks hellaswag,arc_challenge --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,dtype=float16 --tasks hellaswag,arc_challenge --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)",
  "commit_message": "[Core] Optimize block_manager_v2 vs block_manager_v1 (to make V2 default)  (#5602)",
  "commit_date": "2024-07-01T20:10:37-07:00",
  "files_changed": [
    "benchmarks/benchmark_latency.py",
    "tests/conftest.py",
    "tests/core/block/test_block_table.py",
    "tests/core/block/test_cpu_gpu_block_allocator.py",
    "tests/core/block/test_naive_block.py",
    "tests/core/block/test_prefix_caching_block.py",
    "tests/spec_decode/test_batch_expansion.py",
    "vllm/core/block/block_table.py",
    "vllm/core/block/common.py",
    "vllm/core/block/cpu_gpu_block_allocator.py",
    "vllm/core/block/interfaces.py",
    "vllm/core/block/naive_block.py",
    "vllm/core/block/prefix_caching_block.py",
    "vllm/core/block_manager_v2.py",
    "vllm/engine/llm_engine.py",
    "vllm/entrypoints/openai/serving_completion.py",
    "vllm/model_executor/sampling_metadata.py",
    "vllm/outputs.py",
    "vllm/sequence.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 6,
    "num_non_test_files": 13,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 19,
    "num_hunks": 107,
    "num_edited_lines": 1721,
    "num_non_test_edited_lines": 1570,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/benchmarks/benchmark_latency.py b/benchmarks/benchmark_latency.py\nindex a46ee1581..8d0554b0f 100644\n--- a/benchmarks/benchmark_latency.py\n+++ b/benchmarks/benchmark_latency.py\n@@ -46,6 +46,7 @@ def main(args: argparse.Namespace):\n         load_format=args.load_format,\n         distributed_executor_backend=args.distributed_executor_backend,\n         otlp_traces_endpoint=args.otlp_traces_endpoint,\n+        enable_prefix_caching=args.enable_prefix_caching,\n     )\n \n     sampling_params = SamplingParams(\n@@ -220,6 +221,9 @@ if __name__ == '__main__':\n         action='store_true',\n         help='If True, the prefill requests can be chunked based on the '\n         'max_num_batched_tokens')\n+    parser.add_argument(\"--enable-prefix-caching\",\n+                        action='store_true',\n+                        help=\"Enable automatic prefix caching\")\n     parser.add_argument('--use-v2-block-manager', action='store_true')\n     parser.add_argument(\n         \"--ray-workers-use-nsight\",\ndiff --git a/tests/conftest.py b/tests/conftest.py\nindex 0bd24905e..ac802d03b 100644\n--- a/tests/conftest.py\n+++ b/tests/conftest.py\n@@ -474,7 +474,7 @@ class VllmRunner:\n             req_sample_output_strs: List[str] = []\n             for sample in req_output.outputs:\n                 output_str = sample.text\n-                output_ids = sample.token_ids\n+                output_ids = list(sample.token_ids)\n                 req_sample_output_ids.append(prompt_ids + output_ids)\n                 req_sample_output_strs.append(prompt_str + output_str)\n             outputs.append((req_sample_output_ids, req_sample_output_strs))\ndiff --git a/tests/core/block/test_block_table.py b/tests/core/block/test_block_table.py\nindex 496774c8d..e2391a568 100644\n--- a/tests/core/block/test_block_table.py\n+++ b/tests/core/block/test_block_table.py\n@@ -373,8 +373,9 @@ def test_cow(block_size: int, sequence_len: int, append_len: int,\n                                    block_size) - (sequence_len // block_size)\n \n     original_block_table.allocate(token_ids=token_ids, device=Device.GPU)\n-    original_block_ids = original_block_table.physical_block_ids\n+    original_block_ids = original_block_table.physical_block_ids[:]\n \n+    print(\"original_block_ids = {}\".format(original_block_ids))\n     forked_block_table = original_block_table.fork()\n \n     # Expect no additional allocation (copy on _write_).\n@@ -457,7 +458,7 @@ def test_cow_lookahead_simple(block_size: int, sequence_len: int,\n \n     # Allocate lookahead slots.\n     original_block_table.ensure_num_empty_slots(lookahead_slots)\n-    original_block_ids = original_block_table.physical_block_ids\n+    original_block_ids = original_block_table.physical_block_ids[:]\n \n     forked_block_table = original_block_table.fork()\n \ndiff --git a/tests/core/block/test_cpu_gpu_block_allocator.py b/tests/core/block/test_cpu_gpu_block_allocator.py\nindex 44a5be6c1..15b76d909 100644\n--- a/tests/core/block/test_cpu_gpu_block_allocator.py\n+++ b/tests/core/block/test_cpu_gpu_block_allocator.py\n@@ -8,8 +8,8 @@ from vllm.utils import Device, chunk_list\n @pytest.mark.parametrize(\"num_gpu_blocks\", [1024])\n @pytest.mark.parametrize(\"block_size\", [16])\n @pytest.mark.parametrize(\"allocator_type\", [\"naive\", \"prefix_caching\"])\n-def test_allocate_mutable(num_cpu_blocks: int, num_gpu_blocks: int,\n-                          block_size: int, allocator_type: str):\n+def test_allocate_mutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n+                                block_size: int, allocator_type: str):\n     allocator = CpuGpuBlockAllocator.create(\n         allocator_type=allocator_type,\n         num_gpu_blocks=num_gpu_blocks,\n@@ -21,14 +21,14 @@ def test_allocate_mutable(num_cpu_blocks: int, num_gpu_blocks: int,\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n \n     cpu_blocks = [\n-        allocator.allocate_mutable(prev_block=None, device=Device.CPU)\n+        allocator.allocate_mutable_block(prev_block=None, device=Device.CPU)\n         for _ in range(num_cpu_blocks)\n     ]\n     assert allocator.get_num_free_blocks(Device.CPU) == 0\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n \n     gpu_blocks = [\n-        allocator.allocate_mutable(prev_block=None, device=Device.GPU)\n+        allocator.allocate_mutable_block(prev_block=None, device=Device.GPU)\n         for _ in range(num_gpu_blocks)\n     ]\n     assert allocator.get_num_free_blocks(Device.CPU) == 0\n@@ -47,8 +47,8 @@ def test_allocate_mutable(num_cpu_blocks: int, num_gpu_blocks: int,\n @pytest.mark.parametrize(\"num_gpu_blocks\", [1024])\n @pytest.mark.parametrize(\"block_size\", [2])\n @pytest.mark.parametrize(\"allocator_type\", [\"naive\", \"prefix_caching\"])\n-def test_allocate_immutable(num_cpu_blocks: int, num_gpu_blocks: int,\n-                            block_size: int, allocator_type: str):\n+def test_allocate_immutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n+                                  block_size: int, allocator_type: str):\n     allocator = CpuGpuBlockAllocator.create(\n         allocator_type=allocator_type,\n         num_gpu_blocks=num_gpu_blocks,\n@@ -67,18 +67,18 @@ def test_allocate_immutable(num_cpu_blocks: int, num_gpu_blocks: int,\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n \n     cpu_blocks = [\n-        allocator.allocate_immutable(prev_block=None,\n-                                     token_ids=token_ids,\n-                                     device=Device.CPU)\n+        allocator.allocate_immutable_block(prev_block=None,\n+                                           token_ids=token_ids,\n+                                           device=Device.CPU)\n         for token_ids in cpu_token_ids\n     ]\n     assert allocator.get_num_free_blocks(Device.CPU) == 0\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n \n     gpu_blocks = [\n-        allocator.allocate_immutable(prev_block=None,\n-                                     token_ids=token_ids,\n-                                     device=Device.GPU)\n+        allocator.allocate_immutable_block(prev_block=None,\n+                                           token_ids=token_ids,\n+                                           device=Device.GPU)\n         for token_ids in gpu_token_ids\n     ]\n     assert allocator.get_num_free_blocks(Device.CPU) == 0\ndiff --git a/tests/core/block/test_naive_block.py b/tests/core/block/test_naive_block.py\nindex edcdc0c7d..9821ac41b 100644\n--- a/tests/core/block/test_naive_block.py\n+++ b/tests/core/block/test_naive_block.py\n@@ -14,11 +14,11 @@ class TestNaiveBlockAllocator:\n                                prev_block: Optional[Block],\n                                token_ids: List[int]):\n         if allocate_type == \"immutable\":\n-            allocate_block = lambda: allocator.allocate_immutable(\n+            allocate_block = lambda: allocator.allocate_immutable_block(\n                 prev_block=prev_block, token_ids=token_ids)\n         elif allocate_type == \"mutable\":\n-            allocate_block = lambda: allocator.allocate_mutable(prev_block=\n-                                                                prev_block)\n+            allocate_block = lambda: allocator.allocate_mutable_block(\n+                prev_block=prev_block)\n         else:\n             raise ValueError()\n \ndiff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex fcf32cbe9..95858268a 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -26,11 +26,10 @@ class TestPrefixCachingBlock:\n         token_ids = list(range(num_to_fill))\n         mock_allocator = MagicMock(spec=PrefixCachingBlockAllocator)\n \n-        block_with_prev = PrefixCachingBlock(\n-            prev_block=None,\n-            token_ids=token_ids,\n-            block_size=block_size,\n-            prefix_caching_allocator=mock_allocator)\n+        block_with_prev = PrefixCachingBlock(prev_block=None,\n+                                             token_ids=token_ids,\n+                                             block_size=block_size,\n+                                             allocator=mock_allocator)\n \n         if is_curr_block_full:\n             # Expect hash since block is full.\n@@ -71,7 +70,7 @@ class TestPrefixCachingBlock:\n             prev_block=previous_block,\n             token_ids=token_ids,\n             block_size=block_size,\n-            prefix_caching_allocator=mock_allocator,\n+            allocator=mock_allocator,\n         )\n \n         if is_curr_block_full and prev_block_has_hash:\n@@ -138,7 +137,7 @@ class TestPrefixCachingBlock:\n                 prev_block=prev_block,\n                 token_ids=[],\n                 block_size=block_size,\n-                prefix_caching_allocator=allocator,\n+                allocator=allocator,\n             )\n \n             tokens_to_append = token_ids[block_number *\n@@ -159,11 +158,11 @@ class TestPrefixCachingBlockAllocator:\n                                prev_block: Optional[Block],\n                                token_ids: List[int]):\n         if allocate_type == \"immutable\":\n-            allocate_block = lambda: allocator.allocate_immutable(\n+            allocate_block = lambda: allocator.allocate_immutable_block(\n                 prev_block=prev_block, token_ids=token_ids)\n         elif allocate_type == \"mutable\":\n-            allocate_block = lambda: allocator.allocate_mutable(prev_block=\n-                                                                prev_block)\n+            allocate_block = lambda: allocator.allocate_mutable_block(\n+                prev_block=prev_block)\n         else:\n             raise ValueError()\n \n@@ -233,12 +232,13 @@ class TestPrefixCachingBlockAllocator:\n \n         # Expect allocation with unseen hash to fail.\n         with pytest.raises(BlockAllocator.NoFreeBlocksError):\n-            allocator.allocate_immutable(prev_block=chain[-1],\n-                                         token_ids=list(range(block_size)))\n+            allocator.allocate_immutable_block(prev_block=chain[-1],\n+                                               token_ids=list(\n+                                                   range(block_size)))\n \n         # Expect mutable allocation to fail.\n         with pytest.raises(BlockAllocator.NoFreeBlocksError):\n-            allocator.allocate_mutable(prev_block=chain[-1])\n+            allocator.allocate_mutable_block(prev_block=chain[-1])\n \n         # Expect allocation of exact same chain to pass.\n         second_chain = TestPrefixCachingBlockAllocator.create_immutable_chain(\n@@ -270,7 +270,7 @@ class TestPrefixCachingBlockAllocator:\n \n         # Expect mutable allocation to fail.\n         with pytest.raises(BlockAllocator.NoFreeBlocksError):\n-            allocator.allocate_mutable(prev_block=None)\n+            allocator.allocate_mutable_block(prev_block=None)\n \n         block_to_free = chain[-1]\n \n@@ -280,11 +280,11 @@ class TestPrefixCachingBlockAllocator:\n             allocator.free(block_to_free)\n             assert block_to_free.block_id is None, i\n \n-            new_block = allocator.allocate_mutable(prev_block=None)\n+            new_block = allocator.allocate_mutable_block(prev_block=None)\n             assert new_block.block_id == block_id, i\n \n             with pytest.raises(BlockAllocator.NoFreeBlocksError):\n-                allocator.allocate_mutable(prev_block=None)\n+                allocator.allocate_mutable_block(prev_block=None)\n \n             block_to_free = new_block\n \n@@ -376,7 +376,6 @@ class TestPrefixCachingBlockAllocator:\n \n         # Create token ids that will exhaust all blocks.\n         token_ids = list(range(num_blocks_to_consume * block_size))\n-        blocks = list(range(num_blocks_to_consume))\n \n         first_chain = TestPrefixCachingBlockAllocator.create_immutable_chain(\n             block_size=block_size,\n@@ -384,9 +383,6 @@ class TestPrefixCachingBlockAllocator:\n             allocator=allocator,\n         )\n \n-        # mark all blocks in first chain as computed\n-        allocator.mark_blocks_as_computed(blocks)\n-\n         # After zero_point, second_chain's token_ids would be set -1, which\n         # make it different from here comparing with first_chain\n         zero_point = random.randint(1, len(token_ids) - 1)\n@@ -424,15 +420,16 @@ class TestPrefixCachingBlockAllocator:\n                                                 block_size=block_size)\n         token_ids = list(range(block_size))\n \n-        block = allocator.allocate_immutable(prev_block=None,\n-                                             token_ids=token_ids)\n+        block = allocator.allocate_immutable_block(prev_block=None,\n+                                                   token_ids=token_ids)\n \n         assert allocator._refcounter.get(block.block_id) == 1\n-        m = allocator.allocate_mutable(prev_block=None)\n+        m = allocator.allocate_mutable_block(prev_block=None)\n \n         block_id = m.block_id\n         for i in range(block_size):\n             m.append_token_ids([i])\n+\n         # After block get promoted to immutable from mutable, if there is\n         # already same content hash block, then it shall be released into\n         # hashless_allocator\n@@ -452,48 +449,79 @@ class TestPrefixCachingBlockAllocator:\n \n         all_blocks_list = [i for i in range(num_blocks)]\n         zero_ref = {i: 0 for i in range(num_blocks)}\n+        one_ref = {i: 1 for i in range(num_blocks)}\n         allocator = PrefixCachingBlockAllocator(num_blocks=num_blocks,\n                                                 block_size=block_size)\n         token_ids = list(range(num_blocks * block_size))\n \n-        # now we have num_blocks free blocks in hashless allocator\n-        # with internal tracking list _blocks _cached_blocks and evictor\n-        # empty and block's ref shall be 0\n+        # Verify initial/pre-alloc state\n+\n+        # Ensure all blocks are free inside hashless allocator\n         assert list(allocator._hashless_allocator._free_block_indices\n                     ) == all_blocks_list\n-        assert len(allocator._blocks.keys()) == 0\n+        # Ensure no tracked blocks\n+        assert len(allocator._block_tracker.keys()) == num_blocks\n+        for block_id in range(num_blocks):\n+            assert not allocator._block_tracker[block_id].active\n+        # Ensure no cached blocks\n         assert len(allocator._cached_blocks.values()) == 0\n+        # Ensure no evicted blocks\n         assert len(allocator.evictor.free_table.keys()) == 0\n+        # Ensure 0s ref counts for all blocks\n         assert allocator._refcounter._refcounts == zero_ref\n \n         # Allocate immutable chains with only one block residuled in\n         new_block = []\n         for i in range(num_blocks):\n-            block = allocator.allocate_immutable(\n+            block = allocator.allocate_immutable_block(\n                 prev_block=None,\n                 token_ids=token_ids[block_size * i:block_size * (i + 1)])\n             new_block.append(block)\n \n+        # Verify post-alloc state\n+\n+        # Ensure no blocks are free inside hashless allocator\n+        assert (len(allocator._hashless_allocator._free_block_indices) == 0)\n+        # Ensure all blocks are tracked\n+        assert len(allocator._block_tracker.keys()) == num_blocks\n+        for block_id in range(num_blocks):\n+            assert allocator._block_tracker[block_id].active\n+        # Ensure all blocks are cached (all promoted)\n+        assert len(allocator._cached_blocks.values()) == num_blocks\n+        # Ensure no evicted blocks\n+        assert len(allocator.evictor.free_table.keys()) == 0\n+        # Ensure 1s ref counts for all blocks\n+        assert allocator._refcounter._refcounts == one_ref\n+\n         # Free all blocks, and now all blocks shall be in the evictor\n-        # there shall be no tracking data left in _blocks\n+        # there shall be no tracking data left in _block_tracker\n         # all blocks shall be tracked in _cached_blocks\n         # all blocks' ref shall be zero\n         for block in new_block:\n             allocator.free(block)\n \n-        assert len(allocator._blocks.keys()) == 0\n+        # Verify post-free state\n+\n+        # Ensure no tracked blocks\n+        assert len(allocator._block_tracker.keys()) == num_blocks\n+        for block_id in range(num_blocks):\n+            assert not allocator._block_tracker[block_id].active\n+        # Ensure no blocks in hashless allocator (all promoted)\n         assert len(allocator._hashless_allocator._free_block_indices) == 0\n+        # Ensure all blocks are cached\n         assert list(allocator._cached_blocks.values()) == all_blocks_list\n+        # Ensure all blocks are inside the evictor\n         assert list(allocator.evictor.free_table.keys()) == all_blocks_list\n+        # Ensure 0s refcounts\n         assert allocator._refcounter._refcounts == zero_ref\n \n         # Allocate a mutable block, and the first block shall be evicted\n         # and set its content hash into None, ref to 1\n-        mutable = allocator.allocate_mutable(prev_block=None)\n+        mutable = allocator.allocate_mutable_block(prev_block=None)\n \n         assert mutable.block_id == 0\n         assert mutable.content_hash is None\n-        assert 0 in allocator._blocks\n+        assert allocator._block_tracker[0].active\n         assert allocator._refcounter.get(0) == 1\n         assert 0 not in allocator._cached_blocks\n         assert 0 not in allocator.evictor\n@@ -502,27 +530,27 @@ class TestPrefixCachingBlockAllocator:\n         # hashless allocator\n         allocator.free(mutable)\n \n-        assert len(allocator._blocks.keys()) == 0\n+        assert not allocator._block_tracker[0].active\n         assert allocator._refcounter._refcounts == zero_ref\n         assert 0 not in allocator._cached_blocks\n         assert 0 not in allocator.evictor\n         assert 0 in allocator._hashless_allocator._free_block_indices\n \n-        # when allocate immutable with first block_size tokens, we\n+        # When allocate immutable with first block_size tokens, we\n         # shall get free block from hashless allocator, thus no block left\n         # in hashless\n-        block = allocator.allocate_immutable(prev_block=None,\n-                                             token_ids=token_ids[:block_size])\n+        block = allocator.allocate_immutable_block(\n+            prev_block=None, token_ids=token_ids[:block_size])\n \n         assert block.block_id == 0\n         assert len(allocator._hashless_allocator._free_block_indices) == 0\n-        assert 0 in allocator._blocks\n+        assert allocator._block_tracker[0].active\n         assert 0 in allocator._cached_blocks.values()\n         assert allocator._refcounter.get(0) == 1\n         assert 0 not in allocator.evictor\n \n         # allocate mutable block again, it shall be popped from evictor\n-        mutable = allocator.allocate_mutable(prev_block=None)\n+        mutable = allocator.allocate_mutable_block(prev_block=None)\n         assert len(allocator._hashless_allocator._free_block_indices) == 0\n         assert mutable.block_id not in allocator.evictor.free_table\n         assert allocator._refcounter.get(mutable.block_id) == 1\n@@ -619,7 +647,7 @@ class TestPrefixCachingBlockAllocator:\n             block_token_ids = token_ids[block_number *\n                                         block_size:(block_number + 1) *\n                                         block_size]\n-            prev_block = allocator.allocate_immutable(\n+            prev_block = allocator.allocate_immutable_block(\n                 prev_block=prev_block, token_ids=block_token_ids)\n             blocks.append(prev_block)\n \ndiff --git a/tests/spec_decode/test_batch_expansion.py b/tests/spec_decode/test_batch_expansion.py\nindex 42dd90422..c350a2c55 100644\n--- a/tests/spec_decode/test_batch_expansion.py\n+++ b/tests/spec_decode/test_batch_expansion.py\n@@ -90,10 +90,10 @@ def test_create_single_target_seq_group_metadata(k: int):\n \n     assert output.request_id == input_seq_group_metadata.request_id\n     assert len(output.seq_data) == 1\n-    assert output.seq_data[target_seq_id].get_prompt_token_ids(\n-    ) == prompt_tokens\n-    assert output.seq_data[target_seq_id].get_output_token_ids(\n-    ) == prev_output_tokens + token_ids\n+    assert output.seq_data[target_seq_id].get_prompt_token_ids() == tuple(\n+        prompt_tokens)\n+    assert output.seq_data[target_seq_id].get_output_token_ids() == tuple(\n+        prev_output_tokens + token_ids)\n \n     assert len(output.block_tables) == 1\n     assert output.block_tables[\ndiff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex d705f3d91..49e63c231 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -1,5 +1,6 @@\n from typing import List, Optional\n \n+from vllm.core.block.common import BlockList\n from vllm.core.block.interfaces import Block, DeviceAwareBlockAllocator\n from vllm.utils import Device, cdiv, chunk_list\n \n@@ -47,12 +48,10 @@ class BlockTable:\n         self._allocator = block_allocator\n         if _blocks is None:\n             _blocks = []\n-        self._blocks: List[Block] = _blocks\n+        self._blocks: BlockList = BlockList(_blocks)\n \n         self._max_block_sliding_window = max_block_sliding_window\n-        # Use helper method instead of directly calculating, as blocks\n-        # may not be allocated.\n-        self._num_full_slots = len(self._get_all_token_ids())\n+        self._num_full_slots = self._get_num_token_ids()\n \n     @staticmethod\n     def get_num_required_blocks(token_ids: List[int], block_size: int) -> int:\n@@ -88,11 +87,18 @@ class BlockTable:\n         \"\"\"\n         assert not self._is_allocated\n         assert token_ids\n-        self._blocks = self._allocate_blocks_for_token_ids(prev_block=None,\n-                                                           token_ids=token_ids,\n-                                                           device=device)\n+        blocks = self._allocate_blocks_for_token_ids(prev_block=None,\n+                                                     token_ids=token_ids,\n+                                                     device=device)\n+        self.update(blocks)\n         self._num_full_slots = len(token_ids)\n \n+    def update(self, blocks: List[Block]) -> None:\n+        \"\"\"Resets the table to the newly provided blocks \n+        (with their corresponding block ids)\n+        \"\"\"\n+        self._blocks.update(blocks)\n+\n     def append_token_ids(self,\n                          token_ids: List[int],\n                          num_lookahead_slots: int = 0,\n@@ -140,11 +146,11 @@ class BlockTable:\n                                     num_lookahead_slots)\n \n         # Update the blocks with the new tokens\n-        blocks = self._blocks[self._num_full_slots // self._block_size:]\n+        first_block_idx = self._num_full_slots // self._block_size\n         token_blocks = self._chunk_token_blocks_for_append(token_ids)\n \n-        for block, token_block in zip(blocks, token_blocks):\n-            block.append_token_ids(token_block)\n+        for i, token_block in enumerate(token_blocks):\n+            self._blocks.append_token_ids(first_block_idx + i, token_block)\n \n         self._num_full_slots += len(token_ids)\n \n@@ -174,8 +180,8 @@ class BlockTable:\n         for _ in range(blocks_to_allocate):\n             assert len(self._blocks) > 0\n             self._blocks.append(\n-                self._allocator.allocate_mutable(prev_block=self._blocks[-1],\n-                                                 device=device))\n+                self._allocator.allocate_mutable_block(\n+                    prev_block=self._blocks[-1], device=device))\n \n     def fork(self) -> \"BlockTable\":\n         \"\"\"Creates a new BlockTable instance with a copy of the blocks from the\n@@ -209,12 +215,12 @@ class BlockTable:\n         is set to `None`.\n         \"\"\"\n         assert self._is_allocated\n-        for block in self._blocks:\n+        for block in self.blocks:\n             self._allocator.free(block)\n-        self._blocks = []\n+        self._blocks.reset()\n \n     @property\n-    def physical_block_ids(self) -> List[Optional[int]]:\n+    def physical_block_ids(self) -> List[int]:\n         \"\"\"Returns a list of physical block indices for the blocks in the\n         BlockTable.\n \n@@ -228,7 +234,7 @@ class BlockTable:\n                 BlockTable.\n         \"\"\"\n         assert self._is_allocated\n-        return [block.block_id for block in self._blocks]\n+        return self._blocks.ids()\n \n     def get_unseen_token_ids(self, sequence_token_ids: List[int]) -> List[int]:\n         \"\"\"Get the number of \"unseen\" tokens in the sequence.\n@@ -253,17 +259,31 @@ class BlockTable:\n                                        token_ids: List[int],\n                                        device: Device) -> List[Block]:\n         blocks: List[Block] = []\n-        for block_token_ids in chunk_list(token_ids, self._block_size):\n-            if len(block_token_ids) == self._block_size:\n-                # If the block is full, create an immutable block.\n-                prev_block = self._allocator.allocate_immutable(\n-                    prev_block, token_ids=block_token_ids, device=device)\n+\n+        block_token_ids = []\n+        tail_token_ids = []\n+        for cur_token_ids in chunk_list(token_ids, self._block_size):\n+            if len(cur_token_ids) == self._block_size:\n+                block_token_ids.append(cur_token_ids)\n             else:\n-                # Else, partially fill a mutable block with token ids.\n-                prev_block = self._allocator.allocate_mutable(\n-                    prev_block=prev_block, device=device)\n-                prev_block.append_token_ids(block_token_ids)\n-            blocks.append(prev_block)\n+                tail_token_ids.append(cur_token_ids)\n+\n+        if block_token_ids:\n+            blocks.extend(\n+                self._allocator.allocate_immutable_blocks(\n+                    prev_block, block_token_ids=block_token_ids,\n+                    device=device))\n+            prev_block = blocks[-1]\n+\n+        if tail_token_ids:\n+            assert len(tail_token_ids) == 1\n+            cur_token_ids = tail_token_ids[0]\n+\n+            block = self._allocator.allocate_mutable_block(\n+                prev_block=prev_block, device=device)\n+            block.append_token_ids(cur_token_ids)\n+\n+            blocks.append(block)\n \n         return blocks\n \n@@ -274,18 +294,25 @@ class BlockTable:\n         if not self._is_allocated:\n             return token_ids\n \n-        for block in self._blocks:\n+        for block in self.blocks:\n             token_ids.extend(block.token_ids)\n \n         return token_ids\n \n+    def _get_num_token_ids(self) -> int:\n+        res = 0\n+        for block in self.blocks:\n+            res += len(block.token_ids)\n+\n+        return res\n+\n     @property\n     def _is_allocated(self) -> bool:\n         return len(self._blocks) > 0\n \n     @property\n-    def blocks(self) -> Optional[List[Block]]:\n-        return self._blocks\n+    def blocks(self) -> List[Block]:\n+        return self._blocks.list()\n \n     @property\n     def _num_empty_slots(self) -> int:\ndiff --git a/vllm/core/block/common.py b/vllm/core/block/common.py\nindex d2787d696..1e808e21b 100644\n--- a/vllm/core/block/common.py\n+++ b/vllm/core/block/common.py\n@@ -1,4 +1,5 @@\n-from typing import Dict, Iterable, List, Optional, Protocol, Tuple\n+from collections import deque\n+from typing import Deque, Dict, Iterable, List, Optional, Protocol, Tuple\n \n from vllm.core.block.interfaces import Block, BlockAllocator\n \n@@ -95,64 +96,40 @@ class CopyOnWriteTracker:\n \n     The CopyOnWriteTracker class maintains a mapping of source block indices to\n         their corresponding copy-on-write destination block indices. It works in\n-        conjunction with a RefCounter and a BlockAllocator to handle reference\n-        counting and block allocation.\n+        conjunction with a RefCounter.\n \n     Args:\n         refcounter (RefCounter): The reference counter used to track block\n             reference counts.\n-        allocator (BlockAllocator): The block allocator used to allocate and\n-            free blocks.\n     \"\"\"\n \n-    def __init__(\n-        self,\n-        refcounter: RefCounterProtocol,\n-        allocator: BlockAllocator,\n-    ):\n+    def __init__(self, refcounter: RefCounterProtocol):\n         self._copy_on_writes: List[Tuple[BlockId, BlockId]] = []\n         self._refcounter = refcounter\n-        self._allocator = allocator\n-\n-    def cow_block_if_not_appendable(self, block: Block) -> Optional[BlockId]:\n-        \"\"\"Performs a copy-on-write operation on the given block if it is not\n-        appendable.\n-\n-        This method checks the reference count of the given block. If the\n-        reference count is greater than 1, indicating that the block is shared,\n-        a copy-on-write operation is performed. The original block is freed,\n-        and a new block is allocated with the same content. The new block index\n-        is returned.\n-\n-        Args:\n-            block (Block): The block to check for copy-on-write.\n \n-        Returns:\n-            Optional[BlockId]: The block index of the new block if a copy-on\n-                -write operation was performed, or the original block index if\n-                no copy-on-write was necessary.\n+    def is_appendable(self, block: Block) -> bool:\n+        \"\"\"Checks if the block is shared or not. If shared, then it cannot\n+        be appended and needs to be duplicated via copy-on-write\n         \"\"\"\n         block_id = block.block_id\n         if block_id is None:\n-            return block_id\n+            return True\n \n         refcount = self._refcounter.get(block_id)\n-        assert refcount != 0\n-        if refcount > 1:\n-            src_block_id = block_id\n-            # Decrement refcount of the old block.\n-            self._allocator.free(block)\n-\n-            # Allocate a fresh new block.\n-            block_id = self._allocator.allocate_mutable(\n-                prev_block=block.prev_block).block_id\n+        return refcount <= 1\n \n-            # Track src/dst copy.\n-            assert src_block_id is not None\n-            assert block_id is not None\n-            self._copy_on_writes.append((src_block_id, block_id))\n-\n-        return block_id\n+    def record_cow(self, src_block_id: Optional[BlockId],\n+                   trg_block_id: Optional[BlockId]) -> None:\n+        \"\"\"Records a copy-on-write operation from source to target block id\n+        Args:\n+            src_block_id (BlockId): The source block id from which to copy \n+                the data\n+            trg_block_id (BlockId): The target block id to which the data\n+                is copied\n+        \"\"\"\n+        assert src_block_id is not None\n+        assert trg_block_id is not None\n+        self._copy_on_writes.append((src_block_id, trg_block_id))\n \n     def clear_cows(self) -> List[Tuple[BlockId, BlockId]]:\n         \"\"\"Clears the copy-on-write tracking information and returns the current\n@@ -172,6 +149,139 @@ class CopyOnWriteTracker:\n         return cows\n \n \n+class BlockPool:\n+    \"\"\"Used to pre-allocate block objects, in order to avoid excessive python\n+    object allocations/deallocations.\n+    The pool starts from \"pool_size\" objects and will increase to more objects\n+    if necessary\n+\n+    Note that multiple block objects may point to the same physical block id,\n+    which is why this pool is needed, so that it will be easier to support\n+    prefix caching and more complicated sharing of physical blocks.\n+    \"\"\"\n+\n+    def __init__(self, block_size: int, create_block: Block.Factory,\n+                 allocator: BlockAllocator, pool_size: int):\n+        self._block_size = block_size\n+        self._create_block = create_block\n+        self._allocator = allocator\n+        self._pool_size = pool_size\n+        assert self._pool_size >= 0\n+\n+        self._free_ids: Deque[int] = deque(range(self._pool_size))\n+        self._pool = []\n+        for i in range(self._pool_size):\n+            self._pool.append(\n+                self._create_block(prev_block=None,\n+                                   token_ids=[],\n+                                   block_size=self._block_size,\n+                                   allocator=self._allocator,\n+                                   block_id=None))\n+\n+    def increase_pool(self):\n+        \"\"\"Doubles the internal pool size\n+        \"\"\"\n+        cur_pool_size = self._pool_size\n+        new_pool_size = cur_pool_size * 2\n+        self._pool_size = new_pool_size\n+\n+        self._free_ids += deque(range(cur_pool_size, new_pool_size))\n+\n+        for i in range(cur_pool_size, new_pool_size):\n+            self._pool.append(\n+                self._create_block(prev_block=None,\n+                                   token_ids=[],\n+                                   block_size=self._block_size,\n+                                   allocator=self._allocator,\n+                                   block_id=None))\n+\n+    def init_block(self, prev_block: Optional[Block], token_ids: List[int],\n+                   block_size: int, physical_block_id: Optional[int]) -> Block:\n+        if len(self._free_ids) == 0:\n+            self.increase_pool()\n+            assert len(self._free_ids) > 0\n+\n+        pool_id = self._free_ids.popleft()\n+\n+        block = self._pool[pool_id]\n+        block.__init__(  # type: ignore[misc]\n+            prev_block=prev_block,\n+            token_ids=token_ids,\n+            block_size=block_size,\n+            allocator=block._allocator,  # type: ignore[attr-defined] \n+            block_id=physical_block_id)\n+        block.pool_id = pool_id  # type: ignore[attr-defined]\n+        return block\n+\n+    def free_block(self, block: Block) -> None:\n+        self._free_ids.appendleft(block.pool_id)  # type: ignore[attr-defined]\n+\n+\n+class BlockList:\n+    \"\"\"This class is an optimization to allow fast-access to physical \n+    block ids. It maintains a block id list that is updated with the \n+    block list and this avoids the need to reconstruct the block id \n+    list on every iteration of the block manager\n+    \"\"\"\n+\n+    def __init__(self, blocks: List[Block]):\n+        self._blocks: List[Block] = []\n+        self._block_ids: List[int] = []\n+\n+        self.update(blocks)\n+\n+    def _add_block_id(self, block_id: Optional[BlockId]) -> None:\n+        assert block_id is not None\n+        self._block_ids.append(block_id)\n+\n+    def _update_block_id(self, block_index: int,\n+                         new_block_id: Optional[BlockId]) -> None:\n+        assert new_block_id is not None\n+        self._block_ids[block_index] = new_block_id\n+\n+    def update(self, blocks: List[Block]):\n+        self._blocks = blocks\n+\n+        # Cache block ids for fast query\n+        self._block_ids = []\n+        for block in self._blocks:\n+            self._add_block_id(block.block_id)\n+\n+    def append_token_ids(self, block_index: int, token_ids: List[int]) -> None:\n+        block = self._blocks[block_index]\n+        prev_block_id = block.block_id\n+\n+        block.append_token_ids(token_ids)\n+\n+        # CoW or promotion may update the internal block_id\n+        if prev_block_id != block.block_id:\n+            self._update_block_id(block_index, block.block_id)\n+\n+    def append(self, new_block: Block):\n+        self._blocks.append(new_block)\n+        self._add_block_id(new_block.block_id)\n+\n+    def __len__(self) -> int:\n+        return len(self._blocks)\n+\n+    def __getitem__(self, block_index: int) -> Block:\n+        return self._blocks[block_index]\n+\n+    def __setitem__(self, block_index: int, new_block: Block) -> None:\n+        self._blocks[block_index] = new_block\n+        self._update_block_id(block_index, new_block.block_id)\n+\n+    def reset(self):\n+        self._blocks = []\n+        self._block_ids = []\n+\n+    def list(self) -> List[Block]:\n+        return self._blocks\n+\n+    def ids(self) -> List[int]:\n+        return self._block_ids\n+\n+\n def get_all_blocks_recursively(last_block: Block) -> List[Block]:\n     \"\"\"Retrieves all the blocks in a sequence starting from the last block.\n \ndiff --git a/vllm/core/block/cpu_gpu_block_allocator.py b/vllm/core/block/cpu_gpu_block_allocator.py\nindex 255aae9d1..5287cd9c1 100644\n--- a/vllm/core/block/cpu_gpu_block_allocator.py\n+++ b/vllm/core/block/cpu_gpu_block_allocator.py\n@@ -113,11 +113,11 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n     def allocate_or_get_null_block(self) -> Block:\n         if self._null_block is None:\n             self._null_block = NullBlock(\n-                self.allocate_mutable(None, Device.GPU))\n+                self.allocate_mutable_block(None, Device.GPU))\n         return self._null_block\n \n-    def allocate_mutable(self, prev_block: Optional[Block],\n-                         device: Device) -> Block:\n+    def allocate_mutable_block(self, prev_block: Optional[Block],\n+                               device: Device) -> Block:\n         \"\"\"Allocates a new mutable block on the specified device.\n \n         Args:\n@@ -128,10 +128,31 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n         Returns:\n             Block: The newly allocated mutable block.\n         \"\"\"\n-        return self._allocators[device].allocate_mutable(prev_block)\n+        return self._allocators[device].allocate_mutable_block(prev_block)\n \n-    def allocate_immutable(self, prev_block: Optional[Block],\n-                           token_ids: List[int], device: Device) -> Block:\n+    def allocate_immutable_blocks(self, prev_block: Optional[Block],\n+                                  block_token_ids: List[List[int]],\n+                                  device: Optional[Device]) -> List[Block]:\n+        \"\"\"Allocates a new group of immutable blocks with the provided block \n+        token IDs on the specified device.\n+\n+        Args:\n+            prev_block (Optional[Block]): The previous block in the sequence.\n+                Used for prefix hashing.\n+            block_token_ids (List[int]): The list of block token IDs to be \n+                stored in the new blocks.\n+            device (Device): The device on which to allocate the new block.\n+\n+        Returns:\n+            List[Block]: The newly allocated list of immutable blocks \n+                containing the provided block token IDs.\n+        \"\"\"\n+        return self._allocators[device].allocate_immutable_blocks(\n+            prev_block, block_token_ids)\n+\n+    def allocate_immutable_block(self, prev_block: Optional[Block],\n+                                 token_ids: List[int],\n+                                 device: Device) -> Block:\n         \"\"\"Allocates a new immutable block with the provided token IDs on the\n         specified device.\n \n@@ -146,7 +167,7 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n             Block: The newly allocated immutable block containing the provided\n                 token IDs.\n         \"\"\"\n-        return self._allocators[device].allocate_immutable(\n+        return self._allocators[device].allocate_immutable_block(\n             prev_block, token_ids)\n \n     def free(self, block: Block) -> None:\n@@ -161,7 +182,7 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n         block_id = block.block_id\n         assert block_id is not None\n         allocator = self._block_ids_to_allocator[block_id]\n-        return allocator.free(block)\n+        allocator.free(block)\n \n     def fork(self, last_block: Block) -> List[Block]:\n         \"\"\"Creates a new sequence of blocks that shares the same underlying\n@@ -210,8 +231,8 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n         \"\"\"\n         return self._allocators[device].get_physical_block_id(absolute_id)\n \n-    def swap(self, blocks: List[Block], source_device: Device,\n-             dest_device: Device) -> Dict[int, int]:\n+    def swap(self, blocks: List[Block], src_device: Device,\n+             dst_device: Device) -> Dict[int, int]:\n         \"\"\"Execute the swap for the given blocks from source_device\n         on to dest_device, save the current swap mapping and append \n         them to the accumulated `self._swap_mapping` for each \n@@ -219,23 +240,23 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n \n         Args:\n             blocks: List of blocks to be swapped.\n-            source_device (Device): Device to swap the 'blocks' from.\n-            dest_device (Device): Device to swap the 'blocks' to.\n+            src_device (Device): Device to swap the 'blocks' from.\n+            dst_device (Device): Device to swap the 'blocks' to.\n         \n         Returns:\n             Dict[int, int]: Swap mapping from source_device\n                 on to dest_device.\n         \"\"\"\n-        source_block_ids = [block.block_id for block in blocks]\n-        self._allocators[source_device].swap_out(blocks)\n-        self._allocators[dest_device].swap_in(blocks)\n-        dest_block_ids = [block.block_id for block in blocks]\n+        src_block_ids = [block.block_id for block in blocks]\n+        self._allocators[src_device].swap_out(blocks)\n+        self._allocators[dst_device].swap_in(blocks)\n+        dst_block_ids = [block.block_id for block in blocks]\n \n         current_swap_mapping: Dict[int, int] = {}\n-        for src, dest in zip(source_block_ids, dest_block_ids):\n-            if src is not None and dest is not None:\n-                self._swap_mapping[src] = dest\n-                current_swap_mapping[src] = dest\n+        for src_block_id, dst_block_id in zip(src_block_ids, dst_block_ids):\n+            if src_block_id is not None and dst_block_id is not None:\n+                self._swap_mapping[src_block_id] = dst_block_id\n+                current_swap_mapping[src_block_id] = dst_block_id\n         return current_swap_mapping\n \n     def get_num_blocks_touched(self,\n@@ -283,23 +304,25 @@ class CpuGpuBlockAllocator(DeviceAwareBlockAllocator):\n         device = Device.GPU\n         return self._allocators[device].mark_blocks_as_computed(block_ids)\n \n+    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n+                               block_ids: List[int],\n+                               skip_last_block_id: bool) -> List[int]:\n+        # Prefix caching only supported on GPU.\n+        device = Device.GPU\n+        return self._allocators[device].get_computed_block_ids(\n+            prev_computed_block_ids, block_ids, skip_last_block_id)\n+\n     def get_common_computed_block_ids(\n-            self, seq_block_ids: List[List[int]]) -> List[int]:\n+            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n         # Prefix caching only supported on GPU.\n         device = Device.GPU\n         return self._allocators[device].get_common_computed_block_ids(\n-            seq_block_ids)\n+            computed_seq_block_ids)\n \n     @property\n     def all_block_ids(self) -> FrozenSet[int]:\n         return frozenset(self._block_ids_to_allocator.keys())\n \n-    def promote_to_immutable_block(self, block: Block) -> BlockId:\n-        raise NotImplementedError\n-\n-    def cow_block_if_not_appendable(self, block: Block) -> Optional[BlockId]:\n-        raise NotImplementedError\n-\n     def get_and_reset_swaps(self) -> List[Tuple[int, int]]:\n         \"\"\"Returns and clears the mapping of source to destination block IDs.\n         Will be called after every swapping operations for now, and after every\n@@ -341,6 +364,11 @@ class NullBlock(Block):\n     def token_ids(self) -> List[BlockId]:\n         return self._proxy.token_ids\n \n+    @property\n+    def num_tokens_total(self) -> int:\n+        raise NotImplementedError(\n+            \"num_tokens_total is not used for null block\")\n+\n     @property\n     def num_empty_slots(self) -> BlockId:\n         return self._proxy.num_empty_slots\ndiff --git a/vllm/core/block/interfaces.py b/vllm/core/block/interfaces.py\nindex 4b20856a1..ab39832bc 100644\n--- a/vllm/core/block/interfaces.py\n+++ b/vllm/core/block/interfaces.py\n@@ -28,6 +28,13 @@ class Block(ABC):\n     def token_ids(self) -> List[int]:\n         pass\n \n+    @property\n+    @abstractmethod\n+    def num_tokens_total(self) -> int:\n+        \"\"\"The number of tokens till the current block (inclusive)\n+        \"\"\"\n+        pass\n+\n     @property\n     @abstractmethod\n     def num_empty_slots(self) -> int:\n@@ -92,12 +99,18 @@ class Block(ABC):\n class BlockAllocator(ABC):\n \n     @abstractmethod\n-    def allocate_mutable(self, prev_block: Optional[Block]) -> Block:\n+    def allocate_mutable_block(self, prev_block: Optional[Block]) -> Block:\n         pass\n \n     @abstractmethod\n-    def allocate_immutable(self, prev_block: Optional[Block],\n-                           token_ids: List[int]) -> Block:\n+    def allocate_immutable_block(self, prev_block: Optional[Block],\n+                                 token_ids: List[int]) -> Block:\n+        pass\n+\n+    @abstractmethod\n+    def allocate_immutable_blocks(\n+            self, prev_block: Optional[Block],\n+            block_token_ids: List[List[int]]) -> List[Block]:\n         pass\n \n     @abstractmethod\n@@ -146,13 +159,19 @@ class BlockAllocator(ABC):\n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n         pass\n \n+    @abstractmethod\n+    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n+                               block_ids: List[int],\n+                               skip_last_block_id: bool) -> List[int]:\n+        pass\n+\n     @abstractmethod\n     def get_common_computed_block_ids(\n-            self, seq_block_ids: List[List[int]]) -> List[int]:\n+            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n         pass\n \n     @abstractmethod\n-    def cow_block_if_not_appendable(self, block: Block) -> Optional[\"BlockId\"]:\n+    def cow_block_if_not_appendable(self, block: Block) -> BlockId:\n         \"\"\"NOTE: This should not be used besides Block\"\"\"\n         pass\n \n@@ -174,13 +193,20 @@ class BlockAllocator(ABC):\n class DeviceAwareBlockAllocator(ABC):\n \n     @abstractmethod\n-    def allocate_mutable(self, prev_block: Optional[Block],\n-                         device: Device) -> Block:\n+    def allocate_mutable_block(self, prev_block: Optional[Block],\n+                               device: Device) -> Block:\n+        pass\n+\n+    @abstractmethod\n+    def allocate_immutable_block(self, prev_block: Optional[Block],\n+                                 token_ids: List[int],\n+                                 device: Device) -> Block:\n         pass\n \n     @abstractmethod\n-    def allocate_immutable(self, prev_block: Optional[Block],\n-                           token_ids: List[int], device: Device) -> Block:\n+    def allocate_immutable_blocks(self, prev_block: Optional[Block],\n+                                  block_token_ids: List[List[int]],\n+                                  device: Device) -> List[Block]:\n         pass\n \n     @abstractmethod\n@@ -217,9 +243,15 @@ class DeviceAwareBlockAllocator(ABC):\n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n         pass\n \n+    @abstractmethod\n+    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n+                               block_ids: List[int],\n+                               skip_last_block_id: bool) -> List[int]:\n+        pass\n+\n     @abstractmethod\n     def get_common_computed_block_ids(\n-            self, seq_block_ids: List[List[int]]) -> List[int]:\n+            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n         pass\n \n     @abstractmethod\n@@ -230,8 +262,8 @@ class DeviceAwareBlockAllocator(ABC):\n         pass\n \n     @abstractmethod\n-    def swap(self, blocks: List[Block], source_device: Device,\n-             dest_device: Device) -> Dict[int, int]:\n+    def swap(self, blocks: List[Block], src_device: Device,\n+             dst_device: Device) -> Dict[int, int]:\n         pass\n \n     @abstractmethod\ndiff --git a/vllm/core/block/naive_block.py b/vllm/core/block/naive_block.py\nindex 50f27bab3..0c1e88314 100644\n--- a/vllm/core/block/naive_block.py\n+++ b/vllm/core/block/naive_block.py\n@@ -1,6 +1,7 @@\n-from typing import FrozenSet, Iterable, List, Optional, Set, Tuple\n+from collections import deque\n+from typing import Deque, FrozenSet, Iterable, List, Optional, Tuple\n \n-from vllm.core.block.common import (CopyOnWriteTracker, RefCounter,\n+from vllm.core.block.common import (BlockPool, CopyOnWriteTracker, RefCounter,\n                                     get_all_blocks_recursively)\n from vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device\n from vllm.utils import cdiv\n@@ -31,28 +32,39 @@ class NaiveBlockAllocator(BlockAllocator):\n         num_blocks: int,\n         block_size: int,\n         block_ids: Optional[Iterable[int]] = None,\n+        block_pool: Optional[BlockPool] = None,\n     ):\n         if block_ids is None:\n             block_ids = range(num_blocks)\n \n-        self._free_block_indices: Set[BlockId] = set(block_ids)\n+        self._free_block_indices: Deque[BlockId] = deque(block_ids)\n         self._all_block_indices = frozenset(block_ids)\n         assert len(self._all_block_indices) == num_blocks\n \n         self._refcounter = RefCounter(\n             all_block_indices=self._free_block_indices)\n-        self._create_block = create_block\n         self._block_size = block_size\n \n         self._cow_tracker = CopyOnWriteTracker(\n-            refcounter=self._refcounter.as_readonly(),\n-            allocator=self,\n-        )\n-\n-    def allocate_immutable(self,\n-                           prev_block: Optional[Block],\n-                           token_ids: List[int],\n-                           device: Optional[Device] = None) -> Block:\n+            refcounter=self._refcounter.as_readonly())\n+\n+        if block_pool is None:\n+            extra_factor = 4\n+            # Pre-allocate \"num_blocks * extra_factor\" block objects.\n+            # The \"* extra_factor\" is a buffer to allow more block objects\n+            # than physical blocks\n+            self._block_pool = BlockPool(self._block_size, create_block, self,\n+                                         num_blocks * extra_factor)\n+        else:\n+            # In this case, the block pool is provided by the caller,\n+            # which means that there is most likely a need to share\n+            # a block pool between allocators\n+            self._block_pool = block_pool\n+\n+    def allocate_immutable_block(self,\n+                                 prev_block: Optional[Block],\n+                                 token_ids: List[int],\n+                                 device: Optional[Device] = None) -> Block:\n         \"\"\"Allocates a new immutable block with the given token IDs, linked to\n         the previous block.\n \n@@ -66,13 +78,36 @@ class NaiveBlockAllocator(BlockAllocator):\n             Block: The newly allocated immutable block.\n         \"\"\"\n         assert device is None\n-        block = self.allocate_mutable(prev_block=prev_block)\n+        block = self.allocate_mutable_block(prev_block=prev_block)\n         block.append_token_ids(token_ids)\n         return block\n \n-    def allocate_mutable(self,\n-                         prev_block: Optional[Block],\n-                         device: Optional[Device] = None) -> Block:\n+    def allocate_immutable_blocks(\n+            self,\n+            prev_block: Optional[Block],\n+            block_token_ids: List[List[int]],\n+            device: Optional[Device] = None) -> List[Block]:\n+        assert device is None\n+        num_blocks = len(block_token_ids)\n+\n+        block_ids = []\n+        for i in range(num_blocks):\n+            block_ids.append(self._allocate_block_id())\n+\n+        blocks = []\n+        for i in range(num_blocks):\n+            prev_block = self._block_pool.init_block(\n+                prev_block=prev_block,\n+                token_ids=block_token_ids[i],\n+                block_size=self._block_size,\n+                physical_block_id=block_ids[i])\n+            blocks.append(prev_block)\n+\n+        return blocks\n+\n+    def allocate_mutable_block(self,\n+                               prev_block: Optional[Block],\n+                               device: Optional[Device] = None) -> Block:\n         \"\"\"Allocates a new mutable block, linked to the previous block.\n \n         Args:\n@@ -84,20 +119,39 @@ class NaiveBlockAllocator(BlockAllocator):\n             Block: The newly allocated mutable block.\n         \"\"\"\n         assert device is None\n-        block_id = self._allocate_new_block_id()\n-        return self._create_block(\n-            prev_block=prev_block,\n-            token_ids=[],\n-            block_id=block_id,\n-            block_size=self._block_size,\n-            allocator=self,\n-        )\n-\n-    def free(self, block: Block) -> None:\n-        assert block.block_id is not None\n-        self._free_block_id(block.block_id)\n+        block_id = self._allocate_block_id()\n+        block = self._block_pool.init_block(prev_block=prev_block,\n+                                            token_ids=[],\n+                                            block_size=self._block_size,\n+                                            physical_block_id=block_id)\n+        return block\n+\n+    def _allocate_block_id(self) -> BlockId:\n+        if not self._free_block_indices:\n+            raise BlockAllocator.NoFreeBlocksError()\n+\n+        block_id = self._free_block_indices.popleft()\n+        self._refcounter.incr(block_id)\n+        return block_id\n+\n+    def _free_block_id(self, block: Block) -> None:\n+        block_id = block.block_id\n+        assert block_id is not None\n+\n+        refcount = self._refcounter.decr(block_id)\n+        if refcount == 0:\n+            self._free_block_indices.appendleft(block_id)\n+\n         block.block_id = None\n \n+    def free(self, block: Block, keep_block_object: bool = False) -> None:\n+        # Release the physical block id\n+        self._free_block_id(block)\n+\n+        # Release the block object\n+        if not keep_block_object:\n+            self._block_pool.free_block(block)\n+\n     def fork(self, last_block: Block) -> List[Block]:\n         \"\"\"Creates a new sequence of blocks that shares the same underlying\n         memory as the original sequence.\n@@ -120,14 +174,13 @@ class NaiveBlockAllocator(BlockAllocator):\n             refcount = self._refcounter.incr(block.block_id)\n             assert refcount != 1, \"can't fork free'd block\"\n \n-            forked_blocks.append(\n-                self._create_block(\n-                    prev_block=prev_block,\n-                    token_ids=block.token_ids,\n-                    block_id=block.block_id,\n-                    block_size=self._block_size,\n-                    allocator=self,\n-                ))\n+            forked_block = self._block_pool.init_block(\n+                prev_block=prev_block,\n+                token_ids=block.token_ids,\n+                block_size=self._block_size,\n+                physical_block_id=block.block_id)\n+\n+            forked_blocks.append(forked_block)\n             prev_block = forked_blocks[-1]\n \n         return forked_blocks\n@@ -138,20 +191,6 @@ class NaiveBlockAllocator(BlockAllocator):\n     def get_num_total_blocks(self) -> int:\n         return len(self._all_block_indices)\n \n-    def _allocate_new_block_id(self) -> BlockId:\n-        if not self._free_block_indices:\n-            raise BlockAllocator.NoFreeBlocksError()\n-\n-        block_id = next(iter(self._free_block_indices))\n-        self._refcounter.incr(block_id)\n-        self._free_block_indices.remove(block_id)\n-        return block_id\n-\n-    def _free_block_id(self, block_id: BlockId) -> None:\n-        refcount = self._refcounter.decr(block_id)\n-        if refcount == 0:\n-            self._free_block_indices.add(block_id)\n-\n     def get_physical_block_id(self, absolute_id: int) -> int:\n         \"\"\"Returns the zero-offset block id on certain block allocator\n         given the absolute block id.\n@@ -173,7 +212,7 @@ class NaiveBlockAllocator(BlockAllocator):\n     def all_block_ids(self) -> FrozenSet[int]:\n         return self._all_block_indices\n \n-    def cow_block_if_not_appendable(self, block: Block) -> Optional[BlockId]:\n+    def cow_block_if_not_appendable(self, block: Block) -> BlockId:\n         \"\"\"Performs a copy-on-write operation on the given block if it is not\n         appendable.\n \n@@ -181,11 +220,22 @@ class NaiveBlockAllocator(BlockAllocator):\n             block (Block): The block to check for copy-on-write.\n \n         Returns:\n-            Optional[BlockId]: The block index of the new block if a copy-on\n-                -write operation was performed, or the original block index if\n+            BlockId: The block index of the new block if a copy-on-write \n+                operation was performed, or the original block index if\n                 no copy-on-write was necessary.\n         \"\"\"\n-        return self._cow_tracker.cow_block_if_not_appendable(block)\n+        src_block_id = block.block_id\n+        assert src_block_id is not None\n+\n+        if self._cow_tracker.is_appendable(block):\n+            return src_block_id\n+\n+        self._free_block_id(block)\n+        trg_block_id = self._allocate_block_id()\n+\n+        self._cow_tracker.record_cow(src_block_id, trg_block_id)\n+\n+        return trg_block_id\n \n     def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:\n         \"\"\"Returns the copy-on-write source->destination mapping and clears it.\n@@ -213,8 +263,15 @@ class NaiveBlockAllocator(BlockAllocator):\n         \"\"\"\n         pass\n \n+    def get_computed_block_ids(self, prev_computed_block_ids: List[int],\n+                               block_ids: List[int],\n+                               skip_last_block_id: bool) -> List[int]:\n+        \"\"\"No prefix caching here => return empty list\n+        \"\"\"\n+        return []\n+\n     def get_common_computed_block_ids(\n-            self, seq_block_ids: List[List[int]]) -> List[int]:\n+            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n         \"\"\"Determine blocks that can be skipped in prefill.\n \n         Since the naive allocator does not support prefix caching, always return\n@@ -223,7 +280,7 @@ class NaiveBlockAllocator(BlockAllocator):\n         return []\n \n     def promote_to_immutable_block(self, block: Block) -> BlockId:\n-        raise NotImplementedError\n+        raise NotImplementedError(\"There is no promotion for naive blocks\")\n \n     def get_num_blocks_touched(self,\n                                blocks: List[Block],\n@@ -263,17 +320,27 @@ class NaiveBlockAllocator(BlockAllocator):\n \n     def swap_out(self, blocks: List[Block]) -> None:\n         for block in blocks:\n-            self.free(block)\n+            self._free_block_id(block)\n \n     def swap_in(self, blocks: List[Block]) -> None:\n         for block in blocks:\n+            # Here we allocate either immutable or mutable block and then\n+            # extract its block_id. Note that the block object is released\n+            # and the block_id is assigned to \"block\" to allow reusing the\n+            # existing \"block\" object\n             if block.is_full:\n-                alloc = self.allocate_immutable(block.prev_block,\n-                                                block.token_ids)\n+                tmp_block = self.allocate_immutable_block(\n+                    prev_block=block.prev_block, token_ids=block.token_ids)\n             else:\n-                alloc = self.allocate_mutable(block.prev_block)\n-                alloc.append_token_ids(block.token_ids)\n-            block.block_id = alloc.block_id\n+                tmp_block = self.allocate_mutable_block(\n+                    prev_block=block.prev_block)\n+                tmp_block.append_token_ids(block.token_ids)\n+\n+            block_id = tmp_block.block_id\n+            tmp_block.block_id = None\n+            self._block_pool.free_block(tmp_block)\n+\n+            block.block_id = block_id  # Assign block_id\n \n \n class NaiveBlock(Block):\n@@ -315,11 +382,12 @@ class NaiveBlock(Block):\n         self._append_token_ids_no_cow(token_ids)\n \n     def append_token_ids(self, token_ids: List[int]) -> None:\n-        \"\"\"Appends the given token IDs to the block, instructing the allocator\n-        to perform a copy-on-write if necessary.\n+        \"\"\"Appends the given token IDs to the block and performs a \n+        copy-on-write if necessary.\n \n         Args:\n-            token_ids (List[int]): The token IDs to be appended to the block.\n+            token_ids (Optional[List[int]]): The token IDs to be appended \n+                to the block.\n         \"\"\"\n         self._append_token_ids_no_cow(token_ids)\n \n@@ -328,7 +396,16 @@ class NaiveBlock(Block):\n                 self._cow_target))\n \n     def _append_token_ids_no_cow(self, token_ids: List[int]) -> None:\n-        assert self.num_empty_slots >= len(token_ids)\n+        \"\"\"Appends the given token IDs to the block\n+\n+        Args:\n+            token_ids (List[int]): The token IDs to be appended to the block.\n+        \"\"\"\n+        if len(token_ids) == 0:\n+            return\n+\n+        assert len(token_ids) <= self.num_empty_slots\n+\n         self._token_ids.extend(token_ids)\n \n     @property\n@@ -361,12 +438,17 @@ class NaiveBlock(Block):\n \n     @property\n     def num_empty_slots(self) -> int:\n-        return self._block_size - len(self._token_ids)\n+        return self._block_size - len(self.token_ids)\n \n     @property\n     def token_ids(self) -> List[int]:\n         return self._token_ids\n \n+    @property\n+    def num_tokens_total(self) -> int:\n+        raise NotImplementedError(\n+            \"num_tokens_total is not used for naive block\")\n+\n     @property\n     def block_size(self) -> int:\n         return self._block_size\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 2df7d74e4..f272e23ee 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -1,13 +1,13 @@\n \"\"\"Token blocks.\"\"\"\n \n-from itertools import takewhile\n from os.path import commonprefix\n from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n \n from vllm.core.block.common import (CopyOnWriteTracker,\n                                     get_all_blocks_recursively)\n from vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device\n-from vllm.core.block.naive_block import NaiveBlock, NaiveBlockAllocator\n+from vllm.core.block.naive_block import (BlockPool, NaiveBlock,\n+                                         NaiveBlockAllocator)\n from vllm.core.evictor_v2 import EvictionPolicy, Evictor, make_evictor\n from vllm.utils import cdiv\n \n@@ -19,6 +19,30 @@ PrefixHash = int\n _DEFAULT_LAST_ACCESSED_TIME = -1\n \n \n+class BlockTracker:\n+    \"\"\"Used to track the status of a block inside the prefix caching allocator\n+    \"\"\"\n+    __slots__ = (\"active\", \"last_accessed\", \"computed\")\n+\n+    def reset(self):\n+        self.last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME\n+        self.computed: bool = False\n+\n+    def __init__(self):\n+        self.active: bool = False\n+        self.reset()\n+\n+    def enable(self):\n+        assert not self.active\n+        self.active = True\n+        self.reset()\n+\n+    def disable(self):\n+        assert self.active\n+        self.active = False\n+        self.reset()\n+\n+\n class PrefixCachingBlockAllocator(BlockAllocator):\n     \"\"\"A block allocator that implements prefix caching.\n \n@@ -41,12 +65,26 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         block_ids: Optional[Iterable[int]] = None,\n         eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n     ):\n+        if block_ids is None:\n+            block_ids = range(num_blocks)\n+\n+        self._block_size = block_size\n+\n         # A mapping of prefix hash to block index. All blocks which have a\n         # prefix hash will be in this dict, even if they have refcount 0.\n         self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n \n-        # A mapping of blockId to Block to track those cached blocks\n-        self._blocks: Dict[BlockId, Block] = {}\n+        # Used to track status of each physical block id\n+        self._block_tracker: Dict[BlockId, BlockTracker] = {}\n+        for block_id in block_ids:\n+            self._block_tracker[block_id] = BlockTracker()\n+\n+        # Pre-allocate \"num_blocks * extra_factor\" block objects.\n+        # The \"* extra_factor\" is a buffer to allow more block objects\n+        # than physical blocks\n+        extra_factor = 4\n+        self._block_pool = BlockPool(self._block_size, self._create_block,\n+                                     self, num_blocks * extra_factor)\n \n         # An allocator for blocks that do not have prefix hashes.\n         self._hashless_allocator = NaiveBlockAllocator(\n@@ -54,10 +92,9 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             num_blocks=num_blocks,\n             block_size=block_size,\n             block_ids=block_ids,\n+            block_pool=self._block_pool,  # Share block pool here\n         )\n \n-        self._block_size = block_size\n-\n         # Evitor used to maintain how we want to handle those computed blocks\n         # if we find memory pressure is high.\n         self.evictor: Evictor = make_evictor(eviction_policy)\n@@ -68,9 +105,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         self._refcounter = self._hashless_allocator.refcounter\n \n         self._cow_tracker = CopyOnWriteTracker(\n-            refcounter=self._refcounter.as_readonly(),\n-            allocator=self,\n-        )\n+            refcounter=self._refcounter.as_readonly())\n \n     # Implements Block.Factory.\n     def _create_block(\n@@ -90,14 +125,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             token_ids=token_ids,\n             block_size=block_size,\n             block_id=block_id,\n-            prefix_caching_allocator=allocator,\n+            allocator=allocator,\n             computed=computed,\n         )\n \n-    def allocate_immutable(self,\n-                           prev_block: Optional[Block],\n-                           token_ids: List[int],\n-                           device: Optional[Device] = None) -> Block:\n+    def allocate_immutable_block(self,\n+                                 prev_block: Optional[Block],\n+                                 token_ids: List[int],\n+                                 device: Optional[Device] = None) -> Block:\n         \"\"\"Allocates an immutable block with the given token IDs, reusing cached\n         blocks if possible.\n \n@@ -111,29 +146,41 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert device is None\n         assert_prefix_caching_block_or_none(prev_block)\n \n-        block = self._create_block(\n-            prev_block=prev_block,\n-            token_ids=token_ids,\n-            block_size=self._block_size,\n-            allocator=self,\n-        )\n+        # First, try to create a block that points to cached data\n+        block = self._block_pool.init_block(prev_block=prev_block,\n+                                            token_ids=token_ids,\n+                                            block_size=self._block_size,\n+                                            physical_block_id=None)\n         assert block.content_hash is not None\n \n         cached_block_id = self._cached_blocks.get(block.content_hash, None)\n         if cached_block_id is not None:\n             block.block_id = cached_block_id\n-            self._incr_refcount_cached_block(block, block.block_id)\n+            self._incr_refcount_cached_block(block)\n             return block\n+        self._block_pool.free_block(block)\n \n-        block = self.allocate_mutable(prev_block)\n+        # No cached block => Allocate a new block\n+        block = self.allocate_mutable_block(prev_block)\n         block.append_token_ids(token_ids)\n-        assert block.content_hash is not None\n-\n         return block\n \n-    def allocate_mutable(self,\n-                         prev_block: Optional[Block],\n-                         device: Optional[Device] = None) -> Block:\n+    def allocate_immutable_blocks(\n+            self,\n+            prev_block: Optional[Block],\n+            block_token_ids: List[List[int]],\n+            device: Optional[Device] = None) -> List[Block]:\n+        blocks = []\n+        for token_ids in block_token_ids:\n+            prev_block = self.allocate_immutable_block(prev_block=prev_block,\n+                                                       token_ids=token_ids,\n+                                                       device=device)\n+            blocks.append(prev_block)\n+        return blocks\n+\n+    def allocate_mutable_block(self,\n+                               prev_block: Optional[Block],\n+                               device: Optional[Device] = None) -> Block:\n         \"\"\"Allocates a mutable block. If there are no free blocks, this will\n         evict unused cached blocks.\n \n@@ -147,116 +194,154 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert device is None\n         assert_prefix_caching_block_or_none(prev_block)\n \n-        try:\n-            block = self._hashless_allocator.allocate_mutable(\n-                prev_block=prev_block)\n-\n-            assert block.block_id not in self._blocks\n-            assert block.block_id is not None\n-            self._blocks[block.block_id] = block\n-            return block\n-        except BlockAllocator.NoFreeBlocksError:\n-            # We must check the unused cached blocks before raising OOM.\n-            pass\n-\n-        # If the evictor has blocks available for eviction, evict a block\n-        # and return it.\n-        if self.evictor.num_blocks > 0:\n-            # here we get an evicted block, which is only added\n-            # into evictor if its ref counter is 0\n-            # and since its content would be changed, we need\n-            # to remove it from _cached_blocks's tracking list\n-            block_id, content_hash_to_evict = self.evictor.evict()\n-\n-            _block_id = self._cached_blocks[content_hash_to_evict]\n-            assert self._refcounter.get(_block_id) == 0\n-            assert _block_id == block_id\n-\n-            self._cached_blocks.pop(content_hash_to_evict)\n-\n-            self._refcounter.incr(block_id)\n-\n-            # Now this block is pop from evictor and ready to write\n-            # with new content which most probably different with\n-            # original content. So need to tell worker to recompute\n-            # its kvcache\n-            block = self._create_block(\n-                prev_block=prev_block,\n-                token_ids=[],\n-                block_size=self._block_size,\n-                allocator=self,\n-                block_id=block_id,\n-                computed=False,\n-            )\n-            assert block.content_hash is None\n-\n-            assert block.block_id not in self._blocks\n-            assert block.block_id is not None\n-            self._blocks[block.block_id] = block\n-            return block\n-\n-        # No block available in hashless allocator, nor in unused cache blocks.\n-        raise BlockAllocator.NoFreeBlocksError()\n+        block_id = self._allocate_block_id()\n+        block = self._block_pool.init_block(prev_block=prev_block,\n+                                            token_ids=[],\n+                                            block_size=self._block_size,\n+                                            physical_block_id=block_id)\n+        assert not block.computed\n+        assert block.content_hash is None\n+        return block\n \n-    def _incr_refcount_cached_block(self, block: Block,\n-                                    block_id: BlockId) -> None:\n-        # now _incr_refcount_cached_block comes from two place\n-        # allocate_immutable/promote_to_immutable_block where hit\n-        # _cached_blocks hash key.\n-        # In both cases, it means that already exists a already\n-        # computed block which shared with block now\n+    def _incr_refcount_cached_block(self, block: Block) -> None:\n+        # Set this block to be \"computed\" since it is pointing to a\n+        # cached block id (which was already computed)\n         block.computed = True\n \n+        block_id = block.block_id\n+        assert block_id is not None\n+\n         refcount = self._refcounter.incr(block_id)\n         if refcount == 1:\n-            # if block get referred, then it shall not be in evictor\n-            # and put it into _blocks for tracking\n+            # In case a cached block was evicted, restore its tracking\n             if block_id in self.evictor:\n                 self.evictor.remove(block_id)\n-            self._blocks[block_id] = block\n \n-    def free(self, block: Block) -> None:\n-        \"\"\"Decrement the refcount of the block. If the decremented refcount is\n-        zero, store the block in the freelist.\n+            self._track_block_id(block_id, computed=True)\n \n-        If the block has a content hash (meaning it is immutable), then we will\n-        keep the block around in case future allocations require it.\n-        \"\"\"\n-        assert (block.block_id\n-                is not None), \"freeing unallocated block is undefined\"\n+    def _decr_refcount_cached_block(self, block: Block) -> None:\n+        # Ensure this is immutable/cached block\n+        assert block.content_hash is not None\n+\n+        block_id = block.block_id\n+        assert block_id is not None\n+\n+        refcount = self._refcounter.decr(block_id)\n+        if refcount > 0:\n+            block.block_id = None\n+            return\n+        else:\n+            assert refcount == 0\n \n-        self._free_block_id_for_block(block.block_id, block)\n+        # No longer used\n+        assert block.content_hash in self._cached_blocks\n+\n+        # Add the cached block to the evictor\n+        # (This keeps the cached block around so it can be reused)\n+        self.evictor.add(block_id, block.content_hash, block.num_tokens_total,\n+                         self._block_tracker[block_id].last_accessed)\n+\n+        # Stop tracking the block\n+        self._untrack_block_id(block_id)\n \n         block.block_id = None\n \n-    def _free_block_id_for_block(self, block_id: BlockId,\n-                                 block: Block) -> None:\n-        assert isinstance(block, PrefixCachingBlock)\n-\n-        # if we comes from promote_to_immutable_block, it means that\n-        # block.content_hash is never None.\n-        # However we need to release the same content block, so that\n-        # physical block could get reused.\n-        if block.block_id != block_id or block.content_hash is None:\n-            refcount = self._refcounter.get(block_id)\n-            # We have fork case where block would get more than one ref,\n-            # so we cannot free it from tracking if ref cnt large than 1\n-            assert block.block_id is not None\n-            refcount = self._refcounter.get(block.block_id)\n-            if refcount == 1:\n-                del self._blocks[block.block_id]\n-\n-            return self._hashless_allocator.free(block)\n+    def _decr_refcount_hashless_block(self, block: Block) -> None:\n+        block_id = block.block_id\n+        assert block_id is not None\n \n-        refcount = self._refcounter.decr(block_id)\n+        # We may have a fork case where block is shared,\n+        # in which case, we cannot remove it from tracking\n+        refcount = self._refcounter.get(block_id)\n+        if refcount == 1:\n+            self._untrack_block_id(block_id)\n \n-        # If no longer used, add the block to the evictor.\n-        if refcount == 0:\n-            assert block.content_hash in self._cached_blocks\n-            assert block.block_id is not None\n-            del self._blocks[block.block_id]\n-            self.evictor.add(block.block_id, block.content_hash,\n-                             block.num_tokens_total, block.last_accessed)\n+        # Decrement refcount of the block_id, but do not free the block object\n+        # itself (will be handled by the caller)\n+        self._hashless_allocator.free(block, keep_block_object=True)\n+\n+    def _allocate_block_id(self) -> BlockId:\n+        \"\"\"First tries to allocate a block id from the hashless allocator,\n+        and if there are no blocks, then tries to evict an unused cached block.\n+        \"\"\"\n+        hashless_block_id = self._maybe_allocate_hashless_block_id()\n+        if hashless_block_id is not None:\n+            return hashless_block_id\n+\n+        evicted_block_id = self._maybe_allocate_evicted_block_id()\n+        if evicted_block_id is not None:\n+            return evicted_block_id\n+\n+        # No block available in hashless allocator, nor in unused cache blocks.\n+        raise BlockAllocator.NoFreeBlocksError()\n+\n+    def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:\n+        try:\n+            # Allocate mutable block and extract its block_id\n+            block = self._hashless_allocator.allocate_mutable_block(\n+                prev_block=None)\n+            block_id = block.block_id\n+            self._block_pool.free_block(block)\n+\n+            self._track_block_id(block_id, computed=False)\n+            return block_id\n+        except BlockAllocator.NoFreeBlocksError:\n+            return None\n+\n+    def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:\n+        if self.evictor.num_blocks == 0:\n+            return None\n+\n+        # Here we get an evicted block, which is only added\n+        # into evictor if its ref counter is 0\n+        # and since its content would be changed, we need\n+        # to remove it from _cached_blocks's tracking list\n+        block_id, content_hash_to_evict = self.evictor.evict()\n+\n+        # Sanity checks\n+        assert content_hash_to_evict in self._cached_blocks\n+        _block_id = self._cached_blocks[content_hash_to_evict]\n+        assert self._refcounter.get(_block_id) == 0\n+        assert _block_id == block_id\n+\n+        self._cached_blocks.pop(content_hash_to_evict)\n+\n+        self._refcounter.incr(block_id)\n+        self._track_block_id(block_id, computed=False)\n+\n+        return block_id\n+\n+    def _free_block_id(self, block: Block) -> None:\n+        \"\"\"Decrements the refcount of the block. The block may be in two \n+        possible states: (1) immutable/cached or (2) mutable/hashless. \n+        In the first case, the refcount is decremented directly and the block\n+        may be possibly added to the evictor. In other case, hashless \n+        allocator free(..) with keep_block_object=True is called to only free\n+        the block id (since the block object may be reused by the caller)\n+        \"\"\"\n+        block_id = block.block_id\n+        assert block_id is not None, \"Freeing unallocated block is undefined\"\n+\n+        if block.content_hash is not None:\n+            # Immutable: This type of block is always cached, and we want to\n+            # keep it in the evictor for future reuse\n+            self._decr_refcount_cached_block(block)\n+        else:\n+            # Mutable: This type of block is not cached, so we release it\n+            # directly to the hashless allocator\n+            self._decr_refcount_hashless_block(block)\n+\n+        assert block.block_id is None\n+\n+    def free(self, block: Block, keep_block_object: bool = False) -> None:\n+        \"\"\"Release the block (look at free_block_id(..) docs)\n+        \"\"\"\n+        # Release the physical block index\n+        self._free_block_id(block)\n+\n+        # Release the block object to the pool\n+        if not keep_block_object:\n+            self._block_pool.free_block(block)\n \n     def fork(self, last_block: Block) -> List[Block]:\n         \"\"\"Creates a new sequence of blocks that shares the same underlying\n@@ -274,17 +359,20 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         forked_blocks: List[Block] = []\n         prev_block = None\n         for block in source_blocks:\n-            refcount = self._refcounter.incr(block.block_id)\n-            assert refcount != 1, \"can't fork free'd block\"\n-\n-            forked_blocks.append(\n-                self._create_block(\n-                    prev_block=prev_block,\n-                    token_ids=block.token_ids,\n-                    block_id=block.block_id,\n-                    block_size=self._block_size,\n-                    allocator=self,\n-                ))\n+            block_id = block.block_id\n+            assert block_id is not None\n+\n+            refcount = self._refcounter.incr(block_id)\n+            assert refcount != 1, \"can't fork free'd block_id = {}\".format(\n+                block_id)\n+\n+            forked_block = self._block_pool.init_block(\n+                prev_block=prev_block,\n+                token_ids=block.token_ids,\n+                block_size=self._block_size,\n+                physical_block_id=block_id)\n+\n+            forked_blocks.append(forked_block)\n             prev_block = forked_blocks[-1]\n \n         return forked_blocks\n@@ -329,7 +417,7 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n \n         Note that if we already have a cached block with the same content, we\n         will replace the newly-promoted block's mapping with the existing cached\n-        block.\n+        block id.\n \n         Args:\n             block: The mutable block to be promoted.\n@@ -338,23 +426,30 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             BlockId: Either the original block index, or the block index of\n                 the previously cached block matching the same content.\n         \"\"\"\n+        # Ensure block can be promoted\n         assert block.content_hash is not None\n         assert block.block_id is not None\n         assert self._refcounter.get(block.block_id) > 0\n \n-        # If the content hash does not have a corresponding cached block,\n-        # set this block as the cached block.\n         if block.content_hash not in self._cached_blocks:\n+            # No cached content hash => Set this block as cached\n+            # (Note that this block is not computed yet =>\n+            #  Will be computed after free())\n             self._cached_blocks[block.content_hash] = block.block_id\n-        else:\n-            self._free_block_id_for_block(\n-                self._cached_blocks[block.content_hash], block)\n-            self._incr_refcount_cached_block(\n-                block, self._cached_blocks[block.content_hash])\n+            return block.block_id\n \n-        return self._cached_blocks[block.content_hash]\n+        # Reuse the cached content hash\n+        self._decr_refcount_hashless_block(block)\n+        block.block_id = self._cached_blocks[block.content_hash]\n \n-    def cow_block_if_not_appendable(self, block: Block) -> Optional[BlockId]:\n+        # Increment refcount of the cached block and (possibly) restore\n+        # it from the evictor.\n+        # Note that in this case, the block is marked as computed\n+        self._incr_refcount_cached_block(block)\n+\n+        return block.block_id\n+\n+    def cow_block_if_not_appendable(self, block: Block) -> BlockId:\n         \"\"\"Performs a copy-on-write operation on the given block if it is not\n         appendable.\n \n@@ -362,11 +457,22 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             block (Block): The block to check for copy-on-write.\n \n         Returns:\n-            Optional[BlockId]: The block index of the new block if a copy-on\n-                -write operation was performed, or the original block index if\n+            BlockId: The block index of the new block if a copy-on-write \n+                operation was performed, or the original block index if\n                 no copy-on-write was necessary.\n         \"\"\"\n-        return self._cow_tracker.cow_block_if_not_appendable(block)\n+        src_block_id = block.block_id\n+        assert src_block_id is not None\n+\n+        if self._cow_tracker.is_appendable(block):\n+            return src_block_id\n+\n+        self._free_block_id(block)\n+        trg_block_id = self._allocate_block_id()\n+\n+        self._cow_tracker.record_cow(src_block_id, trg_block_id)\n+\n+        return trg_block_id\n \n     def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:\n         \"\"\"Returns the copy-on-write source->destination mapping and clears it.\n@@ -386,8 +492,8 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         \"\"\"\n \n         for block_id in block_ids:\n-            if block_id in self._blocks:\n-                self._blocks[block_id].last_accessed = now\n+            if self._block_tracker[block_id].active:\n+                self._block_tracker[block_id].last_accessed = now\n             elif block_id in self.evictor:\n                 self.evictor.update(block_id, now)\n             else:\n@@ -395,25 +501,46 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        \"\"\"Mark blocks as computed, used in prefix caching.\"\"\"\n+        raise NotImplementedError(\"Marking as computed is incremental\")\n \n-        for block_id in block_ids:\n-            if block_id in self._blocks:\n-                # only those full block is valid for prefix caching\n-                if self._blocks[block_id].is_full:\n-                    self._blocks[block_id].computed = True\n-            elif block_id not in self.evictor:\n-                raise ValueError(f\"Mark {block_id=} as computed which \"\n-                                 \"is not belonged to GPU\")\n+    def _track_block_id(self, block_id: Optional[BlockId],\n+                        computed: bool) -> None:\n+        assert block_id is not None\n+        self._block_tracker[block_id].enable()\n+        self._block_tracker[block_id].computed = computed\n+\n+    def _untrack_block_id(self, block_id: Optional[BlockId]) -> None:\n+        assert block_id is not None\n+        self._block_tracker[block_id].disable()\n \n     def block_is_computed(self, block_id: int) -> bool:\n-        if block_id in self._blocks:\n-            return self._blocks[block_id].computed\n+        if self._block_tracker[block_id].active:\n+            return self._block_tracker[block_id].computed\n         else:\n             return block_id in self.evictor\n \n+    def get_computed_block_ids(self,\n+                               prev_computed_block_ids: List[int],\n+                               block_ids: List[int],\n+                               skip_last_block_id: bool = True) -> List[int]:\n+        prev_prefix_size = len(prev_computed_block_ids)\n+        cur_size = len(block_ids)\n+        if skip_last_block_id:\n+            cur_size -= 1\n+\n+        # Sanity checks\n+        assert cur_size >= 0\n+        assert prev_prefix_size <= cur_size\n+\n+        ret = prev_computed_block_ids\n+        for i in range(prev_prefix_size, cur_size):\n+            block_id = block_ids[i]\n+            if self.block_is_computed(block_id):\n+                ret.append(block_id)\n+        return ret\n+\n     def get_common_computed_block_ids(\n-            self, seq_block_ids: List[List[int]]) -> List[int]:\n+            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n         \"\"\"Return the block ids that are common for a given sequence group.\n \n         Only those blocks that are immutable and already be marked\n@@ -424,14 +551,9 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # prompt is cached. This would cause erroneous behavior in model\n         # runner.\n \n-        ids_list = [\n-            list(\n-                takewhile(lambda block_id: self.block_is_computed(block_id),\n-                          seq[:-1])) for seq in seq_block_ids\n-        ]\n         # It returns a list of int although type annotation says list of string.\n         return commonprefix([\n-            ids for ids in ids_list  # type: ignore\n+            ids for ids in computed_seq_block_ids  # type: ignore\n             if ids != []\n         ])\n \n@@ -473,10 +595,10 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             blocks: List of blocks to be swapped out.\n         \"\"\"\n         for block in blocks:\n-            self.free(block)\n+            self._free_block_id(block)\n \n     def swap_in(self, blocks: List[Block]) -> None:\n-        \"\"\"Execute the swap int actions. Change the block id from \n+        \"\"\"Execute the swap in actions. Change the block id from \n         old allocator to current allocator for each block to finish \n         the block table update. \n \n@@ -484,13 +606,22 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n             blocks: List of blocks to be swapped in.\n         \"\"\"\n         for block in blocks:\n+            # Here we allocate either immutable or mutable block and then\n+            # extract its block_id. Note that the block object is released\n+            # and the block_id is assigned to \"block\" to allow reusing the\n+            # existing \"block\" object\n             if block.is_full:\n-                alloc = self.allocate_immutable(block.prev_block,\n-                                                block.token_ids)\n+                tmp_block = self.allocate_immutable_block(\n+                    prev_block=block.prev_block, token_ids=block.token_ids)\n             else:\n-                alloc = self.allocate_mutable(block.prev_block)\n-                alloc.append_token_ids(block.token_ids)\n-            block.block_id = alloc.block_id\n+                tmp_block = self.allocate_mutable_block(\n+                    prev_block=block.prev_block)\n+                tmp_block.append_token_ids(block.token_ids)\n+\n+            block_id = tmp_block.block_id\n+            self._block_pool.free_block(tmp_block)\n+\n+            block.block_id = block_id  # Assign block_id\n \n \n class PrefixCachingBlock(Block):\n@@ -507,7 +638,7 @@ class PrefixCachingBlock(Block):\n         token_ids (List[int]): The initial token IDs to be stored in the block.\n         block_size (int): The maximum number of token IDs that can be stored in\n             the block.\n-        prefix_caching_allocator (BlockAllocator): The prefix\n+        allocator (BlockAllocator): The prefix\n             caching block allocator associated with this block.\n         block_id (Optional[int], optional): The physical block index\n             of this block. Defaults to None.\n@@ -518,31 +649,55 @@ class PrefixCachingBlock(Block):\n         prev_block: Optional[Block],\n         token_ids: List[int],\n         block_size: int,\n-        prefix_caching_allocator: BlockAllocator,\n+        allocator: BlockAllocator,\n         block_id: Optional[int] = None,\n         computed: bool = False,\n     ):\n-        assert isinstance(prefix_caching_allocator,\n-                          PrefixCachingBlockAllocator), (\n-                              \"Currently this class is only tested with \"\n-                              \"PrefixCachingBlockAllocator.\")\n+        assert isinstance(allocator, PrefixCachingBlockAllocator), (\n+            \"Currently this class is only tested with \"\n+            \"PrefixCachingBlockAllocator. Got instead allocator = {}\".format(\n+                allocator))\n         assert_prefix_caching_block_or_none(prev_block)\n \n         self._prev_block = prev_block\n         self._cached_content_hash: Optional[int] = None\n-        self._cached_num_tokens_total: Optional[int] = None\n-        self._prefix_caching_allocator = prefix_caching_allocator\n+        self._cached_num_tokens_total: int = 0\n+        self._allocator = allocator\n         self._last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME\n         self._computed = computed\n \n-        self._block = NaiveBlock(\n-            prev_block=prev_block,\n-            token_ids=token_ids,\n-            block_size=block_size,\n-            block_id=block_id,\n-            allocator=prefix_caching_allocator,\n-            _cow_target=self,\n-        )\n+        # On the first time, we create the block object, and next we only\n+        # reinitialize it\n+        if hasattr(self, \"_block\"):\n+            self._block.__init__(  # type: ignore[has-type]\n+                prev_block=prev_block,\n+                token_ids=token_ids,\n+                block_size=block_size,\n+                block_id=block_id,\n+                allocator=self._allocator)\n+        else:\n+            self._block = NaiveBlock(prev_block=prev_block,\n+                                     token_ids=token_ids,\n+                                     block_size=block_size,\n+                                     block_id=block_id,\n+                                     allocator=self._allocator)\n+\n+        self._update_num_tokens_total()\n+\n+    def _update_num_tokens_total(self):\n+        \"\"\"Incrementally computes the number of tokens that there is\n+        till the current block (included)\n+        \"\"\"\n+        res = 0\n+\n+        # Add all previous blocks\n+        if self._prev_block is not None:\n+            res += self._prev_block.num_tokens_total\n+\n+        # Add current block\n+        res += len(self.token_ids)\n+\n+        self._cached_num_tokens_total = res\n \n     @property\n     def computed(self) -> bool:\n@@ -564,22 +719,28 @@ class PrefixCachingBlock(Block):\n         \"\"\"Appends the given token IDs to the block and registers the block as\n         immutable if the block becomes full.\n \n-        Internally, the naive block handles CoW.\n-\n         Args:\n             token_ids (List[int]): The token IDs to be appended to the block.\n         \"\"\"\n-        assert token_ids\n+        # Ensure this is mutable block (not promoted)\n+        assert self.content_hash is None\n+        assert not self.computed\n+\n+        if len(token_ids) == 0:\n+            return\n \n-        # naive block handles CoW.\n+        # Ensure there are input tokens\n+        assert token_ids, \"Got token_ids = {}\".format(token_ids)\n+\n+        # Naive block handles CoW.\n         self._block.append_token_ids(token_ids)\n+        self._update_num_tokens_total()\n \n         # If the content hash is present, then the block can be made immutable.\n         # Register ourselves with the allocator, potentially replacing the\n         # physical block index.\n         if self.content_hash is not None:\n-            self.block_id = (self._prefix_caching_allocator.\n-                             promote_to_immutable_block(self))\n+            self.block_id = self._allocator.promote_to_immutable_block(self)\n \n     @property\n     def block_id(self) -> Optional[int]:\n@@ -599,23 +760,6 @@ class PrefixCachingBlock(Block):\n \n     @property\n     def num_tokens_total(self) -> int:\n-        \"\"\"return the total tokens so far.\n-\n-        Here we iterate the block chain till to the first block, while\n-        cache the result in local to prevent repeated computations.\n-        \"\"\"\n-        if self._cached_num_tokens_total is not None:\n-            return self._cached_num_tokens_total\n-\n-        _block: Optional[Block] = self\n-        self._cached_num_tokens_total = 0\n-\n-        # TODO: current implement here take O(N^2), we expect future\n-        # we have O(1) here\n-        while _block is not None:\n-            self._cached_num_tokens_total += len(_block.token_ids)\n-            _block = _block.prev_block\n-\n         return self._cached_num_tokens_total\n \n     @property\n@@ -638,7 +782,6 @@ class PrefixCachingBlock(Block):\n         For the content-based hash to be defined, the current block must be\n         full.\n         \"\"\"\n-\n         # If the hash is already computed, return it.\n         if self._cached_content_hash is not None:\n             return self._cached_content_hash\n@@ -688,7 +831,129 @@ class PrefixCachingBlock(Block):\n         return hash((is_first_block, prev_block_hash, *cur_block_token_ids))\n \n \n+class ComputedBlocksTracker:\n+    \"\"\"Handles caching of per-sequence computed block ids. \n+        When a sequence appears for the first time, it traverses all of the \n+        blocks and detects the prefix of blocks that is computed. On the\n+        subsequent times, it only traverses the new blocks that were added \n+        and updates the already recorded prefix of blocks with the newly \n+        computed blocks.\n+\n+        To avoid redundant traversals, the algorithm also detects when there\n+        is a \"gap\" in the computed prefix. For example, if we have blocks =\n+        [1,2,3,4,5], and we have detected [1,2,3] as the computed prefix, then\n+        we won't try to add more computed blocks to [1,2,3] in this sequence\n+        iteration, and will add more computed blocks only after the sequence is\n+        freed and reused again.\n+\n+        Note that currently, for a given sequence, we also skip the last \n+        block id for caching purposes, to avoid caching of a full sequence\n+    \"\"\"\n+\n+    def __init__(self, allocator):\n+        self._allocator = allocator\n+        self._cached_computed_seq_blocks: Dict[int, Tuple[List[int],\n+                                                          bool]] = {}\n+\n+    def add_seq(self, seq_id: int) -> None:\n+        \"\"\"Start tracking seq_id\n+        \"\"\"\n+        assert seq_id not in self._cached_computed_seq_blocks\n+        self._cached_computed_seq_blocks[seq_id] = ([], False)\n+\n+    def remove_seq(self, seq_id: int) -> None:\n+        \"\"\"Stop tracking seq_id\n+        \"\"\"\n+        assert seq_id in self._cached_computed_seq_blocks\n+        del self._cached_computed_seq_blocks[seq_id]\n+\n+    def get_cached_computed_blocks_and_update(\n+            self, seq_id: int, block_ids: List[int]) -> List[int]:\n+        \"\"\" Look at the class documentation for details\n+        \"\"\"\n+        # Ensure seq_id is already tracked\n+        assert seq_id in self._cached_computed_seq_blocks\n+\n+        # Get cached data (may be empty on the first time)\n+        prev_computed_block_ids, has_gap = self._cached_computed_seq_blocks[\n+            seq_id]\n+\n+        if has_gap:\n+            # When gap is detected, we do not add more computed blocks at this\n+            # sequence iteration\n+            return prev_computed_block_ids\n+\n+        # We do not consider the last block id for caching purposes.\n+        num_cur_blocks = len(block_ids) - 1\n+        assert num_cur_blocks >= 0\n+\n+        if len(prev_computed_block_ids) >= num_cur_blocks:\n+            # Cache HIT\n+            assert len(prev_computed_block_ids) == num_cur_blocks\n+            return prev_computed_block_ids\n+\n+        # If here, then we may possibly add more computed blocks. As a result,\n+        # traverse the additional blocks after prev_computed_block_ids to\n+        # detect more computed blocks and add them.\n+\n+        # Incremental init for seq_id => Look only at the new blocks\n+        computed_block_ids = self._allocator.get_computed_block_ids(  # noqa: E501\n+            prev_computed_block_ids,\n+            block_ids,\n+            skip_last_block_id=\n+            True,  # We skip last block id to avoid caching of full seq\n+        )\n+\n+        # Detect if there is a \"gap\"\n+        has_gap = len(computed_block_ids) < num_cur_blocks\n+\n+        # Record\n+        self._cached_computed_seq_blocks[seq_id] = (computed_block_ids,\n+                                                    has_gap)\n+\n+        return computed_block_ids\n+\n+\n+class LastAccessBlocksTracker:\n+    \"\"\"Manages the last access time of the tracked sequences, in order to allow\n+    an efficient update of allocator's block last access times\n+    \"\"\"\n+\n+    def __init__(self, allocator):\n+        self._allocator = allocator\n+        self._seq_last_access: Dict[int, Optional[float]] = {}\n+\n+    def add_seq(self, seq_id: int) -> None:\n+        \"\"\"Start tracking seq_id\n+        \"\"\"\n+        assert seq_id not in self._seq_last_access\n+        self._seq_last_access[seq_id] = None\n+\n+    def remove_seq(self, seq_id: int) -> None:\n+        \"\"\"Stop tracking seq_id\n+        \"\"\"\n+        assert seq_id in self._seq_last_access\n+        del self._seq_last_access[seq_id]\n+\n+    def update_last_access(self, seq_id: int, time: float) -> None:\n+        assert seq_id in self._seq_last_access\n+        self._seq_last_access[seq_id] = time\n+\n+    def update_seq_blocks_last_access(self, seq_id: int,\n+                                      block_ids: List[int]) -> None:\n+        assert seq_id in self._seq_last_access\n+\n+        ts = self._seq_last_access[seq_id]\n+\n+        if ts is None:\n+            # No last access was recorded, no need to update.\n+            return\n+\n+        self._allocator.mark_blocks_as_accessed(block_ids, ts)\n+\n+\n def assert_prefix_caching_block_or_none(block: Optional[Block]):\n     if block is None:\n         return\n-    assert isinstance(block, PrefixCachingBlock)\n+    assert isinstance(block,\n+                      PrefixCachingBlock), \"Got block = {}\".format(block)\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex 309775237..6a6eebc39 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -7,6 +7,8 @@ from typing import Tuple\n from vllm.core.block.block_table import BlockTable\n from vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\n from vllm.core.block.interfaces import Block\n+from vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,\n+                                                  LastAccessBlocksTracker)\n from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\n from vllm.core.interfaces import AllocStatus, BlockSpaceManager\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n@@ -100,6 +102,11 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         self.block_tables: Dict[SeqId, BlockTable] = {}\n         self.cross_block_tables: Dict[EncoderSeqId, BlockTable] = {}\n \n+        self._computed_blocks_tracker = ComputedBlocksTracker(\n+            self.block_allocator)\n+        self._last_access_blocks_tracker = LastAccessBlocksTracker(\n+            self.block_allocator)\n+\n     def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n         # FIXME(woosuk): Here we assume that all sequences in the group share\n         # the same prompt. This may not be true for preempted sequences.\n@@ -157,10 +164,18 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         block_table: BlockTable = self._allocate_sequence(seq)\n         self.block_tables[seq.seq_id] = block_table\n \n+        # Track seq\n+        self._computed_blocks_tracker.add_seq(seq.seq_id)\n+        self._last_access_blocks_tracker.add_seq(seq.seq_id)\n+\n         # Assign the block table for each sequence.\n         for seq in waiting_seqs[1:]:\n             self.block_tables[seq.seq_id] = block_table.fork()\n \n+            # Track seq\n+            self._computed_blocks_tracker.add_seq(seq.seq_id)\n+            self._last_access_blocks_tracker.add_seq(seq.seq_id)\n+\n         # Allocate cross-attention block table for encoder sequence\n         #\n         # NOTE: Here we assume that all sequences in the group have the same\n@@ -224,11 +239,23 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         return new_cows\n \n     def free(self, seq: Sequence) -> None:\n-        if seq.seq_id not in self.block_tables:\n+        seq_id = seq.seq_id\n+\n+        if seq_id not in self.block_tables:\n             # Already freed or haven't been scheduled yet.\n             return\n-        self.block_tables[seq.seq_id].free()\n-        del self.block_tables[seq.seq_id]\n+\n+        # Update seq block ids with the latest access time\n+        self._last_access_blocks_tracker.update_seq_blocks_last_access(\n+            seq_id, self.block_tables[seq.seq_id].physical_block_ids)\n+\n+        # Untrack seq\n+        self._last_access_blocks_tracker.remove_seq(seq_id)\n+        self._computed_blocks_tracker.remove_seq(seq_id)\n+\n+        # Free table/blocks\n+        self.block_tables[seq_id].free()\n+        del self.block_tables[seq_id]\n \n     def free_cross(self, seq_group: SequenceGroup) -> None:\n         request_id = seq_group.request_id\n@@ -239,9 +266,7 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         del self.cross_block_tables[request_id]\n \n     def get_block_table(self, seq: Sequence) -> List[int]:\n-        assert seq.seq_id in self.block_tables\n         block_ids = self.block_tables[seq.seq_id].physical_block_ids\n-        assert all(b is not None for b in block_ids)\n         return block_ids  # type: ignore\n \n     def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n@@ -252,20 +277,14 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         return block_ids  # type: ignore\n \n     def access_all_blocks_in_seq(self, seq: Sequence, now: float):\n-        # Update the last accessed time of all the blocks accessed\n-        # in this step.\n-        # And the accessed time is only useful for prefix caching now,\n-        # as it support internal evictor policy for which cached\n-        # block could be refilled, to keep cached content could be reused\n-        # at max extend.\n         if self.enable_caching:\n-            block_table = self.block_tables[seq.seq_id]\n-            block_ids: List[Optional[int]] = []\n-            for block_id in block_table.physical_block_ids:\n-                block_ids.append(block_id)\n-            self.block_allocator.mark_blocks_as_accessed(\n-                block_ids,  # type: ignore\n-                now)\n+            # Record the latest access time for the sequence. The actual update\n+            # of the block ids is deferred to the sequence free(..) call, since\n+            # only during freeing of block ids, the blocks are actually added to\n+            # the evictor (which is when the most updated time is required)\n+            # (This avoids expensive calls to mark_blocks_as_accessed(..))\n+            self._last_access_blocks_tracker.update_last_access(\n+                seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         # The only need for mark block as computed is for prefix caching,\n@@ -285,17 +304,26 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n         This method determines which blocks can be safely skipped for all\n         sequences in the sequence group.\n         \"\"\"\n-        seq_block_ids = [\n-            self.block_tables[seq.seq_id].physical_block_ids for seq in seqs\n-        ]\n+        computed_seq_block_ids = []\n+        for seq in seqs:\n+            computed_seq_block_ids.append(\n+                self._computed_blocks_tracker.\n+                get_cached_computed_blocks_and_update(\n+                    seq.seq_id,\n+                    self.block_tables[seq.seq_id].physical_block_ids))\n+\n         # NOTE(sang): This assumes seq_block_ids doesn't contain any None.\n         return self.block_allocator.get_common_computed_block_ids(\n-            seq_block_ids)  # type: ignore\n+            computed_seq_block_ids)  # type: ignore\n \n     def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n         src_block_table = self.block_tables[parent_seq.seq_id]\n         self.block_tables[child_seq.seq_id] = src_block_table.fork()\n \n+        # Track child seq\n+        self._computed_blocks_tracker.add_seq(child_seq.seq_id)\n+        self._last_access_blocks_tracker.add_seq(child_seq.seq_id)\n+\n     def can_swap_in(self, seq_group: SequenceGroup,\n                     num_lookahead_slots: int) -> AllocStatus:\n         \"\"\"Returns the AllocStatus for the given sequence_group \n@@ -323,19 +351,31 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             List[Tuple[int, int]]: The mapping of swapping block from CPU \n                 to GPU.\n         \"\"\"\n-        blocks = self._get_blocks_for_swap(seq_group, SequenceStatus.SWAPPED)\n-        current_swap_mapping = self.block_allocator.swap(\n-            blocks=blocks, source_device=Device.CPU, dest_device=Device.GPU)\n-\n-        block_number_mapping = {\n-            self.block_allocator.get_physical_block_id(Device.CPU,\n-                                                       cpu_block_id):\n-            self.block_allocator.get_physical_block_id(Device.GPU,\n-                                                       gpu_block_id)\n-            for cpu_block_id, gpu_block_id in current_swap_mapping.items()\n-        }\n-        # convert to list of tuples once here\n-        return list(block_number_mapping.items())\n+        physical_block_id_mapping = []\n+        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n+            blocks = self.block_tables[seq.seq_id].blocks\n+            if len(blocks) == 0:\n+                continue\n+\n+            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n+                                                         src_device=Device.CPU,\n+                                                         dst_device=Device.GPU)\n+\n+            # Refresh the block ids of the table (post-swap)\n+            self.block_tables[seq.seq_id].update(blocks)\n+\n+            seq_physical_block_id_mapping = {\n+                self.block_allocator.get_physical_block_id(\n+                    Device.CPU, cpu_block_id):\n+                self.block_allocator.get_physical_block_id(\n+                    Device.GPU, gpu_block_id)\n+                for cpu_block_id, gpu_block_id in seq_swap_mapping.items()\n+            }\n+\n+            physical_block_id_mapping.extend(\n+                list(seq_physical_block_id_mapping.items()))\n+\n+        return physical_block_id_mapping\n \n     def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n         \"\"\"Returns whether we can swap out the given sequence_group \n@@ -355,7 +395,7 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             return True\n         return False\n \n-    def swap_out(self, sequence_group: SequenceGroup) -> List[Tuple[int, int]]:\n+    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n         \"\"\"Returns the block id mapping (from GPU to CPU) generated by\n         swapping out the given sequence_group with num_lookahead_slots.\n \n@@ -366,19 +406,31 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             List[Tuple[int, int]]: The mapping of swapping block from \n                 GPU to CPU.\n         \"\"\"\n-        blocks = self._get_blocks_for_swap(sequence_group,\n-                                           SequenceStatus.RUNNING)\n-        current_swap_mapping = self.block_allocator.swap(\n-            blocks=blocks, source_device=Device.GPU, dest_device=Device.CPU)\n-        block_number_mapping = {\n-            self.block_allocator.get_physical_block_id(Device.GPU,\n-                                                       gpu_block_id):\n-            self.block_allocator.get_physical_block_id(Device.CPU,\n-                                                       cpu_block_id)\n-            for gpu_block_id, cpu_block_id in current_swap_mapping.items()\n-        }\n-        # convert to list of tuples once here\n-        return list(block_number_mapping.items())\n+        physical_block_id_mapping = []\n+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            blocks = self.block_tables[seq.seq_id].blocks\n+            if len(blocks) == 0:\n+                continue\n+\n+            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n+                                                         src_device=Device.GPU,\n+                                                         dst_device=Device.CPU)\n+\n+            # Refresh the block ids of the table (post-swap)\n+            self.block_tables[seq.seq_id].update(blocks)\n+\n+            seq_physical_block_id_mapping = {\n+                self.block_allocator.get_physical_block_id(\n+                    Device.GPU, gpu_block_id):\n+                self.block_allocator.get_physical_block_id(\n+                    Device.CPU, cpu_block_id)\n+                for gpu_block_id, cpu_block_id in seq_swap_mapping.items()\n+            }\n+\n+            physical_block_id_mapping.extend(\n+                list(seq_physical_block_id_mapping.items()))\n+\n+        return physical_block_id_mapping\n \n     def get_num_free_gpu_blocks(self) -> int:\n         return self.block_allocator.get_num_free_blocks(Device.GPU)\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 5886ebc24..c13b17471 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -177,7 +177,8 @@ class LLMEngine:\n             \"enforce_eager=%s, kv_cache_dtype=%s, \"\n             \"quantization_param_path=%s, device_config=%s, \"\n             \"decoding_config=%r, observability_config=%r, \"\n-            \"seed=%d, served_model_name=%s)\",\n+            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n+            \"enable_prefix_caching=%s)\",\n             VLLM_VERSION,\n             model_config.model,\n             speculative_config,\n@@ -204,6 +205,8 @@ class LLMEngine:\n             observability_config,\n             model_config.seed,\n             model_config.served_model_name,\n+            scheduler_config.use_v2_block_manager,\n+            cache_config.enable_prefix_caching,\n         )\n         # TODO(woosuk): Print more configs in debug mode.\n \ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex 8741893c9..1bd095655 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -345,7 +345,7 @@ class OpenAIServingCompletion(OpenAIServing):\n                     out_logprobs = prompt_logprobs\n                     output_text = prompt_text\n                 elif request.echo and request.max_tokens > 0:\n-                    token_ids = prompt_token_ids + output.token_ids\n+                    token_ids = prompt_token_ids + list(output.token_ids)\n                     out_logprobs = (prompt_logprobs + output.logprobs\n                                     if request.logprobs is not None else None)\n                     output_text = prompt_text + output.text\ndiff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex f95de56f3..ad5fb1317 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -427,8 +427,8 @@ class SamplingTensors:\n                 if seq_group.do_sample:\n                     for seq_id in seq_ids:\n                         seq_data = seq_group.seq_data[seq_id]\n-                        prompt_tokens.append(seq_data.prompt_token_ids)\n-                        output_tokens.append(seq_data.output_token_ids)\n+                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n+                        output_tokens.append(list(seq_data.output_token_ids))\n \n         sampling_tensors = SamplingTensors.from_lists(\n             temperatures, top_ps, top_ks, min_ps, presence_penalties,\ndiff --git a/vllm/outputs.py b/vllm/outputs.py\nindex 49f526b5f..4cb7f06bd 100644\n--- a/vllm/outputs.py\n+++ b/vllm/outputs.py\n@@ -1,6 +1,6 @@\n import time\n from dataclasses import dataclass\n-from typing import List, Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n from vllm.lora.request import LoRARequest\n from vllm.sequence import (PromptLogprobs, RequestMetrics, SampleLogprobs,\n@@ -28,7 +28,7 @@ class CompletionOutput:\n \n     index: int\n     text: str\n-    token_ids: List[int]\n+    token_ids: Tuple[int, ...]\n     cumulative_logprob: float\n     logprobs: Optional[SampleLogprobs]\n     finish_reason: Optional[str] = None\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 22cb26dc0..21c558d44 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -116,41 +116,66 @@ class SequenceData:\n         prompt_token_ids: List[int],\n         output_token_ids: Optional[List[int]] = None,\n     ) -> None:\n-        if output_token_ids is None:\n-            output_token_ids = []\n+        self._prompt_token_ids: List[int] = list(prompt_token_ids)\n+        self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n+        self._output_token_ids: List[int] = (\n+            list(output_token_ids) if output_token_ids is not None else [])\n \n-        self.prompt_token_ids = prompt_token_ids\n-        self._prompt_token_ids_tuple = tuple(prompt_token_ids)\n-        self.output_token_ids = output_token_ids\n         self.cumulative_logprob = 0.0\n         # The number of tokens that are computed (that run against the model).\n         self._num_computed_tokens = 0\n         self._stage: SequenceStage = SequenceStage.PREFILL\n \n+        self._update_cached_all_tokens()\n+\n+    def _update_cached_all_tokens(self):\n+        self._cached_all_token_ids: List[int] = (self._prompt_token_ids +\n+                                                 self._output_token_ids)\n+\n+    @property\n+    def prompt_token_ids(self) -> Tuple[int, ...]:\n+        return self._prompt_token_ids_tuple\n+\n+    @prompt_token_ids.setter\n+    def prompt_token_ids(self, new_prompt_token_ids) -> None:\n+        self._prompt_token_ids = list(new_prompt_token_ids)\n+        self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n+        self._update_cached_all_tokens()\n+\n+    @property\n+    def output_token_ids(self) -> Tuple[int, ...]:\n+        return tuple(self._output_token_ids)\n+\n+    @output_token_ids.setter\n+    def output_token_ids(self, new_output_token_ids) -> None:\n+        self._output_token_ids = list(new_output_token_ids)\n+        self._update_cached_all_tokens()\n+\n     def append_token_id(self, token_id: int, logprob: float) -> None:\n-        self.output_token_ids.append(token_id)\n+        self._output_token_ids.append(token_id)\n+        self._cached_all_token_ids.append(token_id)\n         self.cumulative_logprob += logprob\n \n     def get_len(self) -> int:\n-        return len(self.output_token_ids) + len(self.prompt_token_ids)\n+        return len(self._output_token_ids) + len(self._prompt_token_ids)\n \n     def get_prompt_len(self) -> int:\n-        return len(self.prompt_token_ids)\n+        return len(self._prompt_token_ids)\n \n     def get_output_len(self) -> int:\n-        return len(self.output_token_ids)\n+        return len(self._output_token_ids)\n \n     def get_token_ids(self) -> List[int]:\n-        return self.prompt_token_ids + self.output_token_ids\n+        return self._cached_all_token_ids\n \n     def get_prefix_token_ids(\n             self, num_tokens: int\n     ) -> Tuple[Tuple[int, ...], Optional[Tuple[int, ...]]]:\n         \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n-        prompt_length = len(self.prompt_token_ids)\n+        prompt_length = self.get_prompt_len()\n         if num_tokens > prompt_length:\n             return (self._prompt_token_ids_tuple,\n-                    tuple(self.output_token_ids[:num_tokens - prompt_length]))\n+                    tuple(self._output_token_ids[:num_tokens - prompt_length]))\n         else:\n             return (self._prompt_token_ids_tuple[:num_tokens], None)\n \n@@ -183,14 +208,14 @@ class SequenceData:\n         return self.get_len() - self.get_num_computed_tokens()\n \n     def get_last_token_id(self) -> int:\n-        if not self.output_token_ids:\n-            return self.prompt_token_ids[-1]\n-        return self.output_token_ids[-1]\n+        if not self._output_token_ids:\n+            return self._prompt_token_ids[-1]\n+        return self._output_token_ids[-1]\n \n-    def get_prompt_token_ids(self) -> List[int]:\n+    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n         return self.prompt_token_ids\n \n-    def get_output_token_ids(self) -> List[int]:\n+    def get_output_token_ids(self) -> Tuple[int, ...]:\n         return self.output_token_ids\n \n     @property\n@@ -199,8 +224,8 @@ class SequenceData:\n \n     def __repr__(self) -> str:\n         return (f\"SequenceData(\"\n-                f\"prompt_token_ids={self.prompt_token_ids}, \"\n-                f\"output_token_ids={self.output_token_ids}, \"\n+                f\"prompt_token_ids={self._prompt_token_ids}, \"\n+                f\"output_token_ids={self._output_token_ids}, \"\n                 f\"cumulative_logprob={self.cumulative_logprob})\")\n \n \n@@ -306,14 +331,14 @@ class Sequence:\n     def get_token_ids(self) -> List[int]:\n         return self.data.get_token_ids()\n \n-    def get_prompt_token_ids(self) -> List[int]:\n+    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n         return self.data.get_prompt_token_ids()\n \n     def get_last_token_id(self) -> int:\n         return self.data.get_last_token_id()\n \n-    def get_output_token_ids(self) -> List[int]:\n-        return self.data.output_token_ids\n+    def get_output_token_ids(self) -> Tuple[int, ...]:\n+        return self.data.get_output_token_ids()\n \n     def get_cumulative_logprob(self) -> float:\n         return self.data.cumulative_logprob",
  "apis": [
    "CpuGpuBlockAllocator.allocate_mutable_block",
    "CpuGpuBlockAllocator.allocate_immutable_block",
    "PrefixCachingBlockAllocator.allocate_mutable_block",
    "PrefixCachingBlockAllocator.allocate_immutable_block",
    "PrefixCachingBlock.__init__"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/serving_completion.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit changes a benchmark file (\"benchmarks/benchmark_latency.py\") which is a non-test file and affects how the benchmark measures latency by toggling prefix caching. In addition, significant modifications in the block allocator and caching behavior are made in various test files to verify the new behavior. The commit message indicates a performance optimization (\"Optimize block_manager_v2 vs block_manager_v1, to make V2 default\") and the changes affect the core block allocation and caching mechanisms that likely have performance implications. Thus, despite some refactoring and test changes, the modifications are aimed at performance optimization on the CPU.",
  "llm_api_reason": "The commit makes several changes to improve and streamline the block management API used by the core components. In particular, it replaces the old allocation methods (allocate_mutable and allocate_immutable) with new ones (allocate_mutable_block and allocate_immutable_block) in the block allocator classes. The tests have been updated to use the new method names, and even the constructor of the prefix-caching block is changed to expect an \"allocator\" parameter instead of a \"prefix_caching_allocator\". These modifications ensure that the V2 block manager becomes the default mechanism."
}