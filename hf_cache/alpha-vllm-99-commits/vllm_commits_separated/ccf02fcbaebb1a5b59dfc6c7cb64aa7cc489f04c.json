{
  "commit_hash": "ccf02fcbaebb1a5b59dfc6c7cb64aa7cc489f04c",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14848",
  "pr_date": "2025-03-15",
  "timeline_text": "Copy link Collaborator tlrmchlsmth commented Mar 15, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . ‚Ä¶nnecessary Memory Copies  ( #14778 )\" This reverts commit fe66b34 . lm_eval --model vllm \\\n    --model_args pretrained=ibm-ai-platform/Bamba-9B,tensor_parallel_size=1,dtype=auto,gpu_memory_utilization=0.8 \\\n    --tasks gsm8k --limit 100 \\\n    --batch_size auto main:\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |    0|¬±  |     0|\n|     |       |strict-match    |     5|exact_match|‚Üë  |    0|¬±  |     0|\n\n\nthis PR:\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  | 0.22|¬±  |0.0416|\n|     |       |strict-match    |     5|exact_match|‚Üë  | 0.32|¬±  |0.0469| Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ‚Ä¶ 9baec50 ‚Ä¶nnecessary Memory Copies ( #14778 )\"\n\nThis reverts commit fe66b34 . Copy link github-actions bot commented Mar 15, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . DarkLight1337 approved these changes Mar 15, 2025 View reviewed changes Hide details View details vllm-bot merged commit ccf02fc into main Mar 15, 2025 19 checks passed Uh oh! There was an error while loading. Please reload this page . vllm-bot deleted the revert_mamba_vmap branch March 15, 2025 03:45 lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ( ‚Ä¶ 69ebbe1 vllm-project#14848 )\n\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ( ‚Ä¶ 40cd8aa vllm-project#14848 ) RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ( ‚Ä¶ 0129fdd vllm-project#14848 )\n\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:52",
  "has_lm_eval": true,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, gsm8k, gsm8k | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:51:52",
  "models": [
    "ibm-ai-platform/Bamba-9B"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=ibm-ai-platform/Bamba-9B,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ (#14848)",
  "commit_message": "Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ (#14848)",
  "commit_date": "2025-03-14T20:45:42-07:00",
  "files_changed": [
    "vllm/model_executor/layers/mamba/mamba_mixer2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 30,
    "num_non_test_edited_lines": 30,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex 5b19e3f35..b53a540ed 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -466,17 +466,10 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-\n-            if has_initial_states is not None and torch.any(\n-                    has_initial_states):\n-\n-                # vectorized ssm_state zero init\n-                batched_zero_init_func = torch.vmap(\n-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n-                batched_zero_init_func(\n-                    mamba_cache_params.\n-                    state_indices_tensor[~has_initial_states].unsqueeze(\n-                        dim=-1), )\n+            if has_initial_states is not None and any(has_initial_states):\n+                for idx in mamba_cache_params.state_indices_tensor[\n+                        ~has_initial_states]:\n+                    mamba_cache_params.ssm_state[idx].zero_()\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -500,17 +493,10 @@ class MambaMixer2(CustomOp):\n                 dt_limit=(0.0, float(\"inf\")),\n             )\n \n-            # vectorized ssm state update using vmap\n-            # the 1d state_indices_tensor needs to be unsqueezed to avoid vmap\n-            # limitation which doesn't allow use of `item()`\n-            # Note: the lambda capture can happen where ssm_state is initialized\n-            #       instead of here\n-            batched_copy = torch.vmap(\n-                lambda idx, source_state: mamba_cache_params.ssm_state[\n-                    idx].copy_(source_state))\n-            batched_copy(\n-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n-                varlen_state)\n+            # update ssm states\n+            # - varlen state is a (batch, nheads, headdim, dstate) tensor\n+            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n+                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)",
  "apis": [
    "Mamba2DecoderLayer.forward",
    "Mamba2Model.forward",
    "Mamba2ForCausalLM.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba_cache.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit reverts previously introduced performance tweaks‚Äîin this case, vectorized operations using torch.vmap‚Äîswitching back to explicit loops for state initialization and update. It affects a non-test source code file in the high-level model executor module and is directly related to performance optimizations (or their rollback). While it does not add a new performance optimization, it is clearly tied to performance behavior (undoing the optimization tweaks), and meets the criteria given.",
  "llm_api_reason": "This commit reverts recent performance tweaks in the Mamba2 mixer layer. Instead of using torch.vmap for vectorized initialization and update of the SSM state, the code now iterates with for‚Äêloops. Since the MambaMixer2 is used by Mamba2DecoderLayer (which in turn is used in Mamba2Model and Mamba2ForCausalLM), these changes affect the forward computation during model inference."
}