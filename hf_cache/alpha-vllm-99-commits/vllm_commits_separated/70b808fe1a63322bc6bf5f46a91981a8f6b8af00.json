{
  "commit_hash": "70b808fe1a63322bc6bf5f46a91981a8f6b8af00",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14377",
  "pr_date": "2025-03-11",
  "timeline_text": "Copy link Contributor cynthieye commented Mar 6, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . qwen2-vl logic optimization: During each forward propagation, the xformer branch of Qwen2VisionTransformer will execute multiple tensor tolist methods (flash attn branch will execute multiple tensor items) to force the GPU tensor to be copied to the CPU, triggering CUDAMemcpyAsync to increase time consumption. Since the input and output are the same multiple times, it will be executed once, and the remaining will reuse the first result. After optimization, the online environment xformer branch QPS can be improved by 15%, and the flash attn branch QPS can be improved by 7% Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 2 imkero and ywang96 reacted with thumbs up emoji All reactions üëç 2 reactions DarkLight1337 requested review from Isotr0py and ywang96 March 7, 2025 06:41 Isotr0py approved these changes Mar 7, 2025 View reviewed changes Copy link Member Isotr0py left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for this optimization! Can you please also update qwen2.5-vl as well? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 cynthieye reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction vllm/model_executor/models/qwen2_vl.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/models/qwen2_vl.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . cynthieye changed the title feat:Optimize qwen2-vl to reduce cudaMemcpyAsync [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync Mar 10, 2025 cynthieye force-pushed the main branch\n      3 times, most recently\n    from ae09649 to 1fbb69c Compare March 10, 2025 06:53 Isotr0py enabled auto-merge (squash) March 10, 2025 09:50 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 10, 2025 auto-merge was automatically disabled March 10, 2025 13:26 Head branch was pushed to by a user without write access cynthieye force-pushed the main branch\n    from a4d7e3a to 37e543a Compare March 10, 2025 13:26 [Perf]: Optimize qwen2-vl to reduce cudaMemcpyAsync ‚Ä¶ 347de39 Signed-off-by: cynthieye <987073381@qq.com> cynthieye force-pushed the main branch\n    from 37e543a to 347de39 Compare March 10, 2025 13:29 cynthieye mentioned this pull request Mar 10, 2025 [CI failed]: V1 Test Failed due to \"No available memory for the cache blocks\" in GitHub Actions #14574 Closed 1 task empty test ‚Ä¶ fd105c1 Signed-off-by: cynthieye <987073381@qq.com> Copy link Member ywang96 commented Mar 11, 2025 @cynthieye Thank you for making this PR! Can you update this branch with our main branch? I think thr CI error should be fixed on main a while ago. ‚ù§Ô∏è 1 cynthieye reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ywang96 approved these changes Mar 11, 2025 View reviewed changes Copy link Member ywang96 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Left a few comments - Otherwise LGTM! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/models/qwen2_5_vl.py Outdated @@ -259,6 +259,8 @@ def forward( x: torch.Tensor, cu_seqlens: torch.Tensor, rotary_pos_emb: torch.Tensor, max_seqlen: int = None, Copy link Member ywang96 Mar 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Shouldn't max_seqlen be also Optional[int] ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/models/qwen2_5_vl.py Outdated Comment on lines 372 to 373 max_seqlen: int, seqlens: list[int], Copy link Member ywang96 Mar 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Please modify the typing accordingly Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/models/qwen2_vl.py Outdated Comment on lines 310 to 311 max_seqlen: int = None, seqlens: Optional[list[int]] = None, Copy link Member ywang96 Mar 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment ditto Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/models/qwen2_vl.py Outdated Comment on lines 417 to 418 max_seqlen: int, seqlens: list[int], Copy link Member ywang96 Mar 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment ditto Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/models/qwen2_5_vl.py Outdated Comment on lines 372 to 373 max_seqlen: int, seqlens: list[int], Copy link Member ywang96 Mar 11, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think it's probably a good idea to add a small documentation here to indicate that max_seqlen is only used for FA and seqlens is only used to xformers. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions cynthieye added 3 commits March 11, 2025 13:12 [Perf]: Fix formatting issues ‚Ä¶ 9959792 Signed-off-by: cynthieye <987073381@qq.com> Merge remote-tracking branch 'upstream/main' c03f59d [Perf]: Fix formatting issues ‚Ä¶ ddb8dd3 Signed-off-by: cynthieye <987073381@qq.com> ywang96 enabled auto-merge (squash) March 11, 2025 06:25 Hide details View details ywang96 merged commit 70b808f into vllm-project : main Mar 11, 2025 33 checks passed Uh oh! There was an error while loading. Please reload this page . This was referenced Mar 20, 2025 [Bugfix] Fix incorrect qwen2.5-vl attention mask pre-computation #15200 Merged [Misc] Add attention mask pre-computation optimization back to Qwen2.5-VL #15273 Merged lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync ( vllm-project#14377 ) ‚Ä¶ d468e24 Signed-off-by: cynthieye <987073381@qq.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync ( vllm-project#14377 ) ‚Ä¶ 8ece569 Signed-off-by: cynthieye <987073381@qq.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync ( vllm-project#14377 ) ‚Ä¶ 21ac3af Signed-off-by: cynthieye <987073381@qq.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:59",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: QPS, QPS, optimization | TEST: Test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:59",
  "models": [
    "Qwen/Qwen2-VL-2B",
    "Qwen/Qwen2-VL-7B",
    "Qwen/Qwen2.5-VL-3B",
    "Qwen/Qwen2.5-VL-7B"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen2-VL-7B --tasks mmlu --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2-VL-7B --dataset-name random --request-rate 1",
  "commit_subject": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)",
  "commit_message": "[Perf]:Optimize qwen2-vl to reduce cudaMemcpyAsync (#14377)\n\nSigned-off-by: cynthieye <987073381@qq.com>",
  "commit_date": "2025-03-11T07:39:56Z",
  "files_changed": [
    "vllm/model_executor/models/qwen2_5_vl.py",
    "vllm/model_executor/models/qwen2_vl.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 12,
    "num_edited_lines": 94,
    "num_non_test_edited_lines": 94,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ef3d28c80..ae48c7794 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -255,10 +255,12 @@ class Qwen2_5_VisionAttention(nn.Module):\n         return q, k, v\n \n     def forward(\n-        self,\n-        x: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor,\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n         # [s, b, c] --> [s, b, head * 3 * head_dim]\n         x, _ = self.qkv(x)\n@@ -285,7 +287,6 @@ class Qwen2_5_VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -321,7 +322,6 @@ class Qwen2_5_VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -364,11 +364,20 @@ class Qwen2_5_VisionBlock(nn.Module):\n                                      quant_config=quant_config,\n                                      prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n+    def forward(\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n+    ) -> torch.Tensor:\n         x = x + self.attn(self.norm1(x),\n                           cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+                          rotary_pos_emb=rotary_pos_emb,\n+                          max_seqlen=max_seqlen,\n+                          seqlens=seqlens)\n+\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -528,6 +537,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             quant_config=quant_config,\n             prefix=f\"{prefix}.merger\",\n         )\n+        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)\n \n     @property\n     def dtype(self) -> torch.dtype:\n@@ -633,14 +643,25 @@ class Qwen2_5_VisionTransformer(nn.Module):\n \n         # transformers\n         hidden_states = hidden_states.unsqueeze(1)\n+\n+        max_seqlen = None\n+        seqlens = None\n+        if self.attn_backend == _Backend.FLASH_ATTN:\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+        elif self.attn_backend == _Backend.XFORMERS:\n+            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         for layer_num, blk in enumerate(self.blocks):\n             if layer_num in self.fullatt_block_indexes:\n                 cu_seqlens_now = cu_seqlens\n             else:\n                 cu_seqlens_now = cu_window_seqlens\n-            hidden_states = blk(hidden_states,\n-                                cu_seqlens=cu_seqlens_now,\n-                                rotary_pos_emb=rotary_pos_emb)\n+            hidden_states = blk(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens_now,\n+                rotary_pos_emb=rotary_pos_emb,\n+                max_seqlen=max_seqlen,\n+                seqlens=seqlens,\n+            )\n \n         # For Qwen2.5-VL-3B, float16 will overflow at last block\n         # for long visual tokens sequences.\ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex ac3d154dd..0e9fa7183 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -303,10 +303,12 @@ class Qwen2VisionAttention(nn.Module):\n         return q, k, v\n \n     def forward(\n-        self,\n-        x: torch.Tensor,\n-        cu_seqlens: torch.Tensor,\n-        rotary_pos_emb: torch.Tensor,\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n     ) -> torch.Tensor:\n \n         # [s, b, c] --> [s, b, 3 * head * head_dim]\n@@ -329,7 +331,6 @@ class Qwen2VisionAttention(nn.Module):\n \n             q, k, v = (rearrange(x, \"b s ... -> (b s) ...\") for x in [q, k, v])\n \n-            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n             output = flash_attn_varlen_func(q,\n                                             k,\n                                             v,\n@@ -365,7 +366,6 @@ class Qwen2VisionAttention(nn.Module):\n             from xformers import ops as xops\n             from xformers.ops.fmha.attn_bias import BlockDiagonalMask\n \n-            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n             attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,\n                                                        kv_seqlen=None,\n                                                        device=q.device)\n@@ -409,11 +409,22 @@ class Qwen2VisionBlock(nn.Module):\n                                   quant_config=quant_config,\n                                   prefix=f\"{prefix}.mlp\")\n \n-    def forward(self, x: torch.Tensor, cu_seqlens: torch.Tensor,\n-                rotary_pos_emb: torch.Tensor) -> torch.Tensor:\n-        x = x + self.attn(self.norm1(x),\n-                          cu_seqlens=cu_seqlens,\n-                          rotary_pos_emb=rotary_pos_emb)\n+    def forward(\n+            self,\n+            x: torch.Tensor,\n+            cu_seqlens: torch.Tensor,\n+            rotary_pos_emb: torch.Tensor,\n+            max_seqlen: Optional[int] = None,  # Only used for Flash Attention\n+            seqlens: Optional[list[int]] = None,  # Only used for xFormers\n+    ) -> torch.Tensor:\n+        x = x + self.attn(\n+            self.norm1(x),\n+            cu_seqlens=cu_seqlens,\n+            rotary_pos_emb=rotary_pos_emb,\n+            max_seqlen=max_seqlen,\n+            seqlens=seqlens,\n+        )\n+\n         x = x + self.mlp(self.norm2(x))\n         return x\n \n@@ -570,6 +581,7 @@ class Qwen2VisionTransformer(nn.Module):\n             quant_config=quant_config,\n             prefix=f\"{prefix}.merger\",\n         )\n+        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)\n \n     @property\n     def dtype(self) -> torch.dtype:\n@@ -624,8 +636,21 @@ class Qwen2VisionTransformer(nn.Module):\n \n         # transformers\n         x = x.unsqueeze(1)\n+\n+        max_seqlen = None\n+        seqlens = None\n+        if self.attn_backend == _Backend.FLASH_ATTN:\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max().item()\n+        elif self.attn_backend == _Backend.XFORMERS:\n+            seqlens = (cu_seqlens[1:] - cu_seqlens[:-1]).tolist()\n         for blk in self.blocks:\n-            x = blk(x, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n+            x = blk(\n+                x,\n+                cu_seqlens=cu_seqlens,\n+                rotary_pos_emb=rotary_pos_emb,\n+                max_seqlen=max_seqlen,\n+                seqlens=seqlens,\n+            )\n \n         # adapter\n         x = self.merger(x)",
  "apis": [
    "Qwen2VisionAttention.forward",
    "Qwen2VisionBlock.forward",
    "Qwen2VisionTransformer.forward",
    "Qwen2_5_VisionAttention.forward",
    "Qwen2_5_VisionTransformer.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/qwen2_vl.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/qwen2_5_vl.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/multimodal/registry.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/inputs/registry.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/registry.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/chat_templates/registry.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies two non-test source files that are core to the model executor implementation. The changes adjust the forward function signatures by adding optional arguments (max_seqlen and seqlens) used for Flash Attention and xFormers, and remove redundant calculations of these values. These changes are aimed at reducing overhead (e.g., likely to reduce extra cudaMemcpyAsync calls as noted in the commit message) by allowing the caller to provide precomputed values. This modification, while involving refactoring, is targeted at optimizing performance by streamlining the computation pathway in high-level APIs and internal modules. The commit is not just a simple bug fix or feature addition; it is a performance optimization affecting CPU workflows and is general rather than hardware-specific.",
  "llm_api_reason": "The commit adds two optional parameters ‚Äúmax_seqlen‚Äù and ‚Äúseqlens‚Äù to the forward methods in the vision attention, block, and transformer modules for both Qwen2-VL and Qwen2.5-VL implementations so that the appropriate attention mask information is computed only once (or in a more efficient way) and passed to the underlying flash attention / xFormers routines. This change reduces the need for extra cudaMemcpy (thus optimizing performance), and it affects the forward methods of the vision-related modules in both versions. Based on the commit, the affected public python APIs are the forward methods of Qwen2VisionAttention, Qwen2VisionBlock, Qwen2VisionTransformer, Qwen2_5_VisionAttention, and Qwen2_5_VisionTransformer."
}