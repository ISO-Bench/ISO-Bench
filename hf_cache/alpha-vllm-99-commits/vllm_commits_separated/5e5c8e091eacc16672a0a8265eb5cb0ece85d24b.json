{
  "commit_hash": "5e5c8e091eacc16672a0a8265eb5cb0ece85d24b",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13236",
  "pr_date": "2025-02-14",
  "timeline_text": "Copy link Member mgoin commented Feb 13, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . For GPTQMarlin and AWQMarlin it seems the moe_wna16 kernel is faster for experts with dozens of experts, based on testing Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4 (60 experts), TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ (64 experts), and cognitivecomputations/DeepSeek-R1-AWQ (256 experts) cc @ElizaWszola @dsikka Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 Godofnothing reacted with thumbs up emoji All reactions üëç 1 reaction Use moe_wna16 kernel by default for MoEs with many experts ‚Ä¶ bb27d51 Signed-off-by: mgoin <mgoin64@gmail.com> mgoin requested review from robertgshaw2-redhat and tlrmchlsmth as code owners February 13, 2025 20:03 Copy link github-actions bot commented Feb 13, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Fixes ‚Ä¶ 4ac97e1 Signed-off-by: mgoin <mgoin64@gmail.com> Copy link Member Author mgoin commented Feb 13, 2025 @jinzhen-lin please see this PR. After this, I think we could remove moe_wna16 as a larger quant method and just use it as a kernel. What do you think? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Fix type issue ‚Ä¶ 3e07d17 Signed-off-by: mgoin <mgoin64@gmail.com> Copy link Contributor dsikka commented Feb 13, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Thanks for taking this on. Please run and/or update the weight_loading_large tests . I believe all the tests were skipped even when enabled when I last ran them last week so just something to potentially look out for. üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Feb 13, 2025 Fix weight-loading A100 test ‚Ä¶ c13deb5 Signed-off-by: mgoin <mgoin64@gmail.com> mgoin requested a review\n  from youkaichao as a code owner February 14, 2025 16:05 Copy link Member Author mgoin commented Feb 14, 2025 I fixed and ran the \"Weight Loading Multiple GPU Test - Large Models\", however it is failing due to unrelated compressedtensors dtype support issues. I think I can fix this by expanding the moe_wna16 method to compressedtensorsmoe, but will do in a followup All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth approved these changes Feb 14, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Good to land. There is some circular import \"weirdness\" but it can wait for a future refactor along the lines of this RFC #8913 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction mgoin added\n  the force-merge label Feb 14, 2025 dsikka reviewed Feb 14, 2025 View reviewed changes tests/weight_loading/test_weight_loading.py @@ -12,7 +12,7 @@ \"robertgshaw2/zephyr-7b-beta-channelwise-gptq\") REVISION = os.environ.get(\"REVISION\", \"main\") QUANTIZATION = os.environ.get(\"QUANTIZATION\", \"gptq_marlin\") MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \" 89 \") MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \" 80 \") Copy link Contributor dsikka Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment ah good catch Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions dsikka reviewed Feb 14, 2025 View reviewed changes vllm/model_executor/layers/quantization/gptq_marlin.py def __init__(self, weight_bits: int, group_size: int, desc_act: bool, is_sym: bool, lm_head_quantized: bool, dynamic: Dict[str, Dict[str, Union[int, bool]]], full_config: Dict[str, Any]) -> None: Copy link Contributor dsikka Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment What is full_config? Can we add a comment Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor dsikka Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Oh just the config dict, I see Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Member Author mgoin Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It is just the original config saved from from_config so we can forward to MoeWNA16Config Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 dsikka reacted with thumbs up emoji All reactions üëç 1 reaction mgoin added\n  the quantization label Feb 14, 2025 mgoin changed the title Use moe_wna16 kernel by default for MoEs with many experts [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts Feb 14, 2025 Hide details View details simon-mo merged commit 5e5c8e0 into vllm-project : main Feb 14, 2025 35 of 37 checks passed Uh oh! There was an error while loading. Please reload this page . mgoin mentioned this pull request Feb 20, 2025 [Feature]: Add moe_wna16 kernel as a backend for CompressedTensorsWNA16MoEMethod #13575 Closed 1 task hongxiayang pushed a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Feb 25, 2025 [MFM-2025-02-21] Merge main to llama fp8, DeepSeekV3 and PTPC-FP8 ( #445 ) ‚Ä¶ d7fefdf * [ROCM][AMD][TRITON] Halving warps number for fw_prefill to reduce spilling ( vllm-project#12713 )\n\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\n\n* Refactor `Linear` handling in `TransformersModel` ( vllm-project#12727 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [VLM] Add MLA with pure RoPE support for deepseek-vl2 models ( vllm-project#12729 )\n\n* [Misc] Bump the compressed-tensors version ( vllm-project#12736 )\n\n* [Model][Quant] Fix GLM, Fix fused module mappings for quantization ( vllm-project#12634 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Doc] Update PR Reminder with link to Developer Slack ( vllm-project#12748 )\n\n* [Bugfix] Fix OpenVINO model runner ( vllm-project#12750 )\n\n* [V1][Misc] Shorten `FinishReason` enum and use constant strings ( vllm-project#12760 )\n\n* [Doc] Remove performance warning for auto_awq.md ( vllm-project#12743 )\n\n* [Bugfix] Fix 'ModuleNotFoundError: No module named 'intel_extension_for_pytorch'' for --tensor-parallel-size more than 1  ( vllm-project#12546 )\n\n* [core][distributed] exact ray placement control ( vllm-project#12732 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* The code assumes WARP_SIZE to be equal to 32, which is not the case on ROCm ( #406 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* Merging PR vllm-project#12536 Merged via CLI script\n\n* [Hardware][Intel-Gaudi] Enable FusedSDPA support for Intel Gaudi (HPU)\n\n* Add: Support for Sparse24Bitmask Compressed Models\n\n* [VLM] Use shared field to pass token ids to model\n\n* [Docs] Drop duplicate [source] links\n\n* [VLM] Qwen2.5-VL\n\n* [VLM] Update compatibility with transformers 4.49\n\n* [ROCm][Kernel] Using the correct warp_size value\n\n* [Bugfix] Better FP8 supported defaults\n\n* [Misc][Easy] Remove the space from the file name\n\n* [Model] LoRA Support for Ultravox model ( vllm-project#11253 )\n\n* [Bugfix] Fix the test_ultravox.py's license ( vllm-project#12806 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* Improve `TransformersModel` UX ( vllm-project#12785 )\n\n* [Misc] Remove duplicated DeepSeek V2/V3 model definition ( vllm-project#12793 )\n\n* [Misc] Improve error message for incorrect pynvml ( vllm-project#12809 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Update w2 scale loading for GPTQMarlinMoE ( vllm-project#12757 )\n\n* [Docs] Add Google Cloud Slides ( vllm-project#12814 )\n\n* [Attention] Use FA3 for MLA on Hopper ( vllm-project#12807 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [misc] Reduce number of config file requests to HuggingFace ( vllm-project#12797 )\n\nSigned-off-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\nSigned-off-by: <>\nCo-authored-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\n\n* Update README.md 20250205_aiter ( #407 )\n\n* Update README.md 20250205_aiter\n\n* whitespace\n\n* adding VLLM_USE_AITER=0 advice\n\n* [Misc] Remove unnecessary decode call ( vllm-project#12833 )\n\n* [Kernel] Make rotary_embedding ops more flexible with input shape ( vllm-project#12777 )\n\n* [torch.compile] PyTorch 2.6 and nightly compatibility ( vllm-project#12393 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] double quote cmake package in build.inc.md ( vllm-project#12840 )\n\n* [Bugfix] Fix unsupported FA version check for Turing GPU ( vllm-project#12828 )\n\n* [V1] LoRA Support ( vllm-project#10957 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* Add Bamba Model ( vllm-project#10909 )\n\nSigned-off-by: Yu Chin Fabian Lim <flim@sg.ibm.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [MISC] Check space in the file names in the pre commit checks ( vllm-project#12804 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [misc] Revert # 12833 ( vllm-project#12857 )\n\nSigned-off-by: <>\nCo-authored-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\n\n* [Bugfix] FA2 illegal memory access ( vllm-project#12848 )\n\n* Make vllm compatible with verl ( vllm-project#12824 )\n\nCo-authored-by: zhangshulai <zhangshulai@bytedance.com>\n\n* [Bugfix] Missing quant_config in deepseek embedding layer ( vllm-project#12836 )\n\n* Prevent unecessary requests to huggingface hub ( vllm-project#12837 )\n\n* [MISC][EASY] Break check file names into entry and args in the pre-commit hooks ( vllm-project#12880 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [Misc] Remove unnecessary detokenization in multimodal processing ( vllm-project#12868 )\n\n* PR vllm-project#12718 ( vllm-project#12718 )\n\n* [V1] Logprobs and prompt logprobs support ( vllm-project#9880 )\n\nThis PR is adding support for sample logprobs & prompt logprobs to vLLM v1.\n\nNew behavior:\n\n- During model execution, model runner computes sample logprobs (if user-provided logprobs setting is not None) and prompt logprobs (if user-provided prompt_logprobs setting is not None). For both sample and prompt logprobs, the engine core returns 3 vectors: token ids, token logprob values, token ranks. Ranks reflect tokens' 1-indexed positions in the vocabulary vector after sorting the vocabulary by log probability in descending order.\n- In scheduler.update_from_output(), sample and prompt logprobs are incorporated into the EngineCoreOutput data structure which is transferred to the engine client. If multiprocessing is enabled, then sample and prompt logprobs will be (de)serialized when the EngineCoreOutput data structure is (de)serialized.\n- During output processing, the LogprobsProcessor transforms the triplet of token ids, token logprobs values, and token ranks into the OpenAI-compatible List[Dict[token id,Logprob]] format (for sample and prompt logprobs respectively.)\n- Each Logprob instance (whether sample- or prompt-) consists of a token's log-probability, rank, and detokenized string representation. Note that logprob detokenization is handled by the LogprobsProcessor not the detokenizer.\n\nSigned-off-by: Andrew Feldman <afeldman@neuralmagic.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n\nCo-authored-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\n\n* [ROCm] [Feature] [Doc] [Dockerfile] [BugFix] Support Per-Token-Activation Per-Channel-Weight FP8 Quantization Inferencing ( vllm-project#12501 )\n\n* fix rocm get_device name for moe configs ( #359 )\n\n* fix rocm get_device name\n\nuse 'market_name'\nhard-code names for mi308 & mi300\n\n* use gfx and num_CU for device name\n\n* using market_name\n\n* rename MI325_OAM to MI325X\n\n* rm (duplicate) MI300X_OAM\n\n* rename mi308\n\n* [V1] LM Eval With Streaming Integration Tests ( vllm-project#11590 )\n\n* [Bugfix] Fix disagg hang caused by the prefill and decode communication issues ( vllm-project#12723 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [V1][Minor] Remove outdated comment ( vllm-project#12928 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1] Move KV block hashes from Request to KVCacheManager ( vllm-project#12922 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Fix Qwen2_5_VLForConditionalGeneration packed_modules_mapping ( vllm-project#12905 )\n\n* [Misc] Fix typo in the example file ( vllm-project#12896 )\n\nSigned-off-by: Zhao Ke <yingxiongraomingzk@gmail.com>\n\n* [Bugfix] Fix multi-round chat error when mistral tokenizer is used ( vllm-project#12859 )\n\nSigned-off-by: Zifei Tong <zifeitong@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [bugfix] respect distributed_executor_backend in world_size=1 ( vllm-project#12934 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Add offline test for disaggregated prefill ( vllm-project#12418 )\n\n* [V1][Minor] Move cascade attn logic outside _prepare_inputs ( vllm-project#12943 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Build] Make pypi install work on CPU platform ( vllm-project#12874 )\n\n* [Hardware][Intel-Gaudi] Enable long-contexts + LoRA support for Intel Gaudi ( vllm-project#12812 )\n\nSigned-off-by: Sanju C Sudhakaran <scsudhakaran@habana.ai>\n\n* [misc]  Add LoRA to benchmark_serving ( vllm-project#12898 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Misc] Log time consumption on weight downloading ( vllm-project#12926 )\n\n* [CI] Resolve transformers-neuronx version conflict ( vllm-project#12925 )\n\n* [Doc] Correct HF repository for TeleChat2 models ( vllm-project#12949 )\n\n* [Misc] Add qwen2.5-vl BNB support ( vllm-project#12944 )\n\n* [CI/Build] Auto-fix Markdown files ( vllm-project#12941 )\n\n* [Bugfix] Remove unused seq_group_metadata_list from ModelInputForGPU ( vllm-project#12935 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [bugfix] fix early import of flash attention ( vllm-project#12959 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Merged multi-modal processor for GLM4V ( vllm-project#12449 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [V1][Minor] Remove outdated comment ( vllm-project#12968 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [RFC] [Mistral] FP8 format ( vllm-project#10130 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\n\n* [V1] Cache `uses_mrope` in GPUModelRunner ( vllm-project#12969 )\n\n* [core] port pynvml into vllm codebase ( vllm-project#12963 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [MISC] Always import version library first in the vllm package ( vllm-project#12979 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [core] improve error handling when wake up from sleep mode ( vllm-project#12981 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core][rlhf] add colocate example for RLHF ( vllm-project#12984 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] Use msgpack for core request serialization ( vllm-project#12918 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* Check if selected backend is None in get_attn_backend_cls() ( vllm-project#12975 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [core] fix sleep mode and pytorch checkpoint compatibility ( vllm-project#13001 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Add link to tool_choice tracking issue in tool_calling.md ( vllm-project#13003 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [misc] Add retries with exponential backoff for HF file existence check ( vllm-project#13008 )\n\n* [Bugfix] Clean up and fix multi-modal processors ( vllm-project#13012 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* Fix seed parameter behavior in vLLM ( vllm-project#13007 )\n\nSigned-off-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\n\n* Fixing the output formatting ( #414 )\n\n* [Model] Ultravox Model: Support v0.5 Release ( vllm-project#12912 )\n\nSigned-off-by: Farzad Abdolhosseini <farzad@fixie.ai>\n\n* [misc] Fix setup.py condition to avoid AMD from being mistaken with CPU ( vllm-project#13022 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [V1][Minor] Move scheduler outputs to a separate file ( vllm-project#13062 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Docs] Annouce Meta Meetup ( vllm-project#13065 )\n\nSigned-off-by: simon-mo <simon.mo@hey.com>\n\n* [Bugfix] Support missing tool parameters in mistral tokenizer ( vllm-project#12884 )\n\nSigned-off-by: Florian Greinacher <florian.greinacher@siemens.com>\n\n* [Benchmark] Add BurstGPT to benchmark_serving ( vllm-project#13063 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\n\n* [Core] Don't do platform detection at import time ( vllm-project#12933 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Misc] LoRA - Refactor Punica ops tests ( vllm-project#12970 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [Bugfix]: Reasoning output bug according to the chat template change ( vllm-project#13025 )\n\nSigned-off-by: Ce Gao <cegao@tensorchord.ai>\n\n* [V1][Metrics] Add GPU prefix cache hit rate % gauge ( vllm-project#12592 )\n\n* [executor] init `local_rank` as device index ( vllm-project#13027 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [ROCm] Using a more precise memory profiling ( vllm-project#12624 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [Build] Fix cuda link target of cumem_allocator in CPU env ( vllm-project#12863 )\n\nSigned-off-by: YuhongGuo <yuhong.gyh@antgroup.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Platform] add pre_register_and_update function ( vllm-project#12432 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] fix flaky test ( vllm-project#13089 )\n\nSigned-off-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\n\n* [V1][Metrics] Add several request timing histograms ( vllm-project#12644 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* Set `torch_dtype` in `TransformersModel` ( vllm-project#13088 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Misc] Fix typo at comments at metrics.py ( vllm-project#13024 )\n\n* [Bugfix] Do not use resource module on Windows ( vllm-project#12858 ) ( vllm-project#13029 )\n\n* [BugFix] Pop instead of del CUDA_VISIBLE_DEVICES ( vllm-project#12962 )\n\nSigned-off-by: Hollow Man <hollowman@opensuse.org>\n\n* Fix initializing GGUF weights for ColumnParallelLinear when using tensor parallel > 1 ( vllm-project#13023 )\n\n* Add tuned moe config for qwen1.5_moe_A2.7B ( #398 )\n\n* Add tuned moe config for qwen1.5_moe_A2.7B\n\n* Add more sweep parameters on qwen2_moe\n\n* Add tp = 1,2,4,8 after applying PR12838\n\n* Rename config name by deleting \"_OAM\"\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\n\n* [CI/Build][Bugfix] Fix CPU backend default threads num ( vllm-project#13077 )\n\n* Removing non-existent parameter\n\n* [Doc] Improve OpenVINO installation doc ( vllm-project#13102 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Bugfix] Guided decoding falls back to outlines when fails to import xgrammar ( vllm-project#12976 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Misc] Move pre-commit suggestion back to the end ( vllm-project#13114 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [RFC][vllm-API] Support tokenizer registry for customized tokenizer in vLLM ( vllm-project#12518 )\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\n\n* [Model] IBM/NASA Prithvi Geospatial model  ( vllm-project#12830 )\n\n* [ci] Add more source file dependencies for some tests ( vllm-project#13123 )\n\nSigned-off-by: <>\nCo-authored-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\n\n* [Neuron][Kernel] Support Longer Sequences in NKI-based Flash PagedAttention and Improve Efficiency ( vllm-project#12921 )\n\nSigned-off-by: Lingfan Yu <lingfany@amazon.com>\n\n* Bump helm/kind-action from 1.10.0 to 1.12.0 ( vllm-project#11612 )\n\n* Bump actions/stale from 9.0.0 to 9.1.0 ( vllm-project#12462 )\n\n* Bump helm/chart-testing-action from 2.6.1 to 2.7.0 ( vllm-project#12463 )\n\n* Bump actions/setup-python from 5.3.0 to 5.4.0 ( vllm-project#12672 )\n\n* Further reduce the HTTP calls to huggingface.co ( vllm-project#13107 )\n\n* [Misc] AMD Build Improvements ( vllm-project#12923 )\n\n* [Bug] [V1] Try fetching stop_reason from EngineOutput before checking the request ( vllm-project#13108 )\n\n* [Bugfix] Fix num video tokens calculation for Qwen2-VL ( vllm-project#13148 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Frontend] Generate valid tool call IDs when using `tokenizer-mode=mistral` ( vllm-project#12332 )\n\n* [Misc] Delete unused LoRA modules ( vllm-project#13151 )\n\n* Introduce VLLM_CUDART_SO_PATH to allow users specify the .so path ( vllm-project#12998 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [CI/Build] Use mypy matcher for pre-commit CI job ( vllm-project#13162 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Update Benchmark Profiling Scripts ( #417 )\n\n* Update profiling benchmarks\n\n* Fix linter errors\n\n---------\n\nCo-authored-by: AdrianAbeyta <Adrian.Abeyta@amd.com>\n\n* [CORE] [QUANT] Support for GPTQModel's `dynamic` quantization per module override/control ( vllm-project#7086 )\n\n* [Bugfix] Allow fallback to AWQ from AWQMarlin at per-layer granularity ( vllm-project#13119 )\n\n* DS V2V3 fix for same file\n\n* Lint\n\n* updating manfiest ( #416 )\n\n* [CI] Fix failing FP8 cpu offload test ( vllm-project#13170 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* Aiter base ( #419 )\n\n* Using upstream FA repo. Building aiter in the base docker image\n\n* Renaming the file to match upstream naming\n\n* [V1][Bugfix] Copy encoder input ids to fix set iteration issue during VLM abort ( vllm-project#13173 )\n\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com>\n\n* [CI/Build] Ignore ruff warning up007 ( vllm-project#13182 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [perf-benchmark] cleanup unused Docker images and volumes in H100 benchmark instance ( vllm-project#12706 )\n\n* [NVIDIA] Support nvfp4 quantization ( vllm-project#12784 )\n\n* [Bugfix][Example] Fix GCed profiling server for TPU ( vllm-project#12792 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [VLM] Implement merged multimodal processor for Mllama ( vllm-project#11427 )\n\n* Simplify logic of locating CUDART so file path ( vllm-project#13203 )\n\nSigned-off-by: Lu Fang <lufang@fb.com>\n\n* [Build] Automatically use the wheel of the base commit with Python-only build ( vllm-project#13178 )\n\n* [Bugfix] deepseek_r1_reasoning_parser put reason content in wrong field in certain edge case ( vllm-project#13097 )\n\n* [Frontend] Move CLI code into vllm.cmd package ( vllm-project#12971 )\n\n* Allow Unsloth Dynamic 4bit BnB quants to work ( vllm-project#12974 )\n\n* [CI/Build] Allow ruff to auto-fix some issues ( vllm-project#13180 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1][core] Implement pipeline parallel on Ray ( vllm-project#12996 )\n\n* [VLM] Remove input processor from clip and siglip ( vllm-project#13165 )\n\n* [Frontend] Pass pre-created socket to uvicorn ( vllm-project#13113 )\n\n* [V1] Clarify input processing and multimodal feature caching logic ( vllm-project#13211 )\n\n* [VLM] Merged multi-modal processor for Molmo ( vllm-project#12966 )\n\n* [V1][Core] Add worker_base for v1 worker ( vllm-project#12816 )\n\nSigned-off-by: Aoyu <aoyuzhan@amazon.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Aoyu <aoyuzhan@amazon.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Qwen2.5-VL Optimization ( vllm-project#13155 )\n\n* [VLM] Separate text-only and vision variants of the same model architecture ( vllm-project#13157 )\n\n* [Bugfix] Missing Content Type returns 500 Internal Server Error ( vllm-project#13193 )\n\n* [Frontend] Add `/v1/audio/transcriptions` OpenAI API endpoint ( vllm-project#12909 )\n\n* Initial attempt to adjust codeowners to the ROCm fork ( #420 )\n\n* Applying weight padding to deepseek ( #421 )\n\n* Add label if pre-commit passes ( vllm-project#12527 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Model] DeepSeek Tunings ( #423 )\n\n* fused_moe config for DSv3 on MI300X updated\n\n* Add tuning script and post processing script\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* Add modification to fp8_utils for tuning\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* update tuning script and add the configs\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* slightly better tunings\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* benchmark_moe.py is updated to generate more accurate MoE configs and a specific MoE config for DSv3 is added\n\n* Bug in sgl_moe_align_block_size() is fixed by Greg\n\n* Generate fp8_w8a8 config for MI300XHF\n\n* tunings that don't give garbage output\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* More accurate tunings\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* More accurate tunings and reject inaccurate configs\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* add new tunings\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* rename tuning script and add benchmark script to use for optimizing blockwise quant\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove white space from file names\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove white space from file names\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* Remove some unnecessary changes\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* don't use space in file names\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove XHF tunings\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove OAM from file name\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* rmeove OAM from file names\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* yapf\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* update config name\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove benchmark_moe.py changes\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove is_contiguous\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* use more recent fp8_utils.py\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* remove is_contiguous\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n---------\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\n\n* Optimize moe_align_block_size for deepseek_v3 ( vllm-project#12850 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Kernel][Bugfix] Refactor and Fix CUTLASS 2:4 Sparse Kernels ( vllm-project#13198 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Revert \"Add label if pre-commit passes\" ( vllm-project#13242 )\n\n* [ROCm] Avoid using the default stream on ROCm ( vllm-project#13238 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [Kernel] Fix awq error when n is not divisable by 128 ( vllm-project#13227 )\n\n* [V1] Consolidate MM cache size to vllm.envs ( vllm-project#13239 )\n\n* [Bugfix/CI] Turn test_compressed_tensors_2of4_sparse back on ( vllm-project#13250 )\n\n* [Bugfix][CI] Inherit codespell settings from pyproject.toml in the pre-commit-config ( vllm-project#13237 )\n\n* [Bugfix] Offline example of disaggregated prefill ( vllm-project#13214 )\n\n* [Misc] Remove redundant statements in scheduler.py ( vllm-project#13229 )\n\n* Consolidate Llama model usage in tests ( vllm-project#13094 )\n\n* Expand MLA to support most types of quantization ( vllm-project#13181 )\n\n* [V1] LoRA - Enable Serving Usecase ( vllm-project#12883 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* [ROCm][V1] Add intial ROCm support to V1 ( vllm-project#12790 )\n\n* [Bugfix][V1] GPUModelRunner._update_states should return True when there is a finished request in batch ( vllm-project#13126 )\n\n* [WIP] TPU V1 Support Refactored ( vllm-project#13049 )\n\n* [Frontend] Optionally remove memory buffer used for uploading to URLs in run_batch ( vllm-project#12927 )\n\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\n\n* [Bugfix] Fix missing parentheses ( vllm-project#13263 )\n\n* [Misc] Log time consumption of sleep and wake-up ( vllm-project#13115 )\n\nSigned-off-by: Jun Duan <jun.duan.phd@outlook.com>\n\n* [VLM] Keep track of whether prompt replacements have been applied ( vllm-project#13215 )\n\n* [V1] Simplify GPUModelRunner._update_states check ( vllm-project#13265 )\n\n* Support logit_bias in v1 Sampler ( vllm-project#13079 )\n\n* [Core] choice-based structured output with xgrammar ( vllm-project#12632 )\n\n* [Hardware][Gaudi][Bugfix] Fix error for guided decoding ( vllm-project#12317 )\n\n* Removing bad config ( #425 )\n\n* The order in the file is important. One needs to be explicitly be added to each following path for their ownership to apply ( #427 )\n\n* [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts ( vllm-project#13236 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Core] Reduce TTFT with concurrent partial prefills ( vllm-project#10235 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Prashant Gupta <prashantgupta@us.ibm.com>\nCo-authored-by: Prashant Gupta <prashantgupta@us.ibm.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [V1][Core] min_p sampling support ( vllm-project#13191 )\n\nSigned-off-by: Aoyu <aoyuzhan@amazon.com>\nCo-authored-by: Aoyu <aoyuzhan@amazon.com>\n\n* [V1][CI] Fix failed v1-test because of min_p ( vllm-project#13316 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1][Sampler] Don't apply temp for greedy-only ( vllm-project#13311 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [V1][PP] Fix memory profiling in PP ( vllm-project#13315 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix][AMD] Update torch_bindings so that scaled_fp4_quant isn't build on ROCm ( vllm-project#13235 )\n\n* [Bugfix][Docs] Fix offline Whisper ( vllm-project#13274 )\n\n* [Bugfix] Massage MLA's usage of flash attn for RoCM ( vllm-project#13310 )\n\n* [BugFix] Don't scan entire cache dir when loading model ( vllm-project#13302 )\n\n* [Bugfix]Fix search start_index of stop_checker ( vllm-project#13280 )\n\n* [Bugfix] Fix qwen2.5-vl image processor ( vllm-project#13286 )\n\n* [V1][Metrics] Add iteration_tokens_total histogram from V0 ( vllm-project#13288 )\n\n* [AMD] [Model] DeepSeek tunings ( vllm-project#13199 )\n\n* [V1][PP] Run engine busy loop with batch queue ( vllm-project#13064 )\n\n* [ci/build] update flashinfer ( vllm-project#13323 )\n\n* [Doc] [2/N] Add Fuyu E2E example for multimodal processor ( vllm-project#13331 )\n\n* [V1][Spec Decode] Ngram Spec Decode  ( vllm-project#12193 )\n\nSigned-off-by: LiuXiaoxuanPKU <lilyliupku@gmail.com>\n\n* [Quant] Add `SupportsQuant` to phi3 and clip ( vllm-project#13104 )\n\n* [Bugfix] Pin xgrammar to 0.1.11 ( vllm-project#13338 )\n\n* avoid calling hf_list_repo_files for local model\n\nSigned-off-by: isotr0py <2037008807@qq.com>\n\n* annotation\n\nSigned-off-by: isotr0py <2037008807@qq.com>\n\n* [BugFix] Enhance test_pos_encoding to support execution on multi-devices ( vllm-project#13187 )\n\nSigned-off-by: wchen61 <wchen61@foxmail.com>\n\n* [V1] Update doc and examples for H2O-VL ( vllm-project#13349 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [ci] skip failed tests for flashinfer ( vllm-project#13352 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [platform] add base class for communicators ( vllm-project#13208 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix 2 Node and Spec Decode tests ( vllm-project#13341 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Docs] Change myenv to vllm. Update python_env_setup.inc.md ( vllm-project#13325 )\n\n* [V1][BugFix] Add __init__.py to v1/spec_decode/ ( vllm-project#13359 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1][PP] Cache Intermediate Tensors ( vllm-project#13353 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix][Platform][CPU] Fix cuda platform detection on CPU backend edge case ( vllm-project#13358 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1][BugFix] Clean up rejection sampler & Fix warning msg ( vllm-project#13362 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [V1][Misc] Avoid unnecessary log output ( vllm-project#13289 )\n\n* [Feature][Spec Decode] Simplify the use of Eagle Spec Decode ( vllm-project#12304 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Fix spelling error in index.md ( vllm-project#13369 )\n\n* Run v1 benchmark and integrate with PyTorch OSS benchmark database ( vllm-project#13068 )\n\nSigned-off-by: Huy Do <huydhn@gmail.com>\n\n* [MISC] tiny fixes ( vllm-project#13378 )\n\n* [VLM] Check required fields before initializing field config in `DictEmbeddingItems` ( vllm-project#13380 )\n\n* [Model] Support Mamba2 (Codestral Mamba) ( vllm-project#9292 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Yu Chin Fabian Lim <flim@sg.ibm.com>\n\n* [Bugfix] fix xpu communicator ( vllm-project#13368 )\n\nSigned-off-by: yan ma <yan.ma@intel.com>\n\n* [Bugfix] Fix VLLM_USE_MODELSCOPE issue ( vllm-project#13384 )\n\n* Updating PR template to point people to the upstream repo. Updating codeowners ( #431 )\n\n* Enabling the ROCm-vLLM CI on MI250 machines ( #432 )\n\n* Enabling ROCm CI on MI250 machines:\n- correct build target\n- correct queue\n\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\n\n---------\n\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\n\n* Optimization for quantized gemm skinny sizes ( #411 )\n\n* Optimization for quantized gemm skinny sizes\n\n* lint fix\n\n* Add support for bf16/fp16\n\n* code cleanup\n\n* code cleanup\n\n* lint fix2\n\n* cleanup\n\n* Moved the logic into tuned gemm to preserve API compatibility\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* Restricting FP8 wvSplitk to MI300x ( #439 )\n\n* Remove mi300a ( #440 )\n\n* Removing gfx940 and gfx941 targets. These have been deprecated in favor of gfx942 for MI300X\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* Remove from custom kernels as well\n\n---------\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* resolve diff for mixtral8x7B configs ( #437 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* Torch version bump to fix tunable ops ( #442 )\n\n* Advance torch commit to be past pytorch/pytorch#144942 to fix tunable ops\n\n* Make sure to use the submodule commit compatible with the main aiter commit\n\n* bugfix: remove unused  argument passed to the forward pass of ReplicatedLinear layer\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nSigned-off-by: Lu Fang <lufang@fb.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\nSigned-off-by: <>\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nSigned-off-by: Yu Chin Fabian Lim <flim@sg.ibm.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Zhao Ke <yingxiongraomingzk@gmail.com>\nSigned-off-by: Zifei Tong <zifeitong@gmail.com>\nSigned-off-by: Sanju C Sudhakaran <scsudhakaran@habana.ai>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\nSigned-off-by: Farzad Abdolhosseini <farzad@fixie.ai>\nSigned-off-by: kevin <kevin@anyscale.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Florian Greinacher <florian.greinacher@siemens.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Ce Gao <cegao@tensorchord.ai>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: YuhongGuo <yuhong.gyh@antgroup.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Hollow Man <hollowman@opensuse.org>\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\nSigned-off-by: Lingfan Yu <lingfany@amazon.com>\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com>\nSigned-off-by: Aoyu <aoyuzhan@amazon.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nSigned-off-by: Jun Duan <jun.duan.phd@outlook.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Prashant Gupta <prashantgupta@us.ibm.com>\nSigned-off-by: LiuXiaoxuanPKU <lilyliupku@gmail.com>\nSigned-off-by: isotr0py <2037008807@qq.com>\nSigned-off-by: wchen61 <wchen61@foxmail.com>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Huy Do <huydhn@gmail.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: Akash kaothalkar <61960177+Akashcodes732@users.noreply.github.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Sanju C Sudhakaran <scsudhakaran@habana.ai>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Lu Fang <30275821+houseroad@users.noreply.github.com>\nCo-authored-by: Sumit Vij <sumitvij11+github@gmail.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: EC2 Default User <ec2-user@ip-172-31-20-117.us-west-2.compute.internal>\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com>\nCo-authored-by: Jitse Klomp <jitse@jitseklomp.nl>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Yu Chin Fabian Lim <fabianlim@users.noreply.github.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: ZSL98 <36250440+ZSL98@users.noreply.github.com>\nCo-authored-by: zhangshulai <zhangshulai@bytedance.com>\nCo-authored-by: Szymon O≈º√≥g <58388001+SzymonOzog@users.noreply.github.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Amit Garg <mitgarg17495@gmail.com>\nCo-authored-by: afeldman-nm <156691304+afeldman-nm@users.noreply.github.com>\nCo-authored-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-redhat@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Ke Zhao <yingxiongraomingzk@gmail.com>\nCo-authored-by: zifeitong <zifeitong@gmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Shaoting <shaotingf@uchicago.edu>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: Jun Duan <jun.duan.phd@outlook.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: Patrick von Platen <patrick.v.platen@gmail.com>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: ‡ÆÆ‡Æ©‡Øã‡Æú‡Øç‡Æï‡ØÅ‡ÆÆ‡Ææ‡Æ∞‡Øç ‡Æ™‡Æ¥‡Æ©‡Æø‡Æö‡Øç‡Æö‡Ææ‡ÆÆ‡Æø <smartmanoj42857@gmail.com>\nCo-authored-by: Farzad Abdolhosseini <farzad.abdolhosseini@gmail.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Florian Greinacher <florian.greinacher@siemens.com>\nCo-authored-by: Ce Gao <cegao@tensorchord.ai>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Jewon Lee <105219284+je1lee@users.noreply.github.com>\nCo-authored-by: MoonRide303 <130458190+MoonRide303@users.noreply.github.com>\nCo-authored-by: ‚Ñçùï†ùïùùïùùï†ùï® ùïÑùïíùïü <hollowman@opensuse.org>\nCo-authored-by: sky0530 <weiching0530@gmail.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Keyun Tong <tongkeyun@gmail.com>\nCo-authored-by: Christian Pinto <chrpinto@gmail.com>\nCo-authored-by: Lingfan Yu <lingfany@amazon.com>\nCo-authored-by: dependabot[bot] <49699333+dependabot[bot]@users.noreply.github.com>\nCo-authored-by: Shiyan Deng <842974287@qq.com>\nCo-authored-by: bnellnm <49004751+bnellnm@users.noreply.github.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Adrian Abeyta <adabeyta@amd.com>\nCo-authored-by: AdrianAbeyta <Adrian.Abeyta@amd.com>\nCo-authored-by: Qubitium-ModelCloud <qubitium@modelcloud.ai>\nCo-authored-by: Yida Wu <yida.wu@amd.com>\nCo-authored-by: Murali Andoorveedu <37849411+andoorve@users.noreply.github.com>\nCo-authored-by: Kaixi Hou <kaixih@nvidia.com>\nCo-authored-by: LikeSundayLikeRain <monsoon1013@gmail.com>\nCo-authored-by: Daniel Han <danielhanchen@gmail.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Aoyu <aoyuzhang1989@gmail.com>\nCo-authored-by: Aoyu <aoyuzhan@amazon.com>\nCo-authored-by: ÁáÉ <wulipc@163.com>\nCo-authored-by: Vaibhav Jain <vajain@redhat.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Wang Ran (Ê±™ÁÑ∂) <wrran@outlook.com>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: Kero Liang <kerorek@outlook.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-redhat@users.noreply.github.com>\nCo-authored-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nCo-authored-by: Xu Song <xusong.vip@gmail.com>\nCo-authored-by: Yu-Zhou <yu.zhou@intel.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Prashant Gupta <prashantgupta@us.ibm.com>\nCo-authored-by: Lily Liu <lilyliupku@gmail.com>\nCo-authored-by: isotr0py <2037008807@qq.com>\nCo-authored-by: wchen61 <wchen61@foxmail.com>\nCo-authored-by: Âáå <i@ioioi.cn>\nCo-authored-by: yankooo <948162199@qq.com>\nCo-authored-by: Huy Do <huydhn@gmail.com>\nCo-authored-by: Yu Chin Fabian Lim <flim@sg.ibm.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: r.4ntix <antix.blue@gmail.com>\nCo-authored-by: Alexei-V-Ivanov-AMD <156011006+Alexei-V-Ivanov-AMD@users.noreply.github.com>\nCo-authored-by: Hashem Hashemi <159079214+amd-hhashemi@users.noreply.github.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com> mgoin mentioned this pull request Apr 5, 2025 [Kernel] Use moe_wna16 kernel for compressed tensors wna16 moe models #16038 Merged lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many expe‚Ä¶ ‚Ä¶ ce61da9 ‚Ä¶rts ( vllm-project#13236 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Quant][Perf] Use moe_wna16 kernel by default for MoEs with many expe‚Ä¶ ‚Ä¶ 4abde6f ‚Ä¶rts ( vllm-project#13236 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:42",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, benchmark_serving, benchmark_serving | SERVING: Serving, Frontend, Frontend | TEST: test, test, Test",
  "analysis_extracted_at": "2025-09-07 17:52:42",
  "models": [
    "mistralai/Mistral-7B-Instruct-v0.3",
    "Qwen/Qwen2.5-7B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.3,dtype=float16 --tasks hellaswag,arc_challenge --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-7B-Instruct,dtype=float16 --tasks hellaswag,arc_challenge --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model mistralai/Mistral-7B-Instruct-v0.3 --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts (#13236)",
  "commit_message": "[Quant][Perf] Use moe_wna16 kernel by default for MoEs with many experts (#13236)\n\nSigned-off-by: mgoin <mgoin64@gmail.com>",
  "commit_date": "2025-02-14T12:53:42-08:00",
  "files_changed": [
    "tests/weight_loading/test_weight_loading.py",
    "vllm/model_executor/layers/quantization/awq_marlin.py",
    "vllm/model_executor/layers/quantization/gptq_marlin.py",
    "vllm/model_executor/layers/quantization/moe_wna16.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 4,
    "num_hunks": 12,
    "num_edited_lines": 65,
    "num_non_test_edited_lines": 63,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/weight_loading/test_weight_loading.py b/tests/weight_loading/test_weight_loading.py\nindex e456bfab8..9d6b25da7 100644\n--- a/tests/weight_loading/test_weight_loading.py\n+++ b/tests/weight_loading/test_weight_loading.py\n@@ -12,7 +12,7 @@ MODEL_NAME = os.environ.get(\"MODEL_NAME\",\n                             \"robertgshaw2/zephyr-7b-beta-channelwise-gptq\")\n REVISION = os.environ.get(\"REVISION\", \"main\")\n QUANTIZATION = os.environ.get(\"QUANTIZATION\", \"gptq_marlin\")\n-MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"89\")\n+MIN_CAPABILITY = os.environ.get(\"MIN_CAPABILITY\", \"80\")\n \n \n @pytest.mark.skipif(\ndiff --git a/vllm/model_executor/layers/quantization/awq_marlin.py b/vllm/model_executor/layers/quantization/awq_marlin.py\nindex a43b2e597..de4009d7d 100644\n--- a/vllm/model_executor/layers/quantization/awq_marlin.py\n+++ b/vllm/model_executor/layers/quantization/awq_marlin.py\n@@ -17,6 +17,7 @@ from vllm.model_executor.layers.quantization.awq import (AWQConfig,\n                                                          is_layer_skipped_awq)\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig, QuantizeMethodBase)\n+from vllm.model_executor.layers.quantization.moe_wna16 import MoeWNA16Config\n from vllm.model_executor.layers.quantization.utils import replace_parameter\n from vllm.model_executor.layers.quantization.utils.marlin_utils import (\n     apply_awq_marlin_linear, awq_to_marlin_zero_points, check_marlin_supported,\n@@ -134,7 +135,12 @@ class AWQMarlinConfig(QuantizationConfig):\n                     self.full_config).get_quant_method(layer, prefix)\n             return AWQMarlinLinearMethod(self)\n         elif isinstance(layer, FusedMoE):\n-            return AWQMoEMethod(self)\n+            if layer.num_experts > 32:\n+                # For MoEs with many experts the moe_wna16 kernel is faster\n+                return MoeWNA16Config.from_config(\n+                    self.full_config).get_quant_method(layer, prefix)\n+            else:\n+                return AWQMoEMethod(self)\n         return None\n \n     @classmethod\ndiff --git a/vllm/model_executor/layers/quantization/gptq_marlin.py b/vllm/model_executor/layers/quantization/gptq_marlin.py\nindex 0a9d86b00..f421dbd2c 100644\n--- a/vllm/model_executor/layers/quantization/gptq_marlin.py\n+++ b/vllm/model_executor/layers/quantization/gptq_marlin.py\n@@ -10,20 +10,18 @@ from vllm.logger import init_logger\n from vllm.model_executor.layers.fused_moe.layer import (\n     FusedMoE, FusedMoEMethodBase, FusedMoeWeightScaleSupported)\n from vllm.model_executor.layers.linear import (LinearMethodBase,\n-                                               UnquantizedLinearMethod,\n                                                set_weight_attrs)\n from vllm.model_executor.layers.quantization.base_config import (\n-    QuantizationConfig)\n+    QuantizationConfig, QuantizeMethodBase)\n from vllm.model_executor.layers.quantization.kernels.mixed_precision import (\n     MPLinearLayerConfig, choose_mp_linear_kernel)\n+from vllm.model_executor.layers.quantization.moe_wna16 import MoeWNA16Config\n from vllm.model_executor.layers.quantization.utils import replace_parameter\n from vllm.model_executor.layers.quantization.utils.gptq_utils import (\n     get_linear_quant_method)\n from vllm.model_executor.layers.quantization.utils.marlin_utils import (\n     check_marlin_supported, marlin_moe_permute_scales,\n     marlin_repeat_scales_on_all_ranks, verify_marlin_supported)\n-from vllm.model_executor.layers.vocab_parallel_embedding import (\n-    UnquantizedEmbeddingMethod)\n from vllm.model_executor.parameter import (ChannelQuantScaleParameter,\n                                            GroupQuantScaleParameter,\n                                            PackedColumnParameter,\n@@ -44,15 +42,10 @@ class GPTQMarlinConfig(QuantizationConfig):\n         (8, True): scalar_types.uint8b128,\n     }\n \n-    def __init__(\n-        self,\n-        weight_bits: int,\n-        group_size: int,\n-        desc_act: bool,\n-        is_sym: bool,\n-        lm_head_quantized: bool,\n-        dynamic: Dict[str, Dict[str, Union[int, bool]]],\n-    ) -> None:\n+    def __init__(self, weight_bits: int, group_size: int, desc_act: bool,\n+                 is_sym: bool, lm_head_quantized: bool,\n+                 dynamic: Dict[str, Dict[str, Union[int, bool]]],\n+                 full_config: Dict[str, Any]) -> None:\n         if desc_act and group_size == -1:\n             # In this case, act_order == True is the same as act_order == False\n             # (since we have only one group per output channel)\n@@ -90,6 +83,7 @@ class GPTQMarlinConfig(QuantizationConfig):\n         self.group_size = group_size\n         self.desc_act = desc_act\n         self.lm_head_quantized = lm_head_quantized\n+        self.full_config = full_config\n \n         if (weight_bits, is_sym) not in self.TYPE_MAP:\n             raise ValueError(\"Unsupported quantization config: \"\n@@ -132,7 +126,7 @@ class GPTQMarlinConfig(QuantizationConfig):\n         lm_head_quantized = cls.get_from_keys_or(config, [\"lm_head\"],\n                                                  default=False)\n         return cls(weight_bits, group_size, desc_act, is_sym,\n-                   lm_head_quantized, dynamic)\n+                   lm_head_quantized, dynamic, config)\n \n     @classmethod\n     def override_quantization_method(cls, hf_quant_cfg,\n@@ -155,12 +149,15 @@ class GPTQMarlinConfig(QuantizationConfig):\n                         \" faster inference\")\n         return None\n \n-    def get_quant_method(\n-        self, layer: torch.nn.Module, prefix: str\n-    ) -> Optional[Union[\"GPTQMarlinLinearMethod\", \"GPTQMarlinMoEMethod\",\n-                        UnquantizedLinearMethod, UnquantizedEmbeddingMethod]]:\n+    def get_quant_method(self, layer: torch.nn.Module,\n+                         prefix: str) -> Optional[\"QuantizeMethodBase\"]:\n         if isinstance(layer, FusedMoE):\n-            return GPTQMarlinMoEMethod(self)\n+            if layer.num_experts > 32:\n+                # For MoEs with many experts the moe_wna16 kernel is faster\n+                return MoeWNA16Config.from_config(\n+                    self.full_config).get_quant_method(layer, prefix)\n+            else:\n+                return GPTQMarlinMoEMethod(self)\n         return get_linear_quant_method(self, layer, prefix,\n                                        GPTQMarlinLinearMethod)\n \ndiff --git a/vllm/model_executor/layers/quantization/moe_wna16.py b/vllm/model_executor/layers/quantization/moe_wna16.py\nindex b9460e7d7..30eb04698 100644\n--- a/vllm/model_executor/layers/quantization/moe_wna16.py\n+++ b/vllm/model_executor/layers/quantization/moe_wna16.py\n@@ -9,13 +9,8 @@ from vllm.model_executor.layers.fused_moe.layer import (\n     FusedMoE, FusedMoEMethodBase, FusedMoeWeightScaleSupported)\n from vllm.model_executor.layers.linear import (LinearBase,\n                                                UnquantizedLinearMethod)\n-from vllm.model_executor.layers.quantization.awq import AWQConfig\n-from vllm.model_executor.layers.quantization.awq_marlin import AWQMarlinConfig\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig, QuantizeMethodBase)\n-from vllm.model_executor.layers.quantization.gptq import GPTQConfig\n-from vllm.model_executor.layers.quantization.gptq_marlin import (\n-    GPTQMarlinConfig)\n from vllm.model_executor.layers.quantization.utils.marlin_utils import (\n     check_marlin_supports_layer)\n from vllm.model_executor.utils import set_weight_attrs\n@@ -37,6 +32,12 @@ class MoeWNA16Config(QuantizationConfig):\n         self.linear_quant_method = linear_quant_method\n         self.full_config = full_config\n         self.use_marlin = False\n+        # Avoid circular import\n+        from vllm.model_executor.layers.quantization.awq import AWQConfig\n+        from vllm.model_executor.layers.quantization.awq_marlin import (\n+            AWQMarlinConfig)\n+        from vllm.model_executor.layers.quantization.gptq_marlin import (\n+            GPTQMarlinConfig)\n         if self.linear_quant_method == \"gptq\":\n             self.use_marlin = GPTQMarlinConfig.is_gptq_marlin_compatible(\n                 full_config)\n@@ -115,6 +116,8 @@ class MoeWNA16Config(QuantizationConfig):\n         capability_tuple = current_platform.get_device_capability()\n         device_capability = (-1 if capability_tuple is None else\n                              capability_tuple.to_int())\n+        # Avoid circular import\n+        from vllm.model_executor.layers.quantization.awq import AWQConfig\n         awq_min_capability = AWQConfig.get_min_capability()\n \n         gptq_compatible = quant_method == \"gptq\" and \\\n@@ -129,6 +132,13 @@ class MoeWNA16Config(QuantizationConfig):\n         if is_layer_skipped_quant(prefix, self.modules_to_not_convert):\n             return UnquantizedLinearMethod()\n         elif isinstance(layer, LinearBase):\n+            # Avoid circular import\n+            from vllm.model_executor.layers.quantization.awq import AWQConfig\n+            from vllm.model_executor.layers.quantization.awq_marlin import (\n+                AWQMarlinConfig)\n+            from vllm.model_executor.layers.quantization.gptq import GPTQConfig\n+            from vllm.model_executor.layers.quantization.gptq_marlin import (\n+                GPTQMarlinConfig)\n             if self.linear_quant_method == \"gptq\":\n                 if self.use_marlin:\n                     return GPTQMarlinConfig.from_config(",
  "apis": [
    "vllm.model_executor.layers.quantization.awq_marlin.AWQMarlinConfig.get_quant_method",
    "vllm.model_executor.layers.quantization.gptq_marlin.GPTQMarlinConfig.get_quant_method",
    "vllm.model_executor.layers.quantization.moe_wna16.MoeWNA16Config.get_quant_method"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/awq_marlin.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/gptq_marlin.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/moe_wna16.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test source files (e.g., quantization modules in awq_marlin.py, gptq_marlin.py, and moe_wna16.py) and introduces a branch that uses an optimized moe_wna16 kernel for MoEs with many experts. The intent is clearly to select a faster kernel when a layer has more than 32 experts, which will improve inference performance on CPU. This change is a performance optimization rather than a simple refactoring, bug fix, or documentation update. Although one file in tests was also modified, the significant changes are in non-test production code and are performance-related.",
  "llm_api_reason": "This commit adjusts the quantization method selection for MoE layers by adding a check on the number of experts. In both AWQMarlinConfig and GPTQMarlinConfig, if a FusedMoE layer has more than 32 experts, the quantization method is now switched to use the moe_wna16 kernel (via MoeWNA16Config.from_config(...)). Additionally, the default minimum GPU capability used in tests was relaxed from 89 to 80. These changes affect the public get_quant_method APIs in the AWQ and GPTQ marlin modules, as well as the MoeWNA16Config‚Äôs method invoked via those paths.\n\n[APIS] vllm.model_executor.layers.quantization.awq_marlin.AWQMarlinConfig.get_quant_method, vllm.model_executor.layers.quantization.gptq_marlin.GPTQMarlinConfig.get_quant_method, vllm.model_executor.layers.quantization.moe_wna16.MoeWNA16Config.get_quant_method [APIS]"
}