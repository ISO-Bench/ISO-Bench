{
  "commit_hash": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
  "pr_url": "https://github.com/vllm-project/vllm/pull/8050",
  "pr_date": "2024-09-03",
  "timeline_text": "Copy link Collaborator alexm-redhat commented Aug 31, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes the async + multi-step further by implementing a \"fully\" async behavior between the postprocessor and the multi-step execution. Before that, the async was done only for the previous decode steps of the multi-step, where in this PR, the async is done on all previous steps of decode, including the last step of decode (that generates results), and also on the previous prompt executions. For Llama3 8B on H100 with ShareGPT dataset, performance improves by about ~28% vs current main with multi-step + async.  Here are the new results for this benchmark, the TPOT of multi-step is 44.48ms and for multi-step + async is 32.38ms, which is 37% improvement (before that @KuntaiDu reported improvement < 10%) Multi-step, no-async, Llama3 8B on H100 with ShareGPT ============ Serving Benchmark Result ============\nSuccessful requests:                     500       \nBenchmark duration (s):                  18.82     \nTotal input tokens:                      100895    \nTotal generated tokens:                  100377    \nRequest throughput (req/s):              26.57     \nInput token throughput (tok/s):          5361.68   \nOutput token throughput (tok/s):         5334.15   \n---------------Time to First Token----------------\nMean TTFT (ms):                          2991.94   \nMedian TTFT (ms):                        2314.58   \nP99 TTFT (ms):                           8385.04   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          44.48     \nMedian TPOT (ms):                        31.98     \nP99 TPOT (ms):                           199.97    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           272.29    \nMedian ITL (ms):                         244.50    \nP99 ITL (ms):                            1175.28   \n================================================== Multi-step + async, Llama3 8B on H100 with ShareGPT ============ Serving Benchmark Result ============\nSuccessful requests:                     500       \nBenchmark duration (s):                  16.04     \nTotal input tokens:                      100895    \nTotal generated tokens:                  100403    \nRequest throughput (req/s):              31.18     \nInput token throughput (tok/s):          6291.68   \nOutput token throughput (tok/s):         6261.00   \n---------------Time to First Token----------------\nMean TTFT (ms):                          2896.11   \nMedian TTFT (ms):                        2157.79   \nP99 TTFT (ms):                           7457.77   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          32.38     \nMedian TPOT (ms):                        24.64     \nP99 TPOT (ms):                           149.36    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           217.58    \nMedian ITL (ms):                         201.78    \nP99 ITL (ms):                            999.50    \n================================================== TODO Cleanup the PR Verify all tests pass Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸš€ 4 SolitaryThinker, Juelianqvq, yudian0504, and WoosukKwon reacted with rocket emoji All reactions ðŸš€ 4 reactions Copy link github-actions bot commented Aug 31, 2024 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Aug 31, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @robertgshaw2-neuralmagic @WoosukKwon @megha95 @KuntaiDu @comaniac @SolitaryThinker @njhill All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Aug 31, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . The PR is still in rough shape, since I just made it finally work after fixing some complicated race conditions. Will work on cleaning it up tomorrow. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator robertgshaw2-redhat commented Aug 31, 2024 nice job alex All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author alexm-redhat commented Aug 31, 2024 /ready All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 31, 2024 Copy link Collaborator Author alexm-redhat commented Aug 31, 2024 The PR is ready for review All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . alexm-redhat added 2 commits August 31, 2024 20:18 Optimize async + multi-step by making async fully async with respect â€¦ â€¦ dafa498 â€¦to all operations format ca993c7 alexm-redhat force-pushed the async_multi_step_opt branch\n    from e269cc7 to ca993c7 Compare August 31, 2024 20:41 Copy link Collaborator Author alexm-redhat commented Aug 31, 2024 rebased over Andy's logprobs changes, all works All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cleanup f054d70 alexm-redhat changed the title [Performance][Core] Optimize Async + Multi-step [Core] Optimize Async + Multi-step Sep 1, 2024 alexm-redhat added 3 commits September 1, 2024 01:38 fix tests 98a55d7 ping 4474b12 Improve asyncio queues append of request outputs 904006a Copy link Collaborator KuntaiDu commented Sep 2, 2024 Nice job Alex! I am rerunning the benchmark using ur PR and thank you for the great work!!! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac approved these changes Sep 3, 2024 View reviewed changes Copy link Collaborator comaniac left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM. Only nits Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/engine/llm_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/llm_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/llm_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/llm_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/llm_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/llm_engine.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/async_llm_engine.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/engine/output_processor/multi_step.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Cody's review comments 3a8726a comaniac enabled auto-merge (squash) September 3, 2024 16:34 More Cody's comments 997c525 auto-merge was automatically disabled September 3, 2024 16:55 Head branch was pushed to by a user without write access comaniac enabled auto-merge (squash) September 3, 2024 17:20 SolitaryThinker approved these changes Sep 3, 2024 View reviewed changes megha95 reviewed Sep 3, 2024 View reviewed changes tests/multi_step/test_correctness_async_llm.py @@ -103,13 +103,13 @@ async def test_multi_step( model, server_args + distributed_args, num_logprobs, max_wait_seconds=3 * 240) Copy link Contributor megha95 Sep 3, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment why was this change needed? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author alexm-redhat Sep 3, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It was increased originally for multi-step tests, but I think it was still sensitive, so I had one instance when I had a timeout. Increasing more did make the test stable. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details comaniac merged commit 6d646d0 into vllm-project : main Sep 3, 2024 39 checks passed Uh oh! There was an error while loading. Please reload this page . Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Core] Optimize Async + Multi-step ( vllm-project#8050 ) â€¦ 4284212 Signed-off-by: Alvant <alvasian@yandex.ru> WhoisZihan reviewed Nov 1, 2024 View reviewed changes vllm/worker/multi_step_model_runner.py @@ -237,14 +265,22 @@ def _async_process_outputs(self, model_input: StatefulModelInput, output_proc_callback: Callable): # Proceed with pythonization and output_proc in order. # Stop on the first one that fails to pythonize output_proc_callback() Copy link WhoisZihan Nov 1, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Why do we need this extra output callback before we call it for each cached output below? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Core] Optimize Async + Multi-step ( vllm-project#8050 ) â€¦ 5f4e3ee Signed-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:57",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, TTFT | SERVING: Serving, Serving | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:57",
  "models": [
    "meta-llama/Llama-3-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3-8B-Instruct --tasks hellaswag --limit 1000"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3-8B-Instruct --dataset ShareGPT_V3_unfiltered_cleaned_split.json",
  "commit_subject": "[Core] Optimize Async + Multi-step (#8050)",
  "commit_message": "[Core] Optimize Async + Multi-step (#8050)",
  "commit_date": "2024-09-03T18:50:29Z",
  "files_changed": [
    "tests/multi_step/test_correctness_async_llm.py",
    "vllm/engine/async_llm_engine.py",
    "vllm/engine/llm_engine.py",
    "vllm/engine/output_processor/multi_step.py",
    "vllm/sequence.py",
    "vllm/worker/model_runner.py",
    "vllm/worker/multi_step_model_runner.py",
    "vllm/worker/multi_step_worker.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 7,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 8,
    "num_hunks": 43,
    "num_edited_lines": 574,
    "num_non_test_edited_lines": 570,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex d054ca341..0cbe8371e 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -103,13 +103,13 @@ async def test_multi_step(\n         model,\n         server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n     test_completions = await completions_with_server_args(\n         prompts,\n         model,\n         ms_server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n \n     # Assert multi-step scheduling produces identical tokens\n     # to single-step scheduling.\ndiff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 159281dab..7fe8053ff 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -280,40 +280,27 @@ class _AsyncLLMEngine(LLMEngine):\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n \n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -351,26 +338,20 @@ class _AsyncLLMEngine(LLMEngine):\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             # Execute the model.\n             output = await self.model_executor.execute_model_async(\n                 execute_model_req)\n+\n             # we need to do this here so that last step's sampled_token_ids can\n             # be passed to the next iteration for PP.\n             if self.scheduler_config.is_multi_step:\n                 self._update_cached_scheduler_output(virtual_engine, output)\n         else:\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             output = []\n \n         # Finish the current step for all the sequence groups.\n@@ -384,24 +365,22 @@ class _AsyncLLMEngine(LLMEngine):\n                 self.cached_scheduler_outputs[\n                     virtual_engine] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(\n-                        output\n-                    ) == 1, \"Multi step decoding does not work with async output processing.\"  # noqa: E501\n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+            if output and allow_async_output_proc:\n+                assert len(\n+                    output\n+                ) == 1, \"Async postprocessor expects only a single output set\"\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -411,17 +390,12 @@ class _AsyncLLMEngine(LLMEngine):\n \n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n         return ctx.request_outputs\n@@ -640,6 +614,17 @@ class AsyncLLMEngine:\n         self.log_requests = log_requests\n         self.engine = self._init_engine(*args, **kwargs)\n \n+        # This ensures quick processing of request outputs\n+        # so the append to asyncio queues is not delayed,\n+        # especially for multi-step.\n+        #\n+        # TODO: Currently, disabled for engine_use_ray, ask\n+        # Cody/Will/Woosuk about this case.\n+        self.use_process_request_outputs_callback = not self.engine_use_ray\n+        if self.use_process_request_outputs_callback:\n+            self.engine.process_request_outputs_callback = \\\n+                self.process_request_outputs\n+\n         if self.engine_use_ray:\n             print_warning_once(\n                 \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n@@ -883,13 +868,27 @@ class AsyncLLMEngine:\n             request_outputs = await self.engine.step_async(virtual_engine)\n \n         # Put the outputs into the corresponding streams.\n-        finished = True\n+        # If used as a callback, then already invoked inside\n+        # LLMEngine's _process_model_outputs\n+        if not self.use_process_request_outputs_callback:\n+            all_finished = self.process_request_outputs(request_outputs)\n+        else:\n+            # For callback case, we only need to detect when all\n+            # requests are finished\n+            all_finished = all(request_output.finished\n+                               for request_output in request_outputs)\n+\n+        return not all_finished\n+\n+    def process_request_outputs(self, request_outputs) -> bool:\n+        # Put the outputs into the corresponding streams.\n+        all_finished = True\n         for request_output in request_outputs:\n             self._request_tracker.process_request_output(\n                 request_output, verbose=self.log_requests)\n-            finished = finished and request_output.finished\n+            all_finished = all_finished and request_output.finished\n \n-        return not finished\n+        return all_finished\n \n     async def _engine_abort(self, request_ids: Iterable[str]):\n         if self.engine_use_ray:\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 1eab83f3b..8c5ca81fb 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -93,13 +93,14 @@ class SchedulerOutputState:\n @dataclass\n class SchedulerContext:\n     output_queue: Deque[Tuple[Optional[List[SamplerOutput]],\n-                              List[SequenceGroupMetadata],\n-                              SchedulerOutputs]] = field(\n-                                  default_factory=lambda: deque())\n-\n+                              List[SequenceGroupMetadata], SchedulerOutputs,\n+                              bool,\n+                              bool]] = field(default_factory=lambda: deque())\n     request_outputs: List[Union[RequestOutput,\n                                 EmbeddingRequestOutput]] = field(\n                                     default_factory=lambda: [])\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n \n class LLMEngine:\n@@ -357,6 +358,26 @@ class LLMEngine:\n             # different process.\n             self.tokenizer.ping()\n \n+        self.cached_scheduler_outputs = [\n+            SchedulerOutputState()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.scheduler_contexts = [\n+            SchedulerContext()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.async_callbacks = [\n+            functools.partial(self._process_model_outputs,\n+                              ctx=self.scheduler_contexts[v_id])\n+            for v_id in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        # Currently used by AsyncLLMEngine to ensure quick append\n+        # of request outputs to asyncio queues\n+        self.process_request_outputs_callback = None\n+\n         # Create the scheduler.\n         # NOTE: the cache_config here have been updated with the numbers of\n         # GPU and CPU blocks, which are profiled in the distributed executor.\n@@ -364,9 +385,7 @@ class LLMEngine:\n             Scheduler(\n                 scheduler_config, cache_config, lora_config,\n                 parallel_config.pipeline_parallel_size,\n-                functools.partial(self._process_model_outputs,\n-                                  virtual_engine=v_id,\n-                                  is_async=True)\n+                self.async_callbacks[v_id]\n                 if model_config.use_async_output_proc else None)\n             for v_id in range(parallel_config.pipeline_parallel_size)\n         ]\n@@ -417,30 +436,6 @@ class LLMEngine:\n                 ),\n             ))\n \n-        self.cached_scheduler_outputs = [\n-            SchedulerOutputState()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.scheduler_contexts = [\n-            SchedulerContext()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=True)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback_multi_step = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=False)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n     def _initialize_kv_caches(self) -> None:\n         \"\"\"Initialize the KV cache in the worker(s).\n \n@@ -1249,11 +1244,7 @@ class LLMEngine:\n \n         return\n \n-    def _process_model_outputs(self,\n-                               virtual_engine: int,\n-                               is_async: bool,\n-                               sampler_output: Optional[SamplerOutput] = None,\n-                               is_last_output: bool = False) -> None:\n+    def _process_model_outputs(self, ctx: SchedulerContext) -> None:\n         \"\"\"Apply the model output to the sequences in the scheduled seq groups.\n \n         virtual_engine: The engine id to operate on\n@@ -1273,24 +1264,12 @@ class LLMEngine:\n         \"\"\"\n         now = time.time()\n \n-        is_multi_step = sampler_output is not None\n-\n-        ctx: SchedulerContext = self.scheduler_contexts[virtual_engine]\n-\n         if len(ctx.output_queue) == 0:\n             return None\n \n-        if is_multi_step:\n-            # Async + multi-step case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue[0]\n-            assert outputs is None\n-            outputs = [sampler_output]\n-        else:\n-            # Async standard case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue.popleft()\n-\n+        # Get pending async postprocessor\n+        (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n+         is_last_step) = ctx.output_queue.popleft()\n         assert outputs is not None\n \n         # Sanity check\n@@ -1306,6 +1285,7 @@ class LLMEngine:\n             outputs_by_sequence_group = outputs\n \n         finished_before: List[int] = []\n+        finished_now: List[int] = []\n         for i, seq_group_meta in enumerate(seq_group_metadata_list):\n             scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n@@ -1343,26 +1323,44 @@ class LLMEngine:\n \n             if self.model_config.embedding_mode:\n                 self._process_sequence_group_outputs(seq_group, output)\n-                continue\n+            else:\n+                self.output_processor.process_prompt_logprob(seq_group, output)\n+                if seq_group_meta.do_sample:\n+                    self.output_processor.process_outputs(\n+                        seq_group, output, is_async)\n \n-            self.output_processor.process_prompt_logprob(seq_group, output)\n-            if seq_group_meta.do_sample:\n-                self.output_processor.process_outputs(seq_group, output,\n-                                                      is_async)\n+            if seq_group.is_finished():\n+                finished_now.append(i)\n \n-        # For async + multi-step, free finished seqs and create outputs\n-        # only on the final step.\n-        if is_multi_step and not is_last_output:\n-            return\n+        # Generate outputs for the requests that finished this iteration\n+        for i in finished_now:\n+            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n-        for scheduler in self.scheduler:\n-            scheduler.free_finished_seq_groups()\n+            seq_group = scheduled_seq_group.seq_group\n+            seq_group.maybe_set_first_token_time(now)\n+            request_output = RequestOutputFactory.create(seq_group)\n+            ctx.request_outputs.append(request_output)\n \n-        # Create the outputs.\n-        for i, _ in enumerate(seq_group_metadata_list):\n-            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n+        # Free currently finished requests\n+        if finished_now:\n+            for scheduler in self.scheduler:\n+                scheduler.free_finished_seq_groups()\n+\n+        # For multi-step, do not create outputs each iteration\n+        if not is_last_step:\n+            # Immediately process request outputs here (if callback is given)\n+            if (finished_now\n+                    and self.process_request_outputs_callback is not None):\n+                self.process_request_outputs_callback(ctx.request_outputs)\n+            return\n+\n+        # Create the outputs\n+        # Note: scheduled_seq_groups and seq_group_metadata_list\n+        # must match with the indices\n+        for i, scheduled_seq_group in enumerate(\n+                scheduler_outputs.scheduled_seq_groups):\n \n-            if not is_multi_step and i in finished_before:\n+            if i in finished_before or i in finished_now:\n                 continue  # Avoids double processing\n \n             seq_group = scheduled_seq_group.seq_group\n@@ -1376,11 +1374,15 @@ class LLMEngine:\n             request_output = RequestOutputFactory.create(seq_group)\n             ctx.request_outputs.append(request_output)\n \n-        # For async + multi-step, do stats only on the last output.\n-        # Otherwise, do stats if the execution is async\n-        do_stats = is_multi_step or is_async\n+        # Immediately process request outputs here (if callback is given)\n+        if (ctx.request_outputs\n+                and self.process_request_outputs_callback is not None):\n+            self.process_request_outputs_callback(ctx.request_outputs)\n \n-        if do_stats:\n+        # For async case, we need to record the stats here.\n+        # For non-async case, the stats are done in the\n+        # LLMEngine/AsyncLLMEngine directly\n+        if is_async:\n             # Log stats.\n             self.do_log_stats(scheduler_outputs, outputs, finished_before)\n \n@@ -1485,40 +1487,26 @@ class LLMEngine:\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # Skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n-\n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -1555,13 +1543,8 @@ class LLMEngine:\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             output = self.model_executor.execute_model(\n                 execute_model_req=execute_model_req)\n@@ -1573,10 +1556,8 @@ class LLMEngine:\n         else:\n             # Nothing scheduled => If there is pending async postprocessor,\n             # then finish it here.\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             # No outputs in this case\n             output = []\n \n@@ -1590,28 +1571,24 @@ class LLMEngine:\n             if self.scheduler_config.is_multi_step:\n                 self.cached_scheduler_outputs[0] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                # Add results to the output_queue\n-                # (for async or non-async postprocessing)\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            # Add results to the output_queue\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(output) == 1, (\n-                        \"Multi step decoding does not work \"\n-                        \"with async output processing.\")\n+            if output and allow_async_output_proc:\n+                assert len(output) == 1, (\n+                    \"Async postprocessor expects only a single output set\")\n \n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             # Check if need to run the usual non-async path\n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -1620,17 +1597,12 @@ class LLMEngine:\n                 self.do_tracing(scheduler_outputs)\n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n             # Stop the execute model loop in parallel workers until there are\ndiff --git a/vllm/engine/output_processor/multi_step.py b/vllm/engine/output_processor/multi_step.py\nindex e182cee8b..c73db765f 100644\n--- a/vllm/engine/output_processor/multi_step.py\n+++ b/vllm/engine/output_processor/multi_step.py\n@@ -85,9 +85,6 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             no tokens need to be appended since it is already done\n             externally (before the next schedule() call)\n         \"\"\"\n-        # TODO: Add support for async if necessary\n-        assert not is_async\n-\n         # Sequences can be in RUNNING or FINISHED_ABORTED state\n         # once scheduled, as a sequence is moved to FINSIHED_ABORTED\n         # if a client disconnects from the api server.\n@@ -101,19 +98,41 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             \"Beam search not supported in multi-step decoding.\")\n         seq = seqs[0]\n \n-        # Since there's only one sequence per sequence group, we can take the\n-        # first sample.\n-        samples = [output.samples[0] for output in outputs]\n-\n-        # -1 means the output token is not valid (eg. due to spec decode\n-        # rejecting tokens).\n-        valid_samples = [\n-            sample for sample in samples if sample.output_token != -1\n-        ]\n-        assert valid_samples\n-\n-        self._process_seq_outputs(seq, valid_samples,\n-                                  sequence_group.sampling_params)\n+        if is_async:\n+            # Async case: We process tokens one by one. Here, we know the token\n+            # was already appended, so we only need to do the rest of the\n+            # postprocessor: Detokenization + stopping logic\n+            self._process_decode_and_stop(seq, sequence_group.sampling_params)\n+        else:\n+            # Standard multi-step case\n+\n+            # Since there's only one sequence per sequence group,\n+            # we can take the first sample.\n+            samples = [output.samples[0] for output in outputs]\n+\n+            # -1 means the output token is not valid (eg. due to spec decode\n+            # rejecting tokens).\n+            valid_samples = [\n+                sample for sample in samples if sample.output_token != -1\n+            ]\n+            assert valid_samples\n+\n+            self._process_seq_outputs(seq, valid_samples,\n+                                      sequence_group.sampling_params)\n+\n+    def _process_decode_and_stop(self, seq: Sequence,\n+                                 sampling_params: SamplingParams) -> None:\n+        new_char_count = 0\n+        if sampling_params.detokenize:\n+            new_char_count = self.detokenizer.decode_sequence_inplace(\n+                seq, sampling_params)\n+\n+        # TODO(sang): Support lora.\n+        self.stop_checker.maybe_stop_sequence(\n+            seq,\n+            new_char_count=new_char_count,\n+            sampling_params=sampling_params,\n+        )\n \n     def _process_seq_outputs(self, seq: Sequence,\n                              valid_samples: List[SequenceOutput],\n@@ -151,16 +170,7 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n                 logprobs=output_logprob,\n             )\n \n-            new_char_count = 0\n-            if sampling_params.detokenize:\n-                new_char_count = self.detokenizer.decode_sequence_inplace(\n-                    seq, sampling_params)\n+            self._process_decode_and_stop(seq, sampling_params)\n \n-            # TODO(sang): Support lora.\n-            self.stop_checker.maybe_stop_sequence(\n-                seq,\n-                new_char_count=new_char_count,\n-                sampling_params=sampling_params,\n-            )\n             if seq.is_finished():\n                 break\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 87b3d21fa..a5ebf152c 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1225,7 +1225,6 @@ class ExecuteModelRequest(\n     last_sampled_token_ids: Optional[torch.Tensor] = None\n     # Async callback\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n \n     @property\n     def is_first_multi_step(self) -> bool:\n@@ -1272,5 +1271,4 @@ class ExecuteModelRequest(\n             finished_requests_ids=self.finished_requests_ids,\n             last_sampled_token_ids=self.last_sampled_token_ids.clone()\n             if self.last_sampled_token_ids is not None else None,\n-            async_callback=self.async_callback,\n-            use_async_and_multi_step=self.use_async_and_multi_step)\n+            async_callback=self.async_callback)\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 8a3c99a45..74f7d4e08 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -21,6 +21,7 @@ from vllm.attention.backends.utils import CommonAttentionState\n from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                          ModelConfig, ObservabilityConfig, ParallelConfig,\n                          PromptAdapterConfig, SchedulerConfig)\n+from vllm.core.scheduler import SchedulerOutputs\n from vllm.distributed import get_pp_group\n from vllm.distributed.parallel_state import graph_capture\n from vllm.inputs import INPUT_REGISTRY, InputRegistry\n@@ -96,7 +97,8 @@ class ModelInputForGPU(ModelRunnerInputBase):\n     finished_requests_ids: Optional[List[str]] = None\n     virtual_engine: int = 0\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n         tensor_dict = {\ndiff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex be0c75bc0..b52f2a07e 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -22,6 +22,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n+from vllm.utils import PyObjectCache\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -37,6 +38,29 @@ if TYPE_CHECKING:\n logger = init_logger(__name__)\n \n \n+def seq_output_builder():\n+    return SequenceOutput(\n+        0, 0,\n+        {0: Logprob(logprob=float('inf'), rank=None, decoded_token=None)})\n+\n+\n+def completion_seq_group_output_builder():\n+    return CompletionSequenceGroupOutput([], None)\n+\n+\n+# Used by pythonization to reduce python object allocations\n+class PythonizationCache:\n+\n+    def __init__(self):\n+        self.cached_seq_output = PyObjectCache(seq_output_builder)\n+        self.cached_completion_seq_group_output = PyObjectCache(\n+            completion_seq_group_output_builder)\n+\n+    def reset(self):\n+        self.cached_seq_output.reset()\n+        self.cached_completion_seq_group_output.reset()\n+\n+\n @dataclass\n class ModelOutput:\n     \"\"\"The output of a single model forward pass.\n@@ -59,6 +83,7 @@ class ModelOutput:\n     pythonized: bool = False\n     # On-device tensor containing the logprobs of each token.\n     logprobs: Optional[\"torch.Tensor\"] = None\n+    pythonization_cache: Optional[PythonizationCache] = None\n \n     def pythonize(self, input_metadata: \"StatefulModelInput\",\n                   copy_stream: torch.cuda.Stream,\n@@ -97,7 +122,8 @@ class ModelOutput:\n         with torch.cuda.stream(copy_stream):\n             _pythonize_sampler_output(input_metadata, self.sampler_output,\n                                       pinned_sampled_token_buffer,\n-                                      self.sampled_token_ids, self.logprobs)\n+                                      self.sampled_token_ids, self.logprobs,\n+                                      self.pythonization_cache)\n \n         # Erase the logprobs GPU-side tensor.\n         # Note that although _pythonize_sampler_output() runs in its\n@@ -209,6 +235,8 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         self._copy_stream = torch.cuda.Stream()\n         self.pinned_sampled_token_ids: Optional[torch.Tensor] = None\n \n+        self.pythonization_cache = PythonizationCache()\n+\n     def make_model_input_from_broadcasted_tensor_dict(\n             self, tensor_dict: Dict[str, Any]) -> StatefulModelInput:\n         model_input = (StatefulModelInput.from_broadcasted_tensor_dict(\n@@ -237,14 +265,22 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Callable):\n         # Proceed with pythonization and output_proc in order.\n         # Stop on the first one that fails to pythonize\n+        output_proc_callback()\n+\n         cont = True\n         for model_output in model_input.cached_outputs:\n             if not model_output.pythonized:\n                 model_output.maybe_pythonize(model_input, self._copy_stream,\n                                              self.pinned_sampled_token_ids)\n                 if model_output.pythonized:\n-                    output_proc_callback(\n-                        sampler_output=model_output.sampler_output)\n+                    ctx = output_proc_callback.keywords[\"ctx\"]\n+                    is_async = False\n+                    is_last_step = False\n+                    ctx.output_queue.append(\n+                        ([model_output.sampler_output\n+                          ], ctx.seq_group_metadata_list,\n+                         ctx.scheduler_outputs, is_async, is_last_step))\n+                    output_proc_callback()\n                 else:\n                     cont = False\n \n@@ -255,21 +291,46 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Optional[Callable]):\n         assert model_input.frozen_model_input is not None\n \n+        has_async_callback = output_proc_callback is not None\n+\n         outputs = []\n         for output_id in range(len(model_input.cached_outputs)):\n-            is_last_output = output_id == len(model_input.cached_outputs) - 1\n-\n             output = model_input.cached_outputs[output_id]\n-            if not output.pythonized:\n+            is_last_step = output_id == len(model_input.cached_outputs) - 1\n+\n+            # For non-async case:\n+            #   -- We simply add the outputs\n+            # For async case:\n+            #   -- Invoke callback, pythonize, add to callback queue and repeat\n+            #   -- For last output, just add to callback queue\n+            if has_async_callback:\n+                assert output_proc_callback is not None\n+\n+                # Invoke callback before pythonize (to overlap with GPU)\n+                output_proc_callback()\n+\n+                # Pythonize\n+                if not output.pythonized:\n+                    output.pythonize(model_input, self._copy_stream,\n+                                     self.pinned_sampled_token_ids)\n+\n+                    # For non last step, add to callback queue to chain\n+                    # callbacks=>pythonize pairs (for GPU overlap)\n+                    if not is_last_step:\n+                        ctx = output_proc_callback.keywords[  # type: ignore\n+                            \"ctx\"]  # type: ignore\n+                        is_async = False\n+                        is_last_step = False\n+                        ctx.output_queue.append(\n+                            ([output.sampler_output\n+                              ], ctx.seq_group_metadata_list,\n+                             ctx.scheduler_outputs, is_async, is_last_step))\n+                    else:\n+                        outputs.append(output.sampler_output)\n+            else:\n                 output.pythonize(model_input, self._copy_stream,\n                                  self.pinned_sampled_token_ids)\n-\n-                if model_input.frozen_model_input.use_async_and_multi_step:\n-                    assert output_proc_callback is not None\n-                    output_proc_callback(sampler_output=output.sampler_output,\n-                                         is_last_output=is_last_output)\n-\n-            outputs.append(output.sampler_output)\n+                outputs.append(output.sampler_output)\n \n         return outputs\n \n@@ -330,7 +391,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                 model_input, model_input.cached_outputs[-1].sampler_output)\n \n         output_proc_callback = None\n-        if frozen_model_input.use_async_and_multi_step:\n+        if frozen_model_input.async_callback is not None:\n             output_proc_callback = frozen_model_input.async_callback\n             assert output_proc_callback is not None\n             async_callback = functools.partial(\n@@ -367,7 +428,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             model_input.cached_outputs.append(\n                 ModelOutput(output[0], output_ready_event,\n                             output[0].sampled_token_ids, False,\n-                            output[0].logprobs))\n+                            output[0].logprobs, self.pythonization_cache))\n \n             # These GPU tensors are not required by multi-step;\n             # erase them to ensure they are not pythonized or\n@@ -378,7 +439,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n \n             # Pythonize the output if CPU is ahead and the previous step is\n             # ready.\n-            if not frozen_model_input.use_async_and_multi_step:\n+            if frozen_model_input.async_callback is None:\n                 for model_output in model_input.cached_outputs:\n                     model_output.maybe_pythonize(model_input,\n                                                  self._copy_stream,\n@@ -397,6 +458,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         if model_input.is_last_step:\n             outputs = self._final_process_outputs(model_input,\n                                                   output_proc_callback)\n+            self.pythonization_cache.reset()\n             return outputs\n \n         # should be [SamplerOutput]\n@@ -537,6 +599,7 @@ def _pythonize_sampler_output(\n     pinned_sampled_token_buffer: torch.Tensor,\n     sampled_token_ids: torch.Tensor,\n     logprobs_tensor: Optional[torch.Tensor],\n+    cache: Optional[PythonizationCache],\n ) -> None:\n     \"\"\" This function is only called when the output tensors are ready. \n     See :class:`ModelOutput`. \n@@ -597,6 +660,9 @@ def _pythonize_sampler_output(\n \n     for sgdx, (seq_group,\n                sample_result) in enumerate(zip(seq_groups, samples_list)):\n+        if seq_group.sampling_params.logits_processors:\n+            assert len(seq_group.sampling_params.logits_processors) == 0, (\n+                \"Logits Processors are not supported in multi-step decoding\")\n \n         if do_pythonize_logprobs:\n             assert prompt_logprobs is not None\n@@ -621,23 +687,56 @@ def _pythonize_sampler_output(\n         seq_ids = seq_group.seq_ids\n         next_token_ids = sample_result\n         parent_ids = [0]\n-        seq_outputs: List[SequenceOutput] = []\n-        if seq_group.sampling_params.logits_processors:\n-            assert len(seq_group.sampling_params.logits_processors) == 0, (\n-                \"Logits Processors are not supported in multi-step decoding\")\n+\n+        if cache is not None:\n+            completion_seq_group_output: CompletionSequenceGroupOutput = \\\n+                cache.cached_completion_seq_group_output.get_object()\n+            completion_seq_group_output.samples.clear()\n+            seq_outputs: List[\n+                SequenceOutput] = completion_seq_group_output.samples\n+        else:\n+            seq_outputs = []\n+\n         for tdx, (parent_id,\n                   next_token_id) in enumerate(zip(parent_ids, next_token_ids)):\n-            seq_outputs.append(\n-                SequenceOutput(seq_ids[parent_id], next_token_id,\n-                               (group_sample_logprobs[tdx]\n-                                if logprobs_are_requested else {\n-                                    next_token_id:\n-                                    Logprob(logprob=float('inf'),\n-                                            rank=None,\n-                                            decoded_token=None)\n-                                })))\n-        output.outputs.append(\n-            CompletionSequenceGroupOutput(\n-                seq_outputs,\n-                (group_prompt_logprobs if logprobs_are_requested else None)))\n+            if cache is not None:\n+                seq_output: SequenceOutput = cache.cached_seq_output.get_object(\n+                )\n+                seq_output.parent_seq_id = seq_ids[parent_id]\n+                seq_output.output_token = next_token_id\n+\n+                if logprobs_are_requested:\n+                    seq_output.logprobs = group_sample_logprobs[tdx]\n+                else:\n+                    logprobs = next(iter(seq_output.logprobs.values()))\n+                    seq_output.logprobs.clear()\n+\n+                    logprobs.logprob = float('inf')\n+                    logprobs.rank = None\n+                    logprobs.decoded_token = None\n+\n+                    seq_output.logprobs[next_token_id] = logprobs\n+\n+                seq_outputs.append(seq_output)\n+\n+            else:\n+                seq_outputs.append(\n+                    SequenceOutput(seq_ids[parent_id], next_token_id,\n+                                   (group_sample_logprobs[tdx]\n+                                    if logprobs_are_requested else {\n+                                        next_token_id:\n+                                        Logprob(logprob=float('inf'),\n+                                                rank=None,\n+                                                decoded_token=None)\n+                                    })))\n+        if cache is not None:\n+            completion_seq_group_output.prompt_logprobs = \\\n+                group_prompt_logprobs if logprobs_are_requested else None\n+            output.outputs.append(completion_seq_group_output)\n+        else:\n+            output.outputs.append(\n+                CompletionSequenceGroupOutput(\n+                    seq_outputs, (group_prompt_logprobs\n+                                  if logprobs_are_requested else None)))\n+\n     assert len(output.outputs) > 0\ndiff --git a/vllm/worker/multi_step_worker.py b/vllm/worker/multi_step_worker.py\nindex 517b0ab78..562285f82 100644\n--- a/vllm/worker/multi_step_worker.py\n+++ b/vllm/worker/multi_step_worker.py\n@@ -67,9 +67,7 @@ class MultiStepWorker(Worker):\n             if execute_model_req.async_callback:\n                 model_input.frozen_model_input = dataclasses.replace(  # type: ignore\n                     model_input.frozen_model_input,\n-                    async_callback=execute_model_req.async_callback,\n-                    use_async_and_multi_step=execute_model_req.\n-                    use_async_and_multi_step)\n+                    async_callback=execute_model_req.async_callback)\n         else:\n             # on subsequent steps we reuse the worker input and model input\n             multi_step_state = self.multi_step_states[virtual_engine]",
  "apis": [
    "vllm.AsyncLLMEngine.generate",
    "vllm.LLMEngine.step",
    "vllm.MultiStepModelRunner.execute_model"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/async_llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/multi_step_model_runner.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies multiple non-test source files in the core engine, worker, and async processing modules of the project. It refactors the handling of async and multi-step decoding, clears context outputs earlier, introduces a Pythonization cache to reduce object allocations, and ensures quicker processing of appended outputs (for example, by optimizing callback chaining and queue management). These changes are implemented to reduce latency and overhead in the asynchronous scheduling and output processing path, thus improving overall CPU performance of high-level APIs. Although the commit message mentions â€œOptimize Async + Multi-step,â€ the changes are not mere refactoring or bug fixes but are specific improvements aimed at reducing delays and enhancing throughput. Therefore, it satisfies the conditions for a performance/optimization commit.",
  "llm_api_reason": "The commit adjusts several core modules to optimize the async multiâ€step decoding flow. In particular, changes were made in the asynchronous engine implementation (in async_llm_engine.py) to simplify how the asynchronous callbacks and queue processing are handled (for example, removing the separate â€œuse_async_and_multi_stepâ€ flag and reworking the outputâ€queue processing). Similar modifications were made in LLMEngine and its scheduler context, as well as in the workerâ€™s multiâ€step model runner where a PythonizationCache was added to reduce object allocations. These changes affect the internal processing flow when high-level API methods are invoked. For users of vLLM the exposed public APIs â€“ such as the asynchronous generation endpoint and the engine step function â€“ now benefit from these optimizations."
}