{
  "commit_hash": "0ec82edda59aaf5cf3b07aadf4ecce1aa1131add",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21079",
  "pr_date": null,
  "timeline_text": "Copy link Contributor hj-mistral commented Jul 16, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Purpose Move fill ops inside align sum kernel to reduce bubbles. cumsum buffer does not need to be filled with zero. we can use blockscan to do the prefix sum This PR also moves the triton inits into the kernel to make it a fair comparison and also ensure the kernel is usable in the future as a fallback if required. Benchmarks Main branch FP16:\n# vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100\n\nThroughput: 43.75 requests/s, 48024.34 total tokens/s, 4374.91 output tokens/s\nTotal num prompt tokens:  997723\nTotal num output tokens:  100000\n\nFP8:\n# vllm bench throughput --model Qwen/Qwen3-30B-A3B-FP8 --load-format dummy --input-len 1000 --output-len 100\n\nThroughput: 41.04 requests/s, 45049.17 total tokens/s, 4103.87 output tokens/s\nTotal num prompt tokens:  997723\nTotal num output tokens:  100000\n\nKernel benchmark:\n# python3 benchmarks/kernels/benchmark_moe_align_block_size.py\n\nRunning correctness check...\n‚úÖ VLLM implementation works with 64 experts!\n‚úÖ Triton and VLLM implementations match.\nmoe-align-block-size-performance:\n    num_tokens  num_experts  topk       VLLM      Triton\n0          1.0         16.0   1.0  16.448000   23.040000\n1          1.0         16.0   2.0  16.432000   23.104001\n2          1.0         16.0   8.0  16.448000   23.040000\n3          1.0         64.0   1.0  21.600001   25.984000\n4          1.0         64.0   2.0  21.792000   26.048001\n5          1.0         64.0   8.0  21.824000   25.952000\n6          1.0        224.0   1.0  23.680000   40.288001\n7          1.0        224.0   2.0  23.680000   40.320002\n8          1.0        224.0   8.0  23.712000   40.383998\n9          1.0        256.0   1.0  24.607999   43.136001\n10         1.0        256.0   2.0  24.639999   43.104000\n11         1.0        256.0   8.0  24.639999   43.200001\n12         1.0        280.0   1.0  25.248000   45.407999\n13         1.0        280.0   2.0  25.248000   45.343999\n14         1.0        280.0   8.0  25.248000   45.440000\n15         1.0        512.0   1.0  31.136001   69.151998\n16         1.0        512.0   2.0  31.328000   69.119997\n17         1.0        512.0   8.0  31.296000   69.215998\n18        16.0         16.0   1.0  16.511999   23.296000\n19        16.0         16.0   2.0  16.608000   23.520000\n20        16.0         16.0   8.0  17.856000   24.351999\n21        16.0         64.0   1.0  21.792000   26.400000\n22        16.0         64.0   2.0  21.792000   26.656000\n23        16.0         64.0   8.0  22.143999   27.424000\n24        16.0        224.0   1.0  23.871999   41.503999\n25        16.0        224.0   2.0  23.903999   41.600000\n26        16.0        224.0   8.0  24.032000   41.152000\n27        16.0        256.0   1.0  24.768000   43.088000\n28        16.0        256.0   2.0  24.831999   43.136001\n29        16.0        256.0   8.0  24.928000   43.391999\n30        16.0        280.0   1.0  25.152000   45.968000\n31        16.0        280.0   2.0  25.184000   46.080001\n32        16.0        280.0   8.0  25.343999   46.271998\n33        16.0        512.0   1.0  31.264000   69.343999\n34        16.0        512.0   2.0  31.328000   69.504000\n35        16.0        512.0   8.0  31.456001   69.888003\n36       256.0         16.0   1.0  19.200001   25.312001\n37       256.0         16.0   2.0  22.624001   28.576000\n38       256.0         16.0   8.0  18.528000   45.184001\n39       256.0         64.0   1.0  23.104001   28.416000\n40       256.0         64.0   2.0  24.831999   29.023999\n41       256.0         64.0   8.0  20.256000   33.535998\n42       256.0        224.0   1.0  24.256000   42.367999\n43       256.0        224.0   2.0  24.000000   42.943999\n44       256.0        224.0   8.0  24.256000   45.952000\n45       256.0        256.0   1.0  25.119999   44.224001\n46       256.0        256.0   2.0  24.960000   44.192001\n47       256.0        256.0   8.0  25.984000   47.488000\n48       256.0        280.0   1.0  25.312001   46.239998\n49       256.0        280.0   2.0  25.536001   47.327999\n50       256.0        280.0   8.0  26.432000   49.568001\n51       256.0        512.0   1.0  31.488001   69.824003\n52       256.0        512.0   2.0  31.392001   69.856003\n53       256.0        512.0   8.0  32.671999   71.712002\n54      4096.0         16.0   1.0  20.128001   68.896003\n55      4096.0         16.0   2.0  22.720000  114.367999\n56      4096.0         16.0   8.0  36.256000  378.015995\n57      4096.0         64.0   1.0  21.856001   39.391998\n58      4096.0         64.0   2.0  24.639999   51.872000\n59      4096.0         64.0   8.0  41.216001  121.360000\n60      4096.0        224.0   1.0  26.368000   50.976001\n61      4096.0        224.0   2.0  29.023999   56.607999\n62      4096.0        224.0   8.0  45.504000   78.304000\n63      4096.0        256.0   1.0  27.071999   51.968001\n64      4096.0        256.0   2.0  29.824000   58.944002\n65      4096.0        256.0   8.0  45.568001   78.368001\n66      4096.0        280.0   1.0  27.295999   53.056002\n67      4096.0        280.0   2.0  30.272000   59.648000\n68      4096.0        280.0   8.0  43.264002   80.095999\n69      4096.0        512.0   1.0  33.824001   73.600002\n70      4096.0        512.0   2.0  35.551999   77.776000\n71      4096.0        512.0   8.0  49.024001   98.591998 This PR FP16:\n#vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100\n\nThroughput: 43.94 requests/s, 48234.94 total tokens/s, 4394.09 output tokens/s\nTotal num prompt tokens:  997723\nTotal num output tokens:  100000\n\nFP8:\n#vllm bench throughput --model Qwen/Qwen3-30B-A3B-FP8 --load-format dummy --input-len 1000 --output-len 100\n\nThroughput: 41.26 requests/s, 45294.95 total tokens/s, 4126.26 output tokens/s\nTotal num prompt tokens:  997723\nTotal num output tokens:  100000\n\nKernel benchmark:\n# python3 benchmarks/kernels/benchmark_moe_align_block_size.py\n\nRunning correctness check...\n‚úÖ VLLM implementation works with 64 experts!\n‚úÖ Triton and VLLM implementations match.\nmoe-align-block-size-performance:\n    num_tokens  num_experts  topk       VLLM      Triton\n0          1.0         16.0   1.0  17.472001   27.488001\n1          1.0         16.0   2.0  17.600000   30.304000\n2          1.0         16.0   8.0  17.696001   30.880000\n3          1.0         64.0   1.0  25.760001   31.296000\n4          1.0         64.0   2.0  25.855999   31.168001\n5          1.0         64.0   8.0  25.823999   31.488001\n6          1.0        224.0   1.0  21.536000   44.544000\n7          1.0        224.0   2.0  21.344000   44.799998\n8          1.0        224.0   8.0  21.407999   44.736002\n9          1.0        256.0   1.0  22.080000   47.616001\n10         1.0        256.0   2.0  21.568000   47.392000\n11         1.0        256.0   8.0  21.760000   47.711998\n12         1.0        280.0   1.0  21.952000   49.632002\n13         1.0        280.0   2.0  22.336001   49.984001\n14         1.0        280.0   8.0  22.048000   49.952000\n15         1.0        512.0   1.0  25.888000   75.071998\n16         1.0        512.0   2.0  25.952000   75.328000\n17         1.0        512.0   8.0  25.952000   75.007997\n18        16.0         16.0   1.0  17.600000   27.295999\n19        16.0         16.0   2.0  17.600000   28.352000\n20        16.0         16.0   8.0  18.912001   29.696001\n21        16.0         64.0   1.0  25.696000   31.184000\n22        16.0         64.0   2.0  25.632000   30.688001\n23        16.0         64.0   8.0  25.952000   30.944001\n24        16.0        224.0   1.0  21.312000   45.855999\n25        16.0        224.0   2.0  21.183999   45.791999\n26        16.0        224.0   8.0  21.536000   45.440000\n27        16.0        256.0   1.0  21.792000   47.359999\n28        16.0        256.0   2.0  21.760000   47.584001\n29        16.0        256.0   8.0  21.760000   47.807999\n30        16.0        280.0   1.0  22.048000   50.271999\n31        16.0        280.0   2.0  21.888001   50.464001\n32        16.0        280.0   8.0  22.336001   50.624002\n33        16.0        512.0   1.0  25.664000   74.975997\n34        16.0        512.0   2.0  25.696000   75.039998\n35        16.0        512.0   8.0  25.952000   75.135998\n36       256.0         16.0   1.0  20.320000   29.088000\n37       256.0         16.0   2.0  23.871999   32.543998\n38       256.0         16.0   8.0  17.600000   49.279999\n39       256.0         64.0   1.0  26.784001   32.448001\n40       256.0         64.0   2.0  28.384000   32.127999\n41       256.0         64.0   8.0  18.912001   37.535999\n42       256.0        224.0   1.0  21.536000   46.720002\n43       256.0        224.0   2.0  21.695999   47.488000\n44       256.0        224.0   8.0  21.856001   50.175998\n45       256.0        256.0   1.0  22.336001   48.703998\n46       256.0        256.0   2.0  21.952000   48.351999\n47       256.0        256.0   8.0  23.072001   51.711999\n48       256.0        280.0   1.0  22.240000   50.783999\n49       256.0        280.0   2.0  22.752000   52.000001\n50       256.0        280.0   8.0  23.808001   54.639999\n51       256.0        512.0   1.0  26.208000   75.744003\n52       256.0        512.0   2.0  26.335999   75.103998\n53       256.0        512.0   8.0  26.656000   77.215999\n54      4096.0         16.0   1.0  19.168001   72.672002\n55      4096.0         16.0   2.0  22.112001  117.183998\n56      4096.0         16.0   8.0  37.087999  382.703990\n57      4096.0         64.0   1.0  20.352000   43.423999\n58      4096.0         64.0   2.0  23.424000   55.712000\n59      4096.0         64.0   8.0  42.016000  125.568002\n60      4096.0        224.0   1.0  23.264000   55.744000\n61      4096.0        224.0   2.0  26.912000   60.864002\n62      4096.0        224.0   8.0  44.704001   81.919998\n63      4096.0        256.0   1.0  24.383999   56.448001\n64      4096.0        256.0   2.0  27.327999   63.104004\n65      4096.0        256.0   8.0  44.319998   82.496002\n66      4096.0        280.0   1.0  23.808001   57.824001\n67      4096.0        280.0   2.0  27.424000   64.576000\n68      4096.0        280.0   8.0  41.792002   83.967999\n69      4096.0        512.0   1.0  27.744001   79.135999\n70      4096.0        512.0   2.0  30.479999   83.328001\n71      4096.0        512.0   8.0  45.536000  103.808001 Test Result pytest tests/kernels/moe/test_moe_align_block_size.py - PASSED (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 2 mgoin and xuanyu-mistral reacted with heart emoji All reactions ‚ù§Ô∏è 2 reactions Copy link github-actions bot commented Jul 16, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hj-mistral force-pushed the hj-align-kernel branch\n    from 2f3cc21 to 67295ab Compare July 16, 2025 22:08 gemini-code-assist bot reviewed Jul 16, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request aims to speed up MoE alignment kernels by replacing a sequential prefix sum with a parallel version using cub::BlockScan and by moving some tensor initializations from Python into the CUDA kernel to reduce kernel launch overhead. While these changes are effective for performance, I've identified a critical correctness issue in the new parallel prefix sum implementation. It does not correctly handle cases where the number of experts exceeds the number of threads in the CUDA block (1024), which would lead to incorrect calculations. The existing tests do not cover this scenario. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions csrc/moe/moe_align_sum_kernels.cu Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Member mgoin commented Jul 16, 2025 cc @yewentao256 üëç 1 yewentao256 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . yewentao256 reviewed Jul 17, 2025 View reviewed changes Copy link Collaborator yewentao256 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the work! Could you also please benchmark the performance (E2E throughput + kernel latency) and make sure all unit test passes? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/fused_moe/moe_align_block_size.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author hj-mistral commented Jul 17, 2025 Thanks for the work! Could you also please benchmark the performance (E2E throughput + kernel latency) and make sure all unit test passes? Any documentation to follow on how to run both? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hj-mistral added 3 commits July 17, 2025 12:43 Speed up align sum kernels ‚Ä¶ 3cd55fd Signed-off-by: Himanshu Jaju <hj@mistral.ai> assert num_exp < 1024 ‚Ä¶ f6ef4eb Signed-off-by: Himanshu Jaju <hj@mistral.ai> whitespace ‚Ä¶ c898aab Signed-off-by: Himanshu Jaju <hj@mistral.ai> hj-mistral force-pushed the hj-align-kernel branch\n    from b5ee67e to c898aab Compare July 17, 2025 12:43 Copy link Collaborator yewentao256 commented Jul 17, 2025 Any documentation to follow on how to run both? Throughput(fp16)\n\nvllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100\n\nThroughput(fp8)\n\nvllm bench throughput --model Qwen/Qwen3-30B-A3B-FP8 --load-format dummy --input-len 1000 --output-len 100 vllm-source/benchmarks/kernels/benchmark_moe_align_block_size.py vllm-source/tests/kernels/moe/test_moe_align_block_size.py üëç 1 hj-mistral reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the performance Performance-related issues label Jul 18, 2025 Some changes ‚Ä¶ be95db1 Signed-off-by: Himanshu Jaju <hj@mistral.ai> hj-mistral force-pushed the hj-align-kernel branch\n    from a8140c6 to be95db1 Compare July 18, 2025 16:11 Copy link Contributor Author hj-mistral commented Jul 18, 2025 Any documentation to follow on how to run both? Throughput(fp16)\n\nvllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100\n\nThroughput(fp8)\n\nvllm bench throughput --model Qwen/Qwen3-30B-A3B-FP8 --load-format dummy --input-len 1000 --output-len 100 vllm-source/benchmarks/kernels/benchmark_moe_align_block_size.py vllm-source/tests/kernels/moe/test_moe_align_block_size.py All done and added to description, ptal :) All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hj-mistral commented Jul 18, 2025 View reviewed changes csrc/moe/moe_align_sum_kernels.cu int expert_offset = (i - 1) % experts_per_warp; expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset]; // Compute prefix sum over token counts per expert using BlockScan = cub::BlockScan<int32_t, 1024>; Copy link Contributor Author hj-mistral Jul 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment For reviewer: this is what helps this kernel become faster even though its doing more ops now. Unsure how to do this for the small_kernel, but if there's a way we can do this as a follow up PR :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions hj-mistral marked this pull request as ready for review July 18, 2025 17:00 hj-mistral requested review from tlrmchlsmth and WoosukKwon as code owners July 18, 2025 17:00 hj-mistral changed the title [wip] Speed up align sum kernels [perf] Speed up align sum kernels Jul 18, 2025 yewentao256 approved these changes Jul 18, 2025 View reviewed changes Copy link Collaborator yewentao256 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looks good to me, thanks for the work! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 hj-mistral reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Copy link mergify bot commented Jul 19, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @hj-mistral . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jul 19, 2025 mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 19, 2025 mergify bot removed\n  the needs-rebase label Jul 19, 2025 hj-mistral force-pushed the hj-align-kernel branch\n    from 623f56f to 86466d7 Compare July 19, 2025 13:48 hj-mistral requested review from hmellor , jeejeelee , DarkLight1337 and ywang96 as code owners July 19, 2025 13:48 44 hidden items Load more‚Ä¶ mgoin added moe and removed speculative-decoding ci/build v1 multi-modality Related to multi-modality (#4194) tool-calling llama Related to Llama models qwen Related to Qwen models labels Jul 19, 2025 fix ‚Ä¶ a5dfc09 Signed-off-by: Himanshu Jaju <hj@mistral.ai> Copy link Contributor Author hj-mistral commented Jul 21, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . That's a great point and answers my question as well. It is good to see the e2e speedup at least (and a note that FP8 performance looks off..) Don't worry about the DCO as we can resolve it manually before merge. It looks like there are a few related failures in the kernel tests I fixed my incorrect merge, but unsure how to fix the v1-test failure. Seems just an infra error? [2025-07-21T12:56:57Z]   Running command git clone --filter=blob:none --quiet https://github.com/robertgshaw2-neuralmagic/lm-evaluation-harness.git /tmp/pip-req-build-o61noco_\n[2025-07-21T12:56:58Z]   WARNING: Did not find branch or tag 'streaming-api', assuming revision or ref.\n[2025-07-21T12:56:58Z]   Running command git checkout -q streaming-api\n[2025-07-21T12:56:58Z]   error: pathspec 'streaming-api' did not match any file(s) known to git üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member mgoin commented Jul 21, 2025 Yeah the CI infra is just off there and we resolved on main, will request a force merge All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details simon-mo merged commit 0ec82ed into vllm-project : main Jul 21, 2025 96 of 98 checks passed Uh oh! There was an error while loading. Please reload this page . github-project-automation bot moved this to Done in Structured Output Jul 21, 2025 github-project-automation bot moved this to Done in Tool Calling Jul 21, 2025 hj-mistral deleted the hj-align-kernel branch July 21, 2025 18:26 Copy link Member tdoublep commented Jul 22, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . The changes from this PR are causing illegal memory accesses for me. If I deploy with commit before this PR was merged 005ae9be6c22dfa2c2c5580b50b41e67faee4a87 : $ VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve ibm-granite/granite-4.0-tiny-preview --no-enable-prefix-caching\n...\nINFO:     Started server process [604208]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete. Whereas, if I deploy at commit after this PR was merged 0ec82edda59aaf5cf3b07aadf4ecce1aa1131add : $ VLLM_USE_V1=1 VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve ibm-granite/granite-4.0-tiny-preview --no-enable-prefix-caching\n...\n  File \"/home/zrltpa/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1230, in torch_vllm_inplace_fused_experts\n    torch.ops.vllm.inplace_fused_experts(**kwargs)\n  File \"/home/zrltpa/miniforge3/envs/dev-env/lib/python3.12/site-packages/torch/_ops.py\", line 1158, in __call__\n    return self._op(*args, **(kwargs or {}))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zrltpa/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1020, in inplace_fused_experts\n    fused_experts_impl(hidden_states, w1, w2, topk_weights, topk_ids, True,\n  File \"/home/zrltpa/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 1484, in fused_experts_impl\n    invoke_fused_moe_kernel(qcurr_hidden_states,\n  File \"/home/zrltpa/vllm/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 604, in invoke_fused_moe_kernel\n    fused_moe_kernel[grid](\n  File \"/home/zrltpa/miniforge3/envs/dev-env/lib/python3.12/site-packages/triton/runtime/jit.py\", line 347, in <lambda>\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zrltpa/miniforge3/envs/dev-env/lib/python3.12/site-packages/triton/runtime/jit.py\", line 591, in run\n    kernel.run(grid_0, grid_1, grid_2, stream, kernel.function, kernel.packed_metadata,\n  File \"/home/zrltpa/miniforge3/envs/dev-env/lib/python3.12/site-packages/triton/backends/nvidia/driver.py\", line 529, in __call__\n    self.launch(gridX, gridY, gridZ, stream, function, self.launch_cooperative_grid, global_scratch, *args)\nRuntimeError: Triton Error [CUDA]: an illegal memory access was encountered Could we perhaps revert the changes from this PR until we figure out what is going on here? cc @mgoin @tlrmchlsmth This should have been caught by the CI tests...looking into what happened. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member tdoublep commented Jul 22, 2025 Before PR: python -m pytest tests/models/language/generation/test_hybrid.py::test_models[5-64-ibm-granite/granite-4.0-tiny-preview] \n...\n1 passed, 12 warnings in 69.55s (0:01:09) After PR: $ python -m pytest tests/models/language/generation/test_hybrid.py::test_models[5-64-ibm-granite/granite-4.0-tiny-preview]\n...\nFAILED tests/models/language/generation/test_hybrid.py::test_models[5-64-ibm-granite/granite-4.0-tiny-preview] - RuntimeError: Triton Error [CUDA]: operation not supported on global/shared address space\nERROR tests/models/language/generation/test_hybrid.py::test_models[5-64-ibm-granite/granite-4.0-tiny-preview] - RuntimeError: CUDA error: operation not supported on global/shared address space All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member tdoublep commented Jul 22, 2025 OK the reason it passes in CI is that vLLM bumped torch version which in turn bumped Triton version to 3.3.1. That seems to resolve the error that I am seeing. Still a bit weird though? Illegal memory access in 3.3.0 but works fine in 3.3.1? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zixi-qi pushed a commit\n        to zixi-qi/vllm\n      that referenced\n      this pull request Jul 23, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 41d76db Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: qizixi <qizixi@meta.com> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 98e2e2c Signed-off-by: Himanshu Jaju <hj@mistral.ai> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 8954857 Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 8944e23 Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 15e1cba Signed-off-by: Himanshu Jaju <hj@mistral.ai> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 0865b8e Signed-off-by: Himanshu Jaju <hj@mistral.ai> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 885137a Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ a6ae1b9 Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 7672862 Signed-off-by: Himanshu Jaju <hj@mistral.ai> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ 92ef410 Signed-off-by: Himanshu Jaju <hj@mistral.ai>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 27, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ c1bb8c1 Signed-off-by: Himanshu Jaju <hj@mistral.ai> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [perf] Speed up align sum kernels ( vllm-project#21079 ) ‚Ä¶ c6cb0c5 Signed-off-by: Himanshu Jaju <hj@mistral.ai> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:25",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: throughput, Throughput, throughput | SERVING: vllm serve, vllm serve, serve | TEST: Test, test, test",
  "analysis_extracted_at": "2025-09-07 17:50:25",
  "models": [
    "Qwen/Qwen3-30B-A3B",
    "Qwen/Qwen3-30B-A3B-FP8",
    "ibm-granite/granite-4.0-tiny-preview"
  ],
  "lm_eval_commands": null,
  "perf_command": "vllm bench throughput --model Qwen/Qwen3-30B-A3B --load-format dummy --input-len 1000 --output-len 100",
  "commit_subject": "[perf] Speed up align sum kernels (#21079)",
  "commit_message": "[perf] Speed up align sum kernels (#21079)\n\nSigned-off-by: Himanshu Jaju <hj@mistral.ai>",
  "commit_date": "2025-07-21T11:19:23-07:00",
  "files_changed": [
    "benchmarks/kernels/benchmark_moe_align_block_size.py",
    "csrc/moe/moe_align_sum_kernels.cu",
    "vllm/model_executor/layers/fused_moe/moe_align_block_size.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 13,
    "num_edited_lines": 85,
    "num_non_test_edited_lines": 85,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/benchmarks/kernels/benchmark_moe_align_block_size.py b/benchmarks/kernels/benchmark_moe_align_block_size.py\nindex 5170ac09d..1af5a21ca 100644\n--- a/benchmarks/kernels/benchmark_moe_align_block_size.py\n+++ b/benchmarks/kernels/benchmark_moe_align_block_size.py\n@@ -33,15 +33,13 @@ def check_correctness(num_tokens, num_experts=256, block_size=256, topk=8):\n     sorted_ids_triton = torch.empty(\n         (max_num_tokens_padded,), dtype=torch.int32, device=\"cuda\"\n     )\n-    sorted_ids_triton.fill_(topk_ids.numel())  # fill with sentinel value\n-    expert_ids_triton = torch.zeros(\n+    expert_ids_triton = torch.empty(\n         (max_num_tokens_padded // block_size,), dtype=torch.int32, device=\"cuda\"\n     )\n     num_tokens_post_pad_triton = torch.empty((1,), dtype=torch.int32, device=\"cuda\")\n \n     sorted_ids_vllm = torch.empty_like(sorted_ids_triton)\n-    sorted_ids_vllm.fill_(topk_ids.numel())\n-    expert_ids_vllm = torch.zeros_like(expert_ids_triton)\n+    expert_ids_vllm = torch.empty_like(expert_ids_triton)\n     num_tokens_post_pad_vllm = torch.empty_like(num_tokens_post_pad_triton)\n \n     # 2. run implementations\n@@ -102,7 +100,6 @@ def benchmark(num_tokens, num_experts, topk, provider):\n \n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     sorted_ids = torch.empty((max_num_tokens_padded,), dtype=torch.int32, device=\"cuda\")\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = max_num_tokens_padded // block_size\n     expert_ids = torch.empty((max_num_m_blocks,), dtype=torch.int32, device=\"cuda\")\n     num_tokens_post_pad = torch.empty((1,), dtype=torch.int32, device=\"cuda\")\ndiff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu\nindex 462dbd1f8..8bbcf5a67 100644\n--- a/csrc/moe/moe_align_sum_kernels.cu\n+++ b/csrc/moe/moe_align_sum_kernels.cu\n@@ -1,6 +1,7 @@\n #include <torch/all.h>\n #include <ATen/cuda/CUDAContext.h>\n #include <c10/cuda/CUDAGuard.h>\n+#include <cub/cub.cuh>\n \n #include <ATen/ATen.h>\n #include <ATen/cuda/Atomic.cuh>\n@@ -19,9 +20,14 @@ __global__ void moe_align_block_size_kernel(\n     int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,\n     int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,\n     int32_t padded_num_experts, int32_t experts_per_warp, int32_t block_size,\n-    size_t numel, int32_t* __restrict__ cumsum) {\n+    size_t numel, int32_t* __restrict__ cumsum, int32_t max_num_tokens_padded) {\n   extern __shared__ int32_t shared_counts[];\n \n+  // Initialize sorted_token_ids with numel\n+  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {\n+    sorted_token_ids[it] = numel;\n+  }\n+\n   const int warp_id = threadIdx.x / WARP_SIZE;\n   const int my_expert_start = warp_id * experts_per_warp;\n \n@@ -45,18 +51,27 @@ __global__ void moe_align_block_size_kernel(\n \n   __syncthreads();\n \n-  if (threadIdx.x == 0) {\n-    cumsum[0] = 0;\n-    for (int i = 1; i <= num_experts; ++i) {\n-      int expert_count = 0;\n-      int warp_idx = (i - 1) / experts_per_warp;\n-      int expert_offset = (i - 1) % experts_per_warp;\n-      expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset];\n+  // Compute prefix sum over token counts per expert\n+  using BlockScan = cub::BlockScan<int32_t, 1024>;\n+  __shared__ typename BlockScan::TempStorage temp_storage;\n \n-      cumsum[i] =\n-          cumsum[i - 1] + CEILDIV(expert_count, block_size) * block_size;\n-    }\n-    *total_tokens_post_pad = cumsum[num_experts];\n+  int expert_count = 0;\n+  int expert_id = threadIdx.x;\n+  if (expert_id < num_experts) {\n+    int warp_idx = expert_id / experts_per_warp;\n+    int expert_offset = expert_id % experts_per_warp;\n+    expert_count = shared_counts[warp_idx * experts_per_warp + expert_offset];\n+    expert_count = CEILDIV(expert_count, block_size) * block_size;\n+  }\n+\n+  int cumsum_val;\n+  BlockScan(temp_storage).ExclusiveSum(expert_count, cumsum_val);\n+  if (expert_id <= num_experts) {\n+    cumsum[expert_id] = cumsum_val;\n+  }\n+\n+  if (expert_id == num_experts) {\n+    *total_tokens_post_pad = cumsum_val;\n   }\n \n   __syncthreads();\n@@ -67,6 +82,13 @@ __global__ void moe_align_block_size_kernel(\n       expert_ids[i / block_size] = threadIdx.x;\n     }\n   }\n+\n+  // Fill remaining expert_ids with 0\n+  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;\n+  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);\n+  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {\n+    expert_ids[i] = 0;\n+  }\n }\n \n template <typename scalar_t>\n@@ -105,7 +127,12 @@ __global__ void moe_align_block_size_small_batch_expert_kernel(\n     const scalar_t* __restrict__ topk_ids,\n     int32_t* __restrict__ sorted_token_ids, int32_t* __restrict__ expert_ids,\n     int32_t* __restrict__ total_tokens_post_pad, int32_t num_experts,\n-    int32_t block_size, size_t numel) {\n+    int32_t block_size, size_t numel, int32_t max_num_tokens_padded) {\n+  // Initialize sorted_token_ids with numel\n+  for (size_t it = threadIdx.x; it < max_num_tokens_padded; it += blockDim.x) {\n+    sorted_token_ids[it] = numel;\n+  }\n+\n   const size_t tid = threadIdx.x;\n   const size_t stride = blockDim.x;\n \n@@ -153,6 +180,13 @@ __global__ void moe_align_block_size_small_batch_expert_kernel(\n     }\n   }\n \n+  // Fill remaining expert_ids with 0\n+  const size_t fill_start_idx = cumsum[num_experts] / block_size + threadIdx.x;\n+  const size_t expert_ids_size = CEILDIV(max_num_tokens_padded, block_size);\n+  for (size_t i = fill_start_idx; i < expert_ids_size; i += blockDim.x) {\n+    expert_ids[i] = 0;\n+  }\n+\n   for (size_t i = tid; i < numel; i += stride) {\n     int32_t expert_id = topk_ids[i];\n     int32_t rank_post_pad =\n@@ -179,13 +213,17 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n   int threads = 1024;\n   threads = ((threads + WARP_SIZE - 1) / WARP_SIZE) * WARP_SIZE;\n \n+  // BlockScan uses 1024 threads and assigns one thread per expert.\n+  TORCH_CHECK(padded_num_experts < 1024,\n+              \"padded_num_experts must be less than 1024\");\n+\n   VLLM_DISPATCH_INTEGRAL_AND_UNSIGNED_TYPES(\n       topk_ids.scalar_type(), \"moe_align_block_size_kernel\", [&] {\n         // calc needed amount of shared mem for `cumsum` tensors\n         auto options_int =\n             torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());\n         torch::Tensor cumsum_buffer =\n-            torch::zeros({num_experts + 1}, options_int);\n+            torch::empty({num_experts + 1}, options_int);\n         bool small_batch_expert_mode =\n             (topk_ids.numel() < 1024) && (num_experts <= 64);\n \n@@ -203,7 +241,7 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n               sorted_token_ids.data_ptr<int32_t>(),\n               experts_ids.data_ptr<int32_t>(),\n               num_tokens_post_pad.data_ptr<int32_t>(), num_experts, block_size,\n-              topk_ids.numel());\n+              topk_ids.numel(), sorted_token_ids.size(0));\n         } else {\n           auto align_kernel = vllm::moe::moe_align_block_size_kernel<scalar_t>;\n \n@@ -217,7 +255,8 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n               experts_ids.data_ptr<int32_t>(),\n               num_tokens_post_pad.data_ptr<int32_t>(), num_experts,\n               padded_num_experts, experts_per_warp, block_size,\n-              topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>());\n+              topk_ids.numel(), cumsum_buffer.data_ptr<int32_t>(),\n+              sorted_token_ids.size(0));\n \n           const int block_threads = std::min(256, (int)threads);\n           const int num_blocks =\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 3aae183df..2c9ad509f 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -111,6 +111,8 @@ def moe_align_block_size_triton(\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n     tokens_per_thread = cdiv(numel, num_experts)\n+    sorted_token_ids.fill_(numel)\n+    expert_ids.zero_()\n \n     moe_align_block_size_stage1[grid](\n         topk_ids,\n@@ -205,11 +207,8 @@ def moe_align_block_size(\n     sorted_ids = torch.empty((max_num_tokens_padded, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n-    # Expert ids must be zeroed out to prevent index out of bounds error while\n-    # mapping global expert ids to local expert ids in expert parallelism.\n-    expert_ids = torch.zeros((max_num_m_blocks, ),\n+    expert_ids = torch.empty((max_num_m_blocks, ),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n     num_tokens_post_pad = torch.empty((1),",
  "apis": [
    "vllm.model_executor.layers.fused_moe.moe_align_block_size"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/moe_align_block_size.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/kernels/benchmark_moe_align_block_size.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test source files (C++ CUDA kernels and Python modules) and alters algorithmic behavior in the align sum kernels to speed them up. The changes include replacing filled arrays with more efficient initializations, adding a cub::BlockScan based prefix sum for computing cumsum, and additional logic to fill arrays without unnecessary initialization overhead. These modifications are carefully tuned to improve runtime performance on CPU accessible kernels (even though they run on CUDA, they don‚Äôt involve GPU-exclusive workloads for training, and performance is testable without a GPU context running general algorithms). The changes are non-trivial modifications of internal performance-critical kernels rather than simple refactorings, bug fixes, or feature additions. Therefore, based on these performance optimization improvements, the commit satisfies the conditions for being performance related.",
  "llm_api_reason": "The commit refactors both the benchmark and CUDA kernel code used in the MoE alignment operation and updates the Python wrapper in vllm/model_executor/layers/fused_moe/moe_align_block_size.py. In doing so, it changes the initialization and memory filling routines used by the Python function that performs MoE block size alignment. This means that the Python API function moe_align_block_size is affected, as it now relies on the revised internal behavior for aligning tokens and mapping expert IDs."
}