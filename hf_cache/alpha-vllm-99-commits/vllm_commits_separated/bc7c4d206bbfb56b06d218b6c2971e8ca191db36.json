{
  "commit_hash": "bc7c4d206bbfb56b06d218b6c2971e8ca191db36",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13305",
  "pr_date": "2025-04-23",
  "timeline_text": "Copy link Contributor maleksan85 commented Feb 14, 2025 • edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Speed up prefix prefill with vLLM V1 on AMG GPUs Improvements: Vectorization in the context loop (most complex one as k cache shape is very specific) Refactoring for online softmax computation Refactoring to the kernel so autotune might select the best configs per shape Plus adding new spectrum of unrolling/staging in autotuner More details on triton kernel tunning: https://rocm.docs.amd.com/en/docs-6.1.1/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html see last comments Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions SageMoore added 30 commits February 5, 2025 20:42 init … b6b00d7 Signed-off-by: Sage Moore <sage@neuralmagic.com> temporarily remove torch from requirements-build … fa52268 Signed-off-by: Sage Moore <sage@neuralmagic.com> move rocm logic to its own attention backend … f563276 Signed-off-by: Sage Moore <sage@neuralmagic.com> actually add backend … 2a03b92 Signed-off-by: Sage Moore <sage@neuralmagic.com> more rocm refactoring … 4bdf7de Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … 875fcfc …amd-v1 more rocm refactoring … e507e30 Signed-off-by: Sage Moore <sage@neuralmagic.com> hack to fix the multiprocessing isssue … b9ce259 Signed-off-by: Sage Moore <sage@neuralmagic.com> minor print fix … f2cc5e3 Signed-off-by: Sage Moore <sage@neuralmagic.com> remove cruft … d6f6c5c Signed-off-by: Sage Moore <sage@neuralmagic.com> format … 2bf214a Signed-off-by: Sage Moore <sage@neuralmagic.com> modify requirements files … 11411cb Signed-off-by: Sage Moore <sage@neuralmagic.com> remove basic.py changes … c2499bf Signed-off-by: Sage Moore <sage@neuralmagic.com> cleanup … cf6f691 Signed-off-by: Sage Moore <sage@neuralmagic.com> add support for passing in softmax scales to the context_attn_fwd … 4505f53 Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … 9a0416a …amd-v1 added requirements-rocm-build … ef9ae86 Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … 0ccef65 …amd-v1 minor setup.py fix … a00a2d9 Signed-off-by: Sage Moore <sage@neuralmagic.com> add batch size back in … afb15f5 Signed-off-by: Sage Moore <sage@neuralmagic.com> revert setup.py change … 08a25b7 Signed-off-by: Sage Moore <sage@neuralmagic.com> update setup.py … 55eb036 Signed-off-by: Sage Moore <sage@neuralmagic.com> init … 95df571 Signed-off-by: Sage Moore <sage@neuralmagic.com> init … 0bfe435 Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … 4b62de2 …amd-v1\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com> minor fix … d2f3c85 Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … 442bc7b …amd-v1 minor fix … 9472636 Signed-off-by: Sage Moore <sage@neuralmagic.com> Merge branch 'main' of https://github.com/neuralmagic/vllm into sage/… … c7497f3 …prefix-prefill-refactor update error messages … 21d8d6a Signed-off-by: Sage Moore <sage@neuralmagic.com> 83 hidden items Load more… Copy link Contributor Author maleksan85 commented Apr 8, 2025 HIP_VISIBLE_DEVICES=6 VLLM_ENABLE_V1_MULTIPROCESSING=0 VLLM_USE_V1=1 lm_eval --model vllm --model_args pretrained=/data/models/Llama-3.1-8B-Instruct --tasks gsm8k --num_fewshot 5 --batch_size auto -\n-limit 500 2025-04-08:18:10:02,846 INFO     [lm_eval.loggers.evaluation_tracker:272] Output path not provided, skipping saving results aggregated vllm (pretrained=/data/models/Llama-3.1-8B-Instruct), gen_kwargs: (None), limit: 500.0, num_fewshot: 5, batch_size: auto Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ↑ 0.808 ± 0.0176 strict-match 5 exact_match ↑ 0.782 ± 0.0185 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author maleksan85 commented Apr 8, 2025 python3 benchmarks/benchmark_serving.py --backend vllm --model /data/models/Llama-3.1-70B-Instruct --dataset-name random --random-input-len 10000 --random-output-len 100 --num-prompts 300 --seed 42 --ignore-eos --percentile-metrics \"ttft,tpot,itl,e2el\" PR (like 20% gain) ============ Serving Benchmark Result ============\nSuccessful requests:                     300\nBenchmark duration (s):                  409.78\nTotal input tokens:                      3000000\nTotal generated tokens:                  30000\nRequest throughput (req/s):              0.73\nOutput token throughput (tok/s):         73.21\nTotal Token throughput (tok/s):          7394.28\n---------------Time to First Token----------------\nMean TTFT (ms):                          205042.73\nMedian TTFT (ms):                        203406.19\nP99 TTFT (ms):                           400609.81\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1610.15\nMedian TPOT (ms):                        2027.83\nP99 TPOT (ms):                           2239.19\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1610.15\nMedian ITL (ms):                         80.56\nP99 ITL (ms):                            5252.32\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          364447.21\nMedian E2EL (ms):                        404161.34\nP99 E2EL (ms):                           409588.24\n================================================== Upstream ============ Serving Benchmark Result ============\nSuccessful requests:                     300\nBenchmark duration (s):                  498.15\nTotal input tokens:                      3000000\nTotal generated tokens:                  30000\nRequest throughput (req/s):              0.60\nOutput token throughput (tok/s):         60.22\nTotal Token throughput (tok/s):          6082.51\n---------------Time to First Token----------------\nMean TTFT (ms):                          249095.71\nMedian TTFT (ms):                        248711.87\nP99 TTFT (ms):                           488484.85\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1957.47\nMedian TPOT (ms):                        2462.50\nP99 TPOT (ms):                           2732.60\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1957.47\nMedian ITL (ms):                         80.32\nP99 ITL (ms):                            8005.81\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          442885.68\nMedian E2EL (ms):                        492500.58\nP99 E2EL (ms):                           497952.19\n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author maleksan85 commented Apr 8, 2025 • edited Loading Uh oh! There was an error while loading. Please reload this page . python3 benchmarks/benchmark_serving.py --backend vllm --model /data/models/Llama-3.1-70B-Instruct --dataset-name random --random-input-len 5000 --random-output-len 100 --num-prompts 500 --seed 42 --ignore-eos --percentile-metrics \"ttft,tpot,itl,e2el\" PR (10% gain) ============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  319.37\nTotal input tokens:                      2500000\nTotal generated tokens:                  50000\nRequest throughput (req/s):              1.57\nOutput token throughput (tok/s):         156.56\nTotal Token throughput (tok/s):          7984.50\n---------------Time to First Token----------------\nMean TTFT (ms):                          155485.39\nMedian TTFT (ms):                        149836.40\nP99 TTFT (ms):                           310684.27\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1219.18\nMedian TPOT (ms):                        1556.81\nP99 TPOT (ms):                           1629.28\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1219.18\nMedian ITL (ms):                         77.67\nP99 ITL (ms):                            4265.61\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          276184.44\nMedian E2EL (ms):                        310784.82\nP99 E2EL (ms):                           319205.24\n================================================== Upstream ============ Serving Benchmark Result ============\nSuccessful requests:                     500\nBenchmark duration (s):                  355.99\nTotal input tokens:                      2500000\nTotal generated tokens:                  50000\nRequest throughput (req/s):              1.40\nOutput token throughput (tok/s):         140.45\nTotal Token throughput (tok/s):          7163.04\n---------------Time to First Token----------------\nMean TTFT (ms):                          172121.19\nMedian TTFT (ms):                        162339.60\nP99 TTFT (ms):                           349045.74\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          1369.76\nMedian TPOT (ms):                        1699.35\nP99 TPOT (ms):                           1892.04\n---------------Inter-token Latency----------------\nMean ITL (ms):                           1369.76\nMedian ITL (ms):                         78.00\nP99 ITL (ms):                            6167.44\n----------------End-to-end Latency----------------\nMean E2EL (ms):                          307727.51\nMedian E2EL (ms):                        349138.54\nP99 E2EL (ms):                           355831.83\n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . root and others added 9 commits April 9, 2025 03:54 renaming kernel … 5d9a929 Signed-off-by: root <root@banff-cyxtera-s65-4.amd.com>\n\nSigned-off-by:  <> clean up and fix for failed kernel tests … 27f044b Signed-off-by: Aleksandr Malyshev <maleksan@amd.com> clean up and fix for failed kernel tests … cfd60c9 Signed-off-by: Aleksandr Malyshev <maleksan@amd.com> clean up and fix for failed kernel tests … 0a26697 Signed-off-by: Aleksandr Malyshev <maleksan@amd.com> got rid of autotuner and get stable runs right from the first iteration … 35a6e49 Signed-off-by: maleksan85 <maleksan@amd.com> restoring paged attn as there is no autotuning anymore and that will … … 6d5b3f2 …no be error during start\n\nSigned-off-by: maleksan85 <maleksan@amd.com> poking test rerun as one failed and seems not because of this change … 7140d1a Signed-off-by: maleksan85 <maleksan@amd.com> Merge branch 'main' of github.com:vllm-project/vllm into upstream_pre… … 169f714 …fix_prefill_speed_up Merge branch 'upstream/main' into upstream_prefix_prefill_speed_up f437b11 SageMoore reviewed Apr 14, 2025 View reviewed changes Copy link Contributor SageMoore left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looks reasonable. Just a few nits. Thanks for all of the hard work making this kernel faster. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ❤️ 1 maleksan85 reacted with heart emoji All reactions ❤️ 1 reaction vllm/attention/ops/prefix_prefill.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/attention/ops/prefix_prefill.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . maleksan85 added 4 commits April 14, 2025 22:43 comment correction … ba078b6 Signed-off-by: maleksan85 <maleksan@amd.com> dot operation in triton doesn't support k to be 8 so increasing block… … 617ef08 … size to most commonly used\n\nSigned-off-by: maleksan85 <maleksan@amd.com> to kick CIs again Async Engine, Inputs, Utils, Worker Test seems flaky … 771ad9e Signed-off-by: maleksan85 <maleksan@amd.com> to kick CIs again … b6bf365 Signed-off-by: maleksan85 <maleksan@amd.com> bringlein mentioned this pull request Apr 16, 2025 [Kernel] Adding basic Triton JitCache for triton_attn #16606 Open Hide details View details vllm-bot merged commit bc7c4d2 into vllm-project : main Apr 23, 2025 41 of 46 checks passed Uh oh! There was an error while loading. Please reload this page . frieda-huang pushed a commit\n        to frieda-huang/vllm\n      that referenced\n      this pull request Apr 23, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … 5b0368a …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: Frieda (Jingying) Huang <jingyingfhuang@gmail.com> gshtras added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Apr 25, 2025 Upstream merge 2025 04 25 ( #524 ) … 28007b0 * [BugFix] Remove default multiproc executor `collective_rpc` timeout ( vllm-project#17000 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [Core][V1][TPU] Enable structured decoding on TPU V1 ( vllm-project#16499 )\n\nSigned-off-by: Chenyaaang <chenyangli@google.com>\n\n* [Bugfix] validate urls object for multimodal content parts ( vllm-project#16990 )\n\nSigned-off-by: Guillaume Calmettes <gcalmettes@scaleway.com>\n\n* add Dockerfile build vllm against torch nightly ( vllm-project#16936 )\n\nSigned-off-by: Yang Wang <elainewy@meta.com>\n\n* [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-project#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\n\n* [V1][DP] More robust DP/EP dummy request coordination ( vllm-project#16277 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [BugFix] Revert ROCm Custom Paged Attention Env Flag Check ( vllm-project#17022 )\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* Revert \"[Misc] Add S3 environment variables for better support of MinIO.\" ( vllm-project#17021 )\n\n* [misc] tune some env vars for GB200 ( vllm-project#16992 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [INTEL-HPU][v0] Port delayed sampling to upstream ( vllm-project#16949 )\n\nSigned-off-by: Michal Adamczyk <michal.adamczyk@intel.com>\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>\nCo-authored-by: Michal Adamczyk <madamczyk@habana.ai>\n\n* [doc] add download path tips ( vllm-project#17013 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [Bugfix] Triton FA function takes no keyword arguments ( vllm-project#16902 )\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* [V1] Avoid socket errors during shutdown when requests are in in-flight ( vllm-project#16807 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [BugFix] llama4 fa3 fix - RuntimeError: scheduler_metadata must have shape (metadata_size) ( vllm-project#16998 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Misc] Improve readability of get_open_port function. ( vllm-project#17024 )\n\nSigned-off-by: gitover22 <qidizou88@gmail.com>\n\n* [Bugfix] Fix AssertionError: skip_special_tokens=False is not supported for Mistral tokenizers ( vllm-project#16964 )\n\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\n\n* [CI] Run v1/test_serial_utils.py in CI ( vllm-project#16996 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Mistral-format support for compressed-tensors ( vllm-project#16803 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* Categorize `tests/kernels/` based on kernel type ( vllm-project#16799 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Doc] Add top anchor and a note to quantization/bitblas.md ( vllm-project#17042 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* Ensure that `pid` passed to `kill_process_tree` is `int` for `mypy` ( vllm-project#17051 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [CI] Update structured-output label automation ( vllm-project#17055 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Improve Transformers backend model loading QoL ( vllm-project#17039 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* `CacheConfig.block_size` should always be `int` when used ( vllm-project#17052 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Use `@property` and private field for `data_parallel_rank_local` ( vllm-project#17053 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Frontend] Support guidance:no-additional-properties for compatibility with xgrammar ( vllm-project#15949 )\n\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\n\n* [BugFix][V1] Fix int32 token index overflow when preparing input ids ( vllm-project#16806 )\n\n* [V1][Spec Decode] Always use argmax for sampling draft tokens  ( vllm-project#16899 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [CI/Build] workaround for CI build failure ( vllm-project#17070 )\n\nSigned-off-by: csy1204 <josang1204@gmail.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\n\n* [Quantization]add prefix for commandA quantized model ( vllm-project#17017 )\n\n* [Minor] Use larger batch sizes for A100/B100/B200/MI300x ( vllm-project#17073 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Enable V1 usage stats ( vllm-project#16986 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\n\n* More informative error when using Transformers backend ( vllm-project#16988 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Addendum Fix to support FIPS enabled machines with MD5 hashing ( vllm-project#17043 )\n\nSigned-off-by: sydarb <areebsyed237@gmail.com>\n\n* [Bugfix][Core] add seq_id_to_seq_group clearing to avoid memory leak when s… ( vllm-project#16472 )\n\nSigned-off-by: 开哲 <kaizhe.zy@alibaba-inc.com>\nCo-authored-by: 开哲 <kaizhe.zy@alibaba-inc.com>\n\n* [V1] Update structured output ( vllm-project#16812 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [doc] update to hyperlink ( vllm-project#17096 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* Add docs for runai_streamer_sharded ( vllm-project#17093 )\n\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Chore] Remove Sampler from Model Code ( vllm-project#17084 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* Disable enforce_eager for V1 TPU sampler and structured output tests ( vllm-project#17016 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* Simplify `TokenizerGroup` ( vllm-project#16790 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Fix OOT registration test ( vllm-project#17099 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [V1][PP] Optimization: continue scheduling prefill chunks ( vllm-project#17080 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [Misc] Remove OLMo2 config copy ( vllm-project#17066 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Improve static type checking in `LoRAModelRunnerMixin` ( vllm-project#17104 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [V1][Structured Output] Clear xgrammar compiler object when engine core shut down to avoid nanobind leaked warning ( vllm-project#16954 )\n\nSigned-off-by: shen-shanshan <467638484@qq.com>\n\n* [Frontend] Using matryoshka_dimensions control the allowed output dimensions. ( vllm-project#16970 )\n\n* Add missing rocm_skinny_gemms kernel test to CI ( vllm-project#17060 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* [Misc] refactor example series - structured outputs ( vllm-project#17040 )\n\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\n\n* [V1][Spec Decoding] Add num_drafts and num_accepted_tokens_per_position metrics ( vllm-project#16665 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [CI] Add automation for the `tool-calling` github label ( vllm-project#17118 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Updating builkite job for IBM Power  ( vllm-project#17111 )\n\nSigned-off-by: Aaruni Aggarwal <aaruniagg@gmail.com>\n\n* existing torch installation pip command fix for docs ( vllm-project#17059 )\n\n* Molmo Requirements ( vllm-project#17026 )\n\nSigned-off-by: Eyshika Agarwal <eyshikaengineer@gmail.com>\nSigned-off-by: eyshika <eyshikaengineer@gmail.com>\n\n* Add `:markdownhelp:` to `EngineArgs` docs so markdown docstrings render properly ( vllm-project#17124 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Improve configs - `LoRAConfig` + `PromptAdapterConfig` ( vllm-project#16980 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Docs] Generate correct github links for decorated functions ( vllm-project#17125 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* Add collective_rpc to llm engine ( vllm-project#16999 )\n\nSigned-off-by: Yinghai Lu <yinghai@thinkingmachines.ai>\n\n* Add chat template for Llama 4 models ( vllm-project#16428 )\n\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\n\n* [Misc] Add example to run DeepSeek with Ray Serve LLM ( vllm-project#17134 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* Better error message for missing mistral params.json ( vllm-project#17132 )\n\nSigned-off-by: mgoin <mgoin64@gmail.com>\n\n* Use custom address for listening socket ( vllm-project#15988 )\n\nSigned-off-by: Jens Glaser <glaserj@ornl.gov>\n\n* [FEAT] [ROCm]: AITER Fused MOE V1 Support ( vllm-project#16752 )\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: tjtanaa <tunjian.tan@embeddedllm.com>\n\n* [Attention] FA3 decode perf improvement - single mma warp group support for head dim 128 ( vllm-project#16864 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* fix float16 support for kimi-vl ( vllm-project#17156 )\n\nCo-authored-by: zhouzaida <zhouzaida@msh.team>\n\n* [Doc] V1 : Update LoRA status ( vllm-project#17133 )\n\nSigned-off-by: varun sundar rabindranath <vsundarr@redhat.com>\nCo-authored-by: varun sundar rabindranath <vsundarr@redhat.com>\n\n* [Docs] Fix True->true in supported_models.md ( vllm-project#17141 )\n\n* Move missed `SchedulerConfig` args into scheduler config group in `EngineArgs` ( vllm-project#17131 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [Misc] Clean up redundant code in uniproc_executor.py ( vllm-project#16762 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [Bugfix][Misc] Use TritonPlaceholderModule to defensively import triton ( vllm-project#15099 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [Misc] Benchmark Serving Script Support Appending Results ( vllm-project#17028 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Perf]Optimize rotary_emb implementation to use Triton operator for improved inference performance ( vllm-project#16457 )\n\nSigned-off-by: cynthieye <yexin93@qq.com>\nCo-authored-by: MagnetoWang <magnetowang@outlook.com>\n\n* [Bugfix] remove fallback in guided_json (int range, patterns) ( vllm-project#16725 )\n\nSigned-off-by: csy1204 <josang1204@gmail.com>\nCo-authored-by: 조상연[플레이스 AI] <sang-yeon.cho@navercorp.com>\n\n* [Quantization][FP8] Add support for FP8 models with input_scale for output projection and QK quantization ( vllm-project#15734 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Luka Govedič <lgovedic@redhat.com>\nCo-authored-by: Luka Govedič <lgovedic@redhat.com>\n\n* [Doc] Add headings to improve gptqmodel.md ( vllm-project#17164 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* Only turn on FastIncrementalDetokenizer when tokenizers >= 0.21.1 ( vllm-project#17158 )\n\n* [Doc] Add two links to disagg_prefill.md ( vllm-project#17168 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* [Doc] Move todo out of beam search docstring ( vllm-project#17183 )\n\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\n\n* [Bugfix] Fix mistral model tests ( vllm-project#17181 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix Mistral ChatCompletionRequest Body Exception ( vllm-project#16769 )\n\nSigned-off-by: Jasmond Loh <Jasmond.Loh@hotmail.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* Fix API typo and remove FP8 on V1 restriction\n\n---------\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Chenyaaang <chenyangli@google.com>\nSigned-off-by: Guillaume Calmettes <gcalmettes@scaleway.com>\nSigned-off-by: Yang Wang <elainewy@meta.com>\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Michal Adamczyk <michal.adamczyk@intel.com>\nSigned-off-by: Chendi Xue <chendi.xue@intel.com>\nSigned-off-by: reidliu41 <reid201711@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: gitover22 <qidizou88@gmail.com>\nSigned-off-by: chaunceyjiang <chaunceyjiang@gmail.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: csy1204 <josang1204@gmail.com>\nSigned-off-by: sydarb <areebsyed237@gmail.com>\nSigned-off-by: 开哲 <kaizhe.zy@alibaba-inc.com>\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: shen-shanshan <467638484@qq.com>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Aaruni Aggarwal <aaruniagg@gmail.com>\nSigned-off-by: Eyshika Agarwal <eyshikaengineer@gmail.com>\nSigned-off-by: eyshika <eyshikaengineer@gmail.com>\nSigned-off-by: Yinghai Lu <yinghai@thinkingmachines.ai>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Jens Glaser <glaserj@ornl.gov>\nSigned-off-by: varun sundar rabindranath <vsundarr@redhat.com>\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: cynthieye <yexin93@qq.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Luka Govedič <lgovedic@redhat.com>\nSigned-off-by: Alex-Brooks <Alex.Brooks@ibm.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Jasmond Loh <Jasmond.Loh@hotmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Chenyaaang <42742451+Chenyaaang@users.noreply.github.com>\nCo-authored-by: Guillaume Calmettes <gcalmettes@scaleway.com>\nCo-authored-by: Yang Wang <elainewy@meta.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Chauncey <chaunceyjiang@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Chendi.Xue <chendi.xue@intel.com>\nCo-authored-by: Michal Adamczyk <madamczyk@habana.ai>\nCo-authored-by: Reid <61492567+reidliu41@users.noreply.github.com>\nCo-authored-by: reidliu41 <reid201711@gmail.com>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: huafeng <qidizou88@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Yong Hoon Shin <48474650+sarckk@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Sangyeon Cho <josang1204@gmail.com>\nCo-authored-by: Chen Xia <cxia0209@gmail.com>\nCo-authored-by: Areeb Syed <areebsyed237@gmail.com>\nCo-authored-by: 张宇 <zhangyuygss@outlook.com>\nCo-authored-by: 开哲 <kaizhe.zy@alibaba-inc.com>\nCo-authored-by: omer-dayan <omdayan@nvidia.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: wang.yuqi <noooop@126.com>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Aaruni Aggarwal <47731267+AaruniAggarwal@users.noreply.github.com>\nCo-authored-by: Atilla <48064466+atilla00@users.noreply.github.com>\nCo-authored-by: Eyshika Agarwal <eyshikaengineer@gmail.com>\nCo-authored-by: Yinghai Lu <yinghai@thinkingmachines.ai>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: jglaser <glaserj@ornl.gov>\nCo-authored-by: tjtanaa <tunjian.tan@embeddedllm.com>\nCo-authored-by: Zaida Zhou <58739961+zhouzaida@users.noreply.github.com>\nCo-authored-by: zhouzaida <zhouzaida@msh.team>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: varun sundar rabindranath <vsundarr@redhat.com>\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: yexin(叶鑫) <yexin93@qq.com>\nCo-authored-by: MagnetoWang <magnetowang@outlook.com>\nCo-authored-by: 조상연[플레이스 AI] <sang-yeon.cho@navercorp.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Luka Govedič <lgovedic@redhat.com>\nCo-authored-by: Lu Fang <30275821+houseroad@users.noreply.github.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Jasmond L <120363110+JasmondL@users.noreply.github.com> jikunshang pushed a commit\n        to jikunshang/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … c8ceba9 …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com> huydhn mentioned this pull request Apr 29, 2025 Fix some speculative decode tests with tl.dot #17371 Merged lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … 4bf77e2 …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com> adobrzyn pushed a commit\n        to HabanaAI/vllm-fork\n      that referenced\n      this pull request Apr 30, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … d4a8c54 …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: Agata Dobrzyniewicz <adobrzyniewicz@habana.ai> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … f32d058 …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> ckhordiasma mentioned this pull request May 14, 2025 nm vllm ent 0.8.5 sync red-hat-data-services/vllm#139 Merged minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 ( vllm-pro… … b3ce066 …ject#13305 )\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: minpeter <kali2005611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:14",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, gsm8k | PERF: ttft, TTFT, TTFT | SERVING: Serving, Serving, Serving | TEST: test, Test, test",
  "analysis_extracted_at": "2025-09-07 17:51:14",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct",
    "mistralai/Mistral-7B-Instruct-v0.3"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=mistralai/Mistral-7B-Instruct-v0.3,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)",
  "commit_message": "[Kernel][ROCM] Upstream prefix prefill speed up for vLLM V1 (#13305)\n\nSigned-off-by: Sage Moore <sage@neuralmagic.com>\nSigned-off-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nSigned-off-by: Aleksandr Malyshev <maleksan@amd.com>\nSigned-off-by: root <root@banff-cyxtera-s65-4.amd.com>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: <>\nCo-authored-by: Sage Moore <sage@neuralmagic.com>\nCo-authored-by: root <root@banff-cyxtera-s73-5.ctr.dcgpu>\nCo-authored-by: Aleksandr Malyshev <maleksan@amd.com>\nCo-authored-by: qli88 <qiang.li2@amd.com>\nCo-authored-by: root <root@banff-cyxtera-s65-4.amd.com>",
  "commit_date": "2025-04-22T19:11:56-07:00",
  "files_changed": [
    "tests/core/block/e2e/test_correctness.py",
    "vllm/attention/ops/prefix_prefill.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 2,
    "num_hunks": 4,
    "num_edited_lines": 1640,
    "num_non_test_edited_lines": 1634,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/core/block/e2e/test_correctness.py b/tests/core/block/e2e/test_correctness.py\nindex e9b537ed5..9e8e315d8 100644\n--- a/tests/core/block/e2e/test_correctness.py\n+++ b/tests/core/block/e2e/test_correctness.py\n@@ -195,15 +195,15 @@ def test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n     ])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\",\n                          [{\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 2,\n                              \"max_num_seqs\": 2,\n                          }, {\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 3,\n                              \"max_num_seqs\": 2,\n                          }, {\n-                             \"block_size\": 8,\n+                             \"block_size\": 16,\n                              \"max_num_batched_tokens\": 256,\n                              \"max_num_seqs\": 10,\n                          }])\ndiff --git a/vllm/attention/ops/prefix_prefill.py b/vllm/attention/ops/prefix_prefill.py\nindex e0478c2ae..a8c8d8409 100644\n--- a/vllm/attention/ops/prefix_prefill.py\n+++ b/vllm/attention/ops/prefix_prefill.py\n@@ -16,831 +16,778 @@ NUM_WARPS = 4 if current_platform.is_rocm() else 8\n # To check compatibility\n IS_TURING = current_platform.get_device_capability() == (7, 5)\n \n-if triton.__version__ >= \"2.1.0\":\n-\n-    @triton.jit\n-    def _fwd_kernel(\n-        Q,\n-        K,\n-        V,\n-        K_cache,\n-        V_cache,\n-        B_Loc,\n-        sm_scale,\n-        k_scale,\n-        v_scale,\n-        B_Start_Loc,\n-        B_Seqlen,\n-        block_size,\n-        x,\n-        Out,\n-        stride_b_loc_b,\n-        stride_b_loc_s,\n-        stride_qbs,\n-        stride_qh,\n-        stride_qd,\n-        stride_kbs,\n-        stride_kh,\n-        stride_kd,\n-        stride_vbs,\n-        stride_vh,\n-        stride_vd,\n-        stride_obs,\n-        stride_oh,\n-        stride_od,\n-        stride_k_cache_bs,\n-        stride_k_cache_h,\n-        stride_k_cache_d,\n-        stride_k_cache_bl,\n-        stride_k_cache_x,\n-        stride_v_cache_bs,\n-        stride_v_cache_h,\n-        stride_v_cache_d,\n-        stride_v_cache_bl,\n-        num_queries_per_kv: int,\n-        IN_PRECISION: tl.constexpr,\n-        BLOCK_M: tl.constexpr,\n-        BLOCK_DMODEL: tl.constexpr,  # head size\n-        BLOCK_DMODEL_PADDED: tl.constexpr,  # head size padded to a power of 2\n-        BLOCK_N: tl.constexpr,\n-        SLIDING_WINDOW: tl.constexpr,\n-        SKIP_DECODE: tl.constexpr,\n-    ):\n-\n-        cur_batch = tl.program_id(0)\n-        cur_head = tl.program_id(1)\n-        start_m = tl.program_id(2)\n-\n-        cur_kv_head = cur_head // num_queries_per_kv\n-\n-        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n-        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n-        cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)\n-        cur_batch_query_len = (cur_batch_in_all_stop_index -\n-                               cur_batch_in_all_start_index)\n-        cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len\n-\n-        if SKIP_DECODE and cur_batch_query_len == 1:\n-            return\n-\n-        # start position inside of the query\n-        # generally, N goes over kv, while M goes over query_len\n-        block_start_loc = BLOCK_M * start_m\n-\n-        # initialize offsets\n-        # [N]; starts at 0\n-        offs_n = tl.arange(0, BLOCK_N)\n-        # [D]; starts at 0\n-        offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)\n-        # [M]; starts at current position in query\n-        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n-        # [M,D]\n-        off_q = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n-            cur_head * stride_qh + offs_d[None, :] * stride_qd)\n-\n-        dim_mask = tl.where(\n-            tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1,\n-            0).to(tl.int1)  # [D]\n-\n-        q = tl.load(Q + off_q,\n-                    mask=dim_mask[None, :] &\n-                    (offs_m[:, None] < cur_batch_query_len),\n-                    other=0.0)  # [M,D]\n-\n-        # initialize pointer to m and l\n-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")  # [M]\n-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # [M]\n-        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED],\n-                       dtype=tl.float32)  # [M,D]\n-\n-        # compute query against context (no causal mask here)\n-        for start_n in range(0, cur_batch_ctx_len, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n-                         ((start_n + offs_n) // block_size) * stride_b_loc_s,\n-                         mask=(start_n + offs_n) < cur_batch_ctx_len,\n-                         other=0)  # [N]\n-            # [D,N]\n-            off_k = (bn[None, :] * stride_k_cache_bs +\n-                     cur_kv_head * stride_k_cache_h +\n-                     (offs_d[:, None] // x) * stride_k_cache_d +\n-                     ((start_n + offs_n[None, :]) % block_size) *\n-                     stride_k_cache_bl +\n-                     (offs_d[:, None] % x) * stride_k_cache_x)\n-            # [N,D]\n-            off_v = (\n-                bn[:, None] * stride_v_cache_bs +\n-                cur_kv_head * stride_v_cache_h +\n-                offs_d[None, :] * stride_v_cache_d +\n-                (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)\n-            k_load = tl.load(K_cache + off_k,\n-                             mask=dim_mask[:, None] &\n-                             ((start_n + offs_n[None, :]) < cur_batch_ctx_len),\n-                             other=0.0)  # [D,N]\n-\n-            if k_load.dtype.is_fp8():\n-                k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)\n-            else:\n-                k = k_load\n-\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)  # [M,N]\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n-            qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n-                          float(\"-inf\"))\n-            qk *= sm_scale\n-            if SLIDING_WINDOW > 0:\n-                # (cur_batch_ctx_len + offs_m[:, None]) are the positions of\n-                # Q entries in sequence\n-                # (start_n + offs_n[None, :]) are the positions of\n-                # KV entries in sequence\n-                # So the condition makes sure each entry in Q only attends\n-                # to KV entries not more than SLIDING_WINDOW away.\n-                #\n-                # We can't use -inf here, because the\n-                # sliding window may lead to the entire row being masked.\n-                # This then makes m_ij contain -inf, which causes NaNs in\n-                # exp().\n-                qk = tl.where((cur_batch_ctx_len + offs_m[:, None]) -\n-                              (start_n + offs_n[None, :]) < SLIDING_WINDOW, qk,\n-                              -10000)\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)  # [M]\n-            p = tl.exp(qk - m_ij[:, None])  # [M,N]\n-            l_ij = tl.sum(p, 1)  # [M]\n-            # -- update m_i and l_i\n-            m_i_new = tl.maximum(m_i, m_ij)  # [M]\n-            alpha = tl.exp(m_i - m_i_new)  # [M]\n-            beta = tl.exp(m_ij - m_i_new)  # [M]\n-            l_i_new = alpha * l_i + beta * l_ij  # [M]\n-\n-            # -- update output accumulator --\n-            # scale p\n-            p_scale = beta / l_i_new\n-            p = p * p_scale[:, None]\n-            # scale acc\n-            acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v_load = tl.load(V_cache + off_v,\n-                             mask=dim_mask[None, :] &\n-                             ((start_n + offs_n[:, None]) < cur_batch_ctx_len),\n-                             other=0.0)  # [N,D]\n-            if v_load.dtype.is_fp8():\n-                v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)\n-            else:\n-                v = v_load\n-            p = p.to(v.dtype)\n-\n-            acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)\n-            # # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-\n-        off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n-                 offs_d[:, None] * stride_kd)\n-        off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n-                 offs_d[None, :] * stride_vd)\n-        k_ptrs = K + off_k\n-        v_ptrs = V + off_v\n-\n-        # block_mask is 0 when we're already past the current query length\n-        block_mask = tl.where(block_start_loc < cur_batch_query_len, 1, 0)\n-\n-        # compute query against itself (with causal mask)\n-        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            k = tl.load(k_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_kbs,\n-                        mask=dim_mask[:, None] &\n-                        ((start_n + offs_n[None, :]) < cur_batch_query_len),\n-                        other=0.0)\n-\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n-            qk *= sm_scale\n-            # apply causal mask\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n-                          float(\"-inf\"))\n-            if SLIDING_WINDOW > 0:\n-                qk = tl.where(\n-                    offs_m[:, None] - (start_n + offs_n[None, :])\n-                    < SLIDING_WINDOW, qk, -10000)\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)\n-            p = tl.exp(qk - m_ij[:, None])\n-            l_ij = tl.sum(p, 1)\n-            # -- update m_i and l_i\n-            m_i_new = tl.maximum(m_i, m_ij)\n-            alpha = tl.exp(m_i - m_i_new)\n-            beta = tl.exp(m_ij - m_i_new)\n-            l_i_new = alpha * l_i + beta * l_ij\n-            # -- update output accumulator --\n-            # scale p\n-            p_scale = beta / l_i_new\n-            p = p * p_scale[:, None]\n-            # scale acc\n-            acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v = tl.load(v_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_vbs,\n-                        mask=dim_mask[None, :] &\n-                        ((start_n + offs_n[:, None]) < cur_batch_query_len),\n-                        other=0.0)\n-            p = p.to(v.dtype)\n-\n-            acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)\n-            # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-        # initialize pointers to output\n-        off_o = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n-            cur_head * stride_oh + offs_d[None, :] * stride_od)\n-        out_ptrs = Out + off_o\n-        tl.store(out_ptrs,\n-                 acc,\n-                 mask=dim_mask[None, :] &\n-                 (offs_m[:, None] < cur_batch_query_len))\n+\n+# Here's an example autotuner config for this kernel. This config does provide\n+# a performance improvement, but dramatically increases first call latency in\n+# triton 3.2. Because of this tradeoff, it's currently commented out.\n+# @triton.autotune(\n+#     configs=[\n+#         triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, \\\n+#                         \"num_unroll_cache\": 4, \\\n+#                         \"num_unroll_request\": 1 } | \\\n+#                         ({\"kpack\": 2, \"waves_per_eu\": 2} \\\n+#                             if current_platform.is_rocm() else {}), \\\n+#                         num_warps=4, \\\n+#                         num_stages=1)\n+#     ],\n+#     key=[\"BLOCK_SIZE\", \"MAX_Q_LEN\", \"MAX_CTX_LEN\"]\n+# )\n+@triton.jit\n+def _fwd_kernel(Q,\n+                K,\n+                V,\n+                K_cache,\n+                V_cache,\n+                B_Loc,\n+                sm_scale,\n+                k_scale,\n+                v_scale,\n+                B_Start_Loc,\n+                B_Seqlen,\n+                x: tl.constexpr,\n+                Out,\n+                stride_b_loc_b,\n+                stride_b_loc_s,\n+                stride_qbs,\n+                stride_qh,\n+                stride_qd,\n+                stride_kbs,\n+                stride_kh,\n+                stride_kd,\n+                stride_vbs,\n+                stride_vh,\n+                stride_vd,\n+                stride_obs,\n+                stride_oh,\n+                stride_od,\n+                stride_k_cache_bs,\n+                stride_k_cache_h,\n+                stride_k_cache_d,\n+                stride_k_cache_bl: tl.constexpr,\n+                stride_k_cache_x,\n+                stride_v_cache_bs,\n+                stride_v_cache_h,\n+                stride_v_cache_d,\n+                stride_v_cache_bl,\n+                num_queries_per_kv: tl.constexpr,\n+                IN_PRECISION: tl.constexpr,\n+                BLOCK_M: tl.constexpr,\n+                BLOCK_DMODEL: tl.constexpr,\n+                BLOCK_DMODEL_PADDED: tl.constexpr,\n+                BLOCK_SIZE: tl.constexpr,\n+                BLOCK_N: tl.constexpr,\n+                SLIDING_WINDOW: tl.constexpr,\n+                num_unroll_cache: tl.constexpr,\n+                num_unroll_request: tl.constexpr,\n+                SKIP_DECODE: tl.constexpr,\n+                MAX_Q_LEN: tl.constexpr = 0,\n+                MAX_CTX_LEN: tl.constexpr = 0):\n+\n+    cur_batch = tl.program_id(0)\n+    cur_head = tl.program_id(1)\n+    start_m = tl.program_id(2)\n+\n+    cur_kv_head = cur_head // num_queries_per_kv\n+\n+    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n+    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n+    cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)\n+    cur_batch_query_len = (cur_batch_in_all_stop_index -\n+                           cur_batch_in_all_start_index)\n+    cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len\n+\n+    if SKIP_DECODE and cur_batch_query_len == 1:\n         return\n \n-    @triton.jit\n-    def _fwd_kernel_flash_attn_v2(\n-        Q,\n-        K,\n-        V,\n-        K_cache,\n-        V_cache,\n-        B_Loc,\n-        sm_scale,\n-        B_Start_Loc,\n-        B_Seqlen,\n-        B_Ctxlen,\n-        block_size,\n-        x,\n-        Out,\n-        stride_b_loc_b,\n-        stride_b_loc_s,\n-        stride_qbs,\n-        stride_qh,\n-        stride_qd,\n-        stride_kbs,\n-        stride_kh,\n-        stride_kd,\n-        stride_vbs,\n-        stride_vh,\n-        stride_vd,\n-        stride_obs,\n-        stride_oh,\n-        stride_od,\n-        stride_k_cache_bs,\n-        stride_k_cache_h,\n-        stride_k_cache_d,\n-        stride_k_cache_bl,\n-        stride_k_cache_x,\n-        stride_v_cache_bs,\n-        stride_v_cache_h,\n-        stride_v_cache_d,\n-        stride_v_cache_bl,\n-        num_queries_per_kv: int,\n-        BLOCK_M: tl.constexpr,\n-        BLOCK_DMODEL: tl.constexpr,\n-        BLOCK_N: tl.constexpr,\n-    ):\n-        cur_batch = tl.program_id(0)\n-        cur_head = tl.program_id(1)\n-        start_m = tl.program_id(2)\n-\n-        cur_kv_head = cur_head // num_queries_per_kv\n-\n-        cur_batch_ctx_len = tl.load(B_Ctxlen + cur_batch)\n-        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n-        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n-\n-        block_start_loc = BLOCK_M * start_m\n-\n-        # initialize offsets\n-        offs_n = tl.arange(0, BLOCK_N)\n-        offs_d = tl.arange(0, BLOCK_DMODEL)\n-        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n-        off_q = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n-            cur_head * stride_qh + offs_d[None, :] * stride_qd)\n-\n-        q = tl.load(Q + off_q,\n-                    mask=offs_m[:, None]\n-                    < cur_batch_seq_len - cur_batch_ctx_len,\n+    # start position inside of the query\n+    # generally, N goes over kv, while M goes over query_len\n+    block_start_loc = BLOCK_M * start_m\n+\n+    # initialize offsets\n+    # [BLOCK_SIZE]; starts at 0\n+    offs_bs_n = tl.arange(0, BLOCK_SIZE)\n+    # [N]; starts at 0\n+    offs_n = tl.arange(0, BLOCK_N)\n+    # [D]; starts at 0\n+    offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)\n+    # [M]; starts at current position in query\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    # [M,D]\n+    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n+             cur_head * stride_qh + offs_d[None, :] * stride_qd)\n+\n+    dim_mask = tl.where(\n+        tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1,\n+        0).to(tl.int1)  # [D]\n+\n+    q = tl.load(Q + off_q,\n+                mask=dim_mask[None, :] &\n+                (offs_m[:, None] < cur_batch_query_len),\n+                other=0.0)  # [M,D]\n+\n+    # initialize pointer to m and l\n+    m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)\n+    l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED], dtype=tl.float32)  # [M,D]\n+\n+    # compute query against context (no causal mask here)\n+    for start_n in tl.range(0, cur_batch_ctx_len, BLOCK_SIZE, \\\n+                            loop_unroll_factor=num_unroll_cache):\n+        start_n = tl.multiple_of(start_n, BLOCK_SIZE)\n+        # -- compute qk ----\n+        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n+                     (start_n // BLOCK_SIZE) * stride_b_loc_s)\n+        # [D,BLOCK_SIZE]\n+        off_k = (\n+            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +\n+            (offs_d[:, None] // x) * stride_k_cache_d +\n+            ((start_n + offs_bs_n[None, :]) % BLOCK_SIZE) * stride_k_cache_bl +\n+            (offs_d[:, None] % x) * stride_k_cache_x)\n+\n+        # [BLOCK_SIZE,D]\n+        off_v = (bn[:, None] * stride_v_cache_bs +\n+                 cur_kv_head * stride_v_cache_h +\n+                 offs_d[None, :] * stride_v_cache_d +\n+                 offs_bs_n[:, None] * stride_v_cache_bl)\n+\n+        if start_n + BLOCK_SIZE > cur_batch_ctx_len or \\\n+            BLOCK_DMODEL != BLOCK_DMODEL_PADDED:\n+            k_load = tl.load(\n+                K_cache + off_k,\n+                mask=dim_mask[:, None] &\n+                ((start_n + offs_bs_n[None, :]) < cur_batch_ctx_len),\n+                other=0.0)  # [D,N]\n+        else:\n+            k_load = tl.load(K_cache + off_k)\n+\n+        if k_load.dtype.is_fp8():\n+            k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)\n+        else:\n+            k = k_load\n+\n+        qk = tl.zeros([BLOCK_M, BLOCK_SIZE], dtype=tl.float32)  # [M,N]\n+        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+        qk = tl.where((start_n + offs_bs_n[None, :]) < cur_batch_ctx_len, qk,\n+                      float(\"-inf\"))\n+        qk *= sm_scale\n+        if SLIDING_WINDOW > 0:\n+            # (cur_batch_ctx_len + offs_m[:, None]) are the positions of\n+            # Q entries in sequence\n+            # (start_n + offs_bs_n[None, :]) are the positions of\n+            # KV entries in sequence\n+            # So the condition makes sure each entry in Q only attends\n+            # to KV entries not more than SLIDING_WINDOW away.\n+            #\n+            # We can't use -inf here, because the\n+            # sliding window may lead to the entire row being masked.\n+            # This then makes m_ij contain -inf, which causes NaNs in\n+            # exp().\n+            qk = tl.where((cur_batch_ctx_len + offs_m[:, None]) -\n+                          (start_n + offs_bs_n[None, :]) < SLIDING_WINDOW, qk,\n+                          -10000)\n+\n+        # compute running maximum\n+        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, axis=1)\n+        alpha = tl.exp(m_i - m_ij)\n+        acc = acc * alpha[:, None]\n+\n+        # update acc\n+        if start_n + BLOCK_SIZE > cur_batch_ctx_len or \\\n+            BLOCK_DMODEL != BLOCK_DMODEL_PADDED:\n+            v_load = tl.load(\n+                V_cache + off_v,\n+                mask=dim_mask[None, :] &\n+                ((start_n + offs_bs_n[:, None]) < cur_batch_ctx_len),\n+                other=0.0)  # [N,D]\n+        else:\n+            v_load = tl.load(V_cache + off_v)\n+\n+        if v_load.dtype.is_fp8():\n+            v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)\n+        else:\n+            v = v_load\n+        p = p.to(v.dtype)\n+\n+        acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)\n+        # # update m_i and l_i\n+        l_i = l_i * alpha + l_ij\n+        m_i = m_ij\n+\n+    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n+             offs_d[:, None] * stride_kd)\n+    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n+             offs_d[None, :] * stride_vd)\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+\n+    # block_mask is 0 when we're already past the current query length\n+    block_mask = tl.where(block_start_loc < cur_batch_query_len, 1, 0)\n+\n+    # compute query against itself (with causal mask)\n+    for start_n in tl.range(0, \\\n+                        block_mask * (start_m + 1) * BLOCK_M, BLOCK_N, \\\n+                        loop_unroll_factor=num_unroll_request):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs +\n+                    (cur_batch_in_all_start_index + start_n) * stride_kbs,\n+                    mask=dim_mask[:, None] &\n+                    ((start_n + offs_n[None, :]) < cur_batch_query_len),\n                     other=0.0)\n \n-        # # initialize pointer to m and l\n-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n-        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n-\n-        for start_n in range(0, cur_batch_ctx_len, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n-                         ((start_n + offs_n) // block_size) * stride_b_loc_s,\n-                         mask=(start_n + offs_n) < cur_batch_ctx_len,\n-                         other=0)\n-            off_k = (bn[None, :] * stride_k_cache_bs +\n-                     cur_kv_head * stride_k_cache_h +\n-                     (offs_d[:, None] // x) * stride_k_cache_d +\n-                     ((start_n + offs_n[None, :]) % block_size) *\n-                     stride_k_cache_bl +\n-                     (offs_d[:, None] % x) * stride_k_cache_x)\n-            off_v = (\n-                bn[:, None] * stride_v_cache_bs +\n-                cur_kv_head * stride_v_cache_h +\n-                offs_d[None, :] * stride_v_cache_d +\n-                (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)\n-            k = tl.load(K_cache + off_k,\n-                        mask=(start_n + offs_n[None, :]) < cur_batch_ctx_len,\n-                        other=0.0)\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk += tl.dot(q, k)\n-            qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n-                          float(\"-inf\"))\n-            qk *= sm_scale\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)\n-            m_i_new = tl.maximum(m_i, m_ij)\n-            p = tl.math.exp(qk - m_i_new[:, None])\n-            l_ij = tl.sum(p, 1)\n-            # -- update m_i and l_i\n-\n-            alpha = tl.math.exp(m_i - m_i_new)\n-            l_i_new = alpha * l_i + l_ij\n-            # -- update output accumulator --\n-            # scale p\n-            # scale acc\n-            acc_scale = alpha\n-            # acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v = tl.load(V_cache + off_v,\n-                        mask=(start_n + offs_n[:, None]) < cur_batch_ctx_len,\n-                        other=0.0)\n-\n-            p = p.to(v.dtype)\n-            acc += tl.dot(p, v)\n-            # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-\n-        off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n-                 offs_d[:, None] * stride_kd)\n-        off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n-                 offs_d[None, :] * stride_vd)\n-        k_ptrs = K + off_k\n-        v_ptrs = V + off_v\n-\n-        block_mask = tl.where(\n-            block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)\n-\n-        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            k = tl.load(k_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_kbs,\n-                        mask=(start_n + offs_n[None, :])\n-                        < cur_batch_seq_len - cur_batch_ctx_len,\n-                        other=0.0)\n-\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk += tl.dot(q, k)\n-            qk *= sm_scale\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n-                          float(\"-inf\"))\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)\n-            m_i_new = tl.maximum(m_i, m_ij)\n-            p = tl.math.exp(qk - m_i_new[:, None])\n-            l_ij = tl.sum(p, 1)\n-            # -- update m_i and l_i\n-\n-            alpha = tl.math.exp(m_i - m_i_new)\n-            l_i_new = alpha * l_i + l_ij\n-            # -- update output accumulator --\n-            # scale p\n-            # scale acc\n-            acc_scale = alpha\n-            # acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v = tl.load(v_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_vbs,\n-                        mask=(start_n + offs_n[:, None])\n-                        < cur_batch_seq_len - cur_batch_ctx_len,\n-                        other=0.0)\n-\n-            p = p.to(v.dtype)\n-            acc += tl.dot(p, v)\n-            # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-\n-        # acc /= l_i[:, None]\n-        # initialize pointers to output\n-        off_o = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n-            cur_head * stride_oh + offs_d[None, :] * stride_od)\n-        out_ptrs = Out + off_o\n-        tl.store(out_ptrs,\n-                 acc,\n-                 mask=offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len)\n-        return\n-\n-    @triton.jit\n-    def _fwd_kernel_alibi(\n-        Q,\n-        K,\n-        V,\n-        K_cache,\n-        V_cache,\n-        B_Loc,\n-        sm_scale,\n-        k_scale,\n-        v_scale,\n-        B_Start_Loc,\n-        B_Seqlen,\n-        Alibi_slopes,\n-        block_size,\n-        x,\n-        Out,\n-        stride_b_loc_b,\n-        stride_b_loc_s,\n-        stride_qbs,\n-        stride_qh,\n-        stride_qd,\n-        stride_kbs,\n-        stride_kh,\n-        stride_kd,\n-        stride_vbs,\n-        stride_vh,\n-        stride_vd,\n-        stride_obs,\n-        stride_oh,\n-        stride_od,\n-        stride_k_cache_bs,\n-        stride_k_cache_h,\n-        stride_k_cache_d,\n-        stride_k_cache_bl,\n-        stride_k_cache_x,\n-        stride_v_cache_bs,\n-        stride_v_cache_h,\n-        stride_v_cache_d,\n-        stride_v_cache_bl,\n-        num_queries_per_kv: int,\n-        IN_PRECISION: tl.constexpr,\n-        BLOCK_M: tl.constexpr,\n-        BLOCK_DMODEL: tl.constexpr,  # head size\n-        BLOCK_DMODEL_PADDED: tl.constexpr,  # head size padded to a power of 2\n-        BLOCK_N: tl.constexpr,\n-        SKIP_DECODE: tl.constexpr,\n-    ):\n-        # attn_bias[]\n-        cur_batch = tl.program_id(0)\n-        cur_head = tl.program_id(1)\n-        start_m = tl.program_id(2)\n-\n-        cur_kv_head = cur_head // num_queries_per_kv\n-\n-        # cur_batch_seq_len: the length of prompts\n-        # cur_batch_ctx_len: the length of prefix\n-        # cur_batch_in_all_start_index: the start id of the dim=0\n-        cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n-        cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n-        cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)\n-        cur_batch_query_len = (cur_batch_in_all_stop_index -\n-                               cur_batch_in_all_start_index)\n-        cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len\n-\n-        if SKIP_DECODE and cur_batch_query_len == 1:\n-            return\n-\n-        block_start_loc = BLOCK_M * start_m\n-\n-        # initialize offsets\n-        offs_n = tl.arange(0, BLOCK_N)\n-        offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)\n-        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n-        off_q = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n-            cur_head * stride_qh + offs_d[None, :] * stride_qd)\n-\n-        dim_mask = tl.where(\n-            tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1, 0).to(tl.int1)\n-\n-        q = tl.load(Q + off_q,\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+        qk *= sm_scale\n+        # apply causal mask\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n+                      float(\"-inf\"))\n+        if SLIDING_WINDOW > 0:\n+            qk = tl.where(\n+                offs_m[:, None] - (start_n + offs_n[None, :]) < SLIDING_WINDOW,\n+                qk, -10000)\n+\n+        # compute running maximum\n+        m_ij = tl.maximum(m_i, tl.max(qk, axis=1))\n+        p = tl.exp(qk - m_ij[:, None])\n+        l_ij = tl.sum(p, axis=1)\n+        alpha = tl.exp(m_i - m_ij)\n+        acc = acc * alpha[:, None]\n+\n+        # update acc\n+        v = tl.load(v_ptrs +\n+                    (cur_batch_in_all_start_index + start_n) * stride_vbs,\n                     mask=dim_mask[None, :] &\n-                    (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len),\n+                    ((start_n + offs_n[:, None]) < cur_batch_query_len),\n+                    other=0.0)\n+        p = p.to(v.dtype)\n+\n+        acc = tl.dot(p, v, acc=acc, input_precision=IN_PRECISION)\n+        # update m_i and l_i\n+        l_i = l_i * alpha + l_ij\n+        m_i = m_ij\n+\n+    acc = acc / l_i[:, None]\n+\n+    # initialize pointers to output\n+    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n+             cur_head * stride_oh + offs_d[None, :] * stride_od)\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs,\n+             acc,\n+             mask=dim_mask[None, :] & (offs_m[:, None] < cur_batch_query_len))\n+    return\n+\n+\n+@triton.jit\n+def _fwd_kernel_flash_attn_v2(\n+    Q,\n+    K,\n+    V,\n+    K_cache,\n+    V_cache,\n+    B_Loc,\n+    sm_scale,\n+    B_Start_Loc,\n+    B_Seqlen,\n+    B_Ctxlen,\n+    block_size,\n+    x,\n+    Out,\n+    stride_b_loc_b,\n+    stride_b_loc_s,\n+    stride_qbs,\n+    stride_qh,\n+    stride_qd,\n+    stride_kbs,\n+    stride_kh,\n+    stride_kd,\n+    stride_vbs,\n+    stride_vh,\n+    stride_vd,\n+    stride_obs,\n+    stride_oh,\n+    stride_od,\n+    stride_k_cache_bs,\n+    stride_k_cache_h,\n+    stride_k_cache_d,\n+    stride_k_cache_bl,\n+    stride_k_cache_x,\n+    stride_v_cache_bs,\n+    stride_v_cache_h,\n+    stride_v_cache_d,\n+    stride_v_cache_bl,\n+    num_queries_per_kv: int,\n+    BLOCK_M: tl.constexpr,\n+    BLOCK_DMODEL: tl.constexpr,\n+    BLOCK_N: tl.constexpr,\n+):\n+    cur_batch = tl.program_id(0)\n+    cur_head = tl.program_id(1)\n+    start_m = tl.program_id(2)\n+\n+    cur_kv_head = cur_head // num_queries_per_kv\n+\n+    cur_batch_ctx_len = tl.load(B_Ctxlen + cur_batch)\n+    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n+    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n+\n+    block_start_loc = BLOCK_M * start_m\n+\n+    # initialize offsets\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n+             cur_head * stride_qh + offs_d[None, :] * stride_qd)\n+\n+    q = tl.load(Q + off_q,\n+                mask=offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len,\n+                other=0.0)\n+\n+    # # initialize pointer to m and l\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\n+\n+    for start_n in range(0, cur_batch_ctx_len, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n+                     ((start_n + offs_n) // block_size) * stride_b_loc_s,\n+                     mask=(start_n + offs_n) < cur_batch_ctx_len,\n+                     other=0)\n+        off_k = (\n+            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +\n+            (offs_d[:, None] // x) * stride_k_cache_d +\n+            ((start_n + offs_n[None, :]) % block_size) * stride_k_cache_bl +\n+            (offs_d[:, None] % x) * stride_k_cache_x)\n+        off_v = (bn[:, None] * stride_v_cache_bs +\n+                 cur_kv_head * stride_v_cache_h +\n+                 offs_d[None, :] * stride_v_cache_d +\n+                 (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)\n+        k = tl.load(K_cache + off_k,\n+                    mask=(start_n + offs_n[None, :]) < cur_batch_ctx_len,\n+                    other=0.0)\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k)\n+        qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n+                      float(\"-inf\"))\n+        qk *= sm_scale\n+\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        p = tl.math.exp(qk - m_i_new[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+\n+        alpha = tl.math.exp(m_i - m_i_new)\n+        l_i_new = alpha * l_i + l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        # scale acc\n+        acc_scale = alpha\n+        # acc_scale = l_i / l_i_new * alpha\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(V_cache + off_v,\n+                    mask=(start_n + offs_n[:, None]) < cur_batch_ctx_len,\n+                    other=0.0)\n+\n+        p = p.to(v.dtype)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n+             offs_d[:, None] * stride_kd)\n+    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n+             offs_d[None, :] * stride_vd)\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+\n+    block_mask = tl.where(\n+        block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)\n+\n+    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        k = tl.load(k_ptrs +\n+                    (cur_batch_in_all_start_index + start_n) * stride_kbs,\n+                    mask=(start_n + offs_n[None, :])\n+                    < cur_batch_seq_len - cur_batch_ctx_len,\n                     other=0.0)\n \n-        # # initialize pointer to m and l\n-        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n-        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n-        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED], dtype=tl.float32)\n-\n-        alibi_slope = tl.load(Alibi_slopes + cur_head)\n-        alibi_start_q = tl.arange(\n-            0, BLOCK_M) + block_start_loc + cur_batch_ctx_len\n-        alibi_start_k = 0\n-        for start_n in range(0, cur_batch_ctx_len, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n-                         ((start_n + offs_n) // block_size) * stride_b_loc_s,\n-                         mask=(start_n + offs_n) < cur_batch_ctx_len,\n-                         other=0)\n-            off_k = (bn[None, :] * stride_k_cache_bs +\n-                     cur_kv_head * stride_k_cache_h +\n-                     (offs_d[:, None] // x) * stride_k_cache_d +\n-                     ((start_n + offs_n[None, :]) % block_size) *\n-                     stride_k_cache_bl +\n-                     (offs_d[:, None] % x) * stride_k_cache_x)\n-            off_v = (\n-                bn[:, None] * stride_v_cache_bs +\n-                cur_kv_head * stride_v_cache_h +\n-                offs_d[None, :] * stride_v_cache_d +\n-                (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)\n-            k_load = tl.load(K_cache + off_k,\n-                             mask=dim_mask[:, None] &\n-                             ((start_n + offs_n[None, :]) < cur_batch_ctx_len),\n-                             other=0.0)  # [D,N]\n-\n-            if k_load.dtype.is_fp8():\n-                k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)\n-            else:\n-                k = k_load\n-\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n-            qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n-                          float(\"-inf\"))\n-            qk *= sm_scale\n-\n-            # load alibi\n-            alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -\n-                     alibi_start_q[:, None]) * alibi_slope\n-            alibi = tl.where(\n-                (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len),\n-                alibi, float(\"-inf\"))\n-            qk += alibi\n-            alibi_start_k += BLOCK_N\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)\n-            m_i_new = tl.maximum(m_i, m_ij)\n-            p = tl.math.exp(qk - m_i_new[:, None])\n-            l_ij = tl.sum(p, 1)\n-            # -- update m_i and l_i\n-\n-            alpha = tl.math.exp(m_i - m_i_new)\n-            l_i_new = alpha * l_i + l_ij\n-            # -- update output accumulator --\n-            # scale p\n-            # scale acc\n-            acc_scale = alpha\n-            # acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v_load = tl.load(V_cache + off_v,\n-                             mask=dim_mask[None, :] &\n-                             ((start_n + offs_n[:, None]) < cur_batch_ctx_len),\n-                             other=0.0)\n-            if v_load.dtype.is_fp8():\n-                v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)\n-            else:\n-                v = v_load\n-            p = p.to(v.dtype)\n-\n-            acc = tl.dot(p, v, acc=acc, input_precision='ieee')\n-            # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-\n-        off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n-                 offs_d[:, None] * stride_kd)\n-        off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n-                 offs_d[None, :] * stride_vd)\n-        k_ptrs = K + off_k\n-        v_ptrs = V + off_v\n-\n-        block_mask = tl.where(\n-            block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)\n-\n-        # init alibi\n-        alibi_slope = tl.load(Alibi_slopes + cur_head)\n-        alibi_start_q = tl.arange(\n-            0, BLOCK_M) + block_start_loc + cur_batch_ctx_len\n-        alibi_start_k = cur_batch_ctx_len\n-        # # init debugger\n-        # offset_db_q = tl.arange(0, BLOCK_M) + block_start_loc\n-        # offset_db_k = tl.arange(0, BLOCK_N)\n-        # calc q[BLOCK_M, BLOCK_MODEL] mul k[prefix_len: , BLOCK_DMODEL]\n-        for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n-            start_n = tl.multiple_of(start_n, BLOCK_N)\n-            # -- compute qk ----\n-            k = tl.load(k_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_kbs,\n-                        mask=dim_mask[:, None] &\n-                        ((start_n + offs_n[None, :])\n-                         < cur_batch_seq_len - cur_batch_ctx_len),\n-                        other=0.0)\n-\n-            qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n-            qk = tl.dot(q, k, acc=qk, input_precision='ieee')\n-            qk *= sm_scale\n-            qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n-                          float(\"-inf\"))\n-\n-            # load alibi\n-            alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -\n-                     alibi_start_q[:, None]) * alibi_slope\n-            alibi = tl.where(\n-                (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len),\n-                alibi, float(\"-inf\"))\n-            qk += alibi\n-            alibi_start_k += BLOCK_N\n-\n-            # -- compute m_ij, p, l_ij\n-            m_ij = tl.max(qk, 1)\n-            m_i_new = tl.maximum(m_i, m_ij)\n-            p = tl.math.exp(qk - m_i_new[:, None])\n-            l_ij = tl.sum(p, 1)\n-            # -- update m_i and l_i\n-\n-            alpha = tl.math.exp(m_i - m_i_new)\n-            l_i_new = alpha * l_i + l_ij\n-            # -- update output accumulator --\n-            # scale p\n-            # scale acc\n-            acc_scale = alpha\n-            # acc_scale = l_i / l_i_new * alpha\n-            acc = acc * acc_scale[:, None]\n-            # update acc\n-            v = tl.load(v_ptrs +\n-                        (cur_batch_in_all_start_index + start_n) * stride_vbs,\n-                        mask=dim_mask[None, :] &\n-                        ((start_n + offs_n[:, None])\n-                         < cur_batch_seq_len - cur_batch_ctx_len),\n-                        other=0.0)\n-            p = p.to(v.dtype)\n-\n-            acc = tl.dot(p, v, acc=acc, input_precision='ieee')\n-            # update m_i and l_i\n-            l_i = l_i_new\n-            m_i = m_i_new\n-\n-        acc = acc / l_i[:, None]\n-\n-        # initialize pointers to output\n-        off_o = (\n-            (cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n-            cur_head * stride_oh + offs_d[None, :] * stride_od)\n-        out_ptrs = Out + off_o\n-        tl.store(out_ptrs,\n-                 acc,\n-                 mask=dim_mask[None, :] &\n-                 (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len))\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk += tl.dot(q, k)\n+        qk *= sm_scale\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n+                      float(\"-inf\"))\n+\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        p = tl.math.exp(qk - m_i_new[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+\n+        alpha = tl.math.exp(m_i - m_i_new)\n+        l_i_new = alpha * l_i + l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        # scale acc\n+        acc_scale = alpha\n+        # acc_scale = l_i / l_i_new * alpha\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(v_ptrs +\n+                    (cur_batch_in_all_start_index + start_n) * stride_vbs,\n+                    mask=(start_n + offs_n[:, None])\n+                    < cur_batch_seq_len - cur_batch_ctx_len,\n+                    other=0.0)\n+\n+        p = p.to(v.dtype)\n+        acc += tl.dot(p, v)\n+        # update m_i and l_i\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    # acc /= l_i[:, None]\n+    # initialize pointers to output\n+    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n+             cur_head * stride_oh + offs_d[None, :] * stride_od)\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs,\n+             acc,\n+             mask=offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len)\n+    return\n+\n+\n+@triton.jit\n+def _fwd_kernel_alibi(\n+    Q,\n+    K,\n+    V,\n+    K_cache,\n+    V_cache,\n+    B_Loc,\n+    sm_scale,\n+    k_scale,\n+    v_scale,\n+    B_Start_Loc,\n+    B_Seqlen,\n+    Alibi_slopes,\n+    block_size,\n+    x,\n+    Out,\n+    stride_b_loc_b,\n+    stride_b_loc_s,\n+    stride_qbs,\n+    stride_qh,\n+    stride_qd,\n+    stride_kbs,\n+    stride_kh,\n+    stride_kd,\n+    stride_vbs,\n+    stride_vh,\n+    stride_vd,\n+    stride_obs,\n+    stride_oh,\n+    stride_od,\n+    stride_k_cache_bs,\n+    stride_k_cache_h,\n+    stride_k_cache_d,\n+    stride_k_cache_bl,\n+    stride_k_cache_x,\n+    stride_v_cache_bs,\n+    stride_v_cache_h,\n+    stride_v_cache_d,\n+    stride_v_cache_bl,\n+    num_queries_per_kv: int,\n+    IN_PRECISION: tl.constexpr,\n+    BLOCK_M: tl.constexpr,\n+    BLOCK_DMODEL: tl.constexpr,  # head size\n+    BLOCK_DMODEL_PADDED: tl.constexpr,  # head size padded to a power of 2\n+    BLOCK_N: tl.constexpr,\n+    SKIP_DECODE: tl.constexpr,\n+):\n+    # attn_bias[]\n+    cur_batch = tl.program_id(0)\n+    cur_head = tl.program_id(1)\n+    start_m = tl.program_id(2)\n+\n+    cur_kv_head = cur_head // num_queries_per_kv\n+\n+    # cur_batch_seq_len: the length of prompts\n+    # cur_batch_ctx_len: the length of prefix\n+    # cur_batch_in_all_start_index: the start id of the dim=0\n+    cur_batch_seq_len = tl.load(B_Seqlen + cur_batch)\n+    cur_batch_in_all_start_index = tl.load(B_Start_Loc + cur_batch)\n+    cur_batch_in_all_stop_index = tl.load(B_Start_Loc + cur_batch + 1)\n+    cur_batch_query_len = (cur_batch_in_all_stop_index -\n+                           cur_batch_in_all_start_index)\n+    cur_batch_ctx_len = cur_batch_seq_len - cur_batch_query_len\n+\n+    if SKIP_DECODE and cur_batch_query_len == 1:\n         return\n \n-    @torch.inference_mode()\n-    def context_attention_fwd(q,\n-                              k,\n-                              v,\n-                              o,\n-                              kv_cache_dtype: str,\n-                              k_cache,\n-                              v_cache,\n-                              b_loc,\n-                              b_start_loc,\n-                              b_seq_len,\n-                              max_seq_len,\n-                              max_input_len,\n-                              k_scale: torch.Tensor,\n-                              v_scale: torch.Tensor,\n-                              alibi_slopes=None,\n-                              sliding_window=None,\n-                              sm_scale=None,\n-                              skip_decode=False):\n-\n-        q_dtype_is_f32 = q.dtype is torch.float32\n+    block_start_loc = BLOCK_M * start_m\n+\n+    # initialize offsets\n+    offs_n = tl.arange(0, BLOCK_N)\n+    offs_d = tl.arange(0, BLOCK_DMODEL_PADDED)\n+    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\n+    off_q = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_qbs +\n+             cur_head * stride_qh + offs_d[None, :] * stride_qd)\n+\n+    dim_mask = tl.where(\n+        tl.arange(0, BLOCK_DMODEL_PADDED) < BLOCK_DMODEL, 1, 0).to(tl.int1)\n+\n+    q = tl.load(Q + off_q,\n+                mask=dim_mask[None, :] &\n+                (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len),\n+                other=0.0)\n+\n+    # # initialize pointer to m and l\n+    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")\n+    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)\n+    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL_PADDED], dtype=tl.float32)\n+\n+    alibi_slope = tl.load(Alibi_slopes + cur_head)\n+    alibi_start_q = tl.arange(0, BLOCK_M) + block_start_loc + cur_batch_ctx_len\n+    alibi_start_k = 0\n+    for start_n in range(0, cur_batch_ctx_len, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        bn = tl.load(B_Loc + cur_batch * stride_b_loc_b +\n+                     ((start_n + offs_n) // block_size) * stride_b_loc_s,\n+                     mask=(start_n + offs_n) < cur_batch_ctx_len,\n+                     other=0)\n+        off_k = (\n+            bn[None, :] * stride_k_cache_bs + cur_kv_head * stride_k_cache_h +\n+            (offs_d[:, None] // x) * stride_k_cache_d +\n+            ((start_n + offs_n[None, :]) % block_size) * stride_k_cache_bl +\n+            (offs_d[:, None] % x) * stride_k_cache_x)\n+        off_v = (bn[:, None] * stride_v_cache_bs +\n+                 cur_kv_head * stride_v_cache_h +\n+                 offs_d[None, :] * stride_v_cache_d +\n+                 (start_n + offs_n[:, None]) % block_size * stride_v_cache_bl)\n+        k_load = tl.load(K_cache + off_k,\n+                         mask=dim_mask[:, None] &\n+                         ((start_n + offs_n[None, :]) < cur_batch_ctx_len),\n+                         other=0.0)  # [D,N]\n+\n+        if k_load.dtype.is_fp8():\n+            k = (k_load.to(tl.float32) * tl.load(k_scale)).to(q.dtype)\n+        else:\n+            k = k_load\n+\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk = tl.dot(q, k, acc=qk, input_precision=IN_PRECISION)\n+        qk = tl.where((start_n + offs_n[None, :]) < cur_batch_ctx_len, qk,\n+                      float(\"-inf\"))\n+        qk *= sm_scale\n+\n+        # load alibi\n+        alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -\n+                 alibi_start_q[:, None]) * alibi_slope\n+        alibi = tl.where(\n+            (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len), alibi,\n+            float(\"-inf\"))\n+        qk += alibi\n+        alibi_start_k += BLOCK_N\n+\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        p = tl.math.exp(qk - m_i_new[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+\n+        alpha = tl.math.exp(m_i - m_i_new)\n+        l_i_new = alpha * l_i + l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        # scale acc\n+        acc_scale = alpha\n+        # acc_scale = l_i / l_i_new * alpha\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v_load = tl.load(V_cache + off_v,\n+                         mask=dim_mask[None, :] &\n+                         ((start_n + offs_n[:, None]) < cur_batch_ctx_len),\n+                         other=0.0)\n+        if v_load.dtype.is_fp8():\n+            v = (v_load.to(tl.float32) * tl.load(v_scale)).to(q.dtype)\n+        else:\n+            v = v_load\n+        p = p.to(v.dtype)\n+\n+        acc = tl.dot(p, v, acc=acc, input_precision='ieee')\n+        # update m_i and l_i\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    off_k = (offs_n[None, :] * stride_kbs + cur_kv_head * stride_kh +\n+             offs_d[:, None] * stride_kd)\n+    off_v = (offs_n[:, None] * stride_vbs + cur_kv_head * stride_vh +\n+             offs_d[None, :] * stride_vd)\n+    k_ptrs = K + off_k\n+    v_ptrs = V + off_v\n+\n+    block_mask = tl.where(\n+        block_start_loc < cur_batch_seq_len - cur_batch_ctx_len, 1, 0)\n+\n+    # init alibi\n+    alibi_slope = tl.load(Alibi_slopes + cur_head)\n+    alibi_start_q = tl.arange(0, BLOCK_M) + block_start_loc + cur_batch_ctx_len\n+    alibi_start_k = cur_batch_ctx_len\n+    # # init debugger\n+    # offset_db_q = tl.arange(0, BLOCK_M) + block_start_loc\n+    # offset_db_k = tl.arange(0, BLOCK_N)\n+    # calc q[BLOCK_M, BLOCK_MODEL] mul k[prefix_len: , BLOCK_DMODEL]\n+    for start_n in range(0, block_mask * (start_m + 1) * BLOCK_M, BLOCK_N):\n+        start_n = tl.multiple_of(start_n, BLOCK_N)\n+        # -- compute qk ----\n+        k = tl.load(\n+            k_ptrs + (cur_batch_in_all_start_index + start_n) * stride_kbs,\n+            mask=dim_mask[:, None] & ((start_n + offs_n[None, :])\n+                                      < cur_batch_seq_len - cur_batch_ctx_len),\n+            other=0.0)\n+\n+        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n+        qk = tl.dot(q, k, acc=qk, input_precision='ieee')\n+        qk *= sm_scale\n+        qk = tl.where(offs_m[:, None] >= (start_n + offs_n[None, :]), qk,\n+                      float(\"-inf\"))\n+\n+        # load alibi\n+        alibi = (tl.arange(0, BLOCK_N)[None, :] + alibi_start_k -\n+                 alibi_start_q[:, None]) * alibi_slope\n+        alibi = tl.where(\n+            (alibi <= 0) & (alibi_start_q[:, None] < cur_batch_seq_len), alibi,\n+            float(\"-inf\"))\n+        qk += alibi\n+        alibi_start_k += BLOCK_N\n+\n+        # -- compute m_ij, p, l_ij\n+        m_ij = tl.max(qk, 1)\n+        m_i_new = tl.maximum(m_i, m_ij)\n+        p = tl.math.exp(qk - m_i_new[:, None])\n+        l_ij = tl.sum(p, 1)\n+        # -- update m_i and l_i\n+\n+        alpha = tl.math.exp(m_i - m_i_new)\n+        l_i_new = alpha * l_i + l_ij\n+        # -- update output accumulator --\n+        # scale p\n+        # scale acc\n+        acc_scale = alpha\n+        # acc_scale = l_i / l_i_new * alpha\n+        acc = acc * acc_scale[:, None]\n+        # update acc\n+        v = tl.load(\n+            v_ptrs + (cur_batch_in_all_start_index + start_n) * stride_vbs,\n+            mask=dim_mask[None, :] & ((start_n + offs_n[:, None])\n+                                      < cur_batch_seq_len - cur_batch_ctx_len),\n+            other=0.0)\n+        p = p.to(v.dtype)\n+\n+        acc = tl.dot(p, v, acc=acc, input_precision='ieee')\n+        # update m_i and l_i\n+        l_i = l_i_new\n+        m_i = m_i_new\n+\n+    acc = acc / l_i[:, None]\n+\n+    # initialize pointers to output\n+    off_o = ((cur_batch_in_all_start_index + offs_m[:, None]) * stride_obs +\n+             cur_head * stride_oh + offs_d[None, :] * stride_od)\n+    out_ptrs = Out + off_o\n+    tl.store(out_ptrs,\n+             acc,\n+             mask=dim_mask[None, :] &\n+             (offs_m[:, None] < cur_batch_seq_len - cur_batch_ctx_len))\n+    return\n+\n+\n+@torch.inference_mode()\n+def context_attention_fwd(q,\n+                          k,\n+                          v,\n+                          o,\n+                          kv_cache_dtype: str,\n+                          k_cache,\n+                          v_cache,\n+                          b_loc,\n+                          b_start_loc,\n+                          b_seq_len,\n+                          max_seq_len,\n+                          max_input_len,\n+                          k_scale: torch.Tensor,\n+                          v_scale: torch.Tensor,\n+                          alibi_slopes=None,\n+                          sliding_window=None,\n+                          sm_scale=None,\n+                          skip_decode=False):\n+\n+    q_dtype_is_f32 = q.dtype is torch.float32\n+\n+    # Turing does have tensor core for float32 multiplication\n+    # use ieee as fallback for triton kernels work. There is also\n+    # warning on vllm/config.py to inform users this fallback\n+    # implementation\n+    IN_PRECISION = 'ieee' if IS_TURING and q_dtype_is_f32 else None\n+\n+    # Conversion of FP8 Tensor from uint8 storage to\n+    # appropriate torch.dtype for interpretation by Triton\n+    if \"fp8\" in kv_cache_dtype:\n+        assert (k_cache.dtype == torch.uint8)\n+        assert (v_cache.dtype == torch.uint8)\n+\n+        if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):\n+            target_dtype = current_platform.fp8_dtype()\n+        elif kv_cache_dtype == \"fp8_e5m2\":\n+            target_dtype = torch.float8_e5m2\n+        else:\n+            raise ValueError(\"Unsupported FP8 dtype:\", kv_cache_dtype)\n+\n+        k_cache = k_cache.view(target_dtype)\n+        v_cache = v_cache.view(target_dtype)\n+\n+    if (k_cache.dtype == torch.uint8\n+            or v_cache.dtype == torch.uint8 and kv_cache_dtype == \"auto\"):\n+        raise ValueError(\"kv_cache_dtype='auto' unsupported for\\\n+            FP8 KV Cache prefill kernel\")\n+\n+    # shape constraints\n+    Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n+    assert Lq == Lk and Lk == Lv\n+    # round up Lk to a power of 2 - this is required for Triton block size\n+    Lk_padded = triton.next_power_of_2(Lk)\n+\n+    if sm_scale is None:\n+        sm_scale = 1.0 / (Lq**0.5)\n+    batch, head = b_seq_len.shape[0], q.shape[1]\n+    num_queries_per_kv = q.shape[1] // k.shape[1]\n+\n+    assert batch + 1 == len(b_start_loc)\n+\n+    # 0 means \"disable\"\n+    if sliding_window is None or sliding_window <= 0:\n+        sliding_window = 0\n+\n+    if alibi_slopes is not None:\n         # need to reduce num. blocks when using fp32\n         # due to increased use of GPU shared memory\n         # if q.dtype is torch.float32:\n         BLOCK = BASE_BLOCK // 2 if q_dtype_is_f32 else BASE_BLOCK\n-\n-        # Turing does have tensor core for float32 multiplication\n-        # use ieee as fallback for triton kernels work. There is also\n-        # warning on vllm/config.py to inform users this fallback\n-        # implementation\n-        IN_PRECISION = 'ieee' if IS_TURING and q_dtype_is_f32 else None\n-\n-        # Conversion of FP8 Tensor from uint8 storage to\n-        # appropriate torch.dtype for interpretation by Triton\n-        if \"fp8\" in kv_cache_dtype:\n-            assert (k_cache.dtype == torch.uint8)\n-            assert (v_cache.dtype == torch.uint8)\n-\n-            if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):\n-                target_dtype = current_platform.fp8_dtype()\n-            elif kv_cache_dtype == \"fp8_e5m2\":\n-                target_dtype = torch.float8_e5m2\n-            else:\n-                raise ValueError(\"Unsupported FP8 dtype:\", kv_cache_dtype)\n-\n-            k_cache = k_cache.view(target_dtype)\n-            v_cache = v_cache.view(target_dtype)\n-\n-        if (k_cache.dtype == torch.uint8\n-                or v_cache.dtype == torch.uint8 and kv_cache_dtype == \"auto\"):\n-            raise ValueError(\"kv_cache_dtype='auto' unsupported for\\\n-                FP8 KV Cache prefill kernel\")\n-\n-        # shape constraints\n-        Lq, Lk, Lv = q.shape[-1], k.shape[-1], v.shape[-1]\n-        assert Lq == Lk and Lk == Lv\n-        # round up Lk to a power of 2 - this is required for Triton block size\n-        Lk_padded = triton.next_power_of_2(Lk)\n-\n-        if sm_scale is None:\n-            sm_scale = 1.0 / (Lq**0.5)\n-        batch, head = b_seq_len.shape[0], q.shape[1]\n-        num_queries_per_kv = q.shape[1] // k.shape[1]\n-\n-        assert batch + 1 == len(b_start_loc)\n-        grid = (batch, head, triton.cdiv(max_input_len, BLOCK))  # batch, head,\n-\n-        # 0 means \"disable\"\n-        if sliding_window is None or sliding_window <= 0:\n-            sliding_window = 0\n-\n-        if alibi_slopes is not None:\n-            _fwd_kernel_alibi[grid](\n-                q,\n-                k,\n-                v,\n-                k_cache,\n-                v_cache,\n-                b_loc,\n-                sm_scale,\n-                k_scale,\n-                v_scale,\n-                b_start_loc,\n-                b_seq_len,\n-                alibi_slopes,\n-                v_cache.shape[3],\n-                k_cache.shape[4],\n-                o,\n-                b_loc.stride(0),\n-                b_loc.stride(1),\n-                q.stride(0),\n-                q.stride(1),\n-                q.stride(2),\n-                k.stride(0),\n-                k.stride(1),\n-                k.stride(2),\n-                v.stride(0),\n-                v.stride(1),\n-                v.stride(2),\n-                o.stride(0),\n-                o.stride(1),\n-                o.stride(2),\n-                k_cache.stride(0),\n-                k_cache.stride(1),\n-                k_cache.stride(2),\n-                k_cache.stride(3),\n-                k_cache.stride(\n-                    4\n-                ),  #[num_blocks, num_kv_heads, head_size/x, block_size, x]\n-                v_cache.stride(0),\n-                v_cache.stride(1),\n-                v_cache.stride(2),\n-                v_cache.stride(\n-                    3),  #[num_blocks, num_kv_heads, head_size, block_size]\n-                num_queries_per_kv=num_queries_per_kv,\n-                IN_PRECISION=IN_PRECISION,\n-                BLOCK_M=BLOCK,\n-                BLOCK_DMODEL=Lk,\n-                BLOCK_DMODEL_PADDED=Lk_padded,\n-                BLOCK_N=BLOCK,\n-                SKIP_DECODE=skip_decode,\n-                num_warps=NUM_WARPS,\n-                num_stages=1,\n-            )\n-            return\n-\n-        _fwd_kernel[grid](\n+        # batch, head,\n+        grid = (batch, head, triton.cdiv(max_input_len, BLOCK))\n+        _fwd_kernel_alibi[grid](\n             q,\n             k,\n             v,\n@@ -852,6 +799,7 @@ if triton.__version__ >= \"2.1.0\":\n             v_scale,\n             b_start_loc,\n             b_seq_len,\n+            alibi_slopes,\n             v_cache.shape[3],\n             k_cache.shape[4],\n             o,\n@@ -886,9 +834,69 @@ if triton.__version__ >= \"2.1.0\":\n             BLOCK_DMODEL=Lk,\n             BLOCK_DMODEL_PADDED=Lk_padded,\n             BLOCK_N=BLOCK,\n-            SLIDING_WINDOW=sliding_window,\n             SKIP_DECODE=skip_decode,\n             num_warps=NUM_WARPS,\n             num_stages=1,\n         )\n         return\n+\n+    max_seq_len = 0 if max_seq_len is None else max_seq_len\n+    extra_kargs = {}\n+    if current_platform.is_rocm():\n+        extra_kargs = {\"kpack\": 2, \"waves_per_eu\": 2}\n+\n+    grid = lambda META: (batch, head,\n+                         triton.cdiv(max_input_len, META[\"BLOCK_M\"]))\n+    _fwd_kernel[grid](\n+        q,\n+        k,\n+        v,\n+        k_cache,\n+        v_cache,\n+        b_loc,\n+        sm_scale,\n+        k_scale,\n+        v_scale,\n+        b_start_loc,\n+        b_seq_len,\n+        k_cache.shape[4],\n+        o,\n+        b_loc.stride(0),\n+        b_loc.stride(1),\n+        q.stride(0),\n+        q.stride(1),\n+        q.stride(2),\n+        k.stride(0),\n+        k.stride(1),\n+        k.stride(2),\n+        v.stride(0),\n+        v.stride(1),\n+        v.stride(2),\n+        o.stride(0),\n+        o.stride(1),\n+        o.stride(2),\n+        k_cache.stride(0),\n+        k_cache.stride(1),\n+        k_cache.stride(2),\n+        k_cache.stride(3),\n+        k_cache.stride(\n+            4),  #[num_blocks, num_kv_heads, head_size/x, block_size, x]\n+        v_cache.stride(0),\n+        v_cache.stride(1),\n+        v_cache.stride(2),\n+        v_cache.stride(3),  #[num_blocks, num_kv_heads, head_size, block_size]\n+        BLOCK_SIZE=v_cache.shape[3],\n+        num_queries_per_kv=num_queries_per_kv,\n+        IN_PRECISION=IN_PRECISION,\n+        BLOCK_DMODEL=Lk,\n+        BLOCK_DMODEL_PADDED=Lk_padded,\n+        SLIDING_WINDOW=sliding_window,\n+        SKIP_DECODE=skip_decode,\n+        BLOCK_M=128,\n+        BLOCK_N=64,\n+        num_unroll_cache=4,\n+        num_unroll_request=1,\n+        num_warps=4,\n+        num_stages=1,\n+        **extra_kargs)\n+    return",
  "apis": [
    "vllm.attention.ops.prefix_prefill.context_attention_fwd"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/ops/prefix_prefill.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/layer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/layer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit makes extensive modifications to the production code in the source file \"vllm/attention/ops/prefix_prefill.py\" (in addition to minor test adjustment in a test file) and the extensive changes include rewriting kernels with new autotuning configs and parameter adjustments for the main attention computation. The changes are clearly aimed at speeding up the prefix prefill operation (which is a key performance component) by reconfiguring and tuning the kernel parameters. This is a non-trivial performance optimization change rather than a mere bug fix, refactor, or documentation update. The improvements target CPU performance and affect a high-level API in the repo. Therefore, all the conditions for a performance/optimization commit are satisfied.",
  "llm_api_reason": "This commit modifies both a test file and key parts of the Triton kernels in the prefix prefill implementation. In the test file the “block_size” parameter is updated from 8 to 16, while in the vllm/attention/ops/prefix_prefill.py file several Triton‐kernel functions (_fwd_kernel, _fwd_kernel_flash_attn_v2, and _fwd_kernel_alibi) are modified to improve performance (especially for ROCM) and adjust autotuning parameters. Ultimately, the publicly exposed API “context_attention_fwd” (decorated with @torch.inference_mode()) is affected, since it calls into these modified kernels. Other functions are internal helpers and are not considered high‐level APIs, so the affected API is the context_attention_fwd entry point in the prefix_prefill module."
}