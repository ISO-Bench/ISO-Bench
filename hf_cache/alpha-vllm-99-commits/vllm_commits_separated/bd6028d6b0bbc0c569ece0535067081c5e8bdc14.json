{
  "commit_hash": "bd6028d6b0bbc0c569ece0535067081c5e8bdc14",
  "pr_url": "https://github.com/vllm-project/vllm/pull/16512",
  "pr_date": "2025-04-12",
  "timeline_text": "Copy link Member mgoin commented Apr 11, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Clear speedup for latency case, adapted from sgl-project/sglang@ 86a876d (thank you!) Llama Scout FP8 on 2xH100, input/output=1000/1000 batch_size=1 # benchmark\npython benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000 --batch-size 1 --num-iters-warmup 5 --num-iters 5 \n\n# torch.topk\nAvg latency: 12.93838309822604 seconds\n10% percentile latency: 12.891319572227076 seconds\n25% percentile latency: 12.904249292099848 seconds\n50% percentile latency: 12.921604027971625 seconds\n75% percentile latency: 12.932637538062409 seconds\n90% percentile latency: 13.00348993963562 seconds\n99% percentile latency: 13.046001380579547 seconds\n\n# fast_topk\nAvg latency: 12.725665437569841 seconds\n10% percentile latency: 12.664348530210555 seconds\n25% percentile latency: 12.665923552820459 seconds\n50% percentile latency: 12.72062187595293 seconds\n75% percentile latency: 12.734881401993334 seconds\n90% percentile latency: 12.800113665964455 seconds\n99% percentile latency: 12.839253024347126 seconds Llama Scout FP8 on 2xH100, input/output=1000/1000 batch_size=32 # benchmark\npython benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000 --batch-size 32 --num-iters-warmup 3 --num-iters 3\n\n# torch.topk\nAvg latency: 23.997261434715863 seconds\n10% percentile latency: 23.722837531426922 seconds\n25% percentile latency: 23.844304106081836 seconds\n50% percentile latency: 24.04674839717336 seconds\n75% percentile latency: 24.174962244578637 seconds\n90% percentile latency: 24.251890553021802 seconds\n99% percentile latency: 24.298047538087705 seconds\n\n# fast_topk\nAvg latency: 23.815591983729973 seconds\n10% percentile latency: 23.6753818389494 seconds\n25% percentile latency: 23.733925551641732 seconds\n50% percentile latency: 23.831498406128958 seconds\n75% percentile latency: 23.905211627017707 seconds\n90% percentile latency: 23.949439559550957 seconds\n99% percentile latency: 23.975976319070906 seconds Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üéâ 1 yeqcharlotte reacted with hooray emoji All reactions üéâ 1 reaction Optimized topk for topk=1 (Llama-4) ‚Ä¶ a22a82d Signed-off-by: mgoin <mgoin64@gmail.com> Copy link github-actions bot commented Apr 11, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . njhill approved these changes Apr 11, 2025 View reviewed changes Copy link Member njhill left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Wow, nice! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member njhill commented Apr 11, 2025 @mgoin could we use this for other moes too? e.g. in https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py#L886 ? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author mgoin commented Apr 11, 2025 @njhill unfortunately most other moes do not use a topk=1 AFAIK, but maybe the overhead is minimal enough to use just in case üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin added performance Performance-related issues ready ONLY add when PR is ready to merge/full CI is needed labels Apr 12, 2025 houseroad approved these changes Apr 12, 2025 View reviewed changes Copy link Collaborator houseroad left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Oh, nice trick. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details DarkLight1337 merged commit bd6028d into vllm-project : main Apr 12, 2025 64 checks passed Uh oh! There was an error while loading. Please reload this page . yangw-dev pushed a commit\n        to yangw-dev/vllm\n      that referenced\n      this pull request Apr 21, 2025 Optimized topk for topk=1 (Llama-4) ( vllm-project#16512 ) ‚Ä¶ 751844d Signed-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Yang Wang <elainewy@meta.com> jikunshang pushed a commit\n        to jikunshang/vllm\n      that referenced\n      this pull request Apr 29, 2025 Optimized topk for topk=1 (Llama-4) ( vllm-project#16512 ) ‚Ä¶ e6bca68 Signed-off-by: mgoin <mgoin64@gmail.com> lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 Optimized topk for topk=1 (Llama-4) ( vllm-project#16512 ) ‚Ä¶ ef7a8ef Signed-off-by: mgoin <mgoin64@gmail.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 Optimized topk for topk=1 (Llama-4) ( vllm-project#16512 ) ‚Ä¶ 7987452 Signed-off-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:28",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:51:28",
  "models": [
    "RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic,tensor_parallel_size=2 --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_latency.py --model RedHatAI/Llama-4-Scout-17B-16E-Instruct-FP8-dynamic --max-model-len 8000 --tensor-parallel-size 2 --input-len 1000 --output-len 1000 --batch-size 1 --num-iters-warmup 5 --num-iters 5",
  "commit_subject": "Optimized topk for topk=1 (Llama-4) (#16512)",
  "commit_message": "Optimized topk for topk=1 (Llama-4) (#16512)\n\nSigned-off-by: mgoin <mgoin64@gmail.com>",
  "commit_date": "2025-04-12T14:21:08+08:00",
  "files_changed": [
    "vllm/model_executor/models/llama4.py",
    "vllm/model_executor/models/utils.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 3,
    "num_edited_lines": 13,
    "num_non_test_edited_lines": 13,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/models/llama4.py b/vllm/model_executor/models/llama4.py\nindex 8785e9dcf..51efbfe20 100644\n--- a/vllm/model_executor/models/llama4.py\n+++ b/vllm/model_executor/models/llama4.py\n@@ -37,7 +37,7 @@ from vllm.model_executor.layers.rotary_embedding import get_rope\n from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n \n from .llama import LlamaForCausalLM, LlamaMLP, LlamaModel\n-from .utils import (AutoWeightsLoader, extract_layer_index,\n+from .utils import (AutoWeightsLoader, extract_layer_index, fast_topk,\n                     is_pp_missing_parameter)\n \n \n@@ -50,7 +50,7 @@ class Llama4MoE(nn.Module):\n         topk: int,\n         renormalize: bool,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n-        router_scores, router_indices = torch.topk(gating_output, topk, dim=-1)\n+        router_scores, router_indices = fast_topk(gating_output, topk, dim=-1)\n         router_scores = torch.sigmoid(router_scores.float()).to(\n             hidden_states.dtype)\n         return (router_scores, router_indices.to(torch.int32))\ndiff --git a/vllm/model_executor/models/utils.py b/vllm/model_executor/models/utils.py\nindex f197434f3..7ed0560ee 100644\n--- a/vllm/model_executor/models/utils.py\n+++ b/vllm/model_executor/models/utils.py\n@@ -703,3 +703,12 @@ def cast_overflow_tensors(\n         clamp_value = torch.finfo(tensors.dtype).max - offset\n         tensors = torch.clamp(tensors, min=-clamp_value, max=clamp_value)\n     return tensors\n+\n+\n+def fast_topk(values, topk, dim):\n+    if topk == 1:\n+        # Use max along the specified dimension to get both value and index\n+        return torch.max(values, dim=dim, keepdim=True)\n+    else:\n+        # Use topk for efficiency with larger k values\n+        return torch.topk(values, topk, dim=dim)",
  "apis": [
    "vllm.model_executor.models.llama4.Llama4MoE.custom_routing_function",
    "vllm.model_executor.models.utils.fast_topk"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/llama4.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/adapter_commons/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/multimodal/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/profiler/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/kernels/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/cutlass_benchmarks/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/structured_output/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/model_loader/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/benchmarks/lib/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/punica_wrapper/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/sched/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/kv_transfer/kv_connector/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/tool_parsers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/ops/triton_ops/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/quark/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/compressed_tensors/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/llama.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces an alternative implementation for the topk functionality, where for the case topk==1 it uses torch.max (which is generally faster) instead of torch.topk. This change is applied to non-test source code files (llama4.py and utils.py) and is not a mere refactoring or documentation update; rather, it replaces a function call with a more optimized version that improves the performance of a core API call. The modification affects CPU performance and is testable without GPU, fulfilling the performance optimization criteria.",
  "llm_api_reason": "The commit updates the Llama-4 implementation by switching the routing function in Llama4MoE to use a custom ‚Äúfast_topk‚Äù routine instead of directly calling torch.topk. It adds an optimized topk function (fast_topk) in the models/utils.py file, which returns torch.max when topk==1 and falls back to torch.topk otherwise. This change improves efficiency for the common case of topk=1 in Llama-4."
}