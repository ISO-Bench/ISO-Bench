{
  "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7753",
  "pr_date": "2024-08-28",
  "timeline_text": "Copy link Collaborator comaniac commented Aug 21, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Reference PRs: #6144 , #6819 Make @sighingnow and @Juelianqvq as co-authors of this PR. This PR supports prefix caching and chunked prefill to be enabled together. Different from the reference PRs, this PR simplifies the logic of dealing with partial blocks (thanks to @rkooo567 for the suggestion). Here is the execution flow: In scheduler, when determining the new tokens to be scheduled and both chunked prefill and prefix caching are enabled. If all uncomputed tokens can be scheduled (i.e., the last chunk of the prompt), then schedule them all. Otherwise, we always schedule the number of tokens that is divisible by the block size. For example, if the remaining budget is 133 tokens and the block size is 16, we will only schedule (133//16)*16=112 tokens. Although this approach wastes some token budget, it makes the following process straightforward. In prepare input, if all scheduled tokens are cached, we only compute the last block. Note that: We cannot skip all blocks at this moment because model runner doesn't support this case. Currently when block manager determines prefix cache blocks, it will also skip the last block due to the same reason (e.g., https://github.com/vllm-project/vllm/blob/main/vllm/core/block/prefix_caching_block.py#L556 ). This can be improved in the future if we move prefix caching to scheduler so that this case won't happen anymore. Since we guarantee the scheduled tokens are divisible by block size, we don't need to consider partial blocks in prepare input. A test case for functional correctness is also added. Throughput benchmarking results: Model: neuralmagic/Meta-Llama-3-8B-Instruct-FP8 GPU: 1xL4 Number of requests: 600 Average prompt length: 637 (shared prefix ~180, cache hit rate ~20%) Max output length: 200 Block manager v1 Chunked prefill size 2048 Branch ChunkedPrefill PrefixCaching Elapsed Time (s) Throughput (tok/s) main x v 154.37 3631.2 main v x 173.84 3215.1 PR x v 155.88 3596.2 PR v x 174.18 3298.8 PR v v 142.81 3929.7 cc @rkooo567 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 6 rkooo567, Juelianqvq, sam-h-bean, zachzzc, ldmiao, and ywang96 reacted with thumbs up emoji üöÄ 5 cadedaniel, mgoin, sam-h-bean, ywang96, and hibukipanim reacted with rocket emoji All reactions üëç 6 reactions üöÄ 5 reactions Copy link github-actions bot commented Aug 21, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac requested review from zhuohan123 and rkooo567 August 21, 2024 19:22 comaniac changed the title Prefix cache chunked prefill [Performance] Enable chunked prefill and prefix caching together Aug 21, 2024 Copy link Collaborator rkooo567 commented Aug 21, 2024 result seems very good!! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 21, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Hi @comaniac @rkooo567 I would like you folks to notice my last commit on #6144 ( a043643 ). Without it, this PR is still incorrect, and the error can be reproduced with even a single request: request 1: length 120 chunked prefill enabled prefix caching enabled max_num_batched_tokens = 64, max_num_seqs = 64 You will find that with this PR, at the first round, tokens[0:64] is prefilled, at the second round, tokens[96:119] is prefilled, and the tokens between 64 and 96 are skipped. This is because the num_computed_blocks is incorrectly updated as the whole block table for prompt tokens, rather than tokens that are prefilled at the first round. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 21, 2024 IIUC, this PR already guarantees every sequence will have at least one block to compute even it fully hits the cache, so it shouldn't trigger the issue you mentioned? If I missed anything, can you modify the unit test added in this PR so that the problem can be exposed and tested? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 21, 2024 IIUC, this PR already guarantees every sequence will have at least one block to compute even it fully hits the cache, so it shouldn't trigger the issue you mentioned? It is not about fully matched. In the case commented above, there are only 1 request, and the prefill are spited to [0:64] and [64:120], and the second part is treated as prefix matched as the computed_block_nums are updated to [0,1,2,3,4,5,6,7] after the first chunk prefill. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 21, 2024 IIUC, this PR already guarantees every sequence will have at least one block to compute even it fully hits the cache, so it shouldn't trigger the issue you mentioned? If I missed anything, can you modify the unit test added in this PR so that the problem can be exposed and tested? The test case in this PR didn't fail just because the max_num_batched_tokens (14) is smaller than the block size (16). Try larger value like 64. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 21, 2024 IIUC, this PR already guarantees every sequence will have at least one block to compute even it fully hits the cache, so it shouldn't trigger the issue you mentioned? If I missed anything, can you modify the unit test added in this PR so that the problem can be exposed and tested? The test case in this PR didn't fail just because the max_num_batched_tokens (14) is smaller than the block size (16). Try larger value like 64. The size 14 is used to test invalid size. The actual size being tested in this case is 16. Meanwhile, I tried all 16, 32 and 64 but none of them failed. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 21, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . IIUC, this PR already guarantees every sequence will have at least one block to compute even it fully hits the cache, so it shouldn't trigger the issue you mentioned? If I missed anything, can you modify the unit test added in this PR so that the problem can be exposed and tested? The test case in this PR didn't fail just because the max_num_batched_tokens (14) is smaller than the block size (16). Try larger value like 64. The size 14 is used to test invalid size. The actual size being tested in this case is 16. Meanwhile, I tried all 16, 32 and 64 but none of them failed. With max_num_batched_tokens=64, you need sequence length at least to 64 + 2 * block_size to reproduce the problem, 41 is not enough. max_num_batched_tokens=16/32 cannot reproduce the issue, too, as the second block are guaranteed to be recomputed in this PR. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 21, 2024 Ok I could reproduce the issue you pointed out. It actually only happens in block manager v1 as block manager v2 doesn't use this mechanism to mark computed blocks. This may also explain the too good speedup I got. I'll apply your fix in this PR and try to make the test cover this case. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 21, 2024 @sighingnow I applied your commit with some modifications. The test is also changed so that it will fail without fixing the issue in block manager v1. PTAL. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 22, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @sighingnow I applied your commit with some modifications. The test is also changed so that it will fail without fixing the issue in block manager v1. PTAL. Thanks! LGTM. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . rkooo567 reviewed Aug 22, 2024 View reviewed changes Copy link Collaborator rkooo567 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looks good. One question is should we just make scheduler handle prefix caching + chunked prefill correctly and make logics in model_runner simplified? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block_manager_v1.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/scheduler.py Outdated raise ValueError(\"When enabling chunked prefill and \" \"prefix caching, max_num_batched_tokens \" \"(chunk size) must be dividable by \" \"block size, but got \" Copy link Collaborator rkooo567 Aug 22, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment can you also print chunk size and block size along with budget.token_budget % block_size ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author comaniac Aug 23, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It now looks like ValueError: When enabling chunked prefill and prefix caching, max_num_batched_tokens (chunk size) must be dividable by block size, but got chunk_size (30) % block_size (16) = 14 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/scheduler.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/scheduler.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/worker/model_runner.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 22, 2024 @sighingnow I applied your commit with some modifications. The test is also changed so that it will fail without fixing the issue in block manager v1. PTAL. Will the fix for v2 block manager be addressed by this PR as well? The behavior of v2-block-manager looks quite strange and I'm wondering if #7619 is related. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 22, 2024 @sighingnow I applied your commit with some modifications. The test is also changed so that it will fail without fixing the issue in block manager v1. PTAL. Will the fix for v2 block manager be addressed by this PR as well? The behavior of v2-block-manager looks quite strange and I'm wondering if #7619 is related. I have a fix in my local but it would be a separate PR ‚ù§Ô∏è 1 sighingnow reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link JaheimLee commented Aug 22, 2024 Is it for flash-attn backend only or for all backends? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 22, 2024 Is it for flash-attn backend only or for all backends? I've tested flash-attn and FlashInfer so at least these 2 backends work. Need to test xformers later. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Juelianqvq commented Aug 23, 2024 I've tested flash-attn and FlashInfer so at least these 2 backends work. Need to test xformers later. @comaniac https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/flashinfer.py#L360 Really supported here? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 23, 2024 I've tested flash-attn and FlashInfer so at least these 2 backends work. Need to test xformers later. @comaniac https://github.com/vllm-project/vllm/blob/main/vllm/attention/backends/flashinfer.py#L360 Really supported here? Yeah I noticed that too so not fully sure what's going on. Will find some time tomorrow for it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 23, 2024 Updates: More tests are added. Chunk prefill does only support flash attention backend for now. My local test passed because it didn't schedule prefill and decode in the same batch. However, there shouldn't be a blocker for FlashInfer to support chunked prefill, so we should add this support in a follow-up PR. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator sighingnow commented Aug 24, 2024 Updates: More tests are added. Chunk prefill does only support flash attention backend for now. My local test passed because it didn't schedule prefill and decode in the same batch. However, there shouldn't be a blocker for FlashInfer to support chunked prefill, so we should add this support in a follow-up PR. May I know more why you choose to recompute the whole block if it is fully matched? Only recompute the last token is enough and requires no changes in scheduler, and it would be a bit more efficient. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 24, 2024 You're right it would be a bit more efficient to compute only the last token. Meanwhile I found that it might not be that hard to deal with prefix matching in scheduler so that this case would never happen in model runner. I'll give it a try All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac and others added 6 commits August 26, 2024 11:25 done d893717 test 1f16ece Add co-authors ‚Ä¶ 94315d4 Co-authored-by: Tao He <sighingnow@gmail.com>\nCo-authored-by: Juelianqvq <Juelianqvq@noreply.github.com> final 1daa758 fix 79563bf clean up f1e9548 comaniac added 2 commits August 26, 2024 11:26 comments and tests d57951f computel ast 324fcec comaniac force-pushed the prefix-cache-chunked-prefill branch\n    from b305e0d to 324fcec Compare August 26, 2024 19:59 Copy link Collaborator Author comaniac commented Aug 26, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @sighingnow changed to re-compute only the last token. PTAL. @rkooo567 I've tried to move prefix caching to scheduler and it's actually easy for default scheduler. For chunked prefill, we have to refactor the scheduler (e.g., .schedule() , ._schedule_prefill() , .get_new_tokens() ) and block manager (e.g., .can_allocate() ). Since we have to be careful with this refactor and it can be decoupled from this PR, I'll put it in a follow-up PR tracked by #7883 ‚ù§Ô∏è 2 sighingnow and rkooo567 reacted with heart emoji All reactions ‚ù§Ô∏è 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 26, 2024 rkooo567 approved these changes Aug 28, 2024 View reviewed changes Copy link Collaborator rkooo567 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Generally looks good. I'd like to actually also add a warning if the block size is big and prefix caching + CP is enabled (because it can waste a lot of tokens). Maybe if block_size >32, we can print a warning? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tests/core/test_block_manager.py @@ -595,3 +595,43 @@ def test_sliding_window_multi_seq(): # assert all blocks are free now assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill(): Copy link Collaborator rkooo567 Aug 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment do we have corresponding test in v2? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author comaniac Aug 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment We don't need to test v2 because v2 automatically mark touched blocks as computed. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/scheduler.py # to avoid partial block matching. block_size = self.cache_config.block_size reminder = budget.token_budget % block_size if reminder != 0: Copy link Collaborator rkooo567 Aug 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Btw, should we raise this exception at the engine start time instead and just add assert here? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author comaniac Aug 28, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I feel we could just raise here for now because this constraint should be able to be removed once we refactor the schedule to consider prefix caching. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author comaniac commented Aug 28, 2024 Generally looks good. I'd like to actually also add a warning if the block size is big and prefix caching + CP is enabled (because it can waste a lot of tokens). Maybe if block_size >32, we can print a warning? Sure I'll add the warning in a follow-up PR. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details comaniac merged commit e358053 into vllm-project : main Aug 28, 2024 54 checks passed Uh oh! There was an error while loading. Please reload this page . comaniac deleted the prefix-cache-chunked-prefill branch August 28, 2024 07:36 Copy link Contributor Juelianqvq commented Aug 28, 2024 Since this PR has been merged, both #6144 and #6819 can be closed, and are you willing to add me and @sighingnow as the co-authors? @comaniac All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Aug 28, 2024 Ah I intended to do that. Actually I put you two as co-authors in one commit of this PR and I thought it should work when the PR is merged but somehow it didn't...let me try to figure out how to fix that. Also cc @simon-mo All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . kushanam pushed a commit\n        to kushanam/vllm\n      that referenced\n      this pull request Aug 28, 2024 [Performance] Enable chunked prefill and prefix caching together ( vll‚Ä¶ ‚Ä¶ 1fcd098 ‚Ä¶m-project#7753 ) kushanam pushed a commit\n        to kushanam/vllm\n      that referenced\n      this pull request Aug 28, 2024 [Performance] Enable chunked prefill and prefix caching together ( vll‚Ä¶ ‚Ä¶ 2497d44 ‚Ä¶m-project#7753 ) Copy link Collaborator sighingnow commented Aug 29, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . To whom it may concern: after this PR there are still occasional crashes when prefix caching and chunked prefill are enabled at the same time on Nvidia GPUs (inside the flash_attn_varlen_func function in the prefix-enabled attention branch). I investigated the kernel input and find nothing wrong and cannot reproduce it when run the kernel standalone with the pickle saved inputs. I think there are still overflow bugs inside vllm-flash-attention, set the block_size to 256 could fix the issue and the crash disappeared under high pressure. üëç 3 elfiegg, comaniac, and ashgold reacted with thumbs up emoji All reactions üëç 3 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . flozi00 mentioned this pull request Sep 3, 2024 [WIP] Multi Step Chunked Prefill - Prefill Steps #8001 Closed comaniac added a commit\n        to comaniac/vllm\n      that referenced\n      this pull request Sep 3, 2024 Add co-authors of vllm-project#7753 ‚Ä¶ f13313c Co-authored-by: Tao He <sighingnow@gmail.com>\nCo-authored-by: Juelianqvq <Juelianqvq@noreply.github.com> comaniac mentioned this pull request Sep 3, 2024 [Performance] Enable chunked prefill and prefix caching together #8120 Merged Copy link ashgold commented Sep 3, 2024 To whom it may concern: after this PR there are still occasional crashes when prefix caching and chunked prefill are enabled at the same time on Nvidia GPUs (inside the flash_attn_varlen_func function in the prefix-enabled attention branch). I investigated the kernel input and find nothing wrong and cannot reproduce it when run the kernel standalone with the pickle saved outputs. I think there are still overflow bugs inside vllm-flash-attention, set the block_size to 256 could fix the issue and the crash disappeared under high pressure. This looks like a serious bug that needs to be fixed before it can go to production. Thanks for sharing the workaround solution as well. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member hmellor commented Sep 10, 2024 If you are using a model with max_model_len > 32K (i.e. Llama 3.1) then chunked prefill is enabled by default. However, this PR leaves the and not self.enable_prefix_caching condition in this automatic enabling of chunked prefill. This means that a user relying on the automatic enabling of chunked prefill might not notice it becoming disabled when they enable prefix caching. vllm/vllm/engine/arg_utils.py Lines 866 to 891\n      in da1a844 if self . enable_chunked_prefill is None : # If not explicitly set, enable chunked prefill by default for # long context (> 32K) models. This is to avoid OOM errors in the # initial memory profiling phase. if use_long_context : is_gpu = device_config . device_type == \"cuda\" use_sliding_window = ( model_config . get_sliding_window () is not None ) use_spec_decode = self . speculative_model is not None has_seqlen_agnostic_layers = ( model_config . contains_seqlen_agnostic_layers ( parallel_config )) if ( is_gpu and not use_sliding_window and not use_spec_decode and not self . enable_lora and not self . enable_prompt_adapter and not self . enable_prefix_caching and not has_seqlen_agnostic_layers ): self . enable_chunked_prefill = True logger . warning ( \"Chunked prefill is enabled by default for models with \" \"max_model_len > 32K. Currently, chunked prefill might \" \"not work with some features or models. If you \" \"encounter any issues, please disable chunked prefill \" \"by setting --enable-chunked-prefill=False.\" ) if self . enable_chunked_prefill is None : self . enable_chunked_prefill = False cc @comaniac All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author comaniac commented Sep 10, 2024 Good point. I'll file another PR to fix it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . comaniac mentioned this pull request Sep 10, 2024 [MISC] Keep chunked prefill enabled by default with long context when prefix caching is enabled #8342 Merged Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Performance] Enable chunked prefill and prefix caching together ( vll‚Ä¶ ‚Ä¶ 4b6fa2b ‚Ä¶m-project#7753 )\n\nSigned-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Performance] Enable chunked prefill and prefix caching together ( vll‚Ä¶ ‚Ä¶ 49603e3 ‚Ä¶m-project#7753 )\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:09",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: Throughput, Throughput, tok/s | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:48:09",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance] Enable chunked prefill and prefix caching together (#7753)",
  "commit_message": "[Performance] Enable chunked prefill and prefix caching together (#7753)",
  "commit_date": "2024-08-28T00:36:31-07:00",
  "files_changed": [
    "tests/basic_correctness/test_chunked_prefill.py",
    "tests/core/test_block_manager.py",
    "tests/core/test_chunked_prefill_scheduler.py",
    "vllm/core/block_manager_v1.py",
    "vllm/core/block_manager_v2.py",
    "vllm/core/embedding_model_block_manager.py",
    "vllm/core/interfaces.py",
    "vllm/core/scheduler.py",
    "vllm/worker/model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 3,
    "num_non_test_files": 6,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 9,
    "num_hunks": 12,
    "num_edited_lines": 252,
    "num_non_test_edited_lines": 107,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex 1211e6ba5..fc6f829c3 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -6,6 +6,7 @@ prefill requests are chunked.\n \n Run `pytest tests/models/test_chunked_prefill.py`.\n \"\"\"\n+from contextlib import nullcontext\n \n import pytest\n \n@@ -156,3 +157,68 @@ def test_models_with_fp8_kv_cache(\n         name_0=\"no_chunked_prefill\",\n         name_1=\"chunked_prefill\",\n     )\n+\n+\n+@pytest.mark.parametrize(\"max_tokens\", [16])\n+@pytest.mark.parametrize(\"enforce_eager\", [False])\n+@pytest.mark.parametrize(\"chunk_size\", [30, 32])\n+@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n+# NOTE: Increasing this in this suite will fail CI because we currently cannot\n+# reset distributed env properly. Use a value > 1 just when you test.\n+@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n+def test_with_prefix_caching(\n+    vllm_runner,\n+    max_tokens: int,\n+    enforce_eager: bool,\n+    chunk_size: int,\n+    use_v2_block_manager: bool,\n+    tensor_parallel_size: int,\n+) -> None:\n+    \"\"\"\n+    Checks exact match decode with and without prefix caching\n+    with chunked prefill enabled.\n+    \"\"\"\n+    model = \"meta-llama/Llama-2-7b-chat-hf\"\n+    # The common prompt has 142 tokens with Llama-2 tokenizer.\n+    common_prompt = \"You are a helpful AI assistant \" * 20\n+    unique_prompts = [\n+        \"Question\",  # Warmup\n+        \"Question\",  # Fully cached\n+        \"Another question\",  # Partial cached\n+    ]\n+    full_prompts = [f\"{common_prompt}\\n{p}\" for p in unique_prompts]\n+\n+    max_num_batched_tokens = max_num_seqs = chunk_size\n+    outputs = {}  # type: ignore\n+    check_result = True\n+    for enable in (True, False):\n+        with vllm_runner(\n+                model,\n+                dtype=\"half\",\n+                max_num_batched_tokens=max_num_batched_tokens,\n+                enable_chunked_prefill=True,\n+                enable_prefix_caching=enable,\n+                tensor_parallel_size=tensor_parallel_size,\n+                use_v2_block_manager=use_v2_block_manager,\n+                enforce_eager=enforce_eager,\n+                max_num_seqs=max_num_seqs,\n+        ) as vllm_model:\n+            # It should fail when prefix caching is enable and chunk\n+            # size is not a multiple of block size (16).\n+            should_fail = chunk_size % 16 != 0 and enable\n+            check_result &= not should_fail\n+            outputs[enable] = []\n+            # Send the request one-by-one to ensure the cache is populated.\n+            with pytest.raises(ValueError) if should_fail else nullcontext():\n+                for prompt in full_prompts:\n+                    outputs[enable] += vllm_model.generate_greedy([prompt],\n+                                                                  max_tokens)\n+\n+    # Check results only if we did not expect a failure.\n+    if check_result:\n+        check_outputs_equal(\n+            outputs_0_lst=outputs[False],\n+            outputs_1_lst=outputs[True],\n+            name_0=\"w/o prefix caching\",\n+            name_1=\"with prefix caching\",\n+        )\ndiff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex cd306b9e4..2ee9f2082 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -595,3 +595,43 @@ def test_sliding_window_multi_seq():\n \n     # assert all blocks are free now\n     assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n+\n+\n+def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n+    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n+    should only mark a chunk of blocks as computed instead of all blocks.\n+    \"\"\"\n+\n+    block_size = 4\n+    num_cpu_blocks = 0\n+    num_gpu_blocks = 16\n+    block_manager = BlockSpaceManagerV1(block_size,\n+                                        num_gpu_blocks,\n+                                        num_cpu_blocks,\n+                                        watermark=0,\n+                                        enable_caching=True)\n+\n+    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n+    prompt_length = block_size * num_gpu_blocks - 1\n+\n+    # Allocate (reserve) all blocks.\n+    _, seq_group = create_dummy_prompt(\"0\",\n+                                       prompt_length,\n+                                       block_size=block_size)\n+    block_manager.allocate(seq_group)\n+    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n+\n+    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n+    token_chunk_size = int(block_size * 2.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 2\n+\n+    # Actual computed tokens.\n+    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n+\n+    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n+    token_chunk_size = int(block_size * 4.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 7\ndiff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex 6d9c2f3eb..2f6ea632a 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -562,3 +562,42 @@ def test_chunked_prefill_max_seqs():\n     assert len(get_sequence_groups(out)) == max_seqs\n     assert not running[0].is_prefill()\n     assert not running[1].is_prefill()\n+\n+\n+def test_perfix_caching():\n+    \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n+    block_size = 4\n+    max_seqs = 10\n+    max_model_len = 80\n+    max_num_batched_tokens = 64\n+    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n+                                       max_seqs,\n+                                       max_model_len,\n+                                       enable_chunked_prefill=True)\n+    cache_config = CacheConfig(block_size,\n+                               1.0,\n+                               1,\n+                               \"auto\",\n+                               enable_prefix_caching=True)\n+    cache_config.num_cpu_blocks = 0\n+    cache_config.num_gpu_blocks = 32\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+    running: List[SequenceGroup] = []\n+\n+    # Add seq groups to scheduler.\n+    for i in range(2):\n+        _, seq_group = create_dummy_prompt(str(i),\n+                                           block_size=block_size,\n+                                           prompt_length=50)\n+        scheduler.add_seq_group(seq_group)\n+        running.append(seq_group)\n+\n+    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n+    assert set(get_sequence_groups(out)) == set(running)\n+    assert seq_group_meta[0].token_chunk_size == 50\n+    # Verify it is chunked. Note that although the budget is 64-50=14,\n+    # we only allocate full blocks for prefix caching, so only 4*(14//4)=12\n+    # tokens are allocated.\n+    assert seq_group_meta[1].token_chunk_size == 12\n+    assert out.num_prefill_groups == 2\n+    assert out.num_batched_tokens == 62\ndiff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 666723313..24ab9eb66 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -681,14 +681,20 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n             for block in block_table:\n                 block.last_accessed = access_time\n \n-    def compute_full_blocks_in_seq(self, seq: Sequence):\n+    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n         if seq.seq_id not in self.block_tables:\n             return\n-        max_full_block = seq.get_len() // self.block_size - 1\n+\n+        # When chunked prefill is enabled, the computed full blocks\n+        # should be calculated based on the number of computed tokens.\n+        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n+                               token_chunk_size)\n+        computed_full_blocks = max_computed_tokens // self.block_size\n+\n         block_table = self.block_tables[seq.seq_id]\n-        if max_full_block == -1:\n+        if computed_full_blocks == 0:\n             return\n-        for i in reversed(range(max_full_block)):\n+        for i in reversed(range(computed_full_blocks)):\n             if block_table[i].computed:\n                 break\n             block_table[i].computed = True\n@@ -718,10 +724,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n         return commonprefix([ids for ids in ids_list if ids != []])\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         if self.enable_caching:\n             for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq)\n+                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\n         if device == Device.GPU:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex 7d2db43cb..b06385b06 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -290,7 +290,8 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             self._last_access_blocks_tracker.update_last_access(\n                 seq.seq_id, now)\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         # If prefix caching is enabled, mark immutable blocks as computed\n         # right after they have been scheduled (for prefill). This assumes\n         # the scheduler is synchronous so blocks are actually computed when\ndiff --git a/vllm/core/embedding_model_block_manager.py b/vllm/core/embedding_model_block_manager.py\nindex f16f66e99..c47d7d8df 100644\n--- a/vllm/core/embedding_model_block_manager.py\n+++ b/vllm/core/embedding_model_block_manager.py\n@@ -80,7 +80,8 @@ class EmbeddingModelBlockSpaceManager(BlockSpaceManager):\n                                       seq_group: List[Sequence]) -> List[int]:\n         return []\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\ndiff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex becd0d2e7..96f8dd851 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -115,7 +115,8 @@ class BlockSpaceManager(ABC):\n         pass\n \n     @abstractmethod\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     @abstractmethod\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex fbc53afa3..51fde6e4e 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -1226,7 +1226,8 @@ class Scheduler:\n         # will crash the vLLM instance / will not retry.\n         for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n             self.block_manager.mark_blocks_as_computed(\n-                scheduled_seq_group.seq_group)\n+                scheduled_seq_group.seq_group,\n+                scheduled_seq_group.token_chunk_size)\n \n         self._seq_group_metadata_cache[self.next_cache_id].reset()\n \n@@ -1457,10 +1458,27 @@ class Scheduler:\n         for seq in seqs:\n             num_new_tokens += seq.get_num_new_tokens()\n         assert num_new_tokens > 0\n-        # Chunk if a running request cannot fit in.\n-        # If number of seq > 1, it means it is doing beam search in a\n-        # decode phase. Do not chunk in that case.\n+        # Chunk if a running request cannot fit in the given budget.\n+        # If number of seq > 1, it means it is doing beam search\n+        # in a decode phase. Do not chunk.\n         if enable_chunking and len(seqs) == 1:\n-            num_new_tokens = min(num_new_tokens,\n-                                 budget.remaining_token_budget())\n+            remaining_token_budget = budget.remaining_token_budget()\n+            if self.cache_config.enable_prefix_caching:\n+                # When prefix caching is enabled, we always allocate\n+                # the number of new tokens that is dividable by the block size\n+                # to avoid partial block matching.\n+                block_size = self.cache_config.block_size\n+                reminder = budget.token_budget % block_size\n+                if reminder != 0:\n+                    raise ValueError(\"When enabling chunked prefill and \"\n+                                     \"prefix caching, max_num_batched_tokens \"\n+                                     \"(chunk size) must be dividable by \"\n+                                     \"block size, but got chunk_size \"\n+                                     f\"({budget.token_budget}) % block_size \"\n+                                     f\"({block_size}) = {reminder}\")\n+                if remaining_token_budget < num_new_tokens:\n+                    num_new_tokens = (remaining_token_budget //\n+                                      block_size) * block_size\n+            else:\n+                num_new_tokens = min(num_new_tokens, remaining_token_budget)\n         return num_new_tokens\ndiff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4ea1..2b287a5d2 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -501,23 +501,48 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                             and self.sliding_window is None\n                             and inter_data.is_prompt)\n         inter_data.prefix_cache_hit = prefix_cache_hit\n-        if self.chunked_prefill_enabled and prefix_cache_hit:\n-            raise RuntimeError(\n-                \"chunked prefill cannot be used with prefix caching now.\")\n-\n-        # If prefix cache is hit, advance context length to bypass\n-        # hit blocks. Accordingly, input tokens, position and query length\n-        # have to be updated.\n-        if prefix_cache_hit:\n-            assert computed_block_nums is not None\n-            context_len = len(computed_block_nums) * self.block_size\n+\n+        if not prefix_cache_hit:\n+            return\n+\n+        assert computed_block_nums is not None\n+        # The cache hit prompt tokens in this sequence. Note that\n+        # this may be larger than the sequence length if chunked\n+        # prefill is enabled.\n+        prefix_cache_len = len(computed_block_nums) * self.block_size\n+        # The number of so far computed prompt tokens in this sequence.\n+        context_len = inter_data.context_lens[seq_idx]\n+        # The total number of prompt tokens in this sequence.\n+        # When chunked prefill is enabled, this is the token number of\n+        # computed chunks + current chunk.\n+        seq_len = inter_data.seq_lens[seq_idx]\n+        if prefix_cache_len <= context_len:\n+            # We already passed the cache hit region,\n+            # so do normal computation.\n+            pass\n+        elif context_len < prefix_cache_len < seq_len:\n+            # Partial hit. Compute the missing part.\n+            uncomputed_start = prefix_cache_len - context_len\n             inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n             inter_data.input_positions[seq_idx] = inter_data.input_positions[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n+            context_len = prefix_cache_len\n+\n             inter_data.context_lens[seq_idx] = context_len\n             inter_data.query_lens[\n                 seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n+        elif seq_len <= prefix_cache_len:\n+            # Full hit. Only compute the last token to avoid\n+            # erroneous behavior. FIXME: Ideally we should directly\n+            # mark all tokens as computed in the scheduler and do not\n+            # schedule this sequence, so this case should not happen.\n+            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n+                seq_idx][-1:]\n+            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n+                seq_idx][-1:]\n+            inter_data.query_lens[seq_idx] = 1\n+            inter_data.context_lens[seq_idx] = inter_data.seq_lens[seq_idx] - 1\n \n     def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                     seq_idx: int,",
  "apis": [
    "ModelRunner.generate_greedy",
    "Scheduler.schedule",
    "BlockSpaceManager.mark_blocks_as_computed"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/scheduler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/sched/scheduler.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces changes to core components (block manager, scheduler, model_runner) as well as test cases to support running with both chunked prefill and prefix caching enabled. These modifications update internal APIs to compute and mark blocks based on token chunks, adjusting block computation based on performance-related parameters. The changes are non-trivial and modify non-test source code (in modules like block_manager_v1.py, scheduler.py, etc.) that affect runtime request batching and token processing performance on CPU. The commit message categorizes it as a performance improvement, and the modifications aim to optimize resource allocation and computation scheduling. Therefore, the conditions for a performance/optimization commit are met.",
  "llm_api_reason": "The commit enables chunked prefill to work in tandem with prefix caching by modifying both the test suite and underlying components. In tests, new parametrized cases validate that generating outputs via greedy decoding behaves correctly when prefix caching is enabled (or not) with different chunk sizes. In the core code, methods in the block manager (both in v1 and v2, as well as via the common BlockSpaceManager interface) and in the scheduler are updated so that the number of computed blocks is now determined using a token_chunk_size parameter. In addition, the logic in the worker‚Äôs model runner that handles prefix cache hits is refined to properly adjust the input tokens, positions, and query lengths when the computed prefix comes partly from cached blocks. Overall, these changes allow the library‚Äôs high‚Äêlevel inference APIs to deliver correct token batching under the new combined optimization strategy while preserving performance."
}