{
  "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
  "pr_url": "https://github.com/vllm-project/vllm/pull/3279",
  "pr_date": "2024-03-22",
  "timeline_text": "Copy link Member tdoublep commented Mar 8, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . We have been benchmarking vLLM internally using a synthetic workload generator that has been fitted to mimic our production workloads. It stresses the inference server using a varying number of concurrent users, all users send requests that are drawn uniformly from a heterogeneous set of requests with different prompt lengths and number of generated tokens. We have found that for these workloads, vLLM has extremely low TTFT (time to first token) but has relatively high ITL (inter-token latency). An in-depth analysis seems to show that vLLM tends to schedule prompts as soon as possible, resulting in very small prompt batches, which are processed very quickly, but end up starving the decoding phase. This PR adds a new optional feature --scheduler-use-delay which, if enabled, creates an artificial delay before scheduling prompts. The delay is determined dynamically based on the time to perform the last prompt step. This delay allows the waiting queue to fill up with more requests. This gives the opportunity to make larger prompt batches, but due to the heterogeneous nature of the workload, we then hit issues related to padding overhead. It is thus beneficial to combine this scheduler delay with the --scheduler-policy=reorder feature from #2357 which sorts the waiting queue by sequence length. This allows us to create much larger prompt batches whilst staying with the padding limits, and leads to significant improvements in terms of ITL performance. This ITL improvement comes at the expense of TTFT performance, since (a) we are applying an artificial delay before scheduling prompts and (b) we are now processing larger batches which take longer to process. Different use-cases may have a preference towards either metric, which is why we feel this makes sense as an optional feature for now. Benchmarking results (labels on each point indicates the number of concurrent users): Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 3 njhill, ywang96, and zhuohan123 reacted with thumbs up emoji All reactions üëç 3 reactions jvlunteren and others added 2 commits March 7, 2024 18:35 Implement dynamic scheduler delay ‚Ä¶ 0d0d540 Co-authored-by: Thomas Parnell <tpa@zurich.ibm.com> SchedulerConfig: add default value for use_delay 75b7f57 Copy link Collaborator robertgshaw2-redhat commented Mar 8, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Take a look also at the chunked prefill efforts to address this #3106 üëç 1 tdoublep reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author tdoublep commented Mar 8, 2024 @robertgshaw2-neuralmagic Thanks, and agreed: chunked prefill may eventually solve this problem in a different way. We hope that this relatively simple, optional, change can be used to improve performance in the meantime. üëç 4 robertgshaw2-redhat, mgoin, njhill, and ywang96 reacted with thumbs up emoji All reactions üëç 4 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member ywang96 commented Mar 8, 2024 This delay allows the waiting queue to fill up with more requests. This might affect #3168 and IMO it's worth thinking about how to integrate these control changes with each other All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Yard1 commented Mar 8, 2024 @tdoublep We were planning to upstream something similar, but instead of time we used number of decode iterations (\"schedule prefill iteration only after N decode iterations have been completed or there are no running sequences\"). We believe that this scheme is more generic and easier to implement. I'd be happy to make a PR early next week, if you are interested in trying that out. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member njhill commented Mar 8, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @Yard1 could you elaborate on \"more generic and easier to implement\"? Isn't it completely generic and fairly trivial to implement in either case? We found the adaptive time-based approach to work very well, and it makes more sense to me intuitively at least. The goal is to prevent prefills from starving decode progress - the enforced delay is some fraction of the duration of the last prefill and so equivalent to saying that not more than say 50% of time can be spent in prefill. We chose this min delay to be half the last prefill time which ensures at most 66% of time is spent in prefill. Of course like in your case, the min delay only applies while there are still running sequences. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Yard1 commented Mar 8, 2024 Hmm I now see the delay is dynamic. I think thinking in terms of model iterations is simpler, but I suppose that this approach should be just as good. @tdoublep would it be possible for you to open source your benchmarking tool? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author tdoublep commented Mar 11, 2024 @Yard1 Yes - we do plan to open-source the benchmarking tool. We are working through that process internally at the moment. üëç 3 Yard1, ywang96, and richardliaw reacted with thumbs up emoji All reactions üëç 3 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor sh1ng commented Mar 11, 2024 @tdoublep Which value of --scheduler-use-delay combined with --scheduler_reorder_window do you use? I believe the sum of them must be a constant. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author tdoublep commented Mar 12, 2024 @sh1ng --scheduler-use-delay is a boolean option. If set to true, we apply a delay equal to half of the previous time for a prompt step (e.g., the delay is adaptive based on the workload). For the --scheduler_reorder_window we used a very large value (1000) to ensure that all of the requests in the waiting queue are sorted. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author tdoublep commented Mar 15, 2024 Based on the discussion here it sounds like sorting the requests in the waiting queue will no longer be necessary once we merge #3236 which effectively removing padding constraints via 1D query. We have run additional experiments to compare the performance when using 1D query from #3236 , as well as to evaluate the performance if we enable the dynamic delay (from this PR) in combination with 1D query: Conclusion : combining dynamic scheduler delay ( #3279 ) with 1D query ( #3236 ) is even more effective than combining it with sorting requests by length ( #2357 ). üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tdoublep added 4 commits March 20, 2024 15:37 Add test for scheduler_use_delay a7b6735 move use_delay test to end 8f15973 Merge branch 'main' into scheduler-delay 8ef047a code formatting fd1e5da Copy link Member Author tdoublep commented Mar 20, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Update: Added a test case in test_scheduler.py to cover use_delay option. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tdoublep mentioned this pull request Mar 20, 2024 [1/n][Chunked Prefill] Refactor input query shapes #3236 Merged Resolve some conflicts with changes on main 69cda2a Copy link Member Author tdoublep commented Mar 21, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Now that 1D query has been merged, the changes from this PR can be effective when applied on top of main branch. Here is latest round of benchmarking results. I've also included performance data collected using TGIS (our fork of TGI) as an additional reference point: Some conclusions here: We can see that introducing the scheduler delay dramatically improves the ITL when the inference server is under stress  (>2x in some cases), and helps to close the performance gap to TGIS, which is better than vLLM in terms of ITL. The delay has the effect of processing larger batches of prompts, which worsens the TTFT a bit. However, we can see that the TTFT from vLLM after this change is still significantly better than TGIS (>10x in some cases). üëç 1 rkooo567 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 reviewed Mar 21, 2024 View reviewed changes vllm/core/scheduler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tdoublep added 3 commits March 21, 2024 18:59 Factor delay logic into separate function ae28c43 Merge branch 'main' into scheduler-delay 2d2b8e0 Remove print in test 99b0d7d Copy link Collaborator Yard1 commented Mar 21, 2024 Looks good. I think it would be even better if we didn't hardcode it to 0.5. I think we could make the argument a float, and if it is <=0, we don't apply the delay. üëç 2 tdoublep and njhill reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 reviewed Mar 21, 2024 View reviewed changes vllm/core/scheduler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . tdoublep added 2 commits March 21, 2024 19:25 Add some comments e1e3408 Changed use_delay (bool) to delay_factor (float) a114e74 Copy link Member Author tdoublep commented Mar 21, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Looks good. I think it would be even better if we didn't hardcode it to 0.5. I think we could make the argument a float, and if it is <=0, we don't apply the delay. @Yard1 Good idea - there is no reason to assume that 0.5 an optimum for all scenarios. I've updated the code accordingly. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator richardliaw commented Mar 22, 2024 @Yard1 are you approving this PR? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 approved these changes Mar 22, 2024 View reviewed changes Yard1 merged commit cf2f084 into vllm-project : main Mar 22, 2024 tdoublep deleted the scheduler-delay branch March 22, 2024 20:10 Copy link Member Author tdoublep commented Mar 22, 2024 @Yard1 thanks for the review and helpful discussion and suggestions. üöÄ 1 Yard1 reacted with rocket emoji All reactions üöÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator rkooo567 commented Mar 22, 2024 @tdoublep Does vllm have a doc about configuration? Feel like it is worth adding it there if there is. I.e., there are config setttings to optimize throughput over latency, TTFT over ITL or the other way around. But it seems like things are not that well documented üëÄ 1 tdene reacted with eyes emoji All reactions üëÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author tdoublep commented Mar 25, 2024 @rkooo567 I agree it would be good to have documentation like that. The closest thing I can find the the developer documentation, e.g.: https://docs.vllm.ai/en/latest/dev/engine/llm_engine.html Perhaps we should consider adding some more pages there to documentation the ModelConfig , SchedulerConfig etc. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator rkooo567 commented Mar 25, 2024 I see. Yeah +1 we need better doc with configs, but it seems like there's no holistic page that explains this. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . dtrifiro mentioned this pull request May 15, 2024 bump ubi base image tag opendatahub-io/vllm#24 Merged Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:22",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, TTFT | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:49:22",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Dynamic scheduler delay to improve ITL performance  (#3279)",
  "commit_message": "Dynamic scheduler delay to improve ITL performance  (#3279)\n\nCo-authored-by: Jan van Lunteren <jvl@zurich.ibm.com>",
  "commit_date": "2024-03-22T12:28:14-07:00",
  "files_changed": [
    "tests/core/test_scheduler.py",
    "vllm/config.py",
    "vllm/core/scheduler.py",
    "vllm/engine/arg_utils.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 4,
    "num_hunks": 12,
    "num_edited_lines": 74,
    "num_non_test_edited_lines": 40,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 397101fa8..4a690e24e 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -1,5 +1,6 @@\n from typing import List\n import pytest  # noqa\n+import time\n \n from vllm.config import CacheConfig, SchedulerConfig\n from vllm.core.scheduler import Scheduler\n@@ -168,3 +169,36 @@ def test_scheduler_max_seqs():\n     # and one is prompting.\n     _, out = scheduler.schedule()\n     assert set(out.scheduled_seq_groups) == set([all_seq_groups[1]])\n+\n+\n+def test_scheduler_delay_factor():\n+\n+    block_size = 4\n+    scheduler_config = SchedulerConfig(100, 64, 16, delay_factor=0.5)\n+    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n+    cache_config.num_cpu_blocks = 8\n+    cache_config.num_gpu_blocks = 8\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+\n+    # schedule first prompt\n+    _, seq_group = create_dummy_prompt(\"0\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for a second before scheduling next prompt\n+    time.sleep(1)\n+    _, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+\n+    # second prompt should *not* be scheduled\n+    seq_group_meta, out = scheduler.schedule()\n+    assert not out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for more than 0.5 second and try again\n+    time.sleep(0.6)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '1'\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 6dfb51586..2003563e4 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -517,6 +517,8 @@ class SchedulerConfig:\n             iteration.\n         max_model_len: Maximum length of a sequence (including prompt\n             and generated text).\n+        delay_factor: Apply a delay (of delay factor multiplied by previous\n+            prompt latency) before scheduling next prompt.\n     \"\"\"\n \n     def __init__(\n@@ -524,6 +526,7 @@ class SchedulerConfig:\n         max_num_batched_tokens: Optional[int],\n         max_num_seqs: int,\n         max_model_len: int,\n+        delay_factor: float = 0.0,\n     ) -> None:\n         if max_num_batched_tokens is not None:\n             self.max_num_batched_tokens = max_num_batched_tokens\n@@ -533,6 +536,7 @@ class SchedulerConfig:\n             self.max_num_batched_tokens = max(max_model_len, 2048)\n         self.max_num_seqs = max_num_seqs\n         self.max_model_len = max_model_len\n+        self.delay_factor = delay_factor\n         self._verify_args()\n \n     def _verify_args(self) -> None:\ndiff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex be55e8520..4bd0ef360 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -103,6 +103,13 @@ class Scheduler:\n         # Sequence groups in the SWAPPED state.\n         self.swapped: Deque[SequenceGroup] = deque()\n \n+        # Time at previous scheduling step\n+        self.prev_time = 0.0\n+        # Did we schedule a prompt at previous step?\n+        self.prev_prompt = False\n+        # Latency of the last prompt step\n+        self.last_prompt_latency = 0.0\n+\n     @property\n     def lora_enabled(self) -> bool:\n         return bool(self.lora_config)\n@@ -179,7 +186,7 @@ class Scheduler:\n             # are added to the back.\n             leftover_waiting_sequences = deque()\n             num_batched_tokens = 0\n-            while self.waiting:\n+            while self._passed_delay(now) and self.waiting:\n                 seq_group = self.waiting[0]\n                 waiting_seqs = seq_group.get_seqs(\n                     status=SequenceStatus.WAITING)\n@@ -246,6 +253,7 @@ class Scheduler:\n             self.waiting.extendleft(leftover_waiting_sequences)\n \n             if scheduled or ignored_seq_groups:\n+                self.prev_prompt = True\n                 scheduler_outputs = SchedulerOutputs(\n                     scheduled_seq_groups=scheduled,\n                     prompt_run=True,\n@@ -491,3 +499,19 @@ class Scheduler:\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         self.block_manager.mark_blocks_as_computed(seq_group)\n+\n+    def _passed_delay(self, now: float) -> bool:\n+        if self.prev_prompt:\n+            self.last_prompt_latency = now - self.prev_time\n+        self.prev_time, self.prev_prompt = now, False\n+        # Delay scheduling prompts to let waiting queue fill up\n+        if self.scheduler_config.delay_factor > 0 and self.waiting:\n+            earliest_arrival_time = min(\n+                [e.metrics.arrival_time for e in self.waiting])\n+            passed_delay = (\n+                (now - earliest_arrival_time) >\n+                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n+                or not self.running)\n+        else:\n+            passed_delay = True\n+        return passed_delay\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 94c80f428..2070686ea 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -51,6 +51,7 @@ class EngineArgs:\n     max_cpu_loras: Optional[int] = None\n     device: str = 'auto'\n     ray_workers_use_nsight: bool = False\n+    scheduler_delay_factor: float = 0.0\n \n     def __post_init__(self):\n         if self.tokenizer is None:\n@@ -305,6 +306,12 @@ class EngineArgs:\n                             default=EngineArgs.device,\n                             choices=[\"auto\", \"cuda\", \"neuron\"],\n                             help='Device type for vLLM execution.')\n+        parser.add_argument(\n+            '--scheduler-delay-factor',\n+            type=float,\n+            default=EngineArgs.scheduler_delay_factor,\n+            help='Apply a delay (of delay factor multiplied by previous'\n+            'prompt latency) before scheduling next prompt.')\n         return parser\n \n     @classmethod\n@@ -342,7 +349,8 @@ class EngineArgs:\n             ), self.ray_workers_use_nsight)\n         scheduler_config = SchedulerConfig(self.max_num_batched_tokens,\n                                            self.max_num_seqs,\n-                                           model_config.max_model_len)\n+                                           model_config.max_model_len,\n+                                           self.scheduler_delay_factor)\n         lora_config = LoRAConfig(\n             max_lora_rank=self.max_lora_rank,\n             max_loras=self.max_loras,",
  "apis": [
    "vllm.config.SchedulerConfig",
    "vllm.core.scheduler.Scheduler.schedule",
    "vllm.engine.arg_utils.EngineArgs"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/scheduler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/sched/scheduler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/config.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/arg_utils.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces a mechanism to delay scheduling prompts based on previous prompt latency by adding a delay_factor. It modifies non-test files (vllm/config.py, vllm/core/scheduler.py, vllm/engine/arg_utils.py) in a non-trivial way that adjusts the scheduler's behavior to allow the waiting queue to fill up, which should improve overall latency performance. Although a test file is also modified, the core source changes and the commit's message make clear that its goal is to improve ITL performance, and it is not merely a bug fix, simple refactor, or adding a new feature unrelated to performance. The changes affect a high-level scheduling API, impacting CPU performance and are testable on general hardware without reliance on GPUs.",
  "llm_api_reason": "This commit introduces a new dynamic delay factor to the scheduler to improve inter-token latency performance. The changes add a new ‚Äúdelay_factor‚Äù parameter to the SchedulerConfig allowing users to specify that the scheduler should wait a configurable fraction (multiplied by the latency of the previous prompt) before scheduling the next prompt. In the scheduler implementation, new member variables (prev_time, prev_prompt, last_prompt_latency) and a helper method (_passed_delay) have been added to control when a new prompt is allowed. Furthermore, the EngineArgs CLI parser is updated to accept a new command‚Äêline argument (--scheduler-delay-factor) that passes the delay factor to the SchedulerConfig. The tests have been extended with a new test to check that the delay behaves as expected."
}