{
  "commit_hash": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
  "pr_url": "https://github.com/vllm-project/vllm/pull/12563",
  "pr_date": "2025-01-31",
  "timeline_text": "Copy link Contributor xpbowler commented Jan 29, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . [Guided decoding performance optimization] Sending the guided decoding bitmask in xgrammar to the GPU ( self.token_bitmask.to(scores.device) ) is a blocking operation that prevents the CPU from pre-launching the sampler kernels. The CPU waits until decode is complete, then copies the bitmask over. This PR changes the operation to async via setting non-blocking=True . (Current) The CPU is blocked on a cudaStreamSynchronize and only pre-empts the sampling kernels after bitmask application. Below is the Nsys profile for one decode phase from Llama 3.1 8B. With the optimization, this is no longer the case: Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions xpbowler requested a review\n  from mgoin as a code owner January 29, 2025 21:16 Copy link github-actions bot commented Jan 29, 2025 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . xpbowler force-pushed the main branch\n    from e91e01a to 99611c5 Compare January 29, 2025 21:26 mgoin approved these changes Jan 29, 2025 View reviewed changes Copy link Member mgoin left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This makes sense, thanks! LGTM pending green CI Showing the profile is great, also showing an e2e speedup (even if small) would be nice Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mgoin added structured-output ready ONLY add when PR is ready to merge/full CI is needed labels Jan 29, 2025 Copy link Contributor Author xpbowler commented Jan 29, 2025 This makes sense, thanks! LGTM pending green CI Showing the profile is great, also showing an e2e speedup (even if small) would be nice For single request benchmarks with Llama 3.1 8B running on H100, the improvement in tok/s was ~5%: Single request 87.5tok/s, guided unoptimized 92 tok/s, guided optimized üöÄ 2 mgoin and njhill reacted with rocket emoji All reactions üöÄ 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin added\n  the performance Performance-related issues label Jan 29, 2025 aarnphm approved these changes Jan 29, 2025 View reviewed changes xpbowler force-pushed the main branch\n    from 9bae63f to b9681d4 Compare January 30, 2025 15:40 mgoin enabled auto-merge (squash) January 30, 2025 22:16 Ryan N added 3 commits January 31, 2025 20:26 remove blocking bitmask memcpy ‚Ä¶ 4a3d85f Signed-off-by: Ryan N <ryan.nguyen@centml.ai> re-run ci pipeline ‚Ä¶ a7914a8 Signed-off-by: Ryan N <ryan.nguyen@centml.ai> pipeline ‚Ä¶ f8fa0c6 Signed-off-by: Ryan N <ryan.nguyen@centml.ai> auto-merge was automatically disabled January 31, 2025 20:27 Head branch was pushed to by a user without write access xpbowler force-pushed the main branch\n    from b11a83f to f8fa0c6 Compare January 31, 2025 20:27 Hide details View details simon-mo merged commit fc54214 into vllm-project : main Jan 31, 2025 38 of 44 checks passed Uh oh! There was an error while loading. Please reload this page . Isotr0py pushed a commit\n        to Isotr0py/vllm\n      that referenced\n      this pull request Feb 2, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ df7ab19 ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Isotr0py <2037008807@qq.com> srikanthsrnvs pushed a commit\n        to srikanthsrnvs/vllm\n      that referenced\n      this pull request Feb 3, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ d27e55d ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> sahelib25 pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Feb 3, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ 51f5127 ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai> NickLucche pushed a commit\n        to NickLucche/vllm\n      that referenced\n      this pull request Feb 7, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ 5c21ca9 ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai> GWS0428 pushed a commit\n        to GWS0428/VARserve\n      that referenced\n      this pull request Feb 12, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ bea306f ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Feature] Fix guided decoding blocking bitmask memcpy ( vllm-project#1‚Ä¶ ‚Ä¶ 76bd88f ‚Ä¶2563 )\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image]( https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824 )\n\nWith the optimization, this is no longer the case:\n\n![image]( https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7 )\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:46:50",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: tok/s, tok/s, optimization | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:46:50",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --num-prompts 1",
  "commit_subject": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)",
  "commit_message": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image](https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824)\n\nWith the optimization, this is no longer the case:\n\n![image](https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7)\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>",
  "commit_date": "2025-01-31T15:37:30-08:00",
  "files_changed": [
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 4,
    "num_non_test_edited_lines": 4,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 2d8594cb8..ee30ce96f 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -307,8 +307,8 @@ class XGrammarLogitsProcessor:\n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n         # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(scores,\n-                                        self.token_bitmask.to(scores.device))\n+        xgr.apply_token_bitmask_inplace(\n+            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n         if device_type != \"cuda\":\n             scores = scores.to(dtype).to(device_type).squeeze()",
  "apis": [
    "None"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/llm.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file (xgrammar_decoding.py) in a non-trivial way by replacing a blocking GPU tensor transfer with a non-blocking one. The commit message discusses performance improvements by reducing CPU blocking during guided decoding, and includes profiling evidence to support this performance enhancement. This change directly impacts the performance of a high-level API without introducing new features or mere refactoring, and is testable on CPU (it addresses CPU blocking operations). Hence, it satisfies the conditions as a performance optimization commit.",
  "llm_api_reason": "This commit adjusts an internal call in the guided‚Äêdecoding code by adding the non_blocking flag when transferring the token bitmask tensor to the GPU. The change does not modify any public or top-level Python API‚Äîthe change is entirely an internal performance optimization in the model executor‚Äôs guided decoding logic."
}