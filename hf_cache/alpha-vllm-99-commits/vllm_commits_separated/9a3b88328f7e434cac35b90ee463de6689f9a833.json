{
  "commit_hash": "9a3b88328f7e434cac35b90ee463de6689f9a833",
  "pr_url": "https://github.com/vllm-project/vllm/pull/19939",
  "pr_date": "2025-06-21",
  "timeline_text": "Copy link Contributor vadiklyutiy commented Jun 21, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results Purpose Speedup of MRoPE prepare inputs. #16881 got stuck for a while. I combined parts of #16881 and #17617 to minimize changes and brings sufficient speedup. MRotaryEmbedding.get_next_input_positions_tensor takes a lot of time. Use numpy to speedup it. Performance Test Result I used vllm serve Qwen/Qwen2.5-VL-3B-Instruct --disable-log-requests --no-enable-prefix-caching as a server and fib benchmark -rps 50 --input-token-distribution uniform 250 300     --output-token-distribution uniform 150 250 --num-of-imgs-per-req 1     --img-ratios-per-req 512x512 -n 1000 --base-url http://localhost:8000     --endpoint v1/chat/completions --backend openai-chat to make workload (send 50 requests per sec with one 512x512 image per request). I decorated GPUModelRunner._prepare_inputs with nvtx to measure the time. _prepare_inputs average time Before: 3.869 ms With this PR: 1.487 ms (speedup 2.6x) For info, with #17617 : 2.511 ms E2E performance improvement on Qwen2.5-VL-3B-Instruct with high load is around 1.5%. cc @imkero @ywang96 @simon-mo Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Speedup MRoPE prepare inputs ‚Ä¶ c44c815 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> vadiklyutiy requested review from WoosukKwon , robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners June 21, 2025 17:54 Copy link github-actions bot commented Jun 21, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jun 21, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @vadiklyutiy , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request focuses on a targeted performance enhancement for the MRoPE input preparation within the model executor. By transitioning a critical position calculation from a PyTorch-based approach to a more efficient NumPy implementation, it aims to reduce latency in the _prepare_inputs stage, thereby contributing to overall system throughput and responsiveness. The changes are designed to be minimally invasive while delivering substantial speedups in a key computational bottleneck. Highlights Performance Optimization : This pull request introduces a significant performance optimization for the Multi-Query Rotary Positional Embedding (MRoPE) input preparation, specifically targeting the get_next_input_positions_tensor function which was identified as a bottleneck. NumPy Integration : The core of the optimization involves refactoring the position calculation logic to leverage NumPy for improved efficiency. A new static method, mrope_assign_next_input_positions , has been added to MRotaryEmbedding to perform these calculations directly using NumPy arrays. GPU Model Runner Update : The GPUModelRunner has been updated to utilize this new NumPy-based method. This includes creating a NumPy view of the existing CPU-bound position tensor ( self.mrope_positions_cpu.numpy() ) to allow direct, efficient assignment of calculated positions. Measured Speedup : Performance benchmarks show a 2.59x speedup for the _prepare_inputs average time (from 3.869 ms to 1.496 ms) and an overall E2E performance improvement of approximately 1.5% on the Qwen2.5-VL-3B-Instruct model under high load. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with üëç and üëé on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . ‚Ü© Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mergify bot added qwen Related to Qwen models v1 labels Jun 21, 2025 gemini-code-assist bot reviewed Jun 21, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request aims to speed up the MRoPE input preparation by leveraging numpy for calculations on CPU-pinned memory, which is a great approach. The changes look solid and the performance improvement is significant. I have one suggestion to further optimize the new numpy-based function by using vectorized operations instead of nested Python loops. This should provide an additional performance boost and make the code more idiomatic. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/rotary_embedding.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . fix comment ‚Ä¶ 029f1e3 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jun 23, 2025 WoosukKwon approved these changes Jun 23, 2025 View reviewed changes vllm/model_executor/layers/rotary_embedding.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . fix another comment ‚Ä¶ 8baa18e Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> Hide details View details WoosukKwon merged commit 9a3b883 into vllm-project : main Jun 24, 2025 66 of 69 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Member ywang96 commented Jun 24, 2025 Sorry for the late comment but this is great! üëç 1 vadiklyutiy reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yikun mentioned this pull request Jun 24, 2025 [Bugfix] Sync MRotaryEmbedding interface change to recover CI vllm-project/vllm-ascend#1399 Merged Yikun pushed a commit\n        to vllm-project/vllm-ascend\n      that referenced\n      this pull request Jun 24, 2025 [Bugfix] Sync MRotaryEmbedding interface change to recover CI ( #1399 ) ‚Ä¶ 5f5800b ### What this PR does / why we need it?\n\nSync MRotaryEmbedding interface change to recover main CI\n( vllm-project/vllm#19939 )\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nCI passed\n\n---------\n\nSigned-off-by: wangli <wangli858794774@gmail.com> gmarinho2 pushed a commit\n        to gmarinho2/vllm\n      that referenced\n      this pull request Jun 26, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ 2d7f8c3 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> weijinqian0 pushed a commit\n        to weijinqian0/vllm-ascend\n      that referenced\n      this pull request Jun 30, 2025 [Bugfix] Sync MRotaryEmbedding interface change to recover CI ( vllm-p‚Ä¶ ‚Ä¶ f3dc487 ‚Ä¶roject#1399 )\n\n### What this PR does / why we need it?\n\nSync MRotaryEmbedding interface change to recover main CI\n( vllm-project/vllm#19939 )\n\n### Does this PR introduce _any_ user-facing change?\nNo\n\n### How was this patch tested?\nCI passed\n\n---------\n\nSigned-off-by: wangli <wangli858794774@gmail.com> xjpang pushed a commit\n        to xjpang/vllm\n      that referenced\n      this pull request Jun 30, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ 0033778 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> wseaton pushed a commit\n        to wseaton/vllm\n      that referenced\n      this pull request Jun 30, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ 874817e Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: Will Eaton <weaton@redhat.com> wseaton pushed a commit\n        to wseaton/vllm\n      that referenced\n      this pull request Jun 30, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ 3c936c6 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> wwl2755-google pushed a commit\n        to wwl2755-google/vllm\n      that referenced\n      this pull request Jul 1, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ 4807582 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ f9327f0 Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [PERF] Speedup of MRoPE prepare inputs ( vllm-project#19939 ) ‚Ä¶ f84ab7e Signed-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:49",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: throughput, latency, Performance Test | SERVING: vllm serve, serve | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:50:49",
  "models": [
    "Qwen/Qwen2.5-VL-3B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen2.5-VL-3B-Instruct --tasks gsm8k --num_fewshot 5"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen2.5-VL-3B-Instruct --dataset-name random --num-prompts 1000",
  "commit_subject": "[PERF] Speedup of MRoPE prepare inputs (#19939)",
  "commit_message": "[PERF] Speedup of MRoPE prepare inputs (#19939)\n\nSigned-off-by: Vadim Gimpelson <vadim.gimpelson@centml.ai>",
  "commit_date": "2025-06-23T23:01:26-07:00",
  "files_changed": [
    "vllm/model_executor/layers/rotary_embedding.py",
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 4,
    "num_edited_lines": 35,
    "num_non_test_edited_lines": 35,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 9de233896..b7bb2affc 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -26,6 +26,7 @@\n import math\n from typing import Any, Optional, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n from transformers import PretrainedConfig\n@@ -1458,15 +1459,14 @@ class MRotaryEmbedding(RotaryEmbedding):\n         ]\n \n     @staticmethod\n-    def get_next_input_positions_tensor(\n-        mrope_position_delta: int,\n-        context_len: int,\n-        seq_len: int,\n-    ) -> torch.Tensor:\n-        return torch.arange(\n-            mrope_position_delta + context_len,\n-            mrope_position_delta + seq_len,\n-        ).expand(3, -1)\n+    def get_next_input_positions_tensor(out: np.ndarray, out_offset: int,\n+                                        mrope_position_delta: int,\n+                                        context_len: int, num_new_tokens: int):\n+\n+        values = np.arange(mrope_position_delta + context_len,\n+                           mrope_position_delta + context_len + num_new_tokens,\n+                           dtype=out.dtype)\n+        out[:, out_offset:out_offset + num_new_tokens] = values\n \n     @classmethod\n     def omni_get_updates_use_audio_in_video(\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 520d8fb18..40639fdf2 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -262,6 +262,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                 dtype=torch.int64,\n                 device=\"cpu\",\n                 pin_memory=self.pin_memory)\n+            self.mrope_positions_np = self.mrope_positions_cpu.numpy()\n \n         # Only relevant for models using ALiBi (e.g, MPT)\n         self.use_alibi = check_use_alibi(model_config)\n@@ -889,15 +890,13 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n                 dst_start = mrope_pos_ptr\n                 dst_end = mrope_pos_ptr + completion_part_len\n \n-                self.mrope_positions_cpu[:, dst_start:dst_end] = \\\n-                    MRotaryEmbedding.get_next_input_positions_tensor(\n-                        req.mrope_position_delta,\n-                        context_len=num_computed_tokens +\n-                        prompt_part_len,\n-                        seq_len=num_computed_tokens +\n-                        prompt_part_len +\n-                        completion_part_len,\n-                    )\n+                MRotaryEmbedding.get_next_input_positions_tensor(\n+                    out=self.mrope_positions_np,\n+                    out_offset=dst_start,\n+                    mrope_position_delta=req.mrope_position_delta,\n+                    context_len=num_computed_tokens + prompt_part_len,\n+                    num_new_tokens=completion_part_len,\n+                )\n \n                 mrope_pos_ptr += completion_part_len",
  "apis": [
    "MRotaryEmbedding.get_next_input_positions_tensor",
    "GPUModelRunner._calc_mrope_positions"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/rotary_embedding.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/model_runner.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies source code in non-test files (rotary_embedding.py and gpu_model_runner.py) in a non-trivial manner. It replaces a Torch-based implementation with a NumPy-based implementation for preparing input positions, which can reduce overhead and thus improve performance on CPU. The commit message also has a \"[PERF]\" label and a description (\"Speedup of MRoPE prepare inputs\"), and the changes affect a high-level API related to model execution. The modifications are directly aimed at better performance and are testable on CPU without the need for GPU accelerators. All conditions for a performance optimization commit are satisfied.",
  "llm_api_reason": "This commit no longer creates and returns a new tensor for computing MRoPE position offsets. Instead, it modifies the static method in MRotaryEmbedding to take an output numpy array and an offset and write the new position values directly into that array. In addition, the GPU model runner‚Äôs code that uses this method was updated accordingly so that it now passes self.mrope_positions_np (built from the CPU buffer) and proper slice parameters. These changes reduce memory allocations and improve performance while preparing input positions for MRoPE."
}