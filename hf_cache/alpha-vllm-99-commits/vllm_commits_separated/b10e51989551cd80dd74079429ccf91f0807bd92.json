{
  "commit_hash": "b10e51989551cd80dd74079429ccf91f0807bd92",
  "pr_url": "https://github.com/vllm-project/vllm/pull/16135",
  "pr_date": "2025-04-06",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Apr 6, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Minor optimizations Avoid redundant dictionary lookups cached_block_hash_to_block[block_hash] Avoid creating a list by using next Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon added 2 commits April 6, 2025 11:11 [V1][Minor] Optimize get_cached_block â€¦ 94d9874 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Avoid creating list â€¦ 05a922a Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon requested review from robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners April 6, 2025 18:19 Copy link github-actions bot commented Apr 6, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Apr 6, 2025 WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Apr 6, 2025 njhill approved these changes Apr 6, 2025 View reviewed changes comaniac approved these changes Apr 6, 2025 View reviewed changes comaniac enabled auto-merge (squash) April 6, 2025 19:06 Hide details View details comaniac merged commit b10e519 into main Apr 6, 2025 61 checks passed Uh oh! There was an error while loading. Please reload this page . comaniac deleted the minor-cache-opt branch April 6, 2025 20:48 lengrongfu pushed a commit\n        to lengrongfu/vllm\n      that referenced\n      this pull request Apr 7, 2025 [V1][Minor] Optimize get_cached_block ( vllm-project#16135 ) 5aaddbc yangw-dev pushed a commit\n        to yangw-dev/vllm\n      that referenced\n      this pull request Apr 21, 2025 [V1][Minor] Optimize get_cached_block ( vllm-project#16135 ) â€¦ eeeccf2 Signed-off-by: Yang Wang <elainewy@meta.com> lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [V1][Minor] Optimize get_cached_block ( vllm-project#16135 ) ff21ef5 RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [V1][Minor] Optimize get_cached_block ( vllm-project#16135 ) â€¦ 3d2f574 Signed-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:31",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:51:31",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1][Minor] Optimize get_cached_block (#16135)",
  "commit_message": "[V1][Minor] Optimize get_cached_block (#16135)",
  "commit_date": "2025-04-06T20:48:14Z",
  "files_changed": [
    "vllm/v1/core/block_pool.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 10,
    "num_non_test_edited_lines": 10,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/v1/core/block_pool.py b/vllm/v1/core/block_pool.py\nindex 43f30f710..74f3f7852 100644\n--- a/vllm/v1/core/block_pool.py\n+++ b/vllm/v1/core/block_pool.py\n@@ -67,11 +67,11 @@ class BlockPool:\n         Returns:\n             The cached block if it exists, or None.\n         \"\"\"\n-        if block_hash in self.cached_block_hash_to_block:\n-            first_block_id = list(\n-                self.cached_block_hash_to_block[block_hash].keys())[0]\n-            return self.cached_block_hash_to_block[block_hash][first_block_id]\n-        return None\n+        cached_blocks = self.cached_block_hash_to_block.get(block_hash)\n+        if not cached_blocks:\n+            return None\n+        first_block_id = next(iter(cached_blocks))\n+        return cached_blocks[first_block_id]\n \n     def cache_full_blocks(\n         self,",
  "apis": [
    "vllm.v1.core.block_pool.BlockPool.get_cached_block"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/block_pool.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/request.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/adapter_commons/request.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/request.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/structured_output/request.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file and replaces a method used to retrieve a cached block. Instead of building a list of keys and then extracting the first element, it now uses next(iter(...)) directly. Although the commit message includes the term \"Optimize\" and the change is minor, the intention is to improve the performance of the get_cached_block function by reducing unnecessary overhead in list construction. This qualifies as a performance optimization affecting a core API that runs on the CPU and is testable without specific hardware. Therefore, this commit meets the criteria for a performance/optimization-related change.",
  "llm_api_reason": "This commit optimizes the implementation of the get_cached_block method in the BlockPool class. The new code replaces explicit key lookup and list conversion with a more concise approach using the dictionaryâ€™s get() method and next(iter(...)). This change improves readability and potentially performance when retrieving a cached block."
}