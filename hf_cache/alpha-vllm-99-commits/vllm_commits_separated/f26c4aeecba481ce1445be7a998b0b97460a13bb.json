{
  "commit_hash": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
  "pr_url": "https://github.com/vllm-project/vllm/pull/11275",
  "pr_date": null,
  "timeline_text": "Copy link Collaborator ruisearch42 commented Dec 18, 2024 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes ray worker initialization time. In the current code base, ray.get(worker.get_node_ip.remote()) is called for each worker right after we get its handle, and it takes ~3s. This call is expensive because when RayWorkerWrapper.remote() just returns, we get an actor handle, but the actor itself may not be fully initialized yet. At this time, any method call on the actor would need to wait for actor initialization to happen, which can take some time (~3s in this case). And since we are calling ray.get(worker.get_node_ip.remote()) in a serialized manner for each newly created actor handle, this time adds up. For example, when we have TP=4, this would take ~12 seconds. We optimize this by making ray.get(worker.get_node_ip.remote()) calls on all the actor handles after they are created. And since these run in parallel, the total time taken is ~3s. So for TP = 4, this reduces ~9 seconds. I tested the following command: python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray Without this PR, _init_workers_ray takes ~18 seconds. And with it, it takes ~9 seconds. FIX #10283 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 jjyao reacted with thumbs up emoji All reactions üëç 1 reaction Copy link github-actions bot commented Dec 18, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ruisearch42 assigned comaniac Dec 18, 2024 comaniac approved these changes Dec 18, 2024 View reviewed changes Copy link Collaborator comaniac left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/executor/ray_gpu_executor.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . ruisearch42 force-pushed the opt_ray_worker_init branch\n    from dfa2cb8 to 0f453a7 Compare December 18, 2024 01:54 ruisearch42 added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Dec 18, 2024 ruisearch42 and others added 3 commits December 18, 2024 16:22 [Misc] Optimize ray worker initialization time ‚Ä¶ 30c4374 Signed-off-by: Rui Qiao <ruisearch42@gmail.com> up ‚Ä¶ 294e710 Signed-off-by: Rui Qiao <ruisearch42@gmail.com> Update vllm/executor/ray_gpu_executor.py ‚Ä¶ 8254b41 Co-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com> ruisearch42 force-pushed the opt_ray_worker_init branch\n    from 0f453a7 to 8254b41 Compare December 18, 2024 16:22 comaniac enabled auto-merge (squash) December 18, 2024 16:28 up ‚Ä¶ 918f192 Signed-off-by: Rui Qiao <ruisearch42@gmail.com> auto-merge was automatically disabled December 18, 2024 16:32 Head branch was pushed to by a user without write access youkaichao approved these changes Dec 19, 2024 View reviewed changes Copy link Member youkaichao left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment thanks for the fix! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 ruisearch42 reacted with thumbs up emoji All reactions üëç 1 reaction Hide details View details youkaichao merged commit f26c4ae into vllm-project : main Dec 19, 2024 54 checks passed Uh oh! There was an error while loading. Please reload this page . youkaichao reviewed Dec 19, 2024 View reviewed changes vllm/executor/ray_gpu_executor.py @@ -179,7 +188,7 @@ def sort_by_driver_then_worker_ip(worker): 3. Finally, if the work is on a node with smaller IP address, it should be placed first. \"\"\" ip = ray.get( worker .get_node_ip.remote()) ip = worker_to_ip[ worker ] Copy link Member youkaichao Dec 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @ruisearch42 this one looks concerning to me. we should change the tuple to sort, instead of using worker as the key. see the code from #11256 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author ruisearch42 Dec 19, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I see. Can you elaborate a bit on the concern? The pattern of using an external dict for sorting is not uncommon. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member youkaichao Dec 20, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment using an arbitrary python object as a key introduces quite unpredictable behavior and can have silent bugs. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member youkaichao Dec 20, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment it's not about using an external dict, it's about using the worker object as a dict key, which implicitly calls its __hash__ function. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author ruisearch42 Dec 21, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think the default behavior without a custom __hash__ function is to use the object's identity (memory address) as __hash__ and __eq__ , so it's pretty safe unless there is some non-standard user overridden __hash__ and __eq__ ? I think your implementation also makes sense. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions ruisearch42 mentioned this pull request Dec 20, 2024 [Bug]: extremely slow launching time possibly due to calling ray.init() again after it has already been called when launching vllm through ray cluster #11208 Closed 1 task mzusman pushed a commit\n        to mzusman/vllm\n      that referenced\n      this pull request Mar 12, 2025 [Misc] Optimize ray worker initialization time ( vllm-project#11275 ) ‚Ä¶ 073196d Signed-off-by: Rui Qiao <ruisearch42@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:21",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:21",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3 benchmarks/benchmark_latency.py --model meta-llama/Llama-3.1-8B-Instruct --tensor-parallel-size 4  --num-iters-warmup 5 --num-iters 20  --batch-size 8 --input-len 128 --output-len 256 --max-model-len 2048 --no-enable-prefix-caching --distributed-executor-backend ray",
  "commit_subject": "[Misc] Optimize ray worker initialization time (#11275)",
  "commit_message": "[Misc] Optimize ray worker initialization time (#11275)\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>",
  "commit_date": "2024-12-18T23:38:02-08:00",
  "files_changed": [
    "vllm/executor/ray_gpu_executor.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 4,
    "num_edited_lines": 35,
    "num_non_test_edited_lines": 35,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex 4bf5cbbd1..e2c549cbd 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -123,6 +123,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n \n         # Create the workers.\n         driver_ip = get_ip()\n+        workers = []\n         for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n             if not bundle.get(\"GPU\", 0):\n                 continue\n@@ -138,20 +139,30 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 scheduling_strategy=scheduling_strategy,\n                 **ray_remote_kwargs,\n             )(RayWorkerWrapper).remote(vllm_config=self.vllm_config)\n+            workers.append(worker)\n \n-            if self.use_ray_spmd_worker:\n-                self.workers.append(worker)\n-            else:\n-                worker_ip = ray.get(worker.get_node_ip.remote())\n-                if worker_ip == driver_ip and self.driver_dummy_worker is None:\n+        worker_ip_refs = [\n+            worker.get_node_ip.remote()  # type: ignore[attr-defined]\n+            for worker in workers\n+        ]\n+        worker_ips = ray.get(worker_ip_refs)\n+\n+        if not self.use_ray_spmd_worker:\n+            for i in range(len(workers)):\n+                worker = workers[i]\n+                worker_ip = worker_ips[i]\n+                if self.driver_dummy_worker is None and worker_ip == driver_ip:\n                     # If the worker is on the same node as the driver, we use it\n                     # as the resource holder for the driver process.\n                     self.driver_dummy_worker = worker\n                     self.driver_worker = RayWorkerWrapper(\n                         vllm_config=self.vllm_config)\n-                else:\n-                    # Else, added to the list of workers.\n-                    self.workers.append(worker)\n+                    workers.pop(i)\n+                    worker_ips.pop(i)\n+                    self.workers = workers\n+                    break\n+        else:\n+            self.workers = workers\n \n         logger.debug(\"workers: %s\", self.workers)\n         logger.debug(\"driver_dummy_worker: %s\", self.driver_dummy_worker)\n@@ -161,14 +172,12 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 \"adjusting the Ray placement group or running the driver on a \"\n                 \"GPU node.\")\n \n-        worker_ips = [\n-            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n-            for worker in self.workers\n-        ]\n         ip_counts: Dict[str, int] = {}\n         for ip in worker_ips:\n             ip_counts[ip] = ip_counts.get(ip, 0) + 1\n \n+        worker_to_ip = dict(zip(self.workers, worker_ips))\n+\n         def sort_by_driver_then_worker_ip(worker):\n             \"\"\"\n             Sort the workers based on 3 properties:\n@@ -179,7 +188,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             3. Finally, if the work is on a node with smaller IP address, it\n                 should be placed first.\n             \"\"\"\n-            ip = ray.get(worker.get_node_ip.remote())\n+            ip = worker_to_ip[worker]\n             return (ip != driver_ip, ip_counts[ip], ip)\n \n         # After sorting, the workers on the same node will be",
  "apis": [
    "None"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/executor/ray_distributed_executor.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/executor/ray_distributed_executor.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/api_server.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/api_server.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a source file (ray_gpu_executor.py) and alters the logic for initializing and managing Ray worker objects. In particular, it refactors the worker initialization by batching remote IP calls and streamlining worker management (e.g., the removal of redundant calls to ray.get). These changes are designed to reduce startup overhead and improve the efficiency of worker setup‚Äîa performance-related improvement on the CPU. Although the commit message mentions \"Optimize ray worker initialization time,\" the underlying changes go beyond mere refactoring or renaming and tackle a non-trivial performance bottleneck in the initialization process.",
  "llm_api_reason": "The commit changes the internal logic of the ray worker initialization within the RayGPUExecutor. It refactors how worker references are collected, their IPs are retrieved, and how the driver dummy worker is selected‚Äîall resulting in a performance optimization. No public or high‚Äêlevel API interfaces are modified by this change."
}