{
  "commit_hash": "6ce01f30667bbae33f112152e07a3b66b841078f",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7051",
  "pr_date": "2024-08-01",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Aug 1, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR optimizes the overhead of seq_group.get_seqs() , which was reported by @youkaichao . The solution is simple: We maintain seqs: List[Sequence] in addition to seqs_dict: Dict[int, Sequence] , and use seqs for all get_seqs calls. This leads to small performance boost (llama3 8B, 1xH100) Before: Throughput: 23.98 requests/s, 9914.65 tokens/s After: Throughput: 24.52 requests/s, 10138.92 tokens/s Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üöÄ 2 njhill and mgoin reacted with rocket emoji All reactions üöÄ 2 reactions WoosukKwon added 2 commits August 1, 2024 16:13 [Performance] Optimize get_seqs 6aae340 yapf 1f5b63d WoosukKwon requested a review\n  from youkaichao August 1, 2024 23:19 Copy link github-actions bot commented Aug 1, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 1, 2024 njhill approved these changes Aug 1, 2024 View reviewed changes Copy link Member njhill left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment lgtm! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/sequence.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/sequence.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Address review 4d3d3b9 youkaichao reviewed Aug 2, 2024 View reviewed changes vllm/sequence.py @@ -458,25 +459,24 @@ def __init__( self.prompt_adapter_request = prompt_adapter_request self.encoder_seq = encoder_seq self.trace_headers = trace_headers self._first_seq = next(iter(self.seqs_dict.values())) Copy link Member youkaichao Aug 2, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think you can still keep self._first_seq = seqs[0] , and use it to replace self.seqs[0] Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author WoosukKwon Aug 2, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think it doesn't hurt much to use seqs[0] without caching it? _first_seq was introduced to avoid the overhead of retrieving a value from the dictionary. I believe the overhead of seqs[0] will be negligible even if it's Python. Also, since the sequence can be removed, I feel more comfortable with self.seqs[0] than caching the sequence. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions youkaichao reviewed Aug 2, 2024 View reviewed changes Copy link Member youkaichao left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Glad to see it helps performance. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details WoosukKwon merged commit 6ce01f3 into main Aug 2, 2024 60 of 63 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the optimize-get-seqs branch August 2, 2024 01:29 youkaichao mentioned this pull request Aug 4, 2024 [Performance]: From SequenceGroup-native code to Sequence-native code #7116 Closed dtrifiro mentioned this pull request Aug 5, 2024 Sync with upstream@v0.5.4-7-g9118217f opendatahub-io/vllm#120 Closed mawong-amd mentioned this pull request Sep 3, 2024 Reconcile merge differences [fix Custom All Reduce; remove Torchrun & Cython] ROCm/vllm#163 Closed Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Performance] Optimize get_seqs ( vllm-project#7051 ) ‚Ä¶ a02da52 Signed-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Performance] Optimize get_seqs ( vllm-project#7051 ) ‚Ä¶ 2f46dfc Signed-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:23",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: Throughput, Throughput | TEST: CI, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:48:23",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance] Optimize `get_seqs` (#7051)",
  "commit_message": "[Performance] Optimize `get_seqs` (#7051)",
  "commit_date": "2024-08-01T18:29:52-07:00",
  "files_changed": [
    "vllm/core/block_manager_v1.py",
    "vllm/sequence.py",
    "vllm/transformers_utils/detokenizer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 3,
    "num_hunks": 9,
    "num_edited_lines": 44,
    "num_non_test_edited_lines": 44,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex e29eba375..d81648caa 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -700,5 +700,5 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         if self.enable_caching:\n-            for seq in seq_group.seqs_dict.values():\n+            for seq in seq_group.get_seqs():\n                 self.compute_full_blocks_in_seq(seq)\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ab50cfdfd..7ef9387c6 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -444,6 +444,7 @@ class SequenceGroup:\n         prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n     ) -> None:\n         self.request_id = request_id\n+        self.seqs = seqs\n         self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n         self.sampling_params = sampling_params\n         self.metrics = RequestMetrics(arrival_time=arrival_time,\n@@ -458,25 +459,24 @@ class SequenceGroup:\n         self.prompt_adapter_request = prompt_adapter_request\n         self.encoder_seq = encoder_seq\n         self.trace_headers = trace_headers\n-        self._first_seq = next(iter(self.seqs_dict.values()))\n \n     @property\n     def prompt(self) -> Optional[str]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return self._first_seq.prompt\n+        return self.seqs[0].prompt\n \n     @property\n     def prompt_token_ids(self) -> List[int]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return self._first_seq.prompt_token_ids\n+        return self.seqs[0].prompt_token_ids\n \n     @property\n     def multi_modal_data(self) -> \"MultiModalDataDict\":\n         # All sequences in the group should have the same multi-modal data.\n         # We use the multi-modal data of an arbitrary sequence.\n-        return self._first_seq.multi_modal_data\n+        return self.seqs[0].multi_modal_data\n \n     @property\n     def lora_int_id(self) -> int:\n@@ -512,7 +512,7 @@ class SequenceGroup:\n         #   in TPOT, rather than recalculating TTFT (since from the )\n         #   POV of the user, there is simply a long generation delay.\n         if (self.metrics.first_token_time is None\n-                and self.get_seqs()[0].get_output_len() == 1):\n+                and self.seqs[0].get_output_len() == 1):\n             self.metrics.first_token_time = time\n \n     def maybe_set_first_scheduled_time(self, time: float) -> None:\n@@ -548,9 +548,9 @@ class SequenceGroup:\n         self,\n         status: Optional[SequenceStatus] = None,\n     ) -> List[Sequence]:\n-        return list(self.seqs_dict.values()) if status is None else [\n-            seq for seq in self.seqs_dict.values() if seq.status == status\n-        ]\n+        if status is None:\n+            return self.seqs\n+        return [seq for seq in self.seqs if seq.status == status]\n \n     def is_encoder_decoder(self) -> bool:\n         return self.encoder_seq is not None\n@@ -559,22 +559,20 @@ class SequenceGroup:\n         return self.encoder_seq\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n-        return [\n-            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n-        ]\n+        return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n-        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n+        return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):\n         \"\"\"Update number of tokens computed so far.\"\"\"\n-        for seq in self.seqs_dict.values():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 seq.data.update_num_computed_tokens(num_new_computed_tokens)\n \n     def get_num_uncomputed_tokens(self) -> int:\n         num_uncomputed_tokens = 0\n-        for seq in self.get_seqs():\n+        for seq in self.seqs:\n             if not seq.is_finished():\n                 num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n         return num_uncomputed_tokens\n@@ -583,7 +581,7 @@ class SequenceGroup:\n         # Optimization. We don't need to call get_seqs if we don't need to\n         # filter by states.\n         if status is None:\n-            return len(self.seqs_dict)\n+            return len(self.seqs)\n \n         return len(self.get_seqs(status))\n \n@@ -602,23 +600,25 @@ class SequenceGroup:\n         if seq.seq_id in self.seqs_dict:\n             raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n         self.seqs_dict[seq.seq_id] = seq\n+        self.seqs.append(seq)\n \n     def remove(self, seq_id: int) -> None:\n-        if seq_id not in self.seqs_dict:\n+        seq = self.seqs_dict.pop(seq_id, None)\n+        if seq is None:\n             raise ValueError(f\"Sequence {seq_id} not found.\")\n-        del self.seqs_dict[seq_id]\n+        self.seqs.remove(seq)\n \n     def is_finished(self) -> bool:\n-        return all(seq.is_finished() for seq in self.get_seqs())\n+        return all(seq.is_finished() for seq in self.seqs)\n \n     def is_prefill(self) -> bool:\n         # Every sequence should be in the same stage.\n-        return self.get_seqs()[0].is_prefill()\n+        return self.seqs[0].is_prefill()\n \n     def __repr__(self) -> str:\n         return (f\"SequenceGroup(request_id={self.request_id}, \"\n                 f\"sampling_params={self.sampling_params}, \"\n-                f\"num_seqs={len(self.seqs_dict)})\")\n+                f\"num_seqs={len(self.seqs)})\")\n \n \n class SequenceGroupMetadata:\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nindex 76f418674..001af67f3 100644\n--- a/vllm/transformers_utils/detokenizer.py\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -40,7 +40,7 @@ class Detokenizer:\n         assert prms is not None\n \n         # We can pick any sequence for the prompt.\n-        seq = next(iter(seq_group.seqs_dict.values()))\n+        seq = seq_group.get_seqs()[0]\n         # Only prompt, without the generated token.\n         all_token_ids = seq.get_token_ids()\n         prompt_token_ids = all_token_ids[:-1]",
  "apis": [
    "SequenceGroup.get_seqs",
    "SequenceGroup.prompt",
    "SequenceGroup.prompt_token_ids",
    "BlockSpaceManagerV1.mark_blocks_as_computed",
    "Detokenizer.decode_prompt_logprobs_inplace"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/sequence.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/detokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/detokenizer.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The changes introduce an additional list (self.seqs) to store sequences and update various methods to use this list instead of iterating over a dictionary, which reduces overhead when fetching sequences. This modification is applied to key internal APIs (e.g., get_seqs, prompt, get_unfinished_seqs) that can impact performance by reducing unnecessary dictionary operations. Though the commit message contains the word \"Optimize\", careful examination shows that the changes modify core logic to improve performance rather than just renaming functions or refactoring without performance benefits. Additionally, the changes are applied to non-test source files and affect CPU performance in a testable way, satisfying the conditions for a performance-related commit.",
  "llm_api_reason": "This commit refactors how SequenceGroup exposes its sequences. Previously methods and properties (such as prompt, prompt_token_ids, and get_seqs) relied on an internal dictionary (seqs_dict) and a cached ‚Äú_first_seq‚Äù, but now they directly use the list (seqs). Similarly, BlockSpaceManagerV1.mark_blocks_as_computed and Detokenizer.decode_prompt_logprobs_inplace have been updated to fetch sequences from the optimized get_seqs() method. These changes improve performance by avoiding the overhead of reconstructing lists from a dict repeatedly."
}