{
  "commit_hash": "61b8cea3b42feab021d506e9143551de18f9165c",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21137",
  "pr_date": "2025-07-24",
  "timeline_text": "Copy link Collaborator LucasWilkinson commented Jul 17, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results (Optional) The necessary documentation update, such as updating supported_models.md and examples for a new model. Purpose Flash infer prefers host side CPU buffers in many cases, example: https://github.com/flashinfer-ai/flashinfer/blob/3c40456effae8b9c5b1a11c0d1e0594295b1a312/flashinfer/prefill.py#L1430-L1436 So we pass host side buffers (since #20466 we now have access to these) to reduce D2H transfers. Trace from main showing D2H transfers in plan Test Plan Test Result Accuracy Results VLLM_ATTENTION_BACKEND=FLASHINFER lm_eval --model vllm --model_args pretrained=met\na-llama/Meta-Llama-3-8B-Instruct --tasks gsm8k --batch_size auto\n...\nINFO 07-17 20:33:43 [cuda.py:253] Using FlashInfer backend on V1 engine.\n...\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.7536|¬±  |0.0119|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.7551|¬±  |0.0118| Benchmark Results Benchmark Command: python benchmarks/benchmark_throughput.py --model meta-llama/Llama-3.2-3B-Instruct --dataset-name random --input-len 256 --output-len 128 --num-prompts < N > --seed 42 Results (3 runs per condition, mean ¬± standard error): num-prompts Main Branch (req/s) This PR (req/s) 1 1.58 ¬± 0.06 1.90 ¬± 0.03 8 13.06 ¬± 0.11 14.32 ¬± 0.21 16 26.00 ¬± 0.07 28.74 ¬± 0.13 32 47.84 ¬± 0.57 46.53 ¬± 1.57 64 76.14 ¬± 0.45 81.43 ¬± 3.43 128 116.99 ¬± 6.10 127.78 ¬± 7.50 256 164.45 ¬± 6.12 177.70 ¬± 3.88 Tested on NVIDIA B200 GPU with meta-llama/Llama-3.2-3B-Instruct (256‚Üí128 tokens) (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link github-actions bot commented Jul 17, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added rocm Related to AMD ROCm speculative-decoding labels Jul 17, 2025 Copy link mergify bot commented Jul 17, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @LucasWilkinson . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added v1 needs-rebase labels Jul 17, 2025 gemini-code-assist bot reviewed Jul 17, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request is a significant and well-executed refactoring of the attention backend infrastructure. The primary goal of decoupling the metadata builders from the model runner has been achieved, which improves modularity and maintainability. The optimization for FlashInfer by preparing metadata on the CPU is a key improvement and has been implemented correctly. The introduction of CommonAttentionMetadata as a unified data structure is a solid design choice that simplifies the data flow to the attention backends. The refactoring of the speculative decoding logic, particularly in vllm/v1/spec_decode/eagle.py , to remove the Triton kernel in favor of a more readable PyTorch/NumPy implementation is a notable improvement. The addition of a comprehensive test suite in tests/v1/attention/test_attention_backends.py is excellent. It provides strong validation for the correctness of this large-scale refactoring by comparing various backends against a reference implementation under realistic conditions. Overall, the changes are of high quality and represent a positive step forward for the codebase. I have not identified any issues of high or critical severity. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions LucasWilkinson force-pushed the lwilkinson/flash-infer-host-buffers branch\n    from 87ccacf to 8af5f3b Compare July 18, 2025 00:36 mergify bot removed\n  the needs-rebase label Jul 18, 2025 LucasWilkinson marked this pull request as ready for review July 18, 2025 03:54 LucasWilkinson requested review from WoosukKwon , robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners July 18, 2025 03:54 WoosukKwon reviewed Jul 18, 2025 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment BTW why don't we use Numpy instead of PyTorch CPU tensors? Except for some edge cases, Numpy is usually faster in my experience. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor fhl2000 commented Jul 18, 2025 Could we still pass the device tensors to Flashinfer's plan() rather than host tensors? Because we might want to support full cudagraph of Flashinfer in the future (currently implemented in #20059 in rough), which requires managing device-side persistent buffers that can be reused across different decode wrappers. Here, one decode wrapper corresponds to a runtime shape that needs to be captured. Also, if we pass the host tensors to the wrapper,  it seems that H2D transfers still exist.  If I remember correctly, Sglang's implementation overrides the plan functions that still pass host-side persistent buffers, and also explicitly avoids certain D2H transfers. Hope it's helpful! @LucasWilkinson All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author LucasWilkinson commented Jul 18, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . BTW why don't we use Numpy instead of PyTorch CPU tensors? Except for some edge cases, Numpy is usually faster in my experience. Ive found going to and from numpy (i.e. .numpy() , torch::from_numpy can be a bit slow and only worth it if you are gonna do alot of ops; since FlashInfer ultimately wants torch tensors and for most of these theres only one or two ops per tensor im not sure its worth going to numpy; but I can scrub for tensors that are manipulated alot üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author LucasWilkinson commented Jul 18, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Could we still pass the device tensors to Flashinfer's plan() rather than host tensors? Because we might want to support full cudagraph of Flashinfer in the future (currently implemented in #20059 in rough), which requires managing device-side persistent buffers that can be reused across different decode wrappers. Here, one decode wrapper corresponds to a runtime shape that needs to be captured. If you look in FlashInfer's BatchDecodeWithPagedKVCacheWrapper you'll see the buffers get copied in the cudagraph path regardless: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L892-L910 and will get copied to the host: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L925-L926 Also, if we pass the host tensors to the wrapper, it seems that H2D transfers still exist. Yes; however H2D transfers are preferred over D2H as they can be done in a non-blocking fashion and do force synchronization with GPU. For the build call we are trying to optimize the CPU overhead so the fire-and-forget nature of the H2D transfers is better then depending on D2H transfer. If I remember correctly, Sglang's implementation overrides the plan functions that still pass host-side persistent buffers, and also explicitly avoids certain D2H transfers. Thats effectively what this PR does; the CPU buffers in CommonAttentionMetadata are views into the gpu_model_runner s persistent input_batch host side tensors. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor fhl2000 commented Jul 18, 2025 If I remember correctly, Sglang's implementation overrides the plan functions that still pass host-side persistent buffers, Oh my bad! Sorry, I was saying they are passing the device-side buffers. If you look in FlashInfer's BatchDecodeWithPagedKVCacheWrapper you'll see the buffers get copied in the cudagraph path regardless: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L892-L910 and will get copied to the host: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L925-L926 I am wondering if we can override this plan function that lets the wrapper directly own the device-side persistent buffer from VLLM, and avoid any unnecessary copy (device-to-device or host-to-device)?  At least for qo_indptr, which is equivalent to query_start_loc, we already have both cpu and gpu versions of it from common_attn_metadata, so we can just reuse them without any further copy. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author LucasWilkinson commented Jul 18, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . If I remember correctly, Sglang's implementation overrides the plan functions that still pass host-side persistent buffers, Oh my bad! Sorry, I was saying they are passing the device-side buffers. If you look in FlashInfer's BatchDecodeWithPagedKVCacheWrapper you'll see the buffers get copied in the cudagraph path regardless: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L892-L910 and will get copied to the host: https://github.com/flashinfer-ai/flashinfer/blob/1e9a41ad7f0efc5989bb0a2bf7e954902c8c73af/flashinfer/decode.py#L925-L926 I am wondering if we can override this plan function that lets the wrapper directly own the device-side persistent buffer from VLLM, and avoid any unnecessary copy (device-to-device or host-to-device)? At least for qo_indptr, which is equivalent to query_start_loc, we already have both cpu and gpu versions of it from common_attn_metadata, so we can just reuse them without any further copy. Is this what you are referring to? https://github.com/sgl-project/sglang/blob/719b29f218a09642193c4bda2a7ffa32829d5604/python/sglang/srt/layers/attention/flashinfer_backend.py#L1229 ?; not that familiar with sglang. This is an interesting idea; thanks for sharing! Regardless, even in this overridden version they pass host side buffers ( https://github.com/sgl-project/sglang/blob/719b29f218a09642193c4bda2a7ffa32829d5604/python/sglang/srt/layers/attention/flashinfer_backend.py#L1334-L1336 ); so if we want to override plan in the future I think we would still want this PR as a stepping stone (and override plan in follow up PR). üëç 1 fhl2000 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member mgoin commented Jul 18, 2025 Could you make sure to test the trtllm case in the flashinfer backend as well? Just want to make sure this choice is preferable for that backend as well if affected All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . fhl2000 mentioned this pull request Jul 22, 2025 [V1][CUDA] Full cudagraph support for FlashInfer #21367 Merged 4 tasks Copy link Collaborator Author LucasWilkinson commented Jul 23, 2025 @mgoin looks good üëç I think we should land this since its a win and I can follow up if using numpy helps VLLM_LOGGING_LEVEL=INFO cVLLM_USE_TRTLLM_DECODE_ATTENTION=1 VLLM_ATTENTION_BACKEND=FLASHINFER_VLLM_V1 lm_eval   --model vllm   --model_args '{\"pretrained\": \"meta-llama/Meta-Llama\n-3-8B-Instruct\"}'   --tasks gsm8k --batch_size auto\n...\nWARNING 07-23 11:40:01 [flashinfer.py:140] Using TRTLLM decode attention (auto-detected). \n...                                                 \nvllm ({'pretrained': 'meta-llama/Meta-Llama-3-8B-Instruct'}), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.7559|¬±  |0.0118|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.7574|¬±  |0.0118| üëç 1 mgoin reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . LucasWilkinson added 4 commits July 23, 2025 11:44 host buffers ‚Ä¶ 6b18ffb Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\nOptimize V1 FlashInfer backend to use CPU host buffers\n\n- Replace GPU-to-CPU transfers with direct CPU tensor construction\n- Build planning tensors from existing CommonAttentionMetadata CPU buffers\n- Reduce from 6x to 1x .cpu() calls during FlashInfer planning\n- Fix test mocks to handle correct argument count\n- Maintain compatibility with GPUModelRunner and FlashInfer V1 backend\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\ndont transfer block table\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\n\noptimize\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> reorder imports ‚Ä¶ 599ee48 Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> cleanup ‚Ä¶ 4e07e01 Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> cleanup ‚Ä¶ 585548e Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 23, 2025 mgoin approved these changes Jul 23, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Looks good to me, thanks! After review the amount of work we have to do on the CPU is more than I expected, so looking forward to seeing full cg Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 fhl2000 reacted with thumbs up emoji All reactions üëç 1 reaction LucasWilkinson added 2 commits July 23, 2025 13:32 fix attention test ‚Ä¶ 701fdc0 Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> format ‚Ä¶ b087694 Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> LucasWilkinson force-pushed the lwilkinson/flash-infer-host-buffers branch\n    from 155e954 to b087694 Compare July 23, 2025 17:33 format ‚Ä¶ 9723f3d Signed-off-by: Lucas Wilkinson <lwilkins@redhat.com> mgoin enabled auto-merge (squash) July 24, 2025 00:54 Hide details View details vllm-bot merged commit 61b8cea into vllm-project : main Jul 24, 2025 67 of 69 checks passed Uh oh! There was an error while loading. Please reload this page . elvischenv mentioned this pull request Jul 24, 2025 [Bugfix] Fix workspace buffer None issue for Flashinfer TRTLLM Backend #21525 Merged 4 tasks avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 3e6afaf ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 8b86ba2 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: shuw <shuw@nvidia.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 841628b ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ d368f33 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 39d315c ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 9a7c08f ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 965d4ef ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 484d958 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> BoyuanFeng pushed a commit\n        to BoyuanFeng/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 6b0bc15 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Boyuan Feng <boyuan@meta.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ c3786d8 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ f22e665 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Attention] Optimize FlashInfer MetadataBuilder Build call ( vllm-proj‚Ä¶ ‚Ä¶ 593f1b1 ‚Ä¶ect#21137 )\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:01",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, gsm8k | PERF: req/s, req/s, optimization | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:50:01",
  "models": [
    "meta-llama/Meta-Llama-3-8B-Instruct",
    "meta-llama/Llama-3.2-3B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Meta-Llama-3-8B-Instruct,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-3B-Instruct,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Meta-Llama-3-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)",
  "commit_message": "[Attention] Optimize FlashInfer MetadataBuilder Build call (#21137)\n\nSigned-off-by: Lucas Wilkinson <lwilkins@redhat.com>",
  "commit_date": "2025-07-24T03:21:46-07:00",
  "files_changed": [
    "tests/v1/attention/test_attention_backends.py",
    "tests/v1/attention/utils.py",
    "vllm/v1/attention/backends/flashinfer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 3,
    "num_hunks": 16,
    "num_edited_lines": 172,
    "num_non_test_edited_lines": 157,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/tests/v1/attention/test_attention_backends.py b/tests/v1/attention/test_attention_backends.py\nindex b4e0101a0..9bd0b9979 100644\n--- a/tests/v1/attention/test_attention_backends.py\n+++ b/tests/v1/attention/test_attention_backends.py\n@@ -11,7 +11,8 @@ from tests.v1.attention.utils import (BatchSpec, _Backend,\n                                       create_vllm_config,\n                                       get_attention_backend)\n from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE, cdiv\n-from vllm.v1.attention.backends.utils import CommonAttentionMetadata\n+from vllm.v1.attention.backends.utils import (CommonAttentionMetadata,\n+                                              set_kv_cache_layout)\n from vllm.v1.kv_cache_interface import FullAttentionSpec\n \n BACKENDS_TO_TEST = [\n@@ -212,7 +213,7 @@ def run_attention_backend(backend: _Backend, kv_cache_spec: FullAttentionSpec,\n \n         from vllm.v1.attention.backends.flashinfer import PerLayerParameters\n \n-        def mock_get_per_layer_parameters(vllm_config):\n+        def mock_get_per_layer_parameters(vllm_config, impl_cls):\n             # Return mock parameters for a single layer\n             head_size = vllm_config.model_config.get_head_size()\n             return {\n@@ -297,7 +298,8 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n     5. Comparing the vLLM backend's output to the ground-truth SDPA output.\n     \"\"\"\n     batch_spec = BATCH_SPECS[batch_spec_name]\n-    vllm_config = create_vllm_config(model_name=model)\n+    vllm_config = create_vllm_config(model_name=model,\n+                                     max_model_len=max(batch_spec.seq_lens))\n     device = torch.device(\"cuda:0\")\n \n     kv_cache_spec = create_standard_kv_cache_spec(vllm_config)\n@@ -419,6 +421,11 @@ def test_backend_correctness(batch_spec_name: str, model: str):\n         if backend_name == _Backend.FLASHINFER_VLLM_V1:\n             kv_cache_for_backend = kv_cache.transpose(0, 1)\n \n+            # For FlashInfer default to HND layout and\n+            kv_cache_for_backend = kv_cache_for_backend.transpose(\n+                2, 3).contiguous().transpose(2, 3)\n+            set_kv_cache_layout(\"HND\")\n+\n         backend_output = run_attention_backend(backend_name, kv_cache_spec,\n                                                vllm_config, device,\n                                                common_attn_metadata,\ndiff --git a/tests/v1/attention/utils.py b/tests/v1/attention/utils.py\nindex 30cfbdda5..69bd4a206 100644\n--- a/tests/v1/attention/utils.py\n+++ b/tests/v1/attention/utils.py\n@@ -66,7 +66,7 @@ def create_common_attn_metadata(\n     num_computed_tokens_cpu = torch.tensor(context_lens, dtype=torch.int32)\n \n     # Create block table (random for testing)\n-    max_blocks = max(batch_spec.seq_lens) // block_size + 1\n+    max_blocks = (max(batch_spec.seq_lens) + block_size - 1) // block_size\n     block_table_tensor = torch.randint(0,\n                                        max_block_idx,\n                                        (batch_spec.batch_size, max_blocks),\ndiff --git a/vllm/v1/attention/backends/flashinfer.py b/vllm/v1/attention/backends/flashinfer.py\nindex 953ef26c8..94d80d441 100755\n--- a/vllm/v1/attention/backends/flashinfer.py\n+++ b/vllm/v1/attention/backends/flashinfer.py\n@@ -18,6 +18,7 @@ from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n from vllm.config import VllmConfig\n from vllm.logger import init_logger\n from vllm.platforms import current_platform\n+from vllm.utils import cdiv\n from vllm.v1.attention.backends.flash_attn import use_cascade_attention\n from vllm.v1.attention.backends.utils import (\n     AttentionMetadataBuilder, CommonAttentionMetadata, PerLayerParameters,\n@@ -158,7 +159,7 @@ class FlashInferMetadata:\n     # (batch_size + 1,). The cumulative subquery lengths of the sequences in\n     # the batch, used to index into subquery. E.g., if the subquery length\n     # is [4, 6], it is [0, 4, 10].\n-    qo_indptr: torch.Tensor\n+    qo_indptr_cpu: torch.Tensor\n     # An example for paged_kv_indices, paged_kv_indptr:\n     # request 1, page indices [0, 5, 8]\n     # request 2, page indices [1, 6, 7]\n@@ -167,13 +168,13 @@ class FlashInferMetadata:\n     # [0, 5, 8, 1, 6, 7, 3, 4]\n     # paged_kv_indptr is used to index into paged_kv_indices:\n     # [0, 3, 6, 8]\n-    # The indptr of the paged kv cache, shape: [batch_size + 1]\n-    paged_kv_indptr: torch.Tensor\n-    # The page indices of the paged kv cache\n+    # The indptr of the paged kv cache, shape: [batch_size + 1] (CPU for plan)\n+    paged_kv_indptr_cpu: torch.Tensor\n+    # The page indices of the paged kv cache (on device for plan)\n     paged_kv_indices: torch.Tensor\n     # The number of entries in the last page of each request in\n-    # the paged kv cache, shape: [batch_size]\n-    paged_kv_last_page_len: torch.Tensor\n+    # the paged kv cache, shape: [batch_size] (CPU for plan)\n+    paged_kv_last_page_len_cpu: torch.Tensor\n     # The number of query/output heads\n     num_qo_heads: int\n     # The number of key/value heads\n@@ -201,22 +202,17 @@ class FlashInferMetadata:\n     num_prefills: int\n     num_prefill_tokens: int\n \n-    # For cascade attention.\n+    # For cascade attention (CPU for planning).\n     use_cascade: bool\n-    shared_qo_indptr: Optional[torch.Tensor] = None\n-    shared_kv_page_indptr: Optional[torch.Tensor] = None\n-    shared_kv_page_indices: Optional[torch.Tensor] = None\n-    shared_kv_last_page_len: Optional[torch.Tensor] = None\n+    shared_qo_indptr_cpu: Optional[torch.Tensor] = None\n+    shared_kv_page_indptr_cpu: Optional[torch.Tensor] = None\n+    shared_kv_page_indices_cpu: Optional[torch.Tensor] = None\n+    shared_kv_last_page_len_cpu: Optional[torch.Tensor] = None\n \n     prefill_wrapper: Optional[BatchPrefillWithPagedKVCacheWrapper] = None\n     decode_wrapper: Optional[BatchDecodeWithPagedKVCacheWrapper] = None\n     cascade_wrapper: Optional[MultiLevelCascadeAttentionWrapper] = None\n \n-    @property\n-    def query_start_loc(self):\n-        # The GPUModelRunner expects to be able to access this property.\n-        return self.qo_indptr\n-\n     def __post_init__(self):\n         if self.head_dim is not None:\n             FlashInferBackend.validate_head_size(self.head_dim)\n@@ -238,6 +234,12 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n         self.vllm_config = vllm_config\n         self.cache_config = vllm_config.cache_config\n         self.kv_cache_spec = kv_cache_spec\n+        max_num_blocks_per_request = cdiv(\n+            vllm_config.model_config.max_model_len,\n+            self.kv_cache_spec.block_size)\n+        self.block_table_arange = torch.arange(max_num_blocks_per_request,\n+                                               dtype=torch.int32,\n+                                               device=self.device)\n \n     def reorder_batch(self, input_batch: InputBatch,\n                       scheduler_output: SchedulerOutput) -> bool:\n@@ -285,21 +287,25 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n         if self.global_hyperparameters is None:\n             self.global_hyperparameters = infer_global_hyperparameters(\n                 get_per_layer_parameters(self.vllm_config, FlashInferImpl))\n+\n         if attn_metadata.use_cascade:\n             attn_metadata.cascade_wrapper = self._get_cascade_wrapper()\n             attn_metadata.cascade_wrapper.plan(\n-                [attn_metadata.shared_qo_indptr, attn_metadata.qo_indptr],\n                 [\n-                    attn_metadata.shared_kv_page_indptr,\n-                    attn_metadata.paged_kv_indptr\n+                    attn_metadata.shared_qo_indptr_cpu,\n+                    attn_metadata.qo_indptr_cpu\n+                ],\n+                [\n+                    attn_metadata.shared_kv_page_indptr_cpu,\n+                    attn_metadata.paged_kv_indptr_cpu\n                 ],\n                 [\n-                    attn_metadata.shared_kv_page_indices,\n+                    attn_metadata.shared_kv_page_indices_cpu,\n                     attn_metadata.paged_kv_indices\n                 ],\n                 [\n-                    attn_metadata.shared_kv_last_page_len,\n-                    attn_metadata.paged_kv_last_page_len\n+                    attn_metadata.shared_kv_last_page_len_cpu,\n+                    attn_metadata.paged_kv_last_page_len_cpu\n                 ],\n                 attn_metadata.num_qo_heads,\n                 attn_metadata.num_kv_heads,\n@@ -320,22 +326,22 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                 # Decodes are first so prefills start after the last decode\n                 prefill_start = num_decodes\n                 attn_metadata.prefill_wrapper = self._get_prefill_wrapper()\n-                assert attn_metadata.qo_indptr[prefill_start:].shape[\n+                assert attn_metadata.qo_indptr_cpu[prefill_start:].shape[\n                     0] == num_prefills + 1\n-                assert attn_metadata.paged_kv_indptr[prefill_start:].shape[\n+                assert attn_metadata.paged_kv_indptr_cpu[prefill_start:].shape[\n                     0] == num_prefills + 1\n-                assert attn_metadata.paged_kv_last_page_len[\n+                assert attn_metadata.paged_kv_last_page_len_cpu[\n                     prefill_start:].shape[0] == num_prefills\n                 # Since prefill_wrapper.run() will be called with\n                 # query[num_decode_tokens:] we need to adjust the qo_indptr\n                 # to be relative to the start of the prefill queries.\n-                qo_indptr = attn_metadata.qo_indptr[\n-                    prefill_start:] - attn_metadata.qo_indptr[prefill_start]\n+                qo_indptr_cpu = attn_metadata.qo_indptr_cpu[\n+                    prefill_start:] - attn_metadata.qo_indptr_cpu[prefill_start]\n                 attn_metadata.prefill_wrapper.plan(\n-                    qo_indptr,\n-                    attn_metadata.paged_kv_indptr[prefill_start:],\n+                    qo_indptr_cpu,\n+                    attn_metadata.paged_kv_indptr_cpu[prefill_start:],\n                     attn_metadata.paged_kv_indices,\n-                    attn_metadata.paged_kv_last_page_len[prefill_start:],\n+                    attn_metadata.paged_kv_last_page_len_cpu[prefill_start:],\n                     attn_metadata.num_qo_heads,\n                     attn_metadata.num_kv_heads,\n                     attn_metadata.head_dim,\n@@ -357,9 +363,9 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n                         attn_metadata.num_qo_heads, attn_metadata.num_kv_heads,\n                         attn_metadata.head_dim):\n                     attn_metadata.decode_wrapper.plan(\n-                        attn_metadata.paged_kv_indptr[:num_decodes + 1],\n+                        attn_metadata.paged_kv_indptr_cpu[:num_decodes + 1],\n                         attn_metadata.paged_kv_indices,\n-                        attn_metadata.paged_kv_last_page_len[:num_decodes],\n+                        attn_metadata.paged_kv_last_page_len_cpu[:num_decodes],\n                         attn_metadata.num_qo_heads,\n                         attn_metadata.num_kv_heads,\n                         attn_metadata.head_dim,\n@@ -383,55 +389,58 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             split_decodes_and_prefills(common_attn_metadata)\n \n         page_size = self.kv_cache_spec.block_size\n-        device = self.device\n-        qo_indptr = common_attn_metadata.query_start_loc\n         max_seq_len = common_attn_metadata.seq_lens_cpu.max()\n         seq_lens = common_attn_metadata.seq_lens\n+        seq_lens_cpu = common_attn_metadata.seq_lens_cpu\n         block_table_tensor = common_attn_metadata.block_table_tensor\n \n-        block_table_bounds = (seq_lens + page_size - 1) // page_size\n+        block_table_bounds_cpu = (seq_lens_cpu + page_size - 1) // page_size\n \n         use_cascade = common_prefix_len > 0\n         if use_cascade:\n             # Grab the blocks of the shared prefix from the first request.\n             assert common_prefix_len % page_size == 0\n             num_common_kv_blocks = common_prefix_len // page_size\n-            shared_qo_indptr = torch.tensor([0, num_actual_tokens],\n-                                            dtype=torch.int32,\n-                                            device=device)\n-            shared_kv_page_indptr = torch.tensor([0, num_common_kv_blocks],\n-                                                 dtype=torch.int32,\n-                                                 device=device)\n-            shared_kv_page_indices = block_table_tensor[\n+\n+            # Create CPU versions directly for cascade (no GPU versions needed)\n+            shared_qo_indptr_cpu = torch.tensor([0, num_actual_tokens],\n+                                                dtype=torch.int32,\n+                                                device='cpu')\n+            shared_kv_page_indptr_cpu = torch.tensor([0, num_common_kv_blocks],\n+                                                     dtype=torch.int32,\n+                                                     device='cpu')\n+            shared_kv_page_indices_cpu = block_table_tensor[\n                 0, :num_common_kv_blocks]\n-            shared_kv_last_page_len = torch.tensor([page_size],\n-                                                   dtype=torch.int32,\n-                                                   device=device)\n+            shared_kv_last_page_len_cpu = torch.tensor([page_size],\n+                                                       dtype=torch.int32,\n+                                                       device='cpu')\n+\n             # Remove the blocks of the shared prefix from all requests.\n             block_table_tensor = block_table_tensor[:, num_common_kv_blocks:]\n-            block_table_bounds -= num_common_kv_blocks\n+            block_table_bounds_cpu -= num_common_kv_blocks\n         else:\n-            shared_qo_indptr = None\n-            shared_kv_page_indptr = None\n-            shared_kv_page_indices = None\n-            shared_kv_last_page_len = None\n-\n-        mask = (torch.arange(block_table_tensor.size(1),\n-                             dtype=block_table_tensor.dtype,\n-                             device=block_table_tensor.device).unsqueeze(0)\n+            shared_qo_indptr_cpu = None\n+            shared_kv_page_indptr_cpu = None\n+            shared_kv_page_indices_cpu = None\n+            shared_kv_last_page_len_cpu = None\n+\n+        max_num_blocks = block_table_bounds_cpu.max()\n+        block_table_bounds = block_table_bounds_cpu.to(self.device,\n+                                                       non_blocking=True)\n+        mask = (self.block_table_arange[:max_num_blocks].unsqueeze(0)\n                 < block_table_bounds.unsqueeze(1))\n-        paged_kv_indices = block_table_tensor[mask]\n-\n-        paged_kv_indptr = torch.cat([\n-            torch.zeros(1,\n-                        dtype=block_table_bounds.dtype,\n-                        device=block_table_bounds.device),\n-            block_table_bounds.cumsum(dim=0, dtype=torch.int32)\n-        ])\n-\n-        paged_kv_last_page_len = seq_lens % page_size\n-        paged_kv_last_page_len = torch.where(paged_kv_last_page_len == 0,\n-                                             page_size, paged_kv_last_page_len)\n+        paged_kv_indices = block_table_tensor[:, :max_num_blocks][mask]\n+\n+        paged_kv_indptr_cpu = torch.zeros(len(block_table_bounds_cpu) + 1,\n+                                          dtype=torch.int32,\n+                                          device='cpu')\n+        paged_kv_indptr_cpu[1:] = block_table_bounds_cpu.cumsum(\n+            dim=0, dtype=torch.int32)\n+\n+        paged_kv_last_page_len_cpu = seq_lens_cpu % page_size\n+        paged_kv_last_page_len_cpu = torch.where(\n+            paged_kv_last_page_len_cpu == 0, page_size,\n+            paged_kv_last_page_len_cpu)\n         cache_dtype = self.cache_config.cache_dtype\n         if cache_dtype.startswith(\"fp8\"):\n             kv_cache_dtype = FlashInferBackend.get_fp8_dtype_for_flashinfer(\n@@ -440,10 +449,10 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             kv_cache_dtype = self.kv_cache_spec.dtype\n         attn_metadata = FlashInferMetadata(\n             num_actual_tokens=num_actual_tokens,\n-            qo_indptr=qo_indptr,\n-            paged_kv_indptr=paged_kv_indptr,\n+            qo_indptr_cpu=common_attn_metadata.query_start_loc_cpu,\n+            paged_kv_indptr_cpu=paged_kv_indptr_cpu,\n             paged_kv_indices=paged_kv_indices,\n-            paged_kv_last_page_len=paged_kv_last_page_len,\n+            paged_kv_last_page_len_cpu=paged_kv_last_page_len_cpu,\n             num_qo_heads=self.vllm_config.model_config.get_num_attention_heads(\n                 self.vllm_config.parallel_config),\n             num_kv_heads=self.kv_cache_spec.num_kv_heads,\n@@ -457,14 +466,14 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             num_prefills=num_prefills,\n             num_prefill_tokens=num_prefill_tokens,\n             use_cascade=use_cascade,\n-            shared_qo_indptr=shared_qo_indptr,\n-            shared_kv_page_indptr=shared_kv_page_indptr,\n-            shared_kv_page_indices=shared_kv_page_indices,\n-            shared_kv_last_page_len=shared_kv_last_page_len,\n+            shared_qo_indptr_cpu=shared_qo_indptr_cpu,\n+            shared_kv_page_indptr_cpu=shared_kv_page_indptr_cpu,\n+            shared_kv_page_indices_cpu=shared_kv_page_indices_cpu,\n+            shared_kv_last_page_len_cpu=shared_kv_last_page_len_cpu,\n             max_seq_len=max_seq_len,\n             seq_lens=seq_lens,\n             block_table_tensor=block_table_tensor,\n-            workspace_buffer=self._workspace_buffer,\n+            workspace_buffer=self._get_workspace_buffer(),\n         )\n \n         self._plan(num_prefills, num_decodes, attn_metadata)",
  "apis": [
    "vllm.v1.attention.backends.flashinfer.FlashInferMetadata",
    "vllm.v1.attention.backends.flashinfer.FlashInferMetadataBuilder",
    "vllm.v1.attention.backends.utils.set_kv_cache_layout"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/utils/flashinfer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/flashinfer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/flashinfer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/adapter_commons/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/multimodal/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/profiler/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/kernels/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/cutlass_benchmarks/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/structured_output/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/model_loader/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/benchmarks/lib/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/punica_wrapper/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/sched/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/kv_transfer/kv_connector/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/tool_parsers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/ops/triton_ops/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/quark/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/compressed_tensors/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/kv_cache_interface.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The patch modifies non-test source files (e.g., vllm/v1/attention/backends/flashinfer.py) and introduces changes to how metadata is computed and managed for the FlashInfer backend. The changes include reworking tensor layout handling (e.g., CPU vs GPU tensors), refining block calculations, and updating function signatures. These alterations aim to improve performance by optimizing memory layout and processing paths. The changes are non-trivial modifications to internal APIs that impact performance on CPU and are testable without GPU-specific workload instructions. Thus, they satisfy the criteria for a performance/optimization-related commit.",
  "llm_api_reason": "The commit adjusts and optimizes how FlashInfer‚Äôs attention metadata is built. In the tests, it updates the configuration call (passing max_model_len) and makes use of a newly‚Äêexposed utility function to force the ‚ÄúHND‚Äù KV cache layout. In the FlashInfer backend itself, several field names are renamed (for example, ‚Äúqo_indptr‚Äù, ‚Äúpaged_kv_indptr‚Äù, and ‚Äúpaged_kv_last_page_len‚Äù are replaced by their CPU counterparts) and the MetadataBuilder‚Äôs plan() call is updated to use the CPU versions. These changes affect the public FlashInfer metadata and associated configuration functions."
}