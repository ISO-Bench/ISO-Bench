{
  "commit_hash": "2deb029d115dadd012ce5ea70487a207cb025493",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7822",
  "pr_date": null,
  "timeline_text": "Copy link Collaborator comaniac commented Aug 23, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Closes #7619 With the investigation in #7619 , the root cause of block manager v2 low throughput with prefix caching is that block manager v2 doesn't mark prefix cache hit blocks as computed right after scheduling a batch. Specifically, the life cycle of a prefix cache block is as follows: The block is allocated by the first sequence of a batch. At this moment it will be added to  \"cached blocks\", but won't be marked as computed; otherwise the rest sequences in the same batch will skip the computation of this block and result in incorrect output. When the batch of sequence is finished (prefill+decode), the blocks are freed and added to the evictor. When the sequence of a following batch allocates the same block, it will be activated from the evictor and marked as computed. Here is a simple illustration. Note that we assume each sequence is in different batch. seq 1: [allocate-block-uncomputed] -- [prefill] --[decode1] --  ... -- [decodeN] -- [free-block]\nseq 2:                                [allocate-block-uncomputed] -- ...\n...\nseq N:                                                                                          [allocate-block-computed] -- ... Meanwhile, block manager v1 marks the block as computed right after the prefill is scheduled: seq 1: [allocate-block-uncomputed] -- [prefill] --[decode1] --  ... -- [decodeN] -- [free-block]\nseq 2:                                [allocate-block-computed] -- ...\n... This PR fixes this issue by marking allocated blocks as touched, and let scheduler mark them as computed to achieve the same behavior of block manager v1. Benchmark on L4 Command python3 benchmarks/benchmark_prefix_caching.py \\\n    --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 \\\n    --output-len 200 \\\n    --enable-prefix-caching \\\n    [--use-v2-block-manager] Branch Block Manager Warmup (s) Processed (s) main v1 14.5 13.4 main v2 23.6 13.4 PR v1 14.5 13.3 PR v2 14.4 13.3 cc @cadedaniel @rkooo567 @Yard1 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link github-actions bot commented Aug 23, 2024 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Yard1 reviewed Aug 23, 2024 View reviewed changes Copy link Collaborator Yard1 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, some comments Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/core/block/prefix_caching_block.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block/prefix_caching_block.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/core/block/prefix_caching_block.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . comaniac added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 23, 2024 comaniac added 3 commits August 26, 2024 09:29 done w/o test 5daf36c add test 6d8a610 use set 020ac13 comaniac force-pushed the fix-v2-prefix-cache branch\n    from fd9c7c7 to 020ac13 Compare August 26, 2024 16:30 Yard1 approved these changes Aug 26, 2024 View reviewed changes Hide details View details comaniac merged commit 2deb029 into vllm-project : main Aug 26, 2024 42 checks passed Uh oh! There was an error while loading. Please reload this page . comaniac deleted the fix-v2-prefix-cache branch August 26, 2024 18:24 Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Performance][BlockManagerV2] Mark prefix cache block as computed aftâ€¦ â€¦ ed30706 â€¦er schedule ( vllm-project#7822 )\n\nSigned-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Performance][BlockManagerV2] Mark prefix cache block as computed aftâ€¦ â€¦ 9e9c3a0 â€¦er schedule ( vllm-project#7822 )\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:12",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: throughput | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:48:12",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3 benchmarks/benchmark_prefix_caching.py --model neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --output-len 200 --enable-prefix-caching [--use-v2-block-manager]",
  "commit_subject": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)",
  "commit_message": "[Performance][BlockManagerV2] Mark prefix cache block as computed after schedule (#7822)",
  "commit_date": "2024-08-26T11:24:53-07:00",
  "files_changed": [
    "tests/core/block/test_prefix_caching_block.py",
    "vllm/core/block/prefix_caching_block.py",
    "vllm/core/block_manager_v2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 3,
    "num_hunks": 6,
    "num_edited_lines": 63,
    "num_non_test_edited_lines": 32,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/core/block/test_prefix_caching_block.py b/tests/core/block/test_prefix_caching_block.py\nindex c2226870c..25be2dd13 100644\n--- a/tests/core/block/test_prefix_caching_block.py\n+++ b/tests/core/block/test_prefix_caching_block.py\n@@ -708,6 +708,37 @@ class TestPrefixCachingBlockAllocator:\n                                                token_ids=token_ids)\n         assert allocator.get_prefix_cache_hit_rate() > 0.99\n \n+    # Test case for marking cache hit blocks as computed right after\n+    # a batch of prefill sequences are scheduled.\n+    @staticmethod\n+    def test_touch_block():\n+        block_size = 16\n+        common_blocks = 4\n+        allocator = PrefixCachingBlockAllocator(num_blocks=8,\n+                                                block_size=block_size)\n+\n+        common_token_ids = list(range(block_size * common_blocks))\n+\n+        # Mimic the behavior of allocating the same block chain\n+        # (i.e., common prefix) for a batch of 3 different prefill sequences.\n+        for _ in range(3):\n+            blocks = TestPrefixCachingBlockAllocator.create_immutable_chain(\n+                block_size=block_size,\n+                token_ids=common_token_ids,\n+                allocator=allocator,\n+            )\n+            block_ids = [block.block_id for block in blocks]\n+            # The allocated blocks should  be marked as touched\n+            # but not computed.\n+            computed_block_ids = allocator.get_computed_block_ids(\n+                [], block_ids, skip_last_block_id=False)\n+            assert len(computed_block_ids) == 0\n+\n+        allocator.mark_blocks_as_computed([])\n+        computed_block_ids = allocator.get_computed_block_ids(\n+            [], block_ids, skip_last_block_id=False)\n+        assert len(computed_block_ids) == common_blocks\n+\n     @staticmethod\n     def create_immutable_chain(\n         block_size: int,\ndiff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex 432a6651a..a87e814cf 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -1,6 +1,6 @@\n \"\"\"Token blocks.\"\"\"\n from os.path import commonprefix\n-from typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n+from typing import Dict, FrozenSet, Iterable, List, Optional, Set, Tuple\n \n from vllm.core.block.common import (CacheMetricData, CopyOnWriteTracker,\n                                     get_all_blocks_recursively)\n@@ -73,6 +73,11 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # prefix hash will be in this dict, even if they have refcount 0.\n         self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n \n+        # A list of immutable block IDs that have been touched by scheduler\n+        # and should be marked as computed after an entire batch of sequences\n+        # are scheduled.\n+        self._touched_blocks: Set[BlockId] = set()\n+\n         # Used to track status of each physical block id\n         self._block_tracker: Dict[BlockId, BlockTracker] = {}\n         for block_id in block_ids:\n@@ -438,10 +443,14 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         assert self._refcounter.get(block.block_id) > 0\n \n         if block.content_hash not in self._cached_blocks:\n-            # No cached content hash => Set this block as cached\n-            # (Note that this block is not computed yet =>\n-            #  Will be computed after free())\n+            # No cached content hash => Set this block as cached.\n+            # Note that this block cannot be marked as computed yet\n+            # because other sequences in the same batch cannot reuse\n+            # this block.\n             self._cached_blocks[block.content_hash] = block.block_id\n+            # Mark this block as touched so that it can be marked as\n+            # computed after the entire batch of sequences are scheduled.\n+            self._touched_blocks.add(block.block_id)\n             return block.block_id\n \n         # Reuse the cached content hash\n@@ -507,7 +516,10 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n                     \"Mark block as accessed which is not belonged to GPU\")\n \n     def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n-        raise NotImplementedError(\"Marking as computed is incremental\")\n+        # Mark all touched blocks as computed.\n+        for block_id in self._touched_blocks:\n+            self._block_tracker[block_id].computed = True\n+        self._touched_blocks.clear()\n \n     def _track_block_id(self, block_id: Optional[BlockId],\n                         computed: bool) -> None:\ndiff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex b7d9451f1..7d4919a0d 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -287,11 +287,11 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n                 seq.seq_id, now)\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n-        # The only need for mark block as computed is for prefix caching,\n-        # while currently we could determine whether one block is computed\n-        # or not by check whether it has content hash.\n-        # So this function is useless for block_v2.\n-        pass\n+        # If prefix caching is enabled, mark immutable blocks as computed\n+        # right after they have been scheduled (for prefill). This assumes\n+        # the scheduler is synchronous so blocks are actually computed when\n+        # scheduling the next batch.\n+        self.block_allocator.mark_blocks_as_computed([])\n \n     def get_common_computed_block_ids(\n             self, seqs: List[Sequence]) -> GenericSequence[int]:",
  "apis": [
    "PrefixCachingBlockAllocator.mark_blocks_as_computed",
    "BlockSpaceManagerV2.mark_blocks_as_computed"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/prefix_caching_block.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block_manager.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies both production code (in vllm/core/block/prefix_caching_block.py and vllm/core/block_manager_v2.py) and tests (in tests/core/block/test_prefix_caching_block.py). The changes include implementing logic to mark blocks as computed based on a touched blocks set, which streamlines the caching mechanism after scheduling, thus likely reducing redundant computation. Although the commit message mentions \"[Performance]\", our analysis confirms that the changes adjust internal handling to improve caching efficiencyâ€”a fundamental performance optimization. The modifications impact core APIs and address CPU-based performance improvements. Therefore, it satisfies the conditions for a performance-related commit.",
  "llm_api_reason": "The commit adds a new implementation for marking blocks as computed right after scheduling a batch of prefill sequences. In PrefixCachingBlockAllocator, the method mark_blocks_as_computed is now implemented to iterate over the touched blocks, mark them computed, and clear the touched blocks list. Meanwhile, in BlockSpaceManagerV2 the previously empty mark_blocks_as_computed method is updated to call the allocatorâ€™s new implementation. These changes affect the respective publicly exposed APIs."
}