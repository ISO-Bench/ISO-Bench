{
  "commit_hash": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
  "pr_url": "https://github.com/vllm-project/vllm/pull/10785",
  "pr_date": "2024-12-03",
  "timeline_text": "Copy link Collaborator aarnphm commented Nov 29, 2024 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Add initial support for XGrammar for V0 and makes it the default for grammar and json usage. Written in collaboration with @mgoin I'm using the benchmark scripts from #10557 Results for using XGrammar as backend: Throughput: 0.94 requests/s, 1022.46 total tokens/s, 480.27 output tokens/s Correct rate is 100.0 %\nFirst token latency(msecs):\ncount      10.000000\nmean     4552.206317\nstd       734.671745\nmin      3289.774953\n25%      3864.269087\n50%      5102.686635\n75%      5102.717258\nmax      5114.346570\ndtype: float64\nNext token latency(msecs):\ncount    10.000000\nmean     11.906452\nstd       1.409063\nmin      10.831970\n25%      10.837367\n50%      10.854235\n75%      13.227200\nmax      14.325024\ndtype: float64 Comparing to outlines Throughput: 0.22 requests/s, 241.22 total tokens/s, 113.31 output tokens/s Correct rate is 100.0 %\nFirst token latency(msecs):\ncount       10.000000\nmean     38533.083248\nstd         35.807892\nmin      38491.813741\n25%      38491.826321\n50%      38556.601226\n75%      38556.628519\nmax      38568.547848\ndtype: float64\nNext token latency(msecs):\ncount    10.000000\nmean     12.955556\nstd       0.042220\nmin      12.901755\n25%      12.914099\n50%      12.953058\n75%      12.996646\nmax      13.003127\ndtype: float64 NOTE: Running on A100 80GB, with Llama 3.2 3B with chunked prefill enable and JSON grammar Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 9 zhouyuan, choisioo, xuechendi, ywang96, nickandbro, JakubCerven, saattrupdan, hongqing1986, and suc16 reacted with thumbs up emoji All reactions üëç 9 reactions aarnphm added 3 commits November 29, 2024 22:53 --wip-- ‚Ä¶ 41c0031 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> fix: update workaround for pickling ‚Ä¶ c17da0b Signed-off-by: Aaron Pham <contact@aarnphm.xyz> hack: hmm it is a tuple ‚Ä¶ b29dfb3 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> aarnphm requested review from zhuohan123 , youkaichao , alexm-redhat , comaniac and njhill as code owners November 29, 2024 23:45 Copy link github-actions bot commented Nov 29, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added documentation Improvements or additions to documentation ci/build labels Nov 29, 2024 revert: bad merge ‚Ä¶ 1be065b Signed-off-by: Aaron Pham <contact@aarnphm.xyz> aarnphm marked this pull request as draft November 29, 2024 23:46 aarnphm added 2 commits November 30, 2024 00:08 fix: correct use apply_token_bitmask interface ‚Ä¶ ee8e796 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> fix: correctness for prefill ‚Ä¶ cef4201 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> aarnphm marked this pull request as ready for review November 30, 2024 00:16 aarnphm added 3 commits November 30, 2024 00:23 fix: lint error ‚Ä¶ 919e5f8 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> fix: annotations ‚Ä¶ 4d6585b Signed-off-by: Aaron Pham <contact@aarnphm.xyz> fix: format ‚Ä¶ 5d2a43c Signed-off-by: Aaron Pham <contact@aarnphm.xyz> Ubospica reviewed Nov 30, 2024 View reviewed changes Copy link Ubospica left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for your contribution to integrating XGrammar into vLLM! It overall looks good, but there are some minor points to enhance parallelism. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/guided_decoding/__init__.py Outdated guided_params: GuidedDecodingParams, tokenizer ) -> Optional[ LogitsProcessor ] : guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizer, model_config: ModelConfig ) -> LogitsProcessor | None : # CFG grammar not supported by LMFE, so we use outlines instead if guided_params.backend == 'outlines' or guided_params.grammar: Copy link Ubospica Nov 30, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment XGrammar can also do grammar decoding and accelerate it. The grammar formats for XGrammar and Outlines are different. XGrammar uses GBNF format, while Outlines uses lark grammar. That might be documented. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author aarnphm Dec 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment i see, I will add this difference into the docs Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author aarnphm Dec 1, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think we should just remove the grammar check here. If user send grammar they should also specify the backend (probably better to document the cartesian product of the combinations) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 Ubospica reacted with thumbs up emoji All reactions üëç 1 reaction vllm/model_executor/guided_decoding/xgrammar_decoding.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/guided_decoding/xgrammar_decoding.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . joennlae added a commit\n        to 44ai-labs/vllm\n      that referenced\n      this pull request Dec 1, 2024 [Core] add xgrammar as guided generation provider ‚Ä¶ d326148 Essentially a cleaned up version of this `pr`: vllm-project#10785 Especially since `outlines` is rather slow and the new version is though\nto intergrate as they do not focus on being pickleable which is a key\nfeature for us using the multiprocessing engine: dottxt-ai/outlines-core#99 I assume more and more will change over to `xgrammar`.\n\nThis is a minimum implementation. https://arxiv.org/pdf/2411.15100 Signed-off-by: Jannis Sch√∂nleber <joennlae@gmail.com> joennlae mentioned this pull request Dec 1, 2024 [Core] add xgrammar as guided generation provider #10803 Closed aarnphm and others added 3 commits November 30, 2024 20:54 chore: remove grammar mode branch with outlines ‚Ä¶ 3770400 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> Add caching for tokenizer data and grammar compiler ‚Ä¶ 865e2a3 Signed-off-by: mgoin <michael@neuralmagic.com> Merge branch 'feat/xgrammar' of https://github.com/aarnphm/vllm into ‚Ä¶ ‚Ä¶ e5684e2 ‚Ä¶feat/xgrammar Copy link Member mgoin commented Dec 1, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Updated this PR with caches for the tokenizer data and the grammar compiler to avoid constructing these data structures for each request. It isn't pretty but it boosts throughput by about 1.4x. I need to perform more profiling but we are limited by the required-serialization architecture that we currently have. We plan to move the FSM initialization out of the frontend to both simplify the implementation and speed up TTFT. Setup: Llama-3.1-8B-Instruct, 1xH100 Command: python benchmark_guided.py --model meta-llama/Llama-3.1-8B-Instruct --dataset xgrammar_bench --async-engine --output-len 512 --num-prompts 20 --enable-chunked-prefill --guided-decoding-ratio 1 Before: Throughput: 1.46 requests/s, 1189.12 total tokens/s, 748.00 output tokens/s Correct rate is 95.0 % \nFirst token latency(msecs):\ncount      20.000000\nmean     7180.142369\nstd      1212.973158\nmin      4644.173431\n25%      7012.610644\n50%      7578.541221\n75%      8079.524654\nmax      8092.886029\ndtype: float64\nNext token latency(msecs):\ncount    20.000000\nmean     12.662371\nstd       2.336552\nmin      10.942158\n25%      10.942283\n50%      11.864077\n75%      12.990130\nmax      17.550802\ndtype: float64 After: Throughput: 2.12 requests/s, 1726.67 total tokens/s, 1086.13 output tokens/s Correct rate is 95.0 % \nFirst token latency(msecs):\ncount      20.000000\nmean     3254.682581\nstd       290.516334\nmin      2869.083916\n25%      2869.120228\n50%      3449.280638\n75%      3477.460549\nmax      3477.504314\ndtype: float64\nNext token latency(msecs):\ncount    20.000000\nmean     12.054585\nstd       0.550868\nmin      11.643879\n25%      11.643967\n50%      11.674903\n75%      12.786106\nmax      12.786302\ndtype: float64 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . joennlae added a commit\n        to 44ai-labs/vllm\n      that referenced\n      this pull request Dec 1, 2024 [Core] add xgrammar as guided generation provider ‚Ä¶ caf4289 Essentially a cleaned up version of this `pr`: vllm-project#10785 Especially since `outlines` is rather slow and the new version is though\nto intergrate as they do not focus on being pickleable which is a key\nfeature for us using the multiprocessing engine: dottxt-ai/outlines-core#99 I assume more and more will change over to `xgrammar`.\n\nThis is a minimum implementation. https://arxiv.org/pdf/2411.15100 Signed-off-by: Jannis Sch√∂nleber <joennlae@gmail.com> dongxiaolong mentioned this pull request Dec 2, 2024 [Feature]: Integrate with XGrammar for zero-overhead structured generation in LLM inference. #10660 Closed 1 task Copy link Member mgoin commented Dec 2, 2024 @Ubospica do you know when XGrammar can support regex? This would help with covering existing use cases All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . 7 hidden items Load more‚Ä¶ mgoin added 3 commits December 2, 2024 22:54 Fix tests and support json_object ‚Ä¶ 8962301 Signed-off-by: mgoin <michael@neuralmagic.com> Fix test 8d3c671 Merge branch 'main' into feat/xgrammar 9f97093 mgoin requested review from DarkLight1337 , robertgshaw2-redhat and simon-mo as code owners December 2, 2024 22:56 mergify bot added\n  the frontend label Dec 2, 2024 simon-mo changed the title [Core][Performance] Add XGrammar support for guided decoding [Core][Performance] Add XGrammar support for guided decoding and set it as default Dec 3, 2024 simon-mo previously approved these changes Dec 3, 2024 View reviewed changes vllm/entrypoints/llm.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . simon-mo dismissed\n    their stale review December 3, 2024 01:41 if isinstance(params, Sequence) else copy.copy(params), is actually a blocking review. We can only introduce it if it is not perf regression. Move copy down into guided decoding case ‚Ä¶ 975e040 Signed-off-by: mgoin <michael@neuralmagic.com> Copy link Member mgoin commented Dec 3, 2024 Thanks for review @simon-mo I moved the copy into a specific if sampling_params.guided_decoding is not None case - ready for re-review All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . aarnphm added 2 commits December 2, 2024 22:11 chore: fix coallesce type ‚Ä¶ 59221e6 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> chore: add notes for performance ‚Ä¶ 5f49734 Signed-off-by: Aaron Pham <contact@aarnphm.xyz> aarnphm force-pushed the feat/xgrammar branch\n    from 4ee464a to 5f49734 Compare December 3, 2024 03:16 simon-mo approved these changes Dec 3, 2024 View reviewed changes Hide details View details DarkLight1337 merged commit 9323a31 into vllm-project : main Dec 3, 2024 73 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Member hmellor commented Dec 3, 2024 The new dependency in this PR appears to have broken installation on ARM 8.373 ERROR: Could not find a version that satisfies the requirement xgrammar (from versions: none)\n8.419 ERROR: No matching distribution found for xgrammar\n------\nDockerfile.arm:37\n--------------------\n  36 |     \n  37 | >>> RUN --mount=type=cache,target=/root/.cache/pip \\\n  38 | >>>     --mount=type=bind,src=requirements-common.txt,target=requirements-common.txt \\\n  39 | >>>     --mount=type=bind,src=requirements-cpu.txt,target=requirements-cpu.txt \\\n  40 | >>>     pip install -v -r requirements-cpu.txt\n  41 |     \n--------------------\nERROR: failed to solve: process \"/bin/sh -c pip install -v -r requirements-cpu.txt\" did not complete successfully: exit code: 1 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member mgoin commented Dec 3, 2024 Thanks for reporting @hmellor indeed it seems there isn't a manylinux arm wheel available https://pypi.org/project/xgrammar/#files I'll work on a patch fix All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin mentioned this pull request Dec 3, 2024 [Bugfix] Only require XGrammar on x86 #10865 Merged Copy link stefanobranco commented Dec 3, 2024 Obviously super cool to see new integrations, but it does seem a bit hasty to me to immediately change the default? The implementation with outlines core should be able to close the gap after all, and this one does not support regex yet. Or is xgrammar just objectively better? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor joennlae commented Dec 3, 2024 I second this opinion. Currently, the same behaviour cannot be expected from 'grammar`. I added a simple PR with some rudimentary regex + integer range support ( mlc-ai/xgrammar#106 ). I can attest that it is much faster, especially if one uses dynamic schemas. However, we should use outlines as the default, as it supports more cases for now, and the change is not breaking for many. I introduced it as an option in my closed PR ( #10803 ). But I forgot it when I discussed it with @mgoin . üëç 1 robcaulk reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member mgoin commented Dec 3, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Hi @stefanobranco and @joennlae thanks for raising your concern. Our primary concern is immediately improving structured output performance where it is easy to do so while maintaining the same behavior. With xgrammar as the default in supported cases, we still fallback to outlines in several cases covered here https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/guided_decoding/__init__.py#L18-L48 Please let me know if a case isn't being accounted for that is affecting your usage. We do not want to change external behavior. We have several integration tests that I have been using to create these rules, but more test points are certainly welcome! We have several fast-followup items to reduce the special cases around using xgrammar and improving performance even further in V0. We are also working on enabling outlines>=0.1.8 support with the devs of that project. Then of course we will enable the usage of structured output in V1. I hope this is helpful context and we will work on making a public roadmap for longer term goals. Please join the #feat-structured-output channel in slack if you want to have more direct discussion with the people working on this. üëç 2 stefanobranco and joennlae reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin mentioned this pull request Dec 4, 2024 [Bugfix] Fallback to outlines for complex json schemas #10899 Merged Copy link Ubospica commented Dec 5, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Thanks @stefanobranco , @joennlae , @ @mgoin for great feedbacks. The first initial release of XGrammar focuses on performance across grammar and json schema. We would like to ensure the system is holistically design to ensure zero overhead structure output, which aligns with many users needs we also see. Now that initial release land, we are working full steam to enable full support for JSON schema and regex. Thank you for these great feedbacks and please feel free to open new issues on XGrammar to give us feedbacks. Our general mission is to enable bringing flexible, zero-overhead structured generation everywhere, and we are excited to work with the community here to achieve that mission together, thank you for these feedbacks and we love contributions and collaborations to bring better, zero-overhead structured output for everyone üëç 3 Swipe4057, saattrupdan, and WangErXiao reacted with thumbs up emoji All reactions üëç 3 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sleepwalker2017 pushed a commit\n        to sleepwalker2017/vllm\n      that referenced\n      this pull request Dec 13, 2024 [Core][Performance] Add XGrammar support for guided decoding and set ‚Ä¶ ‚Ä¶ edebf1d ‚Ä¶it as default ( vllm-project#10785 )\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com> Copy link ktrapeznikov commented Dec 19, 2024 will this support models that use mistral tokenizers? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor robcaulk commented Feb 14, 2025 @joennlae Pointed out correctly that changing the default value from outlines to xgrammar was a breaking change. This should have been highlighted in the release notes as a breaking change. @mgoin you had the foresight to avoid changing behavior, but unfortunately, this change did change the behavior. The issue now is that the quality of output from xgrammar is not as high. It does not conform to Literal definitions in the schema. Outlines does. This broke quite a bit of our pipeline - as we require Literals. We will define outlines explicitly now to avoid the shortcoming of xgrammar, but I highly recommend to the maintainers ( @simon-mo ) that any breaking changes be properly highlighted in release notes in the future. üëç 2 aastroza and simon-mo reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . aarnphm deleted the feat/xgrammar branch March 19, 2025 11:02 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:38",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, Throughput, Throughput | SERVING: frontend, frontend | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:47:38",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmark_guided.py --model meta-llama/Llama-3.1-8B-Instruct --dataset xgrammar_bench --async-engine --output-len 512 --num-prompts 20 --enable-chunked-prefill --guided-decoding-ratio 1",
  "commit_subject": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)",
  "commit_message": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>",
  "commit_date": "2024-12-03T15:17:00+08:00",
  "files_changed": [
    "docs/source/conf.py",
    "requirements-common.txt",
    "tests/entrypoints/llm/test_guided_generate.py",
    "tests/model_executor/test_guided_processors.py",
    "vllm/config.py",
    "vllm/engine/arg_utils.py",
    "vllm/engine/async_llm_engine.py",
    "vllm/engine/llm_engine.py",
    "vllm/engine/multiprocessing/client.py",
    "vllm/model_executor/guided_decoding/__init__.py",
    "vllm/model_executor/guided_decoding/xgrammar_decoding.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 9,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 11,
    "num_hunks": 22,
    "num_edited_lines": 418,
    "num_non_test_edited_lines": 388,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb45..e9d9ac68c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -178,6 +178,7 @@ autodoc_mock_imports = [\n     \"tensorizer\",\n     \"pynvml\",\n     \"outlines\",\n+    \"xgrammar,\"\n     \"librosa\",\n     \"soundfile\",\n     \"gguf\",\ndiff --git a/requirements-common.txt b/requirements-common.txt\nindex 02e3d65fb..818f72e14 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0\n tiktoken >= 0.6.0  # Required for DBRX tokenizer\n lm-format-enforcer >= 0.10.9, < 0.11\n outlines >= 0.0.43, < 0.1\n+xgrammar\n typing_extensions >= 4.10\n filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\n partial-json-parser # used for parsing partial JSON outputs\ndiff --git a/tests/entrypoints/llm/test_guided_generate.py b/tests/entrypoints/llm/test_guided_generate.py\nindex 67c79415f..c3706f696 100644\n--- a/tests/entrypoints/llm/test_guided_generate.py\n+++ b/tests/entrypoints/llm/test_guided_generate.py\n@@ -159,3 +159,30 @@ def test_validation_against_both_guided_decoding_options(sample_regex, llm):\n                      sampling_params=sampling_params,\n                      use_tqdm=True,\n                      guided_options_request=dict(guided_regex=sample_regex))\n+\n+\n+@pytest.mark.skip_global_cleanup\n+def test_guided_json_object(llm):\n+    sampling_params = SamplingParams(\n+        temperature=1.0,\n+        max_tokens=100,\n+        guided_decoding=GuidedDecodingParams(json_object=True))\n+\n+    outputs = llm.generate(\n+        prompts=(\"Generate a JSON object describing a person with name \"\n+                 \"and age for John Smith who is 31 years old.\"),\n+        sampling_params=sampling_params,\n+        use_tqdm=True)\n+\n+    assert outputs is not None\n+    for output in outputs:\n+        assert output is not None\n+        assert isinstance(output, RequestOutput)\n+\n+        generated_text = output.outputs[0].text\n+        print(generated_text)\n+        assert generated_text is not None\n+\n+        # Parse to verify it is valid JSON\n+        parsed_json = json.loads(generated_text)\n+        assert isinstance(parsed_json, dict)\ndiff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py\nindex 45fab8e96..9f4d81b58 100644\n--- a/tests/model_executor/test_guided_processors.py\n+++ b/tests/model_executor/test_guided_processors.py\n@@ -36,7 +36,8 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):\n \n \n @pytest.mark.asyncio\n-@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\n+@pytest.mark.parametrize(\"backend\",\n+                         [\"outlines\", \"lm-format-enforcer\", \"xgrammar\"])\n async def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                  sample_json_schema):\n     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\ndiff --git a/vllm/config.py b/vllm/config.py\nindex 326340d3f..971eb36d6 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -1789,15 +1789,15 @@ class PoolerConfig:\n \n     step_tag_id: Optional[int] = None\n     \"\"\"\n-    If set, only the score corresponding to the ``step_tag_id`` in the \n+    If set, only the score corresponding to the ``step_tag_id`` in the\n     generated sentence should be returned. Otherwise, the scores for all tokens\n     are returned.\n     \"\"\"\n \n     returned_token_ids: Optional[List[int]] = None\n     \"\"\"\n-    A list of indices for the vocabulary dimensions to be extracted, \n-    such as the token IDs of ``good_token`` and ``bad_token`` in the \n+    A list of indices for the vocabulary dimensions to be extracted,\n+    such as the token IDs of ``good_token`` and ``bad_token`` in the\n     ``math-shepherd-mistral-7b-prm`` model.\n     \"\"\"\n \n@@ -2031,11 +2031,12 @@ def get_served_model_name(model: str,\n class DecodingConfig:\n     \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n \n-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n-    guided_decoding_backend: str = 'outlines'\n+    # Which guided decoding algo to use.\n+    # 'outlines' / 'lm-format-enforcer' / 'xgrammar'\n+    guided_decoding_backend: str = 'xgrammar'\n \n     def __post_init__(self):\n-        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n+        valid_guided_backends = ['outlines', 'lm-format-enforcer', 'xgrammar']\n         backend = self.guided_decoding_backend\n         if backend not in valid_guided_backends:\n             raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\n@@ -2222,7 +2223,7 @@ class CompilationConfig(BaseModel):\n             from Python, functions can also be passed directly via Python object\n             constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`\n         - custom inductor passes: see PassConfig for more details\n-    \n+\n     Why we have different sizes for cudagraph and inductor:\n     - cudagraph: a cudagraph captured for a specific size can only be used\n         for the same size. We need to capture all the sizes we want to use.\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eebd9..3b776c1d9 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -168,7 +168,7 @@ class EngineArgs:\n     scheduler_delay_factor: float = 0.0\n     enable_chunked_prefill: Optional[bool] = None\n \n-    guided_decoding_backend: str = 'outlines'\n+    guided_decoding_backend: str = 'xgrammar'\n     # Speculative decoding configuration.\n     speculative_model: Optional[str] = None\n     speculative_model_quantization: Optional[str] = None\n@@ -364,11 +364,12 @@ class EngineArgs:\n         parser.add_argument(\n             '--guided-decoding-backend',\n             type=str,\n-            default='outlines',\n-            choices=['outlines', 'lm-format-enforcer'],\n+            default='xgrammar',\n+            choices=['outlines', 'lm-format-enforcer', 'xgrammar'],\n             help='Which engine will be used for guided decoding'\n             ' (JSON schema / regex etc) by default. Currently support '\n-            'https://github.com/outlines-dev/outlines and '\n+            'https://github.com/outlines-dev/outlines,'\n+            'https://github.com/mlc-ai/xgrammar, and '\n             'https://github.com/noamgat/lm-format-enforcer.'\n             ' Can be overridden per request via guided_decoding_backend'\n             ' parameter.')\ndiff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 4395588d2..60dccd7a0 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -1,4 +1,5 @@\n import asyncio\n+import copy\n import time\n import weakref\n from functools import partial\n@@ -507,7 +508,8 @@ class _AsyncLLMEngine(LLMEngine):\n                 sampling_params=params,\n                 tokenizer=await self.get_tokenizer_async(lora_request),\n                 default_guided_backend=self.decoding_config.\n-                guided_decoding_backend)\n+                guided_decoding_backend,\n+                model_config=self.model_config)\n \n         self._add_processed_request(\n             request_id=request_id,\n@@ -528,22 +530,30 @@ class _AsyncLLMEngine(LLMEngine):\n \n async def build_guided_decoding_logits_processor_async(\n         sampling_params: SamplingParams, tokenizer: AnyTokenizer,\n-        default_guided_backend: str) -> SamplingParams:\n+        default_guided_backend: str,\n+        model_config: ModelConfig) -> SamplingParams:\n     \"\"\"Constructs logits processors based on the guided_decoding,\n     logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n     those fields and adds the constructed logits processors to the\n     logits_processors field. Modifies sampling params in-place and returns\n     the modified sampling params.\"\"\"\n-    if (guided_decoding := sampling_params.guided_decoding) is None:\n+    if sampling_params.guided_decoding is None:\n         return sampling_params\n \n+    # Defensively copy sampling params since guided decoding logits\n+    # processors can have different state for each request\n+    sampling_params = copy.copy(sampling_params)\n+    guided_decoding = sampling_params.guided_decoding\n+\n     logger.debug(\"Building guided decoding logits processor. \"\n                  \"Params: %s\", guided_decoding)\n \n     guided_decoding.backend = guided_decoding.backend or default_guided_backend\n \n     processor = await get_guided_decoding_logits_processor(\n-        guided_params=guided_decoding, tokenizer=tokenizer)\n+        guided_params=guided_decoding,\n+        tokenizer=tokenizer,\n+        model_config=model_config)\n \n     if processor:\n         if sampling_params.logits_processors is None:\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex dd55aa281..af66b3070 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1,3 +1,4 @@\n+import copy\n import time\n from collections import Counter as collectionsCounter\n from collections import deque\n@@ -1024,9 +1025,9 @@ class LLMEngine:\n         This function updates num_computed_tokens for prompt sequences\n         when Multi-Step is enabled.\n \n-        seq_group: SequenceGroup to update the num_computed_tokens for. \n+        seq_group: SequenceGroup to update the num_computed_tokens for.\n         seq_group_meta: Metadata of the given SequenceGroup.\n-        is_first_step_output: Optional[bool] - \n+        is_first_step_output: Optional[bool] -\n             When available, is_first_step_output indicates if the appended\n             output token is the output of the first-step in multi-step.\n             A value of None indicates that outputs from all steps in\n@@ -2036,7 +2037,11 @@ class LLMEngine:\n \n         logits_processors = []\n \n-        if (guided_decoding := sampling_params.guided_decoding) is not None:\n+        if sampling_params.guided_decoding is not None:\n+            # Defensively copy sampling params since guided decoding logits\n+            # processors can have different state for each request\n+            sampling_params = copy.copy(sampling_params)\n+            guided_decoding = sampling_params.guided_decoding\n \n             logger.debug(\n                 \"Building guided decoding logits processor in \"\n@@ -2047,7 +2052,9 @@ class LLMEngine:\n                 self.decoding_config.guided_decoding_backend\n \n             processor = get_local_guided_decoding_logits_processor(\n-                guided_params=guided_decoding, tokenizer=tokenizer)\n+                guided_params=guided_decoding,\n+                tokenizer=tokenizer,\n+                model_config=self.model_config)\n             if processor:\n                 logits_processors.append(processor)\n \ndiff --git a/vllm/engine/multiprocessing/client.py b/vllm/engine/multiprocessing/client.py\nindex 8383e774d..d21136c03 100644\n--- a/vllm/engine/multiprocessing/client.py\n+++ b/vllm/engine/multiprocessing/client.py\n@@ -474,8 +474,8 @@ class MQLLMEngineClient(EngineClient):\n             trace_headers: OpenTelemetry trace headers.\n             prompt_adapter_request: Prompt Adapter request to use\n                                             for generation, if any.\n-            priority: Priority of the request (lower means earlier handling). \n-                Any priority other than 0 will lead to an error if the \n+            priority: Priority of the request (lower means earlier handling).\n+                Any priority other than 0 will lead to an error if the\n                 scheduling policy is not \"priority\".\n         \"\"\"\n         if inputs is not None:\n@@ -589,6 +589,7 @@ class MQLLMEngineClient(EngineClient):\n                     default_guided_backend=(self.decoding_config.guided_decoding_backend\n                         if self.decoding_config\n                         else DecodingConfig.guided_decoding_backend),\n+                    model_config=self.model_config\n                 )\n \n         # 1) Create output queue for this requests.\ndiff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b67425f..23c31fcfd 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -1,14 +1,54 @@\n-from typing import Optional\n+from __future__ import annotations\n \n-from vllm.logits_process import LogitsProcessor\n-from vllm.sampling_params import GuidedDecodingParams\n+from typing import TYPE_CHECKING\n+\n+from vllm.logger import init_logger\n+\n+if TYPE_CHECKING:\n+    from transformers import PreTrainedTokenizer\n+\n+    from vllm.config import ModelConfig\n+    from vllm.logits_process import LogitsProcessor\n+    from vllm.sampling_params import GuidedDecodingParams\n+\n+logger = init_logger(__name__)\n+\n+\n+def maybe_backend_fallback(\n+        guided_params: GuidedDecodingParams) -> GuidedDecodingParams:\n+    # lm-format-enforce doesn't support grammar, fallback to xgrammar\n+    if (guided_params.backend == \"lm-format-enforcer\"\n+            and guided_params.grammar is not None):\n+        logger.warning(\n+            \"lm-format-enforcer does not support grammar guided decoding. \"\n+            \"Falling back to use xgrammar instead.\")\n+        guided_params.backend = \"xgrammar\"\n+\n+    if guided_params.backend == \"xgrammar\":\n+        # xgrammar doesn't support regex or choice, fallback to outlines\n+        if guided_params.regex is not None or guided_params.choice is not None:\n+            logger.warning(\n+                \"xgrammar only supports json or grammar guided decoding. \"\n+                \"Falling back to use outlines instead.\")\n+            guided_params.backend = \"outlines\"\n+\n+        # xgrammar only supports EBNF grammars and uses the GBNF format\n+        # https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md\n+        elif (guided_params.grammar is not None\n+              and \"::=\" not in guided_params.grammar):\n+            logger.warning(\"xgrammar only supports EBNF grammars. \"\n+                           \"Falling back to use outlines instead.\")\n+            guided_params.backend = \"outlines\"\n+\n+    return guided_params\n \n \n async def get_guided_decoding_logits_processor(\n-        guided_params: GuidedDecodingParams,\n-        tokenizer) -> Optional[LogitsProcessor]:\n+        guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig) -> LogitsProcessor | None:\n+    guided_params = maybe_backend_fallback(guided_params)\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_outlines_guided_decoding_logits_processor)\n@@ -19,17 +59,23 @@ async def get_guided_decoding_logits_processor(\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n+    if guided_params.backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_local_xgrammar_guided_decoding_logits_processor)\n+        return get_local_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer, model_config)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'\")\n \n \n def get_local_guided_decoding_logits_processor(\n-        guided_params: GuidedDecodingParams,\n-        tokenizer) -> Optional[LogitsProcessor]:\n+        guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig) -> LogitsProcessor | None:\n+    guided_params = maybe_backend_fallback(guided_params)\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_local_outlines_guided_decoding_logits_processor)\n@@ -40,7 +86,12 @@ def get_local_guided_decoding_logits_processor(\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n+    if guided_params.backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_local_xgrammar_guided_decoding_logits_processor)\n+        return get_local_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer, model_config)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'\")\ndiff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nnew file mode 100644\nindex 000000000..8287cd6cf\n--- /dev/null\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -0,0 +1,251 @@\n+# noqa: UP007\n+from __future__ import annotations\n+\n+import json\n+from dataclasses import dataclass, field\n+from typing import TYPE_CHECKING, Any, NamedTuple\n+\n+import torch\n+from transformers import PreTrainedTokenizerFast\n+\n+try:\n+    import xgrammar as xgr\n+    from xgrammar.base import _core as xgr_core\n+except ImportError:\n+    pass\n+\n+if TYPE_CHECKING:\n+    from transformers import PreTrainedTokenizer\n+\n+    from vllm.config import ModelConfig\n+    from vllm.sampling_params import GuidedDecodingParams\n+\n+\n+# TODO: passing batch size to max threads here\n+def get_local_xgrammar_guided_decoding_logits_processor(\n+        guided_params: GuidedDecodingParams,\n+        tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig,\n+        max_threads: int = 8):\n+    config = GrammarConfig.from_guided_params(guided_params=guided_params,\n+                                              model_config=model_config,\n+                                              tokenizer=tokenizer,\n+                                              max_threads=max_threads)\n+    return XGrammarLogitsProcessor(config)\n+\n+\n+class TokenizerData(NamedTuple):\n+    \"\"\"Immutable container for cached tokenizer data.\"\"\"\n+    encoded_vocab: list[str]\n+    stop_token_ids: list[int] | None\n+    backend_str: str\n+\n+\n+class TokenizerDataCache:\n+    \"\"\"Cache manager for tokenizer data to avoid repeated processing.\"\"\"\n+    _cache: dict[int, TokenizerData] = {}\n+\n+    @classmethod\n+    def get_tokenizer_data(cls,\n+                           tokenizer: PreTrainedTokenizer) -> TokenizerData:\n+        tokenizer_hash = hash(tokenizer)\n+\n+        if tokenizer_hash not in cls._cache:\n+            # Vendored from xgrammar logic since we cannot pickle the tokenizer\n+            # https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98 # noqa: E501\n+            try:\n+                encoded_vocab = [\n+                    token for token, _ in sorted(tokenizer.get_vocab().items(),\n+                                                 key=lambda x: x[1])\n+                ]\n+            except AttributeError as e:\n+                raise ValueError(\n+                    f\"Cannot get the vocabulary of the tokenizer \"\n+                    f\"{type(tokenizer)}. The tokenizer should have a \"\n+                    \"get_vocab method.\") from e\n+\n+            stop_token_ids = None\n+            backend_str = xgr.VocabType.RAW\n+            if isinstance(tokenizer, PreTrainedTokenizerFast):\n+                backend_str = tokenizer.backend_tokenizer.to_str()\n+                if stop_token_ids is None and hasattr(\n+                        tokenizer,\n+                        \"eos_token_id\") and tokenizer.eos_token_id is not None:\n+                    stop_token_ids = [tokenizer.eos_token_id]\n+\n+            cls._cache[tokenizer_hash] = TokenizerData(\n+                encoded_vocab=encoded_vocab,\n+                stop_token_ids=stop_token_ids,\n+                backend_str=backend_str)\n+\n+        return cls._cache[tokenizer_hash]\n+\n+\n+class GrammarCompilerCache:\n+    \"\"\"\n+    Cache for GrammarCompiler instances based on tokenizer.\n+\n+    This cache reduces the overhead of creating new compiler instances when\n+    using the same tokenizer configuration.\n+    \"\"\"\n+    _cache: dict[str, xgr.GrammarCompiler] = {}\n+\n+    @classmethod\n+    def get_compiler(cls, config: GrammarConfig) -> xgr.GrammarCompiler:\n+        cache_key = str(config.tokenizer_hash)\n+\n+        if cache_key not in cls._cache:\n+            assert config.encoded_vocab is not None\n+            tokenizer_info = xgr.TokenizerInfo._create_from_handle(\n+                xgr_core.TokenizerInfo.from_huggingface(\n+                    config.encoded_vocab, config.backend_str,\n+                    config.vocab_size, config.stop_token_ids))\n+            cls._cache[cache_key] = xgr.GrammarCompiler(\n+                tokenizer_info, max_threads=config.max_threads)\n+\n+        return cls._cache[cache_key]\n+\n+\n+@dataclass\n+class GrammarConfig:\n+    \"\"\"Serializable configuration for grammar compilation\"\"\"\n+    tokenizer_hash: int\n+    vocab_size: int\n+    json_str: str | None = None\n+    grammar_str: str | None = None\n+    json_object: bool | None = None\n+    max_threads: int = 8\n+    # Only populated if tokenizer_hash not in cache\n+    encoded_vocab: list[str] | None = None\n+    stop_token_ids: list[int] | None = None\n+    backend_str: str | None = None\n+\n+    @classmethod\n+    def from_guided_params(cls,\n+                           guided_params: GuidedDecodingParams,\n+                           model_config: ModelConfig,\n+                           tokenizer: PreTrainedTokenizer,\n+                           max_threads: int = 8) -> GrammarConfig:\n+\n+        tokenizer_hash = hash(tokenizer)\n+        # Only get tokenizer data if not already cached\n+        if tokenizer_hash in TokenizerDataCache._cache:\n+            encoded_vocab = None\n+            stop_token_ids = None\n+            backend_str = None\n+        else:\n+            tokenizer_data = TokenizerDataCache.get_tokenizer_data(tokenizer)\n+            encoded_vocab = tokenizer_data.encoded_vocab\n+            stop_token_ids = tokenizer_data.stop_token_ids\n+            backend_str = tokenizer_data.backend_str\n+\n+        if guided_params.json:\n+            if not isinstance(guided_params.json, str):\n+                json_str = json.dumps(guided_params.json)\n+            else:\n+                json_str = guided_params.json\n+            return cls(json_str=json_str,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        elif guided_params.grammar:\n+            return cls(grammar_str=guided_params.grammar,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        elif guided_params.json_object:\n+            return cls(json_object=True,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        else:\n+            raise ValueError(\n+                \"Currently only support JSON and EBNF grammar mode for xgrammar\"\n+            )\n+\n+\n+@dataclass\n+class XGrammarLogitsProcessor:\n+    \"\"\"Wrapper class to support pickle protocol\"\"\"\n+    config: GrammarConfig\n+\n+    ctx: xgr.CompiledGrammar | None = None\n+    token_bitmask: torch.Tensor = None  # type: ignore[assignment]\n+    matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n+    batch_size: int = field(default=1)\n+    prefilled: bool = field(default=False)\n+\n+    def __getstate__(self) -> dict[str, Any]:\n+        return {'config': self.config}\n+\n+    def __setstate__(self, state: dict[str, Any]):\n+        self.config = state['config']\n+\n+        self.ctx = None\n+        self.matchers = []\n+        self.batch_size = 1\n+        self.token_bitmask = None  # type: ignore[assignment]\n+        self.prefilled = False\n+\n+    def _ensure_ctx(self):\n+        \"\"\"Lazily initialize the processor in the worker process\"\"\"\n+        if self.ctx is None:\n+            compiler = GrammarCompilerCache.get_compiler(self.config)\n+            if self.config.json_str is not None:\n+                self.ctx = compiler.compile_json_schema(self.config.json_str)\n+            elif self.config.grammar_str is not None:\n+                self.ctx = compiler.compile_grammar(self.config.grammar_str)\n+            elif self.config.json_object:\n+                self.ctx = compiler.compile_builtin_json_grammar()\n+            else:\n+                raise ValueError(\n+                    \"Invalid configuration for xgrammar logits processor\")\n+\n+    def __call__(self, input_ids: list[int],\n+                 scores: torch.Tensor) -> torch.Tensor:\n+        if self.ctx is None:\n+            self._ensure_ctx()\n+\n+        if len(self.matchers) == 0:\n+            self.matchers = [\n+                xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n+            ]\n+            self.token_bitmask = xgr.allocate_token_bitmask(\n+                self.batch_size, self.config.vocab_size)\n+\n+        if not self.prefilled:\n+            # Have not sampled a token yet\n+            self.prefilled = True\n+        else:\n+            for i, matcher in enumerate(self.matchers):\n+                if not matcher.is_terminated():\n+                    sampled_token = input_ids[-1]\n+                    assert self.matchers[i].accept_token(sampled_token)\n+\n+        for i, matcher in enumerate(self.matchers):\n+            if not matcher.is_terminated():\n+                # @ubospica: ideally, fill_next_token_bitmask should be\n+                # parallelized with model decoding\n+                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n+                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+\n+        # token_bitmask is a CPU tensor for use with accept_token and\n+        # fill_next_token_bitmask so we move it to the device of scores\n+        device_type = scores.device.type\n+        if device_type != \"cuda\":\n+            scores = scores.to(\"cpu\")\n+        xgr.apply_token_bitmask_inplace(scores,\n+                                        self.token_bitmask.to(scores.device))\n+        if device_type != \"cuda\":\n+            scores = scores.to(device_type)\n+\n+        return scores",
  "apis": [
    "vllm.LLM.generate",
    "vllm.config.DecodingConfig",
    "vllm.engine.async_llm_engine.AsyncLLMEngine.generate"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/async_llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit introduces a new guided decoding backend called \"xgrammar\" and changes the default backend from 'outlines' to 'xgrammar'. It involves non-trivial modifications to multiple source files (including the core engine and internal API modules) rather than mere documentation or comment fixes. Its changes, including caching mechanisms in the xgrammar implementation, are aimed at optimizing the guided decoding process on CPU. Although it adds a new backend feature, the focus is on enhancing performance by improving the decoding pipeline, and the commit is tagged with [Core][Performance]. Therefore, it satisfies the performance / optimization criteria even though it introduces new functionality as an optimization improvement.",
  "llm_api_reason": "This commit adds support for a new guided decoding backend called ‚Äúxgrammar‚Äù and updates default values and valid choices accordingly. Changes are made in the configuration (e.g. DecodingConfig and EngineArgs), in the synchronous engine (LLMEngine) and in the async engine (AsyncLLMEngine), which are invoked by high‚Äêlevel APIs such as generate(). These modifications impact how guided decoding is selected and processed throughout the inference pipeline."
}