{
  "commit_hash": "fc7b8d1eefcbe837a56b7c080509417fe5167e6c",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7364",
  "pr_date": "2024-08-09",
  "timeline_text": "Copy link Collaborator alexm-redhat commented Aug 9, 2024 This PR is a followup for #7162 to address leftover review comments and add some more small improvements. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 youkaichao reacted with thumbs up emoji All reactions üëç 1 reaction review comments from Kaichao and hengxinCheung acb7235 Copy link github-actions bot commented Aug 9, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . alexm-redhat mentioned this pull request Aug 9, 2024 [Performance] Optimize e2e overheads: Reduce python allocations #7162 Merged njhill reviewed Aug 9, 2024 View reviewed changes vllm/core/block_manager_v1.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Nick's comment 6297040 njhill approved these changes Aug 9, 2024 View reviewed changes Copy link Collaborator Author alexm-redhat commented Aug 9, 2024 /ready All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 9, 2024 comaniac enabled auto-merge (squash) August 9, 2024 15:47 Hide details View details comaniac merged commit fc7b8d1 into vllm-project : main Aug 9, 2024 58 of 60 checks passed Uh oh! There was an error while loading. Please reload this page . Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [Performance] e2e overheads reduction: Small followup diff ( vllm-proj‚Ä¶ ‚Ä¶ a1ff013 ‚Ä¶ect#7364 )\n\nSigned-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [Performance] e2e overheads reduction: Small followup diff ( vllm-proj‚Ä¶ ‚Ä¶ 87c9e4c ‚Ä¶ect#7364 )\n\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:14",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: CI, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:48:14",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance] e2e overheads reduction: Small followup diff (#7364)",
  "commit_message": "[Performance] e2e overheads reduction: Small followup diff (#7364)",
  "commit_date": "2024-08-09T15:49:36Z",
  "files_changed": [
    "vllm/core/block_manager_v1.py",
    "vllm/sequence.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 2,
    "num_edited_lines": 7,
    "num_non_test_edited_lines": 7,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 622aca66a..ad26d3c51 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -336,9 +336,9 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n \n         # Assign the self-attention block tables for each sequence.\n         if len(wait_seqs) == 1:\n-            self.block_tables[wait_seqs[0].seq_id] = block_table\n+            self.block_tables[seq.seq_id] = block_table\n         else:\n-            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n+            for seq in wait_seqs:\n                 self.block_tables[seq.seq_id] = block_table.copy()\n \n         # Allocate encoder sequence\ndiff --git a/vllm/sequence.py b/vllm/sequence.py\nindex ba477efc5..fd2dc9656 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -655,6 +655,9 @@ class SequenceGroup:\n         return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n+        if self.is_single_seq:\n+            return self.seqs if self.seqs[0].is_finished() else []\n+\n         return [seq for seq in self.seqs if seq.is_finished()]\n \n     def update_num_computed_tokens(self, num_new_computed_tokens: int):",
  "apis": [
    "BlockSpaceManagerV1",
    "SequenceGroup.get_finished_seqs"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/sequence.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies two source files (vllm/core/block_manager_v1.py and vllm/sequence.py), which are non-test files. The diff changes the way block tables are assigned and adds a fast-track branch in get_finished_seqs for the single-sequence case, both of which are internal optimizations to reduce overhead in end-to-end operations. Although the changes are small, they are intended to reduce runtime overhead and improve performance operations on CPU, as indicated by the commit message. These modifications are performance-related optimizations and not mere refactoring or bug fixes.",
  "llm_api_reason": "This commit improves performance by tweaking internal logic. In the BlockSpaceManagerV1 class (in vllm/core/block_manager_v1.py), the code now assigns a block table using the current sequence‚Äôs ID (using \"seq.seq_id\") rather than always taking the first waiting sequence; in the multi-sequence branch, it now iterates directly over the waiting sequences. In vllm/sequence.py, the get_finished_seqs method of the SequenceGroup class is modified to return the finished sequence in a single‚Äêsequence group only if it is actually finished (empty otherwise), avoiding unnecessary overhead. These changes refine block table management and sequence status retrieval for end-to-end performance improvement."
}