{
  "commit_hash": "6a417b8600d4d1e57698a91b71a38446e8fc5c45",
  "pr_url": "https://github.com/vllm-project/vllm/pull/13589",
  "pr_date": "2025-02-20",
  "timeline_text": "Copy link Contributor ajayvohra2005 commented Feb 20, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This fixes a Neuron specific performance issue. Without this fix, Neuron performance degrades quickly when number of concurrent requests >= max_num_seqs . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions fix neuron performance issue 3aaf6a3 Copy link github-actions bot commented Feb 20, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator WoosukKwon commented Feb 20, 2025 cc @liangfu All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . liangfu approved these changes Feb 20, 2025 View reviewed changes Copy link Contributor liangfu left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment thanks for the fix Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details WoosukKwon merged commit 6a417b8 into vllm-project : main Feb 20, 2025 19 of 20 checks passed Uh oh! There was an error while loading. Please reload this page . Akshat-Tripathi pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Mar 3, 2025 fix neuron performance issue ( vllm-project#13589 ) 6b81301 lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 fix neuron performance issue ( vllm-project#13589 ) ‚Ä¶ 353aced Signed-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 fix neuron performance issue ( vllm-project#13589 ) 500b058 liangfu mentioned this pull request May 14, 2025 Remove pre-emption logic for Neuron aws-neuron/upstreaming-to-vllm#17 Closed Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:52:28",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:52:28",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "fix neuron performance issue (#13589)",
  "commit_message": "fix neuron performance issue (#13589)",
  "commit_date": "2025-02-20T10:59:36-08:00",
  "files_changed": [
    "vllm/worker/neuron_worker.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 4,
    "num_non_test_edited_lines": 4,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py\nindex 5f0eb0019..95e7acd02 100644\n--- a/vllm/worker/neuron_worker.py\n+++ b/vllm/worker/neuron_worker.py\n@@ -76,7 +76,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         # Set the number of GPU blocks to be the same as the maximum number of\n         # sequences that can be processed in a single batch. This is equivalent\n         # to schedule without PagedAttention.\n-        num_gpu_blocks = self.scheduler_config.max_num_seqs\n+        num_gpu_blocks = self.scheduler_config.max_num_seqs + 1\n \n         # Swap not yet supported with Neuron backend.\n         num_cpu_blocks = 0\n@@ -90,7 +90,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n \n         # Different values are not tested.\n         assert num_cpu_blocks == 0\n-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs\n+        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1\n \n         self.cache_config.num_gpu_blocks = num_gpu_blocks\n         self.cache_config.num_cpu_blocks = num_cpu_blocks",
  "apis": [
    "NeuronWorker.determine_num_available_blocks",
    "NeuronWorker.initialize_cache"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/neuron_worker.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/neuron_model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/platforms/neuron.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/offline_inference/neuron.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/model_loader/neuron.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The patch modifies a non-test file (neuron_worker.py) where it changes the calculation of the number of GPU blocks by adding 1 to the maximum number of sequences. The commit message ‚Äúfix neuron performance issue‚Äù aligns with a performance-related change. The modification directly alters scheduling behavior for processing batches, which is likely to affect performance at runtime. There is a clear performance intention behind updating both the initialization and the assert statement checks. Therefore, the commit meets the conditions for being performance or optimization related.",
  "llm_api_reason": "The commit fixes a performance issue on the Neuron backend by adjusting the number of GPU KV cache blocks. Previously, the number was set equal to the maximum number of sequences, but now it is increased by 1. This change is reflected in both the determination of available blocks and the corresponding assertion in the cache initialization, which directly affects how NeuronWorker handles device memory and caching for inference."
}