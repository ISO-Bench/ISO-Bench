{
  "commit_hash": "299ebb62b269ce167eb1c71b5e39a1dc1f65ce1c",
  "pr_url": "https://github.com/vllm-project/vllm/pull/16436",
  "pr_date": "2025-04-10",
  "timeline_text": "Copy link Contributor chanh commented Apr 10, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This code inside apply_penalties does advanced indexing on a tensor which triggers nonzero which requires a CPU sync currently with PyTorch. With torch.cuda.set_sync_debug_mode(\"warn\") PyTorch framework confirms this: /home/coder/vllm/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:1067: UserWarning: Synchronization debug mode is a prototype feature and does not yet detect all synchronizing operations (Triggered internally at /pytorch/torch/csrc/cuda/Module.cpp:915.)\n  torch._C._cuda_set_sync_debug_mode(debug_mode)\n/home/coder/vllm/vllm/model_executor/layers/utils.py:52: UserWarning: called a synchronizing CUDA operation (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:152.)\n  logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n/home/coder/vllm/vllm/model_executor/layers/utils.py:54: UserWarning: called a synchronizing CUDA operation (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:152.)\n  logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n/home/coder/vllm/vllm/v1/worker/gpu_model_runner.py:1153: UserWarning: called a synchronizing CUDA operation (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:152.)\n  valid_sampled_token_ids = sampled_token_ids.tolist() This seems to be a known issue and was encountered here: pytorch/pytorch#12461 nonzero that is called in this conversion has a legitimate synchronization - it is necessary to pass the information from the device about how many non-zero elements were found in the boolean index tensor, as this information would be later required on the cpu, to resize the index tensor, and to configure launch parameters/kernel arguments for subsequent kernels. I'm not sure this sync can be avoided, because if mask comes as a result of an operation on the GPU, CPU has no way of getting the number of nonzeros in the mask, which is objectively needed. By refactoring the code to avoid the indexing, we can remove the sync and allow much more of the sampling phase CPU work to overlap with the forward pass on the GPU, providing an 8% speedup to decoding for smaller models. Before: ============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.22    \nTotal input tokens:                      100000    \nTotal generated tokens:                  10000     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         96.88     \nTotal Token throughput (tok/s):          1065.73   \n---------------Time to First Token----------------\nMean TTFT (ms):                          37.21     \nMedian TTFT (ms):                        32.09     \nP99 TTFT (ms):                           71.54     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          6.74      \nMedian TPOT (ms):                        6.67      \nP99 TPOT (ms):                           7.20      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           6.74      \nMedian ITL (ms):                         6.69      \nP99 ITL (ms):                            7.93      \n================================================== After: ============ Serving Benchmark Result ============\nSuccessful requests:                     100       \nBenchmark duration (s):                  103.17    \nTotal input tokens:                      100000    \nTotal generated tokens:                  10000     \nRequest throughput (req/s):              0.97      \nOutput token throughput (tok/s):         96.93     \nTotal Token throughput (tok/s):          1066.19   \n---------------Time to First Token----------------\nMean TTFT (ms):                          35.62     \nMedian TTFT (ms):                        30.71     \nP99 TTFT (ms):                           60.89     \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          6.18      \nMedian TPOT (ms):                        6.11      \nP99 TPOT (ms):                           6.50      \n---------------Inter-token Latency----------------\nMean ITL (ms):                           6.18      \nMedian ITL (ms):                         6.12      \nP99 ITL (ms):                            7.43      \n================================================== Benchmark: VLLM_FLASH_ATTN_VERSION=3 VLLM_USE_V1=1 vllm serve Qwen/Qwen2.5-1.5B-Instruct --enable-prefix-caching --dtype float16 --disable-log-requests -O3\n\nvllm bench serve \\\n        --model Qwen/Qwen2.5-1.5B-Instruct \\\n        --request-rate 1 \\\n        --num-prompts 100 \\\n        --random-input-len 1000 \\\n        --random-output-len 100 \\\n        --tokenizer Qwen/Qwen2.5-1.5B-Instruct \\\n        --ignore-eos Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 3 njhill, houseroad, and WoosukKwon reacted with thumbs up emoji üëÄ 1 mgoin reacted with eyes emoji All reactions üëç 3 reactions üëÄ 1 reaction Chanh Nguyen added 2 commits April 10, 2025 21:17 Fix penalties function causing CUDA sync ‚Ä¶ a319ec0 Signed-off-by: Chanh Nguyen <cnguyen@linkedin.com> Fix penalties function causing CUDA sync ‚Ä¶ cab436d Signed-off-by: Chanh Nguyen <cnguyen@linkedin.com> Copy link github-actions bot commented Apr 10, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . chanh marked this pull request as ready for review April 11, 2025 04:12 Merge branch 'main' into cnguyen/penalties dff03c5 chanh changed the title Speed up decode by remove synchronizing operation in sampler [Core] Speed up decode by remove synchronizing operation in sampler Apr 18, 2025 WoosukKwon self-assigned this Apr 21, 2025 WoosukKwon approved these changes Apr 21, 2025 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @chanh Sorry for the late review. This is really great! Nice optimization! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Apr 21, 2025 WoosukKwon enabled auto-merge (squash) April 21, 2025 16:28 Hide details View details WoosukKwon merged commit 299ebb6 into vllm-project : main Apr 21, 2025 61 checks passed Uh oh! There was an error while loading. Please reload this page . frieda-huang pushed a commit\n        to frieda-huang/vllm\n      that referenced\n      this pull request Apr 23, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ cdcb192 ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com>\nSigned-off-by: Frieda (Jingying) Huang <jingyingfhuang@gmail.com> jikunshang pushed a commit\n        to jikunshang/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ 69e7495 ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com> lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ 603d269 ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com> adobrzyn pushed a commit\n        to HabanaAI/vllm-fork\n      that referenced\n      this pull request Apr 30, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ 56cdbf0 ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com>\nSigned-off-by: Agata Dobrzyniewicz <adobrzyniewicz@habana.ai> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ 375f86a ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> ckhordiasma mentioned this pull request May 14, 2025 nm vllm ent 0.8.5 sync red-hat-data-services/vllm#139 Merged minpeter pushed a commit\n        to minpeter/vllm\n      that referenced\n      this pull request Jun 24, 2025 [Core] Speed up decode by remove synchronizing operation in sampler ( v‚Ä¶ ‚Ä¶ cd510d8 ‚Ä¶llm-project#16436 )\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com>\nSigned-off-by: minpeter <kali2005611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:17",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, Serving, Serving | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:51:17",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "vllm bench serve --model Qwen/Qwen2.5-1.5B-Instruct --request-rate 1 --num-prompts 100 --random-input-len 1000 --random-output-len 100 --tokenizer Qwen/Qwen2.5-1.5B-Instruct --ignore-eos",
  "commit_subject": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)",
  "commit_message": "[Core] Speed up decode by remove synchronizing operation in sampler (#16436)\n\nSigned-off-by: Chanh Nguyen <cnguyen@linkedin.com>\nCo-authored-by: Chanh Nguyen <cnguyen@linkedin.com>",
  "commit_date": "2025-04-21T18:18:22Z",
  "files_changed": [
    "vllm/model_executor/layers/utils.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 13,
    "num_non_test_edited_lines": 13,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/utils.py b/vllm/model_executor/layers/utils.py\nindex a9ef97391..5e56be061 100644\n--- a/vllm/model_executor/layers/utils.py\n+++ b/vllm/model_executor/layers/utils.py\n@@ -47,10 +47,15 @@ def apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n         output_tokens_tensor, vocab_size, num_seqs)\n     repetition_penalties = repetition_penalties.unsqueeze(dim=1).repeat(\n         1, vocab_size)\n-    logits[logits > 0] /= torch.where(prompt_mask | output_mask,\n-                                      repetition_penalties, 1.0)[logits > 0]\n-    logits[logits <= 0] *= torch.where(prompt_mask | output_mask,\n-                                       repetition_penalties, 1.0)[logits <= 0]\n+\n+    # If token appears in prompt or output, apply, otherwise use 1.0 for no-op.\n+    penalties = torch.where(prompt_mask | output_mask, repetition_penalties,\n+                            1.0)\n+\n+    # If logits are positive, divide by penalty, otherwise multiply by penalty.\n+    scaling = torch.where(logits > 0, 1.0 / penalties, penalties)\n+    logits *= scaling\n+\n     # We follow the definition in OpenAI API.\n     # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n     logits -= frequency_penalties.unsqueeze(dim=1) * output_bin_counts",
  "apis": [
    "vllm.model_executor.layers.utils.apply_penalties"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/sample/tpu/sampler.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/serving_completion.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file and changes the core mathematical operations in the decode process. The transformation in the code replaces two separate element-wise operations (division and multiplication) with a single multiplication by a scaling factor computed in a more consolidated manner. The commit message clearly indicates that its goal is to ‚ÄúSpeed up decode,‚Äù meaning it's intended to optimize performance. Although it does not explicitly mention performance in the patch diff, the commit message and the nature of the change (refactoring how the logits are scaled) are directly performance-related and affect the high-level decode function, making it testable on CPU. Thus, the changes satisfy the conditions for a performance or optimization related commit.",
  "llm_api_reason": "The commit modifies the core logic of penalty application in the function that adjusts logits. Instead of performing separate division and multiplication operations based on whether the logits are positive or not, it now computes a penalty tensor (using a torch.where) and then determines a unified scaling factor (again via torch.where) that is applied directly to the logits. This change aims to reduce synchronizing operations in the sampler and boost decode speed."
}