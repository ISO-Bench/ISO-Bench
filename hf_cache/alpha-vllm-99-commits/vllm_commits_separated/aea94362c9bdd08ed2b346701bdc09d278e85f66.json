{
  "commit_hash": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
  "pr_url": "https://github.com/vllm-project/vllm/pull/12287",
  "pr_date": "2025-01-22",
  "timeline_text": "Copy link Member njhill commented Jan 21, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . These help in particular with TTFT, ITL variance, and overall throughput. Break up output processing (detokenization) to avoid blocking the event loop for too long Freeze the heap after startup to reduce GC overhead/pauses Optimize a couple of CPU hotspots seen during profiling Benchmark on A100: VLLM_USE_V1=1 vllm serve meta-llama/Llama-3.2-1B-Instruct --disable-log-requests --port 8001 --max-num-batched-tokens 8192 --no-enable-prefix-caching --uvicorn-log-level=error python benchmarks/benchmark_serving.py \\\n    --backend vllm \\\n    --model meta-llama/Llama-3.2-1B-Instruct \\\n    --dataset-name sharegpt \\\n    --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json \\\n    --ignore-eos \\\n    --port 8001 \\\n    --save-result \\\n    --result-dir results \\\n    --result-filename test.json \\\n    --num-prompts 6000 \\\n    --request-rate inf \\\n    --max-concurrency=400 Before: ============ Serving Benchmark Result ============\nSuccessful requests:                     6000      \nBenchmark duration (s):                  94.31     \nTotal input tokens:                      1350511   \nTotal generated tokens:                  1211959   \nRequest throughput (req/s):              63.62     \nOutput token throughput (tok/s):         12850.45  \nTotal Token throughput (tok/s):          27169.98  \n---------------Time to First Token----------------\nMean TTFT (ms):                          229.23    \nMedian TTFT (ms):                        158.08    \nP99 TTFT (ms):                           1050.70   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          30.02     \nMedian TPOT (ms):                        29.64     \nP99 TPOT (ms):                           68.90     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           28.77     \nMedian ITL (ms):                         23.19     \nP99 ITL (ms):                            386.30    \n================================================== After: ============ Serving Benchmark Result ============\nSuccessful requests:                     6000      \nBenchmark duration (s):                  88.60     \nTotal input tokens:                      1350511   \nTotal generated tokens:                  1211959   \nRequest throughput (req/s):              67.72     \nOutput token throughput (tok/s):         13679.34  \nTotal Token throughput (tok/s):          28922.50  \n---------------Time to First Token----------------\nMean TTFT (ms):                          197.34    \nMedian TTFT (ms):                        168.03    \nP99 TTFT (ms):                           1059.55   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          28.30     \nMedian TPOT (ms):                        27.75     \nP99 TPOT (ms):                           47.38     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           26.64     \nMedian ITL (ms):                         24.38     \nP99 ITL (ms):                            65.19     \n================================================== Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 9 jeejeelee, comaniac, simon-mo, WoosukKwon, ywang96, robertgshaw2-redhat, mgoin, drikster80, and nickandbro reacted with heart emoji üöÄ 1 tlrmchlsmth reacted with rocket emoji All reactions ‚ù§Ô∏è 9 reactions üöÄ 1 reaction njhill requested review from WoosukKwon , robertgshaw2-redhat , ywang96 , comaniac and alexm-redhat as code owners January 21, 2025 23:38 Copy link github-actions bot commented Jan 21, 2025 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the frontend label Jan 21, 2025 [Frontend][V1] Online serving performance improvements ‚Ä¶ 55dd119 These help in particular with TTFT, and ITL variance. Overall throughput doesn't change much.\n\n- Break up output processing (detokenization) to avoid blocking the event loop for too long\n- Freeze the heap after startup to reduce GC overhead/pauses\n- Optimize a couple of CPU hotspots seen during profiling\n\nSigned-off-by: Nick Hill <nhill@redhat.com> njhill force-pushed the v1-perf-smoothing branch\n    from cfc5705 to 55dd119 Compare January 21, 2025 23:39 njhill commented Jan 22, 2025 View reviewed changes vllm/entrypoints/openai/protocol.py @@ -42,23 +42,31 @@ class OpenAIBaseModel(BaseModel): # OpenAI API does allow extra fields model_config = ConfigDict(extra=\"allow\") # Cache class field names field_names: ClassVar[Optional[Set[str]]] = None Copy link Member Author njhill Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment There was noticeable overhead creating this set every time one of these objects is instantiated. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 3 mgoin, DarkLight1337, and ywang96 reacted with thumbs up emoji All reactions üëç 3 reactions vllm/v1/request.py def output_token_ids ( self ) -> ConstantList [ int ]: # Prevent directly appending to the output_token_ids since # all_token_ids should also be updated simultaneously. return ConstantList ( self . _output_token_ids ) Copy link Member Author njhill Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Avoid constructing these objects every time the properties are accessed. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 2 WoosukKwon and DarkLight1337 reacted with thumbs up emoji All reactions üëç 2 reactions Copy link Collaborator WoosukKwon Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Nice catch! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member mgoin Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I actually thought properties were cached after the first call, nice call Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member DarkLight1337 Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I actually thought properties were cached after the first call, nice call That would involve the use of cached_property . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 2 mgoin and njhill reacted with thumbs up emoji All reactions üëç 2 reactions Parallelize output socket IO on client side ‚Ä¶ 0e92b61 Signed-off-by: Nick Hill <nhill@redhat.com> Copy link Collaborator robertgshaw2-redhat commented Jan 22, 2025 Wow, the impact on P99 ITL is crazy. üöÄ 1 mgoin reacted with rocket emoji All reactions üöÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . robertgshaw2-redhat reviewed Jan 22, 2025 View reviewed changes vllm/entrypoints/openai/api_server.py # Mark the startup heap as static so that it's ignored by GC. # Reduces pause times of oldest generation collections. gc.collect() gc.freeze() Copy link Collaborator robertgshaw2-redhat Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Do we need to call unfreeze at some point? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author njhill Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment No, this is mostly static stuff that will be around for the lifetime of the process anyhow. https://www.rippling.com/blog/the-garbage-collector-fights-back Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member Author njhill commented Jan 22, 2025 Combining with #12298 and increasing the max output processing chunk size to 256 gets higher throughput at the cost of slightly more latency variance. Since the benchmark I've been running is 400 concurrent requests, the 256 chunk size essentially just means those will be split into two chunks of ~400. If I disable the chunking completely, the throughput increases to 80 req/sec (with the coalescing), but the inter-response latencies become larger and more uneven. ============ Serving Benchmark Result ============\nSuccessful requests:                     6000      \nBenchmark duration (s):                  84.70     \nTotal input tokens:                      1350511   \nTotal generated tokens:                  1211959   \nRequest throughput (req/s):              70.84     \nOutput token throughput (tok/s):         14308.94  \nTotal Token throughput (tok/s):          30253.69  \n---------------Time to First Token----------------\nMean TTFT (ms):                          198.28    \nMedian TTFT (ms):                        166.40    \nP99 TTFT (ms):                           1128.75   \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          26.76     \nMedian TPOT (ms):                        26.05     \nP99 TPOT (ms):                           50.04     \n---------------Inter-token Latency----------------\nMean ITL (ms):                           29.41     \nMedian ITL (ms):                         26.83     \nP99 ITL (ms):                            75.34     \n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author njhill commented Jan 22, 2025 It would probably be good to also make OUTPUT_PROCESSING_CHUNK_SIZE overridable via an env var. üëç 2 mgoin and ywang96 reacted with thumbs up emoji All reactions üëç 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mgoin reviewed Jan 22, 2025 View reviewed changes vllm/v1/engine/output_processor.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/v1/request.py def output_token_ids ( self ) -> ConstantList [ int ]: # Prevent directly appending to the output_token_ids since # all_token_ids should also be updated simultaneously. return ConstantList ( self . _output_token_ids ) Copy link Member mgoin Jan 22, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I actually thought properties were cached after the first call, nice call Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions ywang96 reviewed Jan 22, 2025 View reviewed changes vllm/v1/engine/async_llm.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . njhill added 2 commits January 22, 2025 08:56 Make max processing chunk size overridable, fix linting ‚Ä¶ aa7f031 Signed-off-by: Nick Hill <nhill@redhat.com> Merge remote-tracking branch 'refs/remotes/origin/main' into v1-perf-‚Ä¶ ‚Ä¶ e6fc61f ‚Ä¶smoothing mgoin approved these changes Jan 22, 2025 View reviewed changes Copy link Member mgoin left a comment ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! I ran an lm-eval test with gsm8k as a smoke test and got the same result as v0 VLLM_USE_V1=1 vllm serve meta-llama/Llama-3.1-8B-Instruct --disable-log-requests --port 8000 --max-num-batched-tokens 8192 --no-enable-prefix-caching\n\nlm_eval --model local-completions --model_args model=meta-llama/Llama-3.1-8B-Instruct,base_url=http://0.0.0.0:8000/v1/completions,num_concurrent=50,tokenized_requests=False --tasks gsm8k --num_fewshot 5\nlocal-completions (model=meta-llama/Llama-3.1-8B-Instruct,base_url=http://0.0.0.0:8000/v1/completions,num_concurrent=50,tokenized_requests=False), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: 1\n|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.7718|¬±  |0.0116|\n|     |       |strict-match    |     5|exact_match|‚Üë  |0.6983|¬±  |0.0126| Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 WoosukKwon reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jan 22, 2025 Copy link mergify bot commented Jan 22, 2025 This pull request has merge conflicts that must be resolved before it can be merged. Please rebase the PR, @njhill . https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/syncing-a-fork All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the needs-rebase label Jan 22, 2025 Merge remote-tracking branch 'origin/main' into v1-perf-smoothing ‚Ä¶ eafe7cb # Conflicts:\n#\tvllm/envs.py mergify bot removed\n  the needs-rebase label Jan 22, 2025 mgoin enabled auto-merge (squash) January 22, 2025 22:18 Hide details View details mgoin merged commit aea9436 into vllm-project : main Jan 22, 2025 51 checks passed Uh oh! There was an error while loading. Please reload this page . njhill deleted the v1-perf-smoothing branch January 22, 2025 23:34 tjtanaa pushed a commit\n        to EmbeddedLLM/vllm\n      that referenced\n      this pull request Jan 28, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ d57c673 ‚Ä¶12287 ) rasmith pushed a commit\n        to rasmith/vllm\n      that referenced\n      this pull request Jan 30, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ f9304d2 ‚Ä¶12287 ) Isotr0py pushed a commit\n        to Isotr0py/vllm\n      that referenced\n      this pull request Feb 2, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ 1f63490 ‚Ä¶12287 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com> hongxiayang added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Feb 3, 2025 [MFM-2025-02-03] Merge Main to llama fp8; With Faster ROCm Paged Atte‚Ä¶ ‚Ä¶ 479b843 ‚Ä¶ntion ( #399 )\n\n* [V1] Avoid sending text prompt to core engine ( vllm-project#11963 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [CI/Build] Add markdown linter ( vllm-project#11857 )\n\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\n\n* [Model] Initialize support for Deepseek-VL2 models ( vllm-project#11578 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Hardware][CPU] Multi-LoRA implementation for the CPU backend ( vllm-project#11100 )\n\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][TPU] workaround fix for MoE on TPU ( vllm-project#11764 )\n\n* [V1][Core][1/n] Logging and Metrics ( vllm-project#11962 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [Model] Support GGUF models newly added in `transformers` 4.46.0 ( vllm-project#9685 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction ( vllm-project#11973 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [MISC] fix typo in kv transfer send recv test ( vllm-project#11983 )\n\n* [Bug] Fix usage of `.transpose()` and `.view()` consecutively. ( vllm-project#11979 )\n\n* [CI][Spec Decode] fix: broken test for EAGLE model ( vllm-project#11972 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Misc] Fix Deepseek V2 fp8 kv-scale remapping ( vllm-project#11947 )\n\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\n\n* [Misc]Minor Changes about Worker ( vllm-project#11555 )\n\nSigned-off-by: Chenguang Li <757486878@qq.com>\n\n* [platform] add ray_device_key ( vllm-project#11948 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Fix Max Token ID for Qwen-VL-Chat ( vllm-project#11980 )\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* [Kernel] unified_attention for Attention.forward ( vllm-project#11967 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Doc][V1] Update model implementation guide for V1 support ( vllm-project#11998 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [Doc] Organise installation documentation into categories and tabs ( vllm-project#11935 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [platform] add device_control env var ( vllm-project#12009 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Platform] Move get_punica_wrapper() function to Platform ( vllm-project#11516 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function ( vllm-project#11982 )\n\nSigned-off-by: elijah <f1renze.142857@gmail.com>\n\n* [Doc] Fix build from source and installation link in README.md ( vllm-project#12013 )\n\nSigned-off-by: Yikun <yikunkero@gmail.com>\n\n* Using list\n\n* [Bugfix] Fix deepseekv3 gate bias error ( vllm-project#12002 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* Revert \"[misc] improve memory profiling ( vllm-project#11809 )\"\n\nThis reverts commit 889e662 .\n\n* Multi-lingual P3L ( #356 )\n\n* Commiting the *multilingual* P3L test.\n\n* Created a *multi-lingual* P3L test.\n\n* Making ruff happy.\n\n* .\n\n* Added a reference to the language-scripture Confluence table.\n\n* Typo fixing.\n\n* Harmonizing naming.\n\n* Fixing comments in the header.\n\n---------\n\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* Trying to make scales work with compileable attention\n\n* [Docs] Add Sky Computing Lab to project intro ( vllm-project#12019 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [HPU][Bugfix] set_forward_context and CI test execution ( vllm-project#12014 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Doc] Update Quantization Hardware Support Documentation ( vllm-project#12025 )\n\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nCo-authored-by: tjtanaa <tunjian.tan@embeddedllm.com>\n\n* [HPU][misc] add comments for explanation ( vllm-project#12034 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix various bugs in multi-modal processor ( vllm-project#12031 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Kernel] Revert the API change of Attention.forward ( vllm-project#12038 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Platform] Add output for Attention Backend ( vllm-project#11981 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix][Kernel] Give unique name to BlockSparseFlashAttention ( vllm-project#12040 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Explain where the engine args go when using Docker ( vllm-project#12041 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Docs lint\n\n* [Doc]: Update the Json Example of the `Engine Arguments` document ( vllm-project#12045 )\n\n* [Misc]  Merge bitsandbytes_stacked_params_mapping and packed_modules_mapping ( vllm-project#11924 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Kernel] Support MulAndSilu ( vllm-project#11624 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [HPU][Bugfix] Don't use /dev/accel/accel0 for HPU autodetection in setup.py ( vllm-project#12046 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Platform] move current_memory_usage() into platform ( vllm-project#11369 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* [V1][BugFix] Fix edge case in VLM scheduling ( vllm-project#12065 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Misc] Add multipstep chunked-prefill support for FlashInfer ( vllm-project#10467 )\n\n* [core] Turn off GPU communication overlap for Ray executor ( vllm-project#12051 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [core] platform agnostic executor via collective_rpc ( vllm-project#11256 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Update examples to remove SparseAutoModelForCausalLM ( vllm-project#12062 )\n\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\n\n* [V1][Prefix Cache] Move the logic of num_computed_tokens into KVCacheManager ( vllm-project#12003 )\n\n* Fix: cases with empty sparsity config ( vllm-project#12057 )\n\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\n\n* Type-fix: make execute_model output type optional ( vllm-project#12020 )\n\n* [Platform] Do not raise error if _Backend is not found ( vllm-project#12023 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\n\n* [Model]: Support internlm3 ( vllm-project#12037 )\n\n* Misc: allow to use proxy in `HTTPConnection` ( vllm-project#12042 )\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\n\n* [Misc][Quark] Upstream Quark format to VLLM ( vllm-project#10765 )\n\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Doc]: Update `OpenAI-Compatible Server` documents ( vllm-project#12082 )\n\n* [Bugfix] use right truncation for non-generative tasks ( vllm-project#12050 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\n\n* [V1][Core] Autotune encoder cache budget ( vllm-project#11895 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix] Fix _get_lora_device for HQQ marlin ( vllm-project#12090 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* Allow hip sources to be directly included when compiling for rocm. ( vllm-project#12087 )\n\n* [Core] Default to using per_token quantization for fp8 when cutlass is supported. ( vllm-project#8651 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Doc] Add documentation for specifying model architecture ( vllm-project#12105 )\n\n* Various cosmetic/comment fixes ( vllm-project#12089 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Bugfix] Remove hardcoded `head_size=256` for Deepseek v2 and v3 ( vllm-project#12067 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Support torchrun and SPMD-style offline inference ( vllm-project#12071 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core] LLM.collective_rpc interface and RLHF example ( vllm-project#12084 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix max image feature size for Llava-one-vision ( vllm-project#12104 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Enable user marker for vllm profiling ( #357 )\n\n* Enable user marker for vllm profiling\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [misc] Add LoRA kernel micro benchmarks ( vllm-project#11579 )\n\n* [Model] Add support for deepseek-vl2-tiny model ( vllm-project#12068 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Deepseek V3 support ( #364 )\n\n* Changing the hard coded datatype to see if it's enough for the model to work\n\n* Picking the upstrteam moe kernel version\n\n* make upstream fix for v3 also works for rocm v2\n\n* Conditional fnuz dtype\n\n* Requantizing from fn to fnuz\n\n* Requantizing moe as well\n\n* Actually requantizing moe weights\n\n* Conditional requantization and assert on padding in block quant\n\n* Format\n\n---------\n\nCo-authored-by: charlifu <charlifu@amd.com>\n\n* [Bugfix] Set enforce_eager automatically for mllama ( vllm-project#12127 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Bugfix] Fix a path bug in disaggregated prefill example script. ( vllm-project#12121 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* [CI]add genai-perf benchmark in nightly benchmark ( vllm-project#10704 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [Doc] Add instructions on using Podman when SELinux is active ( vllm-project#12136 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix issues in CPU build Dockerfile ( vllm-project#12135 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [BugFix] add more `is not None` check in VllmConfig.__post_init__ ( vllm-project#12138 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Add deepseek_vl2 chat template ( vllm-project#12143 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [ROCm][MoE] moe tuning support for rocm ( vllm-project#12049 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [V1] Move more control of kv cache initialization from model_executor to EngineCore ( vllm-project#11960 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [Misc][LoRA] Improve the readability of LoRA error messages ( vllm-project#12102 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [CI/Build][CPU][Bugfix] Fix CPU CI ( vllm-project#12150 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [core] allow callable in collective_rpc ( vllm-project#12151 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix score api for missing max_model_len validation ( vllm-project#12119 )\n\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\n\n* [Bugfix] Mistral tokenizer encode accept list of str ( vllm-project#12149 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [AMD][FP8] Using MI300 FP8 format on ROCm for block_quant ( vllm-project#12134 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [torch.compile] disable logging when cache is disabled ( vllm-project#12043 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [misc] fix cross-node TP ( vllm-project#12166 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [AMD][CI/Build][Bugfix] use pytorch stale wheel ( vllm-project#12172 )\n\nSigned-off-by: hongxyan <hongxyan@amd.com>\n\n* [core] further polish memory profiling ( vllm-project#12126 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docs] Fix broken link in SECURITY.md ( vllm-project#12175 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Model] Port deepseek-vl2 processor, remove dependency ( vllm-project#12169 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] clean up executor class hierarchy between v1 and v0 ( vllm-project#12171 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Support register quantization method out-of-tree ( vllm-project#11969 )\n\n* [V1] Collect env var for usage stats ( vllm-project#12115 )\n\n* [BUGFIX] Move scores to float32 in case of running xgrammar on cpu ( vllm-project#12152 )\n\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\n\n* [Bugfix] Fix multi-modal processors for transformers 4.48 ( vllm-project#12187 )\n\n* [torch.compile] store inductor compiled Python file ( vllm-project#12182 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* benchmark_serving support --served-model-name param ( vllm-project#12109 )\n\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\n\n* [Misc] Add BNB support to GLM4-V model ( vllm-project#12184 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1] Add V1 support of Qwen2-VL ( vllm-project#12128 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Support for fairseq2 Llama ( vllm-project#11442 )\n\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\n\n* [Bugfix] Fix num_heads value for simple connector when tp enabled ( vllm-project#12074 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [torch.compile] fix sym_tensor_indices ( vllm-project#12191 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Move linting to `pre-commit` ( vllm-project#11975 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [DOC] Fix typo in docstring and assert message ( vllm-project#12194 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [DOC] Add missing docstring in LLMEngine.add_request() ( vllm-project#12195 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix incorrect types in LayerwiseProfileResults ( vllm-project#12196 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Model] Add Qwen2 PRM model support ( vllm-project#12202 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Core] Interface for accessing model from `VllmRunner` ( vllm-project#10353 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] add placeholder format.sh ( vllm-project#12206 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [CI/Build] Remove dummy CI steps ( vllm-project#12208 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build] Make pre-commit faster ( vllm-project#12212 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Upgrade Aria to transformers 4.48 ( vllm-project#12203 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] print a message to suggest how to bypass commit hooks ( vllm-project#12217 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core][bugfix] configure env var during import vllm ( vllm-project#12209 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] Remove `_get_cache_block_size` ( vllm-project#12214 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Pass `attention` to impl backend ( vllm-project#12218 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Fix `HfExampleModels.find_hf_info` ( vllm-project#12223 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI] Pass local python version explicitly to pre-commit mypy.sh ( vllm-project#12224 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Using ROCm6.3.1 base docker and building hipblas-common ( #366 )\n\n* [Misc] Update CODEOWNERS ( vllm-project#12229 )\n\n* fix: update platform detection for M-series arm based MacBook processors ( vllm-project#12227 )\n\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\n\n* [misc] add cuda runtime version to usage data ( vllm-project#12190 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [bugfix] catch xgrammar unsupported array constraints ( vllm-project#12210 )\n\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\n\n* [Kernel] optimize moe_align_block_size for cuda graph and large num_experts (e.g. DeepSeek-V3) ( vllm-project#12222 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Add quantization and guided decoding CODEOWNERS ( vllm-project#12228 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [AMD][Build] Porting dockerfiles from the ROCm/vllm fork ( vllm-project#11777 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [BugFix] Fix GGUF tp>1 when vocab_size is not divisible by 64 ( vllm-project#12230 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [ci/build] disable failed and flaky tests ( vllm-project#12240 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Rename `MultiModalInputsV2 -> MultiModalInputs` ( vllm-project#12244 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc]Add BNB quantization for PaliGemmaForConditionalGeneration  ( vllm-project#12237 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Remove redundant TypeVar from base model ( vllm-project#12248 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix mm_limits access for merged multi-modal processor ( vllm-project#12252 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [torch.compile] transparent compilation with more logging ( vllm-project#12246 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1][Bugfix] Fix data item ordering in mixed-modality inference ( vllm-project#12259 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Remove pytorch comments for outlines + compressed-tensors ( vllm-project#12260 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Platform] improve platforms getattr ( vllm-project#12264 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [ci/build] update nightly torch for gh200 test ( vllm-project#12270 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] fix race condition that leads to wrong order of token returned ( vllm-project#10802 )\n\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\n\n* [Kernel] fix moe_align_block_size error condition ( vllm-project#12239 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\n\n* [v1][stats][1/n] Add RequestStatsUpdate and RequestStats types  ( vllm-project#10907 )\n\nSigned-off-by: rickyx <rickyx@anyscale.com>\n\n* [Bugfix] Multi-sequence broken ( vllm-project#11898 )\n\nSigned-off-by: Andy Lo <andy@mistral.ai>\n\n* [Misc] Remove experimental dep from tracing.py ( vllm-project#12007 )\n\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\n\n* [Misc] Set default backend to SDPA for get_vit_attn_backend ( vllm-project#12235 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Core] Free CPU pinned memory on environment cleanup ( vllm-project#10477 )\n\n* Update pre-commit.yml ( #374 )\n\n* Update pre-commit.yml\n\n* Reapplying missing format\n\n* New codespell exclude location\n\n---------\n\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\n\n* [bugfix] moe tuning. rm is_navi() ( vllm-project#12273 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [BUGFIX] When skip_tokenize_init and multistep are set, execution crashes ( vllm-project#12277 )\n\nSigned-off-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Documentation][AMD] Add information about prebuilt ROCm vLLM docker for perf validation purpose ( vllm-project#12281 )\n\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\n\n* [VLM] Simplify post-processing of replacement info ( vllm-project#12269 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ci/lint] Add back default arg for pre-commit ( vllm-project#12279 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [CI] add docker volume prune to neuron CI ( vllm-project#12291 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* [Ci/Build] Fix mypy errors on main ( vllm-project#12296 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Benchmark] More accurate TPOT calc in `benchmark_serving.py` ( vllm-project#12288 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [core] separate builder init and builder prepare for each batch ( vllm-project#12253 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Build] update requirements of no-device ( vllm-project#12299 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [Core] Support fully transparent sleep mode ( vllm-project#11743 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Avoid unnecessary tokenization ( vllm-project#12310 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model][Bugfix]: correct Aria model output ( vllm-project#12309 )\n\nSigned-off-by: xffxff <1247714429@qq.com>\n\n* [Bugfix][VLM] Fix mixed-modality inference backward compatibility for V0 ( vllm-project#12313 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Doc] Add docs for prompt replacement ( vllm-project#12318 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] Fix the error in the tip for the --lora-modules parameter ( vllm-project#12319 )\n\nSigned-off-by: wangerxiao <863579016@qq.com>\n\n* [Misc]  Improve the readability of BNB error messages  ( vllm-project#12320 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* Skip tokenize/detokenize when it is disabled by arg --skip-tokenizer-init ( #367 )\n\n* switching detokenize flag to be False\n\n* detokenize = False for benchmarks\n\n* restoring default in main vllm code for detokenize\n\n* removing extra spaces\n\n* moving detokenize to flag\n\n* adding support for token ids\n\n---------\n\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Bugfix] Fix HPU multiprocessing executor ( vllm-project#12167 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Core] Support `reset_prefix_cache` ( vllm-project#12284 )\n\n* [Frontend][V1] Online serving performance improvements ( vllm-project#12287 )\n\n* [AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD ( vllm-project#12282 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* FP8 FA fixes ( #381 )\n\n* FP8 FA fixes\n\nSummary:\nAdd missing clamp and fix reciprocal scale computation.\n\n* linter\n\n* Returning the use of the proper stream in allreduce ( #382 )\n\n* [Bugfix] Fixing  AMD LoRA CI test. ( vllm-project#12329 )\n\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\n\n* [Docs] Update FP8 KV Cache documentation ( vllm-project#12238 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Docs] Document vulnerability disclosure process ( vllm-project#12326 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1] Add `uncache_blocks` ( vllm-project#12333 )\n\n* [doc] explain common errors around torch.compile ( vllm-project#12340 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][BugFix] Fix dataclass error due to triton package update ( vllm-project#12338 )\n\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\n\n* [Bugfix] Fix k_proj's bias for whisper self attention ( vllm-project#12342 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Kernel] Flash Attention 3 Support ( vllm-project#12093 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Doc] Troubleshooting errors during model inspection ( vllm-project#12351 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Simplify M-RoPE ( vllm-project#12352 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: imkero <kerorek@outlook.com>\n\n* [Bugfix] Fix broken internvl2 inference with v1 ( vllm-project#12360 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] add wake_up doc and some sanity check ( vllm-project#12361 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [torch.compile] decouple compile sizes and cudagraph sizes ( vllm-project#12243 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [FP8][Kernel] Dynamic kv cache scaling factors computation ( vllm-project#11906 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\n\n* [TPU] Update TPU CI to use torchxla nightly on 20250122 ( vllm-project#12334 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Docs] Document Phi-4 support ( vllm-project#12362 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [BugFix] Fix parameter names and `process_after_weight_loading` for W4A16 MoE Group Act Order  ( vllm-project#11528 )\n\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Misc] Fix OpenAI API Compatibility Issues in Benchmark Script ( vllm-project#12357 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Docs] Add meetup slides ( vllm-project#12345 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* Using pytorch commit past the point when rowwise PR ( pytorch/pytorch#144432 ) was merged ( #384 )\n\n* [Docs] Update spec decode + structured output in compat matrix ( vllm-project#12373 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1][Frontend] Coalesce bunched `RequestOutput`s ( vllm-project#12298 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\n\n* Set weights_only=True when using torch.load() ( vllm-project#12366 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Bugfix] Path join when building local path for S3 clone ( vllm-project#12353 )\n\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\n\n* Update compressed-tensors version ( vllm-project#12367 )\n\n* [V1] Increase default batch size for H100/H200 ( vllm-project#12369 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [perf] fix perf regression from vllm-project#12253 ( vllm-project#12380 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Use VisionArena Dataset for VLM Benchmarking ( vllm-project#12389 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [ci/build] fix wheel size check ( vllm-project#12396 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][Doc] Add missing step in setup instructions ( vllm-project#12382 )\n\n* [ci/build] sync default value for wheel size ( vllm-project#12398 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Enable proxy support in benchmark script ( vllm-project#12356 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Bugfix][Kernel] Fix CUDA 11.8 being broken by FA3 build ( vllm-project#12375 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Applying scales rename to fp8 config ( #387 )\n\n* [Misc] Remove deprecated code ( vllm-project#12383 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons). ( vllm-project#12405 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Dev-docker Documentation Updates ( #378 )\n\n* Dev-docker Documentation Updates\n\nMinor updates to several sections, with links to other documents where appropriate.\n\n* Fix formatting of GEMM filename\n\n* README cleanup\n\n- Reorder some sections of the README to make them easier to follow\n- Improve formatting of bash commands\n- Prefer use of huggingface model names instead of hard-coded directories\n- Clean up wording\n\n* Expanded sample commands for Latency and Throughput\n\n* Fix markdown links\n\n* Fix pre-commit errors\n\n* Updates from review\n\nInitial updates to incorporate feedback from a review session held with @t-parry * Update script args to match current recommendations\n\n* Remove recommended max-num-seqs values for now\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [Bugfix][Kernel] Fix moe align block issue for mixtral ( vllm-project#12413 )\n\n* [Bugfix] Fix BLIP-2 processing ( vllm-project#12412 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ROCm][MoE] MI300 tuned configs Mixtral-8x(7B,22B) | fp16, fp8 ( vllm-project#12408 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [Misc] Add FA2 support to ViT MHA layer ( vllm-project#12355 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [TPU][CI] Update torchxla version in requirement-tpu.txt ( vllm-project#12422 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Misc][Bugfix] FA3 support to ViT MHA layer ( vllm-project#12435 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Perf] Reduce scheduling overhead in model runner after cuda sync ( vllm-project#12094 )\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\n\n* [V1][Bugfix] Fix assertion when mm hashing is turned off ( vllm-project#12439 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Misc] Revert FA on ViT vllm-project#12355 and vllm-project#12435 ( vllm-project#12445 )\n\n* [Frontend] generation_config.json for  maximum tokens( vllm-project#12242 )\n\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 ( vllm-project#12417 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Bugfix/CI] Fix broken kernels/test_mha.py ( vllm-project#12450 )\n\n* [Bugfix][Kernel] Fix perf regression caused by PR vllm-project#12405 ( vllm-project#12434 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Build/CI] Fix libcuda.so linkage ( vllm-project#12424 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Frontend] Rerank API (Jina- and Cohere-compatible API)  ( vllm-project#12376 )\n\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\n\n* [DOC] Add link to vLLM blog ( vllm-project#12460 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [V1] Avoid list creation in input preparation ( vllm-project#12457 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Frontend] Support scores endpoint in run_batch ( vllm-project#12430 )\n\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\n\n* [Bugfix] Fix Granite 3.0 MoE model loading ( vllm-project#12446 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix missing seq_start_loc in xformers prefill metadata ( vllm-project#12464 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Minor] Minor optimizations for update_from_output ( vllm-project#12454 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Fix gpt2 GGUF inference ( vllm-project#12467 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Build] Only build 9.0a for scaled_mm and sparse kernels ( vllm-project#12339 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [V1][Metrics] Add initial Prometheus logger ( vllm-project#12416 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V1][CI/Test] Do basic test for top-p & top-k sampling ( vllm-project#12469 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [FlashInfer] Upgrade to 0.2.0 ( vllm-project#11194 )\n\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* Support FP8 FA from Quark format ( #388 )\n\n* Support FP8 FA from Quark format\n\n* Support FP8 FA from Quark format\n\n* nit: update comment\n\n* Direct call on ROCm\n\n* 20250127 docs update ( #392 )\n\n* updating code blocks\n\n* typo\n\n* updated manifest\n\n* Including feedback\n\n* whitespace\n\n* Deepseek instructions\n\n* hyperlink fix\n\n* hyperlink fix\n\n* updating what is new\n\n* cpx update\n\n* typo\n\n* whitespace\n\n* whitespace\n\n* Faster Custom Paged Attention kernels ( #372 )\n\n* integrate new cpa kernel, update tests and benchmark\n\n* added comments to mfma4 kernel\n\n* further comments for mfma16 kernel\n\n* clang-format\n\n* Lint\n\n* add flag for logits rtz conversion and disable by default\n\n* lint\n\n* [Bugfix]: Fix paged attention unit tests of #372 ( #389 )\n\n* [Bugfix]: fix paged attention tests based on the updated kernels in `csrc/attention/paged_attention_v1.cu`,`csrc/attention/paged_attention_v2.cu` and  `csrc/rocm/attention.cu`.\n\n* improve code documentation.\n\n* lint\n\n---------\n\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: TJian <tunjian1996@gmail.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* Using a more precise profiling on ROCm to properly account for weights padding ( #394 )\n\n* Update Dockerfile.rocm\n\n* [Bugfix]: inclucde the env variables required for running FastSyncLLM\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* fix pre-commit lint\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\nSigned-off-by: Chenguang Li <757486878@qq.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Shanshan Shen <467638484@qq.com>\nSigned-off-by: elijah <f1renze.142857@gmail.com>\nSigned-off-by: Yikun <yikunkero@gmail.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: yisheng <yi.sheng@intel.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nSigned-off-by: Ye Qi <yeq@meta.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nSigned-off-by: hongxyan <hongxyan@amd.com>\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nSigned-off-by: rickyx <rickyx@anyscale.com>\nSigned-off-by: Andy Lo <andy@mistral.ai>\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\nSigned-off-by: kevin <kevin@anyscale.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: xffxff <1247714429@qq.com>\nSigned-off-by: wangerxiao <863579016@qq.com>\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Akshat Tripathi <Akshat.tripathi6568@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Yangcheng Li <liyangcheng.lyc@alibaba-inc.com>\nCo-authored-by: Siyuan Li <94890248+liaoyanqing666@users.noreply.github.com>\nCo-authored-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nCo-authored-by: Concurrensee <yida.wu@amd.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: elijah <30852919+e1ijah1@users.noreply.github.com>\nCo-authored-by: Yikun Jiang <yikunkero@gmail.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Steve Luo <36296769+SunflowerAries@users.noreply.github.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Alexei-V-Ivanov-AMD <156011006+Alexei-V-Ivanov-AMD@users.noreply.github.com>\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Konrad Zawora <kzawora@habana.ai>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: maang-h <55082429+maang-h@users.noreply.github.com>\nCo-authored-by: YiSheng5 <yi.sheng@intel.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan <yuan.zhou@intel.com>\nCo-authored-by: jiangjiadi <34134495+jiangjiadi@users.noreply.github.com>\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: WangErXiao <863579016@qq.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Wallas Henrique <wallashss@users.noreply.github.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Guspan Tanadi <36249910+guspan-tanadi@users.noreply.github.com>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Charles Frye <cfrye59@gmail.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: cennn <61925104+cennn@users.noreply.github.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: minmin <rmm0811@gmail.com>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Fred Reiss <frreiss@us.ibm.com>\nCo-authored-by: shaochangxu <85155497+shaochangxu@users.noreply.github.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: sixgod <evethwillbeok@outlook.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Keyun Tong <tongkeyun@gmail.com>\nCo-authored-by: RunningLeon <maningsheng@sensetime.com>\nCo-authored-by: kewang-xlnx <73578509+kewang-xlnx@users.noreply.github.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: tvirolai-amd <teemu.virolainen@amd.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: charlifu <charlifu@amd.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Hongxia Yang <62075498+hongxiayang@users.noreply.github.com>\nCo-authored-by: yancong <32220263+ice-tong@users.noreply.github.com>\nCo-authored-by: Michal Adamczyk <madamczyk@habana.ai>\nCo-authored-by: gujing <925973396@qq.com>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: I≈üƒ±k <41375111+isikhi@users.noreply.github.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cheng Kuan Yong Jason <jasoncky96@gmail.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nCo-authored-by: Ricky Xu <xuchen727@hotmail.com>\nCo-authored-by: Andy Lo <andylolu24@gmail.com>\nCo-authored-by: Adrian Cole <64215+codefromthecrypt@users.noreply.github.com>\nCo-authored-by: Jani Monoses <jani.monoses@gmail.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: zhou fan <1247714429@qq.com>\nCo-authored-by: ilia-cher <30845429+ilia-cher@users.noreply.github.com>\nCo-authored-by: liuzhenwei <zhenweiliu@habana.ai>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\nCo-authored-by: Siyuan Liu <lsiyuan@google.com>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\nCo-authored-by: omer-dayan <omer@run.ai>\nCo-authored-by: Mohit Deopujari <mdeopujari@habana.ai>\nCo-authored-by: Jeremy Arnold <103538711+JArnoldAMD@users.noreply.github.com>\nCo-authored-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nCo-authored-by: Kyle Mistele <kyle@mistele.com>\nCo-authored-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Bowen Wang <abmfy@icloud.com>\nCo-authored-by: Bowen Bao <bowenbao@amd.com>\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com>\nCo-authored-by: sanyalington <shomy.sanyal@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com> hongxiayang added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Feb 5, 2025 [Bug Fix] Missing vllm.envs ( #405 ) ‚Ä¶ 87b3c56 * [Model] Initialize support for Deepseek-VL2 models ( vllm-project#11578 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Hardware][CPU] Multi-LoRA implementation for the CPU backend ( vllm-project#11100 )\n\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [Hardware][TPU] workaround fix for MoE on TPU ( vllm-project#11764 )\n\n* [V1][Core][1/n] Logging and Metrics ( vllm-project#11962 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [Model] Support GGUF models newly added in `transformers` 4.46.0 ( vllm-project#9685 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [V1] [2/n] Logging and Metrics - `OutputProcessor` Abstraction ( vllm-project#11973 )\n\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\n\n* [MISC] fix typo in kv transfer send recv test ( vllm-project#11983 )\n\n* [Bug] Fix usage of `.transpose()` and `.view()` consecutively. ( vllm-project#11979 )\n\n* [CI][Spec Decode] fix: broken test for EAGLE model ( vllm-project#11972 )\n\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\n\n* [Misc] Fix Deepseek V2 fp8 kv-scale remapping ( vllm-project#11947 )\n\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\n\n* [Misc]Minor Changes about Worker ( vllm-project#11555 )\n\nSigned-off-by: Chenguang Li <757486878@qq.com>\n\n* [platform] add ray_device_key ( vllm-project#11948 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Fix Max Token ID for Qwen-VL-Chat ( vllm-project#11980 )\n\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\n\n* [Kernel] unified_attention for Attention.forward ( vllm-project#11967 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Doc][V1] Update model implementation guide for V1 support ( vllm-project#11998 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\n\n* [Doc] Organise installation documentation into categories and tabs ( vllm-project#11935 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [platform] add device_control env var ( vllm-project#12009 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Platform] Move get_punica_wrapper() function to Platform ( vllm-project#11516 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* bugfix: Fix signature mismatch in benchmark's `get_tokenizer` function ( vllm-project#11982 )\n\nSigned-off-by: elijah <f1renze.142857@gmail.com>\n\n* [Doc] Fix build from source and installation link in README.md ( vllm-project#12013 )\n\nSigned-off-by: Yikun <yikunkero@gmail.com>\n\n* Using list\n\n* [Bugfix] Fix deepseekv3 gate bias error ( vllm-project#12002 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* Revert \"[misc] improve memory profiling ( vllm-project#11809 )\"\n\nThis reverts commit 889e662 .\n\n* Multi-lingual P3L ( #356 )\n\n* Commiting the *multilingual* P3L test.\n\n* Created a *multi-lingual* P3L test.\n\n* Making ruff happy.\n\n* .\n\n* Added a reference to the language-scripture Confluence table.\n\n* Typo fixing.\n\n* Harmonizing naming.\n\n* Fixing comments in the header.\n\n---------\n\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* Trying to make scales work with compileable attention\n\n* [Docs] Add Sky Computing Lab to project intro ( vllm-project#12019 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [HPU][Bugfix] set_forward_context and CI test execution ( vllm-project#12014 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Doc] Update Quantization Hardware Support Documentation ( vllm-project#12025 )\n\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nCo-authored-by: tjtanaa <tunjian.tan@embeddedllm.com>\n\n* [HPU][misc] add comments for explanation ( vllm-project#12034 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix various bugs in multi-modal processor ( vllm-project#12031 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Kernel] Revert the API change of Attention.forward ( vllm-project#12038 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Platform] Add output for Attention Backend ( vllm-project#11981 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix][Kernel] Give unique name to BlockSparseFlashAttention ( vllm-project#12040 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Explain where the engine args go when using Docker ( vllm-project#12041 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Docs lint\n\n* [Doc]: Update the Json Example of the `Engine Arguments` document ( vllm-project#12045 )\n\n* [Misc]  Merge bitsandbytes_stacked_params_mapping and packed_modules_mapping ( vllm-project#11924 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Kernel] Support MulAndSilu ( vllm-project#11624 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [HPU][Bugfix] Don't use /dev/accel/accel0 for HPU autodetection in setup.py ( vllm-project#12046 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Platform] move current_memory_usage() into platform ( vllm-project#11369 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* [V1][BugFix] Fix edge case in VLM scheduling ( vllm-project#12065 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Misc] Add multipstep chunked-prefill support for FlashInfer ( vllm-project#10467 )\n\n* [core] Turn off GPU communication overlap for Ray executor ( vllm-project#12051 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [core] platform agnostic executor via collective_rpc ( vllm-project#11256 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Update examples to remove SparseAutoModelForCausalLM ( vllm-project#12062 )\n\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\n\n* [V1][Prefix Cache] Move the logic of num_computed_tokens into KVCacheManager ( vllm-project#12003 )\n\n* Fix: cases with empty sparsity config ( vllm-project#12057 )\n\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\n\n* Type-fix: make execute_model output type optional ( vllm-project#12020 )\n\n* [Platform] Do not raise error if _Backend is not found ( vllm-project#12023 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\n\n* [Model]: Support internlm3 ( vllm-project#12037 )\n\n* Misc: allow to use proxy in `HTTPConnection` ( vllm-project#12042 )\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\n\n* [Misc][Quark] Upstream Quark format to VLLM ( vllm-project#10765 )\n\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Doc]: Update `OpenAI-Compatible Server` documents ( vllm-project#12082 )\n\n* [Bugfix] use right truncation for non-generative tasks ( vllm-project#12050 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\n\n* [V1][Core] Autotune encoder cache budget ( vllm-project#11895 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix] Fix _get_lora_device for HQQ marlin ( vllm-project#12090 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* Allow hip sources to be directly included when compiling for rocm. ( vllm-project#12087 )\n\n* [Core] Default to using per_token quantization for fp8 when cutlass is supported. ( vllm-project#8651 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Doc] Add documentation for specifying model architecture ( vllm-project#12105 )\n\n* Various cosmetic/comment fixes ( vllm-project#12089 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Bugfix] Remove hardcoded `head_size=256` for Deepseek v2 and v3 ( vllm-project#12067 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Support torchrun and SPMD-style offline inference ( vllm-project#12071 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core] LLM.collective_rpc interface and RLHF example ( vllm-project#12084 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix max image feature size for Llava-one-vision ( vllm-project#12104 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Enable user marker for vllm profiling ( #357 )\n\n* Enable user marker for vllm profiling\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [misc] Add LoRA kernel micro benchmarks ( vllm-project#11579 )\n\n* [Model] Add support for deepseek-vl2-tiny model ( vllm-project#12068 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Deepseek V3 support ( #364 )\n\n* Changing the hard coded datatype to see if it's enough for the model to work\n\n* Picking the upstrteam moe kernel version\n\n* make upstream fix for v3 also works for rocm v2\n\n* Conditional fnuz dtype\n\n* Requantizing from fn to fnuz\n\n* Requantizing moe as well\n\n* Actually requantizing moe weights\n\n* Conditional requantization and assert on padding in block quant\n\n* Format\n\n---------\n\nCo-authored-by: charlifu <charlifu@amd.com>\n\n* [Bugfix] Set enforce_eager automatically for mllama ( vllm-project#12127 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Bugfix] Fix a path bug in disaggregated prefill example script. ( vllm-project#12121 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* [CI]add genai-perf benchmark in nightly benchmark ( vllm-project#10704 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [Doc] Add instructions on using Podman when SELinux is active ( vllm-project#12136 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix issues in CPU build Dockerfile ( vllm-project#12135 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [BugFix] add more `is not None` check in VllmConfig.__post_init__ ( vllm-project#12138 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Add deepseek_vl2 chat template ( vllm-project#12143 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [ROCm][MoE] moe tuning support for rocm ( vllm-project#12049 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [V1] Move more control of kv cache initialization from model_executor to EngineCore ( vllm-project#11960 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [Misc][LoRA] Improve the readability of LoRA error messages ( vllm-project#12102 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [CI/Build][CPU][Bugfix] Fix CPU CI ( vllm-project#12150 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [core] allow callable in collective_rpc ( vllm-project#12151 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix score api for missing max_model_len validation ( vllm-project#12119 )\n\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\n\n* [Bugfix] Mistral tokenizer encode accept list of str ( vllm-project#12149 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [AMD][FP8] Using MI300 FP8 format on ROCm for block_quant ( vllm-project#12134 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [torch.compile] disable logging when cache is disabled ( vllm-project#12043 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [misc] fix cross-node TP ( vllm-project#12166 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [AMD][CI/Build][Bugfix] use pytorch stale wheel ( vllm-project#12172 )\n\nSigned-off-by: hongxyan <hongxyan@amd.com>\n\n* [core] further polish memory profiling ( vllm-project#12126 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docs] Fix broken link in SECURITY.md ( vllm-project#12175 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Model] Port deepseek-vl2 processor, remove dependency ( vllm-project#12169 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] clean up executor class hierarchy between v1 and v0 ( vllm-project#12171 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Support register quantization method out-of-tree ( vllm-project#11969 )\n\n* [V1] Collect env var for usage stats ( vllm-project#12115 )\n\n* [BUGFIX] Move scores to float32 in case of running xgrammar on cpu ( vllm-project#12152 )\n\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\n\n* [Bugfix] Fix multi-modal processors for transformers 4.48 ( vllm-project#12187 )\n\n* [torch.compile] store inductor compiled Python file ( vllm-project#12182 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* benchmark_serving support --served-model-name param ( vllm-project#12109 )\n\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\n\n* [Misc] Add BNB support to GLM4-V model ( vllm-project#12184 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1] Add V1 support of Qwen2-VL ( vllm-project#12128 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Support for fairseq2 Llama ( vllm-project#11442 )\n\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\n\n* [Bugfix] Fix num_heads value for simple connector when tp enabled ( vllm-project#12074 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [torch.compile] fix sym_tensor_indices ( vllm-project#12191 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Move linting to `pre-commit` ( vllm-project#11975 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [DOC] Fix typo in docstring and assert message ( vllm-project#12194 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [DOC] Add missing docstring in LLMEngine.add_request() ( vllm-project#12195 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix incorrect types in LayerwiseProfileResults ( vllm-project#12196 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Model] Add Qwen2 PRM model support ( vllm-project#12202 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Core] Interface for accessing model from `VllmRunner` ( vllm-project#10353 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] add placeholder format.sh ( vllm-project#12206 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [CI/Build] Remove dummy CI steps ( vllm-project#12208 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build] Make pre-commit faster ( vllm-project#12212 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Upgrade Aria to transformers 4.48 ( vllm-project#12203 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] print a message to suggest how to bypass commit hooks ( vllm-project#12217 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core][bugfix] configure env var during import vllm ( vllm-project#12209 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] Remove `_get_cache_block_size` ( vllm-project#12214 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Pass `attention` to impl backend ( vllm-project#12218 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Fix `HfExampleModels.find_hf_info` ( vllm-project#12223 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI] Pass local python version explicitly to pre-commit mypy.sh ( vllm-project#12224 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Using ROCm6.3.1 base docker and building hipblas-common ( #366 )\n\n* [Misc] Update CODEOWNERS ( vllm-project#12229 )\n\n* fix: update platform detection for M-series arm based MacBook processors ( vllm-project#12227 )\n\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\n\n* [misc] add cuda runtime version to usage data ( vllm-project#12190 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [bugfix] catch xgrammar unsupported array constraints ( vllm-project#12210 )\n\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\n\n* [Kernel] optimize moe_align_block_size for cuda graph and large num_experts (e.g. DeepSeek-V3) ( vllm-project#12222 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Add quantization and guided decoding CODEOWNERS ( vllm-project#12228 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [AMD][Build] Porting dockerfiles from the ROCm/vllm fork ( vllm-project#11777 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [BugFix] Fix GGUF tp>1 when vocab_size is not divisible by 64 ( vllm-project#12230 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [ci/build] disable failed and flaky tests ( vllm-project#12240 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Rename `MultiModalInputsV2 -> MultiModalInputs` ( vllm-project#12244 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc]Add BNB quantization for PaliGemmaForConditionalGeneration  ( vllm-project#12237 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Remove redundant TypeVar from base model ( vllm-project#12248 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix mm_limits access for merged multi-modal processor ( vllm-project#12252 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [torch.compile] transparent compilation with more logging ( vllm-project#12246 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1][Bugfix] Fix data item ordering in mixed-modality inference ( vllm-project#12259 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Remove pytorch comments for outlines + compressed-tensors ( vllm-project#12260 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Platform] improve platforms getattr ( vllm-project#12264 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [ci/build] update nightly torch for gh200 test ( vllm-project#12270 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] fix race condition that leads to wrong order of token returned ( vllm-project#10802 )\n\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\n\n* [Kernel] fix moe_align_block_size error condition ( vllm-project#12239 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\n\n* [v1][stats][1/n] Add RequestStatsUpdate and RequestStats types  ( vllm-project#10907 )\n\nSigned-off-by: rickyx <rickyx@anyscale.com>\n\n* [Bugfix] Multi-sequence broken ( vllm-project#11898 )\n\nSigned-off-by: Andy Lo <andy@mistral.ai>\n\n* [Misc] Remove experimental dep from tracing.py ( vllm-project#12007 )\n\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\n\n* [Misc] Set default backend to SDPA for get_vit_attn_backend ( vllm-project#12235 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Core] Free CPU pinned memory on environment cleanup ( vllm-project#10477 )\n\n* Update pre-commit.yml ( #374 )\n\n* Update pre-commit.yml\n\n* Reapplying missing format\n\n* New codespell exclude location\n\n---------\n\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\n\n* [bugfix] moe tuning. rm is_navi() ( vllm-project#12273 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [BUGFIX] When skip_tokenize_init and multistep are set, execution crashes ( vllm-project#12277 )\n\nSigned-off-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Documentation][AMD] Add information about prebuilt ROCm vLLM docker for perf validation purpose ( vllm-project#12281 )\n\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\n\n* [VLM] Simplify post-processing of replacement info ( vllm-project#12269 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ci/lint] Add back default arg for pre-commit ( vllm-project#12279 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [CI] add docker volume prune to neuron CI ( vllm-project#12291 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* [Ci/Build] Fix mypy errors on main ( vllm-project#12296 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Benchmark] More accurate TPOT calc in `benchmark_serving.py` ( vllm-project#12288 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [core] separate builder init and builder prepare for each batch ( vllm-project#12253 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Build] update requirements of no-device ( vllm-project#12299 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [Core] Support fully transparent sleep mode ( vllm-project#11743 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Avoid unnecessary tokenization ( vllm-project#12310 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model][Bugfix]: correct Aria model output ( vllm-project#12309 )\n\nSigned-off-by: xffxff <1247714429@qq.com>\n\n* [Bugfix][VLM] Fix mixed-modality inference backward compatibility for V0 ( vllm-project#12313 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Doc] Add docs for prompt replacement ( vllm-project#12318 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] Fix the error in the tip for the --lora-modules parameter ( vllm-project#12319 )\n\nSigned-off-by: wangerxiao <863579016@qq.com>\n\n* [Misc]  Improve the readability of BNB error messages  ( vllm-project#12320 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* Skip tokenize/detokenize when it is disabled by arg --skip-tokenizer-init ( #367 )\n\n* switching detokenize flag to be False\n\n* detokenize = False for benchmarks\n\n* restoring default in main vllm code for detokenize\n\n* removing extra spaces\n\n* moving detokenize to flag\n\n* adding support for token ids\n\n---------\n\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Bugfix] Fix HPU multiprocessing executor ( vllm-project#12167 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Core] Support `reset_prefix_cache` ( vllm-project#12284 )\n\n* [Frontend][V1] Online serving performance improvements ( vllm-project#12287 )\n\n* [AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD ( vllm-project#12282 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* FP8 FA fixes ( #381 )\n\n* FP8 FA fixes\n\nSummary:\nAdd missing clamp and fix reciprocal scale computation.\n\n* linter\n\n* Returning the use of the proper stream in allreduce ( #382 )\n\n* [Bugfix] Fixing  AMD LoRA CI test. ( vllm-project#12329 )\n\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\n\n* [Docs] Update FP8 KV Cache documentation ( vllm-project#12238 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Docs] Document vulnerability disclosure process ( vllm-project#12326 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1] Add `uncache_blocks` ( vllm-project#12333 )\n\n* [doc] explain common errors around torch.compile ( vllm-project#12340 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][BugFix] Fix dataclass error due to triton package update ( vllm-project#12338 )\n\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\n\n* [Bugfix] Fix k_proj's bias for whisper self attention ( vllm-project#12342 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Kernel] Flash Attention 3 Support ( vllm-project#12093 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Doc] Troubleshooting errors during model inspection ( vllm-project#12351 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Simplify M-RoPE ( vllm-project#12352 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: imkero <kerorek@outlook.com>\n\n* [Bugfix] Fix broken internvl2 inference with v1 ( vllm-project#12360 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] add wake_up doc and some sanity check ( vllm-project#12361 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [torch.compile] decouple compile sizes and cudagraph sizes ( vllm-project#12243 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [FP8][Kernel] Dynamic kv cache scaling factors computation ( vllm-project#11906 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\n\n* [TPU] Update TPU CI to use torchxla nightly on 20250122 ( vllm-project#12334 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Docs] Document Phi-4 support ( vllm-project#12362 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [BugFix] Fix parameter names and `process_after_weight_loading` for W4A16 MoE Group Act Order  ( vllm-project#11528 )\n\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Misc] Fix OpenAI API Compatibility Issues in Benchmark Script ( vllm-project#12357 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Docs] Add meetup slides ( vllm-project#12345 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* Using pytorch commit past the point when rowwise PR ( pytorch/pytorch#144432 ) was merged ( #384 )\n\n* [Docs] Update spec decode + structured output in compat matrix ( vllm-project#12373 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1][Frontend] Coalesce bunched `RequestOutput`s ( vllm-project#12298 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\n\n* Set weights_only=True when using torch.load() ( vllm-project#12366 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Bugfix] Path join when building local path for S3 clone ( vllm-project#12353 )\n\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\n\n* Update compressed-tensors version ( vllm-project#12367 )\n\n* [V1] Increase default batch size for H100/H200 ( vllm-project#12369 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [perf] fix perf regression from vllm-project#12253 ( vllm-project#12380 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Use VisionArena Dataset for VLM Benchmarking ( vllm-project#12389 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [ci/build] fix wheel size check ( vllm-project#12396 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][Doc] Add missing step in setup instructions ( vllm-project#12382 )\n\n* [ci/build] sync default value for wheel size ( vllm-project#12398 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Enable proxy support in benchmark script ( vllm-project#12356 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Bugfix][Kernel] Fix CUDA 11.8 being broken by FA3 build ( vllm-project#12375 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Applying scales rename to fp8 config ( #387 )\n\n* [Misc] Remove deprecated code ( vllm-project#12383 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons). ( vllm-project#12405 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Dev-docker Documentation Updates ( #378 )\n\n* Dev-docker Documentation Updates\n\nMinor updates to several sections, with links to other documents where appropriate.\n\n* Fix formatting of GEMM filename\n\n* README cleanup\n\n- Reorder some sections of the README to make them easier to follow\n- Improve formatting of bash commands\n- Prefer use of huggingface model names instead of hard-coded directories\n- Clean up wording\n\n* Expanded sample commands for Latency and Throughput\n\n* Fix markdown links\n\n* Fix pre-commit errors\n\n* Updates from review\n\nInitial updates to incorporate feedback from a review session held with @t-parry * Update script args to match current recommendations\n\n* Remove recommended max-num-seqs values for now\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [Bugfix][Kernel] Fix moe align block issue for mixtral ( vllm-project#12413 )\n\n* [Bugfix] Fix BLIP-2 processing ( vllm-project#12412 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ROCm][MoE] MI300 tuned configs Mixtral-8x(7B,22B) | fp16, fp8 ( vllm-project#12408 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [Misc] Add FA2 support to ViT MHA layer ( vllm-project#12355 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [TPU][CI] Update torchxla version in requirement-tpu.txt ( vllm-project#12422 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Misc][Bugfix] FA3 support to ViT MHA layer ( vllm-project#12435 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Perf] Reduce scheduling overhead in model runner after cuda sync ( vllm-project#12094 )\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\n\n* [V1][Bugfix] Fix assertion when mm hashing is turned off ( vllm-project#12439 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Misc] Revert FA on ViT vllm-project#12355 and vllm-project#12435 ( vllm-project#12445 )\n\n* [Frontend] generation_config.json for  maximum tokens( vllm-project#12242 )\n\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 ( vllm-project#12417 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Bugfix/CI] Fix broken kernels/test_mha.py ( vllm-project#12450 )\n\n* [Bugfix][Kernel] Fix perf regression caused by PR vllm-project#12405 ( vllm-project#12434 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Build/CI] Fix libcuda.so linkage ( vllm-project#12424 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Frontend] Rerank API (Jina- and Cohere-compatible API)  ( vllm-project#12376 )\n\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\n\n* [DOC] Add link to vLLM blog ( vllm-project#12460 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [V1] Avoid list creation in input preparation ( vllm-project#12457 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Frontend] Support scores endpoint in run_batch ( vllm-project#12430 )\n\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\n\n* [Bugfix] Fix Granite 3.0 MoE model loading ( vllm-project#12446 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix missing seq_start_loc in xformers prefill metadata ( vllm-project#12464 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Minor] Minor optimizations for update_from_output ( vllm-project#12454 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Fix gpt2 GGUF inference ( vllm-project#12467 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Build] Only build 9.0a for scaled_mm and sparse kernels ( vllm-project#12339 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [V1][Metrics] Add initial Prometheus logger ( vllm-project#12416 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V1][CI/Test] Do basic test for top-p & top-k sampling ( vllm-project#12469 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [FlashInfer] Upgrade to 0.2.0 ( vllm-project#11194 )\n\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* Support FP8 FA from Quark format ( #388 )\n\n* Support FP8 FA from Quark format\n\n* Support FP8 FA from Quark format\n\n* nit: update comment\n\n* Direct call on ROCm\n\n* 20250127 docs update ( #392 )\n\n* updating code blocks\n\n* typo\n\n* updated manifest\n\n* Including feedback\n\n* whitespace\n\n* Deepseek instructions\n\n* hyperlink fix\n\n* hyperlink fix\n\n* updating what is new\n\n* cpx update\n\n* typo\n\n* whitespace\n\n* whitespace\n\n* Faster Custom Paged Attention kernels ( #372 )\n\n* integrate new cpa kernel, update tests and benchmark\n\n* added comments to mfma4 kernel\n\n* further comments for mfma16 kernel\n\n* clang-format\n\n* Lint\n\n* add flag for logits rtz conversion and disable by default\n\n* lint\n\n* [Bugfix]: Fix paged attention unit tests of #372 ( #389 )\n\n* [Bugfix]: fix paged attention tests based on the updated kernels in `csrc/attention/paged_attention_v1.cu`,`csrc/attention/paged_attention_v2.cu` and  `csrc/rocm/attention.cu`.\n\n* improve code documentation.\n\n* lint\n\n---------\n\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: TJian <tunjian1996@gmail.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* Using a more precise profiling on ROCm to properly account for weights padding ( #394 )\n\n* Update Dockerfile.rocm\n\n* [Bugfix]: inclucde the env variables required for running FastSyncLLM\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* fix pre-commit lint\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* [Bugfix] included missing environment variable\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\nSigned-off-by: Chenguang Li <757486878@qq.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Shanshan Shen <467638484@qq.com>\nSigned-off-by: elijah <f1renze.142857@gmail.com>\nSigned-off-by: Yikun <yikunkero@gmail.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: yisheng <yi.sheng@intel.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nSigned-off-by: Ye Qi <yeq@meta.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nSigned-off-by: hongxyan <hongxyan@amd.com>\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nSigned-off-by: rickyx <rickyx@anyscale.com>\nSigned-off-by: Andy Lo <andy@mistral.ai>\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\nSigned-off-by: kevin <kevin@anyscale.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: xffxff <1247714429@qq.com>\nSigned-off-by: wangerxiao <863579016@qq.com>\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: Akshat Tripathi <Akshat.tripathi6568@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Yangcheng Li <liyangcheng.lyc@alibaba-inc.com>\nCo-authored-by: Siyuan Li <94890248+liaoyanqing666@users.noreply.github.com>\nCo-authored-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nCo-authored-by: Concurrensee <yida.wu@amd.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: elijah <30852919+e1ijah1@users.noreply.github.com>\nCo-authored-by: Yikun Jiang <yikunkero@gmail.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Steve Luo <36296769+SunflowerAries@users.noreply.github.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Alexei-V-Ivanov-AMD <156011006+Alexei-V-Ivanov-AMD@users.noreply.github.com>\nCo-authored-by: Alexei V. Ivanov <alivanov@banff-cyxtera-s65-4.amd.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: Konrad Zawora <kzawora@habana.ai>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: maang-h <55082429+maang-h@users.noreply.github.com>\nCo-authored-by: YiSheng5 <yi.sheng@intel.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan <yuan.zhou@intel.com>\nCo-authored-by: jiangjiadi <34134495+jiangjiadi@users.noreply.github.com>\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: WangErXiao <863579016@qq.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Wallas Henrique <wallashss@users.noreply.github.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Guspan Tanadi <36249910+guspan-tanadi@users.noreply.github.com>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Charles Frye <cfrye59@gmail.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: cennn <61925104+cennn@users.noreply.github.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: minmin <rmm0811@gmail.com>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Fred Reiss <frreiss@us.ibm.com>\nCo-authored-by: shaochangxu <85155497+shaochangxu@users.noreply.github.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: sixgod <evethwillbeok@outlook.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Keyun Tong <tongkeyun@gmail.com>\nCo-authored-by: RunningLeon <maningsheng@sensetime.com>\nCo-authored-by: kewang-xlnx <73578509+kewang-xlnx@users.noreply.github.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: tvirolai-amd <teemu.virolainen@amd.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: charlifu <charlifu@amd.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Hongxia Yang <62075498+hongxiayang@users.noreply.github.com>\nCo-authored-by: yancong <32220263+ice-tong@users.noreply.github.com>\nCo-authored-by: Michal Adamczyk <madamczyk@habana.ai>\nCo-authored-by: gujing <925973396@qq.com>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: I≈üƒ±k <41375111+isikhi@users.noreply.github.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cheng Kuan Yong Jason <jasoncky96@gmail.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nCo-authored-by: Ricky Xu <xuchen727@hotmail.com>\nCo-authored-by: Andy Lo <andylolu24@gmail.com>\nCo-authored-by: Adrian Cole <64215+codefromthecrypt@users.noreply.github.com>\nCo-authored-by: Jani Monoses <jani.monoses@gmail.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: zhou fan <1247714429@qq.com>\nCo-authored-by: ilia-cher <30845429+ilia-cher@users.noreply.github.com>\nCo-authored-by: liuzhenwei <zhenweiliu@habana.ai>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\nCo-authored-by: Siyuan Liu <lsiyuan@google.com>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\nCo-authored-by: omer-dayan <omer@run.ai>\nCo-authored-by: Mohit Deopujari <mdeopujari@habana.ai>\nCo-authored-by: Jeremy Arnold <103538711+JArnoldAMD@users.noreply.github.com>\nCo-authored-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nCo-authored-by: Kyle Mistele <kyle@mistele.com>\nCo-authored-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Bowen Wang <abmfy@icloud.com>\nCo-authored-by: Bowen Bao <bowenbao@amd.com>\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com>\nCo-authored-by: sanyalington <shomy.sanyal@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com> NickLucche pushed a commit\n        to NickLucche/vllm\n      that referenced\n      this pull request Feb 7, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ 0048cc4 ‚Ä¶12287 ) GWS0428 pushed a commit\n        to GWS0428/VARserve\n      that referenced\n      this pull request Feb 12, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ a432d0d ‚Ä¶12287 ) hongxiayang added a commit\n        to ROCm/vllm\n      that referenced\n      this pull request Feb 19, 2025 [FEAT] [AITER] Support AITER operators: Fused MoE, Linear, Norm ( #436 ) ‚Ä¶ 4c8c86d * [Doc] Update Quantization Hardware Support Documentation ( vllm-project#12025 )\n\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nCo-authored-by: tjtanaa <tunjian.tan@embeddedllm.com>\n\n* [HPU][misc] add comments for explanation ( vllm-project#12034 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix various bugs in multi-modal processor ( vllm-project#12031 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Kernel] Revert the API change of Attention.forward ( vllm-project#12038 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Platform] Add output for Attention Backend ( vllm-project#11981 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix][Kernel] Give unique name to BlockSparseFlashAttention ( vllm-project#12040 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Explain where the engine args go when using Docker ( vllm-project#12041 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Docs lint\n\n* [Doc]: Update the Json Example of the `Engine Arguments` document ( vllm-project#12045 )\n\n* [Misc]  Merge bitsandbytes_stacked_params_mapping and packed_modules_mapping ( vllm-project#11924 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Kernel] Support MulAndSilu ( vllm-project#11624 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [HPU][Bugfix] Don't use /dev/accel/accel0 for HPU autodetection in setup.py ( vllm-project#12046 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Platform] move current_memory_usage() into platform ( vllm-project#11369 )\n\nSigned-off-by: Shanshan Shen <467638484@qq.com>\n\n* [V1][BugFix] Fix edge case in VLM scheduling ( vllm-project#12065 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Misc] Add multipstep chunked-prefill support for FlashInfer ( vllm-project#10467 )\n\n* [core] Turn off GPU communication overlap for Ray executor ( vllm-project#12051 )\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\n\n* [core] platform agnostic executor via collective_rpc ( vllm-project#11256 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Doc] Update examples to remove SparseAutoModelForCausalLM ( vllm-project#12062 )\n\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\n\n* [V1][Prefix Cache] Move the logic of num_computed_tokens into KVCacheManager ( vllm-project#12003 )\n\n* Fix: cases with empty sparsity config ( vllm-project#12057 )\n\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\n\n* Type-fix: make execute_model output type optional ( vllm-project#12020 )\n\n* [Platform] Do not raise error if _Backend is not found ( vllm-project#12023 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\n\n* [Model]: Support internlm3 ( vllm-project#12037 )\n\n* Misc: allow to use proxy in `HTTPConnection` ( vllm-project#12042 )\n\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\n\n* [Misc][Quark] Upstream Quark format to VLLM ( vllm-project#10765 )\n\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Doc]: Update `OpenAI-Compatible Server` documents ( vllm-project#12082 )\n\n* [Bugfix] use right truncation for non-generative tasks ( vllm-project#12050 )\n\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\n\n* [V1][Core] Autotune encoder cache budget ( vllm-project#11895 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Bugfix] Fix _get_lora_device for HQQ marlin ( vllm-project#12090 )\n\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\n\n* Allow hip sources to be directly included when compiling for rocm. ( vllm-project#12087 )\n\n* [Core] Default to using per_token quantization for fp8 when cutlass is supported. ( vllm-project#8651 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* [Doc] Add documentation for specifying model architecture ( vllm-project#12105 )\n\n* Various cosmetic/comment fixes ( vllm-project#12089 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [Bugfix] Remove hardcoded `head_size=256` for Deepseek v2 and v3 ( vllm-project#12067 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Support torchrun and SPMD-style offline inference ( vllm-project#12071 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core] LLM.collective_rpc interface and RLHF example ( vllm-project#12084 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix max image feature size for Llava-one-vision ( vllm-project#12104 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Enable user marker for vllm profiling ( #357 )\n\n* Enable user marker for vllm profiling\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [misc] Add LoRA kernel micro benchmarks ( vllm-project#11579 )\n\n* [Model] Add support for deepseek-vl2-tiny model ( vllm-project#12068 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* Deepseek V3 support ( #364 )\n\n* Changing the hard coded datatype to see if it's enough for the model to work\n\n* Picking the upstrteam moe kernel version\n\n* make upstream fix for v3 also works for rocm v2\n\n* Conditional fnuz dtype\n\n* Requantizing from fn to fnuz\n\n* Requantizing moe as well\n\n* Actually requantizing moe weights\n\n* Conditional requantization and assert on padding in block quant\n\n* Format\n\n---------\n\nCo-authored-by: charlifu <charlifu@amd.com>\n\n* [Bugfix] Set enforce_eager automatically for mllama ( vllm-project#12127 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Bugfix] Fix a path bug in disaggregated prefill example script. ( vllm-project#12121 )\n\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\n\n* [CI]add genai-perf benchmark in nightly benchmark ( vllm-project#10704 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [Doc] Add instructions on using Podman when SELinux is active ( vllm-project#12136 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix issues in CPU build Dockerfile ( vllm-project#12135 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [BugFix] add more `is not None` check in VllmConfig.__post_init__ ( vllm-project#12138 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Add deepseek_vl2 chat template ( vllm-project#12143 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [ROCm][MoE] moe tuning support for rocm ( vllm-project#12049 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [V1] Move more control of kv cache initialization from model_executor to EngineCore ( vllm-project#11960 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\n\n* [Misc][LoRA] Improve the readability of LoRA error messages ( vllm-project#12102 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [CI/Build][CPU][Bugfix] Fix CPU CI ( vllm-project#12150 )\n\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\n\n* [core] allow callable in collective_rpc ( vllm-project#12151 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] Fix score api for missing max_model_len validation ( vllm-project#12119 )\n\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\n\n* [Bugfix] Mistral tokenizer encode accept list of str ( vllm-project#12149 )\n\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\n\n* [AMD][FP8] Using MI300 FP8 format on ROCm for block_quant ( vllm-project#12134 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [torch.compile] disable logging when cache is disabled ( vllm-project#12043 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [misc] fix cross-node TP ( vllm-project#12166 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [AMD][CI/Build][Bugfix] use pytorch stale wheel ( vllm-project#12172 )\n\nSigned-off-by: hongxyan <hongxyan@amd.com>\n\n* [core] further polish memory profiling ( vllm-project#12126 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Docs] Fix broken link in SECURITY.md ( vllm-project#12175 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Model] Port deepseek-vl2 processor, remove dependency ( vllm-project#12169 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] clean up executor class hierarchy between v1 and v0 ( vllm-project#12171 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Support register quantization method out-of-tree ( vllm-project#11969 )\n\n* [V1] Collect env var for usage stats ( vllm-project#12115 )\n\n* [BUGFIX] Move scores to float32 in case of running xgrammar on cpu ( vllm-project#12152 )\n\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\n\n* [Bugfix] Fix multi-modal processors for transformers 4.48 ( vllm-project#12187 )\n\n* [torch.compile] store inductor compiled Python file ( vllm-project#12182 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* benchmark_serving support --served-model-name param ( vllm-project#12109 )\n\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\n\n* [Misc] Add BNB support to GLM4-V model ( vllm-project#12184 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1] Add V1 support of Qwen2-VL ( vllm-project#12128 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Support for fairseq2 Llama ( vllm-project#11442 )\n\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\n\n* [Bugfix] Fix num_heads value for simple connector when tp enabled ( vllm-project#12074 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [torch.compile] fix sym_tensor_indices ( vllm-project#12191 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* Move linting to `pre-commit` ( vllm-project#11975 )\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* [DOC] Fix typo in docstring and assert message ( vllm-project#12194 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [DOC] Add missing docstring in LLMEngine.add_request() ( vllm-project#12195 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Bugfix] Fix incorrect types in LayerwiseProfileResults ( vllm-project#12196 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [Model] Add Qwen2 PRM model support ( vllm-project#12202 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Core] Interface for accessing model from `VllmRunner` ( vllm-project#10353 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] add placeholder format.sh ( vllm-project#12206 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [CI/Build] Remove dummy CI steps ( vllm-project#12208 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI/Build] Make pre-commit faster ( vllm-project#12212 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model] Upgrade Aria to transformers 4.48 ( vllm-project#12203 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [misc] print a message to suggest how to bypass commit hooks ( vllm-project#12217 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [core][bugfix] configure env var during import vllm ( vllm-project#12209 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1] Remove `_get_cache_block_size` ( vllm-project#12214 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* [Misc] Pass `attention` to impl backend ( vllm-project#12218 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Fix `HfExampleModels.find_hf_info` ( vllm-project#12223 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [CI] Pass local python version explicitly to pre-commit mypy.sh ( vllm-project#12224 )\n\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\n\n* Using ROCm6.3.1 base docker and building hipblas-common ( #366 )\n\n* [Misc] Update CODEOWNERS ( vllm-project#12229 )\n\n* fix: update platform detection for M-series arm based MacBook processors ( vllm-project#12227 )\n\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\n\n* [misc] add cuda runtime version to usage data ( vllm-project#12190 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\n\n* [bugfix] catch xgrammar unsupported array constraints ( vllm-project#12210 )\n\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\n\n* [Kernel] optimize moe_align_block_size for cuda graph and large num_experts (e.g. DeepSeek-V3) ( vllm-project#12222 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* Add quantization and guided decoding CODEOWNERS ( vllm-project#12228 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\n\n* [AMD][Build] Porting dockerfiles from the ROCm/vllm fork ( vllm-project#11777 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\n\n* [BugFix] Fix GGUF tp>1 when vocab_size is not divisible by 64 ( vllm-project#12230 )\n\nSigned-off-by: NickLucche <nlucches@redhat.com>\n\n* [ci/build] disable failed and flaky tests ( vllm-project#12240 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Rename `MultiModalInputsV2 -> MultiModalInputs` ( vllm-project#12244 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc]Add BNB quantization for PaliGemmaForConditionalGeneration  ( vllm-project#12237 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* [Misc] Remove redundant TypeVar from base model ( vllm-project#12248 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix mm_limits access for merged multi-modal processor ( vllm-project#12252 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [torch.compile] transparent compilation with more logging ( vllm-project#12246 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [V1][Bugfix] Fix data item ordering in mixed-modality inference ( vllm-project#12259 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* Remove pytorch comments for outlines + compressed-tensors ( vllm-project#12260 )\n\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\n\n* [Platform] improve platforms getattr ( vllm-project#12264 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [ci/build] update nightly torch for gh200 test ( vllm-project#12270 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Bugfix] fix race condition that leads to wrong order of token returned ( vllm-project#10802 )\n\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\n\n* [Kernel] fix moe_align_block_size error condition ( vllm-project#12239 )\n\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\n\n* [v1][stats][1/n] Add RequestStatsUpdate and RequestStats types  ( vllm-project#10907 )\n\nSigned-off-by: rickyx <rickyx@anyscale.com>\n\n* [Bugfix] Multi-sequence broken ( vllm-project#11898 )\n\nSigned-off-by: Andy Lo <andy@mistral.ai>\n\n* [Misc] Remove experimental dep from tracing.py ( vllm-project#12007 )\n\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\n\n* [Misc] Set default backend to SDPA for get_vit_attn_backend ( vllm-project#12235 )\n\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Core] Free CPU pinned memory on environment cleanup ( vllm-project#10477 )\n\n* Update pre-commit.yml ( #374 )\n\n* Update pre-commit.yml\n\n* Reapplying missing format\n\n* New codespell exclude location\n\n---------\n\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\n\n* [bugfix] moe tuning. rm is_navi() ( vllm-project#12273 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [BUGFIX] When skip_tokenize_init and multistep are set, execution crashes ( vllm-project#12277 )\n\nSigned-off-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Documentation][AMD] Add information about prebuilt ROCm vLLM docker for perf validation purpose ( vllm-project#12281 )\n\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\n\n* [VLM] Simplify post-processing of replacement info ( vllm-project#12269 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ci/lint] Add back default arg for pre-commit ( vllm-project#12279 )\n\nSigned-off-by: kevin <kevin@anyscale.com>\n\n* [CI] add docker volume prune to neuron CI ( vllm-project#12291 )\n\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\n\n* [Ci/Build] Fix mypy errors on main ( vllm-project#12296 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Benchmark] More accurate TPOT calc in `benchmark_serving.py` ( vllm-project#12288 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\n\n* [core] separate builder init and builder prepare for each batch ( vllm-project#12253 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Build] update requirements of no-device ( vllm-project#12299 )\n\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\n\n* [Core] Support fully transparent sleep mode ( vllm-project#11743 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [VLM] Avoid unnecessary tokenization ( vllm-project#12310 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Model][Bugfix]: correct Aria model output ( vllm-project#12309 )\n\nSigned-off-by: xffxff <1247714429@qq.com>\n\n* [Bugfix][VLM] Fix mixed-modality inference backward compatibility for V0 ( vllm-project#12313 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [Doc] Add docs for prompt replacement ( vllm-project#12318 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Misc] Fix the error in the tip for the --lora-modules parameter ( vllm-project#12319 )\n\nSigned-off-by: wangerxiao <863579016@qq.com>\n\n* [Misc]  Improve the readability of BNB error messages  ( vllm-project#12320 )\n\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\n\n* Skip tokenize/detokenize when it is disabled by arg --skip-tokenizer-init ( #367 )\n\n* switching detokenize flag to be False\n\n* detokenize = False for benchmarks\n\n* restoring default in main vllm code for detokenize\n\n* removing extra spaces\n\n* moving detokenize to flag\n\n* adding support for token ids\n\n---------\n\nCo-authored-by: maleksan85 <maleksan@amd.com>\n\n* [Bugfix] Fix HPU multiprocessing executor ( vllm-project#12167 )\n\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\n\n* [Core] Support `reset_prefix_cache` ( vllm-project#12284 )\n\n* [Frontend][V1] Online serving performance improvements ( vllm-project#12287 )\n\n* [AMD][Quantization] Add TritonScaledMMLinearKernel since int8 is broken for AMD ( vllm-project#12282 )\n\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\n\n* FP8 FA fixes ( #381 )\n\n* FP8 FA fixes\n\nSummary:\nAdd missing clamp and fix reciprocal scale computation.\n\n* linter\n\n* Returning the use of the proper stream in allreduce ( #382 )\n\n* [Bugfix] Fixing  AMD LoRA CI test. ( vllm-project#12329 )\n\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\n\n* [Docs] Update FP8 KV Cache documentation ( vllm-project#12238 )\n\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\n\n* [Docs] Document vulnerability disclosure process ( vllm-project#12326 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [V1] Add `uncache_blocks` ( vllm-project#12333 )\n\n* [doc] explain common errors around torch.compile ( vllm-project#12340 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][BugFix] Fix dataclass error due to triton package update ( vllm-project#12338 )\n\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\n\n* [Bugfix] Fix k_proj's bias for whisper self attention ( vllm-project#12342 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [Kernel] Flash Attention 3 Support ( vllm-project#12093 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Doc] Troubleshooting errors during model inspection ( vllm-project#12351 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [V1] Simplify M-RoPE ( vllm-project#12352 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: imkero <kerorek@outlook.com>\n\n* [Bugfix] Fix broken internvl2 inference with v1 ( vllm-project#12360 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [core] add wake_up doc and some sanity check ( vllm-project#12361 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [torch.compile] decouple compile sizes and cudagraph sizes ( vllm-project#12243 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [FP8][Kernel] Dynamic kv cache scaling factors computation ( vllm-project#11906 )\n\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\n\n* [TPU] Update TPU CI to use torchxla nightly on 20250122 ( vllm-project#12334 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Docs] Document Phi-4 support ( vllm-project#12362 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [BugFix] Fix parameter names and `process_after_weight_loading` for W4A16 MoE Group Act Order  ( vllm-project#11528 )\n\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\n\n* [Misc] Fix OpenAI API Compatibility Issues in Benchmark Script ( vllm-project#12357 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Docs] Add meetup slides ( vllm-project#12345 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* Using pytorch commit past the point when rowwise PR ( pytorch/pytorch#144432 ) was merged ( #384 )\n\n* Integrated ater: kvcache pa gemm rmsnorm\n\n* fix pa\n\n* fix\n\n* replace topk softmax\n\n* [Docs] Update spec decode + structured output in compat matrix ( vllm-project#12373 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* replace fp moe kernel with aiter kernel\n\n* [V1][Frontend] Coalesce bunched `RequestOutput`s ( vllm-project#12298 )\n\nSigned-off-by: Nick Hill <nhill@redhat.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\n\n* Set weights_only=True when using torch.load() ( vllm-project#12366 )\n\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\n\n* [Bugfix] Path join when building local path for S3 clone ( vllm-project#12353 )\n\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\n\n* change ater to aiter\n\n* Update compressed-tensors version ( vllm-project#12367 )\n\n* [V1] Increase default batch size for H100/H200 ( vllm-project#12369 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [perf] fix perf regression from vllm-project#12253 ( vllm-project#12380 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Use VisionArena Dataset for VLM Benchmarking ( vllm-project#12389 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* [ci/build] fix wheel size check ( vllm-project#12396 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Hardware][Gaudi][Doc] Add missing step in setup instructions ( vllm-project#12382 )\n\n* [ci/build] sync default value for wheel size ( vllm-project#12398 )\n\nSigned-off-by: youkaichao <youkaichao@gmail.com>\n\n* [Misc] Enable proxy support in benchmark script ( vllm-project#12356 )\n\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\n\n* [Bugfix][Kernel] Fix CUDA 11.8 being broken by FA3 build ( vllm-project#12375 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Applying scales rename to fp8 config\n\n* Applying scales rename to fp8 config ( #387 )\n\n* Update Dockerfile.rocm\n\n* [Misc] Remove deprecated code ( vllm-project#12383 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix][Kernel] FA3 Fix - RuntimeError: This flash attention build only supports pack_gqa (for build size reasons). ( vllm-project#12405 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Using aiter moe kernel\n\n* Dev-docker Documentation Updates ( #378 )\n\n* Dev-docker Documentation Updates\n\nMinor updates to several sections, with links to other documents where appropriate.\n\n* Fix formatting of GEMM filename\n\n* README cleanup\n\n- Reorder some sections of the README to make them easier to follow\n- Improve formatting of bash commands\n- Prefer use of huggingface model names instead of hard-coded directories\n- Clean up wording\n\n* Expanded sample commands for Latency and Throughput\n\n* Fix markdown links\n\n* Fix pre-commit errors\n\n* Updates from review\n\nInitial updates to incorporate feedback from a review session held with @t-parry * Update script args to match current recommendations\n\n* Remove recommended max-num-seqs values for now\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\n\n* [Bugfix][Kernel] Fix moe align block issue for mixtral ( vllm-project#12413 )\n\n* [Bugfix] Fix BLIP-2 processing ( vllm-project#12412 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [ROCm][MoE] MI300 tuned configs Mixtral-8x(7B,22B) | fp16, fp8 ( vllm-project#12408 )\n\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\n\n* [Misc] Add FA2 support to ViT MHA layer ( vllm-project#12355 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [TPU][CI] Update torchxla version in requirement-tpu.txt ( vllm-project#12422 )\n\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\n\n* [Misc][Bugfix] FA3 support to ViT MHA layer ( vllm-project#12435 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Perf] Reduce scheduling overhead in model runner after cuda sync ( vllm-project#12094 )\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\n\n* [V1][Bugfix] Fix assertion when mm hashing is turned off ( vllm-project#12439 )\n\nSigned-off-by: Roger Wang <ywang@roblox.com>\n\n* fix pa copy\n\n* pa update\n\n* [Misc] Revert FA on ViT vllm-project#12355 and vllm-project#12435 ( vllm-project#12445 )\n\n* [Frontend] generation_config.json for  maximum tokens( vllm-project#12242 )\n\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\n\n* [Bugfix] Disable w16a16 2of4 sparse CompressedTensors24 ( vllm-project#12417 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>\n\n* add fp16 pa support for aiter\n\n* [Bugfix/CI] Fix broken kernels/test_mha.py ( vllm-project#12450 )\n\n* [Bugfix][Kernel] Fix perf regression caused by PR vllm-project#12405 ( vllm-project#12434 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* [Build/CI] Fix libcuda.so linkage ( vllm-project#12424 )\n\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\n\n* [Frontend] Rerank API (Jina- and Cohere-compatible API)  ( vllm-project#12376 )\n\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\n\n* [DOC] Add link to vLLM blog ( vllm-project#12460 )\n\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\n\n* [V1] Avoid list creation in input preparation ( vllm-project#12457 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Frontend] Support scores endpoint in run_batch ( vllm-project#12430 )\n\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\n\n* [Bugfix] Fix Granite 3.0 MoE model loading ( vllm-project#12446 )\n\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\n\n* [Bugfix] Fix missing seq_start_loc in xformers prefill metadata ( vllm-project#12464 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* [V1][Minor] Minor optimizations for update_from_output ( vllm-project#12454 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [Bugfix] Fix gpt2 GGUF inference ( vllm-project#12467 )\n\nSigned-off-by: Isotr0py <2037008807@qq.com>\n\n* aiter build instructions\n\n* [Build] Only build 9.0a for scaled_mm and sparse kernels ( vllm-project#12339 )\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\n\n* Copy to the right path\n\n* [V1][Metrics] Add initial Prometheus logger ( vllm-project#12416 )\n\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\n\n* [V1][CI/Test] Do basic test for top-p & top-k sampling ( vllm-project#12469 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\n\n* [FlashInfer] Upgrade to 0.2.0 ( vllm-project#11194 )\n\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\n\n* Support FP8 FA from Quark format ( #388 )\n\n* Support FP8 FA from Quark format\n\n* Support FP8 FA from Quark format\n\n* nit: update comment\n\n* Direct call on ROCm\n\n* 20250127 docs update ( #392 )\n\n* updating code blocks\n\n* typo\n\n* updated manifest\n\n* Including feedback\n\n* whitespace\n\n* Deepseek instructions\n\n* hyperlink fix\n\n* hyperlink fix\n\n* updating what is new\n\n* cpx update\n\n* typo\n\n* whitespace\n\n* whitespace\n\n* Add env var toggles to disable AITER MoE or PA (both by default on)\n\n* Update accuracy benchmark for batch size > 1\n\n* Add a few more AITER toggles for norm and linear layers\n\n* Faster Custom Paged Attention kernels ( #372 )\n\n* integrate new cpa kernel, update tests and benchmark\n\n* added comments to mfma4 kernel\n\n* further comments for mfma16 kernel\n\n* clang-format\n\n* Lint\n\n* add flag for logits rtz conversion and disable by default\n\n* lint\n\n* [Bugfix]: Fix paged attention unit tests of #372 ( #389 )\n\n* [Bugfix]: fix paged attention tests based on the updated kernels in `csrc/attention/paged_attention_v1.cu`,`csrc/attention/paged_attention_v2.cu` and  `csrc/rocm/attention.cu`.\n\n* improve code documentation.\n\n* lint\n\n---------\n\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: TJian <tunjian1996@gmail.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* Using a more precise profiling on ROCm to properly account for weights padding ( #394 )\n\n* Public aiter repo\n\n* Fail if aiter build failed silently\n\n* Aiter can only be built on MI300x\n\n* Typo fix\n\n* Aiter PA off by default\n\n* Changes to support updated aiter FP8 PA\n\n* Support FP8 and INT8 KV cache according to ROCm/aiter#90 * add moe weight shuffle for dynamic quant and unquantized path\n\nSigned-off-by: charlifu <charlifu@amd.com>\n\n* Use FP16-native PA after support in ROCm/aiter#97 * Fix: Use FP8 pertoken quantize if KV cache dtype is FP8\n\n* revert rocm_flash_attn.py line 883\n\n* Don't enable by default to use an RC for main vllm-dev docker\n\n* use ck moe for bf16 and fp16 fused_moe\n\n* Merge remote-tracking branch 'origin/aiter_intergration_final' into merge-aiter-llama-fp8\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n* [Bugfix] include moe shuffle env variable\n\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\n\n---------\n\nSigned-off-by: tjtanaa <tunjian.tan@embeddedllm.com>\nSigned-off-by: youkaichao <youkaichao@gmail.com>\nSigned-off-by: DarkLight1337 <tlleungac@connect.ust.hk>\nSigned-off-by: Chen Zhang <zhangch99@outlook.com>\nSigned-off-by: wangxiyuan <wangxiyuan1007@gmail.com>\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nSigned-off-by: Jee Jee Li <pandaleefree@gmail.com>\nSigned-off-by: Roger Wang <ywang@roblox.com>\nSigned-off-by: yisheng <yi.sheng@intel.com>\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Liangfu Chen <liangfc@amazon.com>\nSigned-off-by: Russell Bryant <rbryant@redhat.com>\nSigned-off-by: Yuan Zhou <yuan.zhou@intel.com>\nSigned-off-by: Sourashis Roy <sroy@roblox.com>\nSigned-off-by: Nishidha Panpaliya <nishidha.panpaliya@partner.ibm.com>\nSigned-off-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nSigned-off-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Wallas Santos <wallashss@ibm.com>\nSigned-off-by: jiang1.li <jiang1.li@intel.com>\nSigned-off-by: yan ma <yan.ma@intel.com>\nSigned-off-by: Randall Smith <Randall.Smith@amd.com>\nSigned-off-by: Max de Bayser <mbayser@br.ibm.com>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nSigned-off-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nSigned-off-by: Ye Qi <yeq@meta.com>\nSigned-off-by: Mengqing Cao <cmq0113@163.com>\nSigned-off-by: Joe Runde <Joseph.Runde@ibm.com>\nSigned-off-by: Kunshang Ji <kunshang.ji@intel.com>\nSigned-off-by: Kuntai Du <kuntai@uchicago.edu>\nSigned-off-by: Isotr0py <2037008807@qq.com>\nSigned-off-by: Ren MinMin <renmm6@chinaunicom.cn>\nSigned-off-by: Travis Johnson <tsjohnso@us.ibm.com>\nSigned-off-by: Fred Reiss <frreiss@us.ibm.com>\nSigned-off-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nSigned-off-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nSigned-off-by: NickLucche <nlucches@redhat.com>\nSigned-off-by: Rafael Vasquez <rafvasq21@gmail.com>\nSigned-off-by: Akshat Tripathi <akshat@krai.ai>\nSigned-off-by: Oleg Mosalov <oleg@krai.ai>\nSigned-off-by: rshaw@neuralmagic.com <rshaw@neuralmagic.com>\nSigned-off-by: Yida Wu <yidawu@alumni.cmu.edu>\nSigned-off-by: Chenguang Li <757486878@qq.com>\nSigned-off-by: Alex-Brooks <Alex.brooks@ibm.com>\nSigned-off-by: Shanshan Shen <467638484@qq.com>\nSigned-off-by: elijah <f1renze.142857@gmail.com>\nSigned-off-by: Konrad Zawora <kzawora@habana.ai>\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nSigned-off-by: Rahul Tuli <rahul@neuralmagic.com>\nSigned-off-by: kewang-xlnx <kewang@xilinx.com>\nSigned-off-by: kewang2 <kewang2@amd.com>\nSigned-off-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nSigned-off-by: Yuan Tang <terrytangyuan@gmail.com>\nSigned-off-by: Divakar Verma <divakar.verma@amd.com>\nSigned-off-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nSigned-off-by: hongxyan <hongxyan@amd.com>\nSigned-off-by: Michal Adamczyk <madamczyk@habana.ai>\nSigned-off-by: zibai <zibai.gj@alibaba-inc.com>\nSigned-off-by: Martin Gleize <mgleize@meta.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: isikhi <huseyin.isik000@gmail.com>\nSigned-off-by: Jason Cheng <jasoncky96@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com>\nSigned-off-by: Thomas Parnell <tpa@zurich.ibm.com>\nSigned-off-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nSigned-off-by: rickyx <rickyx@anyscale.com>\nSigned-off-by: Andy Lo <andy@mistral.ai>\nSigned-off-by: Adrian Cole <adrian.cole@elastic.co>\nSigned-off-by: maleksan85 <maleksan@amd.com>\nSigned-off-by: Hongxia Yang <hongxyan@amd.com>\nSigned-off-by: kevin <kevin@anyscale.com>\nSigned-off-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: xffxff <1247714429@qq.com>\nSigned-off-by: wangerxiao <863579016@qq.com>\nSigned-off-by: Alexei V. Ivanov <alexei.ivanov@amd.com>\nSigned-off-by: zhenwei <zhenweiliu@habana.ai>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nSigned-off-by: Siyuan Liu <lsiyuan@google.com>\nSigned-off-by: ElizaWszola <eliza@neuralmagic.com>\nSigned-off-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nSigned-off-by: Omer Dayan (SW-GPU) <omer@run.ai>\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>\nSigned-off-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nSigned-off-by: Tyler Michael Smith <tyler@neuralmagic.com>\nSigned-off-by: Kyle Mistele <kyle@mistele.com>\nSigned-off-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nSigned-off-by: Mark McLoughlin <markmc@redhat.com>\nSigned-off-by: Bowen Wang <abmfy@icloud.com>\nSigned-off-by: charlifu <charlifu@amd.com>\nSigned-off-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: youkaichao <youkaichao@gmail.com>\nCo-authored-by: Cyrus Leung <tlleungac@connect.ust.hk>\nCo-authored-by: Chen Zhang <zhangch99@outlook.com>\nCo-authored-by: wangxiyuan <wangxiyuan1007@gmail.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <Gregory.Shtrasberg@amd.com>\nCo-authored-by: maang-h <55082429+maang-h@users.noreply.github.com>\nCo-authored-by: Gregory Shtrasberg <156009573+gshtras@users.noreply.github.com>\nCo-authored-by: Jee Jee Li <pandaleefree@gmail.com>\nCo-authored-by: Roger Wang <136131678+ywang96@users.noreply.github.com>\nCo-authored-by: YiSheng5 <yi.sheng@intel.com>\nCo-authored-by: Zhonghua Deng <abatom@163.com>\nCo-authored-by: Liangfu Chen <liangfc@amazon.com>\nCo-authored-by: XiaobingZhang <xiaobingzhangupc@gmail.com>\nCo-authored-by: Russell Bryant <rbryant@redhat.com>\nCo-authored-by: Yuan <yuan.zhou@intel.com>\nCo-authored-by: Cyrus Leung <cyrus.tl.leung@gmail.com>\nCo-authored-by: jiangjiadi <34134495+jiangjiadi@users.noreply.github.com>\nCo-authored-by: jiadi.jjd <jiadi.jjd@antgroup.com>\nCo-authored-by: sroy745 <142070531+sroy745@users.noreply.github.com>\nCo-authored-by: Jie Fu (ÂÇÖÊù∞) <jiefu@tencent.com>\nCo-authored-by: Divakar Verma <137818590+divakar-amd@users.noreply.github.com>\nCo-authored-by: WangErXiao <863579016@qq.com>\nCo-authored-by: Nishidha <nishidha.panpaliya@partner.ibm.com>\nCo-authored-by: Ilya Lavrenov <ilya.lavrenov@intel.com>\nCo-authored-by: Simon Mo <simon.mo@hey.com>\nCo-authored-by: Wallas Henrique <wallashss@users.noreply.github.com>\nCo-authored-by: Michael Goin <michael@neuralmagic.com>\nCo-authored-by: Li, Jiang <jiang1.li@intel.com>\nCo-authored-by: Yan Ma <yan.ma@intel.com>\nCo-authored-by: Robert Shaw <114415538+robertgshaw2-neuralmagic@users.noreply.github.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: rasmith <Randall.Smith@amd.com>\nCo-authored-by: Tyler Michael Smith <tyler@neuralmagic.com>\nCo-authored-by: Maximilien de Bayser <mbayser@br.ibm.com>\nCo-authored-by: Maxime Fournioux <55544262+mfournioux@users.noreply.github.com>\nCo-authored-by: Guspan Tanadi <36249910+guspan-tanadi@users.noreply.github.com>\nCo-authored-by: Ye (Charlotte) Qi <ye.charlotte.qi@gmail.com>\nCo-authored-by: yeq <yeq@devgpu004.lla3.facebook.com>\nCo-authored-by: Mengqing Cao <cmq0113@163.com>\nCo-authored-by: Charles Frye <cfrye59@gmail.com>\nCo-authored-by: Joe Runde <Joseph.Runde@ibm.com>\nCo-authored-by: Kunshang Ji <kunshang.ji@intel.com>\nCo-authored-by: cennn <61925104+cennn@users.noreply.github.com>\nCo-authored-by: Kuntai Du <kuntai@uchicago.edu>\nCo-authored-by: Isotr0py <mozf@mail2.sysu.edu.cn>\nCo-authored-by: minmin <rmm0811@gmail.com>\nCo-authored-by: Ren MinMin <renmm6@chinaunicom.cn>\nCo-authored-by: Travis Johnson <tsjohnso@us.ibm.com>\nCo-authored-by: Fred Reiss <frreiss@us.ibm.com>\nCo-authored-by: Sungjae Lee <33976427+llsj14@users.noreply.github.com>\nCo-authored-by: shaochangxu <85155497+shaochangxu@users.noreply.github.com>\nCo-authored-by: shaochangxu.scx <shaochangxu.scx@antgroup.com>\nCo-authored-by: Nicol√≤ Lucchesi <nlucches@redhat.com>\nCo-authored-by: sixgod <evethwillbeok@outlook.com>\nCo-authored-by: Isotr0py <2037008807@qq.com>\nCo-authored-by: Rafael Vasquez <rafvasq21@gmail.com>\nCo-authored-by: Akshat Tripathi <Akshat.tripathi6568@gmail.com>\nCo-authored-by: Oleg Mosalov <oleg@krai.ai>\nCo-authored-by: Avshalom Manevich <12231371+avshalomman@users.noreply.github.com>\nCo-authored-by: Yangcheng Li <liyangcheng.lyc@alibaba-inc.com>\nCo-authored-by: Siyuan Li <94890248+liaoyanqing666@users.noreply.github.com>\nCo-authored-by: Concurrensee <yida.wu@amd.com>\nCo-authored-by: Chenguang Li <757486878@qq.com>\nCo-authored-by: Alex Brooks <alex.brooks@ibm.com>\nCo-authored-by: Shanshan Shen <467638484@qq.com>\nCo-authored-by: elijah <30852919+e1ijah1@users.noreply.github.com>\nCo-authored-by: Konrad Zawora <kzawora@habana.ai>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Rui Qiao <161574667+ruisearch42@users.noreply.github.com>\nCo-authored-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: Rahul Tuli <rahul@neuralmagic.com>\nCo-authored-by: Keyun Tong <tongkeyun@gmail.com>\nCo-authored-by: RunningLeon <maningsheng@sensetime.com>\nCo-authored-by: kewang-xlnx <73578509+kewang-xlnx@users.noreply.github.com>\nCo-authored-by: kewang2 <kewang2@amd.com>\nCo-authored-by: Varun Sundar Rabindranath <varunsundar08@gmail.com>\nCo-authored-by: Varun Sundar Rabindranath <varun@neuralmagic.com>\nCo-authored-by: tvirolai-amd <teemu.virolainen@amd.com>\nCo-authored-by: Michael Goin <mgoin@redhat.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: charlifu <charlifu@amd.com>\nCo-authored-by: Yuan Tang <terrytangyuan@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>\nCo-authored-by: Hongxia Yang <62075498+hongxiayang@users.noreply.github.com>\nCo-authored-by: yancong <32220263+ice-tong@users.noreply.github.com>\nCo-authored-by: Michal Adamczyk <madamczyk@habana.ai>\nCo-authored-by: gujing <925973396@qq.com>\nCo-authored-by: imkero <kerorek@outlook.com>\nCo-authored-by: Martin Gleize <mgleize@meta.com>\nCo-authored-by: mgleize user <mgleize@a100-st-p4de24xlarge-4.fair-a100.hpcaas>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: I≈üƒ±k <41375111+isikhi@users.noreply.github.com>\nCo-authored-by: Roger Wang <ywang@roblox.com>\nCo-authored-by: Cheng Kuan Yong Jason <jasoncky96@gmail.com>\nCo-authored-by: Jinzhen Lin <linjinzhen@hotmail.com>\nCo-authored-by: Thomas Parnell <tpa@zurich.ibm.com>\nCo-authored-by: Jannis Sch√∂nleber <joennlae@gmail.com>\nCo-authored-by: Ricky Xu <xuchen727@hotmail.com>\nCo-authored-by: Andy Lo <andylolu24@gmail.com>\nCo-authored-by: Adrian Cole <64215+codefromthecrypt@users.noreply.github.com>\nCo-authored-by: Jani Monoses <jani.monoses@gmail.com>\nCo-authored-by: Kevin H. Luu <kevin@anyscale.com>\nCo-authored-by: Aleksandr Malyshev <164964928+maleksan85@users.noreply.github.com>\nCo-authored-by: maleksan85 <maleksan@amd.com>\nCo-authored-by: Nick Hill <nickhill@us.ibm.com>\nCo-authored-by: zhou fan <1247714429@qq.com>\nCo-authored-by: ilia-cher <30845429+ilia-cher@users.noreply.github.com>\nCo-authored-by: Alexei-V-Ivanov-AMD <156011006+Alexei-V-Ivanov-AMD@users.noreply.github.com>\nCo-authored-by: liuzhenwei <zhenweiliu@habana.ai>\nCo-authored-by: Lucas Wilkinson <LucasWilkinson@users.noreply.github.com>\nCo-authored-by: Micah Williamson <micah.williamson@amd.com>\nCo-authored-by: Siyuan Liu <lsiyuan@google.com>\nCo-authored-by: Dipika Sikka <dipikasikka1@gmail.com>\nCo-authored-by: ElizaWszola <eliza@neuralmagic.com>\nCo-authored-by: Junichi Sato <junichi.sato@sbintuitions.co.jp>\nCo-authored-by: amd-ruitang3 <Rui.Tang2@amd.com>\nCo-authored-by: Robert Shaw <rshaw@neuralmagic.com>\nCo-authored-by: omer-dayan <omer@run.ai>\nCo-authored-by: Mohit Deopujari <mdeopujari@habana.ai>\nCo-authored-by: Jeremy Arnold <103538711+JArnoldAMD@users.noreply.github.com>\nCo-authored-by: chenjun <junchen2@amd.com>\nCo-authored-by: ValarLip <340077269@qq.com>\nCo-authored-by: Matthew Hendrey <matthew.hendrey@gmail.com>\nCo-authored-by: Kyle Mistele <kyle@mistele.com>\nCo-authored-by: Pooya Davoodi <pooya.davoodi@parasail.io>\nCo-authored-by: Mark McLoughlin <markmc@redhat.com>\nCo-authored-by: Bowen Wang <abmfy@icloud.com>\nCo-authored-by: Bowen Bao <bowenbao@amd.com>\nCo-authored-by: arakowsk-amd <182798202+arakowsk-amd@users.noreply.github.com>\nCo-authored-by: Matthew Wong <Matthew.Wong2@amd.com>\nCo-authored-by: sanyalington <shomy.sanyal@amd.com>\nCo-authored-by: Joe Shajrawi <17753158+shajrawi@users.noreply.github.com>\nCo-authored-by: vllmellm <vllm.ellm@embeddedllm.com>\nCo-authored-by: charlifu <chalifu@amd.com> mzusman pushed a commit\n        to mzusman/vllm\n      that referenced\n      this pull request Mar 12, 2025 [Frontend][V1] Online serving performance improvements ( vllm-project#‚Ä¶ ‚Ä¶ d7a090a ‚Ä¶12287 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:04",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm-eval, lm_eval, gsm8k | PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, vllm serve, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:47:04",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct",
    "meta-llama/Llama-3.2-1B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,dtype=float16 --tasks gsm8k --batch_size auto --limit 100",
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.2-1B-Instruct,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Frontend][V1] Online serving performance improvements (#12287)",
  "commit_message": "[Frontend][V1] Online serving performance improvements (#12287)",
  "commit_date": "2025-01-22T22:22:12Z",
  "files_changed": [
    "vllm/entrypoints/openai/api_server.py",
    "vllm/entrypoints/openai/protocol.py",
    "vllm/envs.py",
    "vllm/v1/engine/async_llm.py",
    "vllm/v1/engine/core_client.py",
    "vllm/v1/engine/output_processor.py",
    "vllm/v1/request.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 7,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 7,
    "num_hunks": 17,
    "num_edited_lines": 146,
    "num_non_test_edited_lines": 146,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..f510c4150 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -1,5 +1,6 @@\n import asyncio\n import atexit\n+import gc\n import importlib\n import inspect\n import multiprocessing\n@@ -104,6 +105,11 @@ async def lifespan(app: FastAPI):\n             task.add_done_callback(_running_tasks.remove)\n         else:\n             task = None\n+\n+        # Mark the startup heap as static so that it's ignored by GC.\n+        # Reduces pause times of oldest generation collections.\n+        gc.collect()\n+        gc.freeze()\n         try:\n             yield\n         finally:\ndiff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..80403f77d 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -3,7 +3,7 @@\n import re\n import time\n from argparse import Namespace\n-from typing import Any, Dict, List, Literal, Optional, Union\n+from typing import Any, ClassVar, Dict, List, Literal, Optional, Set, Union\n \n import torch\n from pydantic import BaseModel, ConfigDict, Field, model_validator\n@@ -42,23 +42,31 @@ class OpenAIBaseModel(BaseModel):\n     # OpenAI API does allow extra fields\n     model_config = ConfigDict(extra=\"allow\")\n \n+    # Cache class field names\n+    field_names: ClassVar[Optional[Set[str]]] = None\n+\n     @model_validator(mode=\"before\")\n     @classmethod\n     def __log_extra_fields__(cls, data):\n-        if isinstance(data, dict):\n+\n+        field_names = cls.field_names\n+        if field_names is None:\n+            if not isinstance(data, dict):\n+                return data\n             # Get all class field names and their potential aliases\n             field_names = set()\n             for field_name, field in cls.model_fields.items():\n                 field_names.add(field_name)\n-                if hasattr(field, 'alias') and field.alias:\n-                    field_names.add(field.alias)\n-\n-            # Compare against both field names and aliases\n-            extra_fields = data.keys() - field_names\n-            if extra_fields:\n-                logger.warning(\n-                    \"The following fields were present in the request \"\n-                    \"but ignored: %s\", extra_fields)\n+                if alias := getattr(field, 'alias', None):\n+                    field_names.add(alias)\n+            cls.field_names = field_names\n+\n+        # Compare against both field names and aliases\n+        if any(k not in field_names for k in data):\n+            logger.warning(\n+                \"The following fields were present in the request \"\n+                \"but ignored: %s\",\n+                data.keys() - field_names)\n         return data\n \n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 1e68326b2..3a15e00e7 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -73,6 +73,7 @@ if TYPE_CHECKING:\n     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n     VLLM_DISABLE_COMPILE_CACHE: bool = False\n     VLLM_SERVER_DEV_MODE: bool = False\n+    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n \n \n def get_default_cache_root():\n@@ -474,6 +475,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # e.g. `/reset_prefix_cache`\n     \"VLLM_SERVER_DEV_MODE\":\n     lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n+\n+    # Controls the maximum number of requests to handle in a\n+    # single asyncio task when processing per-token outputs in the\n+    # V1 AsyncLLM interface. It is applicable when handling a high\n+    # concurrency of streaming requests.\n+    # Setting this too high can result in a higher variance of\n+    # inter-message latencies. Setting it too low can negatively impact\n+    # TTFT and overall throughput.\n+    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n+    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n }\n \n # end-env-vars-definition\ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e4411..1505b6250 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -2,9 +2,12 @@ import asyncio\n import os\n from typing import AsyncGenerator, List, Mapping, Optional, Type, Union\n \n+import numpy as np\n+\n from vllm.config import ModelConfig, VllmConfig\n from vllm.engine.arg_utils import AsyncEngineArgs\n from vllm.engine.protocol import EngineClient\n+from vllm.envs import VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n from vllm.inputs import INPUT_REGISTRY, InputRegistry, PromptType\n from vllm.inputs.preprocess import InputPreprocessor\n from vllm.logger import init_logger\n@@ -16,7 +19,7 @@ from vllm.sampling_params import SamplingParams\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\n from vllm.usage.usage_lib import UsageContext\n-from vllm.utils import kill_process_tree\n+from vllm.utils import cdiv, kill_process_tree\n from vllm.v1.engine.core_client import EngineCoreClient\n from vllm.v1.engine.output_processor import OutputProcessor\n from vllm.v1.engine.processor import Processor\n@@ -205,17 +208,15 @@ class AsyncLLM(EngineClient):\n \n             # The output_handler task pushes items into the queue.\n             # This task pulls from the queue and yields to caller.\n-            while True:\n+            finished = False\n+            while not finished:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                out = q.get_nowait() if not q.empty() else await q.get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\n-                if out.finished:\n-                    yield out\n-                    break\n-\n+                finished = out.finished\n                 yield out\n \n         # If the request is disconnected by the client, the\n@@ -233,22 +234,41 @@ class AsyncLLM(EngineClient):\n                 # 1) Pull EngineCoreOutputs from the EngineCore.\n                 outputs = await self.engine_core.get_output_async()\n \n-                # 2) Process EngineCoreOutputs.\n-                processed_outputs = self.output_processor.process_outputs(\n-                    outputs.outputs)\n-                # NOTE: RequestOutputs are pushed to their queues.\n-                assert len(processed_outputs.request_outputs) == 0\n-\n-                # 3) Abort any reqs that finished due to stop strings.\n-                await self.engine_core.abort_requests_async(\n-                    processed_outputs.reqs_to_abort)\n+                # Split outputs into chunks of at most\n+                # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the\n+                # event loop for too long.\n+                num_outputs = len(outputs.outputs)\n+                if num_outputs <= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:\n+                    slices = (outputs.outputs, )\n+                else:\n+                    slices = np.array_split(\n+                        outputs.outputs,\n+                        cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n+\n+                iteration_stats = None\n+                for i, outputs_slice in enumerate(slices):\n+                    # 2) Process EngineCoreOutputs.\n+                    processed_outputs = self.output_processor.process_outputs(\n+                        outputs_slice, iteration_stats)\n+                    # NOTE: RequestOutputs are pushed to their queues.\n+                    assert not processed_outputs.request_outputs\n+                    iteration_stats = processed_outputs.iteration_stats\n+\n+                    # Allow other asyncio tasks to run between chunks\n+                    if i + 1 < len(slices):\n+                        await asyncio.sleep(0)\n+\n+                    # 3) Abort any reqs that finished due to stop strings.\n+                    await self.engine_core.abort_requests_async(\n+                        processed_outputs.reqs_to_abort)\n \n                 # 4) Logging.\n                 # TODO(rob): make into a coroutine and launch it in\n                 # background thread once we add Prometheus.\n+                assert iteration_stats is not None\n                 self._log_stats(\n                     scheduler_stats=outputs.scheduler_stats,\n-                    iteration_stats=processed_outputs.iteration_stats,\n+                    iteration_stats=iteration_stats,\n                 )\n \n         except Exception as e:\ndiff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py\nindex 19b89003c..f3b992d68 100644\n--- a/vllm/v1/engine/core_client.py\n+++ b/vllm/v1/engine/core_client.py\n@@ -1,8 +1,9 @@\n+import asyncio\n import os\n import signal\n import weakref\n from abc import ABC, abstractmethod\n-from typing import List, Type\n+from typing import List, Optional, Type\n \n import msgspec\n import zmq\n@@ -255,10 +256,24 @@ class AsyncMPClient(MPClient):\n             log_stats=True,\n         )\n \n+        self.outputs_queue: Optional[asyncio.Queue[bytes]] = None\n+        self.queue_task: Optional[asyncio.Task] = None\n+\n     async def get_output_async(self) -> EngineCoreOutputs:\n+        if self.outputs_queue is None:\n+            # Perform IO in separate task to parallelize as much as possible\n+            self.outputs_queue = asyncio.Queue()\n+\n+            async def process_outputs_socket():\n+                assert self.outputs_queue is not None\n+                while True:\n+                    (frame, ) = await self.output_socket.recv_multipart(\n+                        copy=False)\n+                    self.outputs_queue.put_nowait(frame.buffer)\n+\n+            self.queue_task = asyncio.create_task(process_outputs_socket())\n \n-        frames = await self.output_socket.recv_multipart(copy=False)\n-        return self.decoder.decode(frames[0].buffer)\n+        return self.decoder.decode(await self.outputs_queue.get())\n \n     async def _send_input(self, request_type: EngineCoreRequestType,\n                           request: EngineCoreRequestUnion) -> None:\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..564eab51b 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -101,6 +101,7 @@ class OutputProcessor:\n     def process_outputs(\n         self,\n         engine_core_outputs: List[EngineCoreOutput],\n+        iteration_stats: Optional[IterationStats] = None,\n     ) -> OutputProcessorOutput:\n         \"\"\"\n         Process the EngineCoreOutputs:\n@@ -133,7 +134,8 @@ class OutputProcessor:\n \n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n-        iteration_stats = IterationStats(self.log_stats)\n+        if not iteration_stats:\n+            iteration_stats = IterationStats(self.log_stats)\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n             req_state = self.request_states.get(req_id)\n@@ -175,8 +177,8 @@ class OutputProcessor:\n             iteration_stats=iteration_stats,\n         )\n \n+    @staticmethod\n     def _make_request_output(\n-        self,\n         request_state: RequestState,\n         detokenizer_output: Optional[DetokenizerOutput],\n     ) -> Optional[RequestOutput]:\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..eefcdaf29 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -64,6 +64,12 @@ class Request:\n         # recomputing.\n         self._kv_block_hashes: List[BlockHashType] = []\n \n+        # Read-only views\n+        # Prevent directly appending to the these lists since\n+        # they should also be updated simultaneously.\n+        self.output_token_ids = ConstantList(self._output_token_ids)\n+        self.all_token_ids = ConstantList(self._all_token_ids)\n+\n     @classmethod\n     def from_engine_core_request(cls, request: EngineCoreRequest) -> \"Request\":\n         return cls(\n@@ -79,18 +85,6 @@ class Request:\n             lora_request=request.lora_request,\n         )\n \n-    @property\n-    def output_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the output_token_ids since\n-        # all_token_ids should also be updated simultaneously.\n-        return ConstantList(self._output_token_ids)\n-\n-    @property\n-    def all_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the all_token_ids since\n-        # output_token_ids should also be updated simultaneously\n-        return ConstantList(self._all_token_ids)\n-\n     def append_output_token_ids(\n         self,\n         token_ids: Union[int, List[int]],",
  "apis": [
    "AsyncLLM.generate",
    "AsyncMPClient.get_output_async",
    "OpenAIBaseModel.__log_extra_fields__"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/api_server.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/api_server.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/protocol.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/protocol.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/async_llm.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies multiple non-test source files and applies non-trivial changes that adjust runtime behavior for improved online serving performance. For example, it adds garbage collection optimizations in the API server (gc.collect(), gc.freeze()), caches field names to avoid redundant computation in the protocol module, and restructures the asynchronous output processing loop to split outputs into chunks and yield control to avoid event loop blocking. These changes are clearly aimed at reducing latency and improving throughput in CPU-bound online serving without introducing new features or simple refactoring. Hence, it satisfies the conditions for a performance/optimization related commit.",
  "llm_api_reason": "The commit makes several performance‚Äêoriented changes to the vLLM code. In the OpenAI API server entrypoint, it adds garbage collection calls (gc.collect and gc.freeze) during app lifespan startup to reduce GC pause times. In the OpenAI protocol, the __log_extra_fields__ validator is modified to cache field names to reduce repeated work when processing request payloads. Within the v1 engine code, the asynchronous inference loop is optimized by splitting output processing into smaller chunks (controlled by the new VLLM_V1_OUTPUT_PROC_CHUNK_SIZE environment variable) and by improving the ‚Äúgenerate‚Äù and ‚Äúget_output_async‚Äù methods to reduce blocking and overhead. These changes affect the high‚Äêlevel inference API and the OpenAI protocol models used for serving results."
}