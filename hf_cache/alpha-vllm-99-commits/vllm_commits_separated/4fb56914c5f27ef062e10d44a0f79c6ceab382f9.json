{
  "commit_hash": "4fb56914c5f27ef062e10d44a0f79c6ceab382f9",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21116",
  "pr_date": "2025-07-17",
  "timeline_text": "Copy link Contributor mickaelseznec commented Jul 17, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Essential Elements of an Effective PR Description Checklist The purpose of the PR, such as \"Fix some issue (link existing issues this PR will resolve)\". The test plan, such as providing test command. The test results, such as pasting the results comparison before and after, or e2e results (Optional) The necessary documentation update, such as updating supported_models.md and examples for a new model. Purpose For MLA models that have a q_lora_rank: fuse q_lora and kv_lora into the same matrix (avoids some traffic + one less kernel call). Also adds a implementation for layernorm to operate on strided input, this avoids memory copy. Test Plan Units tests added for strided layernorm. E2E testing & benchamrks results in this PR Test Result Accuracy main ( 20149d8 ) vllm (pretrained=deepseek-ai/DeepSeek-V3-0324,tensor_parallel_size=8,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: 5, batch_size: auto |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr| |-----|------:|----------------|-----:|-----------|---|-----:|---|-----:| |gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.9469|¬±  |0.0062| |     |       |strict-match    |     5|exact_match|‚Üë  |0.9454|¬±  |0.0063| This PR: vllm (pretrained=deepseek-ai/DeepSeek-V3-0324,add_bos_token=true,tensor_parallel_size=8), gen_kwargs: (None), limit: 250.0, num_fewshot: None, batch_size: auto |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr| |-----|------:|----------------|-----:|-----------|---|----:|---|-----:| |gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |0.952|¬±  |0.0135| |     |       |strict-match    |     5|exact_match|‚Üë  |0.952|¬±  |0.0135| Performance main ( 20149d8 ) venv ‚ùØ python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json INFO 07-15 17:16:08 [__init__.py:253] Automatically detected platform cuda. Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', no_stream=False, max_concurrency=None, model='deepseek-ai/DeepSeek-V3-0324', tokenizer=None, use_beam_search=False, num_prompts=1000, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None) Starting initial single prompt test run... Initial test run completed. Starting main benchmark run... Traffic request rate: inf RPS. Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: None 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:58<00:00, 17.10it/s] ============ Serving Benchmark Result ============ Successful requests:                     1000 Benchmark duration (s):                  58.46 Total input tokens:                      219171 Total generated tokens:                  164272 Request throughput (req/s):              17.10 Output token throughput (tok/s):         2809.81 Total Token throughput (tok/s):          6558.65 ---------------Time to First Token---------------- Mean TTFT (ms):                          8290.64 Median TTFT (ms):                        7975.92 P99 TTFT (ms):                           14349.76 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms):                          177.57 Median TPOT (ms):                        115.76 P99 TPOT (ms):                           434.24 ---------------Inter-token Latency---------------- Mean ITL (ms):                           98.84 Median ITL (ms):                         66.80 P99 ITL (ms):                            435.74 ================================================== This PR: venv ‚ùØ python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json INFO 07-17 10:27:38 [__init__.py:253] Automatically detected platform cuda. Namespace(backend='vllm', base_url=None, host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='sharegpt', dataset_path='ShareGPT_V3_unfiltered_cleaned_split.json', no_stream=False, max_concurrency=None, model='deepseek-ai/DeepSeek-V3-0324', tokenizer=None, use_beam_search=False, num_prompts=1000, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=False, save_detailed=False, append_result=False, metadata=None, result_dir=None, result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=1024, random_output_len=128, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None, ramp_up_strategy=None, ramp_up_start_rps=None, ramp_up_end_rps=None) Starting initial single prompt test run... Initial test run completed. Starting main benchmark run... Traffic request rate: inf RPS. Burstiness factor: 1.0 (Poisson process) Maximum request concurrency: None 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:56<00:00, 17.63it/s] ============ Serving Benchmark Result ============ Successful requests:                     1000 Benchmark duration (s):                  56.72 Total input tokens:                      219171 Total generated tokens:                  165898 Request throughput (req/s):              17.63 Output token throughput (tok/s):         2925.10 Total Token throughput (tok/s):          6789.51 ---------------Time to First Token---------------- Mean TTFT (ms):                          6917.92 Median TTFT (ms):                        6629.26 P99 TTFT (ms):                           12941.51 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms):                          171.18 Median TPOT (ms):                        108.68 P99 TPOT (ms):                           461.18 ---------------Inter-token Latency---------------- Mean ITL (ms):                           95.07 Median ITL (ms):                         67.52 P99 ITL (ms):                            431.03 ================================================== (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üöÄ 4 mgoin, hj-mistral, LucasWilkinson, and simon-mo reacted with rocket emoji All reactions üöÄ 4 reactions mickaelseznec requested review from tlrmchlsmth , WoosukKwon , mgoin and robertgshaw2-redhat as code owners July 17, 2025 10:36 Copy link github-actions bot commented Jul 17, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the deepseek Related to DeepSeek models label Jul 17, 2025 gemini-code-assist bot reviewed Jul 17, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces two significant optimizations: fusing the QKV projection for MLA models and implementing a strided LayerNorm kernel. The changes are well-implemented and should provide the performance benefits described. The fusion of Q-LoRA and KV-LoRA projections into a single matrix operation for DeepSeek-V2 models is a smart optimization that reduces kernel launch overhead and memory traffic. The introduction of MergedReplicatedLinear to handle this fusion is a clean way to extend the existing linear layer infrastructure. The addition of a strided layernorm implementation is crucial for the fusion to be effective, as it avoids expensive .contiguous() calls on tensor slices. The CUDA kernels have been updated correctly to handle the input_stride , and the PyTorch bindings are adjusted accordingly. The test suite has been properly extended to cover the new strided input case for the layernorm kernels, ensuring the correctness of the new implementation. Overall, this is a high-quality contribution that improves performance while maintaining code clarity and correctness. I have no major concerns. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions feat: add fused MLA QKV + strided layernorm ‚Ä¶ e3962ab Signed-off-by: Mickael Seznec <mickael@mistral.ai> mickaelseznec force-pushed the mseznec/merged-qkv-and-strided-layernorm branch\n    from 75b3d50 to e3962ab Compare July 17, 2025 10:38 mgoin requested a review\n  from LucasWilkinson July 17, 2025 12:06 tlrmchlsmth reviewed Jul 17, 2025 View reviewed changes csrc/layernorm_kernels.cu Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . robertgshaw2-redhat reviewed Jul 17, 2025 View reviewed changes vllm/model_executor/models/deepseek_v2.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . LucasWilkinson reviewed Jul 17, 2025 View reviewed changes vllm/model_executor/layers/linear.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . mickaelseznec added 2 commits July 17, 2025 14:06 review: stride->int64_t ‚Ä¶ 3f6b148 Signed-off-by: Mickael Seznec <mickael@mistral.ai> pre-commit ‚Ä¶ 4f77a0d Signed-off-by: Mickael Seznec <mickael@mistral.ai> Copy link Collaborator LucasWilkinson commented Jul 17, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Nice thanks for the contribution! Clean, simple and gives perf; the trifecta haha. Overall looks pretty good to me but I think one of the weight loading experts, i.e. @dsikka or @mgoin should take a look to make sure we dont break 4bit quantized models ‚ù§Ô∏è 1 mickaelseznec reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . fix: better fallback in weight loader ‚Ä¶ 49a9b00 Signed-off-by: Mickael Seznec <mickael@mistral.ai> yewentao256 reviewed Jul 17, 2025 View reviewed changes Copy link Collaborator yewentao256 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for the work! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions csrc/layernorm_kernels.cu Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/layers/linear.py Comment on lines +423 to +424 from vllm.model_executor.layers.quantization.fp8 import ( Fp8LinearMethod, Fp8MoEMethod) Copy link Collaborator yewentao256 Jul 17, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Could we refactor the code, so that we can put import on top of the file without worrying about the circular import instead here? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author mickaelseznec Jul 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Well it's tricky, because FP8Linear already depends on Linear (which makes sense). I don't know how you'd like to proceed. I lazily copy/pasted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/linear.py#L787-L791 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator yewentao256 Jul 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Yeah I am thinking, if A imports B, B imports A. We can have a base file C, move base things into C, so A imports C, B imports C as well. We don't need to do it right now in this pr if you don't wish, could be done by refactor in the future. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author mickaelseznec Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Sure! Here, the best way would probably be to rely on inheritance by defining (and overriding) methods like: QuantizeMethodBase.supports_block_quantization() However, I don't have a complete overview on all the supported cases and potential edge-cases and it might make this PR heavier than needed now. Happy to help with a following PR though :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator yewentao256 Jul 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Sounds great, certainly you can do that in another pr Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions mickaelseznec changed the title feat: add fused MLA QKV + strided layernorm [perf] Add fused MLA QKV + strided layernorm Jul 18, 2025 mickaelseznec added 2 commits July 18, 2025 13:12 review: fewer magic numbers ‚Ä¶ b6f3455 Signed-off-by: Mickael Seznec <mickael@mistral.ai> fix: pre-commit ‚Ä¶ d1be02d Signed-off-by: Mickael Seznec <mickael@mistral.ai> mgoin approved these changes Jul 21, 2025 View reviewed changes Copy link Member mgoin left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Nice work! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Merge branch 'main' into mseznec/merged-qkv-and-strided-layernorm 070dfa4 mgoin enabled auto-merge (squash) July 21, 2025 18:38 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 21, 2025 Hide details View details vllm-bot merged commit 4fb5691 into vllm-project : main Jul 22, 2025 106 of 108 checks passed Uh oh! There was an error while loading. Please reload this page . xuechendi mentioned this pull request Jul 22, 2025 [BUGFIX] deepseek-v2-lite failed due to fused_qkv_a_proj name update #21414 Merged 4 tasks yeqcharlotte pushed a commit\n        to yeqcharlotte/vllm\n      that referenced\n      this pull request Jul 23, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 37ec8cb Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> zixi-qi pushed a commit\n        to zixi-qi/vllm\n      that referenced\n      this pull request Jul 23, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 46b75f4 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: qizixi <qizixi@meta.com> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 7c6c84c Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> benchislett mentioned this pull request Jul 30, 2025 [Bugfix] Fix MTP weight loading #21941 Merged avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ da8f8fe Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> wenscarl pushed a commit\n        to wenscarl/vllm\n      that referenced\n      this pull request Aug 4, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 994dd51 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: shuw <shuw@nvidia.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 95d77b5 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 4402c98 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> fxmarty-amd mentioned this pull request Aug 6, 2025 [Bugfix] Add missing packed_modules_mapping to DeepseekV2ForCausalLM #22352 Merged npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 2e941f0 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ bb2b8ee Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 3c47ab0 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ b771731 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> benchislett mentioned this pull request Aug 14, 2025 [Model] Support deepseek with eagle #21086 Merged 4 tasks diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 52f0b84 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> cjackal mentioned this pull request Aug 25, 2025 [Bug]: DeepSeek-R1 AWQ model loading is not possible in v0.10.0 or later. #23530 Open 1 task epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 28, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ 7b35796 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [perf] Add fused MLA QKV + strided layernorm ( vllm-project#21116 ) ‚Ä¶ c7e4502 Signed-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com> cjackal mentioned this pull request Aug 30, 2025 DeepSeek fix: awq x mergedreplicatedlinear #23764 Open 5 tasks Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:06",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: gsm8k, gsm8k | PERF: ttft, TTFT, TTFT | SERVING: Serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:50:06",
  "models": [
    "deepseek-ai/DeepSeek-V3-0324"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3-0324 --dataset-name sharegpt --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json",
  "commit_subject": "[perf] Add fused MLA QKV + strided layernorm (#21116)",
  "commit_message": "[perf] Add fused MLA QKV + strided layernorm (#21116)\n\nSigned-off-by: Mickael Seznec <mickael@mistral.ai>\nCo-authored-by: mgoin <mgoin64@gmail.com>",
  "commit_date": "2025-07-22T07:07:44-07:00",
  "files_changed": [
    "csrc/layernorm_kernels.cu",
    "csrc/layernorm_quant_kernels.cu",
    "csrc/quantization/fp8/common.cu",
    "tests/kernels/core/test_layernorm.py",
    "vllm/model_executor/layers/linear.py",
    "vllm/model_executor/layers/quantization/fp8.py",
    "vllm/model_executor/models/deepseek_v2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 6,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 7,
    "num_hunks": 46,
    "num_edited_lines": 280,
    "num_non_test_edited_lines": 254,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\nindex d073dd6d2..f051eb070 100644\n--- a/csrc/layernorm_kernels.cu\n+++ b/csrc/layernorm_kernels.cu\n@@ -15,15 +15,16 @@ namespace vllm {\n // TODO(woosuk): Further optimize this kernel.\n template <typename scalar_t>\n __global__ void rms_norm_kernel(\n-    scalar_t* __restrict__ out,           // [..., hidden_size]\n-    const scalar_t* __restrict__ input,   // [..., hidden_size]\n+    scalar_t* __restrict__ out,          // [..., hidden_size]\n+    const scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int64_t input_stride,\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n   __shared__ float s_variance;\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[blockIdx.x * input_stride + idx];\n     variance += x * x;\n   }\n \n@@ -37,7 +38,7 @@ __global__ void rms_norm_kernel(\n   __syncthreads();\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n+    float x = (float)input[blockIdx.x * input_stride + idx];\n     out[blockIdx.x * hidden_size + idx] =\n         ((scalar_t)(x * s_variance)) * weight[idx];\n   }\n@@ -50,7 +51,8 @@ __global__ void rms_norm_kernel(\n template <typename scalar_t, int width>\n __global__ std::enable_if_t<(width > 0) && _typeConvert<scalar_t>::exists>\n fused_add_rms_norm_kernel(\n-    scalar_t* __restrict__ input,         // [..., hidden_size]\n+    scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int64_t input_stride,\n     scalar_t* __restrict__ residual,      // [..., hidden_size]\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n@@ -59,6 +61,7 @@ fused_add_rms_norm_kernel(\n   static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);\n \n   const int vec_hidden_size = hidden_size / width;\n+  const int64_t vec_input_stride = input_stride / width;\n   __shared__ float s_variance;\n   float variance = 0.0f;\n   /* These and the argument pointers are all declared `restrict` as they are\n@@ -73,7 +76,8 @@ fused_add_rms_norm_kernel(\n \n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n     int id = blockIdx.x * vec_hidden_size + idx;\n-    _f16Vec<scalar_t, width> temp = input_v[id];\n+    int64_t strided_id = blockIdx.x * vec_input_stride + idx;\n+    _f16Vec<scalar_t, width> temp = input_v[strided_id];\n     temp += residual_v[id];\n     variance += temp.sum_squares();\n     residual_v[id] = temp;\n@@ -90,10 +94,11 @@ fused_add_rms_norm_kernel(\n \n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n     int id = blockIdx.x * vec_hidden_size + idx;\n+    int64_t strided_id = blockIdx.x * vec_input_stride + idx;\n     _f16Vec<scalar_t, width> temp = residual_v[id];\n     temp *= s_variance;\n     temp *= weight_v[idx];\n-    input_v[id] = temp;\n+    input_v[strided_id] = temp;\n   }\n }\n \n@@ -103,7 +108,8 @@ fused_add_rms_norm_kernel(\n template <typename scalar_t, int width>\n __global__ std::enable_if_t<(width == 0) || !_typeConvert<scalar_t>::exists>\n fused_add_rms_norm_kernel(\n-    scalar_t* __restrict__ input,         // [..., hidden_size]\n+    scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int64_t input_stride,\n     scalar_t* __restrict__ residual,      // [..., hidden_size]\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n@@ -111,7 +117,7 @@ fused_add_rms_norm_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[blockIdx.x * input_stride + idx];\n     z += residual[blockIdx.x * hidden_size + idx];\n     float x = (float)z;\n     variance += x * x;\n@@ -129,7 +135,7 @@ fused_add_rms_norm_kernel(\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n     float x = (float)residual[blockIdx.x * hidden_size + idx];\n-    input[blockIdx.x * hidden_size + idx] =\n+    input[blockIdx.x * input_stride + idx] =\n         ((scalar_t)(x * s_variance)) * weight[idx];\n   }\n }\n@@ -141,11 +147,12 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n               torch::Tensor& weight,  // [hidden_size]\n               double epsilon) {\n   TORCH_CHECK(out.is_contiguous());\n-  TORCH_CHECK(input.is_contiguous());\n+  TORCH_CHECK(input.stride(-1) == 1);\n   TORCH_CHECK(weight.is_contiguous());\n \n   int hidden_size = input.size(-1);\n   int num_tokens = input.numel() / hidden_size;\n+  int64_t input_stride = input.stride(-2);\n \n   dim3 grid(num_tokens);\n   dim3 block(std::min(hidden_size, 1024));\n@@ -153,26 +160,29 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\n     vllm::rms_norm_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        out.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(),\n+        out.data_ptr<scalar_t>(), input.data_ptr<scalar_t>(), input_stride,\n         weight.data_ptr<scalar_t>(), epsilon, num_tokens, hidden_size);\n   });\n }\n \n-#define LAUNCH_FUSED_ADD_RMS_NORM(width)                                       \\\n-  VLLM_DISPATCH_FLOATING_TYPES(                                                \\\n-      input.scalar_type(), \"fused_add_rms_norm_kernel\", [&] {                  \\\n-        vllm::fused_add_rms_norm_kernel<scalar_t, width>                       \\\n-            <<<grid, block, 0, stream>>>(input.data_ptr<scalar_t>(),           \\\n-                                         residual.data_ptr<scalar_t>(),        \\\n-                                         weight.data_ptr<scalar_t>(), epsilon, \\\n-                                         num_tokens, hidden_size);             \\\n+#define LAUNCH_FUSED_ADD_RMS_NORM(width)                                    \\\n+  VLLM_DISPATCH_FLOATING_TYPES(                                             \\\n+      input.scalar_type(), \"fused_add_rms_norm_kernel\", [&] {               \\\n+        vllm::fused_add_rms_norm_kernel<scalar_t, width>                    \\\n+            <<<grid, block, 0, stream>>>(                                   \\\n+                input.data_ptr<scalar_t>(), input_stride,                   \\\n+                residual.data_ptr<scalar_t>(), weight.data_ptr<scalar_t>(), \\\n+                epsilon, num_tokens, hidden_size);                          \\\n       });\n \n void fused_add_rms_norm(torch::Tensor& input,     // [..., hidden_size]\n                         torch::Tensor& residual,  // [..., hidden_size]\n                         torch::Tensor& weight,    // [hidden_size]\n                         double epsilon) {\n+  TORCH_CHECK(residual.is_contiguous());\n+  TORCH_CHECK(weight.is_contiguous());\n   int hidden_size = input.size(-1);\n+  int64_t input_stride = input.stride(-2);\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n@@ -194,9 +204,16 @@ void fused_add_rms_norm(torch::Tensor& input,     // [..., hidden_size]\n   auto inp_ptr = reinterpret_cast<std::uintptr_t>(input.data_ptr());\n   auto res_ptr = reinterpret_cast<std::uintptr_t>(residual.data_ptr());\n   auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());\n-  bool ptrs_are_aligned =\n-      inp_ptr % 16 == 0 && res_ptr % 16 == 0 && wt_ptr % 16 == 0;\n-  if (ptrs_are_aligned && hidden_size % 8 == 0) {\n+  constexpr int vector_width = 8;\n+  constexpr int req_alignment_bytes =\n+      vector_width * 2;  // vector_width * sizeof(bfloat16 or float16) (float32\n+                         // falls back to non-vectorized version anyway)\n+  bool ptrs_are_aligned = inp_ptr % req_alignment_bytes == 0 &&\n+                          res_ptr % req_alignment_bytes == 0 &&\n+                          wt_ptr % req_alignment_bytes == 0;\n+  bool offsets_are_multiple_of_vector_width =\n+      hidden_size % vector_width == 0 && input_stride % vector_width == 0;\n+  if (ptrs_are_aligned && offsets_are_multiple_of_vector_width) {\n     LAUNCH_FUSED_ADD_RMS_NORM(8);\n   } else {\n     LAUNCH_FUSED_ADD_RMS_NORM(0);\ndiff --git a/csrc/layernorm_quant_kernels.cu b/csrc/layernorm_quant_kernels.cu\nindex d595b9e88..0fd5849d9 100644\n--- a/csrc/layernorm_quant_kernels.cu\n+++ b/csrc/layernorm_quant_kernels.cu\n@@ -23,8 +23,9 @@ namespace vllm {\n // TODO(woosuk): Further optimize this kernel.\n template <typename scalar_t, typename fp8_type>\n __global__ void rms_norm_static_fp8_quant_kernel(\n-    fp8_type* __restrict__ out,           // [..., hidden_size]\n-    const scalar_t* __restrict__ input,   // [..., hidden_size]\n+    fp8_type* __restrict__ out,          // [..., hidden_size]\n+    const scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int input_stride,\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float* __restrict__ scale,      // [1]\n     const float epsilon, const int num_tokens, const int hidden_size) {\n@@ -32,7 +33,7 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    const float x = (float)input[blockIdx.x * hidden_size + idx];\n+    const float x = (float)input[blockIdx.x * input_stride + idx];\n     variance += x * x;\n   }\n \n@@ -49,7 +50,7 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n   float const scale_inv = 1.0f / *scale;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    float x = (float)input[blockIdx.x * hidden_size + idx];\n+    float x = (float)input[blockIdx.x * input_stride + idx];\n     float const out_norm = ((scalar_t)(x * s_variance)) * weight[idx];\n     out[blockIdx.x * hidden_size + idx] =\n         scaled_fp8_conversion<true, fp8_type>(out_norm, scale_inv);\n@@ -63,8 +64,9 @@ __global__ void rms_norm_static_fp8_quant_kernel(\n template <typename scalar_t, int width, typename fp8_type>\n __global__ std::enable_if_t<(width > 0) && _typeConvert<scalar_t>::exists>\n fused_add_rms_norm_static_fp8_quant_kernel(\n-    fp8_type* __restrict__ out,           // [..., hidden_size]\n-    scalar_t* __restrict__ input,         // [..., hidden_size]\n+    fp8_type* __restrict__ out,    // [..., hidden_size]\n+    scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int input_stride,\n     scalar_t* __restrict__ residual,      // [..., hidden_size]\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float* __restrict__ scale,      // [1]\n@@ -74,6 +76,7 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   static_assert(sizeof(_f16Vec<scalar_t, width>) == sizeof(scalar_t) * width);\n \n   const int vec_hidden_size = hidden_size / width;\n+  const int vec_input_stride = input_stride / width;\n   __shared__ float s_variance;\n   float variance = 0.0f;\n   /* These and the argument pointers are all declared `restrict` as they are\n@@ -87,8 +90,9 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n       reinterpret_cast<const _f16Vec<scalar_t, width>*>(weight);\n \n   for (int idx = threadIdx.x; idx < vec_hidden_size; idx += blockDim.x) {\n+    int stride_id = blockIdx.x * vec_input_stride + idx;\n     int id = blockIdx.x * vec_hidden_size + idx;\n-    _f16Vec<scalar_t, width> temp = input_v[id];\n+    _f16Vec<scalar_t, width> temp = input_v[stride_id];\n     temp += residual_v[id];\n     variance += temp.sum_squares();\n     residual_v[id] = temp;\n@@ -125,8 +129,9 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n template <typename scalar_t, int width, typename fp8_type>\n __global__ std::enable_if_t<(width == 0) || !_typeConvert<scalar_t>::exists>\n fused_add_rms_norm_static_fp8_quant_kernel(\n-    fp8_type* __restrict__ out,           // [..., hidden_size]\n-    scalar_t* __restrict__ input,         // [..., hidden_size]\n+    fp8_type* __restrict__ out,    // [..., hidden_size]\n+    scalar_t* __restrict__ input,  // [..., hidden_size]\n+    const int input_stride,\n     scalar_t* __restrict__ residual,      // [..., hidden_size]\n     const scalar_t* __restrict__ weight,  // [hidden_size]\n     const float* __restrict__ scale,      // [1]\n@@ -135,7 +140,7 @@ fused_add_rms_norm_static_fp8_quant_kernel(\n   float variance = 0.0f;\n \n   for (int idx = threadIdx.x; idx < hidden_size; idx += blockDim.x) {\n-    scalar_t z = input[blockIdx.x * hidden_size + idx];\n+    scalar_t z = input[blockIdx.x * input_stride + idx];\n     z += residual[blockIdx.x * hidden_size + idx];\n     float x = (float)z;\n     variance += x * x;\n@@ -169,7 +174,9 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n                                torch::Tensor& weight,  // [hidden_size]\n                                torch::Tensor& scale,   // [1]\n                                double epsilon) {\n+  TORCH_CHECK(out.is_contiguous());\n   int hidden_size = input.size(-1);\n+  int input_stride = input.stride(-2);\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n@@ -183,8 +190,9 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n               vllm::rms_norm_static_fp8_quant_kernel<scalar_t, fp8_t>\n                   <<<grid, block, 0, stream>>>(\n                       out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),\n-                      weight.data_ptr<scalar_t>(), scale.data_ptr<float>(),\n-                      epsilon, num_tokens, hidden_size);\n+                      input_stride, weight.data_ptr<scalar_t>(),\n+                      scale.data_ptr<float>(), epsilon, num_tokens,\n+                      hidden_size);\n             });\n       });\n }\n@@ -198,7 +206,7 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n                                                                width, fp8_t> \\\n                   <<<grid, block, 0, stream>>>(                              \\\n                       out.data_ptr<fp8_t>(), input.data_ptr<scalar_t>(),     \\\n-                      residual.data_ptr<scalar_t>(),                         \\\n+                      input_stride, residual.data_ptr<scalar_t>(),           \\\n                       weight.data_ptr<scalar_t>(), scale.data_ptr<float>(),  \\\n                       epsilon, num_tokens, hidden_size);                     \\\n             });                                                              \\\n@@ -210,7 +218,10 @@ void fused_add_rms_norm_static_fp8_quant(\n     torch::Tensor& weight,    // [hidden_size]\n     torch::Tensor& scale,     // [1]\n     double epsilon) {\n+  TORCH_CHECK(out.is_contiguous());\n+  TORCH_CHECK(residual.is_contiguous());\n   int hidden_size = input.size(-1);\n+  int input_stride = input.stride(-2);\n   int num_tokens = input.numel() / hidden_size;\n \n   dim3 grid(num_tokens);\n@@ -234,7 +245,7 @@ void fused_add_rms_norm_static_fp8_quant(\n   auto wt_ptr = reinterpret_cast<std::uintptr_t>(weight.data_ptr());\n   bool ptrs_are_aligned =\n       inp_ptr % 16 == 0 && res_ptr % 16 == 0 && wt_ptr % 16 == 0;\n-  if (ptrs_are_aligned && hidden_size % 8 == 0) {\n+  if (ptrs_are_aligned && hidden_size % 8 == 0 && input_stride % 8 == 0) {\n     LAUNCH_FUSED_ADD_RMS_NORM(8);\n   } else {\n     LAUNCH_FUSED_ADD_RMS_NORM(0);\ndiff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu\nindex f3f9f669e..0e1eab66f 100644\n--- a/csrc/quantization/fp8/common.cu\n+++ b/csrc/quantization/fp8/common.cu\n@@ -88,6 +88,8 @@ void static_scaled_fp8_quant(torch::Tensor& out,          // [..., d]\n                              torch::Tensor const& input,  // [..., d]\n                              torch::Tensor const& scale)  // [1]\n {\n+  TORCH_CHECK(input.is_contiguous());\n+  TORCH_CHECK(out.is_contiguous());\n   int const block_size = 256;\n   int const num_tokens = input.numel() / input.size(-1);\n   int const num_elems = input.numel();\n@@ -111,6 +113,8 @@ void dynamic_scaled_fp8_quant(torch::Tensor& out,          // [..., d]\n                               torch::Tensor const& input,  // [..., d]\n                               torch::Tensor& scale)        // [1]\n {\n+  TORCH_CHECK(input.is_contiguous());\n+  TORCH_CHECK(out.is_contiguous());\n   int const block_size = 256;\n   int const num_tokens = input.numel() / input.size(-1);\n   int const num_elems = input.numel();\ndiff --git a/tests/kernels/core/test_layernorm.py b/tests/kernels/core/test_layernorm.py\nindex 3eac06273..02316ceaa 100644\n--- a/tests/kernels/core/test_layernorm.py\n+++ b/tests/kernels/core/test_layernorm.py\n@@ -26,6 +26,7 @@ CUDA_DEVICES = [\n @pytest.mark.parametrize(\"dtype\", DTYPES)\n @pytest.mark.parametrize(\"seed\", SEEDS)\n @pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n+@pytest.mark.parametrize(\"strided_input\", [False, True])\n @torch.inference_mode()\n def test_rms_norm(\n     num_tokens: int,\n@@ -34,13 +35,17 @@ def test_rms_norm(\n     dtype: torch.dtype,\n     seed: int,\n     device: str,\n+    strided_input: bool,\n ) -> None:\n     current_platform.seed_everything(seed)\n     torch.set_default_device(device)\n     layer = RMSNorm(hidden_size).to(dtype=dtype)\n     layer.weight.data.normal_(mean=1.0, std=0.1)\n     scale = 1 / (2 * hidden_size)\n-    x = torch.randn(num_tokens, hidden_size, dtype=dtype)\n+    last_dim = 2 * hidden_size if strided_input else hidden_size\n+    x = torch.randn(num_tokens, last_dim, dtype=dtype)\n+    x = x[..., :hidden_size]\n+    assert x.is_contiguous() != strided_input\n     x *= scale\n     residual = torch.randn_like(x) * scale if add_residual else None\n \n@@ -72,6 +77,7 @@ def test_rms_norm(\n @pytest.mark.parametrize(\"quant_scale\", [1.0, 0.01, 10.0])\n @pytest.mark.parametrize(\"seed\", SEEDS)\n @pytest.mark.parametrize(\"device\", CUDA_DEVICES)\n+@pytest.mark.parametrize(\"strided_input\", [False, True])\n def test_fused_rms_norm_quant(\n     num_tokens: int,\n     hidden_size: int,\n@@ -80,13 +86,18 @@ def test_fused_rms_norm_quant(\n     quant_scale: float,\n     seed: int,\n     device: str,\n+    strided_input: bool,\n ) -> None:\n     current_platform.seed_everything(seed)\n     torch.set_default_device(device)\n \n     weight = torch.empty(hidden_size, dtype=dtype).normal_(mean=1.0, std=0.1)\n     scale = 1 / (2 * hidden_size)\n-    x = torch.randn(num_tokens, hidden_size, dtype=dtype)\n+    last_dim = 2 * hidden_size if strided_input else hidden_size\n+    x_base = torch.randn(num_tokens, last_dim, dtype=dtype)\n+    x = x_base[..., :hidden_size]\n+    assert x.is_contiguous() != strided_input\n+\n     x *= scale\n     if add_residual:\n         residual = torch.randn_like(x) * scale\n@@ -106,9 +117,11 @@ def test_fused_rms_norm_quant(\n \n         # Unfused kernel is in-place so it goes second\n         # Also use a separate clone of x to avoid modifying the input\n-        x_unfused = x.clone()\n+        x_unfused_base = x_base.clone()\n+        x_unfused = x_unfused_base[..., :hidden_size]\n+        assert x_unfused.is_contiguous() != strided_input\n         torch.ops._C.fused_add_rms_norm(x_unfused, residual, weight, 1e-6)\n-        torch.ops._C.static_scaled_fp8_quant(out_quant, x_unfused,\n+        torch.ops._C.static_scaled_fp8_quant(out_quant, x_unfused.contiguous(),\n                                              quant_scale_t)\n \n         torch.cuda.synchronize()\n@@ -116,7 +129,6 @@ def test_fused_rms_norm_quant(\n                                    residual,\n                                    atol=1e-2,\n                                    rtol=1e-2)\n-\n         opcheck(\n             torch.ops._C.fused_add_rms_norm_static_fp8_quant,\n             (out_quant_fused, x, residual_fused, weight, quant_scale_t, 1e-6))\n@@ -131,7 +143,7 @@ def test_fused_rms_norm_quant(\n         opcheck(torch.ops._C.rms_norm_static_fp8_quant,\n                 (out_quant_fused, x, weight, quant_scale_t, 1e-6))\n \n-    torch.testing.assert_close(out_quant_fused.to(dtype=torch.float32),\n-                               out_quant.to(dtype=torch.float32),\n+    torch.testing.assert_close(out_quant.to(dtype=torch.float32),\n+                               out_quant_fused.to(dtype=torch.float32),\n                                atol=1e-3,\n                                rtol=1e-3)\ndiff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py\nindex 366dfd97d..bb81a663d 100644\n--- a/vllm/model_executor/layers/linear.py\n+++ b/vllm/model_executor/layers/linear.py\n@@ -259,6 +259,8 @@ class LinearBase(torch.nn.Module):\n         if params_dtype is None:\n             params_dtype = torch.get_default_dtype()\n         self.params_dtype = params_dtype\n+        self.quant_config = quant_config\n+        self.prefix = prefix\n         if quant_config is None:\n             self.quant_method: Optional[\n                 QuantizeMethodBase] = UnquantizedLinearMethod()\n@@ -300,6 +302,12 @@ class ReplicatedLinear(LinearBase):\n         *,\n         return_bias: bool = True,\n     ):\n+        # If MergedReplicatedLinear, use output size of each partition.\n+        if hasattr(self, \"output_sizes\"):\n+            self.output_partition_sizes = self.output_sizes\n+        else:\n+            self.output_partition_sizes = [output_size]\n+\n         super().__init__(input_size,\n                          output_size,\n                          skip_bias_add,\n@@ -311,7 +319,8 @@ class ReplicatedLinear(LinearBase):\n         # All the linear layer supports quant method.\n         assert self.quant_method is not None\n         self.quant_method.create_weights(self,\n-                                         self.input_size, [self.output_size],\n+                                         self.input_size,\n+                                         self.output_partition_sizes,\n                                          self.input_size,\n                                          self.output_size,\n                                          self.params_dtype,\n@@ -367,6 +376,73 @@ class ReplicatedLinear(LinearBase):\n         return s\n \n \n+class MergedReplicatedLinear(ReplicatedLinear):\n+    \"\"\"Replicated linear layer.\n+\n+    Args:\n+        input_size: input dimension of the linear layer.\n+        output_size: output dimension of the linear layer.\n+        bias: If true, add bias.\n+        skip_bias_add: If true, skip adding bias but instead return it.\n+        params_dtype: Data type for the parameters.\n+        quant_config: Quantization configure.\n+        prefix: The name of the layer in the state dict, including all parents\n+                        (e.g. model.layers.0.qkv_proj)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        input_size: int,\n+        output_sizes: list[int],\n+        bias: bool = True,\n+        skip_bias_add: bool = False,\n+        params_dtype: Optional[torch.dtype] = None,\n+        quant_config: Optional[QuantizationConfig] = None,\n+        prefix: str = \"\",\n+        *,\n+        return_bias: bool = True,\n+    ):\n+        self.output_sizes = output_sizes\n+        super().__init__(input_size,\n+                         sum(output_sizes),\n+                         bias,\n+                         skip_bias_add,\n+                         params_dtype,\n+                         quant_config,\n+                         prefix=prefix,\n+                         return_bias=return_bias)\n+\n+    def weight_loader(self,\n+                      param: Union[Parameter, BasevLLMParameter],\n+                      loaded_weight: torch.Tensor,\n+                      loaded_shard_id: Optional[int] = None):\n+        assert loaded_shard_id is not None\n+        assert loaded_shard_id < len(self.output_sizes)\n+\n+        if isinstance(param, BlockQuantScaleParameter):\n+            from vllm.model_executor.layers.quantization.fp8 import (\n+                Fp8LinearMethod, Fp8MoEMethod)\n+            assert self.quant_method is not None\n+            assert isinstance(self.quant_method,\n+                              (Fp8LinearMethod, Fp8MoEMethod))\n+            weight_block_size = self.quant_method.quant_config.weight_block_size\n+            assert weight_block_size is not None\n+            block_n, _ = weight_block_size[0], weight_block_size[1]\n+            shard_offset = (\n+                (sum(self.output_sizes[:loaded_shard_id]) + block_n - 1) //\n+                block_n)\n+            shard_size = ((self.output_sizes[loaded_shard_id] + block_n - 1) //\n+                          block_n)\n+        elif isinstance(param, PerTensorScaleParameter):\n+            shard_offset = loaded_shard_id\n+            shard_size = 1\n+        else:\n+            shard_offset = sum(self.output_sizes[:loaded_shard_id])\n+            shard_size = self.output_sizes[loaded_shard_id]\n+\n+        param[shard_offset:shard_offset + shard_size] = loaded_weight\n+\n+\n class ColumnParallelLinear(LinearBase):\n     \"\"\"Linear layer with column parallelism.\n \ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex 35d7545d8..75f8adf34 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -257,9 +257,16 @@ class Fp8LinearMethod(LinearMethodBase):\n                     f\"{input_size_per_partition} is not divisible by \"\n                     f\"weight quantization block_k = {block_k}.\")\n             # Required by column parallel or enabling merged weights\n-            if (tp_size > 1 and output_size // output_size_per_partition\n-                    == tp_size) or len(output_partition_sizes) > 1:\n-                for output_partition_size in output_partition_sizes:\n+            is_tp_split = (tp_size > 1 and\n+                           output_size // output_size_per_partition == tp_size)\n+            is_merged_gemm = len(output_partition_sizes) > 1\n+            if is_tp_split or is_merged_gemm:\n+                sizes_to_check = output_partition_sizes\n+                if not is_tp_split and is_merged_gemm:\n+                    # In case of merged matrices, we allow the last\n+                    # matrix to not be a multiple of block size\n+                    sizes_to_check = output_partition_sizes[:-1]\n+                for output_partition_size in sizes_to_check:\n                     if output_partition_size % block_n != 0:\n                         raise ValueError(\n                             f\"Weight output_partition_size = \"\ndiff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py\nindex 5106b9914..649109777 100644\n--- a/vllm/model_executor/models/deepseek_v2.py\n+++ b/vllm/model_executor/models/deepseek_v2.py\n@@ -42,6 +42,7 @@ from vllm.model_executor.layers.fused_moe import FusedMoE\n from vllm.model_executor.layers.layernorm import RMSNorm\n from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                                MergedColumnParallelLinear,\n+                                               MergedReplicatedLinear,\n                                                ReplicatedLinear,\n                                                RowParallelLinear)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n@@ -336,7 +337,7 @@ class DeepseekV2Attention(nn.Module):\n         kv_a, _ = latent_cache.split(\n             [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n         latent_cache = latent_cache.unsqueeze(1)\n-        kv_a = self.kv_a_layernorm(kv_a.contiguous())\n+        kv_a = self.kv_a_layernorm(kv_a)\n         kv = self.kv_b_proj(kv_a)[0]\n         kv = kv.view(-1, self.num_local_heads,\n                      self.qk_nope_head_dim + self.v_head_dim)\n@@ -407,14 +408,24 @@ class DeepseekV2MLAAttention(nn.Module):\n         self.max_position_embeddings = max_position_embeddings\n \n         if self.q_lora_rank is not None:\n-            self.q_a_proj = ReplicatedLinear(self.hidden_size,\n-                                             self.q_lora_rank,\n-                                             bias=False,\n-                                             quant_config=quant_config,\n-                                             prefix=f\"{prefix}.q_a_proj\")\n+            self.fused_qkv_a_proj = MergedReplicatedLinear(\n+                self.hidden_size,\n+                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim],\n+                bias=False,\n+                quant_config=quant_config,\n+                prefix=f\"{prefix}.fused_qkv_a_proj\")\n+        else:\n+            self.kv_a_proj_with_mqa = ReplicatedLinear(\n+                self.hidden_size,\n+                self.kv_lora_rank + self.qk_rope_head_dim,\n+                bias=False,\n+                quant_config=quant_config,\n+                prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n+\n+        if self.q_lora_rank is not None:\n             self.q_a_layernorm = RMSNorm(self.q_lora_rank,\n                                          eps=config.rms_norm_eps)\n-            self.q_b_proj = ColumnParallelLinear(q_lora_rank,\n+            self.q_b_proj = ColumnParallelLinear(self.q_lora_rank,\n                                                  self.num_heads *\n                                                  self.qk_head_dim,\n                                                  bias=False,\n@@ -427,13 +438,6 @@ class DeepseekV2MLAAttention(nn.Module):\n                                                bias=False,\n                                                quant_config=quant_config,\n                                                prefix=f\"{prefix}.q_proj\")\n-\n-        self.kv_a_proj_with_mqa = ReplicatedLinear(\n-            self.hidden_size,\n-            self.kv_lora_rank + self.qk_rope_head_dim,\n-            bias=False,\n-            quant_config=quant_config,\n-            prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n         self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,\n                                       eps=config.rms_norm_eps)\n         self.kv_b_proj = ColumnParallelLinear(\n@@ -495,15 +499,24 @@ class DeepseekV2MLAAttention(nn.Module):\n         positions: torch.Tensor,\n         hidden_states: torch.Tensor,\n     ) -> torch.Tensor:\n+        q_c = None\n+        kv_lora = None\n+\n         if self.q_lora_rank is not None:\n-            q_c = self.q_a_proj(hidden_states)[0]\n+            qkv_lora = self.fused_qkv_a_proj(hidden_states)[0]\n+            q_c, kv_lora = qkv_lora.split(\n+                [self.q_lora_rank, self.kv_lora_rank + self.qk_rope_head_dim],\n+                dim=-1,\n+            )\n             q_c = self.q_a_layernorm(q_c)\n             q = self.q_b_proj(q_c)[0]\n         else:\n+            kv_lora = self.kv_a_proj_with_mqa(hidden_states)[0]\n             q = self.q_proj(hidden_states)[0]\n-        kv_c, k_pe = self.kv_a_proj_with_mqa(hidden_states)[0].split(\n-            [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n-        kv_c_normed = self.kv_a_layernorm(kv_c.contiguous())\n+\n+        kv_c, k_pe = kv_lora.split([self.kv_lora_rank, self.qk_rope_head_dim],\n+                                   dim=-1)\n+        kv_c_normed = self.kv_a_layernorm(kv_c)\n \n         q = q.view(-1, self.num_local_heads, self.qk_head_dim)\n         # Add head dim of 1 to k_pe\n@@ -837,6 +850,8 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP, MixtureOfExperts):\n             # (param_name, shard_name, shard_id)\n             (\"gate_up_proj\", \"gate_proj\", 0),\n             (\"gate_up_proj\", \"up_proj\", 1),\n+            (\"fused_qkv_a_proj\", \"q_a_proj\", 0),\n+            (\"fused_qkv_a_proj\", \"kv_a_proj_with_mqa\", 1),\n         ]\n \n         # Params for weights, fp8 weight scales, fp8 activation scales\n@@ -871,6 +886,12 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP, MixtureOfExperts):\n                 if ((\"mlp.experts.\" in name) and name not in params_dict):\n                     continue\n                 name = name.replace(weight_name, param_name)\n+\n+                # QKV fusion is optional, fall back to normal\n+                # weight loading if it's not enabled\n+                if ((param_name == \"fused_qkv_a_proj\")\n+                        and name not in params_dict):\n+                    continue\n                 # Skip loading extra bias for GPTQ models.\n                 if name.endswith(\".bias\") and name not in params_dict:\n                     continue",
  "apis": [
    "vllm.model_executor.layers.layernorm.RMSNorm",
    "vllm.model_executor.layers.linear.MergedReplicatedLinear",
    "vllm.model_executor.models.deepseek_v2.DeepseekV2ForCausalLM"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/layernorm.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/deepseek_v2.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes non-trivial changes to CUDA kernel code in several source files. It alters how memory strides (input_stride) are used in layer normalization and quantization kernels, modifying indexing and alignment checks to support strided inputs and vectorized memory accesses. These modifications are performance related as they optimize low-level kernel behavior, improve memory access patterns, and are directly involved in the high-level API performance (layernorm/fused norm operations) without being mere refactors, bug fixes, or documentation edits. Although the commit message mentions \"[perf]\" and \"strided layernorm\", these changes go beyond simple renaming (they modify computations and launching parameters) and directly affect CPU-shot execution and testable GPU kernels in a performance optimization context.",
  "llm_api_reason": "The commit changes several CUDA kernels for RMS normalization and its fused add variant by adding a new input_stride parameter to support nonstandard memory layouts. These backend kernel modifications feed into the Python‚Äêside custom op wrapped in the RMSNorm class. In addition, the fused QKV support has been added in the merged replicated linear layer, with updates to how DeepseekV2‚Äôs attention modules invoke the fused QKV projection. Tests were also enhanced to exercise both contiguous and strided inputs. Overall, this commit affects the Python API for RMSNorm, the merged replicated linear layer, and the deepseek_v2 model that uses these ops."
}