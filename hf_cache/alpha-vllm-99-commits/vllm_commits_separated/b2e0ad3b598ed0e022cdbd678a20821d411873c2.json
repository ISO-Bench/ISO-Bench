{
  "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
  "pr_url": "https://github.com/vllm-project/vllm/pull/10339",
  "pr_date": null,
  "timeline_text": "Copy link Collaborator andoorve commented Nov 14, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Maintaining multiple names here will cause both to be refcounted which increases the peak memory. This will manifest as more blocks on top of each other in the memory profile: This change will increase the number of available blocks as a result of profiling especially with longer context lengths. I will follow up with a more detailed investigation in another PR/Issue that discusses this in more depth. However, creating this PR as well now as this is more or less a well-contained low-risk change. Can add to more models as well once we review this. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions andoorve marked this pull request as ready for review November 14, 2024 18:38 Copy link github-actions bot commented Nov 14, 2024 üëã Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . andoorve requested review from DarkLight1337 and youkaichao November 14, 2024 18:38 [Perf] Reduce peak memory usage ‚Ä¶ 5625ebe Maintaining multiple names here will cause both to be refcounted which increases the peak memory. This will manifest as more blocks on top of each other in the memory profile.\n\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com> andoorve force-pushed the llama-memory branch\n    from 358dd7e to 5625ebe Compare November 14, 2024 18:44 Copy link Member mgoin commented Nov 14, 2024 Great idea! We could apply this to many other models ‚ù§Ô∏è 1 andoorve reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . andoorve requested a review\n  from mgoin November 14, 2024 20:12 mgoin added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Nov 14, 2024 DarkLight1337 enabled auto-merge (squash) November 14, 2024 23:38 DarkLight1337 approved these changes Nov 14, 2024 View reviewed changes youkaichao reviewed Nov 15, 2024 View reviewed changes vllm/model_executor/models/llama.py @@ -90,8 +90,8 @@ def __init__( self.act_fn = SiluAndMul() def forward(self, x): gate_up, _ = self.gate_up_proj(x) Copy link Member youkaichao Nov 15, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think torch.compile can do something similar, without renaming variables. to keep the original semantic, maybe adding del x would be more intuitive. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author andoorve Nov 15, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I think torch.compile can do something similar, without renaming variables. Yes, it can completely alleviate this problem, even when we consider cross-function refcounting which I'll cover in my investigation write-up. to keep the original semantic, maybe adding del x would be more intuitive. I think you might mean in this case del gate_up ? Yes indeed we can add del s and make the variable names more descriptive. I just kept it as x to avoid adding extra del s and be similar to style of the rest of the function. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details DarkLight1337 merged commit b2e0ad3 into vllm-project : main Nov 15, 2024 63 checks passed Uh oh! There was an error while loading. Please reload this page . andoorve deleted the llama-memory branch November 15, 2024 00:56 andoorve mentioned this pull request Nov 20, 2024 [DNM][Discussion] Example to decrease live tensors for activation memory. #10473 Closed sleepwalker2017 pushed a commit\n        to sleepwalker2017/vllm\n      that referenced\n      this pull request Dec 13, 2024 [Perf] Reduce peak memory usage of llama ( vllm-project#10339 ) ‚Ä¶ d26d246 Signed-off-by: andoorve <37849411+andoorve@users.noreply.github.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:46",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: profile, profile, profiling | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:46",
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks hellaswag --num_fewshot 0"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct --dataset-name sharegpt --num-prompts 100",
  "commit_subject": "[Perf] Reduce peak memory usage of llama (#10339)",
  "commit_message": "[Perf] Reduce peak memory usage of llama (#10339)\n\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com>",
  "commit_date": "2024-11-15T00:38:20Z",
  "files_changed": [
    "vllm/model_executor/models/llama.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 4,
    "num_non_test_edited_lines": 4,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..e53631ef1 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,8 +90,8 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n-        x = self.act_fn(gate_up)\n+        x, _ = self.gate_up_proj(x)\n+        x = self.act_fn(x)\n         x, _ = self.down_proj(x)\n         return x",
  "apis": [
    "LlamaModel.forward",
    "LlamaForCausalLM.forward"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/llama.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/serving_models.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The patch modifies the source code in a non-test file (llama.py) and performs a non-trivial change in the data flow of the model's MLP layer. While the commit message hints at reducing peak memory usage‚Äîa performance optimization‚Äîthe actual changes involve rearranging how tensor outputs are assigned and processed. This indicates a subtle change to improve memory usage (and indirectly performance) without adding new features or merely refactoring. The change is related to a core API within the model executor, affecting CPU performance. Thus, the commit satisfies the conditions for being performance related.",
  "llm_api_reason": "The commit modifies the LlamaMLP.forward method by inlining the projection output directly into the activation function call instead of storing it under a different variable name (‚Äúgate_up‚Äù). Although LlamaMLP is an internal helper, its change will affect the overall forward pass of the Llama model. Thus, the high-level inference APIs that users invoke‚Äînamely, the forward methods of LlamaModel and LlamaForCausalLM‚Äîare indirectly affected."
}