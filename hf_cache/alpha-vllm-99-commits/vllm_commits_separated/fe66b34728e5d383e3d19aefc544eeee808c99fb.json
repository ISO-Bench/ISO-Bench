{
  "commit_hash": "fe66b34728e5d383e3d19aefc544eeee808c99fb",
  "pr_url": "https://github.com/vllm-project/vllm/pull/14778",
  "pr_date": "2025-03-14",
  "timeline_text": "Copy link Contributor cyang49 commented Mar 13, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . We found an issue while profiling vLLM running Bamba-9B model inference. Before: As can be seen in the Nsight Systems trace, per Mamba layer there are 2 phases where frequent memory copies happen. They are not necessary, or can be fused to reduce the number of copies. This PR fixes these issues. After: For the test case (offline mode, batch size=64, short prompt) I used, the fix reduces the prefill mamba layer latency from 5ms to 3ms. The results from benchmark_serving on single H100-80GB GPU Before: ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  283.22    \nTotal input tokens:                      215201    \nTotal generated tokens:                  198343    \nRequest throughput (req/s):              3.53      \nOutput token throughput (tok/s):         700.32    \nTotal Token throughput (tok/s):          1460.17   \n---------------Time to First Token----------------\nMean TTFT (ms):                          105627.40 \nMedian TTFT (ms):                        94728.54  \nP99 TTFT (ms):                           264194.77 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          393.83    \nMedian TPOT (ms):                        413.59    \nP99 TPOT (ms):                           615.34    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           339.72    \nMedian ITL (ms):                         589.56    \nP99 ITL (ms):                            751.76    \n================================================== After: python benchmarks/benchmark_serving.py --model $MODEL_PATH  --dataset-name sharegpt     --dataset-path ShareGPT_V3_unfiltered_cleaned_split.json\n\n============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  260.19    \nTotal input tokens:                      215201    \nTotal generated tokens:                  198343    \nRequest throughput (req/s):              3.84      \nOutput token throughput (tok/s):         762.29    \nTotal Token throughput (tok/s):          1589.37   \n---------------Time to First Token----------------\nMean TTFT (ms):                          96566.51  \nMedian TTFT (ms):                        84883.05  \nP99 TTFT (ms):                           245639.66 \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          366.31    \nMedian TPOT (ms):                        371.88    \nP99 TPOT (ms):                           680.49    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           311.69    \nMedian ITL (ms):                         507.96    \nP99 ITL (ms):                            741.83    \n================================================== The total token throughput improved by about 8%. Note: There is another sequential for loop which can be fixed similarly. My test case doesn't hit this control path, though. @fabianlim could you comment? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link github-actions bot commented Mar 13, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cyang49 force-pushed the pr_mamba2_optimizations branch\n      2 times, most recently\n    from c33319f to 7fe5d58 Compare March 13, 2025 19:24 tlrmchlsmth reviewed Mar 13, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Nice performance pickup. Is this the other sequential for loop you mentioned? vllm/vllm/model_executor/layers/mamba/mamba_mixer2.py Lines 470 to 472\n      in 02fcaa3 for idx in mamba_cache_params . state_indices_tensor [ ~ has_initial_states ]: mamba_cache_params . ssm_state [ idx ]. zero_ () Do you want to handle it in this PR? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/model_executor/layers/mamba/mamba_mixer2.py Comment on lines +502 to +510 batched_copy = torch.vmap( lambda idx, source_state: mamba_cache_params.ssm_state[ idx].copy_(source_state)) Copy link Collaborator tlrmchlsmth Mar 13, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This might be handy to have as a method of MambaCacheParams in mamba_cache.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 1 fabianlim reacted with thumbs up emoji All reactions üëç 1 reaction Copy link Contributor Author cyang49 Mar 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @tlrmchlsmth could you clarify if you mean to have this logic as a member function of MambaCacheParams ? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator tlrmchlsmth Mar 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment That's right, that's what I meant, although I don's see a way to factor out commonality between batched_copy and batched_zero_init_func so I'm not sure it would clean anything up. # Note: the lambda capture can happen where ssm_state is initialized\n       #       instead of here Is there some overhead that we should try to avoid here? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author cyang49 Mar 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The current lambda capture code is safe. The comment is just theorizing about removing redundancy. I don't know this part well enough yet. Attempting to \"optimize\" may introduce bugs. I'd leave it as is for now. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author cyang49 commented Mar 13, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Nice performance pickup. Is this the other sequential for loop you mentioned? vllm/vllm/model_executor/layers/mamba/mamba_mixer2.py Lines 470 to 472\n      in 02fcaa3 for idx in mamba_cache_params . state_indices_tensor [ ~ has_initial_states ]: mamba_cache_params . ssm_state [ idx ]. zero_ () Do you want to handle it in this PR? I need @fabianlim 's input on how to hit that case. It can be a separate PR or if I know how to test it tomorrow. Next week I'll be traveling and may not have time to do it All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor fabianlim commented Mar 14, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . since the mamba2 unit tests are not automated maybe we should run them once ? @tlrmchlsmth @cyang49 this will be true if, at least one of the sequences in the current step has an initial state, which is determined by the prescence of a context. This means that either a i) chunked prefill step or ii) decode step will hit this case. has_initial_states = attn_metadata.context_lens_tensor > 0 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 14, 2025 I vectorized the zero init loop and observed a slight improvement in total token throughput ============ Serving Benchmark Result ============\nSuccessful requests:                     1000\nBenchmark duration (s):                  247.81\nTotal input tokens:                      215201\nTotal generated tokens:                  198343\nRequest throughput (req/s):              4.04\nOutput token throughput (tok/s):         800.37\nTotal Token throughput (tok/s):          1668.76\n---------------Time to First Token----------------\nMean TTFT (ms):                          97128.43\nMedian TTFT (ms):                        89290.52\nP99 TTFT (ms):                           233402.22\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          360.62\nMedian TPOT (ms):                        355.58\nP99 TPOT (ms):                           992.48\n---------------Inter-token Latency----------------\nMean ITL (ms):                           294.58\nMedian ITL (ms):                         503.07\nP99 ITL (ms):                            570.28\n================================================== üëç 1 fabianlim reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 14, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @tlrmchlsmth it would be nice if we can merge this one soon, if the functionality & no negative performance impact are verified. I noticed from the trace that there are other inefficiencies in  mamba2, but I'll submit a separate PR after my trip. Let me know if there's anything else that needs changing. Thanks! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth approved these changes Mar 14, 2025 View reviewed changes Copy link Collaborator tlrmchlsmth left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, thanks! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tlrmchlsmth added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Mar 14, 2025 cyang49 added 4 commits March 14, 2025 13:03 vectorize copy loop for speedup ‚Ä¶ e0883a3 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> replace any with torch.any to reduce overhead ‚Ä¶ 81488ad Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> lint ‚Ä¶ 86ca9b5 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> Vectorize zero init of ssm_state ‚Ä¶ 0142ba3 Signed-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> cyang49 force-pushed the pr_mamba2_optimizations branch\n    from 9584558 to 0142ba3 Compare March 14, 2025 17:11 Copy link Member DarkLight1337 commented Mar 14, 2025 Some CI failures have recently been fixed on main, so I suggest you to merge from main if you haven't already üëç 1 cyang49 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details tlrmchlsmth merged commit fe66b34 into vllm-project : main Mar 14, 2025 31 checks passed Uh oh! There was an error while loading. Please reload this page . cyang49 deleted the pr_mamba2_optimizations branch March 14, 2025 21:24 Copy link Contributor yury-tokpanov commented Mar 14, 2025 Testing this. We did notice the same in our profiles of mamba2. Overall, occupancy was pretty low in comparison to flash attention kernels. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . tlrmchlsmth added a commit\n      that referenced\n      this pull request Mar 15, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ‚Ä¶ 9baec50 ‚Ä¶nnecessary Memory Copies ( #14778 )\"\n\nThis reverts commit fe66b34 . yury-tokpanov added a commit\n        to Zyphra/vllm\n      that referenced\n      this pull request Mar 15, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ ‚Ä¶ efb7f02 ‚Ä¶nnecessary Memory Copies ( vllm-project#14778 )\"\n\nThis reverts commit fe66b34 . tlrmchlsmth mentioned this pull request Mar 15, 2025 Revert \"[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of U‚Ä¶ #14848 Merged Copy link Contributor yury-tokpanov commented Mar 15, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . this breaks mamba2 based models, unfortunately: Command I ran on H100: lm_eval --model vllm --model_args pretrained=ibm-ai-platform/Bamba-9B,dtype=float16,gpu_memory_utilization=0.9,max_model_len=4096 --batch_size auto --trust_remote_code --cache_requests true --tasks gsm8k bamba-9b with this PR: Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ‚Üë 0.0781 ¬± 0.0074 strict-match 5 exact_match ‚Üë 0.0569 ¬± 0.0064 bamba-9b with PR reverted: Tasks Version Filter n-shot Metric Value Stderr gsm8k 3 flexible-extract 5 exact_match ‚Üë 0.2449 ¬± 0.0118 strict-match 5 exact_match ‚Üë 0.3692 ¬± 0.0133 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cyang49 restored the pr_mamba2_optimizations branch March 15, 2025 01:47 Copy link Contributor Author cyang49 commented Mar 15, 2025 Weird, it passed when I tested locally? Both value and stderr should be 0s? |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |    0|¬±  |     0|\n|     |       |strict-match    |     5|exact_match|‚Üë  |    0|¬±  |     0| All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor yury-tokpanov commented Mar 15, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Weird, it passed when I tested locally? Both value and stderr should be 0s? |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |    0|¬±  |     0|\n|     |       |strict-match    |     5|exact_match|‚Üë  |    0|¬±  |     0| No, it shouldn't be 0 accuracy. 0 means the model failed completely on a test. For the full gsm8k eval Bamba-9b should be around 37% on a strict-match accuracy (with around 1% stderr). I checked other mamba2 models (Codestral-7B, Zamba2), they are also down. Do you have Slack? I'd suggest you join vLLM dev Slack, we have a channel there to discuss hybrid models: https://slack.vllm.ai/ üëÄ 1 cyang49 reacted with eyes emoji All reactions üëÄ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 15, 2025 Weird, it passed when I tested locally? Both value and stderr should be 0s? |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|‚Üë  |    0|¬±  |     0|\n|     |       |strict-match    |     5|exact_match|‚Üë  |    0|¬±  |     0| No, it shouldn't be 9 accuracy. 0 means the model failed completely on a test. For the full gsm8k eval Bamba-9b should be around 37% on a strict-match accuracy (with around 1% stderr). I checked other mamba2 models (Codestral-7B, Zamba2), they are also down. Do you have Slack? I'd suggest you join vLLM dev Slack, we have a channel there to discuss hybrid models: https://slack.vllm.ai/ Ah, thanks for explaining. I'll debug it when I get a chance. I'll also get on the vllm slack üëç 1 yury-tokpanov reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor yury-tokpanov commented Mar 15, 2025 I think the issue is with ssm state copy, zero-initialization appears to be working fine. üëç 1 cyang49 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 15, 2025 I think the issue is with ssm state copy, zero-initialization appears to be working fine. It could also be that lm-eval doesn't go through the zero-init path, though All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author cyang49 commented Mar 15, 2025 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . It appears the problem is that the semantics of the line mamba_cache_params.ssm_state[idx].copy_(varlen_state[i]) in the for loop is different from mamba_cache_params.ssm_state[idx].copy_(source_state) in the lambda function :( In the former, idx is a scalar integer value and the in-place copy happens, but in the latter, idx is an integer tensor and the indexing semantics is different. I suspect that the in-place copy doesn't happen as expected - I experimented with these two cases in the python interpreter.. It looks like the in-place zero_() part should  have the same issue. Not sure why it didn't cause a problem for gsm8k All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . cyang49 mentioned this pull request Mar 15, 2025 [Model] RE: Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies #14857 Merged Copy link Contributor fabianlim commented Mar 15, 2025 @cyang49 when idx is a tensor is a copy-view, so thats why the inplace does not update the master copy. That is why i needed to loop it with a scalar in the first place. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . lulmer pushed a commit\n        to lulmer/vllm\n      that referenced\n      this pull request Apr 7, 2025 [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessa‚Ä¶ ‚Ä¶ b5a740f ‚Ä¶ry Memory Copies ( vllm-project#14778 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Louis Ulmer <ulmerlouis@gmail.com> ckhordiasma mentioned this pull request Apr 17, 2025 [do not merge] pr test for nm changes into 2.20 red-hat-data-services/vllm#107 Closed shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessa‚Ä¶ ‚Ä¶ 7f3f2fc ‚Ä¶ry Memory Copies ( vllm-project#14778 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessa‚Ä¶ ‚Ä¶ fa2cba1 ‚Ä¶ry Memory Copies ( vllm-project#14778 )\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:55",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm-eval, gsm8k | PERF: TTFT, TTFT, TTFT | SERVING: Serving, Serving, Serving | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:51:55",
  "models": [
    "ibm-ai-platform/Bamba-9B"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=ibm-ai-platform/Bamba-9B,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model ibm-ai-platform/Bamba-9B --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14778)",
  "commit_message": "[Model] Mamba2 Prefill Performance Tweaks: Fixing Flurry of Unnecessary Memory Copies  (#14778)\n\nSigned-off-by: Chih-Chieh-Yang <7364402+cyang49@users.noreply.github.com>",
  "commit_date": "2025-03-14T16:36:18-04:00",
  "files_changed": [
    "vllm/model_executor/layers/mamba/mamba_mixer2.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 30,
    "num_non_test_edited_lines": 30,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex b53a540ed..5b19e3f35 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -466,10 +466,17 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-            if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+\n+            if has_initial_states is not None and torch.any(\n+                    has_initial_states):\n+\n+                # vectorized ssm_state zero init\n+                batched_zero_init_func = torch.vmap(\n+                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())\n+                batched_zero_init_func(\n+                    mamba_cache_params.\n+                    state_indices_tensor[~has_initial_states].unsqueeze(\n+                        dim=-1), )\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -493,10 +500,17 @@ class MambaMixer2(CustomOp):\n                 dt_limit=(0.0, float(\"inf\")),\n             )\n \n-            # update ssm states\n-            # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            # vectorized ssm state update using vmap\n+            # the 1d state_indices_tensor needs to be unsqueezed to avoid vmap\n+            # limitation which doesn't allow use of `item()`\n+            # Note: the lambda capture can happen where ssm_state is initialized\n+            #       instead of here\n+            batched_copy = torch.vmap(\n+                lambda idx, source_state: mamba_cache_params.ssm_state[\n+                    idx].copy_(source_state))\n+            batched_copy(\n+                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),\n+                varlen_state)\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)",
  "apis": [
    "MambaMixer2.forward_cuda",
    "mamba_mixer2"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/mamba/mamba_mixer2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba2.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/mamba_cache.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a core file (mamba_mixer2.py) in a non-test location where vectorized operations (using torch.vmap) are introduced. This change replaces loop-based memory copy operations in two places (zero initializations and state updates) with batched vectorized operations. Its intent is to reduce unnecessary memory copies, hence optimizing performance for prefill operations. The adjustments are non-trivial modifications to source code (not just documentation or refactoring) and are aimed at improving CPU performance without involving GPU-specific changes. Therefore, this commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "The commit replaces two explicit Python loops in the prefill branch of the forward_cuda method of the MambaMixer2 custom op with vectorized versions using torch.vmap. This improves the performance by avoiding per-index iteration when zeroing and copying the ssm_state from the mamba_cache_params. Since the change is in the forward_cuda implementation of the MambaMixer2 custom op (and indirectly in its registered function mamba_mixer2), the affected high‚Äêlevel APIs are those that invoke these operations during model inference in Mamba2."
}