{
  "commit_hash": "ec3b5ce9ccb4262194a16a8b1c31ffd6b3b824b9",
  "pr_url": "https://github.com/vllm-project/vllm/pull/1338",
  "pr_date": "2023-10-13",
  "timeline_text": "Copy link Collaborator Yard1 commented Oct 13, 2023 Two main changes: if we are using a fast tokenizer, we do not enter the slow _convert_tokens_to_string_with_added_encoders loop as the fast tokenizers do not use it in base transformers Use cached properties for added_tokens_encoder and all_special_tokens . Those 2 changes improved detokenization speed for 4096 tokens from 13ms to 2ms. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ❤️ 1 WoosukKwon reacted with heart emoji All reactions ❤️ 1 reaction Improve detokenization performance 09e8491 Copy link Collaborator Author Yard1 commented Oct 13, 2023 cc @WoosukKwon @zhuohan123 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon self-requested a review October 13, 2023 16:43 WoosukKwon approved these changes Oct 13, 2023 View reviewed changes Copy link Collaborator WoosukKwon left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 LGTM! Thanks for the contribution! This resolves the performance degradation after upgrading tokenizers. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/transformers_utils/tokenizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . WoosukKwon merged commit ec3b5ce into vllm-project : main Oct 13, 2023 Yard1 deleted the use_fast_tokenizer branch October 13, 2023 17:09 hongxiayang pushed a commit\n        to hongxiayang/vllm\n      that referenced\n      this pull request Feb 13, 2024 Improve detokenization performance ( vllm-project#1338 ) 69ae127 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:32",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NONE",
  "analysis_extracted_at": "2025-09-07 17:49:32",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Improve detokenization performance (#1338)",
  "commit_message": "Improve detokenization performance (#1338)",
  "commit_date": "2023-10-13T09:59:07-07:00",
  "files_changed": [
    "vllm/transformers_utils/tokenizer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 2,
    "num_edited_lines": 7,
    "num_non_test_edited_lines": 7,
    "commit_year": 2023
  },
  "diff_text": "diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex 6dafdac96..49e7007ae 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -81,10 +81,11 @@ def _convert_tokens_to_string_with_added_encoders(\n     # even when the loop body is very simple.\n     sub_texts = []\n     current_sub_text = []\n+    all_special_tokens = set(tokenizer.all_special_tokens)\n     for token in output_tokens:\n-        if skip_special_tokens and token in tokenizer.all_special_tokens:\n+        if skip_special_tokens and token in all_special_tokens:\n             continue\n-        if token in tokenizer.added_tokens_encoder:\n+        if token in tokenizer.get_added_vocab():\n             if current_sub_text:\n                 sub_text = tokenizer.convert_tokens_to_string(current_sub_text)\n                 sub_texts.append(sub_text)\n@@ -129,7 +130,7 @@ def detokenize_incrementally(\n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the\n     # surrounding ids.\n-    if not getattr(tokenizer, \"added_tokens_encoder\", {}):\n+    if tokenizer.is_fast or not tokenizer.get_added_vocab():\n         prefix_text = tokenizer.convert_tokens_to_string(\n             output_tokens[prefix_offset:read_offset])\n         new_text = tokenizer.convert_tokens_to_string(",
  "apis": [
    "vllm.transformers_utils.tokenizer.get_tokenizer",
    "vllm.transformers_utils.detokenizer_utils.detokenize_incrementally"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/detokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/detokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer_base.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file (tokenizer.py) in a non-trivial way by revising how special tokens are checked and how the added vocabulary is used, with an intention to improve the performance of tokenization/detokenization. The changes are not just cosmetic or refactoring; they adjust the logic to potentially optimize the performance of detokenization on the CPU. The commit message \"Improve detokenization performance\" aligns with the performance optimization goal, and the modifications are applied to a high-level API for tokenization which can impact overall efficiency. Therefore, this commit satisfies the conditions for being performance or optimization related.",
  "llm_api_reason": "This commit makes performance improvements to the detokenization process. It refactors how special tokens are checked during conversion by precomputing the set of all special tokens and by replacing direct attribute access of added token mappings (i.e. tokenizer.added_tokens_encoder) with a method call (i.e. tokenizer.get_added_vocab()). In addition, an extra condition is now used (using tokenizer.is_fast) to decide which branch of the detokenization logic to use. These changes affect the internal incremental detokenization routines that higher‐level APIs rely upon for converting model output tokens into text."
}