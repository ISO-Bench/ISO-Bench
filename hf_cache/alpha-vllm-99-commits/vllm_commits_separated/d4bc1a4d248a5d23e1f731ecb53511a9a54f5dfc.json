{
  "commit_hash": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
  "pr_url": "No PR found",
  "pr_date": null,
  "timeline_text": "",
  "timeline_extracted_at": null,
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NO_PR",
  "analysis_extracted_at": "",
  "models": [
    "facebook/opt-125m",
    "facebook/opt-350m",
    "facebook/opt-1.3b",
    "facebook/opt-2.7b",
    "facebook/opt-6.7b"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=facebook/opt-125m --tasks lambada_openai --batch_size 32"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model facebook/opt-125m --num-prompts 100",
  "commit_subject": "Add unoptimized OPT Attention",
  "commit_message": "Add unoptimized OPT Attention",
  "commit_date": "2023-02-23T09:31:55Z",
  "files_changed": [
    "cacheflow/models/attention.py",
    "cacheflow/models/opt.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 9,
    "num_edited_lines": 191,
    "num_non_test_edited_lines": 191,
    "commit_year": 2023
  },
  "diff_text": "diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nnew file mode 100644\nindex 000000000..c36f06c79\n--- /dev/null\n+++ b/cacheflow/models/attention.py\n@@ -0,0 +1,118 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+import torch.nn as nn\n+import xformers.ops as xops\n+\n+from cacheflow import ops\n+from cacheflow.models import InputMetadata\n+\n+\n+class OPTCacheFlowAttention(nn.Module):\n+\n+    def __init__(self, scale: float) -> None:\n+        super().__init__()\n+        self.scale = scale\n+\n+        # Shape-agnostic attention mask.\n+        self.attention_mask = xops.LowerTriangularMask()\n+\n+    def multi_query_kv_attention(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+    ) -> None:\n+        out = xops.memory_efficient_attention(\n+            query, key, value, attn_bias=self.attention_mask, scale=self.scale)\n+        # FIXME(woosuk): Directly write the attention output.\n+        output.copy_(out, non_blocking=True)\n+\n+    def single_query_cached_kv_attention(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        input_metadata: InputMetadata,\n+    ) -> None:\n+        num_heads = value_cache.shape[1]\n+        head_size = value_cache.shape[3]\n+        block_size = value_cache.shape[2]\n+        block_tables = input_metadata.block_tables\n+\n+        # FIXME(woosuk): Replace the following with a custom op.\n+        for i in range(input_metadata.num_generation_tokens):\n+            q = query[i]\n+            block_table = block_tables[i]\n+            context_len = int(input_metadata.context_lens[i])\n+            keys = []\n+            for j in range(context_len):\n+                block_number = block_table[j // block_size]\n+                block_offset = j % block_size\n+                k = key_cache[block_number, :, :, block_offset, :]\n+                k = k.view(num_heads, head_size)\n+                keys.append(k)\n+            keys = torch.stack(keys, dim=-1)\n+            logits = q @ keys\n+            attention_weights = torch.softmax(logits, dim=-1)\n+\n+            values = []\n+            for j in range(context_len):\n+                block_number = block_table[j // block_size]\n+                block_offset = j % block_size\n+                v = value_cache[block_number, :, block_offset, :]\n+                values.append(v)\n+            values = torch.stack(values, dim=-1)\n+            out = attention_weights @ values\n+            output[i].copy_(out, non_blocking=True)\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n+        # Reshape the input tensors.\n+        num_heads = value_cache.shape[1]\n+        head_size = value_cache.shape[3]\n+        query = query.view(-1, num_heads, head_size)\n+        key = key.view(-1, num_heads, head_size)\n+        value = value.view(-1, num_heads, head_size)\n+\n+        # Compute the attention op for prompts.\n+        output = torch.empty_like(query)\n+        start_idx = 0\n+        for i in range(input_metadata.num_prompts):\n+            prompt_len = input_metadata.prompt_lens[i]\n+            out = output[start_idx:start_idx + prompt_len]\n+            q = query[start_idx:start_idx + prompt_len]\n+            k = key[start_idx:start_idx + prompt_len]\n+            v = value[start_idx:start_idx + prompt_len]\n+            self.multi_query_kv_attention(out, q, k, v)\n+            start_idx += prompt_len\n+\n+        # Wait until the cache op is done.\n+        if cache_event is not None:\n+            cache_event.wait()\n+\n+        # Reshape the keys and values and store them in the cache.\n+        ops.reshape_and_cache(\n+            key, value, key_cache, value_cache, input_metadata.slot_mapping)\n+\n+        if input_metadata.num_generation_tokens > 0:\n+            # Compute the attention op for generation tokens.\n+            self.single_query_cached_kv_attention(\n+                output[start_idx:],\n+                query[start_idx:],\n+                key_cache,\n+                value_cache,\n+                input_metadata)\n+\n+        # Reshape the output tensor.\n+        return output.view(-1, num_heads * head_size)\ndiff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py\nindex 3a340317a..234ab2635 100644\n--- a/cacheflow/models/opt.py\n+++ b/cacheflow/models/opt.py\n@@ -1,9 +1,17 @@\n \"\"\"1D OPT model compatible with HuggingFace weights.\"\"\"\n+from typing import Dict, List, Optional, Tuple\n+\n import torch\n from torch import nn\n from transformers import OPTConfig\n from transformers import PreTrainedModel\n \n+from cacheflow.models import InputMetadata\n+from cacheflow.models.attention import OPTCacheFlowAttention\n+from cacheflow.models.sample import Sampler\n+\n+KVCache = Tuple[torch.Tensor, torch.Tensor]\n+\n \n class OPTLearnedPositionalEmbedding(nn.Embedding):\n \n@@ -31,17 +39,27 @@ class OPTAttention(nn.Module):\n         self.head_dim = embed_dim // num_heads\n         self.scaling = self.head_dim**-0.5\n \n+        # TODO(woosuk): Fuse the three linear layers into one QKV linear layer.\n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        q = self.q_proj(hidden_states) * self.scaling\n+        self.attn = OPTCacheFlowAttention(scale=self.scaling)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        kv_cache: KVCache,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n+        q = self.q_proj(hidden_states)\n         k = self.k_proj(hidden_states)\n         v = self.v_proj(hidden_states)\n-        # TODO\n-        attn_output = None\n+        key_cache, value_cache = kv_cache\n+        attn_output = self.attn(\n+            q, k, v, key_cache, value_cache, input_metadata, cache_event)\n         output = self.out_proj(attn_output)\n         return output\n \n@@ -66,13 +84,23 @@ class OPTDecoderLayer(nn.Module):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        kv_cache: KVCache,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n         # Self Attention\n         residual = hidden_states\n         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n         if self.do_layer_norm_before:\n             hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states = self.self_attn(hidden_states=hidden_states)\n+        hidden_states = self.self_attn(\n+            hidden_states=hidden_states,\n+            kv_cache=kv_cache,\n+            input_metadata=input_metadata,\n+            cache_event=cache_event)\n         hidden_states = residual + hidden_states\n         # 350m applies layer norm AFTER attention\n         if not self.do_layer_norm_before:\n@@ -145,6 +173,9 @@ class OPTDecoder(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n     ) -> torch.Tensor:\n         inputs_embeds = self.embed_tokens(input_ids)\n         pos_embeds = self.embed_positions(positions)\n@@ -153,8 +184,14 @@ class OPTDecoder(OPTPreTrainedModel):\n             inputs_embeds = self.project_in(inputs_embeds)\n         hidden_states = inputs_embeds + pos_embeds\n \n-        for layer in self.layers:\n-            hidden_states = layer(hidden_states)\n+        for i in range(len(self.layers)):\n+            if cache_events is None:\n+                cache_event = None\n+            else:\n+                cache_event = cache_events[i]\n+            layer = self.layers[i]\n+            hidden_states = layer(\n+                hidden_states, kv_caches[i], input_metadata, cache_event)\n \n         if self.final_layer_norm is not None:\n             hidden_states = self.final_layer_norm(hidden_states)\n@@ -175,8 +212,12 @@ class OPTModel(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n     ) -> torch.Tensor:\n-        return self.decoder(input_ids, positions)\n+        return self.decoder(\n+            input_ids, positions, kv_caches, input_metadata, cache_events)\n \n \n class OPTForCausalLM(OPTPreTrainedModel):\n@@ -185,9 +226,9 @@ class OPTForCausalLM(OPTPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = OPTModel(config)\n-\n         # the lm_head weight is automatically tied to the embed tokens weight\n         self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n+        self.sampler = Sampler(embedding=self.lm_head.weight)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -196,7 +237,11 @@ class OPTForCausalLM(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n-    ) -> torch.Tensor:\n-        hidden_states = self.model.decoder(input_ids, positions)\n-        logits = self.lm_head(hidden_states).contiguous()\n-        return logits\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n+    ) -> Dict[int, Tuple[int, int]]:\n+        hidden_states = self.model(\n+            input_ids, positions, kv_caches, input_metadata, cache_events)\n+        next_tokens = self.sampler(hidden_states, input_metadata)\n+        return next_tokens",
  "apis": [
    "OPTCacheFlowAttention.forward",
    "OPTAttention.forward",
    "OPTDecoderLayer.forward",
    "OPTDecoder.forward",
    "OPTForCausalLM.forward"
  ],
  "affected_paths": [],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit adds a new OPTCacheFlowAttention module that leverages xformers' memory_efficient_attention and implements a cached key/value mechanism for generation. It integrates caching into the top-level OPT attention process by modifying high-level APIs in the model (e.g., updating the forward methods in OPTAttention, OPTDecoderLayer, OPTDecoder, and OPTForCausalLM) so that the attention calculations can benefit from more efficient memory usage. Despite the commit message reading “Add unoptimized OPT Attention”, the changes aim to affect execution time and memory performance by utilizing specialized operations and restructured data flows. The modifications are non-trivial, affect non-test source files, and target performance improvements on CPU (via more efficient attention operations and caching), rather than just bug fixes or refactoring.</REASON>\n[ANSWER]YES[/ANSWER]",
  "llm_api_reason": "This commit introduces a new attention module class (OPTCacheFlowAttention) and integrates it into several higher-level OPT model components. Specifically, it adds the new OPTCacheFlowAttention class with its forward (and helper) methods, then updates the forward methods of OPTAttention, OPTDecoderLayer, OPTDecoder, and OPTForCausalLM to incorporate caching and new attention behavior. These methods are the primary Python APIs affected by the changes."
}