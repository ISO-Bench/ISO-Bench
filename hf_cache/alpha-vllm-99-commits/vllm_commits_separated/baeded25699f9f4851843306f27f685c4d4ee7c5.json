{
  "commit_hash": "baeded25699f9f4851843306f27f685c4d4ee7c5",
  "pr_url": "https://github.com/vllm-project/vllm/pull/12601",
  "pr_date": "2025-02-01",
  "timeline_text": "Copy link Collaborator LucasWilkinson commented Jan 31, 2025 Based off of: #12528 that needs to land first Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸŽ‰ 7 robertgshaw2-redhat, ywang96, gaocegege, mgoin, tlrmchlsmth, houseroad, and jovany-wang reacted with hooray emoji All reactions ðŸŽ‰ 7 reactions LucasWilkinson and others added 21 commits January 30, 2025 16:57 squashed commits â€¦ 27ad92c Co-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> fix VLLM_MLA_PERFORM_MATRIX_ABSORPTION=0 â€¦ c34e5ca Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> more cleanups â€¦ f2cac91 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Update utils.py â€¦ 068e672 Co-authored-by: Michael Goin <mgoin64@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Update vllm/attention/backends/mla/utils.py â€¦ 31b802c Co-authored-by: Michael Goin <mgoin64@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> review comments â€¦ 634eee6 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> renaming for consistency â€¦ 7487429 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Update vllm/config.py â€¦ d27826d Co-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> review comments â€¦ 8bdc14a Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> review comments â€¦ 09d814c Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Update vllm/attention/backends/mla/utils.py â€¦ 4a46014 Co-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> disable MLA for v3 for now â€¦ 0881475 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> fix failing test â€¦ 37e39f4 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> fix mypy â€¦ cfb2d26 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> fix mypy â€¦ 5afc1bf Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> add cuda graph support â€¦ 54ba87d Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> ci fix â€¦ 31c34bf Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Revert \"add cuda graph support\" â€¦ 433322b Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> Fix TP > 1 cuda graphs â€¦ f2b2500 Co-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> cleanup â€¦ 2d61054 Co-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> cleanup â€¦ 645622c Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> LucasWilkinson requested review from tlrmchlsmth , WoosukKwon , mgoin , robertgshaw2-redhat , zhuohan123 , youkaichao , alexm-redhat , comaniac and njhill as code owners January 31, 2025 04:18 35 hidden items Load moreâ€¦ mgoin approved these changes Feb 1, 2025 View reviewed changes vllm/model_executor/model_loader/loader.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . simon-mo and others added 2 commits February 1, 2025 00:56 Update loader.py â€¦ 0d66687 Co-authored-by: Michael Goin <mgoin64@gmail.com>\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> format â€¦ 5fe1d1d Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> LucasWilkinson force-pushed the mla-fp8 branch\n    from 282eec1 to 5fe1d1d Compare February 1, 2025 00:57 LucasWilkinson added 2 commits February 1, 2025 01:13 reduce split kv amount â€¦ 5d5071c Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> fix none type error â€¦ 7ac6f52 Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> mgoin mentioned this pull request Feb 1, 2025 Disable chunked prefill and/or prefix caching when MLA is enabled #12638 Closed ci fix â€¦ dc0e2af Signed-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com> LucasWilkinson mentioned this pull request Feb 1, 2025 [Attention] MLA with chunked prefill #12639 Merged 4 tasks Hide details View details simon-mo merged commit baeded2 into vllm-project : main Feb 1, 2025 42 of 44 checks passed Uh oh! There was an error while loading. Please reload this page . Isotr0py pushed a commit\n        to Isotr0py/vllm\n      that referenced\n      this pull request Feb 2, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ c22f65d )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights\n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nSigned-off-by: Isotr0py <2037008807@qq.com> srikanthsrnvs pushed a commit\n        to srikanthsrnvs/vllm\n      that referenced\n      this pull request Feb 3, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ bb94260 )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights\n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>\nSigned-off-by: Srikanth Srinivas <srikanth@astrum.ai> Syst3m1cAn0maly mentioned this pull request Feb 3, 2025 [Bug]: MLA Warnings when using FP8 KV cache in v0.7.1 #12680 Closed 1 task sahelib25 pushed a commit\n        to krai/vllm\n      that referenced\n      this pull request Feb 3, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ 06f14ab )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights\n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> xuechendi referenced\n      this pull request\n        in yangw1234/habana-vllm-fork Feb 3, 2025 [Attention] Deepseek v3 MLA support with FP8 compute (#12601) â€¦ baf04c8 This PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights\n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> houseroad mentioned this pull request Feb 4, 2025 DeepSeek: MLA attention pytorch/pytorch#146330 Open NickLucche pushed a commit\n        to NickLucche/vllm\n      that referenced\n      this pull request Feb 7, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ 6bb84bb )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> GWS0428 pushed a commit\n        to GWS0428/VARserve\n      that referenced\n      this pull request Feb 12, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ bd83b50 )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> gshtras reviewed Feb 14, 2025 View reviewed changes vllm/attention/backends/mla/utils.py def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \\ Tuple[Tuple[int, int], Tuple[int, int]]: if isinstance(layer.quant_method, Fp8LinearMethod): if layer.quant_method.block_quant is not None: Copy link Collaborator gshtras Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Fp8LinearMethod.block_quant is a boolean, is there meant to be a check for False instead? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member mgoin Feb 14, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Yes this is a bug, I fixed it here #13181 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions LucasWilkinson mentioned this pull request Feb 25, 2025 Implement MLA for deepseek v3/r1 #12597 Closed yangulei pushed a commit\n        to yangulei/vllm-fork\n      that referenced\n      this pull request Mar 11, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ b339458 )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights\n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> shreyankg pushed a commit\n        to shreyankg/vllm\n      that referenced\n      this pull request May 3, 2025 [Attention] Deepseek v3 MLA support with FP8 compute ( vllm-project#12601 â€¦ 28320d1 )\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:46:44",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "TEST: test, ci, ci",
  "analysis_extracted_at": "2025-09-07 17:46:44",
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=deepseek-ai/DeepSeek-V3,dtype=float16 --tasks hellaswag --num_fewshot 0"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3 --dtype float16",
  "commit_subject": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)",
  "commit_message": "[Attention] Deepseek v3 MLA support with FP8 compute (#12601)\n\nThis PR implements the Deepseek V3 support by performing matrix absorption the fp8 weights \n\n---------\n\nSigned-off-by: Lucas Wilkinson <lwilkinson@neuralmagic.com>\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nCo-authored-by: simon-mo <simon.mo@hey.com>\nCo-authored-by: Michael Goin <mgoin64@gmail.com>\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>\nCo-authored-by: Tyler Michael Smith <tysmith@redhat.com>\nCo-authored-by: Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>",
  "commit_date": "2025-01-31T21:52:51-08:00",
  "files_changed": [
    "vllm/attention/backends/mla/utils.py",
    "vllm/attention/backends/triton_mla.py",
    "vllm/attention/layer.py",
    "vllm/config.py",
    "vllm/envs.py",
    "vllm/model_executor/layers/quantization/utils/fp8_utils.py",
    "vllm/model_executor/layers/quantization/utils/quant_utils.py",
    "vllm/model_executor/model_loader/loader.py",
    "vllm/model_executor/models/deepseek_v3.py",
    "vllm/worker/cache_engine.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 10,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 10,
    "num_hunks": 37,
    "num_edited_lines": 665,
    "num_non_test_edited_lines": 665,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/attention/backends/mla/utils.py b/vllm/attention/backends/mla/utils.py\nindex c6c8a6034..e8fec234c 100644\n--- a/vllm/attention/backends/mla/utils.py\n+++ b/vllm/attention/backends/mla/utils.py\n@@ -1,17 +1,29 @@\n from abc import abstractmethod\n from dataclasses import dataclass\n-from typing import Any, Dict, Generic, List, Optional\n+from typing import Any, Dict, Generic, List, Optional, Tuple\n \n import torch\n+from compressed_tensors.quantization import QuantizationStrategy\n \n from vllm import _custom_ops as ops\n from vllm import envs\n from vllm.attention.backends.abstract import (AttentionLayer,\n                                               AttentionMetadata,\n                                               MLAAttentionImpl, T)\n-from vllm.distributed import get_tensor_model_parallel_world_size\n+from vllm.distributed import (get_tensor_model_parallel_world_size,\n+                              tensor_model_parallel_all_reduce)\n from vllm.model_executor.layers.linear import (ColumnParallelLinear,\n-                                               RowParallelLinear)\n+                                               LinearBase, RowParallelLinear,\n+                                               UnquantizedLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501\n+    CompressedTensorsLinearMethod)\n+from vllm.model_executor.layers.quantization.compressed_tensors.schemes import (\n+    CompressedTensorsW8A8Fp8)\n+from vllm.model_executor.layers.quantization.fp8 import Fp8LinearMethod\n+from vllm.model_executor.layers.quantization.utils.fp8_utils import (\n+    apply_fp8_linear_generic, current_platform_fp8_dtype, is_fp8)\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    scaled_dequantize, scaled_quantize)\n from vllm.model_executor.layers.rotary_embedding import RotaryEmbedding\n from vllm.vllm_flash_attn import flash_attn_varlen_func\n \n@@ -25,11 +37,11 @@ class MLACommonMetadata(AttentionMetadata):\n \n class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     \"\"\"\n-    Common class for implementing repeated parts \n-    \n+    Common class for implementing repeated parts\n+\n     Main reference: DeepseekV2 paper, and FlashInfer Implementation\n     (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n-    \n+\n     Deepseek's MLA attention works the following way:\n     * Use a single latent vector to represent the entire KV cache.\n     * The attention \"simulates\" a multi-head attention, while the compute is\n@@ -46,7 +58,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         * V: V head dim.\n         * kv_c: latent/compressed KV\n         * q_c: latent/compressed Q\n-        \n+\n         #\n         # Outside the MLA attention backend\n         #\n@@ -55,21 +67,21 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n            kv_c_k_pe (B, Lkv+R).\n         2. The kv_c_k_pe is split into kv_c (B, Lkv) and k_pe (B, R). cq\n            and kv_c are normalized.\n-        \n+\n         #\n         # Inside the MLA attention backend\n         #\n \n         * if prefill:\n-        \n-        3. The q_c is then projected up into the multi-head version. \n-           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope \n-             (B, N, P) and q_pe (B, N, R). \n+\n+        3. The q_c is then projected up into the multi-head version.\n+           * q_c goes from (B, Lq) to (B, N, (P+R)), which is split into q_nope\n+             (B, N, P) and q_pe (B, N, R).\n         4. q_pe, k_pe are then passed through rotary embeddings.\n         5. kv_c and k_pe are concatenated and inserted into the cache\n-        6. The kv_c is then projected up into the multi-head version. \n-           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope \n-             dimensions for K and V, which is split into k_nope (B, N, P) \n+        6. The kv_c is then projected up into the multi-head version.\n+           * kv_c goes from (B, Lkv) to (B, N, (P+V)) which has the nope\n+             dimensions for K and V, which is split into k_nope (B, N, P)\n              and v (B, N, V).\n         7. q (B, N, (P+R)) and k (B, N, (P+R)) matrices are assembled from\n            q_nope, q_pe, k_nope, k_pe.\n@@ -112,7 +124,7 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n     From @tsu-bin's calculation, we only want to use the absorption technique\n     for decode. The prefill algorithm should still use the up-projected MHA\n     for less flops and memory usage.\n-    \n+\n     \"\"\"\n \n     def __init__(\n@@ -162,8 +174,19 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _v_up_proj_and_o_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n-            return self.o_proj_absorbed(\n-                x.reshape(-1, self.num_heads * self.kv_lora_rank))[0]\n+            if is_fp8(self.W_UV_O):\n+                output_parallel = apply_fp8_linear_generic(\n+                    x.flatten(start_dim=1), self.W_UV_O, self.W_UV_O_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape)\n+            else:\n+                output_parallel = torch.matmul(x.flatten(start_dim=1),\n+                                               self.W_UV_O)\n+            if self.tp_size > 1:\n+                output = tensor_model_parallel_all_reduce(output_parallel)\n+            else:\n+                output = output_parallel\n+            return output\n         else:\n             x = torch.einsum(\"bnl,lnv->bnv\", x, self.W_UV)\n             return self.o_proj(x.reshape(-1,\n@@ -171,6 +194,12 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n     def _q_proj_and_k_up_proj(self, x):\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            if is_fp8(self.W_Q_UK):\n+                return apply_fp8_linear_generic(\n+                    x, self.W_Q_UK, self.W_Q_UK_scales,\n+                    self.reqaunt_input_group_shape,\n+                    self.reqaunt_weight_group_shape).view(\n+                        -1, self.num_heads, self.kv_lora_rank)\n             return torch.matmul(x, self.W_Q_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n         else:\n@@ -179,8 +208,91 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             return torch.einsum(\"bnp,lnp->bnl\", x, self.W_UK)\\\n                 .view(-1, self.num_heads, self.kv_lora_rank)\n \n-    def process_weights_after_loading(self):\n-        kv_b_proj_weight = self.kv_b_proj.weight.T\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n+\n+        def is_layer_fp8(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, Fp8LinearMethod) or\\\n+                (isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8))\n+\n+        def quantization_scheme_supported(layer: LinearBase) -> bool:\n+            return isinstance(layer.quant_method, UnquantizedLinearMethod) or \\\n+                is_layer_fp8(layer)\n+\n+        # TODO(lucas) This is very gross, we need a more wide scale refactor of\n+        # all the FP8 code with a more standard way of\n+        # defining schemes/group-shapes, we should also potentially force\n+        # quant_methods to support a decompress function\n+        #\n+        # returns input_group_shape, weight_group_shape\n+        def get_scale_group_shapes_for_fp8(layer: LinearBase) -> \\\n+            Tuple[Tuple[int, int], Tuple[int, int]]:\n+            if isinstance(layer.quant_method, Fp8LinearMethod):\n+                if layer.quant_method.block_quant is not None:\n+                    weight_block_size = \\\n+                        layer.quant_method.quant_config.weight_block_size\n+                    # per-token-group (1, X), block-quantized (X, Y)\n+                    return (1, weight_block_size[-1]), weight_block_size\n+                else:\n+                    return (-1, -1), (-1, -1)  # per-tensor, per-tensor\n+            elif isinstance(layer.quant_method, CompressedTensorsLinearMethod)\\\n+                and isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                # this is hacky but we always assume the for\n+                # CompressedTensorsW8A8Fp8 the input is dynamic per-token\n+                # we ignore if it is static-per-tensor since we are going to\n+                # requantize after later anyways\n+                strategy = layer.scheme.strategy\n+                if strategy == QuantizationStrategy.TENSOR:\n+                    return (1, -1), (-1, -1)  # per-token, per-tensor\n+                elif strategy == QuantizationStrategy.CHANNEL:\n+                    return (1, -1), (-1, 1)  # per-token, per-channel\n+                else:\n+                    raise NotImplementedError(\n+                        f\"QuantizationStrategy.{strategy} is not supported for \"\n+                        \"fp8 MLA, please run with VLLM_MLA_DISABLE=1\")\n+            else:\n+                raise NotImplementedError(\n+                    \"Can't determine scale group shapes for \"\n+                    f\"{layer.quant_method}, please run with VLLM_MLA_DISABLE=1\"\n+                )\n+\n+        def get_scales(layer: LinearBase) -> torch.Tensor:\n+            if hasattr(layer, \"weight_scale_inv\"):\n+                return layer.weight_scale_inv\n+            return layer.weight_scale\n+\n+        def get_and_maybe_dequant_weights(layer: LinearBase):\n+            if is_layer_fp8(layer):\n+                if isinstance(layer.quant_method, \\\n+                    CompressedTensorsLinearMethod) and \\\n+                    isinstance(layer.scheme, CompressedTensorsW8A8Fp8):\n+                    # NOTE(lucas): note sure why but `CompressedTensorsW8A8Fp8`\n+                    # seems to store weights as (input, output) instead of\n+                    # (output, input) so we need to transpose\n+                    weight = layer.weight.T  # standardize to (output, input)\n+                else:\n+                    weight = layer.weight\n+                _, weight_scale_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(layer)\n+                scales = get_scales(layer)\n+\n+                return scaled_dequantize(weight, scales,\n+                                         weight_scale_group_shape)\n+            else:\n+                return layer.weight\n+\n+        if not (quantization_scheme_supported(self.kv_b_proj) and\\\n+            quantization_scheme_supported(self.q_proj) and\\\n+                quantization_scheme_supported(self.o_proj)):\n+            raise NotImplementedError(\n+                \"Only FP8 and UnquantizedLinearMethod are supported for MLA\"\n+                \", please run with VLLM_MLA_DISABLE=1\")\n+\n+        weight_dtype = self.kv_b_proj.weight.dtype\n+        assert self.o_proj.weight.dtype == weight_dtype\n+        assert self.q_proj.weight.dtype == weight_dtype\n+\n+        kv_b_proj_weight = get_and_maybe_dequant_weights(self.kv_b_proj).T\n         assert kv_b_proj_weight.shape == (\n             self.kv_lora_rank,\n             self.num_heads * (self.qk_nope_head_dim + self.v_head_dim)), (\n@@ -198,18 +310,35 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         W_UK, W_UV = kv_b_proj_weight.split(\n             [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n \n-        q_proj = self.q_proj.weight.T\\\n+        q_proj_weight = get_and_maybe_dequant_weights(self.q_proj).T\\\n                 .view(-1, self.num_heads, self.qk_head_dim)\n \n         # can be W_Q or W_UQ depending q_lora_rank, the former if\n         # q_lora_rank is None, the latter otherwise. From the Attention backend\n         # perspective though we call these both W_Q and rely on the layer\n         # to pass in the correct matrix\n-        W_Q = q_proj[..., :self.qk_nope_head_dim]\n-        self.W_QR = q_proj[..., self.qk_nope_head_dim:]\\\n+        W_Q = q_proj_weight[..., :self.qk_nope_head_dim]\n+        self.W_QR = q_proj_weight[..., self.qk_nope_head_dim:]\\\n             .flatten(start_dim=1).contiguous()\n \n+        # W_QR is small so for simplicity we dont bother requantizing it\n+        self.W_QR = self.W_QR.to(act_dtype)\n+\n         if envs.VLLM_MLA_PERFORM_MATRIX_ABSORPTION:\n+            requantization_enabled = not envs.VLLM_MLA_DISABLE_REQUANTIZATION\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                # This assumes it wise to requantize using the same group shapes\n+                # (i.e. strategy, per-tensor, per-channel, block etc.) that the\n+                # weights were originally quantized\n+                requant_input_group_shape, requant_weight_group_shape = \\\n+                    get_scale_group_shapes_for_fp8(self.q_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.kv_b_proj)\n+                assert (requant_input_group_shape, requant_weight_group_shape)\\\n+                    == get_scale_group_shapes_for_fp8(self.o_proj)\n+                self.reqaunt_input_group_shape = requant_input_group_shape\n+                self.reqaunt_weight_group_shape = requant_weight_group_shape\n+\n             #\n             # Perform matrix-absorption following\n             #     https://github.com/flashinfer-ai/flashinfer/pull/551\n@@ -223,25 +352,44 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n             # latter otherwise\n             # basically if q_lora_rank is none we are absorbing into q_proj\n             # instead of UQ\n-            self.W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n+            W_Q_UK = torch.einsum(\"qnd,lnd -> qnl\", W_Q, W_UK)\\\n                 .flatten(start_dim=1).contiguous()\n \n-            W_O = self.o_proj.weight\\\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_Q_UK, W_Q_UK_scales = scaled_quantize(\n+                    W_Q_UK,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_Q_UK = W_Q_UK.T.contiguous()\n+                self.W_Q_UK_scales = W_Q_UK_scales.T.contiguous()\n+            else:\n+                self.W_Q_UK = W_Q_UK.to(act_dtype)\n+\n+            W_O = get_and_maybe_dequant_weights(self.o_proj)\\\n                 .view(-1, self.num_heads, self.v_head_dim)\n-            self.W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n+            W_UV_O = torch.einsum(\"lnd,hnd -> nlh\", W_UV, W_O)\\\n                 .flatten(start_dim=0, end_dim=1).contiguous()\n \n-            tp_size = get_tensor_model_parallel_world_size()\n-            self.o_proj_absorbed = RowParallelLinear(\n-                self.W_UV_O.shape[0] * tp_size,\n-                self.W_UV_O.shape[1],\n-                bias=False,\n-                # TODO(lucas) figure out how to properly forward quant_method\n-                #quant_config=self.o_proj.quant_method,\n-            )\n-\n-            self.o_proj_absorbed.weight = torch.nn.Parameter(self.W_UV_O.T)\n+            if is_fp8(weight_dtype) and requantization_enabled:\n+                W_UV_O, W_UV_O_scales = scaled_quantize(\n+                    W_UV_O,\n+                    self.reqaunt_weight_group_shape,\n+                    quant_dtype=current_platform_fp8_dtype)\n+                # For FP8 save the transpose so we can use\n+                # `apply_w8a8_block_fp8_linear` directly\n+                self.W_UV_O = W_UV_O.T.contiguous()\n+                self.W_UV_O_scales = W_UV_O_scales.T.contiguous()\n+            else:\n+                self.W_UV_O = W_UV_O.to(act_dtype)\n+\n+            self.tp_size = get_tensor_model_parallel_world_size()\n         else:\n+            if is_fp8(weight_dtype):\n+                raise NotImplementedError(\n+                    \"Currently fp8 requires matrix absorption\")\n+\n             self.W_UV = W_UV\n             self.W_UK = W_UK\n             self.W_Q = W_Q.flatten(start_dim=1)\ndiff --git a/vllm/attention/backends/triton_mla.py b/vllm/attention/backends/triton_mla.py\nindex da09bb70b..95dc119a4 100644\n--- a/vllm/attention/backends/triton_mla.py\n+++ b/vllm/attention/backends/triton_mla.py\n@@ -57,14 +57,12 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_kv_cache_shape(\n-            num_blocks: int,\n-            block_size: int,\n-            num_kv_heads: int,  # assumed to be 1 for MLA\n-            kv_lora_rank: int,  # passed via head_size\n+        num_blocks: int,\n+        block_size: int,\n+        num_kv_heads: int,  # assumed to be 1 for MLA\n+        head_size: int,\n     ) -> Tuple[int, ...]:\n-        # TODO(lucas): remove hardcoding k_pe size as 1/8th of kv_lora_rank\n-        k_pe_size = kv_lora_rank // 8\n-        return (num_blocks, block_size, kv_lora_rank + k_pe_size)\n+        return (num_blocks, block_size, head_size)\n \n     @staticmethod\n     def swap_blocks(\n@@ -83,7 +81,7 @@ class TritonMLABackend(AttentionBackend):\n \n     @staticmethod\n     def get_supported_head_sizes() -> List[int]:\n-        return [512]\n+        return [576]\n \n \n class TritonMLAState(AttentionState):\n@@ -624,8 +622,6 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             self.multimodal_placeholder_maps.items()\n         }\n \n-        num_kv_splits = 8\n-\n         return TritonMLAMetadata(\n             num_prefills=self.num_prefills,\n             slot_mapping=slot_mapping_tensor,\n@@ -645,7 +641,7 @@ class TritonMLAMetadataBuilder(AttentionMetadataBuilder[TritonMLAMetadata]):\n             context_lens_tensor=context_lens_tensor,\n             block_tables=block_tables,\n             use_cuda_graph=use_captured_graph,\n-            num_kv_splits=num_kv_splits,\n+            num_kv_splits=4,  # TODO(lucas) add heuristic\n             head_dim=self.runner.model_config.get_head_size(),\n         )\n \ndiff --git a/vllm/attention/layer.py b/vllm/attention/layer.py\nindex 9b804a29a..b97165f62 100644\n--- a/vllm/attention/layer.py\n+++ b/vllm/attention/layer.py\n@@ -200,9 +200,9 @@ class Attention(nn.Module):\n         s += f\", backend={self.impl.__class__.__name__}\"\n         return s\n \n-    def process_weights_after_loading(self):\n+    def process_weights_after_loading(self, act_dtype: torch.dtype):\n         if hasattr(self.impl, \"process_weights_after_loading\"):\n-            self.impl.process_weights_after_loading()\n+            self.impl.process_weights_after_loading(act_dtype)\n \n \n class MultiHeadAttention(nn.Module):\ndiff --git a/vllm/config.py b/vllm/config.py\nindex f6bd8b1ad..f998502ee 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -739,18 +739,19 @@ class ModelConfig:\n     @property\n     def is_deepseek_mla(self) -> bool:\n         # TODO add deepseek_v3\n-        return hasattr(self.hf_text_config,\n-                       \"model_type\") and (self.hf_text_config.model_type\n-                                          in ('deepseek_v2'))\n+        return (hasattr(self.hf_text_config, \"model_type\")) \\\n+                and (self.hf_text_config.model_type in \\\n+                    ('deepseek_v2', 'deepseek_v3'))\\\n+                and (self.hf_text_config.kv_lora_rank is not None)\n \n     def get_head_size(self) -> int:\n         # TODO remove hard code\n         if self.is_deepseek_mla:\n+            qk_rope_head_dim = getattr(self.hf_text_config, \"qk_rope_head_dim\",\n+                                       0)\n             if self.use_mla:\n-                return self.hf_text_config.kv_lora_rank\n+                return self.hf_text_config.kv_lora_rank + qk_rope_head_dim\n             else:\n-                qk_rope_head_dim = getattr(self.hf_text_config,\n-                                           \"qk_rope_head_dim\", 0)\n                 qk_nope_head_dim = getattr(self.hf_text_config,\n                                            \"qk_nope_head_dim\", 0)\n                 if qk_rope_head_dim and qk_nope_head_dim:\n@@ -969,6 +970,32 @@ class ModelConfig:\n \n     @property\n     def use_mla(self) -> bool:\n+        if self.quantization is not None and self.quantization not in [\\\n+            \"fp8\", \"compressed-tensors\"]:\n+            logger.warning(\n+                \"MLA is not supported with %s quantization. \"\n+                \"Disabling MLA.\", self.quantization)\n+            return False\n+\n+        # If using a \"compressed-tensors\" checkpoint, check that all groups\n+        # have fp8 for both weights and activations.\n+        if self.quantization == \"compressed-tensors\":\n+            quant_config = self._parse_quant_hf_config()\n+            for group_name, cfg in quant_config.get(\"config_groups\",\n+                                                    (\"\", {})).items():\n+                act_cfg = cfg.get(\"input_activations\", {})\n+                act_type = None if act_cfg is None else act_cfg.get(\"type\", \"\")\n+                w_cfg = cfg.get(\"weights\", {})\n+                w_type = None if w_cfg is None else w_cfg.get(\"type\", \"\")\n+                if act_type != \"fp8\" or w_type != \"fp8\":\n+                    logger.warning(\n+                        \"compressed-tensors MLA support requires fp8 \"\n+                        \"activations and weights in group '%s', but got \"\n+                        \"activations type '%s' and weights type '%s'.\\n \"\n+                        \"Full config: %s\", group_name, act_type, w_type,\n+                        quant_config)\n+                    return False\n+\n         use_mla = (self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE)\n         return use_mla\n \ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 2a18e3b9b..25098070b 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -79,6 +79,7 @@ if TYPE_CHECKING:\n     VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n     VLLM_MLA_DISABLE: bool = False\n     VLLM_MLA_PERFORM_MATRIX_ABSORPTION: bool = True\n+    VLLM_MLA_DISABLE_REQUANTIZATION: bool = False\n \n \n def get_default_cache_root():\n@@ -519,7 +520,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # storing more weights, W_Q_UK and W_UV_O, so can increase memory usage,\n     # the is enabled by default\n     \"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\":\n-    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\")))\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_PERFORM_MATRIX_ABSORPTION\", \"1\"))),\n+\n+    # When running MLA with matrix-absorption enabled and fp8 quantized weights\n+    # we perform the matrix-absorption in float32 precision, after the matrices\n+    # are absorbed we requantize the weights back to fp8, this flag can be used\n+    # to disable the requantization step, and instead convert the absorbed\n+    # matrices to match the activation type. This can lead to higher memory and\n+    # compute usage but better preserves the accuracy of the original model.\n+    \"VLLM_MLA_DISABLE_REQUANTIZATION\":\n+    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE_REQUANTIZATION\", \"0\")))\n }\n \n # end-env-vars-definition\ndiff --git a/vllm/model_executor/layers/quantization/utils/fp8_utils.py b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\nindex ccebff341..850820f66 100644\n--- a/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/fp8_utils.py\n@@ -2,7 +2,7 @@\n import functools\n import json\n import os\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n import triton\n@@ -10,10 +10,24 @@ import triton.language as tl\n \n from vllm import _custom_ops as ops\n from vllm.logger import init_logger\n+from vllm.model_executor.layers.quantization.utils.quant_utils import (\n+    _normalize_quant_group_shape, scaled_dequantize)\n+from vllm.model_executor.layers.quantization.utils.w8a8_utils import (\n+    apply_fp8_linear)\n from vllm.platforms import current_platform\n \n logger = init_logger(__name__)\n \n+current_platform_fp8_dtype = (torch.float8_e4m3fnuz\n+                              if current_platform.is_rocm() else\n+                              torch.float8_e4m3fn)\n+\n+\n+def is_fp8(x: Union[torch.dtype, torch.Tensor]) -> bool:\n+    if isinstance(x, torch.Tensor):\n+        x = x.dtype\n+    return x == torch.float8_e4m3fn or x == torch.float8_e4m3fnuz\n+\n \n def apply_w8a8_block_fp8_linear(\n     input: torch.Tensor,\n@@ -55,6 +69,42 @@ def apply_w8a8_block_fp8_linear(\n     return output.to(dtype=input.dtype).view(*output_shape)\n \n \n+# Unify the interface between `apply_w8a8_block_fp8_linear` and\n+# `apply_fp8_linear`\n+# NOTE(lucas): this is quite messy, we should think through this more formally\n+def apply_fp8_linear_generic(\n+        input: torch.Tensor,\n+        weight: torch.Tensor,\n+        weight_scale: torch.Tensor,\n+        input_group_shape: Tuple[int, int],\n+        weight_group_shape: Tuple[int, int],\n+        input_scale: Optional[torch.Tensor] = None,  # static scale if one\n+) -> torch.Tensor:\n+    # View input as 2D matrix for fp8 methods\n+    input = input.view(-1, input.shape[-1])\n+\n+    weight_group_shape = _normalize_quant_group_shape(\\\n+        weight, weight_group_shape)\n+    input_group_shape = _normalize_quant_group_shape(input, input_group_shape)\n+\n+    def is_dim_blocked(dim, shape, group_shape):\n+        return group_shape < shape[dim] and group_shape > 1\n+\n+    if is_dim_blocked(0, weight.shape, weight_group_shape[0])\\\n+     and is_dim_blocked(1, weight.shape, weight_group_shape[1]) and\\\n+     input_group_shape == (1, weight_group_shape[1]):\n+        return apply_w8a8_block_fp8_linear(input, weight,\n+                                           list(weight_group_shape),\n+                                           weight_scale)\n+    else:\n+        # Despite having linear in the it doesn't conform to\n+        # `torch.nn.functional.linear` which is defined as `input @ weight.T`\n+        # so we explicitly transpose the weight matrix here\n+        return apply_fp8_linear(input, weight.T, weight_scale.T,\n+                         use_per_token_if_dynamic=\\\n+                             (input_group_shape == (1, input.shape[1])))\n+\n+\n def input_to_float8(\n         x: torch.Tensor,\n         dtype: Optional[torch.dtype] = None\n@@ -75,7 +125,6 @@ def input_to_float8(\n def block_quant_to_tensor_quant(\n     x_q_block: torch.Tensor,\n     x_s: torch.Tensor,\n-    block_size: List[int],\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"This function converts block-wise quantization to tensor-wise\n     quantization. The inputs are block-wise quantization tensor `x_q_block`,\n@@ -83,26 +132,7 @@ def block_quant_to_tensor_quant(\n     The outputs are tensor-wise quantization tensor and tensor-wise\n     quantization scale. Note only float8 is supported for now.\n     \"\"\"\n-    block_n, block_k = block_size[0], block_size[1]\n-    n, k = x_q_block.shape\n-    n_tiles = (n + block_n - 1) // block_n\n-    k_tiles = (k + block_k - 1) // block_k\n-    assert n_tiles == x_s.shape[0]\n-    assert k_tiles == x_s.shape[1]\n-\n-    x_dq_block = x_q_block.to(torch.float32)\n-\n-    x_dq_block_tiles = [[\n-        x_dq_block[\n-            j * block_n:min((j + 1) * block_n, n),\n-            i * block_k:min((i + 1) * block_k, k),\n-        ] for i in range(k_tiles)\n-    ] for j in range(n_tiles)]\n-\n-    for i in range(k_tiles):\n-        for j in range(n_tiles):\n-            x_dq_block_tiles[j][i][:, :] = x_dq_block_tiles[j][i] * x_s[j][i]\n-\n+    x_dq_block = scaled_dequantize(x_q_block, x_s)\n     x_q_tensor, scale = input_to_float8(x_dq_block, dtype=x_q_block.dtype)\n     return x_q_tensor, scale\n \ndiff --git a/vllm/model_executor/layers/quantization/utils/quant_utils.py b/vllm/model_executor/layers/quantization/utils/quant_utils.py\nindex 83055d600..95e785dcc 100644\n--- a/vllm/model_executor/layers/quantization/utils/quant_utils.py\n+++ b/vllm/model_executor/layers/quantization/utils/quant_utils.py\n@@ -1,5 +1,5 @@\n \"\"\"This file is used for /tests and /benchmarks\"\"\"\n-from typing import List, Optional\n+from typing import List, Optional, Tuple\n \n import numpy\n import torch\n@@ -20,6 +20,120 @@ FUSED_LAYER_NAME_MAPPING = {\n }\n \n \n+# Normalize the group_shape to the full extent for any dims that are -1\n+def _normalize_quant_group_shape(x: torch.Tensor, group_shape: Tuple[int,\n+                                                                     int]):\n+    # -1 means full extent\n+    return (group_shape[0] if group_shape[0] > 0 else x.shape[-2],\n+            group_shape[1] if group_shape[1] > 0 else x.shape[-1])\n+\n+\n+# Useful when treating N-dimensional group scaling as extended numpy-style\n+# broadcasting in numpy simply stretches dimensions with an extent of 1 to match\n+# the target shape by repeating the data along that dimension (broadcasting)\n+# , we extend these semantics to say if the extent of a dimension in the\n+# source shape is not 1 and does not match the target shape we repeat each\n+# element along that dimension src_shape[dim] // target_shape[dim] times\n+# example if we have:\n+#       a = [[1, 2], and target_shape = (2, 4)\n+#            [3, 4]]\n+# then we would expand a to:\n+#       a = [[1, 1, 2, 2],\n+#            [3, 3, 4, 4]]\n+# NOTE this function this function does not explicitly broadcast dimensions\n+# with an extent of 1, since this can be done implicitly by pytorch\n+def group_broadcast(t, shape):\n+    for i, s in enumerate(shape):\n+        if t.shape[i] != s and t.shape[i] != 1:\n+            assert s % t.shape[i] == 0\n+            t = t.unsqueeze(i + 1)\\\n+                .expand(*t.shape[:i+1], s // t.shape[i], *t.shape[i+1:])\\\n+                .flatten(i, i + 1)\n+    return t\n+\n+\n+# Quantize assuming once scale per group of elements with shape group_shape,\n+# example group shapes:\n+#  * (-1, -1)   for per-tensor quantization\n+#  * (1, -1)    for per-row quantization\n+#  * (-1, 1)    for per-column quantization\n+#  * (128, 128) for 128x128 deepseek style block quantization\n+#  * (1, 128)   for deepseek style activation quantization\n+#               (i.e. per-token-per-group)\n+def scaled_quantize(\n+    x: torch.Tensor,\n+    group_shape: Tuple[int, int],\n+    quant_dtype: torch.dtype,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    group_shape = _normalize_quant_group_shape(x, group_shape)\n+    assert quant_dtype.is_floating_point, \\\n+        \"currently `scaled_quantize` only supports floating point dtypes \" \\\n+        \"but could be extended to support other dtypes\"\n+\n+    finfo = torch.finfo(quant_dtype)\n+\n+    # Reshape (M, N) into (BLK_M, BLOCK_SIZE_M, BLK_N, BLOCK_SIZE_N)\n+    assert x.ndim == 2\n+    assert x.shape[0] % group_shape[0] == 0 and x.shape[1] % group_shape[1] == 0\n+    blk_m, blk_n = x.shape[0] // group_shape[0], x.shape[1] // group_shape[1]\n+    x_blkd = x.reshape(blk_m, group_shape[0], blk_n, group_shape[1])\n+\n+    # Permute to (BLK_M, BLK_N, BLOCK_SIZE_M, BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd.permute(0, 2, 1, 3)\n+    # Flatten to (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N)\n+    x_blkd_permd = x_blkd_permd.flatten(start_dim=2)\n+\n+    # Compute scales\n+    min_val, max_val = x_blkd_permd.aminmax(dim=-1)\n+    amax = torch.maximum(min_val.abs(), max_val.abs()).clamp(min=1e-12)\n+    scale = finfo.max / amax\n+\n+    # Apply scale and convert form:\n+    # (BLK_M, BLK_N, BLOCK_SIZE_M * BLOCK_SIZE_N) to (M, N)\n+    x_scl_sat = (x_blkd_permd * scale.unsqueeze(-1))\\\n+        .clamp(min=finfo.min, max=finfo.max)\\\n+        .reshape(blk_m, blk_n, group_shape[0], group_shape[1])\\\n+        .permute(0, 2, 1, 3)\\\n+        .reshape(x.shape)\n+\n+    return x_scl_sat.to(quant_dtype).contiguous(), scale.float().reciprocal()\n+\n+\n+# inverses `scaled_quantize`\n+def scaled_dequantize(\n+    x_q: torch.Tensor,\n+    x_s: torch.Tensor,\n+    group_shape: Optional[Tuple[int, int]] = None,\n+    out_dtype: torch.dtype = torch.float32,\n+) -> Tuple[torch.Tensor, torch.Tensor]:\n+    if group_shape is not None:\n+        group_shape = _normalize_quant_group_shape(x_q, group_shape)\n+\n+    if x_s.ndim == 0:  # scalar\n+        x_s = x_s.unsqueeze(-1).unsqueeze(-1)  # convert to (1, 1) tensor\n+    if x_s.ndim == 1:\n+        if group_shape is None:\n+            raise AssertionError(\n+                \"if x_s is 1D tensor, group_shape must be provided otherwise \"\n+                \"its ambiguous which dimension to broadcast x_s to\")\n+        # unsqueeze the scales for the dimension where we want to broadcast\n+        # across the full extent\n+        if group_shape[0] == x_q.shape[-2]:\n+            x_s = x_s.unsqueeze(-2)\n+        elif group_shape[1] == x_q.shape[-1]:\n+            x_s = x_s.unsqueeze(-1)\n+        else:\n+            raise AssertionError(\n+                \"if x_s is a vector we should be broadcasting it to the full \"\n+                \"extent of one of the dimensions\")\n+\n+    if group_shape is not None:\n+        assert x_s.shape[-1] == x_q.shape[-1] // group_shape[1]\n+        assert x_s.shape[-2] == x_q.shape[-2] // group_shape[0]\n+    x_s = group_broadcast(x_s.to(torch.float32), x_q.shape)\n+    return (x_q.to(torch.float32) * x_s).to(out_dtype)\n+\n+\n def pack_quantized_values_into_int32(w_q: torch.Tensor,\n                                      wtype: ScalarType,\n                                      packed_dim: int = 0):\ndiff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py\nindex 62babcddd..4be511d12 100644\n--- a/vllm/model_executor/model_loader/loader.py\n+++ b/vllm/model_executor/model_loader/loader.py\n@@ -398,11 +398,13 @@ class DefaultModelLoader(BaseModelLoader):\n                     # parameters onto device for processing and back off after.\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n-                elif isinstance(module, Attention) and \\\n+                if isinstance(module, Attention) and \\\n                     hasattr(module, \"process_weights_after_loading\"):\n                     # When attention modules need to process weights after\n                     # currently only used by MLA\n-                    module.process_weights_after_loading()\n+                    # TODO(lucas): see if there is a way to unify the signatures\n+                    # of process_weights_after_loading\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -439,6 +441,11 @@ class DummyModelLoader(BaseModelLoader):\n                     with device_loading_context(\n                             module, torch.device(device_config.device)):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \n@@ -633,6 +640,12 @@ class ShardedStateLoader(BaseModelLoader):\n                     quant_method = getattr(module, \"quant_method\", None)\n                     if quant_method is not None:\n                         quant_method.process_weights_after_loading(module)\n+                    if isinstance(module, Attention) and \\\n+                        hasattr(module, \"process_weights_after_loading\"):\n+                        # When attention modules need to process weights after\n+                        # currently only used by MLA\n+                        module.process_weights_after_loading(\n+                            model_config.dtype)\n             rank = get_tensor_model_parallel_rank()\n             pattern = os.path.join(\n                 local_model_path,\n@@ -1272,7 +1285,7 @@ class GGUFModelLoader(BaseModelLoader):\n \n class RunaiModelStreamerLoader(BaseModelLoader):\n     \"\"\"\n-        Model loader that can load safetensors \n+        Model loader that can load safetensors\n         files from local FS or S3 bucket.\n     \"\"\"\n \n@@ -1369,6 +1382,11 @@ class RunaiModelStreamerLoader(BaseModelLoader):\n                 if quant_method is not None:\n                     with device_loading_context(module, target_device):\n                         quant_method.process_weights_after_loading(module)\n+                if isinstance(module, Attention) and \\\n+                    hasattr(module, \"process_weights_after_loading\"):\n+                    # When attention modules need to process weights after\n+                    # currently only used by MLA\n+                    module.process_weights_after_loading(model_config.dtype)\n         return model.eval()\n \n \ndiff --git a/vllm/model_executor/models/deepseek_v3.py b/vllm/model_executor/models/deepseek_v3.py\nindex 0b44f0d06..f6ab53c85 100644\n--- a/vllm/model_executor/models/deepseek_v3.py\n+++ b/vllm/model_executor/models/deepseek_v3.py\n@@ -27,7 +27,7 @@ from torch import nn\n from transformers import PretrainedConfig\n \n from vllm.attention import Attention, AttentionMetadata\n-from vllm.config import CacheConfig, VllmConfig\n+from vllm.config import CacheConfig, ModelConfig, VllmConfig\n from vllm.distributed import (get_pp_group,\n                               get_tensor_model_parallel_world_size,\n                               tensor_model_parallel_all_reduce)\n@@ -333,12 +333,156 @@ class DeepseekV3Attention(nn.Module):\n         return output\n \n \n+class DeepseekV3MLAAttention(nn.Module):\n+    \"\"\"\n+    Main reference: DeepseekV2 paper, and FlashInfer Implementation\n+    (https://arxiv.org/abs/2405.04434 and https://github.com/flashinfer-ai/flashinfer/pull/551).\n+    \n+    For more info see MLACommonImpl in: vllm/attention/backends/mla/utils.py\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config: PretrainedConfig,\n+        hidden_size: int,\n+        num_heads: int,\n+        qk_nope_head_dim: int,\n+        qk_rope_head_dim: int,\n+        v_head_dim: int,\n+        q_lora_rank: Optional[int],\n+        kv_lora_rank: int,\n+        rope_theta: float = 10000,\n+        rope_scaling: Optional[Dict[str, Any]] = None,\n+        max_position_embeddings: int = 8192,\n+        cache_config: Optional[CacheConfig] = None,\n+        quant_config: Optional[QuantizationConfig] = None,\n+        prefix: str = \"\",\n+    ) -> None:\n+        super().__init__()\n+        self.hidden_size = hidden_size\n+        self.qk_nope_head_dim = qk_nope_head_dim\n+        self.qk_rope_head_dim = qk_rope_head_dim\n+        self.qk_head_dim = qk_nope_head_dim + qk_rope_head_dim\n+        self.v_head_dim = v_head_dim\n+\n+        self.q_lora_rank = q_lora_rank\n+        self.kv_lora_rank = kv_lora_rank\n+\n+        self.num_heads = num_heads\n+        tp_size = get_tensor_model_parallel_world_size()\n+        assert num_heads % tp_size == 0\n+        self.num_local_heads = num_heads // tp_size\n+\n+        self.scaling = self.qk_head_dim**-0.5\n+        self.rope_theta = rope_theta\n+        self.max_position_embeddings = max_position_embeddings\n+\n+        if self.q_lora_rank is not None:\n+            self.q_a_proj = ReplicatedLinear(self.hidden_size,\n+                                             self.q_lora_rank,\n+                                             bias=False,\n+                                             quant_config=quant_config,\n+                                             prefix=f\"{prefix}.q_a_proj\")\n+            self.q_a_layernorm = RMSNorm(self.q_lora_rank,\n+                                         eps=config.rms_norm_eps)\n+            self.q_b_proj = ColumnParallelLinear(q_lora_rank,\n+                                                 self.num_heads *\n+                                                 self.qk_head_dim,\n+                                                 bias=False,\n+                                                 quant_config=quant_config,\n+                                                 prefix=f\"{prefix}.q_b_proj\")\n+        else:\n+            self.q_proj = ColumnParallelLinear(self.hidden_size,\n+                                               self.num_heads *\n+                                               self.qk_head_dim,\n+                                               bias=False,\n+                                               quant_config=quant_config,\n+                                               prefix=f\"{prefix}.q_proj\")\n+\n+        self.kv_a_proj_with_mqa = ReplicatedLinear(\n+            self.hidden_size,\n+            self.kv_lora_rank + self.qk_rope_head_dim,\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_a_proj_with_mqa\")\n+        self.kv_a_layernorm = RMSNorm(self.kv_lora_rank,\n+                                      eps=config.rms_norm_eps)\n+        self.kv_b_proj = ColumnParallelLinear(\n+            self.kv_lora_rank,\n+            self.num_heads * (self.qk_nope_head_dim + self.v_head_dim),\n+            bias=False,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.kv_b_proj\")\n+        self.o_proj = RowParallelLinear(self.num_heads * self.v_head_dim,\n+                                        self.hidden_size,\n+                                        bias=False,\n+                                        quant_config=quant_config,\n+                                        prefix=f\"{prefix}.o_proj\")\n+\n+        rope_scaling[\"rope_type\"] = 'deepseek_yarn'\n+        self.rotary_emb = get_rope(qk_rope_head_dim,\n+                                   rotary_dim=qk_rope_head_dim,\n+                                   max_position=max_position_embeddings,\n+                                   base=rope_theta,\n+                                   rope_scaling=rope_scaling,\n+                                   is_neox_style=False)\n+        if rope_scaling:\n+            mscale_all_dim = rope_scaling.get(\"mscale_all_dim\", False)\n+            scaling_factor = rope_scaling[\"factor\"]\n+            mscale = yarn_get_mscale(scaling_factor, float(mscale_all_dim))\n+            self.scaling = self.scaling * mscale * mscale\n+\n+        self.mla_attn = Attention(\n+            num_heads=self.num_local_heads,\n+            head_size=self.kv_lora_rank,\n+            scale=self.scaling,\n+            num_kv_heads=1,\n+            cache_config=cache_config,\n+            quant_config=quant_config,\n+            prefix=f\"{prefix}.attn\",\n+            use_mla=True,\n+            # MLA Args\n+            q_lora_rank=self.q_lora_rank,\n+            kv_lora_rank=self.kv_lora_rank,\n+            qk_nope_head_dim=self.qk_nope_head_dim,\n+            qk_rope_head_dim=self.qk_rope_head_dim,\n+            qk_head_dim=self.qk_head_dim,\n+            v_head_dim=self.v_head_dim,\n+            rotary_emb=self.rotary_emb,\n+            q_proj=self.q_proj if self.q_lora_rank is None else self.q_b_proj,\n+            kv_b_proj=self.kv_b_proj,\n+            o_proj=self.o_proj,\n+        )\n+\n+        self.prefix = prefix\n+        self.debug_layer_idx = int(self.prefix.split(\".\")[-2])\n+\n+    def forward(\n+        self,\n+        positions: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        kv_cache: torch.Tensor,\n+        attn_metadata: AttentionMetadata,\n+    ) -> torch.Tensor:\n+        if self.q_lora_rank is not None:\n+            ckq = self.q_a_proj(hidden_states)[0]\n+            hidden_states_or_q_c = self.q_a_layernorm(ckq)\n+        else:\n+            hidden_states_or_q_c = hidden_states\n+        kv_c, k_pe = self.kv_a_proj_with_mqa(hidden_states)[0].split(\n+            [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n+        kv_c_normed = self.kv_a_layernorm(kv_c.contiguous())\n+        return self.mla_attn(hidden_states_or_q_c, kv_c_normed, k_pe, kv_cache,\n+                             attn_metadata)\n+\n+\n class DeepseekV3DecoderLayer(nn.Module):\n \n     def __init__(\n         self,\n         config: PretrainedConfig,\n         prefix: str,\n+        model_config: ModelConfig,\n         cache_config: Optional[CacheConfig] = None,\n         quant_config: Optional[QuantizationConfig] = None,\n     ) -> None:\n@@ -351,7 +495,11 @@ class DeepseekV3DecoderLayer(nn.Module):\n         # DecoderLayers are created with `make_layers` which passes the prefix\n         # with the layer's index.\n         layer_idx = int(prefix.split(sep='.')[-1])\n-        self.self_attn = DeepseekV3Attention(\n+        if model_config.use_mla:\n+            attn_cls = DeepseekV3MLAAttention\n+        else:\n+            attn_cls = DeepseekV3Attention\n+        self.self_attn = attn_cls(\n             config=config,\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n@@ -428,6 +576,7 @@ class DeepseekV3Model(nn.Module):\n         super().__init__()\n \n         config = vllm_config.model_config.hf_config\n+        model_config = vllm_config.model_config\n         cache_config = vllm_config.cache_config\n         quant_config = vllm_config.quant_config\n \n@@ -447,6 +596,7 @@ class DeepseekV3Model(nn.Module):\n             lambda prefix: DeepseekV3DecoderLayer(\n                 config,\n                 prefix,\n+                model_config=model_config,\n                 cache_config=cache_config,\n                 quant_config=quant_config,\n             ),\ndiff --git a/vllm/worker/cache_engine.py b/vllm/worker/cache_engine.py\nindex 08316ba74..c427b759b 100644\n--- a/vllm/worker/cache_engine.py\n+++ b/vllm/worker/cache_engine.py\n@@ -110,7 +110,9 @@ class CacheEngine:\n             parallel_config, LayerBlockType.attention)\n \n         key_cache_block = cache_config.block_size * num_heads * head_size\n-        value_cache_block = key_cache_block\n+        # For MLA there is no value cache, since the latent vector\n+        # is joint keys and values.\n+        value_cache_block = key_cache_block if not model_config.use_mla else 0\n         total = num_attention_layers * (key_cache_block + value_cache_block)\n         if cache_config.cache_dtype == \"auto\":\n             dtype = model_config.dtype",
  "apis": [
    "vllm.attention.backends.MLACommonImpl.process_weights_after_loading",
    "vllm.attention.layer.Attention.process_weights_after_loading",
    "vllm.config.ModelConfig.get_head_size",
    "vllm.config.ModelConfig.is_deepseek_mla",
    "vllm.model_executor.models.deepseek_v3.DeepseekV3MLAAttention"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/adapter_commons/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/multimodal/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/profiler/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/kernels/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/benchmarks/cutlass_benchmarks/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/structured_output/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/spec_decode/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/core/block/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/models/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/model_loader/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/benchmarks/lib/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/punica_wrapper/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/attention/backends/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/core/sched/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/kv_transfer/kv_connector/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/entrypoints/openai/tool_parsers/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/lora/ops/triton_ops/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/quark/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/quantization/compressed_tensors/utils.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/attention/layer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/layer.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes non-trivial modifications to multiple core source files in the repository (e.g. attention/backends/mla/utils.py, triton_mla.py, model layer files, etc.), and it introduces changes to the matrix absorption process in the MLA (Multi-Head Attention) implementation. The modifications include handling of FP8 quantization, optimizing weight processing, and adjusting API configurations (e.g., changes in requantization, matrix absorption, and performance flags). These are performance-critical changes and optimizations in the model's attention mechanism, which are intended to improve computational efficiency (especially on CPU) while supporting FP8 compute. The commit does not merely fix bugs, refactor code, or add new features but instead updates internal APIs to enhance performance. Therefore, this commit satisfies the optimization and performance-related conditions.",
  "llm_api_reason": "This commit introduces Deepseek V3â€™s MLA (Multiâ€Head Latent Attention) support with FP8 compute by updating several parts of the MLA backend. In the MLA utils file, changes adjust quantization logic (including fp8â€specific matrix absorption and conditional reâ€‘quantization) in the common MLA implementation. The API of process_weights_after_loading has been updated to now accept an activation dtype argument, which is then called from the attention layerâ€™s process_weights_after_loading. Also, ModelConfigâ€™s is_deepseek_mla property and get_head_size method now take into account the Deepseek V3 MLA mode (and its additional head-dim contribution) while applying appropriate quantization checks. New DeepseekV3MLAAttention class is introduced in the deepseek_v3 model module so that when the model is configured for MLA it instantiates the appropriate attention module. These changes affect the core attentionâ€loading and weightâ€processing API calls for MLA, as well as model configuration behavior for Deepseek V3 models."
}