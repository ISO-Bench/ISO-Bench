{
  "commit_hash": "8bc68e198c4c90ddc2e54fa76eb81c2c714bb1cd",
  "pr_url": "https://github.com/vllm-project/vllm/pull/4208",
  "pr_date": "2024-05-13",
  "timeline_text": "Copy link Collaborator sangstar commented Apr 19, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . Automatically detect vLLM-tensorized model, update tensorizer to version 2.9.0 This PR accomplishes several things: Updates docstrings to account for tensorizer refactor in [Core] Refactor model loading code #4097 in the tensorize_vllm_examples.py example script, and slight corrections to the docstrings of the new, refactored functions. Allows models to be automatically inferred as a vLLM-tensorized model . Accomplishes this by placing a meta-tensor \"footprint\" in the serialized model, and removing it at runtime. vllm_tensorized as an arg has been removed. Updates tensorizer to the full release of 2.9.0. PR Checklist (Click to Expand) Thank you for your contribution to vLLM! Before submitting the pull request, please ensure the PR meets the following criteria. This helps vLLM maintain the code quality and improve the efficiency of the review process. PR Title and Classification Only specific types of PRs will be reviewed. The PR title is prefixed appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [CI/Build] for build or continuous integration improvements. [Doc] for documentation fixes and improvements. [Model] for adding a new model or improving an existing model. Model name should appear in the title. [Frontend] For changes on the vLLM frontend (e.g., OpenAI API server, LLM class, etc.) [Kernel] for changes affecting CUDA kernels or other compute kernels. [Core] for changes in the core vLLM logic (e.g., LLMEngine , AsyncLLMEngine , Scheduler , etc.) [Hardware][Vendor] for hardware-specific changes. Vendor name should appear in the prefix (e.g., [Hardware][AMD] ). [Misc] for PRs that do not fit the above categories. Please use this sparingly. Note: If the PR spans more than one category, please include all relevant prefixes. Code Quality The PR need to meet the following code quality standards: We adhere to Google Python style guide and Google C++ style guide . Pass all linter checks. Please use format.sh to format your code. The code need to be well-documented to ensure future contributors can easily understand the code. Include sufficient tests to ensure the project to stay correct and robust. This includes both unit tests and integration tests. Please add documentation to docs/source/ if the PR modifies the user-facing behaviors of vLLM. It helps vLLM user understand and utilize the new features or changes. Notes for Large Changes Please keep the changes as concise as possible. For major architectural changes (>500 LOC excluding kernel/data/config/test), we would expect a GitHub issue (RFC) discussing the technical design and justification. Otherwise, we will tag it with rfc-required and might not go through the PR. What to Expect for the Reviews The goal of the vLLM team is to be a transparent reviewing machine . We would like to make the review process transparent and efficient and make sure no contributor feel confused or frustrated. However, the vLLM team is small, so we need to prioritize some PRs over others. Here is what you can expect from the review process: After the PR is submitted, the PR will be assigned to a reviewer. Every reviewer will pick up the PRs based on their expertise and availability. After the PR is assigned, the reviewer will provide status update every 2-3 days. If the PR is not reviewed within 7 days, please feel free to ping the reviewer or the vLLM team. After the review, the reviewer will put an action-required label on the PR if there are changes required. The contributor should address the comments and ping the reviewer to re-review the PR. Please respond to all comments within a reasonable time frame. If a comment isn't clear or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Thank You Finally, thank you for taking the time to read these guidelines and for your interest in contributing to vLLM. Your contributions make vLLM a great tool for everyone! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions sangstar added 15 commits April 18, 2024 16:32 perf: Update tensorizer versions to new release 97131c0 perf: Update tensorizer versions to new release 1ba6bc5 docs: Remove unnecessary comma 5e58d6f refactor: (WIP) Allow detection of vLLM-tensorized model â€¦ 62006f9 WIP because the codes needs to be cleaned up, and the current work\nrefactoring the example script in to importable functions from\n`tensorizer.py` is still in progress, which will allow for better\nforward compatibility and better testing. tests: Add testing for vLLM-tensorized model has same output cbeb2cb tests: Fix redundant variables a80b5ce perf: Update example script, add logging for deserialization 1486dcd tests: Get tests to pass e019350 docs: Update docs to reflect accurate function descriptions 31a5076 Run yapf and ruff d68f128 chore: Remove todo 287bfbb chore: Fix yapf formatting f3393bd chore: Disable yapf from interfering with isort for testing script 04c78bf chore: Disable yapf at testing script import block 9658a1a fix: Instantiate load partials only when tensorizer imported 96af687 Copy link Collaborator Author sangstar commented Apr 22, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @Yard1 @ywang96 Some QoL improvements for tensorizer and some corrected docstrings (as per the great refactor from @Yard1 ), and an update for tensorizer as version 1.9.0 is officially released. No longer need to specify if a model is vLLM-tensorized beforehand, as I've implemented a way for this to be inferred implicitly by registering a meta tensor into the model during serialization with a vllm-tensorized-marker and removing it during deserialization. ðŸš€ 1 Yard1 reacted with rocket emoji All reactions ðŸš€ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sangstar added 5 commits April 22, 2024 14:29 Merge remote-tracking branch 'upstream/main' into sangstar/tensorizerâ€¦ â€¦ 5890ded â€¦-update\n\n# Conflicts:\n#\tdocs/source/models/engine_args.rst perf: Update and streamline docs on tensorizing a vLLM model b702901 docs: Correct docstring, add tensorizer docs link for more info 43a298a docs: Fix S3_ENDPOINT_URL naming 2a61b9a docs: Additionally fix S3_ENDPOINT_URL naming on example script 2b2012a Copy link Collaborator Author sangstar commented Apr 29, 2024 Further made some improvements with documentation. Important fixes explaining how to use tensorizer with the refactored changes (as the example script predates the refactor) so hoping to get eyes on this! Cheers :D @ywang96 @Yard1 ðŸ‘ 1 ywang96 reacted with thumbs up emoji All reactions ðŸ‘ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sangstar added 5 commits April 29, 2024 10:17 tests: Add tensorize_vllm_model.py to Examples Test for regression a1b5971 Merge remote-tracking branch 'upstream/main' into sangstar/tensorizerâ€¦ â€¦ 6e7bfae â€¦-update Run yapf and ruff, update docs 77817d1 perf: Force serialization and deserialization test in example script 19495cf fix: Not double-initiating model for deserialize case in example 1fe66be sangstar mentioned this pull request May 3, 2024 [Frontend] [Core] feat: Add model loading using tensorizer #3476 Merged Copy link Member ywang96 commented May 4, 2024 Will take a look once I have some bandwidth - thanks for the continuous contribution to vLLM! â¤ï¸ 1 sangstar reacted with heart emoji All reactions â¤ï¸ 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ywang96 self-assigned this May 4, 2024 sangstar added 2 commits May 6, 2024 09:28 Merge remote-tracking branch 'upstream/main' into sangstar/tensorizerâ€¦ â€¦ 449753c â€¦-update\n\n# Conflicts:\n#\trequirements-dev.txt\n#\tsetup.py\n#\ttests/tensorizer_loader/tensorize_vllm_model_for_testing.py chore: Update initializing env 9c2f7f8 bbrowning reviewed May 9, 2024 View reviewed changes examples/tensorize_vllm_model.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . ywang96 reviewed May 12, 2024 View reviewed changes Copy link Member ywang96 left a comment â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thank you @sangstar for the continuous contribution! I left some questions. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . â¤ï¸ 1 sangstar reacted with heart emoji All reactions â¤ï¸ 1 reaction examples/tensorize_vllm_model.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/model_loader/tensorizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/model_executor/model_loader/loader.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . chore: Reallow vllm_tensorized parameter, envs fix 246f636 sangstar requested a review\n  from ywang96 May 12, 2024 13:02 sangstar added 3 commits May 12, 2024 09:09 Merge remote-tracking branch 'refs/remotes/upstream/main' into sangstâ€¦ â€¦ a86ab10 â€¦ar/tensorizer-update chore: Install tensorizer for Examples Test 829e24b style: Remove trailing whitespace 7271ea2 Copy link Collaborator Author sangstar commented May 12, 2024 @ywang96 Resolved comments! Let me know if anything else is needed. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sangstar added 2 commits May 13, 2024 13:56 Merge remote-tracking branch 'upstream/main' into sangstar/tensorizerâ€¦ â€¦ ac7341e â€¦-update\n\n# Conflicts:\n#\tvllm/model_executor/model_loader/loader.py Run yapf and ruff 0abbe10 ywang96 reviewed May 13, 2024 View reviewed changes vllm/model_executor/model_loader/tensorizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . sangstar requested a review\n  from ywang96 May 13, 2024 19:48 Copy link Collaborator Author sangstar commented May 13, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @ywang96 Resolved comments! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ywang96 approved these changes May 13, 2024 View reviewed changes Copy link Member ywang96 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment ðŸš€ LGTM! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ðŸš€ 1 sangstar reacted with rocket emoji All reactions ðŸš€ 1 reaction Copy link Collaborator Author sangstar commented May 13, 2024 @ywang96 Checks passed and ready to merge! ðŸ˜„ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ywang96 merged commit 8bc68e1 into vllm-project : main May 13, 2024 sangstar deleted the sangstar/tensorizer-update branch May 14, 2024 14:04 robertgshaw2-redhat pushed a commit\n        to neuralmagic/nm-vllm\n      that referenced\n      this pull request May 19, 2024 [Frontend] [Core] perf: Automatically detect vLLM-tensorized model, uâ€¦ â€¦ 7dd2e73 â€¦pdate `tensorizer` to version 2.9.0 ( vllm-project#4208 ) dtrifiro pushed a commit\n        to dtrifiro/vllm\n      that referenced\n      this pull request May 21, 2024 [Frontend] [Core] perf: Automatically detect vLLM-tensorized model, uâ€¦ â€¦ 64d2fdc â€¦pdate `tensorizer` to version 2.9.0 ( vllm-project#4208 ) sangstar mentioned this pull request Jun 13, 2024 [Doc] Update documentation on Tensorizer #5471 Merged sangstar mentioned this pull request Jun 20, 2025 [Frontend] [Core] Integrate Tensorizer in to S3 loading machinery, allow passing arbitrary arguments during save/load #19619 Merged Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:54",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "SERVING: API server, OpenAI API server, Frontend | TEST: test, Test, test",
  "analysis_extracted_at": "2025-09-07 17:48:54",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)",
  "commit_message": "[Frontend] [Core] perf: Automatically detect vLLM-tensorized model, update `tensorizer` to version 2.9.0 (#4208)",
  "commit_date": "2024-05-13T14:57:07-07:00",
  "files_changed": [
    ".buildkite/test-pipeline.yaml",
    "examples/tensorize_vllm_model.py",
    "requirements-dev.txt",
    "setup.py",
    "tests/tensorizer_loader/tensorize_vllm_model_for_testing.py",
    "tests/tensorizer_loader/test_tensorizer.py",
    "vllm/engine/arg_utils.py",
    "vllm/envs.py",
    "vllm/model_executor/model_loader/loader.py",
    "vllm/model_executor/model_loader/tensorizer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 2,
    "num_non_test_files": 8,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 10,
    "num_hunks": 40,
    "num_edited_lines": 782,
    "num_non_test_edited_lines": 348,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 4feea786f..3c3da41c3 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -60,11 +60,13 @@ steps:\n   mirror_hardwares: [amd]\n   commands:\n     # install aws cli for llava_example.py\n-    - pip install awscli\n+    # install tensorizer for tensorize_vllm_model.py\n+    - pip install awscli tensorizer\n     - python3 offline_inference.py\n     - python3 offline_inference_with_prefix.py\n     - python3 llm_engine_example.py\n     - python3 llava_example.py\n+    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors\n \n - label: Kernels Test %N\n   command: pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT\ndiff --git a/examples/tensorize_vllm_model.py b/examples/tensorize_vllm_model.py\nindex e2456168d..8b74ae1d7 100644\n--- a/examples/tensorize_vllm_model.py\n+++ b/examples/tensorize_vllm_model.py\n@@ -1,23 +1,20 @@\n import argparse\n import dataclasses\n+import json\n import os\n-import time\n import uuid\n from functools import partial\n-from typing import Type\n \n-import torch\n-import torch.nn as nn\n-from tensorizer import (DecryptionParams, EncryptionParams, TensorDeserializer,\n-                        TensorSerializer, stream_io)\n-from tensorizer.utils import convert_bytes, get_mem_usage, no_init_or_tensor\n-from transformers import AutoConfig, PretrainedConfig\n+from tensorizer import stream_io\n \n-from vllm.distributed import initialize_model_parallel\n+from vllm import LLM\n+from vllm.distributed import (init_distributed_environment,\n+                              initialize_model_parallel)\n from vllm.engine.arg_utils import EngineArgs\n from vllm.engine.llm_engine import LLMEngine\n-from vllm.model_executor.model_loader.tensorizer import TensorizerArgs\n-from vllm.model_executor.models import ModelRegistry\n+from vllm.model_executor.model_loader.tensorizer import (TensorizerArgs,\n+                                                         TensorizerConfig,\n+                                                         serialize_vllm_model)\n \n # yapf conflicts with isort for this docstring\n # yapf: disable\n@@ -27,25 +24,25 @@ deserialize vLLM models. These models can be loaded using tensorizer\n to the GPU extremely quickly over an HTTP/HTTPS endpoint, an S3 endpoint,\n or locally. Tensor encryption and decryption is also supported, although \n libsodium must be installed to use it. Install vllm with tensorizer support \n-using `pip install vllm[tensorizer]`.\n+using `pip install vllm[tensorizer]`. To learn more about tensorizer, visit\n+https://github.com/coreweave/tensorizer\n \n To serialize a model, install vLLM from source, then run something \n like this from the root level of this repository:\n \n python -m examples.tensorize_vllm_model \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n+   --model facebook/opt-125m \\\n    serialize \\\n-   --serialized-directory s3://my-bucket/ \\\n-   --suffix vllm\n+   --serialized-directory s3://my-bucket \\\n+   --suffix v1\n    \n Which downloads the model from HuggingFace, loads it into vLLM, serializes it,\n and saves it to your S3 bucket. A local directory can also be used. This\n assumes your S3 credentials are specified as environment variables\n-in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and `S3_ENDPOINT`.\n-To provide S3 credentials directly, you can provide `--s3-access-key-id` and \n-`--s3-secret-access-key`, as well as `--s3-endpoint` as CLI args to this \n-script.\n+in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and \n+`S3_ENDPOINT_URL`. To provide S3 credentials directly, you can provide \n+`--s3-access-key-id` and `--s3-secret-access-key`, as well as `--s3-endpoint` \n+as CLI args to this script.\n \n You can also encrypt the model weights with a randomly-generated key by \n providing a `--keyfile` argument.\n@@ -57,7 +54,7 @@ python -m examples.tensorize_vllm_model \\\n    --model EleutherAI/gpt-j-6B \\\n    --dtype float16 \\\n    deserialize \\\n-   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/vllm/model.tensors\n+   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/v1/model.tensors\n \n Which downloads the model tensors from your S3 bucket and deserializes them.\n \n@@ -71,26 +68,30 @@ Or for deserializing:\n \n `python -m examples.tensorize_vllm_model deserialize --help`.\n \n-Once a model is serialized, it can be used to load the model when running the\n-OpenAI inference client at `vllm/entrypoints/openai/api_server.py` by providing\n-the `--tensorizer-uri` CLI argument that is functionally the same as the\n-`--path-to-tensors` argument in this script, along with `--vllm-tensorized`, to\n-signify that the model to be deserialized is a vLLM model, rather than a \n-HuggingFace `PreTrainedModel`, which can also be deserialized using tensorizer\n-in the same inference server, albeit without the speed optimizations. To\n-deserialize an encrypted file, the `--encryption-keyfile` argument can be used\n-to provide the path to the keyfile used to encrypt the model weights. For\n-information on all the arguments that can be used to configure tensorizer's\n-deserialization, check out the tensorizer options argument group in the\n-`vllm/entrypoints/openai/api_server.py` script with `--help`.\n-\n-Tensorizer can also be invoked with the `LLM` class directly to load models:\n+Once a model is serialized, tensorizer can be invoked with the `LLM` class \n+directly to load models:\n \n     llm = LLM(model=\"facebook/opt-125m\",\n               load_format=\"tensorizer\",\n-              tensorizer_uri=path_to_opt_tensors,\n-              num_readers=3,\n-              vllm_tensorized=True)\n+              model_loader_extra_config=TensorizerConfig(\n+                    tensorizer_uri = path_to_tensors,\n+                    num_readers=3,\n+                    )\n+              )\n+            \n+A serialized model can be used during model loading for the vLLM OpenAI\n+inference server. `model_loader_extra_config` is exposed as the CLI arg\n+`--model-loader-extra-config`, and accepts a JSON string literal of the\n+TensorizerConfig arguments desired.\n+\n+In order to see all of the available arguments usable to configure \n+loading with tensorizer that are given to `TensorizerConfig`, run:\n+\n+`python -m examples.tensorize_vllm_model deserialize --help`\n+\n+under the `tensorizer options` section. These can also be used for\n+deserialization in this example script, although `--tensorizer-uri` and\n+`--path-to-tensors` are functionally the same in this case.\n \"\"\"\n \n \n@@ -158,95 +159,35 @@ def parse_args():\n         help=(\"Path to a binary key to use to decrypt the model weights,\"\n               \" if the model was serialized with encryption\"))\n \n-    return parser.parse_args()\n-\n-\n-def make_model_contiguous(model):\n-    # Ensure tensors are saved in memory contiguously\n-    for param in model.parameters():\n-        param.data = param.data.contiguous()\n-\n-\n-def _get_vllm_model_architecture(config: PretrainedConfig) -> Type[nn.Module]:\n-    architectures = getattr(config, \"architectures\", [])\n-    for arch in architectures:\n-        model_cls = ModelRegistry.load_model_cls(arch)\n-        if model_cls is not None:\n-            return model_cls\n-    raise ValueError(\n-        f\"Model architectures {architectures} are not supported for now. \"\n-        f\"Supported architectures: {ModelRegistry.get_supported_archs()}\")\n-\n-\n-def serialize():\n-\n-    eng_args_dict = {f.name: getattr(args, f.name) for f in\n-                     dataclasses.fields(EngineArgs)}\n-    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n-    engine = LLMEngine.from_engine_args(engine_args)\n+    TensorizerArgs.add_cli_args(deserialize_parser)\n \n-    model = (engine.model_executor.driver_worker.\n-             model_runner.model)\n-\n-    encryption_params = EncryptionParams.random() if keyfile else None\n-    if keyfile:\n-        with _write_stream(keyfile) as stream:\n-            stream.write(encryption_params.key)\n-\n-    with _write_stream(model_path) as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-        serializer.close()\n+    return parser.parse_args()\n \n-    print(\"Serialization complete. Model tensors saved to\", model_path)\n-    if keyfile:\n-        print(\"Key saved to\", keyfile)\n \n \n def deserialize():\n-    config = AutoConfig.from_pretrained(model_ref)\n-\n-    with no_init_or_tensor():\n-        model_class = _get_vllm_model_architecture(config)\n-        model = model_class(config)\n-\n-    before_mem = get_mem_usage()\n-    start = time.time()\n-\n-    if keyfile:\n-        with _read_stream(keyfile) as stream:\n-            key = stream.read()\n-            decryption_params = DecryptionParams.from_key(key)\n-            tensorizer_args.deserializer_params['encryption'] = \\\n-                decryption_params\n-\n-    with (_read_stream(model_path)) as stream, TensorDeserializer(\n-            stream, **tensorizer_args.deserializer_params) as deserializer:\n-        deserializer.load_into_module(model)\n-        end = time.time()\n-\n-    # Brag about how fast we are.\n-    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n-    duration = end - start\n-    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n-    after_mem = get_mem_usage()\n-    print(\n-        f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\"\n+    llm = LLM(model=args.model,\n+              load_format=\"tensorizer\",\n+              model_loader_extra_config=tensorizer_config\n     )\n-    print(f\"Memory usage before: {before_mem}\")\n-    print(f\"Memory usage after: {after_mem}\")\n+    return llm\n \n-    return model\n \n \n args = parse_args()\n \n-s3_access_key_id = (args.s3_access_key_id or os.environ.get(\"S3_ACCESS_KEY_ID\")\n-                    or None)\n-s3_secret_access_key = (args.s3_secret_access_key\n-                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\") or None)\n+s3_access_key_id = (getattr(args, 's3_access_key_id', None)\n+                    or os.environ.get(\"S3_ACCESS_KEY_ID\", None))\n+s3_secret_access_key = (getattr(args, 's3_secret_access_key', None)\n+                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\", None))\n+s3_endpoint = (getattr(args, 's3_endpoint', None)\n+               or os.environ.get(\"S3_ENDPOINT_URL\", None))\n \n-s3_endpoint = (args.s3_endpoint or os.environ.get(\"S3_ENDPOINT_URL\") or None)\n+credentials = {\n+    \"s3_access_key_id\": s3_access_key_id,\n+    \"s3_secret_access_key\": s3_secret_access_key,\n+    \"s3_endpoint\": s3_endpoint\n+}\n \n _read_stream, _write_stream = (partial(\n     stream_io.open_stream,\n@@ -263,20 +204,41 @@ model_name = model_ref.split(\"/\")[1]\n os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n os.environ[\"MASTER_PORT\"] = \"8080\"\n \n-torch.distributed.init_process_group(world_size=1, rank=0)\n+init_distributed_environment(world_size=1, rank=0, local_rank=0)\n initialize_model_parallel()\n \n keyfile = args.keyfile if args.keyfile else None\n \n+\n+if args.model_loader_extra_config:\n+    config = json.loads(args.model_loader_extra_config)\n+    tensorizer_args = TensorizerConfig(**config)._construct_tensorizer_args()\n+    tensorizer_args.tensorizer_uri = args.path_to_tensors\n+else:\n+    tensorizer_args = None\n+\n if args.command == \"serialize\":\n+    eng_args_dict = {f.name: getattr(args, f.name) for f in\n+                     dataclasses.fields(EngineArgs)}\n+\n+    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n+    engine = LLMEngine.from_engine_args(engine_args)\n+\n     input_dir = args.serialized_directory.rstrip('/')\n     suffix = args.suffix if args.suffix else uuid.uuid4().hex\n     base_path = f\"{input_dir}/vllm/{model_ref}/{suffix}\"\n     model_path = f\"{base_path}/model.tensors\"\n-    serialize()\n+    tensorizer_config = TensorizerConfig(\n+        tensorizer_uri=model_path,\n+        **credentials)\n+    serialize_vllm_model(engine, tensorizer_config, keyfile)\n elif args.command == \"deserialize\":\n-    tensorizer_args = TensorizerArgs.from_cli_args(args)\n-    model_path = args.path_to_tensors\n+    if not tensorizer_args:\n+        tensorizer_config = TensorizerConfig(\n+            tensorizer_uri=args.path_to_tensors,\n+            encryption_keyfile = keyfile,\n+            **credentials\n+        )\n     deserialize()\n else:\n     raise ValueError(\"Either serialize or deserialize must be specified.\")\ndiff --git a/requirements-dev.txt b/requirements-dev.txt\nindex 796c9e37d..4f6c27d95 100644\n--- a/requirements-dev.txt\n+++ b/requirements-dev.txt\n@@ -14,7 +14,7 @@ types-setuptools\n \n # testing\n pytest\n-tensorizer==2.9.0\n+tensorizer>=2.9.0\n pytest-forked\n pytest-asyncio\n pytest-rerunfailures\ndiff --git a/setup.py b/setup.py\nindex 0dc8818b4..a66af2c5d 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -426,7 +426,7 @@ setup(\n     install_requires=get_requirements(),\n     ext_modules=ext_modules,\n     extras_require={\n-        \"tensorizer\": [\"tensorizer==2.9.0\"],\n+        \"tensorizer\": [\"tensorizer>=2.9.0\"],\n     },\n     cmdclass={\"build_ext\": cmake_build_ext} if not _is_neuron() else {},\n     package_data=package_data,\ndiff --git a/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py b/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py\ndeleted file mode 100644\nindex 0e113ab64..000000000\n--- a/tests/tensorizer_loader/tensorize_vllm_model_for_testing.py\n+++ /dev/null\n@@ -1,245 +0,0 @@\n-import argparse\n-import dataclasses\n-import os\n-import time\n-import uuid\n-from functools import partial\n-from typing import Type\n-\n-import torch.nn as nn\n-from tensorizer import (DecryptionParams, EncryptionParams, TensorDeserializer,\n-                        TensorSerializer, stream_io)\n-from tensorizer.utils import convert_bytes, get_mem_usage, no_init_or_tensor\n-from transformers import AutoConfig, PretrainedConfig\n-\n-from vllm.distributed import (init_distributed_environment,\n-                              initialize_model_parallel)\n-from vllm.engine.arg_utils import EngineArgs\n-from vllm.engine.llm_engine import LLMEngine\n-from vllm.model_executor.model_loader.tensorizer import TensorizerArgs\n-from vllm.model_executor.models import ModelRegistry\n-\n-# yapf conflicts with isort for this docstring\n-# yapf: disable\n-\"\"\"\n-tensorize_vllm_model.py is a script that can be used to serialize and \n-deserialize vLLM models. These models can be loaded using tensorizer directly \n-to the GPU extremely quickly. Tensor encryption and decryption is also \n-supported, although libsodium must be installed to use it. Install\n-vllm with tensorizer support using `pip install vllm[tensorizer]`.\n-\n-To serialize a model, you can run something like this:\n-\n-python tensorize_vllm_model.py \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n-   serialize \\\n-   --serialized-directory s3://my-bucket/ \\\n-   --suffix vllm\n-\n-Which downloads the model from HuggingFace, loads it into vLLM, serializes it,\n-and saves it to your S3 bucket. A local directory can also be used.\n-\n-You can also encrypt the model weights with a randomly-generated key by \n-providing a `--keyfile` argument.\n-\n-To deserialize a model, you can run something like this:\n-\n-python tensorize_vllm_model.py \\\n-   --model EleutherAI/gpt-j-6B \\\n-   --dtype float16 \\\n-   deserialize \\\n-   --path-to-tensors s3://my-bucket/vllm/EleutherAI/gpt-j-6B/vllm/model.tensors\n-\n-Which downloads the model tensors from your S3 bucket and deserializes them.\n-To provide S3 credentials, you can provide `--s3-access-key-id` and \n-`--s3-secret-access-key`, as well as `--s3-endpoint` as CLI args to this script,\n-the OpenAI entrypoint, as arguments for LLM(), or as environment variables\n-in the form of `S3_ACCESS_KEY_ID`, `S3_SECRET_ACCESS_KEY`, and `S3_ENDPOINT`.\n-\n-\n-You can also provide a `--keyfile` argument to decrypt the model weights if \n-they were serialized with encryption.\n-\n-For more information on the available arguments, run \n-`python tensorize_vllm_model.py --help`.\n-\"\"\"\n-\n-\n-def parse_args():\n-    parser = argparse.ArgumentParser(\n-        description=\"An example script that can be used to serialize and \"\n-                    \"deserialize vLLM models. These models \"\n-                    \"can be loaded using tensorizer directly to the GPU \"\n-                    \"extremely quickly. Tensor encryption and decryption is \"\n-                    \"also supported, although libsodium must be installed to \"\n-                    \"use it.\")\n-    parser = TensorizerArgs.add_cli_args(EngineArgs.add_cli_args(parser))\n-    subparsers = parser.add_subparsers(dest='command')\n-\n-    serialize_parser = subparsers.add_parser(\n-        'serialize', help=\"Serialize a model to `--serialized-directory`\")\n-\n-    serialize_parser.add_argument(\n-        \"--suffix\",\n-        type=str,\n-        required=False,\n-        help=(\n-            \"The suffix to append to the serialized model directory, which is \"\n-            \"used to construct the location of the serialized model tensors, \"\n-            \"e.g. if `--serialized-directory` is `s3://my-bucket/` and \"\n-            \"`--suffix` is `v1`, the serialized model tensors will be \"\n-            \"saved to \"\n-            \"`s3://my-bucket/vllm/EleutherAI/gpt-j-6B/v1/model.tensors`. \"\n-            \"If none is provided, a random UUID will be used.\"))\n-    serialize_parser.add_argument(\n-        \"--serialized-directory\",\n-        type=str,\n-        required=True)\n-\n-    serialize_parser.add_argument(\n-        \"--keyfile\",\n-        type=str,\n-        required=False,\n-        help=(\"Encrypt the model weights with a randomly-generated binary key,\"\n-              \" and save the key at this path\"))\n-\n-    deserialize_parser = subparsers.add_parser(\n-        'deserialize',\n-        help=(\"Deserialize a model from `--path-to-tensors`\"\n-              \" to verify it can be loaded and used.\"))\n-\n-    deserialize_parser.add_argument(\n-        \"--path-to-tensors\",\n-        type=str,\n-        required=True,\n-        help=\"The local path or S3 URI to the model tensors to deserialize. \")\n-\n-    deserialize_parser.add_argument(\n-        \"--keyfile\",\n-        type=str,\n-        required=False,\n-        help=(\"Path to a binary key to use to decrypt the model weights,\"\n-              \" if the model was serialized with encryption\"))\n-\n-    return parser.parse_args()\n-\n-\n-def make_model_contiguous(model):\n-    # Ensure tensors are saved in memory contiguously\n-    for param in model.parameters():\n-        param.data = param.data.contiguous()\n-\n-\n-def _get_vllm_model_architecture(config: PretrainedConfig) -> Type[nn.Module]:\n-    architectures = getattr(config, \"architectures\", [])\n-    for arch in architectures:\n-        model_cls = ModelRegistry.load_model_cls(arch)\n-        if model_cls is not None:\n-            return model_cls\n-    raise ValueError(\n-        f\"Model architectures {architectures} are not supported for now. \"\n-        f\"Supported architectures: {ModelRegistry.get_supported_archs()}\")\n-\n-\n-def serialize():\n-    eng_args_dict = {f.name: getattr(args, f.name) for f in\n-                     dataclasses.fields(EngineArgs)}\n-    engine_args = EngineArgs.from_cli_args(argparse.Namespace(**eng_args_dict))\n-    engine = LLMEngine.from_engine_args(engine_args)\n-\n-    model = (engine.model_executor.driver_worker.\n-             model_runner.model)\n-\n-    encryption_params = EncryptionParams.random() if keyfile else None\n-    if keyfile:\n-        with _write_stream(keyfile) as stream:\n-            stream.write(encryption_params.key)\n-\n-    with _write_stream(model_path) as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-        serializer.close()\n-\n-    print(\"Serialization complete. Model tensors saved to\", model_path)\n-    if keyfile:\n-        print(\"Key saved to\", keyfile)\n-\n-\n-def deserialize():\n-    config = AutoConfig.from_pretrained(model_ref)\n-\n-    with no_init_or_tensor():\n-        model_class = _get_vllm_model_architecture(config)\n-        model = model_class(config)\n-\n-    before_mem = get_mem_usage()\n-    start = time.time()\n-\n-    if keyfile:\n-        with _read_stream(keyfile) as stream:\n-            key = stream.read()\n-            decryption_params = DecryptionParams.from_key(key)\n-            tensorizer_args.deserializer_params['encryption'] = \\\n-                decryption_params\n-\n-    with (_read_stream(model_path)) as stream, TensorDeserializer(\n-            stream, **tensorizer_args.deserializer_params) as deserializer:\n-        deserializer.load_into_module(model)\n-        end = time.time()\n-\n-    # Brag about how fast we are.\n-    total_bytes_str = convert_bytes(deserializer.total_tensor_bytes)\n-    duration = end - start\n-    per_second = convert_bytes(deserializer.total_tensor_bytes / duration)\n-    after_mem = get_mem_usage()\n-    print(\n-        f\"Deserialized {total_bytes_str} in {end - start:0.2f}s, {per_second}/s\"\n-    )\n-    print(f\"Memory usage before: {before_mem}\")\n-    print(f\"Memory usage after: {after_mem}\")\n-\n-    return model\n-\n-\n-args = parse_args()\n-\n-s3_access_key_id = (args.s3_access_key_id or os.environ.get(\"S3_ACCESS_KEY_ID\")\n-                    or None)\n-s3_secret_access_key = (args.s3_secret_access_key\n-                        or os.environ.get(\"S3_SECRET_ACCESS_KEY\") or None)\n-\n-s3_endpoint = (args.s3_endpoint or os.environ.get(\"S3_ENDPOINT_URL\") or None)\n-\n-_read_stream, _write_stream = (partial(\n-    stream_io.open_stream,\n-    mode=mode,\n-    s3_access_key_id=s3_access_key_id,\n-    s3_secret_access_key=s3_secret_access_key,\n-    s3_endpoint=s3_endpoint,\n-) for mode in (\"rb\", \"wb+\"))\n-\n-model_ref = args.model\n-\n-model_name = model_ref.split(\"/\")[1]\n-\n-os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n-os.environ[\"MASTER_PORT\"] = \"8080\"\n-\n-init_distributed_environment(world_size=1, rank=0, local_rank=0)\n-initialize_model_parallel()\n-\n-keyfile = args.keyfile if args.keyfile else None\n-\n-if args.command == \"serialize\":\n-    input_dir = args.serialized_directory.rstrip('/')\n-    suffix = args.suffix if args.suffix else uuid.uuid4().hex\n-    base_path = f\"{input_dir}/vllm/{model_ref}/{suffix}\"\n-    model_path = f\"{base_path}/model.tensors\"\n-    serialize()\n-elif args.command == \"deserialize\":\n-    tensorizer_args = TensorizerArgs.from_cli_args(args)\n-    model_path = args.path_to_tensors\n-    deserialize()\n-else:\n-    raise ValueError(\"Either serialize or deserialize must be specified.\")\ndiff --git a/tests/tensorizer_loader/test_tensorizer.py b/tests/tensorizer_loader/test_tensorizer.py\nindex ad4748c5e..1579d53a7 100644\n--- a/tests/tensorizer_loader/test_tensorizer.py\n+++ b/tests/tensorizer_loader/test_tensorizer.py\n@@ -10,12 +10,19 @@ import ray\n import torch\n \n from vllm import SamplingParams\n-from vllm.model_executor.model_loader.tensorizer import (\n-    EncryptionParams, TensorizerConfig, TensorSerializer,\n-    is_vllm_serialized_tensorizer, load_with_tensorizer, open_stream)\n+# yapf: disable\n+from vllm.model_executor.model_loader.tensorizer import (TensorizerConfig,\n+                                                         TensorSerializer,\n+                                                         is_vllm_tensorized,\n+                                                         load_with_tensorizer,\n+                                                         open_stream,\n+                                                         serialize_vllm_model)\n \n from ..utils import ServerRunner\n \n+# yapf conflicts with isort for this docstring\n+\n+\n prompts = [\n     \"Hello, my name is\",\n     \"The president of the United States is\",\n@@ -40,7 +47,7 @@ def is_curl_installed():\n \n @pytest.fixture(autouse=True)\n def tensorizer_config():\n-    config = TensorizerConfig(tensorizer_uri=\"vllm\", vllm_tensorized=True)\n+    config = TensorizerConfig(tensorizer_uri=\"vllm\")\n     return config\n \n \n@@ -59,47 +66,6 @@ def test_load_with_tensorizer(mock_agent, tensorizer_config):\n     assert result == mock_agent_instance.deserialize.return_value\n \n \n-def test_is_vllm_model_with_vllm_in_uri(tensorizer_config):\n-    tensorizer_config.vllm_tensorized = True\n-\n-    result = is_vllm_serialized_tensorizer(tensorizer_config)\n-\n-    assert result is True\n-\n-\n-def test_is_vllm_model_without_vllm_in_uri(tensorizer_config):\n-    tensorizer_config.vllm_tensorized = False\n-\n-    result = is_vllm_serialized_tensorizer(tensorizer_config)\n-\n-    assert result is False\n-\n-\n-def test_deserialized_vllm_model_has_same_outputs(vllm_runner, tmp_path):\n-    vllm_model = vllm_runner(model_ref)\n-    model_path = tmp_path / (model_ref + \".tensors\")\n-    outputs = vllm_model.generate(prompts, sampling_params)\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream)\n-        serializer.write_module(model)\n-    del vllm_model, model\n-    gc.collect()\n-    torch.cuda.empty_cache()\n-    loaded_vllm_model = vllm_runner(\n-        model_ref,\n-        load_format=\"tensorizer\",\n-        model_loader_extra_config=TensorizerConfig(tensorizer_uri=model_path,\n-                                                   num_readers=1,\n-                                                   vllm_tensorized=True),\n-    )\n-    deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n-\n-    # Assumes SamplingParams being seeded ensures the outputs are deterministic\n-    assert outputs == deserialized_outputs\n-\n-\n @pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n def test_can_deserialize_s3(vllm_runner):\n     model_ref = \"EleutherAI/pythia-1.4b\"\n@@ -110,7 +76,6 @@ def test_can_deserialize_s3(vllm_runner):\n                                   model_loader_extra_config=TensorizerConfig(\n                                       tensorizer_uri=tensorized_path,\n                                       num_readers=1,\n-                                      vllm_tensorized=False,\n                                       s3_endpoint=\"object.ord1.coreweave.com\",\n                                   ))\n \n@@ -126,29 +91,26 @@ def test_deserialized_encrypted_vllm_model_has_same_outputs(\n     model_path = tmp_path / (model_ref + \".tensors\")\n     key_path = tmp_path / (model_ref + \".key\")\n     outputs = vllm_model.generate(prompts, sampling_params)\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n \n-    encryption_params = EncryptionParams.random()\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream, encryption=encryption_params)\n-        serializer.write_module(model)\n-    with open_stream(key_path, \"wb+\") as stream:\n-        stream.write(encryption_params.key)\n-    del vllm_model, model\n+    config_for_serializing = TensorizerConfig(tensorizer_uri=model_path)\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         config_for_serializing,\n+                         encryption_key_path=key_path)\n+\n+    del vllm_model\n     gc.collect()\n     torch.cuda.empty_cache()\n-    loaded_vllm_model = vllm_runner(model_ref,\n-                                    load_format=\"tensorizer\",\n-                                    model_loader_extra_config=TensorizerConfig(\n-                                        tensorizer_uri=model_path,\n-                                        encryption_keyfile=key_path,\n-                                        num_readers=1,\n-                                        vllm_tensorized=True))\n+\n+    config_for_deserializing = TensorizerConfig(tensorizer_uri=model_path,\n+                                                encryption_keyfile=key_path)\n+\n+    loaded_vllm_model = vllm_runner(\n+        model_ref,\n+        load_format=\"tensorizer\",\n+        model_loader_extra_config=config_for_deserializing)\n \n     deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n \n-    # Assumes SamplingParams being seeded ensures the outputs are deterministic\n     assert outputs == deserialized_outputs\n \n \n@@ -169,7 +131,7 @@ def test_deserialized_hf_model_has_same_outputs(hf_runner, vllm_runner,\n                                   model_loader_extra_config=TensorizerConfig(\n                                       tensorizer_uri=model_path,\n                                       num_readers=1,\n-                                      vllm_tensorized=False))\n+                                  ))\n \n     deserialized_outputs = loaded_hf_model.generate_greedy(\n         prompts, max_tokens=max_tokens)\n@@ -190,12 +152,11 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n     # Serialize model before deserializing and binding LoRA adapters\n     vllm_model = vllm_runner(model_ref, )\n     model_path = tmp_path / (model_ref + \".tensors\")\n-    model = (vllm_model.model.llm_engine.model_executor.driver_worker.\n-             model_runner.model)\n-    with open_stream(model_path, \"wb+\") as stream:\n-        serializer = TensorSerializer(stream)\n-        serializer.write_module(model)\n-    del vllm_model, model\n+\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         TensorizerConfig(tensorizer_uri=model_path))\n+\n+    del vllm_model\n     gc.collect()\n     torch.cuda.empty_cache()\n     loaded_vllm_model = vllm_runner(\n@@ -204,7 +165,6 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n         model_loader_extra_config=TensorizerConfig(\n             tensorizer_uri=model_path,\n             num_readers=1,\n-            vllm_tensorized=True,\n         ),\n         enable_lora=True,\n         max_loras=1,\n@@ -220,58 +180,28 @@ def test_vllm_model_can_load_with_lora(vllm_runner, tmp_path):\n \n def test_load_without_tensorizer_load_format(vllm_runner):\n     with pytest.raises(ValueError):\n-        vllm_runner(model_ref,\n-                    model_loader_extra_config=TensorizerConfig(\n-                        tensorizer_uri=\"test\", vllm_tensorized=False))\n-\n-\n-@pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n-def test_tensorize_vllm_model(tmp_path):\n-    # Test serialize command\n-    serialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"serialize\", \"--serialized-directory\", tmp_path,\n-        \"--suffix\", \"tests\"\n-    ]\n-    result = subprocess.run(serialize_args, capture_output=True, text=True)\n-    print(result.stdout)  # Print the output of the serialize command\n-\n-    assert result.returncode == 0, (f\"Serialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n-\n-    path_to_tensors = f\"{tmp_path}/vllm/{model_ref}/tests/model.tensors\"\n-\n-    # Test deserialize command\n-    deserialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"deserialize\", \"--path-to-tensors\",\n-        path_to_tensors\n-    ]\n-    result = subprocess.run(deserialize_args, capture_output=True, text=True)\n-    assert result.returncode == 0, (f\"Deserialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n+        vllm_runner(\n+            model_ref,\n+            model_loader_extra_config=TensorizerConfig(tensorizer_uri=\"test\"))\n \n \n @pytest.mark.skipif(not is_curl_installed(), reason=\"cURL is not installed\")\n-def test_openai_apiserver_with_tensorizer(tmp_path):\n+def test_openai_apiserver_with_tensorizer(vllm_runner, tmp_path):\n     ## Serialize model\n-    serialize_args = [\n-        \"python3\", tensorize_model_for_testing_script, \"--model\", model_ref,\n-        \"--dtype\", \"float16\", \"serialize\", \"--serialized-directory\", tmp_path,\n-        \"--suffix\", \"tests\"\n-    ]\n-    result = subprocess.run(serialize_args, capture_output=True, text=True)\n-    print(result.stdout)  # Print the output of the serialize command\n+    vllm_model = vllm_runner(model_ref, )\n+    model_path = tmp_path / (model_ref + \".tensors\")\n \n-    assert result.returncode == 0, (f\"Serialize command failed with output:\"\n-                                    f\"\\n{result.stdout}\\n{result.stderr}\")\n+    serialize_vllm_model(vllm_model.model.llm_engine,\n+                         TensorizerConfig(tensorizer_uri=model_path))\n \n-    path_to_tensors = f\"{tmp_path}/vllm/{model_ref}/tests/model.tensors\"\n     model_loader_extra_config = {\n-        \"tensorizer_uri\": path_to_tensors,\n-        \"vllm_tensorized\": True\n+        \"tensorizer_uri\": str(model_path),\n     }\n \n+    del vllm_model\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+\n     ## Start OpenAI API server\n     openai_args = [\n         \"--model\", model_ref, \"--dtype\", \"float16\", \"--load-format\",\n@@ -304,10 +234,10 @@ def test_openai_apiserver_with_tensorizer(tmp_path):\n \n def test_raise_value_error_on_invalid_load_format(vllm_runner):\n     with pytest.raises(ValueError):\n-        vllm_runner(model_ref,\n-                    load_format=\"safetensors\",\n-                    model_loader_extra_config=TensorizerConfig(\n-                        tensorizer_uri=\"test\", vllm_tensorized=False))\n+        vllm_runner(\n+            model_ref,\n+            load_format=\"safetensors\",\n+            model_loader_extra_config=TensorizerConfig(tensorizer_uri=\"test\"))\n \n \n def test_tensorizer_with_tp(vllm_runner):\n@@ -321,8 +251,29 @@ def test_tensorizer_with_tp(vllm_runner):\n             model_loader_extra_config=TensorizerConfig(\n                 tensorizer_uri=tensorized_path,\n                 num_readers=1,\n-                vllm_tensorized=False,\n                 s3_endpoint=\"object.ord1.coreweave.com\",\n             ),\n             tensor_parallel_size=2,\n         )\n+\n+\n+def test_vllm_tensorized_model_has_same_outputs(vllm_runner, tmp_path):\n+    model_ref = \"facebook/opt-125m\"\n+    model_path = tmp_path / (model_ref + \".tensors\")\n+    config = TensorizerConfig(tensorizer_uri=str(model_path))\n+\n+    vllm_model = vllm_runner(model_ref)\n+    outputs = vllm_model.generate(prompts, sampling_params)\n+    serialize_vllm_model(vllm_model.model.llm_engine, config)\n+\n+    assert is_vllm_tensorized(config)\n+    del vllm_model\n+    gc.collect()\n+    torch.cuda.empty_cache()\n+\n+    loaded_vllm_model = vllm_runner(model_ref,\n+                                    load_format=\"tensorizer\",\n+                                    model_loader_extra_config=config)\n+    deserialized_outputs = loaded_vllm_model.generate(prompts, sampling_params)\n+\n+    assert outputs == deserialized_outputs\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 163723b4b..fd5338c46 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -167,8 +167,8 @@ class EngineArgs:\n             '* \"dummy\" will initialize the weights with random values, '\n             'which is mainly for profiling.\\n'\n             '* \"tensorizer\" will load the weights using tensorizer from '\n-            'CoreWeave which assumes tensorizer_uri is set to the location of '\n-            'the serialized weights.')\n+            'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n+            'section for more information.\\n')\n         parser.add_argument(\n             '--dtype',\n             type=str,\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 91cc8f3be..68d8a074d 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -145,7 +145,7 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n \n     # S3 access information, used for tensorizer to load model from S3\n     \"S3_ACCESS_KEY_ID\":\n-    lambda: os.environ.get(\"S3_ACCESS_KEY\", None),\n+    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n     \"S3_SECRET_ACCESS_KEY\":\n     lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n     \"S3_ENDPOINT_URL\":\ndiff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py\nindex fc9c8aa0a..b14824a35 100644\n--- a/vllm/model_executor/model_loader/loader.py\n+++ b/vllm/model_executor/model_loader/loader.py\n@@ -17,7 +17,7 @@ from vllm.logger import init_logger\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig)\n from vllm.model_executor.model_loader.tensorizer import (\n-    TensorizerConfig, is_vllm_serialized_tensorizer, load_with_tensorizer,\n+    TensorizerConfig, is_vllm_tensorized, load_with_tensorizer,\n     tensorizer_weights_iterator)\n from vllm.model_executor.model_loader.utils import (get_model_architecture,\n                                                     set_default_torch_dtype)\n@@ -291,7 +291,7 @@ class TensorizerLoader(BaseModelLoader):\n         tensorizer_args = self.tensorizer_config._construct_tensorizer_args()\n         return tensorizer_weights_iterator(tensorizer_args)\n \n-    def _load_model_unserialized(\n+    def _load_model_serialized_cpu(\n         self,\n         model_config: ModelConfig,\n         device_config: DeviceConfig,\n@@ -299,11 +299,12 @@ class TensorizerLoader(BaseModelLoader):\n         vision_language_config: Optional[VisionLanguageConfig],\n         cache_config: CacheConfig,\n     ) -> nn.Module:\n-        \"\"\"Load an unserialized model with tensorizer.\n+        \"\"\"Load a serialized model with tensorizer to the CPU.\n \n-        Unserialized here means \"not serialized with tensorizer\". This\n-        should still be faster than default HuggingFace loading, but will\n-        be slower than loading a tensorizer-serialized model.\n+        This is only necessary when the model isn't vLLM-tensorized (see\n+        examples/tensorize_vllm_model.py) This should still be faster than\n+        default HuggingFace loading, but will be slower than loading a\n+        vLLM-tensorized model.\n         \"\"\"\n         with set_default_torch_dtype(model_config.dtype):\n             with torch.device(device_config.device):\n@@ -324,8 +325,9 @@ class TensorizerLoader(BaseModelLoader):\n     ) -> nn.Module:\n         \"\"\"Load a serialized model with tensorizer.\n \n-        See the examples/tensorize_vllm_model.py example \"\n-        script for serializing vLLM models.\"\"\"\n+        Expects a vLLM-tensorized model. See the\n+        examples/tensorize_vllm_model.py example script\n+        for serializing vLLM models.\"\"\"\n         with set_default_torch_dtype(model_config.dtype):\n             with torch.device(device_config.device):\n                 model_class = get_model_architecture(model_config)[0]\n@@ -353,15 +355,15 @@ class TensorizerLoader(BaseModelLoader):\n                    cache_config: CacheConfig) -> nn.Module:\n         self._verify_config(model_config, parallel_config)\n \n-        if is_vllm_serialized_tensorizer(self.tensorizer_config):\n+        if is_vllm_tensorized(self.tensorizer_config):\n             return self._load_model_serialized(model_config, device_config,\n                                                lora_config,\n                                                vision_language_config,\n                                                cache_config)\n-        return self._load_model_unserialized(model_config, device_config,\n-                                             lora_config,\n-                                             vision_language_config,\n-                                             cache_config)\n+        return self._load_model_serialized_cpu(model_config, device_config,\n+                                               lora_config,\n+                                               vision_language_config,\n+                                               cache_config)\n \n \n def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:\ndiff --git a/vllm/model_executor/model_loader/tensorizer.py b/vllm/model_executor/model_loader/tensorizer.py\nindex 219a2a392..2cf4ce5f8 100644\n--- a/vllm/model_executor/model_loader/tensorizer.py\n+++ b/vllm/model_executor/model_loader/tensorizer.py\n@@ -5,6 +5,7 @@ import os\n import time\n import typing\n from dataclasses import dataclass\n+from functools import partial\n from typing import Generator, Optional, Tuple, Type, Union\n \n import torch\n@@ -13,6 +14,7 @@ from transformers import PretrainedConfig\n \n import vllm.envs as envs\n from vllm.config import ModelConfig, ParallelConfig\n+from vllm.engine.llm_engine import LLMEngine\n from vllm.logger import init_logger\n from vllm.model_executor.layers.quantization.base_config import (\n     QuantizationConfig)\n@@ -27,6 +29,11 @@ try:\n     from tensorizer.stream_io import open_stream\n     from tensorizer.utils import (convert_bytes, get_mem_usage,\n                                   no_init_or_tensor)\n+\n+    _read_stream, _write_stream = (partial(\n+        open_stream,\n+        mode=mode,\n+    ) for mode in (\"rb\", \"wb+\"))\n except ImportError as e:\n     tensorizer_error_msg = str(e)\n \n@@ -43,7 +50,7 @@ logger = init_logger(__name__)\n class TensorizerConfig:\n     tensorizer_uri: Union[io.BufferedIOBase, io.RawIOBase, typing.BinaryIO,\n                           str, bytes, os.PathLike, int]\n-    vllm_tensorized: bool\n+    vllm_tensorized: Optional[bool] = False\n     verify_hash: Optional[bool] = False\n     num_readers: Optional[int] = None\n     encryption_keyfile: Optional[str] = None\n@@ -93,17 +100,11 @@ def load_with_tensorizer(tensorizer_config: TensorizerConfig,\n     return tensorizer.deserialize()\n \n \n-def is_vllm_serialized_tensorizer(tensorizer_config: TensorizerConfig) -> bool:\n-    if tensorizer_config is None:\n-        return False\n-    return tensorizer_config.vllm_tensorized\n-\n-\n @dataclass\n class TensorizerArgs:\n     tensorizer_uri: Union[io.BufferedIOBase, io.RawIOBase, typing.BinaryIO,\n                           str, bytes, os.PathLike, int]\n-    vllm_tensorized: bool\n+    vllm_tensorized: Optional[bool] = False\n     verify_hash: Optional[bool] = False\n     num_readers: Optional[int] = None\n     encryption_keyfile: Optional[str] = None\n@@ -121,7 +122,9 @@ class TensorizerArgs:\n           vLLM model. This is used to determine the behavior of the \n           TensorDeserializer when loading tensors from a serialized model.\n           It is far faster to deserialize a vLLM model as it utilizes\n-          tensorizer's optimized GPU loading.\n+          tensorizer's optimized GPU loading. Note that this is now\n+          deprecated, as serialized vLLM models are now automatically\n+          inferred as vLLM models.\n       verify_hash: If True, the hashes of each tensor will be verified against \n           the hashes stored in the metadata. A `HashMismatchError` will be \n           raised if any of the hashes do not match.\n@@ -158,6 +161,7 @@ class TensorizerArgs:\n             \"encryption\": self.encryption_keyfile,\n             \"num_readers\": self.num_readers\n         }\n+\n         if self.encryption_keyfile:\n             with open_stream(\n                     self.encryption_keyfile,\n@@ -177,7 +181,14 @@ class TensorizerArgs:\n             'tensorizer options',\n             description=('Options for configuring the behavior of the'\n                          ' tensorizer deserializer when '\n-                         '--load-format=tensorizer'))\n+                         'load_format=tensorizer is specified when '\n+                         'initializing an LLMEngine, either via the CLI '\n+                         'when running the vLLM OpenAI inference server '\n+                         'with a JSON string passed to '\n+                         '--model-loader-extra-config or as arguments given '\n+                         'to TensorizerConfig when passed to '\n+                         'model_loader_extra_config in the constructor '\n+                         'for LLMEngine.'))\n \n         group.add_argument(\n             \"--tensorizer-uri\",\n@@ -222,13 +233,6 @@ class TensorizerArgs:\n             help=\"The endpoint for the S3 bucket. Can also be set via the \"\n             \"S3_ENDPOINT_URL environment variable.\",\n         )\n-        group.add_argument(\n-            \"--vllm-tensorized\",\n-            action=\"store_true\",\n-            help=\"If enabled, indicates that the serialized model is a vLLM \"\n-            \"model. This is used to determine the behavior of the \"\n-            \"TensorDeserializer when loading tensors from a \"\n-            \"serialized model.\")\n \n         return parser\n \n@@ -322,10 +326,9 @@ class TensorizerAgent:\n         \"\"\"\n         before_mem = get_mem_usage()\n         start = time.perf_counter()\n-        with open_stream(\n-                self.tensorizer_args.tensorizer_uri,\n-                mode=\"rb\",\n-                **self.tensorizer_args.stream_params,\n+        with _read_stream(\n+                self.tensorizer_config.tensorizer_uri,\n+                **self.tensorizer_args.stream_params\n         ) as stream, TensorDeserializer(\n                 stream,\n                 dtype=self.tensorizer_config.dtype,\n@@ -345,6 +348,7 @@ class TensorizerAgent:\n \n         self._check_tensors_on_meta_device()\n         self._resize_lora_embeddings()\n+        del self.model.vllm_tensorized_marker\n         return self.model.eval()\n \n \n@@ -366,3 +370,63 @@ def tensorizer_weights_iterator(\n         for name, param in state.items():\n             yield name, param\n     del state\n+\n+\n+def is_vllm_tensorized(tensorizer_config: \"TensorizerConfig\") -> bool:\n+    \"\"\"\n+    Infer if the model is a vLLM model by checking the weights for\n+    a vLLM tensorized marker.\n+\n+    Args:\n+        tensorizer_config: The TensorizerConfig object containing the\n+            tensorizer_uri to the serialized model.\n+\n+    Returns:\n+        bool: True if the model is a vLLM model, False otherwise.\n+    \"\"\"\n+    tensorizer_args = tensorizer_config._construct_tensorizer_args()\n+    deserializer = TensorDeserializer(open_stream(\n+        tensorizer_args.tensorizer_uri, **tensorizer_args.stream_params),\n+                                      **tensorizer_args.deserializer_params,\n+                                      lazy_load=True)\n+    if tensorizer_config.vllm_tensorized:\n+        logger.warning(\n+            \"Please note that newly serialized vLLM models are automatically \"\n+            \"inferred as vLLM models, so setting vllm_tensorized=True is \"\n+            \"only necessary for models serialized prior to this change.\")\n+        return True\n+    if (\".vllm_tensorized_marker\" in deserializer):\n+        return True\n+    return False\n+\n+\n+def get_pretensorized_vllm_model(engine: \"LLMEngine\") -> nn.Module:\n+    model = (engine.model_executor.driver_worker.model_runner.model)\n+    model.register_parameter(\n+        \"vllm_tensorized_marker\",\n+        nn.Parameter(torch.tensor((1, ), device=\"meta\"), requires_grad=False))\n+    return model\n+\n+\n+def serialize_vllm_model(engine: \"LLMEngine\",\n+                         tensorizer_config : TensorizerConfig,\n+                         encryption_key_path: Optional[str] = None) \\\n+        -> nn.Module:\n+\n+    model = get_pretensorized_vllm_model(engine)\n+    tensorizer_args = tensorizer_config._construct_tensorizer_args()\n+    encryption_params = None\n+    if encryption_key_path is not None:\n+        encryption_params = EncryptionParams.random()\n+        with _write_stream(encryption_key_path,\n+                           **tensorizer_args.stream_params) as stream:\n+            stream.write(encryption_params.key)\n+\n+    with _write_stream(tensorizer_args.tensorizer_uri,\n+                       **tensorizer_args.stream_params) as stream:\n+        serializer = TensorSerializer(stream, encryption=encryption_params)\n+        serializer.write_module(model)\n+        serializer.close()\n+    logger.info(\"Successfully serialized model to %s\",\n+                str(tensorizer_args.tensorizer_uri))\n+    return model",
  "apis": [
    "vllm.LLM",
    "vllm.model_executor.model_loader.tensorizer.TensorizerConfig",
    "vllm.model_executor.model_loader.tensorizer.serialize_vllm_model",
    "vllm.model_executor.model_loader.tensorizer.is_vllm_tensorized"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/others/tensorize_vllm_model.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/model_loader/tensorizer.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit significantly revises the model serialization/deserialization workflow by introducing automatic detection of vLLM-tensorized models and consolidates the behavior into the tensorizer API. The changes are made in non-test files (e.g., examples, core model loaders, environment configuration) and modify performanceâ€critical code paths related to model loading on the CPU. Although the commit message includes â€œperf:â€ and the word â€œoptimizeâ€ isnâ€™t directly in the code, the alterations (like updating function names to reflect serialized CPU loading vs. unserialized loading and removing legacy flags) indicate an intent to improve the performance of model loading. The changes go beyond simple refactoring or bug fixes; they alter the internal API to streamline operations that are performance sensitive, while the modifications remain testable on CPU.",
  "llm_api_reason": "This commit mostly updates the tensorizer integration and its usage in serialization/deserialization of vLLM models. The changes update the example script and tests to automatically detect vLLMâ€tensorized models (removing manual flags), adjust CLI instructions, and update help messages. In addition, the commit replaces the old â€œis_vllm_serialized_tensorizerâ€ check with a new â€œis_vllm_tensorizedâ€ function and refactors the serialization helper to automatically add the tensorized marker. These updates affect core model loading via LLM and tensorizer configuration and serialization APIs."
}