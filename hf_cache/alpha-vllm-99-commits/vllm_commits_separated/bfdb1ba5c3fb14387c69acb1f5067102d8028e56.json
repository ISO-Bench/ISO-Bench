{
  "commit_hash": "bfdb1ba5c3fb14387c69acb1f5067102d8028e56",
  "pr_url": "https://github.com/vllm-project/vllm/pull/3469",
  "pr_date": null,
  "timeline_text": "Copy link Collaborator Yard1 commented Mar 18, 2024 PR Checklist (Click to expand. Please read before submitting.) Thank you for your contribution to vLLM! Before submitting the pull request, please ensure the PR meets the following criteria. This helps vLLM maintain the code quality and improve the efficiency of the review process. PR Title and Classification Only specific types of PRs will be reviewed. The PR title is prefixed appropriately to indicate the type of change. Please use one of the following: [Bugfix] for bug fixes. [CI/Build] for build or continuous integration improvements. [Doc] for documentation fixes and improvements. [Model] for adding a new model or improving an existing model. Model name should appear in the title. [Frontend] For changes on the vLLM frontend (e.g., OpenAI API server, LLM class, etc.) [Kernel] for changes affecting CUDA kernels or other compute kernels. [Core] for changes in the core vLLM logic (e.g., LLMEngine , AsyncLLMEngine , Scheduler , etc.) [Hardware][Vendor] for hardware-specific changes. Vendor name should appear in the prefix (e.g., [Hardware][AMD] ). [Misc] for PRs that do not fit the above categories. Please use this sparingly. Note: If the PR spans more than one category, please include all relevant prefixes. Code Quality The PR need to meet the following code quality standards: We adhere to Google Python style guide and Google C++ style guide . Pass all linter checks. Please use format.sh to format your code. The code need to be well-documented to ensure future contributors can easily understand the code. Include sufficient tests to ensure the project to stay correct and robust. This includes both unit tests and integration tests. Please add documentation to docs/source/ if the PR modifies the user-facing behaviors of vLLM. It helps vLLM user understand and utilize the new features or changes. Notes for Large Changes Please keep the changes as concise as possible. For major architectural changes (>500 LOC excluding kernel/data/config/test), we would expect a GitHub issue (RFC) discussing the technical design and justification. Otherwise, we will tag it with rfc-required and might not go through the PR. What to Expect for the Reviews The goal of the vLLM team is to be a transparent reviewing machine . We would like to make the review process transparent and efficient and make sure no contributor feel confused or frustrated. However, the vLLM team is small, so we need to prioritize some PRs over others. Here is what you can expect from the review process: After the PR is submitted, the PR will be assigned to a reviewer. Every reviewer will pick up the PRs based on their expertise and availability. After the PR is assigned, the reviewer will provide status update every 2-3 days. If the PR is not reviewed within 7 days, please feel free to ping the reviewer or the vLLM team. After the review, the reviewer will put an action-required label on the PR if there are changes required. The contributor should address the comments and ping the reviewer to re-review the PR. Please respond to all comments within a reasonable time frame. If a comment isn't clear or you disagree with a suggestion, feel free to ask for clarification or discuss the suggestion. Thank You Finally, thank you for taking the time to read these guidelines and for your interest in contributing to vLLM. Your contributions make vLLM a great tool for everyone! This PR improved detokenization performance for the prefill step by doing the following: Avoiding the detokenization of the entire prompt when unnecessary Improving logprob token detokenization to avoid repeated computation Making prompt logprob detokenization incremental In order to facilitate testing, the detokenization logic is moved to its own abstraction. Benchmark results (BS=1, the gain will be linear depending on the number of input tokens in a batch) on a single A10 GPU, with 5 logprobs to decode: python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1 Before PR: Avg latency: 0.292 seconds After PR: Avg latency: 0.287 seconds Benchmark results on a single A10 GPU, with 5 prompt logprobs to decode: python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1 Before PR: Avg latency: 2.133 seconds After PR: Avg latency: 0.362 seconds Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . üëç 5 MeloYang05, robertgshaw2-redhat, esmeetu, WoosukKwon, and ywang96 reacted with thumbs up emoji üëÄ 4 njhill, robertgshaw2-redhat, WoosukKwon, and ywang96 reacted with eyes emoji All reactions üëç 5 reactions üëÄ 4 reactions Yard1 and others added 3 commits March 16, 2024 22:15 WIP 5b9153d WIP 8e37cfa Add co-author ‚Ä¶ ff9c9a5 Co-authored-by: MeloYang <meloyang05@gmail.com> Yard1 requested review from esmeetu , zhuohan123 and simon-mo March 18, 2024 17:21 Fix CI e4c2ebb Yard1 changed the title Improve detokenization performance for prefill [Core] Improve detokenization performance for prefill Mar 18, 2024 richardliaw assigned simon-mo Mar 18, 2024 Fix test 3171bbf Copy link Collaborator WoosukKwon commented Mar 22, 2024 @simon-mo Kindly reminder for this PR. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . simon-mo approved these changes Mar 22, 2024 View reviewed changes Copy link Collaborator simon-mo left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I'm stamping this but could useful for another eye on this. But since it's mostly moving things around + utilizing existing functions to achieve something, I think it's mergable. I tried my best to understand the code, left some comments for readability that please feel free to address. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/transformers_utils/detokenizer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/transformers_utils/tokenizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/transformers_utils/detokenizer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/transformers_utils/detokenizer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/transformers_utils/detokenizer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . njhill reviewed Mar 22, 2024 View reviewed changes Copy link Member njhill left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @Yard1 this looks great, thanks. I agree with all @simon-mo 's comments. I like the Detokenizer class separation but wonder whether this could be taken a bit further: have Detokenizer be stateful and self-contained, it could contain the prefix_offset , read_offset and output_text fields that are currently in Sequence , and itself be a field of Sequence (for output tokens). A separate instance of it could be used for the prompt tokens. WDYT? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author Yard1 commented Mar 22, 2024 ‚Ä¢ edited Loading Uh oh! There was an error while loading. Please reload this page . @njhill I like that idea, though we'd need to figure out a good design to share the tokenizer object across the instances. I think the default assumption may be that each Detokenizer has it's own HF tokenizer, but we'd like it to be shared. Maybe we could have something like DetokenizationState in the Sequence All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Review feedback c7e933f Copy link Member njhill commented Mar 22, 2024 I think the default assumption may be that each Detokenizer has it's own HF tokenizer, @Yard1 I'm not sure I follow why that would be the case or why it would matter? I don't see the problem with multiple Detokenizer instances referencing the same tokenizer? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author Yard1 commented Mar 22, 2024 @njhill Oh yeah from technical standpoint it's all clear, but from design standpoint multiple instances sharing the same object makes it hard to realize at a glance whether that object is shared or separate, which may lead to issues later (\"As a new developer, I want to modify the tokenizer in this sequence for some reason, so I will just do that without realizing it's shared across all sequences\") All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member njhill commented Mar 22, 2024 Ah, makes sense! though I think it's not uncommon for such things to be shared. I.e. the field is just seen as a pointer to the tokenizer used by this detokenizer. In any case maybe a comment on the field making clear that it's shared would help with that? And I don't mean to imply that this PR should necessarily be held up for this change, could always be done as a follow-on. üëç 1 Yard1 reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author Yard1 commented Mar 22, 2024 @njhill definitely! I think it would be a good followup (even just breaking up some of the big sequence.py classes would be good) üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Fix test daa87e9 Yard1 merged commit bfdb1ba into vllm-project : main Mar 22, 2024 Yard1 deleted the improve_detokenization_for_prefill branch March 22, 2024 20:44 dtrifiro mentioned this pull request May 15, 2024 bump ubi base image tag opendatahub-io/vllm#24 Merged gc-fu pushed a commit\n        to analytics-zoo/vllm\n      that referenced\n      this pull request Jul 2, 2024 [Core] Improve detokenization performance for prefill ( vllm-project#3469 ‚Ä¶ d60ae0f )\n\nCo-authored-by: MeloYang <meloyang05@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:49:18",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: latency, latency, latency | SERVING: API server, OpenAI API server, Frontend | TEST: test, test, test",
  "analysis_extracted_at": "2025-09-07 17:49:18",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python /home/ray/default/vllm_public/benchmarks/benchmark_latency.py --model meta-llama/Llama-2-7b-chat-hf  --batch-size 1 --output-len 2 --input-len 1000 --num-iters 1",
  "commit_subject": "[Core] Improve detokenization performance for prefill (#3469)",
  "commit_message": "[Core] Improve detokenization performance for prefill (#3469)\n\nCo-authored-by: MeloYang <meloyang05@gmail.com>",
  "commit_date": "2024-03-22T13:44:12-07:00",
  "files_changed": [
    "tests/tokenization/test_detokenize.py",
    "vllm/engine/llm_engine.py",
    "vllm/transformers_utils/detokenizer.py",
    "vllm/transformers_utils/tokenizer.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 1,
    "num_non_test_files": 3,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 4,
    "num_hunks": 13,
    "num_edited_lines": 474,
    "num_non_test_edited_lines": 311,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py\nindex 442173939..082034083 100644\n--- a/tests/tokenization/test_detokenize.py\n+++ b/tests/tokenization/test_detokenize.py\n@@ -1,13 +1,17 @@\n import pytest\n \n from transformers import AutoTokenizer\n+from typing import List, Dict\n \n+from vllm.sequence import Sequence, Logprob, SamplingParams, SequenceGroup\n+from vllm.transformers_utils.tokenizer_group import get_tokenizer_group\n from vllm.transformers_utils.tokenizer import detokenize_incrementally\n+from vllm.transformers_utils.detokenizer import Detokenizer\n \n TRUTH = [\n-    \"Hello here, this is a simple test\",  # noqa: E501\n-    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",  # noqa: E501\n-    \"ÊàëÂæàÊÑüË∞¢‰Ω†ÁöÑÁÉ≠ÊÉÖ\"  # noqa: E501\n+    \"Hello here, this is a simple test\",\n+    \"vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. It is designed to be used in production environments, where inference and serving\",  # noqa\n+    \"ÊàëÂæàÊÑüË∞¢‰Ω†ÁöÑÁÉ≠ÊÉÖ\"\n ]\n TOKENIZERS = [\n     \"facebook/opt-125m\",\n@@ -24,12 +28,12 @@ TOKENIZERS = [\n \n \n def _run_incremental_decode(tokenizer, all_input_ids,\n-                            skip_special_tokens: bool):\n+                            skip_special_tokens: bool, starting_index: int):\n     decoded_text = \"\"\n     offset = 0\n     token_offset = 0\n     prev_tokens = None\n-    for i in range(len(all_input_ids)):\n+    for i in range(starting_index, len(all_input_ids)):\n         new_tokens, text, offset, token_offset = detokenize_incrementally(\n             tokenizer,\n             all_input_ids[:i + 1],\n@@ -46,17 +50,152 @@ def _run_incremental_decode(tokenizer, all_input_ids,\n \n \n @pytest.mark.parametrize(\"truth\", TRUTH)\n+@pytest.mark.parametrize(\"with_prompt\", [True, False])\n @pytest.mark.parametrize(\"tokenizer_id\", TOKENIZERS)\n @pytest.mark.parametrize(\"skip_special_tokens\", (True, False))\n-def test_decode_streaming(tokenizer_id, truth, skip_special_tokens):\n+def test_decode_streaming(tokenizer_id, truth, with_prompt,\n+                          skip_special_tokens):\n     tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n-    all_input_ids = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n+    if with_prompt:\n+        truth_tokens = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n+        prompt_input_ids = truth_tokens[:len(truth) // 2]\n+        generated_input_ids = truth_tokens[len(truth) // 2:]\n+        all_input_ids = prompt_input_ids + generated_input_ids\n+        starting_index = len(prompt_input_ids)\n+        prompt = tokenizer.decode(prompt_input_ids,\n+                                  skip_special_tokens=skip_special_tokens)\n+        generated = truth[len(prompt):]\n+    else:\n+        generated = truth\n+        starting_index = 0\n+        all_input_ids = tokenizer(truth, add_special_tokens=False)[\"input_ids\"]\n     if skip_special_tokens:\n-        all_input_ids = ([tokenizer.bos_token_id]\n-                         if tokenizer.bos_token_id is not None else\n-                         []) + all_input_ids + [tokenizer.eos_token_id]\n+        if tokenizer.bos_token_id is not None:\n+            all_input_ids = [tokenizer.bos_token_id] + all_input_ids\n+            starting_index += 1\n+        all_input_ids = all_input_ids + [tokenizer.eos_token_id]\n \n     decoded_text = _run_incremental_decode(\n-        tokenizer, all_input_ids, skip_special_tokens=skip_special_tokens)\n+        tokenizer,\n+        all_input_ids,\n+        skip_special_tokens=skip_special_tokens,\n+        starting_index=starting_index)\n \n-    assert decoded_text == truth\n+    assert decoded_text == generated\n+\n+\n+@pytest.fixture\n+def detokenizer(tokenizer_name: str) -> Detokenizer:\n+    init_kwargs = dict(\n+        tokenizer_id=tokenizer_name,\n+        enable_lora=False,\n+        max_num_seqs=100,\n+        max_input_length=None,\n+        tokenizer_mode=\"auto\",\n+        trust_remote_code=False,\n+        revision=None,\n+    )\n+\n+    tokenizer_group = get_tokenizer_group(\n+        None,\n+        **init_kwargs,\n+    )\n+\n+    return Detokenizer(tokenizer_group)\n+\n+\n+@pytest.fixture(name=\"complete_sequence_token_ids\")\n+def create_complete_sequence_token_ids(complete_sequence: str,\n+                                       tokenizer_name: str) -> List[int]:\n+    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n+    complete_sequence_token_ids = tokenizer(complete_sequence)[\"input_ids\"]\n+    return complete_sequence_token_ids\n+\n+\n+def create_sequence(prompt_token_ids=None):\n+    prompt_token_ids = prompt_token_ids or [1]\n+    return Sequence(\n+        seq_id=0,\n+        prompt=\"<s>\",\n+        prompt_token_ids=prompt_token_ids,\n+        block_size=16,\n+    )\n+\n+\n+def create_dummy_logprobs(\n+        complete_sequence_token_ids: List[int]) -> List[Dict[int, Logprob]]:\n+    return [{\n+        token_id: Logprob(logprob=0.0),\n+        token_id + 1: Logprob(logprob=0.1)\n+    } for token_id in complete_sequence_token_ids]\n+\n+\n+@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n+@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n+@pytest.mark.parametrize(\"skip_special_tokens\", [True, False])\n+def test_decode_sequence_logprobs(complete_sequence: str,\n+                                  complete_sequence_token_ids: List[int],\n+                                  detokenizer: Detokenizer,\n+                                  skip_special_tokens: bool):\n+    \"\"\"Verify Detokenizer decodes logprobs correctly.\"\"\"\n+    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n+                                     logprobs=2)\n+\n+    # Run sequentially.\n+    seq = create_sequence()\n+    dummy_logprobs = create_dummy_logprobs(complete_sequence_token_ids)\n+    sequential_logprobs_text_chosen_token = []\n+    sequential_logprobs_text_other_token = []\n+    for new_token, logprobs in zip(complete_sequence_token_ids,\n+                                   dummy_logprobs):\n+        seq.append_token_id(new_token, logprobs)\n+        detokenizer.decode_sequence_inplace(seq, sampling_params)\n+        sequential_logprobs_text_chosen_token.append(\n+            seq.output_logprobs[-1][new_token].decoded_token)\n+        sequential_logprobs_text_other_token.append(\n+            seq.output_logprobs[-1][new_token + 1].decoded_token)\n+    sequential_result = seq.output_text\n+\n+    assert sequential_result == \"\".join(sequential_logprobs_text_chosen_token)\n+    assert sequential_result != \"\".join(sequential_logprobs_text_other_token)\n+\n+    if skip_special_tokens:\n+        # Text for logprobs for the chosen token should be the same as the\n+        # generated text. Note that this will only be true if we skip\n+        # special tokens.\n+        assert sequential_result == complete_sequence\n+\n+\n+@pytest.mark.parametrize(\"complete_sequence\", TRUTH)\n+@pytest.mark.parametrize(\"tokenizer_name\", TOKENIZERS)\n+@pytest.mark.parametrize(\"skip_special_tokens\", [True])\n+def test_decode_prompt_logprobs(complete_sequence: str,\n+                                complete_sequence_token_ids: List[int],\n+                                detokenizer: Detokenizer,\n+                                skip_special_tokens: bool):\n+    \"\"\"Verify Detokenizer decodes prompt logprobs correctly.\"\"\"\n+    sampling_params = SamplingParams(skip_special_tokens=skip_special_tokens,\n+                                     prompt_logprobs=1)\n+\n+    # Run sequentially.\n+    seq = create_sequence(complete_sequence_token_ids)\n+    seq_group = SequenceGroup(request_id=\"1\",\n+                              seqs=[seq],\n+                              sampling_params=sampling_params,\n+                              arrival_time=0.0)\n+    dummy_logprobs = create_dummy_logprobs(complete_sequence_token_ids)\n+    detokenizer.decode_prompt_logprobs_inplace(seq_group, dummy_logprobs)\n+    decoded_prompt_logprobs = dummy_logprobs\n+\n+    if skip_special_tokens:\n+        # Text for logprobs for the chosen token should be the same as the\n+        # prompt text. Note that this will only be true if we skip\n+        # special tokens.\n+        assert complete_sequence == \"\".join([\n+            logprobs[token_id].decoded_token for token_id, logprobs in zip(\n+                complete_sequence_token_ids, decoded_prompt_logprobs)\n+        ])\n+        assert complete_sequence != \"\".join([\n+            logprobs[token_id + 1].decoded_token for token_id, logprobs in zip(\n+                complete_sequence_token_ids, decoded_prompt_logprobs)\n+        ])\ndiff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 724782841..283b5d9ac 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1,5 +1,5 @@\n import time\n-from typing import Dict, Iterable, List, Optional, Tuple, Type, Union\n+from typing import Iterable, List, Optional, Tuple, Type, Union\n \n from transformers import PreTrainedTokenizer\n \n@@ -15,11 +15,11 @@ from vllm.engine.ray_utils import initialize_ray_cluster\n from vllm.logger import init_logger\n from vllm.outputs import RequestOutput\n from vllm.sampling_params import SamplingParams\n-from vllm.sequence import (Logprob, SamplerOutput, Sequence, SequenceGroup,\n+from vllm.sequence import (SamplerOutput, Sequence, SequenceGroup,\n                            SequenceGroupOutput, SequenceOutput, SequenceStatus)\n-from vllm.transformers_utils.tokenizer import detokenize_incrementally\n from vllm.transformers_utils.tokenizer_group import (BaseTokenizerGroup,\n                                                      get_tokenizer_group)\n+from vllm.transformers_utils.detokenizer import Detokenizer\n from vllm.utils import Counter\n \n logger = init_logger(__name__)\n@@ -97,6 +97,7 @@ class LLMEngine:\n         self._verify_args()\n \n         self._init_tokenizer()\n+        self.detokenizer = Detokenizer(self.tokenizer)\n         self.seq_counter = Counter()\n \n         self.model_executor = executor_class(model_config, cache_config,\n@@ -153,7 +154,7 @@ class LLMEngine:\n         raise RuntimeError(\"LLMEngine should not be pickled!\")\n \n     def get_tokenizer(self) -> \"PreTrainedTokenizer\":\n-        return self.tokenizer.get_lora_tokenizer()\n+        return self.tokenizer.get_lora_tokenizer(None)\n \n     def get_tokenizer_for_seq(self,\n                               sequence: Sequence) -> \"PreTrainedTokenizer\":\n@@ -370,13 +371,8 @@ class LLMEngine:\n         # Process prompt logprobs\n         prompt_logprobs = outputs.prompt_logprobs\n         if prompt_logprobs is not None:\n-            # We can pick any sequence for the prompt.\n-            seq = next(iter(seq_group.seqs_dict.values()))\n-            all_token_ids = seq.get_token_ids()\n-            for i, prompt_logprobs_for_token in enumerate(prompt_logprobs):\n-                self._decode_logprobs(seq, seq_group.sampling_params,\n-                                      prompt_logprobs_for_token,\n-                                      all_token_ids[:i])\n+            self.detokenizer.decode_prompt_logprobs_inplace(\n+                seq_group, prompt_logprobs)\n             seq_group.prompt_logprobs = prompt_logprobs\n \n         # Process samples\n@@ -420,7 +416,8 @@ class LLMEngine:\n             child_seqs.append((parent, parent))\n \n         for seq, _ in child_seqs:\n-            self._decode_sequence(seq, seq_group.sampling_params)\n+            self.detokenizer.decode_sequence_inplace(seq,\n+                                                     seq_group.sampling_params)\n             self._check_stop(seq, seq_group.sampling_params)\n \n         # Non-beam search case\n@@ -713,51 +710,6 @@ class LLMEngine:\n             time_e2e_requests=time_e2e_requests,\n         )\n \n-    def _decode_logprobs(self, seq: Sequence, prms: SamplingParams,\n-                         logprobs: Dict[int, Logprob],\n-                         all_input_ids: List[int]) -> None:\n-        if not logprobs:\n-            return\n-        for token_id, sample_logprob in logprobs.items():\n-            if (sample_logprob.decoded_token is None and token_id != -1):\n-                all_input_ids_with_logprob = all_input_ids[:-1] + [token_id]\n-                (_, new_text, prefix_offset,\n-                 read_offset) = detokenize_incrementally(\n-                     self.get_tokenizer_for_seq(seq),\n-                     all_input_ids=all_input_ids_with_logprob,\n-                     prev_tokens=seq.tokens,\n-                     prefix_offset=seq.prefix_offset,\n-                     read_offset=seq.read_offset,\n-                     skip_special_tokens=prms.skip_special_tokens,\n-                     spaces_between_special_tokens=prms.\n-                     spaces_between_special_tokens,\n-                 )\n-                sample_logprob.decoded_token = new_text\n-\n-    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:\n-        \"\"\"Decodes the new token for a sequence.\"\"\"\n-        all_input_ids = seq.get_token_ids()\n-        self._decode_logprobs(seq, prms, seq.output_logprobs[-1],\n-                              all_input_ids)\n-\n-        (new_tokens, new_output_text, prefix_offset,\n-         read_offset) = detokenize_incrementally(\n-             self.get_tokenizer_for_seq(seq),\n-             all_input_ids=all_input_ids,\n-             prev_tokens=seq.tokens,\n-             prefix_offset=seq.prefix_offset,\n-             read_offset=seq.read_offset,\n-             skip_special_tokens=prms.skip_special_tokens,\n-             spaces_between_special_tokens=prms.spaces_between_special_tokens,\n-         )\n-        if seq.tokens is None:\n-            seq.tokens = new_tokens\n-        else:\n-            seq.tokens.extend(new_tokens)\n-        seq.prefix_offset = prefix_offset\n-        seq.read_offset = read_offset\n-        seq.output_text += new_output_text\n-\n     def _check_stop(self, seq: Sequence,\n                     sampling_params: SamplingParams) -> None:\n         \"\"\"Stop the finished sequences.\"\"\"\ndiff --git a/vllm/transformers_utils/detokenizer.py b/vllm/transformers_utils/detokenizer.py\nnew file mode 100644\nindex 000000000..1f322b367\n--- /dev/null\n+++ b/vllm/transformers_utils/detokenizer.py\n@@ -0,0 +1,155 @@\n+from typing import List, Dict, Optional\n+from transformers import PreTrainedTokenizer\n+from vllm.sequence import Sequence, Logprob, SequenceGroup, SamplingParams\n+from vllm.transformers_utils.tokenizer import (detokenize_incrementally,\n+                                               convert_prompt_ids_to_tokens)\n+from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n+    BaseTokenizerGroup)\n+\n+# Used eg. for marking rejected tokens in spec decoding.\n+INVALID_TOKEN_ID = -1\n+\n+\n+class Detokenizer:\n+    \"\"\"Provides methods to decode the output of a model into text.\"\"\"\n+\n+    def __init__(self, tokenizer_group: BaseTokenizerGroup):\n+        self.tokenizer_group = tokenizer_group\n+\n+    def get_tokenizer_for_seq(self,\n+                              sequence: Sequence) -> \"PreTrainedTokenizer\":\n+        \"\"\"Returns the HF tokenizer to use for a given sequence.\"\"\"\n+        return self.tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n+\n+    def decode_prompt_logprobs_inplace(\n+            self, seq_group: SequenceGroup,\n+            prompt_logprobs: List[Optional[Dict[int, Logprob]]]) -> None:\n+        \"\"\"Decodes the logprobs for the prompt of a sequence group.\n+\n+        Args:\n+            seq_group: The sequence group to decode.\n+            prompt_logprobs: The logprobs to decode.\n+        \n+        Returns:\n+            The prompt logprobs with the decoded tokens.\n+        \"\"\"\n+        prms = seq_group.sampling_params\n+        # We can pick any sequence for the prompt.\n+        seq = next(iter(seq_group.seqs_dict.values()))\n+        # Only prompt, without the generated token.\n+        all_token_ids = seq.get_token_ids()\n+        prompt_token_ids = all_token_ids[:-1]\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+        prefix_offset = 0\n+        read_offset = 0\n+        next_iter_prefix_offset = 0\n+        next_iter_read_offset = 0\n+        next_iter_tokens = []\n+        prev_tokens = None\n+\n+        for token_position, prompt_logprobs_for_token in enumerate(\n+                prompt_logprobs):\n+            if not prompt_logprobs_for_token:\n+                continue\n+            for token_id, sample_logprob in prompt_logprobs_for_token.items():\n+                if (sample_logprob.decoded_token is None\n+                        and token_id != INVALID_TOKEN_ID):\n+                    prompt_token_ids_with_token = (\n+                        prompt_token_ids[:token_position] + [token_id])\n+                    (new_tokens, new_text, new_prefix_offset,\n+                     new_read_offset) = detokenize_incrementally(\n+                         tokenizer=tokenizer,\n+                         all_input_ids=prompt_token_ids_with_token,\n+                         prev_tokens=prev_tokens,\n+                         prefix_offset=prefix_offset,\n+                         read_offset=read_offset,\n+                         skip_special_tokens=prms.skip_special_tokens,\n+                         spaces_between_special_tokens=prms.\n+                         spaces_between_special_tokens,\n+                     )\n+\n+                    sample_logprob.decoded_token = new_text\n+\n+                    # Use the offsets & prev tokens corresponding to\n+                    # real tokens to ensure detokenization is consistent\n+                    # actual with prompt.\n+                    if token_id == all_token_ids[token_position]:\n+                        next_iter_prefix_offset = new_prefix_offset\n+                        next_iter_read_offset = new_read_offset\n+                        next_iter_tokens = new_tokens\n+\n+            # Advance to the next token position.\n+            prefix_offset = next_iter_prefix_offset\n+            read_offset = next_iter_read_offset\n+            if prev_tokens is None:\n+                prev_tokens = next_iter_tokens\n+            else:\n+                prev_tokens.extend(next_iter_tokens)\n+\n+    def decode_sequence_inplace(self, seq: Sequence,\n+                                prms: SamplingParams) -> None:\n+        \"\"\"Decodes the new token for a sequence. In-place operation.\n+\n+        Args:\n+            seq: The sequence to decode.\n+            prms: The sampling parameters used to generate the sequence.\n+        \"\"\"\n+        all_input_ids = seq.get_token_ids()\n+        token_id_generated_this_iteration = all_input_ids[-1]\n+        tokenizer = self.get_tokenizer_for_seq(seq)\n+\n+        # Convert prompt token IDs to tokens if necessary.\n+        # Do it here so that we don't have to repeat this\n+        # computation for each logprob.\n+        if seq.tokens is None:\n+            (seq.tokens, seq.prefix_offset,\n+             seq.read_offset) = convert_prompt_ids_to_tokens(\n+                 tokenizer=tokenizer,\n+                 prompt_ids=all_input_ids[:-1],\n+                 skip_special_tokens=prms.skip_special_tokens,\n+             )\n+\n+        (new_tokens, new_decoded_token_text, prefix_offset,\n+         read_offset) = detokenize_incrementally(\n+             tokenizer=tokenizer,\n+             all_input_ids=all_input_ids,\n+             prev_tokens=seq.tokens,\n+             prefix_offset=seq.prefix_offset,\n+             read_offset=seq.read_offset,\n+             skip_special_tokens=prms.skip_special_tokens,\n+             spaces_between_special_tokens=prms.spaces_between_special_tokens,\n+         )\n+\n+        # Decode logprobs\n+        logprobs = seq.output_logprobs[-1]\n+        if logprobs:\n+            previous_tokens = all_input_ids[:-1]\n+            for token_id, sample_logprob in logprobs.items():\n+                # If the token was generated this iteration,\n+                # use the provided text.\n+                if token_id == token_id_generated_this_iteration:\n+                    sample_logprob.decoded_token = new_decoded_token_text\n+                    continue\n+\n+                if (sample_logprob.decoded_token is None\n+                        and token_id != INVALID_TOKEN_ID):\n+                    all_input_ids_with_logprob = previous_tokens + [token_id]\n+                    (_, new_text, _, _) = detokenize_incrementally(\n+                        tokenizer=tokenizer,\n+                        all_input_ids=all_input_ids_with_logprob,\n+                        prev_tokens=seq.tokens,\n+                        prefix_offset=seq.prefix_offset,\n+                        read_offset=seq.read_offset,\n+                        skip_special_tokens=prms.skip_special_tokens,\n+                        spaces_between_special_tokens=prms.\n+                        spaces_between_special_tokens,\n+                    )\n+                    sample_logprob.decoded_token = new_text\n+\n+        if seq.tokens is None:\n+            seq.tokens = new_tokens\n+        else:\n+            seq.tokens.extend(new_tokens)\n+        seq.prefix_offset = prefix_offset\n+        seq.read_offset = read_offset\n+        seq.output_text += new_decoded_token_text\ndiff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py\nindex f7a1a19a8..eebdacc49 100644\n--- a/vllm/transformers_utils/tokenizer.py\n+++ b/vllm/transformers_utils/tokenizer.py\n@@ -158,6 +158,34 @@ def _convert_tokens_to_string_with_added_encoders(\n         return \"\".join(sub_texts)\n \n \n+# 5 is an arbitrary value that should work for all\n+# tokenizers (bigger = more conservative).\n+INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET = 5\n+\n+\n+def convert_prompt_ids_to_tokens(\n+    tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n+    prompt_ids: List[int],\n+    skip_special_tokens: bool = False,\n+) -> Tuple[List[str], int, int]:\n+    \"\"\"Converts the prompt ids to tokens and returns the tokens and offsets\n+    for incremental detokenization.\n+\n+    Note that not all tokens are converted to strings. Only the tokens that\n+    are necessary for incremental detokenization are converted to strings.\n+    \"\"\"\n+    # Offset a little more in case we have special tokens.\n+    prefix_offset = max(\n+        len(prompt_ids) - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET - 2, 0)\n+    # We do not need to convert the whole prompt to tokens.\n+    new_tokens = tokenizer.convert_ids_to_tokens(\n+        prompt_ids[prefix_offset:], skip_special_tokens=skip_special_tokens)\n+    prefix_offset = max(\n+        len(new_tokens) - INITIAL_INCREMENTAL_DETOKENIZATION_OFFSET, 0)\n+    read_offset = len(new_tokens)\n+    return new_tokens, prefix_offset, read_offset\n+\n+\n # Based on\n # https://github.com/huggingface/text-generation-inference/blob/v0.9.4/server/text_generation_server/models/model.py#L62C9-L62C15\n # under Apache 2.0 license\n@@ -165,31 +193,53 @@ def detokenize_incrementally(\n     tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast],\n     all_input_ids: List[int],\n     prev_tokens: Optional[List[str]],\n-    prefix_offset: int = 0,\n-    read_offset: int = 0,\n+    prefix_offset: int,\n+    read_offset: int,\n     skip_special_tokens: bool = False,\n     spaces_between_special_tokens: bool = True,\n ) -> Tuple[List[str], str, int, int]:\n+    \"\"\"Detokenizes the input ids incrementally and returns the new tokens\n+    and the new text.\n+\n+    If `prev_tokens` is None, this function will convert the input ids to\n+    tokens and return the tokens and the new text. Otherwise, it will return the\n+    new tokens and the new text.\n+\n+    This function will also return the new prefix offset and the new read\n+    offset to be used in the next iteration.\n+\n+    The offsets are necessary to defeat cleanup algorithms in the decode which\n+    decide to add a space or not depending on the surrounding ids.\n+\n+    Args:\n+        tokenizer: The tokenizer to use.\n+        all_input_ids: The input ids. The last id is the new token id.\n+        prev_tokens: The previous tokens. If None, this function will convert\n+            the input ids to tokens and return the tokens and the new text.\n+        prefix_offset: The prefix offset.\n+        read_offset: The read offset.\n+        skip_special_tokens: Whether to skip special tokens.\n+        spaces_between_special_tokens: Whether to add spaces between special\n+            tokens.\n+    \"\"\"\n     new_token_id = all_input_ids[-1]\n     # This is the first iteration for this sequence\n-    if prev_tokens is None:\n-        new_tokens = tokenizer.convert_ids_to_tokens(\n-            all_input_ids, skip_special_tokens=skip_special_tokens)\n-        output_tokens = new_tokens\n-        # 5 is an arbitrary value that should work for all\n-        # tokenizers (bigger = more conservative).\n-        # Subtract 1 extra to account for the generated token.\n-        prefix_offset = max(len(output_tokens) - 6, 0)\n-        # If the first new token is a special token, we can't skip 1 extra token\n-        if skip_special_tokens and new_token_id in tokenizer.all_special_ids:\n-            read_offset = max(len(output_tokens), 0)\n-        else:\n-            read_offset = max(len(output_tokens) - 1, 0)\n-    else:\n-        # Put new_token_id in a list so skip_special_tokens is respected\n-        new_tokens = tokenizer.convert_ids_to_tokens(\n-            [new_token_id], skip_special_tokens=skip_special_tokens)\n-        output_tokens = prev_tokens + new_tokens\n+    is_first_iter = prev_tokens is None\n+    if is_first_iter:\n+        (prev_tokens, prefix_offset,\n+         read_offset) = convert_prompt_ids_to_tokens(\n+             tokenizer,\n+             all_input_ids[:-1],\n+             skip_special_tokens=skip_special_tokens)\n+\n+    # Put new_token_id in a list so skip_special_tokens is respected\n+    new_tokens = tokenizer.convert_ids_to_tokens(\n+        [new_token_id], skip_special_tokens=skip_special_tokens)\n+    output_tokens = prev_tokens + new_tokens\n+\n+    # If this is the first iteration, return all tokens.\n+    if is_first_iter:\n+        new_tokens = output_tokens\n \n     # The prefix text is necessary only to defeat cleanup algorithms in\n     # the decode which decide to add a space or not depending on the",
  "apis": [
    "vllm.LLMEngine",
    "vllm.transformers_utils.detokenizer.Detokenizer"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/detokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/detokenizer.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/transformers_utils/tokenizer.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit introduces a new Detokenizer module and refactors existing engine and tokenizer code to use it, thereby improving the incremental detokenization and logprob decoding processes. The changes affect non-test source code (e.g., vllm/engine/llm_engine.py, vllm/transformers_utils/detokenizer.py, and vllm/transformers_utils/tokenizer.py) and are aimed at optimizing the existing detokenization performance for inference, especially for prefill scenarios. This is not a simple refactoring or bug fix‚Äîit specifically targets performance improvements in the core API running on CPU.",
  "llm_api_reason": "This commit refactors and enhances the detokenization workflow. A new Detokenizer class is introduced in the transformers_utils/detokenizer module to centralize and improve incremental decoding of token outputs. The LLMEngine has been updated to use this Detokenizer (for example, replacing direct calls to detokenize_incrementally when decoding prompt logprobs and generated tokens) and its integration is verified through newly added tests for decoding both prompt and sequence logprobs. These changes optimize detokenization performance and improve test coverage for the decoding logic."
}