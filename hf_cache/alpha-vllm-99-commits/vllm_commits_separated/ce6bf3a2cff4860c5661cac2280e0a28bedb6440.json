{
  "commit_hash": "ce6bf3a2cff4860c5661cac2280e0a28bedb6440",
  "pr_url": "https://github.com/vllm-project/vllm/pull/7898",
  "pr_date": "2024-08-28",
  "timeline_text": "Copy link Member youkaichao commented Aug 27, 2024 We have 2 types of runtime overhead in TPU: Dynamo guard evaluation overhead, chooses which code to run torch xla overhead, convert function input to xla input We can manage to remove the first one, via adding one layer dispatcher above Dynamo. I did systematic measurement this time, and find that: pure xla execution takes 7ms for every decoding step combining both overhead ( the current main branch), it takes 8.2ms for every decoding step removing Dynamo overhead (this PR), it takes 8.0ms for every decoding step It turns out the xla overhead is the main overhead. But I think it is still worthwhile to get rid of the Dynamo overhead before we remove the xla overhead. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions custom dispatch 248d4db Copy link github-actions bot commented Aug 27, 2024 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which consists a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of default ones by unblocking the steps in your fast-check build on Buildkite UI. Once the PR is approved and ready to go, please make sure to run full CI as it is required to merge (or just use auto-merge). To run full CI, you can do one of these: Comment /ready on the PR Add ready label to the PR Enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . refine 8f4ed39 Copy link Member Author youkaichao commented Aug 27, 2024 NOTE: my test code is still https://github.com/vllm-project/vllm/blob/main/examples/offline_inference_tpu.py All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member Author youkaichao commented Aug 27, 2024 this looks surprisingly effective. I run python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b main: Throughput: 16.70 requests/s, 8549.39 tokens/s this PR: Throughput: 17.39 requests/s, 8902.73 tokens/s it counts as 4% throughput improvement All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . youkaichao added 17 commits August 27, 2024 00:57 add wrapper 2d8b20a update 9f752fd add wrapper test 4be616a fix 026a525 update wrapper 7a1dd38 separate tests 1f0f148 add tests 7531186 update tests 31e9e7b multi wrappers ace38e2 use wrapper 31a9e06 fix 0a349f5 fix 12cb164 more explanation f483660 add tests ec52afc add package fabce9a update tests b9fff4c add tests f5019fc youkaichao requested a review\n  from WoosukKwon August 27, 2024 18:12 add init e3692ba WoosukKwon approved these changes Aug 28, 2024 View reviewed changes vllm/worker/tpu_model_runner.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/worker/tpu_model_runner.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/compilation/wrapper.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . youkaichao and others added 3 commits August 28, 2024 15:26 Update vllm/worker/tpu_model_runner.py â€¦ 746036c Co-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Merge branch 'main' into custom_dispatch 80ce2bd fix args a0bac86 youkaichao enabled auto-merge (squash) August 28, 2024 23:09 youkaichao disabled auto-merge August 28, 2024 23:09 github-actions bot added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Aug 28, 2024 Hide details View details youkaichao merged commit ce6bf3a into vllm-project : main Aug 28, 2024 26 of 31 checks passed Uh oh! There was an error while loading. Please reload this page . youkaichao deleted the custom_dispatch branch August 28, 2024 23:10 youkaichao mentioned this pull request Aug 28, 2024 [torch.compile] remove reset #7975 Merged Alvant pushed a commit\n        to compressa-ai/vllm\n      that referenced\n      this pull request Oct 26, 2024 [torch.compile] avoid Dynamo guard evaluation overhead ( vllm-project#â€¦ â€¦ 7da14a0 â€¦7898 )\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: Alvant <alvasian@yandex.ru> LeiWang1999 pushed a commit\n        to LeiWang1999/vllm-bitblas\n      that referenced\n      this pull request Mar 26, 2025 [torch.compile] avoid Dynamo guard evaluation overhead ( vllm-project#â€¦ â€¦ 74301d6 â€¦7898 )\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>\nSigned-off-by: LeiWang1999 <leiwang1999@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:48:04",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: Throughput, Throughput, throughput | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:48:04",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmarks/benchmark_throughput.py  --input-len 256 --output-len 256 --model google/gemma-2b",
  "commit_subject": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)",
  "commit_message": "[torch.compile] avoid Dynamo guard evaluation overhead (#7898)\n\nCo-authored-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2024-08-28T16:10:12-07:00",
  "files_changed": [
    ".buildkite/run-tpu-test.sh",
    ".buildkite/test-pipeline.yaml",
    "tests/compile/test_wrapper.py",
    "tests/tpu/__init__.py",
    "tests/tpu/test_custom_dispatcher.py",
    "vllm/compilation/__init__.py",
    "vllm/compilation/wrapper.py",
    "vllm/envs.py",
    "vllm/worker/tpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 3,
    "num_non_test_files": 6,
    "only_test_files": 0,
    "only_non_test_files": 0,
    "num_files": 9,
    "num_hunks": 11,
    "num_edited_lines": 201,
    "num_non_test_edited_lines": 133,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/.buildkite/run-tpu-test.sh b/.buildkite/run-tpu-test.sh\nindex 335ffd83f..6989c94d4 100644\n--- a/.buildkite/run-tpu-test.sh\n+++ b/.buildkite/run-tpu-test.sh\n@@ -12,4 +12,4 @@ remove_docker_container\n # For HF_TOKEN.\n source /etc/environment\n # Run a simple end-to-end example.\n-docker run --privileged --net host --shm-size=16G -it -e HF_TOKEN=$HF_TOKEN --name tpu-test vllm-tpu /bin/bash -c \"python3 -m pip install git+https://github.com/thuml/depyf.git && python3 /workspace/vllm/tests/tpu/test_compilation.py && python3 /workspace/vllm/examples/offline_inference_tpu.py\"\n+docker run --privileged --net host --shm-size=16G -it -e HF_TOKEN=$HF_TOKEN --name tpu-test vllm-tpu /bin/bash -c \"python3 -m pip install git+https://github.com/thuml/depyf.git && python3 -m pip install pytest  && pytest -v -s /workspace/vllm/tests/tpu/test_custom_dispatcher.py && python3 /workspace/vllm/tests/tpu/test_compilation.py && python3 /workspace/vllm/examples/offline_inference_tpu.py\"\ndiff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 9f449ff65..235db72ee 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -173,6 +173,7 @@ steps:\n   - vllm/\n   commands:\n     - pytest -v -s ./compile/test_full_graph.py\n+    - pytest -v -s ./compile/test_wrapper.py\n \n \n - label: Vision Language Models Test # 42min\ndiff --git a/tests/compile/test_wrapper.py b/tests/compile/test_wrapper.py\nnew file mode 100644\nindex 000000000..cef516ade\n--- /dev/null\n+++ b/tests/compile/test_wrapper.py\n@@ -0,0 +1,59 @@\n+from typing import Optional\n+\n+import torch\n+\n+from vllm.compilation.wrapper import TorchCompileWrapperWithCustomDispacther\n+\n+\n+class MyMod(torch.nn.Module):\n+\n+    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        if cache is not None:\n+            return x + cache\n+        return x * 2\n+\n+\n+class MyWrapper(TorchCompileWrapperWithCustomDispacther):\n+\n+    def __init__(self, model):\n+        self.model = model\n+        compiled_callable = torch.compile(self.forward, backend=\"eager\")\n+        super().__init__(compiled_callable)\n+\n+    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        # this is the function to be compiled\n+        return self.model(x, cache)\n+\n+    def __call__(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n+        # let torch.compile compile twice\n+        if len(self.compiled_codes) == 2:\n+            dispatch_id = 0 if cache is None else 1\n+            with self.dispatch_to_code(dispatch_id):\n+                return self.forward(x, cache)\n+        else:\n+            return self.compiled_callable(x, cache)\n+\n+\n+def test_torch_compile_wrapper():\n+    mod = MyMod()\n+    wrappers = []\n+    for i in range(3):\n+        torch._dynamo.reset()\n+        wrapper = MyWrapper(mod)\n+        wrappers.append(wrapper)\n+        x = torch.tensor([1])\n+        wrapper(x, None)  # profile run, compile\n+        # create a cache tensor\n+        cache = torch.tensor([2])\n+        wrapper(x, cache)  # warm up with cache, recompile\n+\n+        # for new input, dispatch to the compiled code directly\n+        new_x = torch.tensor([3])\n+        assert wrapper(new_x,\n+                       None).item() == 6  # dispatch to the first compiled code\n+        assert wrapper(\n+            new_x, cache).item() == 5  # dispatch to the second compiled code\n+\n+    for wrapper in wrappers:\n+        # make sure they have independent compiled codes\n+        assert len(wrapper.compiled_codes) == 2\ndiff --git a/tests/tpu/__init__.py b/tests/tpu/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/tests/tpu/test_custom_dispatcher.py b/tests/tpu/test_custom_dispatcher.py\nnew file mode 100644\nindex 000000000..7f3fb5953\n--- /dev/null\n+++ b/tests/tpu/test_custom_dispatcher.py\n@@ -0,0 +1,9 @@\n+from ..utils import compare_two_settings\n+\n+\n+def test_custom_dispatcher():\n+    compare_two_settings(\"google/gemma-2b\",\n+                         arg1=[\"--enforce-eager\"],\n+                         arg2=[\"--enforce-eager\"],\n+                         env1={\"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\": \"0\"},\n+                         env2={})\ndiff --git a/vllm/compilation/__init__.py b/vllm/compilation/__init__.py\nnew file mode 100644\nindex 000000000..e69de29bb\ndiff --git a/vllm/compilation/wrapper.py b/vllm/compilation/wrapper.py\nnew file mode 100644\nindex 000000000..c3d863299\n--- /dev/null\n+++ b/vllm/compilation/wrapper.py\n@@ -0,0 +1,81 @@\n+import os\n+import sys\n+from abc import abstractmethod\n+from contextlib import contextmanager\n+from types import CodeType\n+from typing import Callable, List\n+\n+import torch\n+\n+import vllm.envs as envs\n+\n+\n+class TorchCompileWrapperWithCustomDispacther:\n+    \"\"\"\n+    A wrapper class for torch.compile, with a custom dispatch logic.\n+    Subclasses should:\n+    1. Implement the forward method\n+    2. Implement the dispatch logic in the __call__ method\n+        It can use `self.compiled_codes` to access the compiled bytecode,\n+        and `with self.dispatch_to_code(index):` to dispatch to\n+        the compiled code.\n+    3. Implement the `__init__` method to determine how to call\n+        `torch.compile` over the forward method.\n+    \"\"\"\n+\n+    def __init__(self, compiled_callable: Callable):\n+        self.compiled_callable = compiled_callable\n+        self.original_code_object = self.__class__.forward.__code__\n+        self.compiled_codes: List[CodeType] = []\n+        torch._dynamo.convert_frame.register_bytecode_hook(self.bytecode_hook)\n+\n+        # read the env var to determine whether to use the custom dispatcher\n+        # subclasses can use this to switch between the custom dispatcher\n+        # and the default Dynamo guard mechanism.\n+        self.use_custom_dispatcher: bool = \\\n+            envs.VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\n+\n+    def __call__(self, *args, **kwargs):\n+        \"\"\"Implement the dispatch logic here, beyond the torch.compile level.\n+        NOTE: this function can have additional arguments beyond the forward\n+         method, for directly dispatching to the compiled code.\n+        \"\"\"\n+        return self.compiled_callable(*args, **kwargs)\n+\n+    @abstractmethod\n+    def forward(self, *args, **kwargs):\n+        ...\n+\n+    def bytecode_hook(self, old_code: CodeType, new_code: CodeType):\n+        \"\"\"Hook to save the compiled bytecode for direct execution.\"\"\"\n+        if old_code is not self.original_code_object:\n+            return\n+        # code borrowed from https://github.com/thuml/depyf/blob/f4ad79fadee27ea113b4c75202db1eb1a11c0dbc/depyf/explain/enable_debugging.py#L25\n+        frame = sys._getframe()\n+        while True:\n+            frame = frame.f_back\n+            code_name = frame.f_code.co_name\n+            file_name = frame.f_code.co_filename.split(os.path.sep)[-1]\n+            if code_name == \"_compile\" and file_name == \"convert_frame.py\":\n+                break\n+        frame = frame.f_locals[\"frame\"]\n+        assert frame.f_code == old_code\n+\n+        if frame.f_locals[\"self\"] is not self:\n+            return\n+\n+        self.compiled_codes.append(new_code)\n+\n+    @contextmanager\n+    def dispatch_to_code(self, index: int):\n+        \"\"\"Context manager to dispatch to the compiled code.\n+        Why does this work? Because Dynamo guarantees that the compiled\n+        bytecode has exactly the same arguments, cell variables, and free\n+        variables as the original code. Therefore we can directly switch\n+        the code object in the function and call it.\n+\n+        See https://dev-discuss.pytorch.org/t/what-is-the-relationship-requirement-among-original-bytecode-transformed-bytecode-and-bytecode-returned-by-hooks-in-dynamo/1693/7 for more details.\n+        \"\"\" # noqa\n+        self.__class__.forward.__code__ = self.compiled_codes[index]\n+        yield\n+        self.__class__.forward.__code__ = self.original_code_object\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 4faafd9da..590698416 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -196,6 +196,10 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # Internal flag to enable Dynamo graph capture\n     \"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\":\n     lambda: int(os.environ.get(\"VLLM_TEST_DYNAMO_GRAPH_CAPTURE\", \"0\")),\n+    \"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\":\n+    lambda:\n+    (os.environ.get(\"VLLM_DYNAMO_USE_CUSTOM_DISPATCHER\", \"True\").lower() in\n+     (\"true\", \"1\")),\n \n     # local rank of the process in the distributed setting, used to determine\n     # the GPU device id\ndiff --git a/vllm/worker/tpu_model_runner.py b/vllm/worker/tpu_model_runner.py\nindex 01daa64b5..a7ceb84ef 100644\n--- a/vllm/worker/tpu_model_runner.py\n+++ b/vllm/worker/tpu_model_runner.py\n@@ -10,6 +10,7 @@ import torch_xla.core.xla_model as xm\n import torch_xla.runtime as xr\n \n from vllm.attention import AttentionMetadata, get_attn_backend\n+from vllm.compilation.wrapper import TorchCompileWrapperWithCustomDispacther\n from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, ModelConfig,\n                          ParallelConfig, SchedulerConfig)\n from vllm.logger import init_logger\n@@ -144,11 +145,7 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n             )\n         model = model.eval()\n         xm.wait_device_ops()\n-        model = ModelWrapper(model)\n-        self.model = torch.compile(model,\n-                                   backend=\"openxla\",\n-                                   fullgraph=True,\n-                                   dynamic=False)\n+        self.model = ModelWrapper(model)\n \n     def _dummy_run(\n         self,\n@@ -235,8 +232,15 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n             torch._dynamo.mark_dynamic(t, 0)\n             torch._dynamo.mark_dynamic(p, 0)\n         # Dummy run.\n-        self.model(token_ids, position_ids, attn_metadata, input_lens, t, p,\n-                   num_samples, kv_caches)\n+        self.model(token_ids,\n+                   position_ids,\n+                   attn_metadata,\n+                   input_lens,\n+                   t,\n+                   p,\n+                   num_samples,\n+                   kv_caches,\n+                   is_prompt=is_prompt)\n \n     def warmup_model(\n         self,\n@@ -530,7 +534,7 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n                     if getattr(arg, \"context_lens\", None) is not None:\n                         arg.context_lens = arg.context_lens.to(self.device)\n                 new_args.append(arg)\n-            return self.model(*new_args)\n+            return self.model(*new_args, is_prompt=is_prompt)\n \n         num_prefills = model_input.attn_metadata.num_prefills\n         is_prompt = num_prefills > 0\n@@ -601,11 +605,32 @@ class TPUModelRunner(ModelRunnerBase[ModelInputForTPU]):\n         return [SamplerOutput(sampler_outputs)]\n \n \n-class ModelWrapper(nn.Module):\n+class ModelWrapper(TorchCompileWrapperWithCustomDispacther):\n \n     def __init__(self, model: nn.Module):\n-        super().__init__()\n         self.model = model\n+        compiled_callable = torch.compile(self.forward,\n+                                          backend=\"openxla\",\n+                                          fullgraph=True,\n+                                          dynamic=False)\n+        super().__init__(compiled_callable)\n+\n+    def __call__(self, *args, is_prompt: bool, **kwargs):\n+        if len(self.compiled_codes) < 3 or not self.use_custom_dispatcher:\n+            # not fully compiled yet, or not using the custom dispatcher,\n+            # let PyTorch handle it\n+            return self.compiled_callable(*args, **kwargs)\n+        # the 3 compiled codes are:\n+        # 0: for profiling\n+        # 1: for prompt\n+        # 2: for decode\n+        # dispatch to the compiled code directly, skip PyTorch\n+        if is_prompt:\n+            with self.dispatch_to_code(1):\n+                return self.forward(*args, **kwargs)\n+        else:\n+            with self.dispatch_to_code(2):\n+                return self.forward(*args, **kwargs)\n \n     def forward(\n         self,",
  "apis": [
    "vllm.compilation.wrapper.TorchCompileWrapperWithCustomDispacther",
    "vllm.worker.tpu_model_runner.ModelWrapper"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/compilation/wrapper.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/tpu_model_runner.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "This commit introduces a custom dispatcher for torch.compile to avoid the Dynamo guard evaluation overhead. It adds new functionality in the compilation wrapper and modifies the ModelWrapper in the TPU model runner, which are non-test source files. These changes are geared toward reducing runtime overhead by dispatching directly to precompiled code paths and, thus, have a performance optimization focus. The modifications are non-trivial and affect top-level API behavior, meeting the performance-related criteria.",
  "llm_api_reason": "This commit introduces a custom dispatcher for torch.compile by adding a new wrapper class (with a slight typo in its name) in the vllm/compilation module. The changes add tests in the compile and TPU test suites to verify that the custom dispatch behavior works as intended (with dispatching based on whether a cache is provided) and to ensure that multiple compiled bytecodes are maintained independently. In addition, the TPU model runnerâ€™s ModelWrapper has been updated to subclass this new custom dispatcher so that it can dispatch to different compiled graphs (profiling, prompt, and decode) based on input parameters. Overall, the change aims to avoid the Dynamo guard evaluation overhead by switching directly between precompiled â€œcode objects.â€"
}