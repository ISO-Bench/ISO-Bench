{
  "commit_hash": "dcc6cfb991cd76369aad96e04424f29c8fecdbd8",
  "pr_url": "https://github.com/vllm-project/vllm/pull/21193",
  "pr_date": "2025-07-19",
  "timeline_text": "Copy link Contributor varun-sundar-rabindranath commented Jul 18, 2025 â€¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Purpose Tweak the num_warps and NUM_STAGES (num pipeline stages for prefetching) values of the kernel. Local micro-benchmark numbers: main: Benchmark: E=256, T=2048, H=7168, group_size=128, repeat=200\ntokens=4: \tquant_silu_mul 0.030ms \t\ntokens=8: \tquant_silu_mul 0.056ms \t\ntokens=16: \tquant_silu_mul 0.106ms \t\ntokens=32: \tquant_silu_mul 0.204ms \t\ntokens=64: \tquant_silu_mul 0.402ms \t\ntokens=128: \tquant_silu_mul 0.799ms \t\ntokens=256: \tquant_silu_mul 1.579ms \t\ntokens=384: \tquant_silu_mul 2.366ms \t\ntokens=512: \tquant_silu_mul 3.148ms \t\ntokens=1024: \tquant_silu_mul 6.272ms \t\ntokens=2048: \tquant_silu_mul 12.522ms This PR: Benchmark: E=256, T=2048, H=7168, group_size=128, repeat=200\ntokens=4: \tquant_silu_mul 0.017ms \t\ntokens=8: \tquant_silu_mul 0.032ms \t\ntokens=16: \tquant_silu_mul 0.057ms \t\ntokens=32: \tquant_silu_mul 0.108ms \t\ntokens=64: \tquant_silu_mul 0.211ms \t\ntokens=128: \tquant_silu_mul 0.417ms \t\ntokens=256: \tquant_silu_mul 0.830ms \t\ntokens=384: \tquant_silu_mul 1.234ms \t\ntokens=512: \tquant_silu_mul 1.639ms \t\ntokens=1024: \tquant_silu_mul 3.254ms \t\ntokens=2048: \tquant_silu_mul 6.514ms Note: micro-benchmarking script from https://github.com/tlrmchlsmth/ptgq_fp8 E2E Perf server command : VLLM_ALL2ALL_BACKEND=\"deepep_low_latency\" VLLM_USE_DEEP_GEMM=1  canhazgpu run -g 2 --  vllm serve Qwen/Qwen3-30B-A3B-FP8  --trust-remote-code --enable-expert-parallel --data-parallel-size 2 --port 9010 --no-enable-prefix-caching benchmark command : python3 ./benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dataset-name sharegpt --port 9010 --dataset-path ./ShareGPT_V3_unfiltered_cleaned_split.json Methodology: Start the server and execute the benchmark command 3 times. Report the best Total Token Throughput numbers. main : ============ Serving Benchmark Result ============\nSuccessful requests:                 \t1000 \t \nBenchmark duration (s):              \t32.44\t \nTotal input tokens:                  \t217393    \nTotal generated tokens:              \t201847    \nRequest throughput (req/s):          \t30.83\t \nOutput token throughput (tok/s):     \t6222.53   \nTotal Token throughput (tok/s):      \t12924.31  \n---------------Time to First Token----------------\nMean TTFT (ms):                      \t6470.31   \nMedian TTFT (ms):                    \t6734.54   \nP99 TTFT (ms):                       \t12538.94  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                      \t192.93    \nMedian TPOT (ms):                    \t76.87\t \nP99 TPOT (ms):                       \t773.24    \n---------------Inter-token Latency----------------\nMean ITL (ms):                       \t61.06\t \nMedian ITL (ms):                     \t35.02\t \nP99 ITL (ms):                        \t778.17    \n================================================== This PR: ============ Serving Benchmark Result ============\nSuccessful requests:                     1000      \nBenchmark duration (s):                  30.64     \nTotal input tokens:                      217393    \nTotal generated tokens:                  201847    \nRequest throughput (req/s):              32.64     \nOutput token throughput (tok/s):         6587.82   \nTotal Token throughput (tok/s):          13683.03  \n---------------Time to First Token----------------\nMean TTFT (ms):                          6416.49   \nMedian TTFT (ms):                        6604.24   \nP99 TTFT (ms):                           11718.61  \n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          174.51    \nMedian TPOT (ms):                        66.36     \nP99 TPOT (ms):                           776.26    \n---------------Inter-token Latency----------------\nMean ITL (ms):                           54.63     \nMedian ITL (ms):                         27.40     \nP99 ITL (ms):                            779.23    \n================================================== Test Plan local testing : pytest -s tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py e2e testing : server command : VLLM_ALL2ALL_BACKEND=\"deepep_low_latency\" VLLM_USE_DEEP_GEMM=1  canhazgpu run -g 2 --  vllm serve Qwen/Qwen3-30B-A3B-FP8  --trust-remote-code --enable-expert-parallel --data-parallel-size 2 --port 9010 --no-enable-prefix-caching lm_eval command : lm_eval --model local-completions --tasks gsm8k --model_args model=Qwen/Qwen3-30B-A3B-FP8,base_url=http://127.0.0.1:9010/v1/completions,num_concurrent=30,max_retries=3 --limit 100 Test Result tests/kernels/moe/test_silu_mul_fp8_quant_deep_gemm.py test passes locally lm_eval output : |Tasks|Version|     Filter     |n-shot|  Metric   |   |Value|   |Stderr|\n|-----|------:|----------------|-----:|-----------|---|----:|---|-----:|\n|gsm8k|      3|flexible-extract|     5|exact_match|â†‘  | 0.84|Â±  |0.0368|\n|     |       |strict-match    |     5|exact_match|â†‘  | 0.95|Â±  |0.0219| (Optional) Documentation Update Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions num_warps & num_stages tweak â€¦ f134464 Signed-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com> Copy link github-actions bot commented Jul 18, 2025 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. ðŸ’¬ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jul 18, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces performance optimizations to the silu_mul_fp8_quant_deep_gemm Triton kernel. The changes involve switching from a manual while loop to tl.range to enable software pipelining, and tuning the num_warps and NUM_STAGES parameters. The code modifications are correct and follow Triton best practices for performance. The provided micro-benchmarks demonstrate a significant performance improvement, which validates the tuning choices. The changes are well-contained and improve the efficiency of the kernel as intended. I have no further comments. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tlrmchlsmth approved these changes Jul 18, 2025 View reviewed changes tlrmchlsmth added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 18, 2025 tlrmchlsmth enabled auto-merge (squash) July 18, 2025 16:36 simon-mo disabled auto-merge July 19, 2025 06:09 Hide details View details simon-mo merged commit dcc6cfb into vllm-project : main Jul 19, 2025 78 of 79 checks passed Uh oh! There was an error while loading. Please reload this page . hj-mistral pushed a commit\n        to hj-mistral/vllm\n      that referenced\n      this pull request Jul 19, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 58ad0a6 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Himanshu Jaju <hj@mistral.ai> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ d07d2ed â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> tlrmchlsmth mentioned this pull request Jul 24, 2025 [RFC]: Data Parallel Attention and Expert Parallel MoEs #16037 Open 37 tasks avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 5ee1aab â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 5070713 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ c87a2d4 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 60013fe â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 7a09a5b â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 999d5e4 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ ef2c87e â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ cbc3340 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 27, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ 463fcc1 â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm â€¦ â€¦ ec28a1c â€¦kernel ( vllm-project#21193 )\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:33",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL: lm_eval, lm_eval, lm_eval | PERF: TTFT, TTFT, TTFT | SERVING: vllm serve, vllm serve, Serving | TEST: Test, Test, test",
  "analysis_extracted_at": "2025-09-07 17:50:33",
  "models": [
    "Qwen/Qwen3-30B-A3B-FP8"
  ],
  "lm_eval_commands": [
    "lm_eval --model vllm --model_args pretrained=Qwen/Qwen3-30B-A3B-FP8,dtype=float16 --tasks gsm8k --batch_size auto --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Qwen/Qwen3-30B-A3B-FP8 --dtype float16 --num-prompts 300 --seed 0",
  "commit_subject": "[Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)",
  "commit_message": "[Kernel][Performance] Tweak MoE Batched silu_mul_fp8_quant_deep_gemm kernel (#21193)\n\nSigned-off-by: Varun Sundar Rabindranath <vsundarr@redhat.com>\nCo-authored-by: Varun Sundar Rabindranath <vsundarr@redhat.com>",
  "commit_date": "2025-07-18T23:09:51-07:00",
  "files_changed": [
    "vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 4,
    "num_edited_lines": 9,
    "num_non_test_edited_lines": 9,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\nindex 628aa5c7b..3ccddb529 100644\n--- a/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\n+++ b/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\n@@ -55,6 +55,7 @@ def _silu_mul_fp8_quant_deep_gemm(\n \n     # Meta ---------------------------------------------------------------\n     BLOCK: tl.constexpr,\n+    NUM_STAGES: tl.constexpr,\n ):\n     G = H // GROUP_SIZE\n \n@@ -73,8 +74,7 @@ def _silu_mul_fp8_quant_deep_gemm(\n     cols = cols.to(tl.int64)\n     mask_h = cols < BLOCK\n \n-    t = tl.zeros([], tl.int64)\n-    while t < n_tokens:\n+    for t in tl.range(0, n_tokens, num_stages=NUM_STAGES):\n         base_i_offset = (e * stride_i_e + t * stride_i_t +\n                          g * GROUP_SIZE * stride_i_h)\n         base_yq_offset = (e * stride_yq_e + t * stride_yq_t +\n@@ -102,8 +102,6 @@ def _silu_mul_fp8_quant_deep_gemm(\n         tl.store(y_q_ptr + base_yq_offset + cols * stride_yq_h, y_q, mask=mask)\n         tl.store(y_s_ptr + base_ys_offset, y_s)\n \n-        t += 1\n-\n \n def silu_mul_fp8_quant_deep_gemm(\n     y: torch.Tensor,  # (E, T, 2*H) float32\n@@ -180,7 +178,8 @@ def silu_mul_fp8_quant_deep_gemm(\n         fp8_max,\n         is_blackwell_deep_gemm_used(),\n         BLOCK=group_size,\n-        num_warps=4,\n+        NUM_STAGES=8,\n+        num_warps=1,\n     )\n \n     return y_q, y_s",
  "apis": [
    "vllm.model_executor.layers.fused_moe.batched_deep_gemm_moe.silu_mul_fp8_quant_deep_gemm",
    "vllm.model_executor.layers.fused_moe.batched_deep_gemm_moe.BatchedDeepGemmExperts.apply"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/batched_triton_or_deep_gemm_moe.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/model_executor/layers/fused_moe/deep_gemm_moe.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies the file \"vllm/model_executor/layers/fused_moe/batched_deep_gemm_moe.py\", which is a non-test source file, changing the looping structure from a while loop to a for-loop call to tl.range with a parameter NUM_STAGES, and adjusting function parameters (NUM_STAGES and num_warps). These changes appear to alter the performance characteristics of the MoE deep gemm kernel. Although the commit title mentions performance, the changes are non-trivial and intended to affect execution efficiency rather than a mere refactoring or bug fix. The modifications are applicable on CPU and affect a core computational kernel, meeting the criteria for performance optimization.",
  "llm_api_reason": "The commit changes the kernel code in batched_deep_gemm_moe.py. It modifies the lowâ€level silu_mul_fp8_quant_deep_gemm kernel by switching from a whileâ€‘loop to a forâ€‘loop using a new NUM_STAGES parameter and adjusts kernel launch parameters (NUM_STAGES and num_warps). Since the silu_mul_fp8_quant_deep_gemm function is the entry point for this fused quantization kernel and is used by the BatchedDeepGemmExperts class (specifically in its apply method), both become affected APIs by this performance tweak."
}