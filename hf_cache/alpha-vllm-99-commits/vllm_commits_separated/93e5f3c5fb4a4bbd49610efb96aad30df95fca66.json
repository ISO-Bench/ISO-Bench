{
  "commit_hash": "93e5f3c5fb4a4bbd49610efb96aad30df95fca66",
  "pr_url": "https://github.com/vllm-project/vllm/pull/16484",
  "pr_date": "2025-04-12",
  "timeline_text": "Copy link Contributor SnowCharmQ commented Apr 11, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . This PR enhances the performance of the method _prepare_inputs in gpu_model_runner.py by replacing the original Python loop implementation with map and numpy array operations. On my clusters, it can achieve nearly a twofold performance improvement. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Optimize prepare inputs for GPU model runner 7018c25 SnowCharmQ requested review from WoosukKwon , robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners April 11, 2025 13:00 Copy link github-actions bot commented Apr 11, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . mergify bot added\n  the v1 label Apr 11, 2025 Format code ‚Ä¶ d150bd3 Signed-off-by: snowcharm <snowcharmqq@gmail.com> njhill reviewed Apr 11, 2025 View reviewed changes Copy link Member njhill left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks @SnowCharmQ , this is great! On my clusters, it can achieve nearly a twofold performance improvement. Presumably you're referring to the improvement of this loop, not end-to-end? :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/v1/worker/gpu_model_runner.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author SnowCharmQ commented Apr 11, 2025 Thanks @SnowCharmQ , this is great! On my clusters, it can achieve nearly a twofold performance improvement. Presumably you're referring to the improvement of this loop, not end-to-end? :) Hi @njhill , the improvement refers to the loop exactly. Sorry for the confusion :) üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Improve readability ‚Ä¶ ef6fdea Co-authored-by: Nick Hill <nhill@redhat.com> njhill added ready ONLY add when PR is ready to merge/full CI is needed performance Performance-related issues labels Apr 11, 2025 njhill approved these changes Apr 11, 2025 View reviewed changes Copy link Contributor Author SnowCharmQ commented Apr 12, 2025 Hi @njhill , I noticed an issue with the CI check. Do you have any idea what might be going wrong and how it can be resolved? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details DarkLight1337 merged commit 93e5f3c into vllm-project : main Apr 12, 2025 56 of 57 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Member DarkLight1337 commented Apr 12, 2025 I retried the test and it passes now üëç 1 njhill reacted with thumbs up emoji All reactions üëç 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . SnowCharmQ deleted the perf-runner branch April 19, 2025 08:44 yangw-dev pushed a commit\n        to yangw-dev/vllm\n      that referenced\n      this pull request Apr 21, 2025 [Perf] Optimize Preparing Inputs for GPU Model Runner ( vllm-project#1‚Ä¶ ‚Ä¶ 17c1504 ‚Ä¶6484 )\n\nSigned-off-by: snowcharm <snowcharmqq@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Yang Wang <elainewy@meta.com> jikunshang pushed a commit\n        to jikunshang/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Perf] Optimize Preparing Inputs for GPU Model Runner ( vllm-project#1‚Ä¶ ‚Ä¶ 7b6eb48 ‚Ä¶6484 )\n\nSigned-off-by: snowcharm <snowcharmqq@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com> lk-chen pushed a commit\n        to lk-chen/vllm\n      that referenced\n      this pull request Apr 29, 2025 [Perf] Optimize Preparing Inputs for GPU Model Runner ( vllm-project#1‚Ä¶ ‚Ä¶ 3e46b61 ‚Ä¶6484 )\n\nSigned-off-by: snowcharm <snowcharmqq@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com> RichardoMrMu pushed a commit\n        to RichardoMrMu/vllm\n      that referenced\n      this pull request May 12, 2025 [Perf] Optimize Preparing Inputs for GPU Model Runner ( vllm-project#1‚Ä¶ ‚Ä¶ 5e88ae2 ‚Ä¶6484 )\n\nSigned-off-by: snowcharm <snowcharmqq@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>\nSigned-off-by: Mu Huai <tianbowen.tbw@antgroup.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:51:25",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: improvement, improvement, improvement | TEST: test, test, CI",
  "analysis_extracted_at": "2025-09-07 17:51:25",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Perf] Optimize Preparing Inputs for GPU Model Runner (#16484)",
  "commit_message": "[Perf] Optimize Preparing Inputs for GPU Model Runner (#16484)\n\nSigned-off-by: snowcharm <snowcharmqq@gmail.com>\nCo-authored-by: Nick Hill <nhill@redhat.com>",
  "commit_date": "2025-04-12T22:54:37+08:00",
  "files_changed": [
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 1,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 1,
    "num_hunks": 1,
    "num_edited_lines": 12,
    "num_non_test_edited_lines": 12,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 0e70d77e1..70e8bd75e 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -484,14 +484,10 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         self.input_batch.block_table.commit(num_reqs)\n \n         # Get the number of scheduled tokens for each request.\n-        # TODO: The Python loop can be slow. Optimize.\n-        num_scheduled_tokens = np.empty(num_reqs, dtype=np.int32)\n-        max_num_scheduled_tokens = 0\n-        for i, req_id in enumerate(self.input_batch.req_ids):\n-            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n-            num_scheduled_tokens[i] = num_tokens\n-            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n-                                           num_tokens)\n+        req_ids = self.input_batch.req_ids\n+        tokens = [scheduler_output.num_scheduled_tokens[i] for i in req_ids]\n+        num_scheduled_tokens = np.array(tokens, dtype=np.int32)\n+        max_num_scheduled_tokens = max(tokens)\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]",
  "apis": [
    "vllm.v1.worker.gpu_model_runner.GPUModelRunner._prepare_inputs"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/worker/model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies a non-test source file (gpu_model_runner.py) by replacing a Python loop with a list comprehension and numpy array conversion to prepare token inputs. This change is non-trivial and targets the performance of the data preparation stage, which runs on the CPU even though it is part of the GPU model runner. The commit message multiplies performance implications and the change optimizes a CPU-bound operation without being specific to only one hardware type. Therefore, it satisfies the performance/optimization conditions.",
  "llm_api_reason": "This commit optimizes the way GPUModelRunner prepares input tokens by replacing an explicit Python loop with a list comprehension and a call to np.array, which results in faster computation of the number of scheduled tokens and the maximum scheduled token count. This change affects the internal method responsible for preparing inputs in the GPUModelRunner."
}