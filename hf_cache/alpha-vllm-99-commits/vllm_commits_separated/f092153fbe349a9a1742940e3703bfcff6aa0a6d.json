{
  "commit_hash": "f092153fbe349a9a1742940e3703bfcff6aa0a6d",
  "pr_url": "https://github.com/vllm-project/vllm/pull/11111",
  "pr_date": "2024-12-12",
  "timeline_text": "Copy link Collaborator WoosukKwon commented Dec 11, 2024 â€¢ edited Loading Uh oh! There was an error while loading. Please reload this page . This PR simplifies the input preparation code further while optimizing it by utilizing more persistent buffers. Creating new tensors can introduce considerable overhead for small-batch inputs, so persistent buffers effectively reduce latency. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions tmp â€¦ 73a8b20 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Copy link github-actions bot commented Dec 11, 2024 ðŸ‘‹ Hi! Thank you for contributing to the vLLM project. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can do one of these: Add ready label to the PR Enable auto-merge. ðŸš€ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . WoosukKwon added 2 commits December 11, 2024 11:13 comment â€¦ dbac8f5 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> comment â€¦ 734a7b7 Signed-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> WoosukKwon marked this pull request as ready for review December 11, 2024 19:15 WoosukKwon requested review from robertgshaw2-redhat , njhill , ywang96 , comaniac and alexm-redhat as code owners December 11, 2024 19:15 WoosukKwon added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Dec 11, 2024 Copy link Collaborator alexm-redhat commented Dec 11, 2024 Nice idea! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . alexm-redhat approved these changes Dec 11, 2024 View reviewed changes vllm/v1/worker/gpu_model_runner.py dtype=torch.int32, device=\"cpu\", pin_memory=self.pin_memory) self.slot_mapping_np = self.slot_mapping_cpu.numpy() Copy link Collaborator alexm-redhat Dec 11, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Does the resulting numpy here shares the memory buffer of the source tensor? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Author WoosukKwon Dec 12, 2024 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The returned ndarray and the tensor will share their storage, so changes to the tensor will be reflected in the ndarray and vice versa. Yes. That's the trick here :) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details WoosukKwon merged commit f092153 into main Dec 12, 2024 65 checks passed Uh oh! There was an error while loading. Please reload this page . WoosukKwon deleted the v1-opt-prep branch December 12, 2024 07:14 markmc mentioned this pull request Dec 12, 2024 Enable mypy checking on V1 code #11105 Merged sleepwalker2017 pushed a commit\n        to sleepwalker2017/vllm\n      that referenced\n      this pull request Dec 13, 2024 [V1] Use more persistent buffers to optimize input preparation overheâ€¦ â€¦ 2e703c8 â€¦ads ( vllm-project#11111 )\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:47:32",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF: latency | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:47:32",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)",
  "commit_message": "[V1] Use more persistent buffers to optimize input preparation overheads (#11111)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "commit_date": "2024-12-11T23:14:20-08:00",
  "files_changed": [
    "vllm/v1/worker/gpu_input_batch.py",
    "vllm/v1/worker/gpu_model_runner.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 2,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 2,
    "num_hunks": 7,
    "num_edited_lines": 138,
    "num_non_test_edited_lines": 138,
    "commit_year": 2024
  },
  "diff_text": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex 25d95ac6e..9046b37f6 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -53,14 +53,23 @@ class InputBatch:\n         self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n         self.req_id_to_index: Dict[str, int] = {}\n \n-        self.token_ids_cpu = np.empty((max_num_reqs, max_model_len),\n-                                      dtype=np.int32)\n+        # TODO(woosuk): This buffer could be too large if max_model_len is big.\n+        # Find a way to reduce the CPU memory usage.\n+        self.token_ids_cpu_tensor = torch.zeros(\n+            (max_num_reqs, max_model_len),\n+            device=\"cpu\",\n+            dtype=torch.int32,\n+            pin_memory=pin_memory,\n+        )\n+        self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n         self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros((max_num_reqs, max_num_blocks_per_req),\n-                                       device=self.device,\n-                                       dtype=torch.int32)\n+        self.block_table = torch.zeros(\n+            (max_num_reqs, max_num_blocks_per_req),\n+            device=self.device,\n+            dtype=torch.int32,\n+        )\n         self.block_table_cpu_tensor = torch.zeros(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex e75be21ef..aa91255e6 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -67,6 +67,7 @@ class GPUModelRunner:\n         self.max_model_len = model_config.max_model_len\n         self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n         self.max_num_tokens = scheduler_config.max_num_batched_tokens\n+        self.max_num_reqs = scheduler_config.max_num_seqs\n \n         # Model-related.\n         self.num_attn_layers = model_config.get_num_layers_by_block_type(\n@@ -88,7 +89,7 @@ class GPUModelRunner:\n         self.requests: Dict[str, CachedRequestState] = {}\n         # Persistent batch.\n         self.input_batch = InputBatch(\n-            max_num_reqs=self.scheduler_config.max_num_seqs,\n+            max_num_reqs=self.max_num_reqs,\n             max_model_len=self.max_model_len,\n             max_num_blocks_per_req=self.max_num_blocks_per_req,\n             device=self.device,\n@@ -117,6 +118,32 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int32,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.input_ids_np = self.input_ids_cpu.numpy()\n+        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+                                         dtype=torch.int64,\n+                                         device=\"cpu\",\n+                                         pin_memory=self.pin_memory)\n+        self.positions_np = self.positions_cpu.numpy()\n+        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+                                            dtype=torch.int32,\n+                                            device=\"cpu\",\n+                                            pin_memory=self.pin_memory)\n+        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                               dtype=torch.int32,\n+                                               device=\"cpu\",\n+                                               pin_memory=self.pin_memory)\n+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+                                             dtype=torch.int32,\n+                                             device=\"cpu\",\n+                                             pin_memory=self.pin_memory)\n+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n         # Keep the states of the pre-empted requests.\n@@ -241,22 +268,14 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        indices = np.arange(num_reqs)\n-        req_indices = np.repeat(indices, num_scheduled_tokens)\n+        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange_matrix = np.tile(np.arange(max_num_scheduled_tokens),\n-                                (num_reqs, 1))\n-        mask = arange_matrix < num_scheduled_tokens[:, np.newaxis]\n-        arange = arange_matrix[mask]\n+        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n \n         # Get positions.\n-        positions = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        positions_np = positions.numpy()\n+        positions_np = self.positions_np[:total_num_scheduled_tokens]\n         np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n                arange,\n                out=positions_np)\n@@ -267,16 +286,13 @@ class GPUModelRunner:\n         # where M is the max_model_len.\n         token_indices = (positions_np +\n                          req_indices * self.input_batch.token_ids_cpu.shape[1])\n-        token_indices = torch.from_numpy(token_indices)\n-        input_ids = torch.empty((total_num_scheduled_tokens, ),\n-                                dtype=torch.int32,\n-                                device=\"cpu\",\n-                                pin_memory=self.pin_memory)\n-        torch.index_select(torch.from_numpy(\n-            self.input_batch.token_ids_cpu).flatten(),\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                            0,\n-                           token_indices,\n-                           out=input_ids)\n+                           torch.from_numpy(token_indices),\n+                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n \n         # Calculate the slot mapping.\n         # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n@@ -284,45 +300,40 @@ class GPUModelRunner:\n         # where K is the max_num_blocks_per_req and the block size is 2.\n         # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n         # because M (max_model_len) is not necessarily divisible by block_size.\n-        block_numbers = self.input_batch.block_table_cpu_tensor.flatten()[\n-            req_indices * self.max_num_blocks_per_req +\n-            positions_np // self.block_size]\n-        block_offsets = torch.from_numpy(positions_np % self.block_size)\n-        slot_mapping = torch.empty((total_num_scheduled_tokens, ),\n-                                   dtype=torch.int32,\n-                                   device=\"cpu\",\n-                                   pin_memory=self.pin_memory)\n-        torch.add(block_numbers * self.block_size,\n-                  block_offsets,\n-                  out=slot_mapping)\n+        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n+                               positions_np // self.block_size)\n+        # NOTE(woosuk): We use torch.index_select instead of np.take here\n+        # because torch.index_select is much faster than np.take for large\n+        # tensors.\n+        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n+                         [block_table_indices].numpy())\n+        block_offsets = positions_np % self.block_size\n+        np.add(block_numbers * self.block_size,\n+               block_offsets,\n+               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n \n         # Prepare the attention metadata.\n-        query_start_loc = torch.empty((num_reqs + 1, ),\n-                                      dtype=torch.int32,\n-                                      device=\"cpu\",\n-                                      pin_memory=self.pin_memory)\n-        query_start_loc_np = query_start_loc.numpy()\n-        query_start_loc_np[0] = 0\n-        np.cumsum(num_scheduled_tokens, out=query_start_loc_np[1:])\n+        self.query_start_loc_np[0] = 0\n+        np.cumsum(num_scheduled_tokens,\n+                  out=self.query_start_loc_np[1:num_reqs + 1])\n \n         seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                     num_scheduled_tokens)\n         max_seq_len = seq_lens.max()\n-        seq_start_loc = torch.empty((num_reqs + 1, ),\n-                                    dtype=torch.int32,\n-                                    device=\"cpu\",\n-                                    pin_memory=self.pin_memory)\n-        seq_start_loc_np = seq_start_loc.numpy()\n-        seq_start_loc_np[0] = 0\n-        np.cumsum(seq_lens, out=seq_start_loc_np[1:])\n-\n-        self.input_ids[:total_num_scheduled_tokens].copy_(input_ids,\n-                                                          non_blocking=True)\n-        self.positions[:total_num_scheduled_tokens].copy_(positions,\n-                                                          non_blocking=True)\n-        query_start_loc = query_start_loc.to(self.device, non_blocking=True)\n-        seq_start_loc = seq_start_loc.to(self.device, non_blocking=True)\n-        slot_mapping = slot_mapping.to(self.device, non_blocking=True).long()\n+        self.seq_start_loc_np[0] = 0\n+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])\n+\n+        # Copy the tensors to the GPU.\n+        self.input_ids[:total_num_scheduled_tokens].copy_(\n+            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        self.positions[:total_num_scheduled_tokens].copy_(\n+            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n+        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n+            self.device, non_blocking=True)\n+        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n+            self.device, non_blocking=True).long()\n         attn_metadata = FlashAttentionMetadata(\n             num_actual_tokens=total_num_scheduled_tokens,\n             max_query_len=max_num_scheduled_tokens,",
  "apis": [
    "InputBatch.__init__",
    "GPUModelRunner.__init__",
    "GPUModelRunner._update_states"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_model_runner.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/worker/gpu_input_batch.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/engine/llm_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/v1/engine/llm_engine.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit modifies non-test source files in the \"vllm/v1/worker\" directory. It replaces existing buffer allocation with persistent buffers using torch tensors (with pinned memory) and adjusts data indexing and copying operations. The changes are non-trivial, modifying data structures to reduce input preparation overheads, which is a performance optimization. It doesnâ€™t merely refactor code or address a bug, but enhances performance on CPU for the high-level API that handles input batching. Although GPU interactions exist, the optimizations primarily target CPU memory operations. Overall, the commit meets the criteria for a performance optimization commit.",
  "llm_api_reason": "This commit replaces dynamicallyâ€allocated numpy buffers with preâ€allocated persistent torch tensors (later converted to numpy) in the InputBatch constructor and also adds several new persistent CPU buffers in GPUModelRunner (e.g. input_ids_cpu, positions_cpu, slot_mapping_cpu, query_start_loc_cpu, and seq_start_loc_cpu). These changes optimize the input preparation workflow by reducing onâ€‘theâ€‘fly memory allocation overheads during state updates and index computations."
}