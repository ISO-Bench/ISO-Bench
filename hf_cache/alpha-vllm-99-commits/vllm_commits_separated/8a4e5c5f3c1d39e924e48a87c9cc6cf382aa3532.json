{
  "commit_hash": "8a4e5c5f3c1d39e924e48a87c9cc6cf382aa3532",
  "pr_url": "https://github.com/vllm-project/vllm/pull/20906",
  "pr_date": "2025-07-17",
  "timeline_text": "Copy link Contributor Abatom commented Jul 14, 2025 ‚Ä¢ edited by github-actions bot Loading Uh oh! There was an error while loading. Please reload this page . Enhance Performance and code readability for P2pNcclConnector, follow-up #18242 . The design document and user manual can be found in docs/design/v1/p2p_nccl_connector.md . The KVCache sender offloads the KVCache extraction and reshape operations to a dedicated sending thread, thereby reducing the load on the main thread and lowering the TTFT. Fix the issue of occasional garbled output when receiving a temporarily created empty tensor and ncclRecv are not in the same stream for \"GET\". Optimize the proxy so that when there are no active requests, instances P or D can be automatically removed. Handling abnormal crashes will be addressed in a follow-up PR; in PR [V1][P/D]Support automatic instance removal after crash for P2pNcclConnector #20006 I attempted to cover this scenario and found that removal is far more complex than scaling up. Resolving the P2pNcclConnector crash caused by PR [KVConnector] Aggregate finished requests on the scheduler #19555 . In PR [V1][P/D]Enhance Performance for P2pNcclConnector #20074 , I attempted to support asynchronous loading of the KV cache and release the KV cache after sending is completed; however, performance actually degraded. Once this PR is merged, I will revisit these two features on top of it. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Abatom added 8 commits July 13, 2025 22:57 reshape ‚Ä¶ 4c9bbfd Signed-off-by: Abatom <abzhonghua@gmail.com> Merge branch 'main' into send 6efd67d bool ‚Ä¶ 8144527 Signed-off-by: Abatom <abzhonghua@gmail.com> bool ‚Ä¶ 2f71a7d Signed-off-by: Abatom <abzhonghua@gmail.com> staticmethod ‚Ä¶ 31cff5c Signed-off-by: Abatom <abzhonghua@gmail.com> code readability ‚Ä¶ d3c9ee0 Signed-off-by: Abatom <abzhonghua@gmail.com> proxy ‚Ä¶ d72dfb9 Signed-off-by: Abatom <abzhonghua@gmail.com> mod doc ‚Ä¶ 8ed454d Signed-off-by: Abatom <abzhonghua@gmail.com> Abatom requested a review\n  from hmellor as a code owner July 14, 2025 07:07 mergify bot added\n  the documentation Improvements or additions to documentation label Jul 14, 2025 Copy link github-actions bot commented Jul 14, 2025 üëã Hi! Thank you for contributing to the vLLM project. üí¨ Join our developer Slack at https://slack.vllm.ai to discuss your PR in #pr-reviews, coordinate on features in #feat- channels, or join special interest groups in #sig- channels. Just a reminder: PRs would not trigger full CI run by default. Instead, it would only run fastcheck CI which starts running only a small and essential subset of CI tests to quickly catch errors. You can run other CI tests on top of those by going to your fastcheck build on Buildkite UI (linked in the PR checks section) and unblock them. If you do not have permission to unblock, ping simon-mo or khluu to add you in our Buildkite org. Once the PR is approved and ready to go, your PR reviewer(s) can run CI to test the changes comprehensively before merging. To run CI, PR reviewers can either: Add ready label to the PR or enable auto-merge. üöÄ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . gemini-code-assist bot reviewed Jul 14, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @Abatom , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request focuses on enhancing the performance and code readability of the P2pNcclConnector. It introduces optimizations to reduce main thread load, fixes a garbled output issue, and optimizes proxy resource management. Additionally, internal methods have been refactored to improve code quality. Highlights Performance Enhancement : Offloads KVCache extraction and reshape operations to a dedicated sending thread, reducing the load on the main thread and lowering TTFT. Bug Fix : Resolves an issue causing occasional garbled output when receiving a temporarily created empty tensor, ensuring ncclRecv operations are synchronized. Proxy Optimization : Improves proxy functionality by automatically removing P or D instances when there are no active requests, optimizing resource utilization. Code Modernization : Refactors internal methods to improve code readability and maintainability. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with üëç and üëé on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . ‚Ü© Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jul 14, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces significant performance and readability enhancements for the P2pNcclConnector . Key changes include offloading KVCache extraction to a dedicated sending thread, fixing a race condition with ncclRecv by ensuring operations are on the same CUDA stream, and implementing automatic cleanup of stale proxy instances. The refactoring of methods from private to public in p2p_nccl_engine.py also improves the API design. Overall, the changes are well-aligned with the PR's objectives. I've identified one critical bug related to a method rename that was missed, which would cause a runtime error. I've also included a few medium-severity suggestions to improve code clarity and maintainability. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Abatom added 10 commits July 14, 2025 15:21 have_sent_tensor_id ‚Ä¶ ca02a98 Signed-off-by: Abatom <abzhonghua@gmail.com> mod log ‚Ä¶ 4441b2c Signed-off-by: Abatom <abzhonghua@gmail.com> SendQueueItem ‚Ä¶ 8adac0c Signed-off-by: Abatom <abzhonghua@gmail.com> console ‚Ä¶ 416e6b7 Signed-off-by: Abatom <abzhonghua@gmail.com> console ‚Ä¶ 81b2f0b Signed-off-by: Abatom <abzhonghua@gmail.com> format ‚Ä¶ 847282b Signed-off-by: Abatom <abzhonghua@gmail.com> PUT_ASYNC ‚Ä¶ f97ecf9 Signed-off-by: Abatom <abzhonghua@gmail.com> mod doc ‚Ä¶ 5eb5edc Signed-off-by: Abatom <abzhonghua@gmail.com> format ‚Ä¶ b85043e Signed-off-by: Abatom <abzhonghua@gmail.com> SPDX ‚Ä¶ 8126ed0 Signed-off-by: Abatom <abzhonghua@gmail.com> Abatom changed the title [WIP][V1][P/D]Enhance Performance and code readability for P2pNcclConnector [V1][P/D]Enhance Performance and code readability for P2pNcclConnector Jul 14, 2025 Abatom added 5 commits July 15, 2025 16:45 mod doc ‚Ä¶ a5fcacd Signed-off-by: Abatom <abzhonghua@gmail.com> Merge branch 'main' into send 4256b01 no_compile_layers ‚Ä¶ 6af393a Signed-off-by: Abatom <abzhonghua@gmail.com> format ‚Ä¶ cd11f33 Signed-off-by: Abatom <abzhonghua@gmail.com> mod doc ‚Ä¶ 113993c Signed-off-by: Abatom <abzhonghua@gmail.com> KuntaiDu approved these changes Jul 16, 2025 View reviewed changes Copy link Collaborator KuntaiDu left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ‚ù§Ô∏è 1 Abatom reacted with heart emoji All reactions ‚ù§Ô∏è 1 reaction simon-mo added\n  the ready ONLY add when PR is ready to merge/full CI is needed label Jul 17, 2025 Hide details View details simon-mo merged commit 8a4e5c5 into vllm-project : main Jul 17, 2025 80 of 82 checks passed Uh oh! There was an error while loading. Please reload this page . hj-mistral pushed a commit\n        to hj-mistral/vllm\n      that referenced\n      this pull request Jul 19, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ cc76e0b vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Himanshu Jaju <hj@mistral.ai> LyrisZhong pushed a commit\n        to LyrisZhong/vllm\n      that referenced\n      this pull request Jul 23, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ 2f0aa79 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> avigny pushed a commit\n        to avigny/vllm\n      that referenced\n      this pull request Jul 31, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ aef48d4 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: avigny <47987522+avigny@users.noreply.github.com> x22x22 pushed a commit\n        to x22x22/vllm\n      that referenced\n      this pull request Aug 5, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ e9ea31d vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: x22x22 <wadeking@qq.com> Pradyun92 pushed a commit\n        to Pradyun92/vllm\n      that referenced\n      this pull request Aug 6, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ be0e12d vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> npanpaliya pushed a commit\n        to odh-on-pz/vllm-upstream\n      that referenced\n      this pull request Aug 6, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ e2e9f64 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> jinzhen-lin pushed a commit\n        to jinzhen-lin/vllm\n      that referenced\n      this pull request Aug 9, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ a3af660 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Jinzhen Lin <linjinzhen@hotmail.com> paulpak58 pushed a commit\n        to paulpak58/vllm\n      that referenced\n      this pull request Aug 13, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ e42182b vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Paul Pak <paulpak58@gmail.com> taneem-ibrahim pushed a commit\n        to taneem-ibrahim/vllm\n      that referenced\n      this pull request Aug 14, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ 65558a4 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> diegocastanibm pushed a commit\n        to diegocastanibm/vllm\n      that referenced\n      this pull request Aug 15, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ f189c1c vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>\nSigned-off-by: Diego-Castan <diego.castan@ibm.com> Abatom mentioned this pull request Aug 22, 2025 [Bugfix][V1][P/D]Fix the issue where repeated requests for the same input produce abnormal outputs for P2pNcclConnector #23403 Merged 4 tasks epwalsh pushed a commit\n        to epwalsh/vllm\n      that referenced\n      this pull request Aug 27, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ 67bdc76 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> googlercolin pushed a commit\n        to googlercolin/vllm\n      that referenced\n      this pull request Aug 29, 2025 [V1][P/D]Enhance Performance and code readability for P2pNcclConnector ( ‚Ä¶ bc17546 vllm-project#20906 )\n\nSigned-off-by: Abatom <abzhonghua@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-07 17:50:37",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF: TTFT, TTFT, Optimization | SERVING: online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . examples/online_serving | TEST: test, CI, CI",
  "analysis_extracted_at": "2025-09-07 17:50:37",
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)",
  "commit_message": "[V1][P/D]Enhance Performance and code readability for P2pNcclConnector (#20906)\n\nSigned-off-by: Abatom <abzhonghua@gmail.com>",
  "commit_date": "2025-07-16T22:13:00-07:00",
  "files_changed": [
    "docs/design/v1/p2p_nccl_connector.md",
    "examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py",
    "vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py",
    "vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py"
  ],
  "functions_changed": [],
  "stats": {
    "num_test_files": 0,
    "num_non_test_files": 4,
    "only_test_files": 0,
    "only_non_test_files": 1,
    "num_files": 4,
    "num_hunks": 48,
    "num_edited_lines": 522,
    "num_non_test_edited_lines": 522,
    "commit_year": 2025
  },
  "diff_text": "diff --git a/docs/design/v1/p2p_nccl_connector.md b/docs/design/v1/p2p_nccl_connector.md\nindex b1df93cfc..8f6a2b3b2 100644\n--- a/docs/design/v1/p2p_nccl_connector.md\n+++ b/docs/design/v1/p2p_nccl_connector.md\n@@ -31,7 +31,7 @@ Each P/D instance periodically sends a heartbeat packet to the Proxy/Router (cur\n \n ## KV Cache Transfer Methods\n \n-There are three methods for KVcache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVcache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVcache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVcache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVcache from the P instance once it has allocated space for the KVcache.\n+There are three methods for KVCache transfer: PUT, GET, and PUT_ASYNC. These methods can be specified using the `--kv-transfer-config` and `kv_connector_extra_config` parameters, specifically through the `send_type` field. Both PUT and PUT_ASYNC involve the P instance actively sending KVCache to the D instance. The difference is that PUT is a synchronous transfer method that blocks the main process, while PUT_ASYNC is an asynchronous transfer method. PUT_ASYNC uses a dedicated thread for sending KVCache, which means it does not block the main process. In contrast, the GET method involves the P instance saving the KVCache to the memory buffer after computing the prefill. The D instance then actively retrieves the computed KVCache from the P instance once it has allocated space for the KVCache.\n \n Experimental results have shown that the performance of these methods, from highest to lowest, is as follows: PUT_ASYNC ‚Üí GET ‚Üí PUT.\n \n@@ -39,13 +39,13 @@ Experimental results have shown that the performance of these methods, from high\n \n As long as the address of the counterpart is known, point-to-point KV cache transfer (using NCCL) can be performed, without being constrained by rank and world size. To support dynamic scaling (expansion and contraction) of instances with PD disaggregation. This means that adding or removing P/D instances does not require a full system restart.\n \n-Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVcache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVcache data itself.\n+Each P/D instance only needs to create a single `P2pNcclEngine` instance. This instance maintains a ZMQ Server, which runs a dedicated thread to listen on the `zmq_addr` address and receive control flow requests from other instances. These requests include requests to establish an NCCL connection and requests to send KVCache metadata (such as tensor shapes and data types). However, it does not actually transmit the KVCache data itself.\n \n-When a P instance and a D instance transmit KVcache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVcache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVcache transmission can be performed, without being restricted by rank or world size.\n+When a P instance and a D instance transmit KVCache for the first time, they need to establish a ZMQ connection and an NCCL group. For subsequent KVCache transmissions, this ZMQ connection and NCCL group are reused. The NCCL group consists of only two ranks, meaning the world size is equal to 2. This design is intended to support dynamic scaling, which means that adding or removing P/D instances does not require a full system restart. As long as the address of the counterpart is known, point-to-point KVCache transmission can be performed, without being restricted by rank or world size.\n \n ## NCCL Group Topology\n \n-Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVcache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.\n+Currently, only symmetric TP (Tensor Parallelism) methods are supported for KVCache transmission. Asymmetric TP and PP (Pipeline Parallelism) methods will be supported in the future. Figure 2 illustrates the 1P2D setup, where each instance has a TP (Tensor Parallelism) degree of 2. There are a total of 7 NCCL groups: three vLLM instances each have one NCCL group with TP=2. Additionally, the 0th GPU card of the P instance establishes an NCCL group with the 0th GPU card of each D instance. Similarly, the 1st GPU card of the P instance establishes an NCCL group with the 1st GPU card of each D instance.\n \n ![image2](https://github.com/user-attachments/assets/837e61d6-365e-4cbf-8640-6dd7ab295b36)\n \n@@ -53,32 +53,18 @@ Each NCCL group occupies a certain amount of GPU memory buffer for communication\n \n ## GPU Memory Buffer and Tensor Memory Pool\n \n-The trade-off in the size of the memory buffer is as follows: For P instances, the memory buffer is not required in PUT and PUT_ASYNC modes, but it is necessary in GET mode. For D instances, a memory buffer is needed in all three modes. The memory buffer for D instances should not be too large. Similarly, for P instances in GET mode, the memory buffer should also not be too large. The memory buffer of D instances is used to temporarily store KVcache sent by P instances. If it is too large, it will reduce the KVcache space available for normal inference by D instances, thereby decreasing the inference batch size and ultimately leading to a reduction in output throughput. The size of the memory buffer is configured by the parameter `kv_buffer_size`, measured in bytes, and is typically set to 5%ÔΩû10% of the memory size.\n+The trade-off in the size of the memory buffer is as follows: For P instances, the memory buffer is not required in PUT and PUT_ASYNC modes, but it is necessary in GET mode. For D instances, a memory buffer is needed in all three modes. The memory buffer for D instances should not be too large. Similarly, for P instances in GET mode, the memory buffer should also not be too large. The memory buffer of D instances is used to temporarily store KVCache sent by P instances. If it is too large, it will reduce the KVCache space available for normal inference by D instances, thereby decreasing the inference batch size and ultimately leading to a reduction in output throughput. The size of the memory buffer is configured by the parameter `kv_buffer_size`, measured in bytes, and is typically set to 5%ÔΩû10% of the memory size.\n \n-If the `--max-num-seqs` parameter for P instances is set to a large value, due to the large batch size, P instances will generate a large amount of KVcache simultaneously. This may exceed the capacity of the memory buffer of D instances, resulting in KVcache loss. Once KVcache is lost, D instances need to recompute Prefill, which is equivalent to performing Prefill twice. Consequently, the time-to-first-token (TTFT) will significantly increase, leading to degraded performance.\n+If the `--max-num-seqs` parameter for P instances is set to a large value, due to the large batch size, P instances will generate a large amount of KVCache simultaneously. This may exceed the capacity of the memory buffer of D instances, resulting in KVCache loss. Once KVCache is lost, D instances need to recompute Prefill, which is equivalent to performing Prefill twice. Consequently, the time-to-first-token (TTFT) will significantly increase, leading to degraded performance.\n \n-To address the above issues, I have designed and developed a local Tensor memory pool for storing KVcache, inspired by the buddy system used in Linux memory modules. Since the memory is sufficiently large, typically in the TB range on servers, there is no need to consider prefix caching or using block-based designs to reuse memory, thereby saving space. When the memory buffer is insufficient, KVcache can be directly stored in the Tensor memory pool, and D instances can subsequently retrieve KVcache from it. The read and write speed is that of PCIe, with PCIe 4.0 having a speed of approximately 21 GB/s, which is usually faster than the Prefill speed. Otherwise, solutions like Mooncake and lmcache would not be necessary. The Tensor memory pool acts as a flood diversion area, typically unused except during sudden traffic surges. In the worst-case scenario, my solution performs no worse than the normal situation with a Cache store.\n+To address the above issues, I have designed and developed a local Tensor memory pool for storing KVCache, inspired by the buddy system used in Linux memory modules. Since the memory is sufficiently large, typically in the TB range on servers, there is no need to consider prefix caching or using block-based designs to reuse memory, thereby saving space. When the memory buffer is insufficient, KVCache can be directly stored in the Tensor memory pool, and D instances can subsequently retrieve KVCache from it. The read and write speed is that of PCIe, with PCIe 4.0 having a speed of approximately 21 GB/s, which is usually faster than the Prefill speed. Otherwise, solutions like Mooncake and lmcache would not be necessary. The Tensor memory pool acts as a flood diversion area, typically unused except during sudden traffic surges. In the worst-case scenario, my solution performs no worse than the normal situation with a Cache store.\n \n # Install vLLM\n \n ??? console \"Commands\"\n \n     ```shell\n-    # Enter the home directory or your working directory.\n-    cd /home\n-\n-    # Download the installation package, and I will update the commit-id in time. You can directly copy the command.\n-    wget https://vllm-wheels.s3.us-west-2.amazonaws.com/9112b443a042d8d815880b8780633882ad32b183/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n-\n-    # Download the code repository.\n-    git clone -b xpyd-v1 https://github.com/Abatom/vllm.git\n-    cd vllm\n-\n-    # Set the installation package path.\n-    export VLLM_PRECOMPILED_WHEEL_LOCATION=/home/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\n-\n-    # installation\n-    pip install -e . -v\n+    pip install \"vllm>=0.9.2\"\n     ```\n \n # Run xPyD\n@@ -90,7 +76,7 @@ To address the above issues, I have designed and developed a local Tensor memory\n - You may need to modify the `kv_buffer_size` and `port` in the following commands (if there is a conflict).\n - `PUT_ASYNC` offers the best performance and should be prioritized.\n - The `--port` must be consistent with the `http_port` in the `--kv-transfer-config`.\n-- The `disagg_prefill_proxy_xpyd.py` script will use port 10001 (for receiving client requests) and port 30001 (for receiving service discovery from P and D instances).\n+- The `disagg_proxy_p2p_nccl_xpyd.py` script will use port 10001 (for receiving client requests) and port 30001 (for receiving service discovery from P and D instances).\n - The node running the proxy must have `quart` installed.\n - Supports multiple nodes; you just need to modify the `proxy_ip` and `proxy_port` in `--kv-transfer-config`.\n - In the following examples, it is assumed that **the proxy's IP is 10.0.1.1**.\n@@ -100,8 +86,8 @@ To address the above issues, I have designed and developed a local Tensor memory\n ### Proxy (e.g. 10.0.1.1)\n \n ```shell\n-cd {your vllm directory}/examples/online_serving/disagg_xpyd/\n-python3 disagg_prefill_proxy_xpyd.py &\n+cd {your vllm directory}/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/\n+python3 disagg_proxy_p2p_nccl_xpyd.py &\n ```\n \n ### Prefill1 (e.g. 10.0.1.2 or 10.0.1.1)\n@@ -111,7 +97,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=0 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20005 \\\n+        --port 20001 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -123,7 +109,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.9 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20005\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20001\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Decode1 (e.g. 10.0.1.3 or 10.0.1.1)\n@@ -133,7 +119,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=1 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20009 \\\n+        --port 20002 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -145,7 +131,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.7 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20009\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20002\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Decode2 (e.g. 10.0.1.4 or 10.0.1.1)\n@@ -167,7 +153,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.7 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Decode3 (e.g. 10.0.1.5 or 10.0.1.1)\n@@ -177,7 +163,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=3 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20008 \\\n+        --port 20004 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -189,7 +175,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.7 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20008\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20004\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ## Run 3P1D\n@@ -197,8 +183,8 @@ python3 disagg_prefill_proxy_xpyd.py &\n ### Proxy (e.g. 10.0.1.1)\n \n ```shell\n-cd {your vllm directory}/examples/online_serving/disagg_xpyd/\n-python3 disagg_prefill_proxy_xpyd.py &\n+cd {your vllm directory}/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/\n+python3 disagg_proxy_p2p_nccl_xpyd.py &\n ```\n \n ### Prefill1 (e.g. 10.0.1.2 or 10.0.1.1)\n@@ -208,7 +194,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=0 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20005 \\\n+        --port 20001 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -220,7 +206,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.9 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20005\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"21001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20001\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Prefill2 (e.g. 10.0.1.3 or 10.0.1.1)\n@@ -230,7 +216,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=1 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20009 \\\n+        --port 20002 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -242,7 +228,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.9 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20009\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"22001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20002\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Prefill3 (e.g. 10.0.1.4 or 10.0.1.1)\n@@ -264,7 +250,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.9 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_producer\",\"kv_buffer_size\":\"1e1\",\"kv_port\":\"23001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20003\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n ### Decode1 (e.g. 10.0.1.5 or 10.0.1.1)\n@@ -274,7 +260,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n     ```shell\n     VLLM_USE_V1=1 CUDA_VISIBLE_DEVICES=3 vllm serve {your model directory} \\\n         --host 0.0.0.0 \\\n-        --port 20008 \\\n+        --port 20004 \\\n         --tensor-parallel-size 1 \\\n         --seed 1024 \\\n         --served-model-name base_model \\\n@@ -286,7 +272,7 @@ python3 disagg_prefill_proxy_xpyd.py &\n         --gpu-memory-utilization 0.7 \\\n         --disable-log-request \\\n         --kv-transfer-config \\\n-        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20008\",\"send_type\":\"PUT_ASYNC\",\"nccl_num_channels\":\"16\"}}' > /var/vllm.log 2>&1 &\n+        '{\"kv_connector\":\"P2pNcclConnector\",\"kv_role\":\"kv_consumer\",\"kv_buffer_size\":\"8e9\",\"kv_port\":\"24001\",\"kv_connector_extra_config\":{\"proxy_ip\":\"10.0.1.1\",\"proxy_port\":\"30001\",\"http_port\":\"20004\"}}' > /var/vllm.log 2>&1 &\n     ```\n \n # Single request\n@@ -334,24 +320,6 @@ pgrep python | xargs kill -9 && pkill -f python\n \n # Test data\n \n-## **Scenario 1**: 1K input & 1K output tokens, E2E P99 latency ~20s\n-- **1P5D (6√óA800) vs vLLM (1√óA800)**:\n-  - Throughput ‚Üë7.2% (1085 ‚Üí 6979/6)\n-  - ITL (P99) ‚Üì81.3% (120ms ‚Üí 22.9ms)\n-  - TTFT (P99) ‚Üë26.8% (175ms ‚Üí 222ms)\n-  - TPOT: No change\n-\n-- **1P6D (7√óA800) vs vLLM (1√óA800)**:\n-  - Throughput ‚Üë9.6% (1085 ‚Üí 8329/7)\n-  - ITL (P99) ‚Üì81.0% (120ms ‚Üí 22.7ms)\n-  - TTFT (P99) ‚Üë210% (175ms ‚Üí543ms)\n-  - TPOT: No change\n-\n-## **Scenario 2**: 1K input & 200 output tokens, E2E P99 latency ~4s\n-- **1P1D (2√óA800) vs vLLM (1√óA800)**:\n-  - Throughput ‚Üë37.4% (537 ‚Üí 1476/2)\n-  - ITL (P99) ‚Üì81.8% (127ms ‚Üí 23.1ms)\n-  - TTFT (P99) ‚Üë41.8% (160ms ‚Üí 227ms)\n-  - TPOT: No change\n-\n-![testdata](https://github.com/user-attachments/assets/f791bfc7-9f3d-4e5c-9171-a42f9f4da627)\n+## **Scenario**: 1K input & 200 output tokens, E2E P99 latency ~2s\n+\n+![testdata](https://github.com/user-attachments/assets/cef0953b-4567-4bf9-b940-405b92a28eb1)\ndiff --git a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\nindex 4e82424d6..ec58a1830 100644\n--- a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n+++ b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n@@ -4,7 +4,9 @@\n import os\n import socket\n import threading\n+import time\n import uuid\n+from typing import Any\n \n import aiohttp\n import msgpack\n@@ -12,12 +14,25 @@ import zmq\n from quart import Quart, make_response, request\n \n count = 0\n-prefill_instances: dict[str, str] = {}  # http_address: zmq_address\n-decode_instances: dict[str, str] = {}  # http_address: zmq_address\n+prefill_instances: dict[str, Any] = {}  # http_address: (zmq_address, stamp)\n+decode_instances: dict[str, Any] = {}  # http_address: (zmq_address, stamp)\n \n prefill_cv = threading.Condition()\n decode_cv = threading.Condition()\n \n+DEFAULT_PING_SECONDS = 5\n+\n+\n+def _remove_oldest_instances(instances: dict[str, Any]) -> None:\n+    oldest_key = next(iter(instances), None)\n+    while oldest_key is not None:\n+        value = instances[oldest_key]\n+        if value[1] > time.time():\n+            break\n+        print(f\"üî¥Remove [HTTP:{oldest_key}, ZMQ:{value[0]}, stamp:{value[1]}]\")\n+        instances.pop(oldest_key, None)\n+        oldest_key = next(iter(instances), None)\n+\n \n def _listen_for_register(poller, router_socket):\n     while True:\n@@ -31,12 +46,23 @@ def _listen_for_register(poller, router_socket):\n                 global prefill_instances\n                 global prefill_cv\n                 with prefill_cv:\n-                    prefill_instances[data[\"http_address\"]] = data[\"zmq_address\"]\n+                    node = prefill_instances.pop(data[\"http_address\"], None)\n+                    prefill_instances[data[\"http_address\"]] = (\n+                        data[\"zmq_address\"],\n+                        time.time() + DEFAULT_PING_SECONDS,\n+                    )\n+                    _remove_oldest_instances(prefill_instances)\n+\n             elif data[\"type\"] == \"D\":\n                 global decode_instances\n                 global decode_cv\n                 with decode_cv:\n-                    decode_instances[data[\"http_address\"]] = data[\"zmq_address\"]\n+                    node = decode_instances.pop(data[\"http_address\"], None)\n+                    decode_instances[data[\"http_address\"]] = (\n+                        data[\"zmq_address\"],\n+                        time.time() + DEFAULT_PING_SECONDS,\n+                    )\n+                    _remove_oldest_instances(decode_instances)\n             else:\n                 print(\n                     \"Unexpected, Received message from %s, data: %s\",\n@@ -44,6 +70,9 @@ def _listen_for_register(poller, router_socket):\n                     data,\n                 )\n \n+            if node is None:\n+                print(f\"üîµAdd [HTTP:{data['http_address']}, ZMQ:{data['zmq_address']}]\")\n+\n \n def start_service_discovery(hostname, port):\n     if not hostname:\n@@ -105,12 +134,14 @@ async def handle_request():\n         with prefill_cv:\n             prefill_list = list(prefill_instances.items())\n             prefill_addr, prefill_zmq_addr = prefill_list[count % len(prefill_list)]\n+            prefill_zmq_addr = prefill_zmq_addr[0]\n \n         global decode_instances\n         global decode_cv\n         with decode_cv:\n             decode_list = list(decode_instances.items())\n             decode_addr, decode_zmq_addr = decode_list[count % len(decode_list)]\n+            decode_zmq_addr = decode_zmq_addr[0]\n \n         print(\n             f\"handle_request count: {count}, [HTTP:{prefill_addr}, \"\ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\nindex 52f589a6d..d47a75461 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n@@ -13,7 +13,6 @@ from vllm.distributed.kv_transfer.kv_connector.v1.base import (\n from vllm.distributed.kv_transfer.kv_connector.v1.p2p.p2p_nccl_engine import (\n     P2pNcclEngine)\n from vllm.distributed.parallel_state import get_world_group\n-from vllm.forward_context import get_forward_context\n from vllm.logger import init_logger\n from vllm.v1.attention.backends.mla.common import MLACommonMetadata\n from vllm.v1.core.sched.output import SchedulerOutput\n@@ -238,32 +237,16 @@ class P2pNcclConnector(KVConnectorBase_V1):\n \n         assert self.p2p_nccl_engine is not None\n \n-        def extract_kv_from_layer(\n-            layer: torch.Tensor,\n-            slot_mapping: torch.Tensor,\n-        ) -> torch.Tensor:\n-            \"\"\"Extract the KV cache from the layer.\n-\n-            Assume the shape of the layer is (2, num_pages, page_size, xxx)\n-            if MLA is not used, and (num_pages, page_size, xxx) otherwise.\n-            \"\"\"\n-            if isinstance(attn_metadata, MLACommonMetadata):\n-                num_pages, page_size = layer.shape[0], layer.shape[1]\n-                return layer.reshape(num_pages * page_size, -1)[slot_mapping,\n-                                                                ...]\n-            num_pages, page_size = layer.shape[1], layer.shape[2]\n-            return layer.reshape(2, num_pages * page_size, -1)[:, slot_mapping,\n-                                                               ...]\n-\n         connector_metadata = self._get_connector_metadata()\n         assert isinstance(connector_metadata, P2pNcclConnectorMetadata)\n         for request in connector_metadata.requests:\n             request_id = request.request_id\n             ip, port = self.parse_request_id(request_id, True)\n             remote_address = ip + \":\" + str(port + self._rank)\n-            kv_cache = extract_kv_from_layer(kv_layer, request.slot_mapping)\n-            self.p2p_nccl_engine.send_tensor(request_id + \"#\" + layer_name,\n-                                             kv_cache, remote_address)\n+            self.p2p_nccl_engine.send_tensor(\n+                request_id + \"#\" + layer_name, kv_layer, remote_address,\n+                request.slot_mapping,\n+                isinstance(attn_metadata, MLACommonMetadata))\n \n     def wait_for_save(self):\n         if self.is_producer:\n@@ -286,9 +269,10 @@ class P2pNcclConnector(KVConnectorBase_V1):\n \n         assert self.p2p_nccl_engine is not None\n \n-        forward_context: ForwardContext = get_forward_context()\n+        no_compile_layers = (\n+            self._vllm_config.compilation_config.static_forward_context)\n         return self.p2p_nccl_engine.get_finished(finished_req_ids,\n-                                                 forward_context)\n+                                                 no_compile_layers)\n \n     # ==============================\n     # Scheduler-side methods\n@@ -418,14 +402,6 @@ class P2pNcclConnector(KVConnectorBase_V1):\n                                  block_ids=block_ids,\n                                  block_size=self._block_size)\n \n-        # Requests loaded asynchronously are not in the scheduler_output.\n-        # for request_id in self._requests_need_load:\n-        #     request, block_ids = self._requests_need_load[request_id]\n-        #     meta.add_request(request_id=request.request_id,\n-        #                      token_ids=request.prompt_token_ids,\n-        #                      block_ids=block_ids,\n-        #                      block_size=self._block_size)\n-\n         self._requests_need_load.clear()\n         return meta\n \ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\nindex 6c9ccb2e3..b94f2296d 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n@@ -8,7 +8,8 @@ import time\n import typing\n from collections import deque\n from contextlib import contextmanager\n-from typing import TYPE_CHECKING, Any, Optional\n+from dataclasses import dataclass\n+from typing import Any, Optional\n \n import msgpack\n import torch\n@@ -21,9 +22,6 @@ from vllm.distributed.kv_transfer.kv_connector.v1.p2p.tensor_memory_pool import\n     TensorMemoryPool)\n from vllm.utils import current_stream, get_ip\n \n-if TYPE_CHECKING:\n-    from vllm.forward_context import ForwardContext\n-\n logger = logging.getLogger(__name__)\n \n DEFAULT_MEM_POOL_SIZE_GB = 32\n@@ -59,6 +57,15 @@ def set_p2p_nccl_context(num_channels: str):\n                 os.environ.pop(var, None)\n \n \n+@dataclass\n+class SendQueueItem:\n+    tensor_id: str\n+    remote_address: str\n+    tensor: torch.Tensor\n+    slot_mapping: torch.Tensor\n+    is_mla: bool\n+\n+\n class P2pNcclEngine:\n \n     def __init__(self,\n@@ -112,24 +119,26 @@ class P2pNcclEngine:\n         self.send_stream = torch.cuda.Stream()\n         self.recv_stream = torch.cuda.Stream()\n \n-        mem_pool_size_gb = self.config.get_from_extra_config(\n-            \"mem_pool_size_gb\", DEFAULT_MEM_POOL_SIZE_GB)\n-        self.pool = TensorMemoryPool(max_block_size=int(mem_pool_size_gb) *\n-                                     1024**3)  # GB\n+        mem_pool_size_gb = float(\n+            self.config.get_from_extra_config(\"mem_pool_size_gb\",\n+                                              DEFAULT_MEM_POOL_SIZE_GB))\n+        self.pool = TensorMemoryPool(max_block_size=int(mem_pool_size_gb *\n+                                                        1024**3))  # GB\n \n         # The sending type includes tree mutually exclusive options:\n         # PUT, GET, PUT_ASYNC.\n-        self.send_type = self.config.get_from_extra_config(\"send_type\", \"PUT\")\n+        self.send_type = self.config.get_from_extra_config(\n+            \"send_type\", \"PUT_ASYNC\")\n         if self.send_type == \"GET\":\n             # tensor_id: torch.Tensor\n             self.send_store: dict[str, torch.Tensor] = {}\n         else:\n             # PUT or PUT_ASYNC\n             # tensor_id: torch.Tensor\n-            self.send_queue: deque[list[Any]] = deque()\n+            self.send_queue: deque[SendQueueItem] = deque()\n             self.send_request_id_to_tensor_ids: dict[str, set[str]] = {}\n             if self.send_type == \"PUT_ASYNC\":\n-                self._send_thread = threading.Thread(target=self._send_async,\n+                self._send_thread = threading.Thread(target=self.send_async,\n                                                      daemon=True)\n                 self._send_thread.start()\n \n@@ -146,13 +155,12 @@ class P2pNcclEngine:\n             \"nccl_num_channels\", \"8\")\n \n         self._listener_thread = threading.Thread(\n-            target=self._listen_for_requests, daemon=True)\n+            target=self.listen_for_requests, daemon=True)\n         self._listener_thread.start()\n \n         self._ping_thread = None\n         if port_offset == 0 and self.proxy_address != \"\":\n-            self._ping_thread = threading.Thread(target=self._ping,\n-                                                 daemon=True)\n+            self._ping_thread = threading.Thread(target=self.ping, daemon=True)\n             self._ping_thread.start()\n \n         logger.info(\n@@ -162,7 +170,7 @@ class P2pNcclEngine:\n             self.http_address, self.zmq_address, self.proxy_address,\n             self.send_type, self.buffer_size_threshold, self.nccl_num_channels)\n \n-    def _create_connect(self, remote_address: typing.Optional[str] = None):\n+    def create_connect(self, remote_address: typing.Optional[str] = None):\n         assert remote_address is not None\n         if remote_address not in self.socks:\n             sock = self.context.socket(zmq.DEALER)\n@@ -184,7 +192,7 @@ class P2pNcclEngine:\n                     comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                         2, unique_id, rank)\n                 self.comms[remote_address] = (comm, rank)\n-                logger.info(\"ü§ùncclCommInitRank Success, %süëâ%s, MyRank: %s\",\n+                logger.info(\"ü§ùncclCommInitRank Success, %süëâ%s, MyRank:%s\",\n                             self.zmq_address, remote_address, rank)\n \n         return self.socks[remote_address], self.comms[remote_address]\n@@ -194,44 +202,54 @@ class P2pNcclEngine:\n         tensor_id: str,\n         tensor: torch.Tensor,\n         remote_address: typing.Optional[str] = None,\n+        slot_mapping: torch.Tensor = None,\n+        is_mla: bool = False,\n     ) -> bool:\n         if remote_address is None:\n             with self.recv_store_cv:\n                 self.recv_store[tensor_id] = tensor\n                 self.recv_store_cv.notify()\n             return True\n-        else:\n-            if self.send_type == \"PUT\":\n-                return self._send_sync(tensor_id, tensor, remote_address)\n-            elif self.send_type == \"PUT_ASYNC\":\n-                with self.send_queue_cv:\n-                    self.send_queue.append([tensor_id, remote_address, tensor])\n-                    self.send_queue_cv.notify()\n-            else:  # GET\n-                with self.send_store_cv:\n-                    tensor_size = tensor.element_size() * tensor.numel()\n-                    while (self.buffer_size + tensor_size\n-                           > self.buffer_size_threshold):\n-                        oldest_tenser_id = next(iter(self.send_store))\n-                        oldest_tenser = self.send_store.pop(oldest_tenser_id)\n-                        oldest_tenser_size = oldest_tenser.element_size(\n-                        ) * oldest_tenser.numel()\n-                        self.buffer_size -= oldest_tenser_size\n-                        logger.info(\n-                            \"‚õî[GET]Send to %s, tensor_id:%s, tensor_size:%d,\"\n-                            \" buffer_size:%d, oldest_tenser_size:%d, rank:%d\",\n-                            remote_address, tensor_id, tensor_size,\n-                            self.buffer_size, oldest_tenser_size, self.rank)\n-\n-                    self.send_store[tensor_id] = tensor\n-                    self.buffer_size += tensor_size\n-                    logger.debug(\n-                        \"üîµ[GET]Send to %s, tensor_id:%s, tensor_size:%d, \"\n-                        \"shape:%s, rank:%d, buffer_size:%d(%.2f%%)\",\n-                        remote_address, tensor_id, tensor_size, tensor.shape,\n-                        self.rank, self.buffer_size,\n-                        self.buffer_size / self.buffer_size_threshold * 100)\n \n+        item = SendQueueItem(tensor_id=tensor_id,\n+                             remote_address=remote_address,\n+                             tensor=tensor,\n+                             slot_mapping=slot_mapping,\n+                             is_mla=is_mla)\n+\n+        if self.send_type == \"PUT\":\n+            return self.send_sync(item)\n+\n+        if self.send_type == \"PUT_ASYNC\":\n+            with self.send_queue_cv:\n+                self.send_queue.append(item)\n+                self.send_queue_cv.notify()\n+            return True\n+\n+        # GET\n+        with self.send_store_cv:\n+            tensor_size = tensor.element_size() * tensor.numel()\n+            while (self.buffer_size + tensor_size\n+                   > self.buffer_size_threshold):\n+                oldest_tenser_id = next(iter(self.send_store))\n+                oldest_tenser = self.send_store.pop(oldest_tenser_id)\n+                oldest_tenser_size = oldest_tenser.element_size(\n+                ) * oldest_tenser.numel()\n+                self.buffer_size -= oldest_tenser_size\n+                logger.info(\n+                    \"‚õî[GET]Send to %s, tensor_id:%s, tensor_size:%d,\"\n+                    \" buffer_size:%d, oldest_tenser_size:%d, rank:%d\",\n+                    remote_address, tensor_id, tensor_size, self.buffer_size,\n+                    oldest_tenser_size, self.rank)\n+\n+            self.send_store[tensor_id] = tensor\n+            self.buffer_size += tensor_size\n+            logger.debug(\n+                \"üîµ[GET]Send to %s, tensor_id:%s, tensor_size:%d, \"\n+                \"shape:%s, rank:%d, buffer_size:%d(%.2f%%)\", remote_address,\n+                tensor_id, tensor_size, tensor.shape, self.rank,\n+                self.buffer_size,\n+                self.buffer_size / self.buffer_size_threshold * 100)\n         return True\n \n     def recv_tensor(\n@@ -267,7 +285,7 @@ class P2pNcclEngine:\n             return None\n \n         if remote_address not in self.socks:\n-            self._create_connect(remote_address)\n+            self.create_connect(remote_address)\n \n         sock = self.socks[remote_address]\n         comm, rank = self.comms[remote_address]\n@@ -282,121 +300,121 @@ class P2pNcclEngine:\n                            remote_address, tensor_id, data[\"ret\"])\n             return None\n \n-        tensor = torch.empty(data[\"shape\"],\n-                             dtype=getattr(torch, data[\"dtype\"]),\n-                             device=self.device)\n+        with torch.cuda.stream(self.recv_stream):\n+            tensor = torch.empty(data[\"shape\"],\n+                                 dtype=getattr(torch, data[\"dtype\"]),\n+                                 device=self.device)\n \n-        self._recv(comm, tensor, rank ^ 1, self.recv_stream)\n+        self.recv(comm, tensor, rank ^ 1, self.recv_stream)\n \n         return tensor\n \n-    def _listen_for_requests(self):\n+    def listen_for_requests(self):\n         while True:\n             socks = dict(self.poller.poll())\n-            if self.router_socket in socks:\n-                remote_address, message = self.router_socket.recv_multipart()\n-                data = msgpack.loads(message)\n-                if data[\"cmd\"] == \"NEW\":\n-                    unique_id = self.nccl.unique_id_from_bytes(\n-                        bytes(data[\"unique_id\"]))\n-                    with torch.cuda.device(self.device):\n-                        rank = 1\n-                        with set_p2p_nccl_context(self.nccl_num_channels):\n-                            comm: ncclComm_t = self.nccl.ncclCommInitRank(\n-                                2, unique_id, rank)\n-                        self.comms[remote_address.decode()] = (comm, rank)\n-                        logger.info(\n-                            \"ü§ùncclCommInitRank Success, %süëà%s, MyRank:%s\",\n-                            self.zmq_address, remote_address.decode(), rank)\n-                elif data[\"cmd\"] == \"PUT\":\n-                    tensor_id = data[\"tensor_id\"]\n-                    try:\n-                        with torch.cuda.stream(self.recv_stream):\n-                            tensor = torch.empty(data[\"shape\"],\n-                                                 dtype=getattr(\n-                                                     torch, data[\"dtype\"]),\n-                                                 device=self.device)\n-                        self.router_socket.send_multipart(\n-                            [remote_address, b\"0\"])\n-                        comm, rank = self.comms[remote_address.decode()]\n-                        self._recv(comm, tensor, rank ^ 1, self.recv_stream)\n-                        tensor_size = tensor.element_size() * tensor.numel()\n-                        if (self.buffer_size + tensor_size\n-                                > self.buffer_size_threshold):\n-                            # Store Tensor in memory pool\n-                            addr = self.pool.store_tensor(tensor)\n-                            tensor = (addr, tensor.dtype, tensor.shape)\n-                            logger.warning(\n-                                \"üî¥[PUT]Recv Tensor, Out Of Threshold, \"\n-                                \"%süëà%s, data:%s, addr:%d\", self.zmq_address,\n-                                remote_address.decode(), data, addr)\n-                        else:\n-                            self.buffer_size += tensor_size\n-\n-                    except torch.cuda.OutOfMemoryError:\n-                        self.router_socket.send_multipart(\n-                            [remote_address, b\"1\"])\n-                        tensor = None\n+            if self.router_socket not in socks:\n+                continue\n+\n+            remote_address, message = self.router_socket.recv_multipart()\n+            data = msgpack.loads(message)\n+            if data[\"cmd\"] == \"NEW\":\n+                unique_id = self.nccl.unique_id_from_bytes(\n+                    bytes(data[\"unique_id\"]))\n+                with torch.cuda.device(self.device):\n+                    rank = 1\n+                    with set_p2p_nccl_context(self.nccl_num_channels):\n+                        comm: ncclComm_t = self.nccl.ncclCommInitRank(\n+                            2, unique_id, rank)\n+                    self.comms[remote_address.decode()] = (comm, rank)\n+                    logger.info(\"ü§ùncclCommInitRank Success, %süëà%s, MyRank:%s\",\n+                                self.zmq_address, remote_address.decode(),\n+                                rank)\n+            elif data[\"cmd\"] == \"PUT\":\n+                tensor_id = data[\"tensor_id\"]\n+                try:\n+                    with torch.cuda.stream(self.recv_stream):\n+                        tensor = torch.empty(data[\"shape\"],\n+                                             dtype=getattr(\n+                                                 torch, data[\"dtype\"]),\n+                                             device=self.device)\n+                    self.router_socket.send_multipart([remote_address, b\"0\"])\n+                    comm, rank = self.comms[remote_address.decode()]\n+                    self.recv(comm, tensor, rank ^ 1, self.recv_stream)\n+                    tensor_size = tensor.element_size() * tensor.numel()\n+                    if (self.buffer_size + tensor_size\n+                            > self.buffer_size_threshold):\n+                        # Store Tensor in memory pool\n+                        addr = self.pool.store_tensor(tensor)\n+                        tensor = (addr, tensor.dtype, tensor.shape)\n                         logger.warning(\n-                            \"üî¥[PUT]Recv Tensor, Out Of Memory, %süëà%s, \"\n-                            \"data:%s\", self.zmq_address,\n-                            remote_address.decode(), data)\n-\n-                    with self.recv_store_cv:\n-                        self.recv_store[tensor_id] = tensor\n-                        self._have_received_tensor_id(tensor_id)\n-                        self.recv_store_cv.notify()\n-\n-                elif data[\"cmd\"] == \"GET\":\n-                    tensor_id = data[\"tensor_id\"]\n-                    with self.send_store_cv:\n-                        tensor = self.send_store.pop(tensor_id, None)\n-                        if tensor is not None:\n-                            data = {\n-                                \"ret\": 0,\n-                                \"shape\": tensor.shape,\n-                                \"dtype\":\n-                                str(tensor.dtype).replace(\"torch.\", \"\")\n-                            }\n-                            # LRU\n-                            self.send_store[tensor_id] = tensor\n-                            self._have_sent_tensor_id(tensor_id)\n-                        else:\n-                            data = {\"ret\": 1}\n-\n-                    self.router_socket.send_multipart(\n-                        [remote_address, msgpack.dumps(data)])\n-\n-                    if data[\"ret\"] == 0:\n-                        comm, rank = self.comms[remote_address.decode()]\n-                        self._send(comm, tensor.to(self.device), rank ^ 1,\n-                                   self.send_stream)\n-                else:\n+                            \"üî¥[PUT]Recv Tensor, Out Of Threshold, \"\n+                            \"%süëà%s, data:%s, addr:%d\", self.zmq_address,\n+                            remote_address.decode(), data, addr)\n+                    else:\n+                        self.buffer_size += tensor_size\n+\n+                except torch.cuda.OutOfMemoryError:\n+                    self.router_socket.send_multipart([remote_address, b\"1\"])\n+                    tensor = None\n                     logger.warning(\n-                        \"üößUnexpected, Received message from %s, data:%s\",\n-                        remote_address, data)\n+                        \"üî¥[PUT]Recv Tensor, Out Of Memory, %süëà%s, \"\n+                        \"data:%s\", self.zmq_address, remote_address.decode(),\n+                        data)\n \n-    def _have_sent_tensor_id(self, tensor_id: str):\n+                with self.recv_store_cv:\n+                    self.recv_store[tensor_id] = tensor\n+                    self.have_received_tensor_id(tensor_id)\n+                    self.recv_store_cv.notify()\n+\n+            elif data[\"cmd\"] == \"GET\":\n+                tensor_id = data[\"tensor_id\"]\n+                with self.send_store_cv:\n+                    tensor = self.send_store.pop(tensor_id, None)\n+                    if tensor is not None:\n+                        data = {\n+                            \"ret\": 0,\n+                            \"shape\": tensor.shape,\n+                            \"dtype\": str(tensor.dtype).replace(\"torch.\", \"\")\n+                        }\n+                        # LRU\n+                        self.send_store[tensor_id] = tensor\n+                        self.have_sent_tensor_id(tensor_id)\n+                    else:\n+                        data = {\"ret\": 1}\n+\n+                self.router_socket.send_multipart(\n+                    [remote_address, msgpack.dumps(data)])\n+\n+                if data[\"ret\"] == 0:\n+                    comm, rank = self.comms[remote_address.decode()]\n+                    self.send(comm, tensor.to(self.device), rank ^ 1,\n+                              self.send_stream)\n+            else:\n+                logger.warning(\n+                    \"üößUnexpected, Received message from %s, data:%s\",\n+                    remote_address, data)\n+\n+    def have_sent_tensor_id(self, tensor_id: str):\n         request_id = tensor_id.split('#')[0]\n         if request_id not in self.send_request_id_to_tensor_ids:\n             self.send_request_id_to_tensor_ids[request_id] = set()\n         self.send_request_id_to_tensor_ids[request_id].add(tensor_id)\n \n-    def _have_received_tensor_id(self, tensor_id: str):\n+    def have_received_tensor_id(self, tensor_id: str):\n         request_id = tensor_id.split('#')[0]\n         if request_id not in self.recv_request_id_to_tensor_ids:\n             self.recv_request_id_to_tensor_ids[request_id] = set()\n         self.recv_request_id_to_tensor_ids[request_id].add(tensor_id)\n \n-    def _send_async(self):\n+    def send_async(self):\n         while True:\n             with self.send_queue_cv:\n                 while not self.send_queue:\n                     self.send_queue_cv.wait()\n-                tensor_id, remote_address, tensor = self.send_queue.popleft()\n+                item = self.send_queue.popleft()\n                 if not self.send_queue:\n                     self.send_queue_cv.notify()\n-            self._send_sync(tensor_id, tensor, remote_address)\n+            self.send_sync(item)\n \n     def wait_for_sent(self):\n         if self.send_type == \"PUT_ASYNC\":\n@@ -409,22 +427,21 @@ class P2pNcclEngine:\n                 \"üöß[PUT_ASYNC]It took %.3fms to wait for the send_queue\"\n                 \" to be empty, rank:%d\", duration * 1000, self.rank)\n \n-    def _send_sync(\n-        self,\n-        tensor_id: str,\n-        tensor: torch.Tensor,\n-        remote_address: typing.Optional[str] = None,\n-    ) -> bool:\n-        if remote_address is None:\n+    def send_sync(self, item: SendQueueItem) -> bool:\n+        if item.remote_address is None:\n             return False\n-        if remote_address not in self.socks:\n-            self._create_connect(remote_address)\n+        if item.remote_address not in self.socks:\n+            self.create_connect(item.remote_address)\n \n-        sock = self.socks[remote_address]\n-        comm, rank = self.comms[remote_address]\n+        with self.send_stream:\n+            tensor = self.extract_kv_from_layer(item.is_mla, item.tensor,\n+                                                item.slot_mapping)\n+\n+        sock = self.socks[item.remote_address]\n+        comm, rank = self.comms[item.remote_address]\n         data = {\n             \"cmd\": \"PUT\",\n-            \"tensor_id\": tensor_id,\n+            \"tensor_id\": item.tensor_id,\n             \"shape\": tensor.shape,\n             \"dtype\": str(tensor.dtype).replace(\"torch.\", \"\")\n         }\n@@ -435,20 +452,21 @@ class P2pNcclEngine:\n             logger.error(\n                 \"üî¥Send Tensor, Peer Out Of Memory/Threshold, %s üëâ %s, \"\n                 \"MyRank:%s, data:%s, tensor:%s, size:%fGB, response:%s\",\n-                self.zmq_address, remote_address, rank, data, tensor.shape,\n+                self.zmq_address, item.remote_address, rank, data,\n+                tensor.shape,\n                 tensor.element_size() * tensor.numel() / 1024**3,\n                 response.decode())\n             return False\n \n-        self._send(comm, tensor.to(self.device), rank ^ 1, self.send_stream)\n+        self.send(comm, tensor.to(self.device), rank ^ 1, self.send_stream)\n \n         if self.send_type == \"PUT_ASYNC\":\n-            self._have_sent_tensor_id(tensor_id)\n+            self.have_sent_tensor_id(item.tensor_id)\n \n         return True\n \n     def get_finished(\n-        self, finished_req_ids: set[str], forward_context: \"ForwardContext\"\n+            self, finished_req_ids: set[str], no_compile_layers\n     ) -> tuple[Optional[set[str]], Optional[set[str]]]:\n         \"\"\"\n         Notifies worker-side connector ids of requests that have\n@@ -463,7 +481,7 @@ class P2pNcclEngine:\n \n         # Clear the buffer upon request completion.\n         for request_id in finished_req_ids:\n-            for layer_name in forward_context.no_compile_layers:\n+            for layer_name in no_compile_layers:\n                 tensor_id = request_id + \"#\" + layer_name\n                 if tensor_id in self.recv_store:\n                     with self.recv_store_cv:\n@@ -472,7 +490,6 @@ class P2pNcclEngine:\n                             request_id, None)\n                         self.recv_request_id_to_tensor_ids.pop(\n                             request_id, None)\n-                    addr = 0\n                     if isinstance(tensor, tuple):\n                         addr, _, _ = tensor\n                         self.pool.free(addr)\n@@ -485,7 +502,7 @@ class P2pNcclEngine:\n \n         return finished_sending or None, finished_recving or None\n \n-    def _ping(self):\n+    def ping(self):\n         sock = self.context.socket(zmq.DEALER)\n         sock.setsockopt_string(zmq.IDENTITY, self.zmq_address)\n         logger.debug(\"ping start, zmq_address:%s\", self.zmq_address)\n@@ -499,7 +516,7 @@ class P2pNcclEngine:\n             sock.send(msgpack.dumps(data))\n             time.sleep(3)\n \n-    def _send(self, comm, tensor: torch.Tensor, dst: int, stream=None):\n+    def send(self, comm, tensor: torch.Tensor, dst: int, stream=None):\n         assert tensor.device == self.device, (\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n@@ -512,7 +529,7 @@ class P2pNcclEngine:\n                                comm, cudaStream_t(stream.cuda_stream))\n         stream.synchronize()\n \n-    def _recv(self, comm, tensor: torch.Tensor, src: int, stream=None):\n+    def recv(self, comm, tensor: torch.Tensor, src: int, stream=None):\n         assert tensor.device == self.device, (\n             f\"this nccl communicator is created to work on {self.device}, \"\n             f\"but the input tensor is on {tensor.device}\")\n@@ -531,3 +548,21 @@ class P2pNcclEngine:\n             self._send_thread.join()\n         if self._ping_thread is not None:\n             self._ping_thread.join()\n+\n+    @staticmethod\n+    def extract_kv_from_layer(\n+        is_mla: bool,\n+        layer: torch.Tensor,\n+        slot_mapping: torch.Tensor,\n+    ) -> torch.Tensor:\n+        \"\"\"Extract the KV cache from the layer.\n+        Assume the shape of the layer is (2, num_pages, page_size, xxx)\n+        if MLA is not used, and (num_pages, page_size, xxx) otherwise.\n+        \"\"\"\n+        if is_mla:\n+            num_pages, page_size = layer.shape[0], layer.shape[1]\n+            return layer.reshape(num_pages * page_size, -1)[slot_mapping, ...]\n+\n+        num_pages, page_size = layer.shape[1], layer.shape[2]\n+        return layer.reshape(2, num_pages * page_size, -1)[:, slot_mapping,\n+                                                           ...]",
  "apis": [
    "vllm.distributed.kv_transfer.kv_connector.v1.p2p.P2pNcclConnector.start_load_kv",
    "vllm.distributed.kv_transfer.kv_connector.v1.p2p.P2pNcclConnector.get_finished",
    "vllm.distributed.kv_transfer.kv_connector.v1.p2p.P2pNcclEngine.send_tensor",
    "vllm.distributed.kv_transfer.kv_connector.v1.p2p.P2pNcclEngine.send_sync",
    "vllm.distributed.kv_transfer.kv_connector.v1.p2p.P2pNcclEngine.extract_kv_from_layer"
  ],
  "affected_paths": [
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py",
    "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py"
  ],
  "repo_path": "/Users/fortuna/buckets/gso_bucket/analysis/repos/vllm",
  "llm_reason": "The commit makes modifications to both documentation and the core implementation of the P2pNcclConnector and its engine. It changes default settings (e.g., defaulting the send type to \"PUT_ASYNC\"), refactors internal asynchronous sending logic (introducing a typed SendQueueItem for clarity and safety), and adjusts memory pool handling, all of which are intended to improve the throughput and reduce latency in transfer of KV cache data. These changes affect the performance of the core, high-level API of the system and are not just trivial refactoring or documentation fixes. The improvements focus on enhancing data transfer performance on CPU and are testable without dependency on GPU/TPU specifics. Therefore, the commit satisfies the criteria for being performance or optimization related.",
  "llm_api_reason": "This commit mostly involves documentation updates (changing \"KVcache\" to \"KVCache\" for consistency), improvements to code readability, and some internal refactoring. In particular, it updates the P2pNcclConnector and P2pNcclEngine classes. The changes include modifying how the connector sends KVCache data (e.g. updating start_load_kv so that it calls engine.send_tensor with extra arguments), refactoring internal methods (renaming _send_sync to send_sync, _create_connect to create_connect, etc.), and moving the KV extraction logic into a new static method extract_kv_from_layer in P2pNcclEngine. These modifications affect the Python APIs for the P2pNcclConnector and P2pNcclEngine classes."
}