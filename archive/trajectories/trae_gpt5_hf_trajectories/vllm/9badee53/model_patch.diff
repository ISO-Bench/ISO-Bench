diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py
index 122e2ed..228523f 100644
--- a/vllm/entrypoints/llm.py
+++ b/vllm/entrypoints/llm.py
@@ -1373,8 +1373,11 @@ class LLM:
         outputs: list[Union[RequestOutput, PoolingRequestOutput]] = []
         total_in_toks = 0
         total_out_toks = 0
-        while self.llm_engine.has_unfinished_requests():
-            step_outputs = self.llm_engine.step()
+        llm_engine = self.llm_engine
+        has_unfinished = llm_engine.has_unfinished_requests
+        step = llm_engine.step
+        while has_unfinished():
+            step_outputs = step()
             for output in step_outputs:
                 if output.finished:
                     outputs.append(output)
@@ -1383,15 +1386,16 @@ class LLM:
                             # Calculate tokens only for RequestOutput
                             assert output.prompt_token_ids is not None
                             total_in_toks += len(output.prompt_token_ids)
-                            in_spd = total_in_toks / pbar.format_dict["elapsed"]
-                            total_out_toks += sum(
-                                len(stp.token_ids) for stp in output.outputs)
-                            out_spd = (total_out_toks /
-                                       pbar.format_dict["elapsed"])
-                            pbar.postfix = (
-                                f"est. speed input: {in_spd:.2f} toks/s, "
-                                f"output: {out_spd:.2f} toks/s")
-                        pbar.update(1)
+                            elapsed = pbar.format_dict.get("elapsed", 0.0)  # type: ignore[attr-defined]
+                            if elapsed > 0:
+                                in_spd = total_in_toks / elapsed
+                                total_out_toks += sum(
+                                    len(stp.token_ids) for stp in output.outputs)
+                                out_spd = (total_out_toks / elapsed)
+                                pbar.postfix = (  # type: ignore[attr-defined]
+                                    f"est. speed input: {in_spd:.2f} toks/s, "
+                                    f"output: {out_spd:.2f} toks/s")
+                        pbar.update(1)  # type: ignore[attr-defined]
 
         if use_tqdm:
             pbar.close()
diff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py
index 98e9ea0..9c72ffd 100644
--- a/vllm/entrypoints/openai/serving_chat.py
+++ b/vllm/entrypoints/openai/serving_chat.py
@@ -206,13 +206,15 @@ class OpenAIServingChat(OpenAIServing):
         # Schedule the request and get the result generator.
         generators: list[AsyncGenerator[RequestOutput, None]] = []
         try:
+            # Hoist default sampling params and trace headers outside the loop
+            default_sampling_params = (
+                self.model_config.get_diff_sampling_param())
+            trace_headers = (None if raw_request is None else await
+                             self._get_trace_headers(raw_request.headers))
             for i, engine_prompt in enumerate(engine_prompts):
                 sampling_params: Union[SamplingParams, BeamSearchParams]
                 default_max_tokens = self.max_model_len - len(
                     engine_prompt["prompt_token_ids"])
-                # Build default sampling params
-                default_sampling_params = (
-                    self.model_config.get_diff_sampling_param())
                 if request.use_beam_search:
                     sampling_params = request.to_beam_search_params(
                         default_max_tokens, default_sampling_params)
@@ -228,9 +230,6 @@ class OpenAIServingChat(OpenAIServing):
                                  lora_request=lora_request,
                                  prompt_adapter_request=prompt_adapter_request)
 
-                trace_headers = (None if raw_request is None else await
-                                 self._get_trace_headers(raw_request.headers))
-
                 if isinstance(sampling_params, BeamSearchParams):
                     generator = self.engine_client.beam_search(
                         prompt=engine_prompt,
diff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py
index ed09af8..c6c47a1 100644
--- a/vllm/entrypoints/openai/serving_completion.py
+++ b/vllm/entrypoints/openai/serving_completion.py
@@ -115,13 +115,15 @@ class OpenAIServingCompletion(OpenAIServing):
         # Schedule the request and get the result generator.
         generators: list[AsyncGenerator[RequestOutput, None]] = []
         try:
+            # Hoist default sampling params and trace headers outside the loop
+            default_sampling_params = (
+                self.model_config.get_diff_sampling_param())
+            trace_headers = (None if raw_request is None else await
+                             self._get_trace_headers(raw_request.headers))
             for i, engine_prompt in enumerate(engine_prompts):
                 sampling_params: Union[SamplingParams, BeamSearchParams]
                 default_max_tokens = self.max_model_len - len(
                     engine_prompt["prompt_token_ids"])
-                # Build default sampling params
-                default_sampling_params = (
-                    self.model_config.get_diff_sampling_param())
                 if request.use_beam_search:
                     sampling_params = request.to_beam_search_params(
                         default_max_tokens, default_sampling_params)
@@ -139,9 +141,6 @@ class OpenAIServingCompletion(OpenAIServing):
                                  lora_request=lora_request,
                                  prompt_adapter_request=prompt_adapter_request)
 
-                trace_headers = (None if raw_request is None else await
-                                 self._get_trace_headers(raw_request.headers))
-
                 if isinstance(sampling_params, BeamSearchParams):
                     generator = self.engine_client.beam_search(
                         prompt=engine_prompt,
diff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py
index 77f016a..51d92d4 100644
--- a/vllm/entrypoints/openai/serving_transcription.py
+++ b/vllm/entrypoints/openai/serving_transcription.py
@@ -173,24 +173,26 @@ class OpenAIServingTranscription(OpenAIServing):
         audio_data: bytes,
     ) -> PromptType:
         # Validate request
+        lang = request.language
         # TODO language should be optional and can be guessed.
         # For now we default to en. See
         # https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/generation_whisper.py#L1520
-        lang_token = f"<|{request.language}|>" if request.language else "<|en|>"
-        if request.language:
-            if request.language in ISO639_1_SUPPORTED_LANGS:
+        lang_token = f"<|{lang}|>" if lang else "<|en|>"
+        if lang:
+            if lang in ISO639_1_SUPPORTED_LANGS:
                 pass
-            elif request.language in ISO639_1_OTHER_LANGS:
+            elif lang in ISO639_1_OTHER_LANGS:
                 logger.warning(
                     "The selected language %s has limited accuracy with"
                     " reported WER>=0.5. Results may be less accurate "
-                    "for this choice.", request.language)
+                    "for this choice.", lang)
             else:
+                supported = list(ISO639_1_SUPPORTED_LANGS.values())
+                limited = list(ISO639_1_OTHER_LANGS.values())
                 raise ValueError(
-                    f"Unsupported language: {request.language}."
-                    "Language should be one of:" +
-                    f" {list(ISO639_1_SUPPORTED_LANGS.values())}" +
-                    f"or {list(ISO639_1_OTHER_LANGS.values())}")
+                    f"Unsupported language: {lang}."
+                    "Language should be one of:" + f" {supported}" +
+                    f"or {limited}")
 
         if len(audio_data) / 1024**2 > MAX_AUDIO_CLIP_FILESIZE_MB:
             raise ValueError("Maximum file size exceeded.")
