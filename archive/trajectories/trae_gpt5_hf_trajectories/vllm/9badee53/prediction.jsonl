{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-beebf47", "model_patch": "diff --git a/vllm/entrypoints/llm.py b/vllm/entrypoints/llm.py\nindex 122e2ed..228523f 100644\n--- a/vllm/entrypoints/llm.py\n+++ b/vllm/entrypoints/llm.py\n@@ -1373,8 +1373,11 @@ class LLM:\n         outputs: list[Union[RequestOutput, PoolingRequestOutput]] = []\n         total_in_toks = 0\n         total_out_toks = 0\n-        while self.llm_engine.has_unfinished_requests():\n-            step_outputs = self.llm_engine.step()\n+        llm_engine = self.llm_engine\n+        has_unfinished = llm_engine.has_unfinished_requests\n+        step = llm_engine.step\n+        while has_unfinished():\n+            step_outputs = step()\n             for output in step_outputs:\n                 if output.finished:\n                     outputs.append(output)\n@@ -1383,15 +1386,16 @@ class LLM:\n                             # Calculate tokens only for RequestOutput\n                             assert output.prompt_token_ids is not None\n                             total_in_toks += len(output.prompt_token_ids)\n-                            in_spd = total_in_toks / pbar.format_dict[\"elapsed\"]\n-                            total_out_toks += sum(\n-                                len(stp.token_ids) for stp in output.outputs)\n-                            out_spd = (total_out_toks /\n-                                       pbar.format_dict[\"elapsed\"])\n-                            pbar.postfix = (\n-                                f\"est. speed input: {in_spd:.2f} toks/s, \"\n-                                f\"output: {out_spd:.2f} toks/s\")\n-                        pbar.update(1)\n+                            elapsed = pbar.format_dict.get(\"elapsed\", 0.0)  # type: ignore[attr-defined]\n+                            if elapsed > 0:\n+                                in_spd = total_in_toks / elapsed\n+                                total_out_toks += sum(\n+                                    len(stp.token_ids) for stp in output.outputs)\n+                                out_spd = (total_out_toks / elapsed)\n+                                pbar.postfix = (  # type: ignore[attr-defined]\n+                                    f\"est. speed input: {in_spd:.2f} toks/s, \"\n+                                    f\"output: {out_spd:.2f} toks/s\")\n+                        pbar.update(1)  # type: ignore[attr-defined]\n \n         if use_tqdm:\n             pbar.close()\ndiff --git a/vllm/entrypoints/openai/serving_chat.py b/vllm/entrypoints/openai/serving_chat.py\nindex 98e9ea0..9c72ffd 100644\n--- a/vllm/entrypoints/openai/serving_chat.py\n+++ b/vllm/entrypoints/openai/serving_chat.py\n@@ -206,13 +206,15 @@ class OpenAIServingChat(OpenAIServing):\n         # Schedule the request and get the result generator.\n         generators: list[AsyncGenerator[RequestOutput, None]] = []\n         try:\n+            # Hoist default sampling params and trace headers outside the loop\n+            default_sampling_params = (\n+                self.model_config.get_diff_sampling_param())\n+            trace_headers = (None if raw_request is None else await\n+                             self._get_trace_headers(raw_request.headers))\n             for i, engine_prompt in enumerate(engine_prompts):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n-                # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\n@@ -228,9 +230,6 @@ class OpenAIServingChat(OpenAIServing):\n                                  lora_request=lora_request,\n                                  prompt_adapter_request=prompt_adapter_request)\n \n-                trace_headers = (None if raw_request is None else await\n-                                 self._get_trace_headers(raw_request.headers))\n-\n                 if isinstance(sampling_params, BeamSearchParams):\n                     generator = self.engine_client.beam_search(\n                         prompt=engine_prompt,\ndiff --git a/vllm/entrypoints/openai/serving_completion.py b/vllm/entrypoints/openai/serving_completion.py\nindex ed09af8..c6c47a1 100644\n--- a/vllm/entrypoints/openai/serving_completion.py\n+++ b/vllm/entrypoints/openai/serving_completion.py\n@@ -115,13 +115,15 @@ class OpenAIServingCompletion(OpenAIServing):\n         # Schedule the request and get the result generator.\n         generators: list[AsyncGenerator[RequestOutput, None]] = []\n         try:\n+            # Hoist default sampling params and trace headers outside the loop\n+            default_sampling_params = (\n+                self.model_config.get_diff_sampling_param())\n+            trace_headers = (None if raw_request is None else await\n+                             self._get_trace_headers(raw_request.headers))\n             for i, engine_prompt in enumerate(engine_prompts):\n                 sampling_params: Union[SamplingParams, BeamSearchParams]\n                 default_max_tokens = self.max_model_len - len(\n                     engine_prompt[\"prompt_token_ids\"])\n-                # Build default sampling params\n-                default_sampling_params = (\n-                    self.model_config.get_diff_sampling_param())\n                 if request.use_beam_search:\n                     sampling_params = request.to_beam_search_params(\n                         default_max_tokens, default_sampling_params)\n@@ -139,9 +141,6 @@ class OpenAIServingCompletion(OpenAIServing):\n                                  lora_request=lora_request,\n                                  prompt_adapter_request=prompt_adapter_request)\n \n-                trace_headers = (None if raw_request is None else await\n-                                 self._get_trace_headers(raw_request.headers))\n-\n                 if isinstance(sampling_params, BeamSearchParams):\n                     generator = self.engine_client.beam_search(\n                         prompt=engine_prompt,\ndiff --git a/vllm/entrypoints/openai/serving_transcription.py b/vllm/entrypoints/openai/serving_transcription.py\nindex 77f016a..51d92d4 100644\n--- a/vllm/entrypoints/openai/serving_transcription.py\n+++ b/vllm/entrypoints/openai/serving_transcription.py\n@@ -173,24 +173,26 @@ class OpenAIServingTranscription(OpenAIServing):\n         audio_data: bytes,\n     ) -> PromptType:\n         # Validate request\n+        lang = request.language\n         # TODO language should be optional and can be guessed.\n         # For now we default to en. See\n         # https://github.com/huggingface/transformers/blob/main/src/transformers/models/whisper/generation_whisper.py#L1520\n-        lang_token = f\"<|{request.language}|>\" if request.language else \"<|en|>\"\n-        if request.language:\n-            if request.language in ISO639_1_SUPPORTED_LANGS:\n+        lang_token = f\"<|{lang}|>\" if lang else \"<|en|>\"\n+        if lang:\n+            if lang in ISO639_1_SUPPORTED_LANGS:\n                 pass\n-            elif request.language in ISO639_1_OTHER_LANGS:\n+            elif lang in ISO639_1_OTHER_LANGS:\n                 logger.warning(\n                     \"The selected language %s has limited accuracy with\"\n                     \" reported WER>=0.5. Results may be less accurate \"\n-                    \"for this choice.\", request.language)\n+                    \"for this choice.\", lang)\n             else:\n+                supported = list(ISO639_1_SUPPORTED_LANGS.values())\n+                limited = list(ISO639_1_OTHER_LANGS.values())\n                 raise ValueError(\n-                    f\"Unsupported language: {request.language}.\"\n-                    \"Language should be one of:\" +\n-                    f\" {list(ISO639_1_SUPPORTED_LANGS.values())}\" +\n-                    f\"or {list(ISO639_1_OTHER_LANGS.values())}\")\n+                    f\"Unsupported language: {lang}.\"\n+                    \"Language should be one of:\" + f\" {supported}\" +\n+                    f\"or {limited}\")\n \n         if len(audio_data) / 1024**2 > MAX_AUDIO_CLIP_FILESIZE_MB:\n             raise ValueError(\"Maximum file size exceeded.\")\n", "model_name_or_path": "gpt-5-2025-08-07"}
