diff --git a/Dockerfile b/Dockerfile
index f41753a..408dfdd 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -3,6 +3,10 @@ FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS dev
 RUN apt-get update -y \
     && apt-get install -y python3-pip
 
+# Reduce Python overhead in containers
+ENV PYTHONDONTWRITEBYTECODE=1
+ENV PYTHONUNBUFFERED=1
+
 WORKDIR /workspace
 
 # install build and runtime dependencies
diff --git a/README.md b/README.md
index 84cadee..2f2dda2 100644
--- a/README.md
+++ b/README.md
@@ -84,6 +84,12 @@ Visit our [documentation](https://vllm.readthedocs.io/en/latest/) to get started
 - [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)
 - [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)
 
+
+## Performance notes
+
+- Internal configuration utilities now reduce per-call allocations by reusing precomputed constants.
+- Mixtral model path avoids redundant tensor allocations and repeated attribute lookups for improved throughput.
+
 ## Contributing
 
 We welcome and value any contributions and collaborations.
diff --git a/docs/source/models/supported_models.rst b/docs/source/models/supported_models.rst
index e21cdd6..86db9a1 100644
--- a/docs/source/models/supported_models.rst
+++ b/docs/source/models/supported_models.rst
@@ -74,6 +74,8 @@ Otherwise, please refer to :ref:`Adding a New Model <adding_a_new_model>` for in
 Alternatively, you can raise an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ project.
 
 .. note::
+   Mixtral support relies on the MegaBlocks library; ensure it is installed for optimal performance.
+
     Currently, the ROCm version of vLLM does not support Mixtral.
     Additionally, it only supports Mistral for context lengths up to 4096.
 
diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..439d40f
--- /dev/null
+++ b/model_patch.diff
@@ -0,0 +1,287 @@
+diff --git a/vllm/config.py b/vllm/config.py
+index 6bafa73..b368904 100644
+--- a/vllm/config.py
++++ b/vllm/config.py
+@@ -12,6 +12,41 @@ logger = init_logger(__name__)
+ 
+ _GB = 1 << 30
+ 
++# Performance-related module-level constants to avoid repeated allocations
++_SUPPORTED_LOAD_FORMATS = ("auto", "pt", "safetensors", "npcache", "dummy")
++_ROCM_NOT_SUPPORTED_LOAD_FORMAT = frozenset(("safetensors",))
++_POSSIBLE_MAX_LEN_KEYS = (
++    "max_position_embeddings",
++    "n_positions",
++    "max_seq_len",
++    "seq_length",
++    "max_sequence_length",
++    "max_seq_length",
++    "seq_len",
++)
++_KV_HEAD_ATTRS = (
++    "n_head_kv",
++    "num_kv_heads",
++    "num_key_value_heads",
++    "multi_query_group_num",
++)
++_FALCON_MODEL_TYPES = ("falcon", "RefinedWeb", "RefinedWebModel")
++_DEFAULT_MAX_LEN = 2048
++
++# Precompute ROCm supported dtype names once
++_STR_DTYPE_TO_TORCH_DTYPE = {
++    "half": torch.float16,
++    "float16": torch.float16,
++    "float": torch.float32,
++    "float32": torch.float32,
++    "bfloat16": torch.bfloat16,
++}
++
++_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]
++_ROCM_SUPPORTED_DTYPE_NAMES = tuple(
++    k for k in _STR_DTYPE_TO_TORCH_DTYPE.keys() if k not in _ROCM_NOT_SUPPORTED_DTYPE
++)
++
+ 
+ class ModelConfig:
+     """Configuration for the model.
+@@ -98,24 +133,18 @@ class ModelConfig:
+ 
+     def _verify_load_format(self) -> None:
+         load_format = self.load_format.lower()
+-        supported_load_format = [
+-            "auto", "pt", "safetensors", "npcache", "dummy"
+-        ]
+-        rocm_not_supported_load_format = ["safetensors"]
+-        if load_format not in supported_load_format:
++        if load_format not in _SUPPORTED_LOAD_FORMATS:
+             raise ValueError(
+                 f"Unknown load format: {self.load_format}. Must be one of "
+                 "'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.")
+         if is_hip():
+-            if load_format in ["safetensors"]:
+-                rocm_supported_load_format = [
+-                    f for f in supported_load_format
+-                    if (f not in rocm_not_supported_load_format)
+-                ]
++            if load_format in _ROCM_NOT_SUPPORTED_LOAD_FORMAT:
++                rocm_supported_load_format = tuple(
++                    f for f in _SUPPORTED_LOAD_FORMATS
++                    if f not in _ROCM_NOT_SUPPORTED_LOAD_FORMAT)
+                 raise ValueError(
+-                    f"load format \'{load_format}\' is not supported in ROCm. "
+-                    f"Supported load format are "
+-                    f"{rocm_supported_load_format}")
++                    f"load format '{load_format}' is not supported in ROCm. "
++                    f"Supported load format are {rocm_supported_load_format}")
+             # Force ROCm to load from pt weights if nothing specific is set
+             if load_format == "auto":
+                 load_format = "pt"
+@@ -211,9 +240,8 @@ class ModelConfig:
+         # NOTE: for falcon, when new_decoder_architecture is True, the
+         # multi_query flag is ignored and we use n_head_kv for the number of
+         # KV heads.
+-        falcon_model_types = ["falcon", "RefinedWeb", "RefinedWebModel"]
+         new_decoder_arch_falcon = (
+-            self.hf_config.model_type in falcon_model_types
++            self.hf_config.model_type in _FALCON_MODEL_TYPES
+             and getattr(self.hf_config, "new_decoder_architecture", False))
+         if not new_decoder_arch_falcon and getattr(self.hf_config,
+                                                    "multi_query", False):
+@@ -221,16 +249,7 @@ class ModelConfig:
+             # Currently, tensor parallelism is not supported in this case.
+             return 1
+ 
+-        attributes = [
+-            # For Falcon:
+-            "n_head_kv",
+-            "num_kv_heads",
+-            # For LLaMA-2:
+-            "num_key_value_heads",
+-            # For ChatGLM:
+-            "multi_query_group_num",
+-        ]
+-        for attr in attributes:
++        for attr in _KV_HEAD_ATTRS:
+             num_kv_heads = getattr(self.hf_config, attr, None)
+             if num_kv_heads is not None:
+                 return num_kv_heads
+@@ -360,12 +379,9 @@ class SchedulerConfig:
+         max_model_len: int,
+         max_paddings: int,
+     ) -> None:
+-        if max_num_batched_tokens is not None:
+-            self.max_num_batched_tokens = max_num_batched_tokens
+-        else:
+-            # If max_model_len is too short, use 2048 as the default value for
+-            # higher throughput.
+-            self.max_num_batched_tokens = max(max_model_len, 2048)
++        self.max_num_batched_tokens = (max_num_batched_tokens
++                                        if max_num_batched_tokens is not None
++                                        else max(max_model_len, 2048))
+         self.max_num_seqs = max_num_seqs
+         self.max_model_len = max_model_len
+         self.max_paddings = max_paddings
+@@ -387,17 +403,6 @@ class SchedulerConfig:
+                 f"({self.max_num_seqs}).")
+ 
+ 
+-_STR_DTYPE_TO_TORCH_DTYPE = {
+-    "half": torch.float16,
+-    "float16": torch.float16,
+-    "float": torch.float32,
+-    "float32": torch.float32,
+-    "bfloat16": torch.bfloat16,
+-}
+-
+-_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]
+-
+-
+ def _get_and_verify_dtype(
+     config: PretrainedConfig,
+     dtype: Union[str, torch.dtype],
+@@ -418,21 +423,17 @@ def _get_and_verify_dtype(
+             else:
+                 torch_dtype = config_dtype
+         else:
+-            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:
++            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE.get(dtype)
++            if torch_dtype is None:
+                 raise ValueError(f"Unknown dtype: {dtype}")
+-            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]
+     elif isinstance(dtype, torch.dtype):
+         torch_dtype = dtype
+     else:
+         raise ValueError(f"Unknown dtype: {dtype}")
+ 
+     if is_hip() and torch_dtype == torch.float32:
+-        rocm_supported_dtypes = [
+-            k for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()
+-            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)
+-        ]
+-        raise ValueError(f"dtype \'{dtype}\' is not supported in ROCm. "
+-                         f"Supported dtypes are {rocm_supported_dtypes}")
++        raise ValueError(f"dtype '{dtype}' is not supported in ROCm. "
++                         f"Supported dtypes are {_ROCM_SUPPORTED_DTYPE_NAMES}")
+ 
+     # Verify the dtype.
+     if torch_dtype != config_dtype:
+@@ -455,22 +456,12 @@ def _get_and_verify_max_len(
+ ) -> int:
+     """Get and verify the model's maximum length."""
+     derived_max_model_len = float("inf")
+-    possible_keys = [
+-        # OPT
+-        "max_position_embeddings",
+-        # GPT-2
+-        "n_positions",
+-        # MPT
+-        "max_seq_len",
+-        # ChatGLM2
+-        "seq_length",
+-        # Others
+-        "max_sequence_length",
+-        "max_seq_length",
+-        "seq_len",
+-    ]
+-    for key in possible_keys:
+-        max_len_key = getattr(hf_config, key, None)
++    hf_dict = getattr(hf_config, "__dict__", None)
++    for key in _POSSIBLE_MAX_LEN_KEYS:
++        if hf_dict is not None:
++            max_len_key = hf_dict.get(key)
++        else:
++            max_len_key = getattr(hf_config, key, None)
+         if max_len_key is not None:
+             derived_max_model_len = min(derived_max_model_len, max_len_key)
+     if derived_max_model_len == float("inf"):
+@@ -478,13 +469,12 @@ def _get_and_verify_max_len(
+             # If max_model_len is specified, we use it.
+             return max_model_len
+ 
+-        default_max_len = 2048
+         logger.warning(
+             "The model's config.json does not contain any of the following "
+             "keys to determine the original maximum length of the model: "
+-            f"{possible_keys}. Assuming the model's maximum length is "
+-            f"{default_max_len}.")
+-        derived_max_model_len = default_max_len
++            f"{_POSSIBLE_MAX_LEN_KEYS}. Assuming the model's maximum length is "
++            f"{_DEFAULT_MAX_LEN}.")
++        derived_max_model_len = _DEFAULT_MAX_LEN
+ 
+     rope_scaling = getattr(hf_config, "rope_scaling", None)
+     if rope_scaling is not None:
+diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py
+index 5596884..5fbaa8f 100644
+--- a/vllm/model_executor/models/__init__.py
++++ b/vllm/model_executor/models/__init__.py
+@@ -39,7 +39,7 @@ _MODELS = {
+ }
+ 
+ # Models not supported by ROCm.
+-_ROCM_UNSUPPORTED_MODELS = ["MixtralForCausalLM"]
++_ROCM_UNSUPPORTED_MODELS = frozenset(["MixtralForCausalLM"])
+ 
+ # Models partially supported by ROCm.
+ # Architecture -> Reason.
+diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py
+index 8e0a094..241a0e6 100644
+--- a/vllm/model_executor/models/mixtral.py
++++ b/vllm/model_executor/models/mixtral.py
+@@ -98,6 +98,7 @@ class MixtralAttention(nn.Module):
+         self.head_dim = hidden_size // self.total_num_heads
+         self.q_size = self.num_heads * self.head_dim
+         self.kv_size = self.num_kv_heads * self.head_dim
++        self._split_sizes = (self.q_size, self.kv_size, self.kv_size)
+         self.scaling = self.head_dim**-0.5
+         self.rope_theta = rope_theta
+         self.sliding_window = sliding_window
+@@ -138,7 +139,7 @@ class MixtralAttention(nn.Module):
+         cache_event: Optional[torch.cuda.Event],
+     ) -> torch.Tensor:
+         qkv, _ = self.wqkv(hidden_states)
+-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
++        q, k, v = qkv.split(self._split_sizes, dim=-1)
+         q, k = self.rotary_emb(positions, q, k)
+         k_cache, v_cache = kv_cache
+         attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,
+@@ -243,7 +244,7 @@ class BlockSparseMoE(nn.Module):
+         column_indices_t = row_indices.gather(0, gather_indices.long())
+         block_offsets_t = gather_indices.int()
+ 
+-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)
++        zero = row_indices.new_zeros(1)
+         nnz_per_column = ops.histogram(column_indices, block_columns)
+         nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)
+         offsets_t = torch.cat([zero, nnz_per_column])
+@@ -427,15 +428,17 @@ class MixtralDecoderLayer(nn.Module):
+         input_metadata: InputMetadata,
+         cache_event: Optional[torch.cuda.Event],
+     ) -> torch.Tensor:
++        attn_norm = self.attention_norm
++        ffn_norm = self.ffn_norm
+         r = self.attention(
+             positions=positions,
+-            hidden_states=self.attention_norm(x),
++            hidden_states=attn_norm(x),
+             kv_cache=kv_cache,
+             input_metadata=input_metadata,
+             cache_event=cache_event,
+         )
+         h = x + r
+-        r = self.block_sparse_moe(self.ffn_norm(h))
++        r = self.block_sparse_moe(ffn_norm(h))
+         out = h + r
+         return out
+ 
+@@ -477,9 +480,8 @@ class MixtralForCausalLM(nn.Module):
+         hidden_states = self.tok_embeddings(input_ids)
+ 
+         # forward
+-        for i in range(len(self.layers)):
++        for i, layer in enumerate(self.layers):
+             cache_event = None if cache_events is None else cache_events[i]
+-            layer = self.layers[i]
+             hidden_states = layer(
+                 positions,
+                 hidden_states,
diff --git a/vllm/config.py b/vllm/config.py
index 6bafa73..b368904 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -12,6 +12,41 @@ logger = init_logger(__name__)
 
 _GB = 1 << 30
 
+# Performance-related module-level constants to avoid repeated allocations
+_SUPPORTED_LOAD_FORMATS = ("auto", "pt", "safetensors", "npcache", "dummy")
+_ROCM_NOT_SUPPORTED_LOAD_FORMAT = frozenset(("safetensors",))
+_POSSIBLE_MAX_LEN_KEYS = (
+    "max_position_embeddings",
+    "n_positions",
+    "max_seq_len",
+    "seq_length",
+    "max_sequence_length",
+    "max_seq_length",
+    "seq_len",
+)
+_KV_HEAD_ATTRS = (
+    "n_head_kv",
+    "num_kv_heads",
+    "num_key_value_heads",
+    "multi_query_group_num",
+)
+_FALCON_MODEL_TYPES = ("falcon", "RefinedWeb", "RefinedWebModel")
+_DEFAULT_MAX_LEN = 2048
+
+# Precompute ROCm supported dtype names once
+_STR_DTYPE_TO_TORCH_DTYPE = {
+    "half": torch.float16,
+    "float16": torch.float16,
+    "float": torch.float32,
+    "float32": torch.float32,
+    "bfloat16": torch.bfloat16,
+}
+
+_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]
+_ROCM_SUPPORTED_DTYPE_NAMES = tuple(
+    k for k in _STR_DTYPE_TO_TORCH_DTYPE.keys() if k not in _ROCM_NOT_SUPPORTED_DTYPE
+)
+
 
 class ModelConfig:
     """Configuration for the model.
@@ -98,24 +133,18 @@ class ModelConfig:
 
     def _verify_load_format(self) -> None:
         load_format = self.load_format.lower()
-        supported_load_format = [
-            "auto", "pt", "safetensors", "npcache", "dummy"
-        ]
-        rocm_not_supported_load_format = ["safetensors"]
-        if load_format not in supported_load_format:
+        if load_format not in _SUPPORTED_LOAD_FORMATS:
             raise ValueError(
                 f"Unknown load format: {self.load_format}. Must be one of "
                 "'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.")
         if is_hip():
-            if load_format in ["safetensors"]:
-                rocm_supported_load_format = [
-                    f for f in supported_load_format
-                    if (f not in rocm_not_supported_load_format)
-                ]
+            if load_format in _ROCM_NOT_SUPPORTED_LOAD_FORMAT:
+                rocm_supported_load_format = tuple(
+                    f for f in _SUPPORTED_LOAD_FORMATS
+                    if f not in _ROCM_NOT_SUPPORTED_LOAD_FORMAT)
                 raise ValueError(
-                    f"load format \'{load_format}\' is not supported in ROCm. "
-                    f"Supported load format are "
-                    f"{rocm_supported_load_format}")
+                    f"load format '{load_format}' is not supported in ROCm. "
+                    f"Supported load format are {rocm_supported_load_format}")
             # Force ROCm to load from pt weights if nothing specific is set
             if load_format == "auto":
                 load_format = "pt"
@@ -211,9 +240,8 @@ class ModelConfig:
         # NOTE: for falcon, when new_decoder_architecture is True, the
         # multi_query flag is ignored and we use n_head_kv for the number of
         # KV heads.
-        falcon_model_types = ["falcon", "RefinedWeb", "RefinedWebModel"]
         new_decoder_arch_falcon = (
-            self.hf_config.model_type in falcon_model_types
+            self.hf_config.model_type in _FALCON_MODEL_TYPES
             and getattr(self.hf_config, "new_decoder_architecture", False))
         if not new_decoder_arch_falcon and getattr(self.hf_config,
                                                    "multi_query", False):
@@ -221,16 +249,7 @@ class ModelConfig:
             # Currently, tensor parallelism is not supported in this case.
             return 1
 
-        attributes = [
-            # For Falcon:
-            "n_head_kv",
-            "num_kv_heads",
-            # For LLaMA-2:
-            "num_key_value_heads",
-            # For ChatGLM:
-            "multi_query_group_num",
-        ]
-        for attr in attributes:
+        for attr in _KV_HEAD_ATTRS:
             num_kv_heads = getattr(self.hf_config, attr, None)
             if num_kv_heads is not None:
                 return num_kv_heads
@@ -360,12 +379,9 @@ class SchedulerConfig:
         max_model_len: int,
         max_paddings: int,
     ) -> None:
-        if max_num_batched_tokens is not None:
-            self.max_num_batched_tokens = max_num_batched_tokens
-        else:
-            # If max_model_len is too short, use 2048 as the default value for
-            # higher throughput.
-            self.max_num_batched_tokens = max(max_model_len, 2048)
+        self.max_num_batched_tokens = (max_num_batched_tokens
+                                        if max_num_batched_tokens is not None
+                                        else max(max_model_len, 2048))
         self.max_num_seqs = max_num_seqs
         self.max_model_len = max_model_len
         self.max_paddings = max_paddings
@@ -387,17 +403,6 @@ class SchedulerConfig:
                 f"({self.max_num_seqs}).")
 
 
-_STR_DTYPE_TO_TORCH_DTYPE = {
-    "half": torch.float16,
-    "float16": torch.float16,
-    "float": torch.float32,
-    "float32": torch.float32,
-    "bfloat16": torch.bfloat16,
-}
-
-_ROCM_NOT_SUPPORTED_DTYPE = ["float", "float32"]
-
-
 def _get_and_verify_dtype(
     config: PretrainedConfig,
     dtype: Union[str, torch.dtype],
@@ -418,21 +423,17 @@ def _get_and_verify_dtype(
             else:
                 torch_dtype = config_dtype
         else:
-            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:
+            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE.get(dtype)
+            if torch_dtype is None:
                 raise ValueError(f"Unknown dtype: {dtype}")
-            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]
     elif isinstance(dtype, torch.dtype):
         torch_dtype = dtype
     else:
         raise ValueError(f"Unknown dtype: {dtype}")
 
     if is_hip() and torch_dtype == torch.float32:
-        rocm_supported_dtypes = [
-            k for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()
-            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)
-        ]
-        raise ValueError(f"dtype \'{dtype}\' is not supported in ROCm. "
-                         f"Supported dtypes are {rocm_supported_dtypes}")
+        raise ValueError(f"dtype '{dtype}' is not supported in ROCm. "
+                         f"Supported dtypes are {_ROCM_SUPPORTED_DTYPE_NAMES}")
 
     # Verify the dtype.
     if torch_dtype != config_dtype:
@@ -455,22 +456,12 @@ def _get_and_verify_max_len(
 ) -> int:
     """Get and verify the model's maximum length."""
     derived_max_model_len = float("inf")
-    possible_keys = [
-        # OPT
-        "max_position_embeddings",
-        # GPT-2
-        "n_positions",
-        # MPT
-        "max_seq_len",
-        # ChatGLM2
-        "seq_length",
-        # Others
-        "max_sequence_length",
-        "max_seq_length",
-        "seq_len",
-    ]
-    for key in possible_keys:
-        max_len_key = getattr(hf_config, key, None)
+    hf_dict = getattr(hf_config, "__dict__", None)
+    for key in _POSSIBLE_MAX_LEN_KEYS:
+        if hf_dict is not None:
+            max_len_key = hf_dict.get(key)
+        else:
+            max_len_key = getattr(hf_config, key, None)
         if max_len_key is not None:
             derived_max_model_len = min(derived_max_model_len, max_len_key)
     if derived_max_model_len == float("inf"):
@@ -478,13 +469,12 @@ def _get_and_verify_max_len(
             # If max_model_len is specified, we use it.
             return max_model_len
 
-        default_max_len = 2048
         logger.warning(
             "The model's config.json does not contain any of the following "
             "keys to determine the original maximum length of the model: "
-            f"{possible_keys}. Assuming the model's maximum length is "
-            f"{default_max_len}.")
-        derived_max_model_len = default_max_len
+            f"{_POSSIBLE_MAX_LEN_KEYS}. Assuming the model's maximum length is "
+            f"{_DEFAULT_MAX_LEN}.")
+        derived_max_model_len = _DEFAULT_MAX_LEN
 
     rope_scaling = getattr(hf_config, "rope_scaling", None)
     if rope_scaling is not None:
diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py
index 5596884..5fbaa8f 100644
--- a/vllm/model_executor/models/__init__.py
+++ b/vllm/model_executor/models/__init__.py
@@ -39,7 +39,7 @@ _MODELS = {
 }
 
 # Models not supported by ROCm.
-_ROCM_UNSUPPORTED_MODELS = ["MixtralForCausalLM"]
+_ROCM_UNSUPPORTED_MODELS = frozenset(["MixtralForCausalLM"])
 
 # Models partially supported by ROCm.
 # Architecture -> Reason.
diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py
index 8e0a094..241a0e6 100644
--- a/vllm/model_executor/models/mixtral.py
+++ b/vllm/model_executor/models/mixtral.py
@@ -98,6 +98,7 @@ class MixtralAttention(nn.Module):
         self.head_dim = hidden_size // self.total_num_heads
         self.q_size = self.num_heads * self.head_dim
         self.kv_size = self.num_kv_heads * self.head_dim
+        self._split_sizes = (self.q_size, self.kv_size, self.kv_size)
         self.scaling = self.head_dim**-0.5
         self.rope_theta = rope_theta
         self.sliding_window = sliding_window
@@ -138,7 +139,7 @@ class MixtralAttention(nn.Module):
         cache_event: Optional[torch.cuda.Event],
     ) -> torch.Tensor:
         qkv, _ = self.wqkv(hidden_states)
-        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k, v = qkv.split(self._split_sizes, dim=-1)
         q, k = self.rotary_emb(positions, q, k)
         k_cache, v_cache = kv_cache
         attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata,
@@ -243,7 +244,7 @@ class BlockSparseMoE(nn.Module):
         column_indices_t = row_indices.gather(0, gather_indices.long())
         block_offsets_t = gather_indices.int()
 
-        zero = torch.zeros((1, ), dtype=torch.int32, device=row_indices.device)
+        zero = row_indices.new_zeros(1)
         nnz_per_column = ops.histogram(column_indices, block_columns)
         nnz_per_column = ops.inclusive_cumsum(nnz_per_column, 0)
         offsets_t = torch.cat([zero, nnz_per_column])
@@ -427,15 +428,17 @@ class MixtralDecoderLayer(nn.Module):
         input_metadata: InputMetadata,
         cache_event: Optional[torch.cuda.Event],
     ) -> torch.Tensor:
+        attn_norm = self.attention_norm
+        ffn_norm = self.ffn_norm
         r = self.attention(
             positions=positions,
-            hidden_states=self.attention_norm(x),
+            hidden_states=attn_norm(x),
             kv_cache=kv_cache,
             input_metadata=input_metadata,
             cache_event=cache_event,
         )
         h = x + r
-        r = self.block_sparse_moe(self.ffn_norm(h))
+        r = self.block_sparse_moe(ffn_norm(h))
         out = h + r
         return out
 
@@ -477,9 +480,8 @@ class MixtralForCausalLM(nn.Module):
         hidden_states = self.tok_embeddings(input_ids)
 
         # forward
-        for i in range(len(self.layers)):
+        for i, layer in enumerate(self.layers):
             cache_event = None if cache_events is None else cache_events[i]
-            layer = self.layers[i]
             hidden_states = layer(
                 positions,
                 hidden_states,
