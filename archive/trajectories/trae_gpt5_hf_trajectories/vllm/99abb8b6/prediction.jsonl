{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3a1e648", "model_patch": "diff --git a/tests/v1/sample/test_rejection_sampler.py b/tests/v1/sample/test_rejection_sampler.py\nindex 84139a4..0f56f70 100644\n--- a/tests/v1/sample/test_rejection_sampler.py\n+++ b/tests/v1/sample/test_rejection_sampler.py\n@@ -345,8 +345,8 @@ def estimate_rejection_sampling_pdf(\n                                             num_samples, k)\n \n     # Bonus tokens not used but required.\n-    bonus_token_ids = torch.zeros((1, 1), dtype=torch.int64,\n-                                  device=DEVICE).repeat(num_samples, 1)\n+    bonus_token_ids = torch.zeros((num_samples, 1), dtype=torch.int64,\n+                                  device=DEVICE)\n \n     sampling_metadata = create_sampling_metadata(all_greedy=False)\n     output_token_ids = sampler(draft_token_ids.tolist(), draft_probs,\ndiff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex edae654..b75b487 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -40,7 +40,7 @@ class LogprobsTensors(NamedTuple):\n         )\n \n \n-@dataclass\n+@dataclass(slots=True)\n class SamplerOutput:\n \n     # [num_reqs, max_num_generated_tokens]\n@@ -53,7 +53,7 @@ class SamplerOutput:\n \n # ModelRunnerOutput is serialized and sent to the scheduler process.\n # This is expensive for torch.Tensor so prefer to use list instead.\n-@dataclass\n+@dataclass(slots=True)\n class ModelRunnerOutput:\n \n     # [num_reqs]\n", "model_name_or_path": "gpt-5-2025-08-07"}
