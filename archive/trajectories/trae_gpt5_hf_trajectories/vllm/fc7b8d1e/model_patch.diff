diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py
index 622aca66a..ad26d3c51 100644
--- a/vllm/core/block_manager_v1.py
+++ b/vllm/core/block_manager_v1.py
@@ -336,9 +336,9 @@ class BlockSpaceManagerV1(BlockSpaceManager):
 
         # Assign the self-attention block tables for each sequence.
         if len(wait_seqs) == 1:
-            self.block_tables[wait_seqs[0].seq_id] = block_table
+            self.block_tables[seq.seq_id] = block_table
         else:
-            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):
+            for seq in wait_seqs:
                 self.block_tables[seq.seq_id] = block_table.copy()
 
         # Allocate encoder sequence
diff --git a/vllm/sequence.py b/vllm/sequence.py
index ba477efc5..963e37b94 100644
--- a/vllm/sequence.py
+++ b/vllm/sequence.py
@@ -655,15 +655,26 @@ class SequenceGroup:
         return [seq for seq in self.seqs if not seq.is_finished()]
 
     def get_finished_seqs(self) -> List[Sequence]:
+        if self.is_single_seq:
+            return self.seqs if self.seqs[0].is_finished() else []
+
         return [seq for seq in self.seqs if seq.is_finished()]
 
     def update_num_computed_tokens(self, num_new_computed_tokens: int):
         """Update number of tokens computed so far."""
+        if self.is_single_seq:
+            seq = self.seqs[0]
+            if not seq.is_finished():
+                seq.data.update_num_computed_tokens(num_new_computed_tokens)
+            return
         for seq in self.seqs:
             if not seq.is_finished():
                 seq.data.update_num_computed_tokens(num_new_computed_tokens)
 
     def get_num_uncomputed_tokens(self) -> int:
+        if self.is_single_seq:
+            seq = self.seqs[0]
+            return 0 if seq.is_finished() else seq.data.get_num_uncomputed_tokens()
         num_uncomputed_tokens = 0
         for seq in self.seqs:
             if not seq.is_finished():
