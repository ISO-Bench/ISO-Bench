{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-76b4944", "model_patch": "diff --git a/docs/design/v1/p2p_nccl_connector.md b/docs/design/v1/p2p_nccl_connector.md\nindex b1df93c..89cd13f 100644\n--- a/docs/design/v1/p2p_nccl_connector.md\n+++ b/docs/design/v1/p2p_nccl_connector.md\n@@ -2,6 +2,14 @@ An implementation of xPyD with dynamic scaling based on point-to-point communica\n \n # Detailed Design\n \n+\n+## Performance considerations\n+- Precompile regex patterns used for request_id parsing to avoid repeated compilation.\n+- Specify tensor dtypes explicitly (e.g., torch.long) to avoid dtype inference overhead during tensor creation.\n+- Reuse long-lived aiohttp ClientSession for proxy forwarding instead of creating per-request sessions.\n+- Minimize environment variable lookups by binding os.environ to a local reference in hot paths.\n+- Reduce string allocations by using str.partition instead of str.split when extracting prefixes.\n+\n ## Overall Process\n As shown in Figure 1, the overall process of this **PD disaggregation** solution is described through a request flow:  \n \ndiff --git a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\nindex 4e82424..8a0bd43 100644\n--- a/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n+++ b/examples/online_serving/disaggregated_serving_p2p_nccl_xpyd/disagg_proxy_p2p_nccl_xpyd.py\n@@ -28,13 +28,9 @@ def _listen_for_register(poller, router_socket):\n             #        \"zmq_address\": \"ip:port\"}\n             data = msgpack.loads(message)\n             if data[\"type\"] == \"P\":\n-                global prefill_instances\n-                global prefill_cv\n                 with prefill_cv:\n                     prefill_instances[data[\"http_address\"]] = data[\"zmq_address\"]\n             elif data[\"type\"] == \"D\":\n-                global decode_instances\n-                global decode_cv\n                 with decode_cv:\n                     decode_instances[data[\"http_address\"]] = data[\"zmq_address\"]\n             else:\n@@ -68,6 +64,15 @@ def start_service_discovery(hostname, port):\n AIOHTTP_TIMEOUT = aiohttp.ClientTimeout(total=6 * 60 * 60)\n \n app = Quart(__name__)\n+_AIOHTTP_SESSION: aiohttp.ClientSession | None = None\n+\n+\n+async def _get_session() -> aiohttp.ClientSession:\n+    global _AIOHTTP_SESSION\n+    if _AIOHTTP_SESSION is None or _AIOHTTP_SESSION.closed:\n+        _AIOHTTP_SESSION = aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT)\n+    return _AIOHTTP_SESSION\n+\n \n \n def random_uuid() -> str:\n@@ -75,19 +80,19 @@ def random_uuid() -> str:\n \n \n async def forward_request(url, data, request_id):\n-    async with aiohttp.ClientSession(timeout=AIOHTTP_TIMEOUT) as session:\n-        headers = {\n-            \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n-            \"X-Request-Id\": request_id,\n-        }\n-        async with session.post(url=url, json=data, headers=headers) as response:\n-            if response.status == 200:\n-                if True:\n-                    async for chunk_bytes in response.content.iter_chunked(1024):\n-                        yield chunk_bytes\n-                else:\n-                    content = await response.read()\n-                    yield content\n+    session = await _get_session()\n+    headers = {\n+        \"Authorization\": f\"Bearer {os.environ.get('OPENAI_API_KEY')}\",\n+        \"X-Request-Id\": request_id,\n+    }\n+    async with session.post(url=url, json=data, headers=headers) as response:\n+        if response.status == 200:\n+            if True:\n+                async for chunk_bytes in response.content.iter_chunked(1024):\n+                    yield chunk_bytes\n+            else:\n+                content = await response.read()\n+                yield content\n \n \n @app.route(\"/v1/completions\", methods=[\"POST\"])\n@@ -100,14 +105,10 @@ async def handle_request():\n         prefill_request[\"max_tokens\"] = 1\n \n         global count\n-        global prefill_instances\n-        global prefill_cv\n         with prefill_cv:\n             prefill_list = list(prefill_instances.items())\n             prefill_addr, prefill_zmq_addr = prefill_list[count % len(prefill_list)]\n \n-        global decode_instances\n-        global decode_cv\n         with decode_cv:\n             decode_list = list(decode_instances.items())\n             decode_addr, decode_zmq_addr = decode_list[count % len(decode_list)]\ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\nindex 52f589a..99f7c85 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_connector.py\n@@ -26,6 +26,10 @@ if TYPE_CHECKING:\n \n logger = init_logger(__name__)\n \n+# Precompile regex patterns to avoid recompilation on each parse\n+_PREFILL_RE = re.compile(r\"___prefill_addr_(.*):(\\d+)___\")\n+_DECODE_RE = re.compile(r\"___decode_addr_(.*):(\\d+)\")\n+\n \n @dataclass\n class ReqMeta:\n@@ -40,10 +44,10 @@ class ReqMeta:\n     def make_meta(request_id: str, token_ids: list[int], block_ids: list[int],\n                   block_size: int) -> \"ReqMeta\":\n         valid_num_tokens = len(token_ids)\n-        token_ids_tensor = torch.tensor(token_ids)\n-        block_ids_tensor = torch.tensor(block_ids)\n+        token_ids_tensor = torch.tensor(token_ids, dtype=torch.long)\n+        block_ids_tensor = torch.tensor(block_ids, dtype=torch.long)\n         num_blocks = block_ids_tensor.shape[0]\n-        block_offsets = torch.arange(0, block_size)\n+        block_offsets = torch.arange(0, block_size, dtype=torch.long)\n         slot_mapping = block_offsets.reshape((1, block_size)) + \\\n                 block_ids_tensor.reshape((num_blocks, 1)) * block_size\n         slot_mapping = slot_mapping.flatten()[:valid_num_tokens]\n@@ -456,13 +460,10 @@ class P2pNcclConnector(KVConnectorBase_V1):\n     @staticmethod\n     def parse_request_id(request_id: str, is_prefill=True) -> tuple[str, int]:\n         # Regular expression to match the string hostname and integer port\n-        if is_prefill:\n-            pattern = r\"___decode_addr_(.*):(\\d+)\"\n-        else:\n-            pattern = r\"___prefill_addr_(.*):(\\d+)___\"\n+        pattern = _DECODE_RE if is_prefill else _PREFILL_RE\n \n-        # Use re.search to find the pattern in the request_id\n-        match = re.search(pattern, request_id)\n+        # Use pre-compiled regex to find the pattern in the request_id\n+        match = pattern.search(request_id)\n         if match:\n             # Extract the ranks\n             ip = match.group(1)\ndiff --git a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\nindex 6c9ccb2..7adca43 100644\n--- a/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n+++ b/vllm/distributed/kv_transfer/kv_connector/v1/p2p/p2p_nccl_engine.py\n@@ -41,22 +41,23 @@ def set_p2p_nccl_context(num_channels: str):\n         'NCCL_ALGO',  # RING,TREE\n     ]\n \n+    env = os.environ\n     for var in env_vars:\n-        original_values[var] = os.environ.get(var)\n+        original_values[var] = env.get(var)\n \n     logger.info(\"set_p2p_nccl_context, original_values: %s\", original_values)\n \n     try:\n-        os.environ['NCCL_MAX_NCHANNELS'] = num_channels\n-        os.environ['NCCL_MIN_NCHANNELS'] = num_channels\n-        os.environ['NCCL_CUMEM_ENABLE'] = '1'\n+        env['NCCL_MAX_NCHANNELS'] = num_channels\n+        env['NCCL_MIN_NCHANNELS'] = num_channels\n+        env['NCCL_CUMEM_ENABLE'] = '1'\n         yield\n     finally:\n         for var in env_vars:\n             if original_values[var] is not None:\n-                os.environ[var] = original_values[var]\n+                env[var] = original_values[var]\n             else:\n-                os.environ.pop(var, None)\n+                env.pop(var, None)\n \n \n class P2pNcclEngine:\n@@ -295,6 +296,7 @@ class P2pNcclEngine:\n             socks = dict(self.poller.poll())\n             if self.router_socket in socks:\n                 remote_address, message = self.router_socket.recv_multipart()\n+                raddr = remote_address.decode()\n                 data = msgpack.loads(message)\n                 if data[\"cmd\"] == \"NEW\":\n                     unique_id = self.nccl.unique_id_from_bytes(\n@@ -304,10 +306,10 @@ class P2pNcclEngine:\n                         with set_p2p_nccl_context(self.nccl_num_channels):\n                             comm: ncclComm_t = self.nccl.ncclCommInitRank(\n                                 2, unique_id, rank)\n-                        self.comms[remote_address.decode()] = (comm, rank)\n+                        self.comms[raddr] = (comm, rank)\n                         logger.info(\n                             \"\ud83e\udd1dncclCommInitRank Success, %s\ud83d\udc48%s, MyRank:%s\",\n-                            self.zmq_address, remote_address.decode(), rank)\n+                            self.zmq_address, raddr, rank)\n                 elif data[\"cmd\"] == \"PUT\":\n                     tensor_id = data[\"tensor_id\"]\n                     try:\n@@ -318,7 +320,7 @@ class P2pNcclEngine:\n                                                  device=self.device)\n                         self.router_socket.send_multipart(\n                             [remote_address, b\"0\"])\n-                        comm, rank = self.comms[remote_address.decode()]\n+                        comm, rank = self.comms[raddr]\n                         self._recv(comm, tensor, rank ^ 1, self.recv_stream)\n                         tensor_size = tensor.element_size() * tensor.numel()\n                         if (self.buffer_size + tensor_size\n@@ -329,7 +331,7 @@ class P2pNcclEngine:\n                             logger.warning(\n                                 \"\ud83d\udd34[PUT]Recv Tensor, Out Of Threshold, \"\n                                 \"%s\ud83d\udc48%s, data:%s, addr:%d\", self.zmq_address,\n-                                remote_address.decode(), data, addr)\n+                                raddr, data, addr)\n                         else:\n                             self.buffer_size += tensor_size\n \n@@ -340,7 +342,7 @@ class P2pNcclEngine:\n                         logger.warning(\n                             \"\ud83d\udd34[PUT]Recv Tensor, Out Of Memory, %s\ud83d\udc48%s, \"\n                             \"data:%s\", self.zmq_address,\n-                            remote_address.decode(), data)\n+                            raddr, data)\n \n                     with self.recv_store_cv:\n                         self.recv_store[tensor_id] = tensor\n@@ -368,7 +370,7 @@ class P2pNcclEngine:\n                         [remote_address, msgpack.dumps(data)])\n \n                     if data[\"ret\"] == 0:\n-                        comm, rank = self.comms[remote_address.decode()]\n+                        comm, rank = self.comms[raddr]\n                         self._send(comm, tensor.to(self.device), rank ^ 1,\n                                    self.send_stream)\n                 else:\n@@ -377,13 +379,13 @@ class P2pNcclEngine:\n                         remote_address, data)\n \n     def _have_sent_tensor_id(self, tensor_id: str):\n-        request_id = tensor_id.split('#')[0]\n+        request_id = tensor_id.partition('#')[0]\n         if request_id not in self.send_request_id_to_tensor_ids:\n             self.send_request_id_to_tensor_ids[request_id] = set()\n         self.send_request_id_to_tensor_ids[request_id].add(tensor_id)\n \n     def _have_received_tensor_id(self, tensor_id: str):\n-        request_id = tensor_id.split('#')[0]\n+        request_id = tensor_id.partition('#')[0]\n         if request_id not in self.recv_request_id_to_tensor_ids:\n             self.recv_request_id_to_tensor_ids[request_id] = set()\n         self.recv_request_id_to_tensor_ids[request_id].add(tensor_id)\n", "model_name_or_path": "gpt-5-2025-08-07"}
