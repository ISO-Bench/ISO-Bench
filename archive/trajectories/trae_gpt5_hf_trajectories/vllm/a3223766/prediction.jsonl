{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-bc8a8ce", "model_patch": "diff --git a/vllm/v1/sample/logits_processor.py b/vllm/v1/sample/logits_processor.py\nindex 3a4c259..50d3add 100644\n--- a/vllm/v1/sample/logits_processor.py\n+++ b/vllm/v1/sample/logits_processor.py\n@@ -229,7 +229,7 @@ class MinPLogitsProcessor(LogitsProcessor):\n         super().__init__()\n         self.min_p_count: int = 0\n \n-        self.min_p_cpu_tensor = torch.zeros((max_num_reqs, ),\n+        self.min_p_cpu_tensor = torch.empty((max_num_reqs, ),\n                                             dtype=torch.float32,\n                                             device=\"cpu\",\n                                             pin_memory=pin_memory)\n@@ -246,6 +246,8 @@ class MinPLogitsProcessor(LogitsProcessor):\n             self.min_p_device = self.min_p_cpu_tensor\n         # Current slice of the device tensor\n         self.min_p: torch.Tensor = self.min_p_device[:0]\n+        # Cached log(min_p) per request to avoid recomputing every apply\n+        self.log_min_p: torch.Tensor = self.min_p_device[:0]\n \n     def is_argmax_invariant(self) -> bool:\n         \"\"\"Min-p never impacts greedy sampling\"\"\"\n@@ -294,23 +296,22 @@ class MinPLogitsProcessor(LogitsProcessor):\n                 self.min_p.copy_(self.min_p_cpu_tensor[:size],\n                                  non_blocking=True)\n             self.min_p.unsqueeze_(1)\n+            # Precompute log(min_p) once when updated\n+            self.log_min_p = torch.log(self.min_p)\n+        elif not self.min_p_count:\n+            # No active min_p: keep cached tensors empty\n+            self.log_min_p = self.min_p_device[:0]\n \n     def apply(self, logits: torch.Tensor) -> torch.Tensor:\n         if not self.min_p_count:\n             return logits\n \n-        # Convert logits to probability distribution\n-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n-        # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Adjust min_p\n-        adjusted_min_p = max_probabilities.mul_(self.min_p)\n-        # Identify valid tokens using threshold comparison\n-        invalid_token_mask = probability_values < adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[invalid_token_mask] = -float('inf')\n+        # Use logits-space threshold to avoid softmax:\n+        # keep tokens where z_i - z_max >= log(min_p)\n+        max_logits = torch.amax(logits, dim=-1, keepdim=True)\n+        log_min_p = self.log_min_p if self.log_min_p.numel() else torch.log(self.min_p)\n+        threshold = max_logits + log_min_p\n+        logits.masked_fill_(logits < threshold, float('-inf'))\n         return logits\n \n \n@@ -381,6 +382,8 @@ class LogitBiasLogitsProcessor(LogitsProcessor):\n                                  self._device_tensor(tok_ids, torch.int32))\n \n     def _device_tensor(self, data: list, dtype: torch.dtype) -> torch.Tensor:\n+        if self.device.type == \"cpu\":\n+            return torch.tensor(data, device=\"cpu\", dtype=dtype)\n         return (torch.tensor(data,\n                              device=\"cpu\",\n                              dtype=dtype,\n@@ -480,6 +483,8 @@ class MinTokensLogitsProcessor(LogitsProcessor):\n                                  self._device_tensor(tok_ids, torch.int32))\n \n     def _device_tensor(self, data: list, dtype: torch.dtype) -> torch.Tensor:\n+        if self.device.type == \"cpu\":\n+            return torch.tensor(data, device=\"cpu\", dtype=dtype)\n         return (torch.tensor(data,\n                              device=\"cpu\",\n                              dtype=dtype,\n", "model_name_or_path": "gpt-5-2025-08-07"}
