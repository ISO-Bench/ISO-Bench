{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-89ac266", "model_patch": "diff --git a/vllm/config.py b/vllm/config.py\nindex 6bfe94b..8f9889e 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -5014,6 +5014,9 @@ def get_current_model_prefix() -> str:\n     return _current_prefix\n \n \n+# Precompiled pattern to speed up repeated checks for printed object reprs\n+_OBJECT_PRINT_RE = re.compile(r\"at 0x[a-fA-F0-9]{2,16}>\")\n+\n def contains_object_print(text):\n     \"\"\"\n     Check if the text looks like a printed Python object, e.g.\n@@ -5027,9 +5030,7 @@ def contains_object_print(text):\n     Returns:\n         result (bool): `True` if a match is found, `False` otherwise.\n     \"\"\"\n-    pattern = r'at 0x[a-fA-F0-9]{2,16}>'\n-    match = re.search(pattern, text)\n-    return match is not None\n+    return _OBJECT_PRINT_RE.search(text) is not None\n \n \n def assert_hashable(text):\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 0eff741..5c5c746 100755\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -6,6 +6,17 @@ import os\n import sys\n import tempfile\n from typing import TYPE_CHECKING, Any, Callable, Optional\n+from urllib.parse import urlparse\n+\n+# Precompute defaults to avoid repeated getenv/expanduser work at runtime\n+_DEFAULT_CACHE_ROOT = os.getenv(\n+    \"XDG_CACHE_HOME\",\n+    os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n+)\n+_DEFAULT_CONFIG_ROOT = os.getenv(\n+    \"XDG_CONFIG_HOME\",\n+    os.path.join(os.path.expanduser(\"~\"), \".config\"),\n+)\n \n if TYPE_CHECKING:\n     VLLM_HOST_IP: str = \"\"\n@@ -146,17 +157,11 @@ if TYPE_CHECKING:\n \n \n def get_default_cache_root():\n-    return os.getenv(\n-        \"XDG_CACHE_HOME\",\n-        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n-    )\n+    return _DEFAULT_CACHE_ROOT\n \n \n def get_default_config_root():\n-    return os.getenv(\n-        \"XDG_CONFIG_HOME\",\n-        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n-    )\n+    return _DEFAULT_CONFIG_ROOT\n \n \n def maybe_convert_int(value: Optional[str]) -> Optional[int]:\n@@ -182,7 +187,6 @@ def get_vllm_port() -> Optional[int]:\n     try:\n         return int(port)\n     except ValueError as err:\n-        from urllib.parse import urlparse\n         parsed = urlparse(port)\n         if parsed.scheme:\n             raise ValueError(\n@@ -994,17 +998,48 @@ environment_variables: dict[str, Callable[[], Any]] = {\n }\n \n # --8<-- [end:env-vars-definition]\n+# Simple cache to avoid repeated lambda evaluations for env values\n+_ENV_CACHE: dict[str, Any] = {}\n+_ENV_KEYS: Optional[tuple[str, ...]] = None\n \n \n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation of environment variables with caching\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        try:\n+            return _ENV_CACHE[name]\n+        except KeyError:\n+            value = environment_variables[name]()\n+            _ENV_CACHE[name] = value\n+            return value\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \n def __dir__():\n-    return list(environment_variables.keys())\n+    global _ENV_KEYS\n+    if _ENV_KEYS is None:\n+        _ENV_KEYS = tuple(environment_variables.keys())\n+    return list(_ENV_KEYS)\n+\n+\n+def _invalidate_env_cache(name: Optional[str] = None) -> None:\n+    if name is None:\n+        _ENV_CACHE.clear()\n+    else:\n+        _ENV_CACHE.pop(name, None)\n+\n+# Static list of env vars that influence computation graphs; avoid\n+# re-allocating this list on every compute_hash() call.\n+_ENV_VARS_TO_HASH: tuple[str, ...] = (\n+    \"VLLM_PP_LAYER_PARTITION\",\n+    \"VLLM_MLA_DISABLE\",\n+    \"VLLM_USE_TRITON_FLASH_ATTN\",\n+    \"VLLM_USE_TRITON_AWQ\",\n+    \"VLLM_DP_RANK\",\n+    \"VLLM_DP_SIZE\",\n+    \"VLLM_USE_STANDALONE_COMPILE\",\n+    \"VLLM_FUSED_MOE_CHUNK_SIZE\",\n+)\n \n \n def is_set(name: str):\n@@ -1021,6 +1056,7 @@ def set_vllm_use_v1(use_v1: bool):\n             \"explicitly by the user. Please raise this as a Github \"\n             \"Issue and explicitly set VLLM_USE_V1=0 or 1.\")\n     os.environ[\"VLLM_USE_V1\"] = \"1\" if use_v1 else \"0\"\n+    _invalidate_env_cache(\"VLLM_USE_V1\")\n \n \n def compute_hash() -> str:\n@@ -1037,26 +1073,14 @@ def compute_hash() -> str:\n \n     # summarize environment variables\n     def factorize(name: str):\n-        if __getattr__(name):\n-            factors.append(__getattr__(name))\n-        else:\n-            factors.append(\"None\")\n+        val = __getattr__(name)\n+        factors.append(val if val else \"None\")\n \n     # The values of envs may affects the computation graph.\n     # TODO(DefTruth): hash all environment variables?\n     # for key in environment_variables:\n     #     factorize(key)\n-    environment_variables_to_hash = [\n-        \"VLLM_PP_LAYER_PARTITION\",\n-        \"VLLM_MLA_DISABLE\",\n-        \"VLLM_USE_TRITON_FLASH_ATTN\",\n-        \"VLLM_USE_TRITON_AWQ\",\n-        \"VLLM_DP_RANK\",\n-        \"VLLM_DP_SIZE\",\n-        \"VLLM_USE_STANDALONE_COMPILE\",\n-        \"VLLM_FUSED_MOE_CHUNK_SIZE\",\n-    ]\n-    for key in environment_variables_to_hash:\n+    for key in _ENV_VARS_TO_HASH:\n         if key in environment_variables:\n             factorize(key)\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
