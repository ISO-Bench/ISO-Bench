{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3d92516", "model_patch": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex a55d79b..61b8f26 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -286,6 +286,10 @@ class Scheduler:\n         # Latency of the last prompt step\n         self.last_prompt_latency = 0.0\n \n+\n+        # Cache common policies to avoid repeated factory lookups\n+        self._fcfs_policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n+\n     @property\n     def lora_enabled(self) -> bool:\n         return bool(self.lora_config)\n@@ -343,8 +347,7 @@ class Scheduler:\n                         self.free_seq(seq)\n \n     def has_unfinished_seqs(self) -> bool:\n-        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n-            self.swapped) != 0\n+        return bool(self.waiting) or bool(self.running) or bool(self.swapped)\n \n     def get_num_unfinished_seq_groups(self) -> int:\n         return len(self.waiting) + len(self.running) + len(self.swapped)\n@@ -717,8 +720,8 @@ class Scheduler:\n             remaining_waiting, prefills = self._schedule_prefills(\n                 self.waiting, budget, curr_loras, enable_chunking=False)\n \n-        fcfs_policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n         # Don't schedule decodes if prefills are scheduled.\n+        fcfs_policy = self._fcfs_policy\n         # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n         # only contains decode requests, not chunked prefills.\n         if len(prefills.seq_groups) == 0:\n@@ -801,7 +804,7 @@ class Scheduler:\n             self.swapped, SchedulerSwappedInOutputs.create_empty())\n \n         # Decoding should be always scheduled first by fcfs.\n-        fcfs_policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n+        fcfs_policy = self._fcfs_policy\n         remaining_running, running_scheduled = self._schedule_running(\n             self.running,\n             budget,\n@@ -897,8 +900,7 @@ class Scheduler:\n \n         # Create input data structures.\n         seq_group_metadata_list: List[SequenceGroupMetadata] = []\n-        for i, scheduled_seq_group in enumerate(\n-                scheduler_outputs.scheduled_seq_groups):\n+        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n             seq_group = scheduled_seq_group.seq_group\n             token_chunk_size = scheduled_seq_group.token_chunk_size\n             seq_group.maybe_set_first_scheduled_time(now)\n", "model_name_or_path": "gpt-5-2025-08-07"}
