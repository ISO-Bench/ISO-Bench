diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py
index 5d8b3f423..eaaa48cdd 100644
--- a/vllm/v1/sample/ops/topk_topp_sampler.py
+++ b/vllm/v1/sample/ops/topk_topp_sampler.py
@@ -89,18 +89,18 @@ class TopKTopPSampler(nn.Module):
         p: Optional[torch.Tensor],
     ) -> torch.Tensor:
         """More optimized implementation for top-k and top-p sampling."""
-        probs = logits.softmax(dim=-1, dtype=torch.float32)
         if k is None and p is None:
             # We prefer `random_sample` over `flashinfer_sample` when sorting is
             # not needed. This is because `random_sample` does not require
             # CPU-GPU synchronization while `flashinfer_sample` does.
+            probs = logits.softmax(dim=-1, dtype=torch.float32)
             return random_sample(probs, generators)
         if generators:
             logger.warning("FlashInfer 0.2.3+ does not support "
                            "per-request generators. Falling back to "
                            "PyTorch-native implementation.")
             return self.forward_native(logits, generators, k, p)
-        return flashinfer_sample(probs, k, p, generators)
+        return flashinfer_sample(logits, k, p, generators)
 
     def forward_tpu(
         self,
@@ -200,7 +200,7 @@ def apply_top_k_top_p(
         logits_sort.masked_fill_(top_p_mask, -float("inf"))
 
     # Re-sort the probabilities.
-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)
+    logits.scatter_(dim=-1, index=logits_idx, src=logits_sort)
     return logits
 
 
@@ -254,12 +254,12 @@ def random_sample(
 
 
 def flashinfer_sample(
-    probs: torch.Tensor,
+    logits: torch.Tensor,
     k: Optional[torch.Tensor],
     p: Optional[torch.Tensor],
     generators: dict[int, torch.Generator],
 ) -> torch.Tensor:
-    """Sample from the probabilities using FlashInfer.
+    """Sample from the logits using FlashInfer.
 
     Statistically, this function is equivalent to the `random_sample` function.
     However, this function is faster because it avoids sorting the logits tensor
@@ -275,17 +275,45 @@ def flashinfer_sample(
     """
     assert not (k is None and p is None)
 
+    # Prefer logits-based FlashInfer APIs when available to avoid redundant
+    # softmax on the host side. Fallback to *_from_probs if necessary.
+    fi = flashinfer.sampling
+
+    def _to_probs(x: torch.Tensor) -> torch.Tensor:
+        return x.softmax(dim=-1, dtype=torch.float32)
+
     if k is None:
         # Top-p only.
-        next_token_ids = flashinfer.sampling.top_p_sampling_from_probs(
-            probs, p, deterministic=True)
+        if hasattr(fi, 'top_p_sampling_from_logits'):
+            next_token_ids = fi.top_p_sampling_from_logits(
+                logits, p, deterministic=True)
+        elif hasattr(fi, 'top_p_sampling'):
+            next_token_ids = fi.top_p_sampling(
+                logits, p, deterministic=True)
+        else:
+            next_token_ids = fi.top_p_sampling_from_probs(
+                _to_probs(logits), p, deterministic=True)
     elif p is None:
         # Top-k only.
-        next_token_ids = flashinfer.sampling.top_k_sampling_from_probs(
-            probs, k, deterministic=True)
+        if hasattr(fi, 'top_k_sampling_from_logits'):
+            next_token_ids = fi.top_k_sampling_from_logits(
+                logits, k, deterministic=True)
+        elif hasattr(fi, 'top_k_sampling'):
+            next_token_ids = fi.top_k_sampling(
+                logits, k, deterministic=True)
+        else:
+            next_token_ids = fi.top_k_sampling_from_probs(
+                _to_probs(logits), k, deterministic=True)
     else:
         # Both top-k and top-p.
-        next_token_ids = (flashinfer.sampling.top_k_top_p_sampling_from_probs(
-            probs, k, p, deterministic=True))
+        if hasattr(fi, 'top_k_top_p_sampling_from_logits'):
+            next_token_ids = fi.top_k_top_p_sampling_from_logits(
+                logits, k, p, deterministic=True)
+        elif hasattr(fi, 'top_k_top_p_sampling'):
+            next_token_ids = fi.top_k_top_p_sampling(
+                logits, k, p, deterministic=True)
+        else:
+            next_token_ids = fi.top_k_top_p_sampling_from_probs(
+                _to_probs(logits), k, p, deterministic=True)
 
     return next_token_ids.view(-1)
