{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-2f38518", "model_patch": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e791453..9ee989e 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -59,7 +59,7 @@ class InputBatch:\n         # Find a way to reduce the CPU memory usage.\n         # This buffer is not directly transferred to the GPU, so it does not\n         # need to be pinned.\n-        self.token_ids_cpu_tensor = torch.zeros(\n+        self.token_ids_cpu_tensor = torch.empty(\n             (max_num_reqs, max_model_len),\n             device=\"cpu\",\n             dtype=torch.int32,\n@@ -70,12 +70,12 @@ class InputBatch:\n         self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n-        self.block_table = torch.zeros(\n+        self.block_table = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=self.device,\n             dtype=torch.int32,\n         )\n-        self.block_table_cpu_tensor = torch.zeros(\n+        self.block_table_cpu_tensor = torch.empty(\n             (max_num_reqs, max_num_blocks_per_req),\n             device=\"cpu\",\n             dtype=torch.int32,\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54..56d9443 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -143,31 +143,42 @@ class GPUModelRunner:\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n-                                            dtype=torch.int32,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n+                                            dtype=torch.int64,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                              dtype=torch.int32,\n                                              device=\"cpu\",\n                                              pin_memory=self.pin_memory)\n         self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n+        # Preallocate device buffers to avoid per-step allocations.\n+        self.query_start_loc_device = torch.empty(self.max_num_reqs + 1,\n+                                                  dtype=torch.int32,\n+                                                  device=self.device)\n+        self.seq_start_loc_device = torch.empty(self.max_num_reqs + 1,\n+                                                dtype=torch.int32,\n+                                                device=self.device)\n+        self.slot_mapping_device = torch.empty(self.max_num_tokens,\n+                                               dtype=torch.int64,\n+                                               device=self.device)\n+\n \n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n@@ -356,12 +367,16 @@ class GPUModelRunner:\n             self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n         self.positions[:total_num_scheduled_tokens].copy_(\n             self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n-        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n-            self.device, non_blocking=True)\n-        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n-            self.device, non_blocking=True)\n-        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n-            self.device, non_blocking=True).long()\n+        self.query_start_loc_device[:num_reqs + 1].copy_(\n+            self.query_start_loc_cpu[:num_reqs + 1], non_blocking=True)\n+        query_start_loc = self.query_start_loc_device[:num_reqs + 1]\n+        self.seq_start_loc_device[:num_reqs + 1].copy_(\n+            self.seq_start_loc_cpu[:num_reqs + 1], non_blocking=True)\n+        seq_start_loc = self.seq_start_loc_device[:num_reqs + 1]\n+        self.slot_mapping_device[:total_num_scheduled_tokens].copy_(\n+            self.slot_mapping_cpu[:total_num_scheduled_tokens],\n+            non_blocking=True)\n+        slot_mapping = self.slot_mapping_device[:total_num_scheduled_tokens]\n \n         # Prepare for cascade attention if needed.\n         common_prefix_len = (scheduler_output.num_common_prefix_blocks *\n", "model_name_or_path": "gpt-5-2025-08-07"}
