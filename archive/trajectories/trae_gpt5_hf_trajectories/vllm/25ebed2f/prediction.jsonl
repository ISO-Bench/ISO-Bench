{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d263bd9", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex abcd4b0..97e6615 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -107,43 +107,49 @@ class GPUModelRunner:\n             reversed(self.vllm_config.compilation_config.capture_sizes))\n \n         # Persistent buffers for CUDA graphs.\n-        self.input_ids = torch.zeros(self.max_num_tokens,\n+        self.input_ids = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int32,\n                                      device=self.device)\n-        self.positions = torch.zeros(self.max_num_tokens,\n+        self.positions = torch.empty(self.max_num_tokens,\n                                      dtype=torch.int64,\n                                      device=self.device)\n-        self.inputs_embeds = torch.zeros(\n+        self.inputs_embeds = torch.empty(\n             (self.max_num_tokens, self.hidden_size),\n             dtype=self.dtype,\n             device=self.device)\n \n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.input_ids_np = self.input_ids_cpu.numpy()\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n+        self.slot_mapping_cpu = torch.empty(self.max_num_tokens,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n         self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.seq_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                              dtype=torch.int32,\n                                              device=\"cpu\",\n                                              pin_memory=self.pin_memory)\n         self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n \n+        # Pre-allocated CPU buffer for block number gather to avoid per-step allocation.\n+        self._block_numbers_cpu = torch.empty(self.max_num_tokens,\n+                                              dtype=torch.int32,\n+                                              device=\"cpu\",\n+                                              pin_memory=self.pin_memory)\n+\n     def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n         # Remove stopped requests from the cached states.\n         # Keep the states of the pre-empted requests.\n@@ -255,25 +261,25 @@ class GPUModelRunner:\n             non_blocking=True)\n \n         # Get the number of scheduled tokens for each request.\n-        # TODO: The Python loop can be slow. Optimize.\n-        num_scheduled_tokens = []\n-        max_num_scheduled_tokens = 0\n-        for req_id in self.input_batch.req_ids[:num_reqs]:\n-            assert req_id is not None\n-            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n-            num_scheduled_tokens.append(num_tokens)\n-            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n-                                           num_tokens)\n-        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n+        # Vectorized construction to reduce Python overhead.\n+        req_ids_view = self.input_batch.req_ids[:num_reqs]\n+        num_scheduled_tokens = np.fromiter(\n+            (scheduler_output.num_scheduled_tokens[req_id] for req_id in req_ids_view),\n+            dtype=np.int32,\n+            count=num_reqs,\n+        )\n+        max_num_scheduled_tokens = int(num_scheduled_tokens.max())\n         assert max_num_scheduled_tokens > 0\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n         req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n \n-        # Get batched arange.\n+        # Get batched arange using vectorization to avoid Python overhead.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n+        base = np.arange(max_num_scheduled_tokens, dtype=np.int64)\n+        mask = base < num_scheduled_tokens[:, None]\n+        arange = np.broadcast_to(base, (num_reqs, max_num_scheduled_tokens))[mask]\n \n         # Get positions.\n         positions_np = self.positions_np[:total_num_scheduled_tokens]\n@@ -306,8 +312,11 @@ class GPUModelRunner:\n         # NOTE(woosuk): We use torch.index_select instead of np.take here\n         # because torch.index_select is much faster than np.take for large\n         # tensors.\n-        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n-                         [block_table_indices].numpy())\n+        torch.index_select(self.input_batch.block_table_cpu_tensor.flatten(),\n+                           0,\n+                           torch.from_numpy(block_table_indices),\n+                           out=self._block_numbers_cpu[:total_num_scheduled_tokens])\n+        block_numbers = self._block_numbers_cpu[:total_num_scheduled_tokens].numpy()\n         block_offsets = positions_np % self.block_size\n         np.add(block_numbers * self.block_size,\n                block_offsets,\n@@ -648,6 +657,6 @@ class GPUModelRunner:\n             num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n         for _ in range(self.num_attn_layers):\n             self.kv_caches.append(\n-                torch.zeros(kv_cache_shape,\n+                torch.empty(kv_cache_shape,\n                             dtype=self.kv_cache_dtype,\n                             device=self.device))\n", "model_name_or_path": "gpt-5-2025-08-07"}
