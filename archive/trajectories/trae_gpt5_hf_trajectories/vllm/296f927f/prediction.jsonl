{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0032903", "model_patch": "diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py\nindex fec6d61..029403b 100644\n--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py\n+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py\n@@ -68,19 +68,18 @@ class Mixer2RMSNormGated(CustomOp):\n         #   3. The general case can be pretty complicated so we AllGather\n         #      the input and then redundantly compute the RMSNorm.\n         input_dtype = x.dtype\n-        x = x * nn.functional.silu(gate.to(torch.float32))\n+        x = x * nn.functional.silu(gate.float())\n \n         if self.n_groups == 1:\n             if self.tp_size > 1:\n-                # Compute local sum and then reduce to obtain global sum\n-                local_sums = x.pow(2).sum(dim=-1, keepdim=True)\n+                local_sums = x.square().sum(dim=-1, keepdim=True)\n                 global_sums = tensor_model_parallel_all_reduce(local_sums)\n                 # Calculate the variance\n                 count = self.tp_size * x.shape[-1]\n                 variance = (global_sums / count)\n \n             else:\n-                variance = x.pow(2).mean(-1, keepdim=True)\n+                variance = x.square().mean(-1, keepdim=True)\n             x = x * torch.rsqrt(variance + self.variance_epsilon)\n         else:\n             redundant_tp: bool = self.n_groups % self.tp_size != 0\n@@ -91,7 +90,7 @@ class Mixer2RMSNormGated(CustomOp):\n             *prefix_dims, hidden_dim = x.shape\n             group_count = hidden_dim // self.group_size\n             x_grouped = x.view(*prefix_dims, group_count, self.group_size)\n-            variance = x_grouped.pow(2).mean(-1, keepdim=True)\n+            variance = x_grouped.square().mean(-1, keepdim=True)\n             x_grouped = x_grouped * torch.rsqrt(variance +\n                                                 self.variance_epsilon)\n             x = x_grouped.view(*prefix_dims, hidden_dim)\n@@ -116,7 +115,7 @@ class Mixer2RMSNormGated(CustomOp):\n \n         # cast x and gate to float32 before silu\n         out = torch.empty_like(x)\n-        y = x * nn.functional.silu(gate.to(torch.float32))\n+        y = x * nn.functional.silu(gate.float())\n         ops.rms_norm(\n             out,\n             y.to(x.dtype),\n@@ -420,8 +419,7 @@ class MambaMixer2(CustomOp):\n         )\n \n         # 2. Convolution sequence transformation\n-        conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0),\n-                                               self.conv1d.weight.size(2))\n+        conv_weights = self.conv1d.weight.squeeze(1)\n \n         if has_prefill:\n             # |---------- N-1 iteration --------|\n@@ -444,8 +442,7 @@ class MambaMixer2(CustomOp):\n                 query_start_loc=attn_metadata.query_start_loc).transpose(\n                     0, 1)[:seq_len]\n \n-            # TODO: Why is this needed?\n-            hidden_states_B_C = hidden_states_B_C.contiguous()\n+\n         else:\n             hidden_states_B_C = causal_conv1d_update(\n                 hidden_states_B_C,\n@@ -470,10 +467,8 @@ class MambaMixer2(CustomOp):\n         if has_prefill:\n \n             initial_states = None\n-            if has_initial_states is not None and any(has_initial_states):\n-                for idx in mamba_cache_params.state_indices_tensor[\n-                        ~has_initial_states]:\n-                    mamba_cache_params.ssm_state[idx].zero_()\n+            if has_initial_states is not None and has_initial_states.any().item():\n+                mamba_cache_params.ssm_state[mamba_cache_params.state_indices_tensor[~has_initial_states]].zero_()\n                 initial_states = mamba_cache_params.ssm_state[\n                     mamba_cache_params.state_indices_tensor]\n \n@@ -499,19 +494,17 @@ class MambaMixer2(CustomOp):\n \n             # update ssm states\n             # - varlen state is a (batch, nheads, headdim, dstate) tensor\n-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):\n-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])\n+            mamba_cache_params.ssm_state[mamba_cache_params.state_indices_tensor].copy_(varlen_state)\n \n             # - reshape\n             hidden_states = scan_output.view(seq_len, -1)\n         else:\n \n             n_groups = self.n_groups // self.tp_size\n-            A = self.A[:, None, ...][:, :, None].expand(\n-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n-            dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n-            dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n-            D = self.D[:, None, ...].expand(-1, self.head_dim)\n+            A = self.A.float().view(-1, 1, 1).expand(-1, self.head_dim, self.ssm_state_size)\n+            dt = dt.unsqueeze(-1).expand(-1, -1, self.head_dim)\n+            dt_bias = self.dt_bias.view(-1, 1).expand(-1, self.head_dim)\n+            D = self.D.view(-1, 1).expand(-1, self.head_dim)\n             B = B.view(-1, n_groups, B.shape[1] // n_groups)\n             C = C.view(-1, n_groups, C.shape[1] // n_groups)\n             hidden_states_reshaped = hidden_states.view(\n", "model_name_or_path": "gpt-5-2025-08-07"}
