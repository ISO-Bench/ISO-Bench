{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-733e7c9", "model_patch": "diff --git a/vllm/v1/sample/ops/topk_topp_sampler.py b/vllm/v1/sample/ops/topk_topp_sampler.py\nindex 1dea711..8fd8caf 100644\n--- a/vllm/v1/sample/ops/topk_topp_sampler.py\n+++ b/vllm/v1/sample/ops/topk_topp_sampler.py\n@@ -140,6 +140,27 @@ def apply_top_k_top_p(\n     \"\"\"\n     if k is None and p is None:\n         return logits\n+\n+    # Fast path for top-k only when k is uniform across the batch.\n+    if k is not None and p is None:\n+        # Treat k >= vocab_size as no-op.\n+        vocab_size = logits.size(1)\n+        k_long = k.to(torch.long)\n+        if (k_long >= vocab_size).all():\n+            return logits\n+        # If k is a scalar or all rows share the same k, use torch.topk.\n+        if k_long.numel() == 1 or (k_long.ndim == 1 and torch.all(k_long == k_long[0])):\n+            kk = int(k_long.reshape(-1)[0].item())\n+            if kk <= 0:\n+                # All masked out.\n+                logits.fill_(-float(\"inf\"))\n+                return logits\n+            topk_values, topk_indices = torch.topk(logits, kk, dim=-1)\n+            mask = torch.ones_like(logits, dtype=torch.bool)\n+            mask.scatter_(-1, topk_indices, False)\n+            logits.masked_fill_(mask, -float('inf'))\n+            return logits\n+\n     logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n \n     if k is not None:\n@@ -159,8 +180,8 @@ def apply_top_k_top_p(\n         top_p_mask[:, -1] = False\n         logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n \n-    # Re-sort the probabilities.\n-    logits = logits_sort.scatter(dim=-1, index=logits_idx, src=logits_sort)\n+    # Re-sort the probabilities back to the original order (in-place in logits).\n+    logits.scatter_(dim=-1, index=logits_idx, src=logits_sort)\n     return logits\n \n \ndiff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 397a049..45c0519 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -153,25 +153,30 @@ class Sampler(nn.Module):\n         \"\"\"\n         assert token_ids.dtype == torch.int64\n         # Find the topK values.\n-        topk_logprobs, topk_indices = torch.topk(logprobs,\n-                                                 num_logprobs,\n-                                                 dim=-1)\n+        topk_logprobs, topk_indices = torch.topk(logprobs, num_logprobs, dim=-1)\n \n-        # Get with the logprob of the prompt or sampled token.\n+        # Get the logprob of the prompt or sampled token.\n         token_ids = token_ids.unsqueeze(-1)\n         token_logprobs = logprobs.gather(-1, token_ids)\n \n         # Compute the ranks of the actual token.\n         token_ranks = (logprobs >= token_logprobs).sum(-1)\n \n-        # Concatenate together with the topk.\n-        indices = torch.cat((token_ids, topk_indices), dim=1)\n-        logprobs = torch.cat((token_logprobs, topk_logprobs), dim=1)\n-\n-        # Use int32 to reduce the tensor size.\n-        indices = indices.to(torch.int32)\n-\n-        return LogprobsTensors(indices, logprobs, token_ranks)\n+        # Preallocate output tensors to avoid concatenation overhead.\n+        num_rows = logprobs.size(0)\n+        out_indices = torch.empty((num_rows, num_logprobs + 1),\n+                                  dtype=torch.int32,\n+                                  device=logprobs.device)\n+        out_logprobs = torch.empty((num_rows, num_logprobs + 1),\n+                                   dtype=logprobs.dtype,\n+                                   device=logprobs.device)\n+        # Fill first column with selected token, remaining with top-k.\n+        out_indices[:, :1] = token_ids\n+        out_indices[:, 1:] = topk_indices.to(torch.int32)\n+        out_logprobs[:, :1] = token_logprobs\n+        out_logprobs[:, 1:] = topk_logprobs\n+\n+        return LogprobsTensors(out_indices, out_logprobs, token_ranks)\n \n     def apply_penalties(\n         self,\n@@ -203,17 +208,11 @@ class Sampler(nn.Module):\n         Filters logits using adaptive probability thresholding.\n         \"\"\"\n         # Convert logits to probability distribution\n-        probability_values = torch.nn.functional.softmax(logits, dim=-1)\n-        # Calculate maximum probabilities per sequence\n-        max_probabilities = torch.amax(probability_values,\n-                                       dim=-1,\n-                                       keepdim=True)\n-        # Reshape min_p for broadcasting\n-        adjusted_min_p = min_p.unsqueeze(1) * max_probabilities\n-        # Identify valid tokens using threshold comparison\n-        valid_token_mask = probability_values >= adjusted_min_p\n-        # Apply mask using boolean indexing\n-        logits[~valid_token_mask] = -float('inf')\n+        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n+        # Compute per-row threshold\n+        threshold = min_p.unsqueeze(1) * probabilities.amax(dim=-1, keepdim=True)\n+        # Mask tokens below threshold in-place\n+        logits.masked_fill_(probabilities < threshold, -float('inf'))\n         return logits\n \n     def apply_logits_bias(\n", "model_name_or_path": "gpt-5-2025-08-07"}
