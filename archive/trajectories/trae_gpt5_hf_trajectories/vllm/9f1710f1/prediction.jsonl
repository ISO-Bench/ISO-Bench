{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-e642ec9", "model_patch": "diff --git a/vllm/attention/backends/mla/common.py b/vllm/attention/backends/mla/common.py\nindex 8184b07..15f682e 100644\n--- a/vllm/attention/backends/mla/common.py\n+++ b/vllm/attention/backends/mla/common.py\n@@ -961,10 +961,9 @@ class MLACommonMetadataBuilder(AttentionMetadataBuilder[T], Generic[T]):\n             chunk_seq_lens = (chunk_ends - context_chunk_starts).clamp(min=0)\n             _context_chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                 torch.int32)\n-            zero = torch.zeros(num_chunks, dtype=torch.int32, device=device)\\\n-                .unsqueeze(-1)\n-            context_chunk_cu_seq_lens = \\\n-                torch.cat([zero, _context_chunk_cu_seq_lens], dim=1)\n+            context_chunk_cu_seq_lens = torch.empty((num_chunks, self.num_prefills + 1), dtype=torch.int32, device=device)\n+            context_chunk_cu_seq_lens[:, 0] = 0\n+            context_chunk_cu_seq_lens[:, 1:] = _context_chunk_cu_seq_lens\n             context_chunk_max_seq_lens = \\\n                 chunk_seq_lens.max(dim=1).values.tolist()\n             context_chunk_seq_tot = chunk_seq_lens.sum(dim=1).tolist()\n@@ -1295,6 +1294,9 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n         assert attn_metadata.context_chunk_workspace is not None\n         workspace = attn_metadata.context_chunk_workspace\n \n+\n+        tmp_output = None\n+        tmp_output_lse = None\n         for i in range(iters):\n             toks = prefill_metadata.context_chunk_seq_tot[i]\n \n@@ -1322,9 +1324,8 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n             # For MLA the v head dim is smaller than qk head dim so we pad\n             # out v with 0s to match the qk head dim\n-            v_padded = torch.nn.functional.pad(v,\n-                                               [0, q.shape[-1] - v.shape[-1]],\n-                                               value=0)\n+            pad = q.shape[-1] - v.shape[-1]\n+            v_padded = v if pad == 0 else torch.nn.functional.pad(v, [0, pad], value=0)\n \n             if is_hip and envs.VLLM_USE_TRITON_FLASH_ATTN:\n                 attn_output, attn_softmax_lse = self.triton_fa_func(\n@@ -1373,18 +1374,20 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                if tmp_output is None:\n+                    tmp_output = torch.empty_like(output)\n+                    tmp_output_lse = torch.empty_like(output_lse)\n                 merge_attn_states(\n-                    output=output_tmp,\n-                    output_lse=output_lse_tmp,\n+                    output=tmp_output,\n+                    output_lse=tmp_output_lse,\n                     prefix_output=output,\n                     prefix_lse=output_lse,\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                output, tmp_output = tmp_output, output\n+                output_lse, tmp_output_lse = tmp_output_lse, output_lse\n+\n \n         return output, output_lse\n \n@@ -1412,8 +1415,9 @@ class MLACommonImpl(MLAAttentionImpl[T], Generic[T]):\n \n         # For MLA the v head dim is smaller than qk head dim so we pad out\n         # v with 0s to match the qk head dim\n-        v_padded = torch.nn.functional.pad(v, [0, q.shape[-1] - v.shape[-1]],\n-                                           value=0)\n+        pad = q.shape[-1] - v.shape[-1]\n+        v_padded = v if pad == 0 else torch.nn.functional.pad(v, [0, pad], value=0)\n+\n \n         if is_hip and envs.VLLM_USE_TRITON_FLASH_ATTN:\n             output = self.triton_fa_func(\ndiff --git a/vllm/v1/attention/backends/mla/common.py b/vllm/v1/attention/backends/mla/common.py\nindex c98262e..4636a8b 100644\n--- a/vllm/v1/attention/backends/mla/common.py\n+++ b/vllm/v1/attention/backends/mla/common.py\n@@ -532,14 +532,13 @@ class MLACommonMetadataBuilder(Generic[M]):\n                 chunk_seq_lens = (chunk_ends - chunk_starts).clamp(min=0)\n                 _chunk_cu_seq_lens = chunk_seq_lens.cumsum(dim=1).to(\n                     torch.int32)\n-                zero = torch.zeros(num_chunks,\n-                                   dtype=torch.int32,\n-                                   device=device).unsqueeze(-1)\n+                cu_seq_lens = torch.empty((num_chunks, self._num_prefills + 1), dtype=torch.int32, device=device)\n+                cu_seq_lens[:, 0] = 0\n+                cu_seq_lens[:, 1:] = _chunk_cu_seq_lens\n \n                 chunked_context_metadata = \\\n                     MLACommonPrefillMetadata.ChunkedContextMetadata(\n-                    cu_seq_lens=torch.cat(\n-                        [zero, _chunk_cu_seq_lens], dim=1),\n+                    cu_seq_lens=cu_seq_lens,\n                     starts=chunk_starts,\n                     seq_tot=chunk_seq_lens.sum(dim=1).tolist(),\n                     max_seq_lens=chunk_seq_lens.max(dim=1).values.tolist(),\n@@ -861,6 +860,9 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n         iters = len(prefill_metadata.chunked_context.seq_tot)\n         workspace = prefill_metadata.chunked_context.workspace\n \n+\n+        tmp_output = None\n+        tmp_output_lse = None\n         for i in range(iters):\n             toks = prefill_metadata.chunked_context.seq_tot[i]\n \n@@ -888,9 +890,8 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n \n             # For MLA the v head dim is smaller than qk head dim so we pad\n             # out v with 0s to match the qk head dim\n-            v_padded = torch.nn.functional.pad(v,\n-                                               [0, q.shape[-1] - v.shape[-1]],\n-                                               value=0)\n+            pad = q.shape[-1] - v.shape[-1]\n+            v_padded = v if pad == 0 else torch.nn.functional.pad(v, [0, pad], value=0)\n \n             attn_output, attn_softmax_lse = self.flash_attn_varlen_func(\n                 q=q,\n@@ -909,18 +910,19 @@ class MLACommonImpl(MLAAttentionImpl[M], Generic[M]):\n                 output = attn_output\n                 output_lse = attn_softmax_lse\n             else:\n-                output_tmp = torch.empty_like(output)\n-                output_lse_tmp = torch.empty_like(output_lse)\n+                if tmp_output is None:\n+                    tmp_output = torch.empty_like(output)\n+                    tmp_output_lse = torch.empty_like(output_lse)\n                 merge_attn_states(\n-                    output=output_tmp,\n-                    output_lse=output_lse_tmp,\n+                    output=tmp_output,\n+                    output_lse=tmp_output_lse,\n                     prefix_output=output,\n                     prefix_lse=output_lse,\n                     suffix_output=attn_output,\n                     suffix_lse=attn_softmax_lse,\n                 )\n-                output = output_tmp\n-                output_lse = output_lse_tmp\n+                output, tmp_output = tmp_output, output\n+                output_lse, tmp_output_lse = tmp_output_lse, output_lse\n \n         return output, output_lse\n \n", "model_name_or_path": "gpt-5-2025-08-07"}
