diff --git a/csrc/moe/moe_permute_unpermute_op.cu b/csrc/moe/moe_permute_unpermute_op.cu
index 13aecd800..e827d9b11 100644
--- a/csrc/moe/moe_permute_unpermute_op.cu
+++ b/csrc/moe/moe_permute_unpermute_op.cu
@@ -49,7 +49,7 @@ void moe_permute(
   auto permuted_experts_id = torch::empty_like(topk_ids);
   auto dst_row_id2src_row_id_map = torch::empty_like(src_row_id2dst_row_id_map);
   auto align_expert_first_token_offset =
-      torch::zeros_like(expert_first_token_offset);
+      torch::empty_like(expert_first_token_offset);
 
   CubKeyValueSorter sorter{};
   int64_t* valid_num_ptr = nullptr;
diff --git a/vllm/model_executor/layers/fused_moe/cutlass_moe.py b/vllm/model_executor/layers/fused_moe/cutlass_moe.py
index ff49d7bb7..9f4b0cf12 100644
--- a/vllm/model_executor/layers/fused_moe/cutlass_moe.py
+++ b/vllm/model_executor/layers/fused_moe/cutlass_moe.py
@@ -136,18 +136,10 @@ def run_cutlass_moe_fp8(
                                      dtype=torch.int32,
                                      device=device)
 
-        # With expert_map each Rank processes only a subset of experts. As
-        # a result not all of a_map and c2 tensors are filled. We fill it
-        # zeros for correctness.
-        if expert_map is not None:
-            a_map = torch.zeros((local_topk_ids.numel()),
-                                dtype=torch.int32,
-                                device=device)
-        else:
-            a_map = torch.empty((local_topk_ids.numel()),
-                                dtype=torch.int32,
-                                device=device)
-
+        # Always allocate without initialization; fill only where needed.
+        a_map = torch.empty((local_topk_ids.numel()),
+                            dtype=torch.int32,
+                            device=device)
         c_map = torch.empty((local_topk_ids.numel()),
                             dtype=torch.int32,
                             device=device)
@@ -156,6 +148,12 @@ def run_cutlass_moe_fp8(
                                     problem_sizes1, problem_sizes2, a_map,
                                     c_map, global_num_experts, N, K)
 
+        # Zero only invalid entries to avoid reading garbage during shuffle.
+        if expert_map is not None:
+            invalid_mask = (local_topk_ids.reshape(-1) < 0)
+            if invalid_mask.any().item():
+                a_map.masked_fill_(invalid_mask, 0)
+
         a1q = ops.shuffle_rows(a1q, a_map)
         a1q_scale = (ops.shuffle_rows(a1q_scale, a_map)
                      if per_act_token else a1q_scale)
@@ -186,7 +184,10 @@ def run_cutlass_moe_fp8(
         c2, a2_scale, use_per_token_if_dynamic=per_act_token)
 
     if expert_map is not None:
-        c3.fill_(0)
+        # Zero only rows corresponding to invalid tokens to avoid stale data.
+        invalid_rows = (local_topk_ids.reshape(-1) < 0).nonzero(as_tuple=False).reshape(-1).to(dtype=torch.int64, device=c3.device)
+        if invalid_rows.numel() > 0:
+            c3.index_fill_(0, invalid_rows, 0)
 
     ops.cutlass_moe_mm(c3, a2q, w2, a2q_scale, w2_scale, expert_offsets,
                        problem_sizes2, ab_strides2, ab_strides2, c_strides2,
diff --git a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
index 1a31410c3..2c68ccd3e 100644
--- a/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
+++ b/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors_moe.py
@@ -203,7 +203,7 @@ class CompressedTensorsW4A4MoeMethod(CompressedTensorsMoEMethod):
         round_up_multiple = lambda x, m: (x + m - 1) // m * m
         M_padded = round_up_multiple(M, 128)
         K_padded = round_up_multiple(K, 4)
-        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype)
+        padded_scale = torch.zeros((B, M_padded, K_padded), dtype=scale.dtype, device=scale.device)
         padded_scale[:B, :M, :K] = scale
         batches, rows, cols = padded_scale.shape
         assert rows % 128 == 0
