diff --git a/tests/tokenization/test_detokenize.py b/tests/tokenization/test_detokenize.py
index 442173939..62440d001 100644
--- a/tests/tokenization/test_detokenize.py
+++ b/tests/tokenization/test_detokenize.py
@@ -41,7 +41,7 @@ def _run_incremental_decode(tokenizer, all_input_ids,
         if prev_tokens is None:
             prev_tokens = new_tokens
         else:
-            prev_tokens += new_tokens
+            prev_tokens.extend(new_tokens)
     return decoded_text
 
 
diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py
index 724782841..d0eab1cc4 100644
--- a/vllm/engine/llm_engine.py
+++ b/vllm/engine/llm_engine.py
@@ -718,12 +718,18 @@ class LLMEngine:
                          all_input_ids: List[int]) -> None:
         if not logprobs:
             return
+        tokenizer = self.get_tokenizer_for_seq(seq)
+        special_ids = tokenizer.all_special_ids
         for token_id, sample_logprob in logprobs.items():
             if (sample_logprob.decoded_token is None and token_id != -1):
+                # Fast-path skip if this is a special token and we are skipping them.
+                if prms.skip_special_tokens and token_id in special_ids:
+                    sample_logprob.decoded_token = ""
+                    continue
                 all_input_ids_with_logprob = all_input_ids[:-1] + [token_id]
                 (_, new_text, prefix_offset,
                  read_offset) = detokenize_incrementally(
-                     self.get_tokenizer_for_seq(seq),
+                     tokenizer,
                      all_input_ids=all_input_ids_with_logprob,
                      prev_tokens=seq.tokens,
                      prefix_offset=seq.prefix_offset,
diff --git a/vllm/transformers_utils/tokenizer.py b/vllm/transformers_utils/tokenizer.py
index f7a1a19a8..04cb1f753 100644
--- a/vllm/transformers_utils/tokenizer.py
+++ b/vllm/transformers_utils/tokenizer.py
@@ -137,21 +137,24 @@ def _convert_tokens_to_string_with_added_encoders(
     # even when the loop body is very simple.
     sub_texts = []
     current_sub_text = []
-    all_special_tokens = set(tokenizer.all_special_tokens)
+    # tokenizer.all_special_tokens is already a set via get_cached_tokenizer.
+    all_special_tokens = tokenizer.all_special_tokens
+    added_vocab = tokenizer.get_added_vocab()
+    append_sub_text = sub_texts.append
+    convert_sub = tokenizer.convert_tokens_to_string
+
     for token in output_tokens:
         if skip_special_tokens and token in all_special_tokens:
             continue
-        if token in tokenizer.get_added_vocab():
+        if token in added_vocab:
             if current_sub_text:
-                sub_text = tokenizer.convert_tokens_to_string(current_sub_text)
-                sub_texts.append(sub_text)
+                sub_texts.append(convert_sub(current_sub_text))
                 current_sub_text = []
-            sub_texts.append(token)
+            append_sub_text(token)
         else:
             current_sub_text.append(token)
     if current_sub_text:
-        sub_text = tokenizer.convert_tokens_to_string(current_sub_text)
-        sub_texts.append(sub_text)
+        sub_texts.append(convert_sub(current_sub_text))
     if spaces_between_special_tokens:
         return " ".join(sub_texts)
     else:
@@ -171,6 +174,12 @@ def detokenize_incrementally(
     spaces_between_special_tokens: bool = True,
 ) -> Tuple[List[str], str, int, int]:
     new_token_id = all_input_ids[-1]
+    # Fast path when we already have previous tokens and the new token is a
+    # special token to be skipped: avoid any string conversion work.
+    if prev_tokens is not None and skip_special_tokens and (
+            new_token_id in tokenizer.all_special_ids):
+        return [], "", prefix_offset, read_offset
+
     # This is the first iteration for this sequence
     if prev_tokens is None:
         new_tokens = tokenizer.convert_ids_to_tokens(
@@ -195,10 +204,9 @@ def detokenize_incrementally(
     # the decode which decide to add a space or not depending on the
     # surrounding ids.
     if tokenizer.is_fast or not tokenizer.get_added_vocab():
-        prefix_text = tokenizer.convert_tokens_to_string(
-            output_tokens[prefix_offset:read_offset])
-        new_text = tokenizer.convert_tokens_to_string(
-            output_tokens[prefix_offset:])
+        convert = tokenizer.convert_tokens_to_string
+        prefix_text = convert(output_tokens[prefix_offset:read_offset])
+        new_text = convert(output_tokens[prefix_offset:])
     else:
         prefix_text = _convert_tokens_to_string_with_added_encoders(
             tokenizer,
