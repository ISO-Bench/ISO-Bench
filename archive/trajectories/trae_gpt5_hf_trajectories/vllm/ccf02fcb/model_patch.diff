diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
index 5b19e3f35..3519cbad9 100644
--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
@@ -467,16 +467,10 @@ class MambaMixer2(CustomOp):
 
             initial_states = None
 
-            if has_initial_states is not None and torch.any(
-                    has_initial_states):
-
-                # vectorized ssm_state zero init
-                batched_zero_init_func = torch.vmap(
-                    lambda idx: mamba_cache_params.ssm_state[idx].zero_())
-                batched_zero_init_func(
-                    mamba_cache_params.
-                    state_indices_tensor[~has_initial_states].unsqueeze(
-                        dim=-1), )
+            if has_initial_states is not None and any(has_initial_states):
+
+                for idx in mamba_cache_params.state_indices_tensor[~has_initial_states]:
+                    mamba_cache_params.ssm_state[idx].zero_()
                 initial_states = mamba_cache_params.ssm_state[
                     mamba_cache_params.state_indices_tensor]
 
@@ -505,12 +499,9 @@ class MambaMixer2(CustomOp):
             # limitation which doesn't allow use of `item()`
             # Note: the lambda capture can happen where ssm_state is initialized
             #       instead of here
-            batched_copy = torch.vmap(
-                lambda idx, source_state: mamba_cache_params.ssm_state[
-                    idx].copy_(source_state))
-            batched_copy(
-                mamba_cache_params.state_indices_tensor.unsqueeze(dim=-1),
-                varlen_state)
+            for idx, source_state in zip(
+                mamba_cache_params.state_indices_tensor, varlen_state):
+                mamba_cache_params.ssm_state[idx].copy_(source_state)
 
             # - reshape
             hidden_states = scan_output.view(seq_len, -1)
@@ -518,7 +509,7 @@ class MambaMixer2(CustomOp):
 
             n_groups = self.n_groups // self.tp_size
             A = self.A[:, None, ...][:, :, None].expand(
-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)
+                -1, self.head_dim, self.ssm_state_size)
             dt = dt[:, :, None].expand(-1, -1, self.head_dim)
             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)
             D = self.D[:, None, ...].expand(-1, self.head_dim)
