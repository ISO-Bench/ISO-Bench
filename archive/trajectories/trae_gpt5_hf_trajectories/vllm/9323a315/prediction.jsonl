{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3257d44", "model_patch": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb..7782249 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -16,11 +16,13 @@ import os\n import sys\n from typing import List\n \n-import requests\n+\n from sphinx.ext import autodoc\n \n logger = logging.getLogger(__name__)\n-sys.path.append(os.path.abspath(\"../..\"))\n+base_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\"))\n+if base_path not in sys.path:\n+    sys.path.append(base_path)\n \n # -- Project information -----------------------------------------------------\n \n@@ -106,6 +108,7 @@ def get_repo_base_and_branch(pr_number):\n     if _cached_base and _cached_branch:\n         return _cached_base, _cached_branch\n \n+    import requests\n     url = f\"https://api.github.com/repos/vllm-project/vllm/pulls/{pr_number}\"\n     response = requests.get(url)\n     if response.status_code == 200:\n@@ -185,8 +188,9 @@ autodoc_mock_imports = [\n     \"decord\",\n ]\n \n+_modules = sys.modules\n for mock_target in autodoc_mock_imports:\n-    if mock_target in sys.modules:\n+    if mock_target in _modules:\n         logger.info(\n             \"Potentially problematic mock target (%s) found; \"\n             \"autodoc_mock_imports cannot mock modules that have already \"\ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eeb..76e1e81 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -28,9 +28,9 @@ if TYPE_CHECKING:\n \n logger = init_logger(__name__)\n \n-ALLOWED_DETAILED_TRACE_MODULES = [\"model\", \"worker\", \"all\"]\n+ALLOWED_DETAILED_TRACE_MODULES = (\"model\", \"worker\", \"all\")\n \n-DEVICE_OPTIONS = [\n+DEVICE_OPTIONS = (\n     \"auto\",\n     \"cuda\",\n     \"neuron\",\n@@ -39,8 +39,9 @@ DEVICE_OPTIONS = [\n     \"tpu\",\n     \"xpu\",\n     \"hpu\",\n-]\n+)\n \n+TASK_CHOICES = get_args(TaskOption)\n \n def nullable_str(val: str):\n     if not val or val == \"None\":\n@@ -63,16 +64,18 @@ def nullable_kvs(val: str) -> Optional[Mapping[str, int]]:\n \n     out_dict: Dict[str, int] = {}\n     for item in val.split(\",\"):\n-        kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n-        if len(kv_parts) != 2:\n+        # Split once; avoid unnecessary lower() on numeric values\n+        if \"=\" not in item:\n             raise argparse.ArgumentTypeError(\n                 \"Each item should be in the form KEY=VALUE\")\n-        key, value = kv_parts\n+        key_part, value_part = item.split(\"=\", 1)\n+        key = key_part.lower().strip()\n+        value_str = value_part.strip()\n \n         try:\n-            parsed_value = int(value)\n+            parsed_value = int(value_str)\n         except ValueError as exc:\n-            msg = f\"Failed to parse value of item {key}={value}\"\n+            msg = f\"Failed to parse value of item {key}={value_str}\"\n             raise argparse.ArgumentTypeError(msg) from exc\n \n         if key in out_dict and out_dict[key] != parsed_value:\n@@ -233,7 +236,7 @@ class EngineArgs:\n         parser.add_argument(\n             '--task',\n             default=EngineArgs.task,\n-            choices=get_args(TaskOption),\n+            choices=TASK_CHOICES,\n             help='The task to use the model for. Each vLLM instance only '\n             'supports one task, even if the same model can be used for '\n             'multiple tasks. When the model only supports one task, \"auto\" '\ndiff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b6742..a9889d1 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -4,21 +4,29 @@ from vllm.logits_process import LogitsProcessor\n from vllm.sampling_params import GuidedDecodingParams\n \n \n+_LMFE_LOCAL_FN = None\n+_OUTLINES_GETTER = None\n+_OUTLINES_LOCAL_GETTER = None\n+\n async def get_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n-        # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n-        from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n-            get_outlines_guided_decoding_logits_processor)\n-        return await get_outlines_guided_decoding_logits_processor(\n-            guided_params, tokenizer)\n+    if guided_params.grammar or guided_params.backend == 'outlines':\n+        global _OUTLINES_GETTER\n+        if _OUTLINES_GETTER is None:\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_outlines_guided_decoding_logits_processor as _fn)\n+            _OUTLINES_GETTER = _fn\n+        return await _OUTLINES_GETTER(guided_params, tokenizer)\n     if guided_params.backend == 'lm-format-enforcer':\n+        global _LMFE_LOCAL_FN\n+        if _LMFE_LOCAL_FN is not None:\n+            return _LMFE_LOCAL_FN(guided_params, tokenizer)\n         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n-        return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n-            guided_params, tokenizer)\n+        _LMFE_LOCAL_FN = get_local_lm_format_enforcer_guided_decoding_logits_processor\n+        return _LMFE_LOCAL_FN(guided_params, tokenizer)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n@@ -29,17 +37,22 @@ def get_local_guided_decoding_logits_processor(\n         guided_params: GuidedDecodingParams,\n         tokenizer) -> Optional[LogitsProcessor]:\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n-        # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n-        from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n-            get_local_outlines_guided_decoding_logits_processor)\n-        return get_local_outlines_guided_decoding_logits_processor(\n+    if guided_params.grammar or guided_params.backend == 'outlines':\n+        global _OUTLINES_LOCAL_GETTER\n+        if _OUTLINES_LOCAL_GETTER is None:\n+            from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n+                get_local_outlines_guided_decoding_logits_processor as _fn)\n+            _OUTLINES_LOCAL_GETTER = _fn\n+        return _OUTLINES_LOCAL_GETTER(\n             guided_params, tokenizer)\n     if guided_params.backend == 'lm-format-enforcer':\n+        global _LMFE_LOCAL_FN\n+        if _LMFE_LOCAL_FN is not None:\n+            return _LMFE_LOCAL_FN(guided_params, tokenizer)\n         from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n-        return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n-            guided_params, tokenizer)\n+        _LMFE_LOCAL_FN = get_local_lm_format_enforcer_guided_decoding_logits_processor\n+        return _LMFE_LOCAL_FN(guided_params, tokenizer)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n", "model_name_or_path": "gpt-5-2025-08-07"}
