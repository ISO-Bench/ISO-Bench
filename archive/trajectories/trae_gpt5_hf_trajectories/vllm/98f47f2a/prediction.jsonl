{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-8c1e77f", "model_patch": "diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 5f8535e..c127ff8 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -36,7 +36,7 @@ class FlashAttentionBackend(AttentionBackend):\n         num_kv_heads: int,\n         head_size: int,\n     ) -> Tuple[int, ...]:\n-        if block_size % 16 != 0:\n+        if block_size & 15 != 0:\n             raise ValueError(\"Block size must be a multiple of 16.\")\n         return (2, num_blocks, block_size, num_kv_heads, head_size)\n \n@@ -82,7 +82,7 @@ class FlashAttentionImpl(AttentionImpl):\n         self.scale = float(scale)\n         self.num_kv_heads = num_kv_heads\n         if alibi_slopes is not None:\n-            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n+            alibi_slopes = torch.as_tensor(alibi_slopes, dtype=torch.float32)\n         self.alibi_slopes = alibi_slopes\n         if sliding_window is None:\n             self.sliding_window = (-1, -1)\n@@ -135,24 +135,31 @@ class FlashAttentionImpl(AttentionImpl):\n         assert k_scale == 1.0 and v_scale == 1.0, (\n             \"key/v_scale is not supported in FlashAttention.\")\n \n-        output = torch.empty_like(query)\n-        torch.ops.vllm.unified_v1_flash_attention(\n-            output,\n-            query,\n-            key,\n-            value,\n-            self.num_heads,\n-            self.head_size,\n-            self.num_kv_heads,\n-            kv_cache,\n-            self.kv_cache_dtype,\n-            k_scale,\n-            v_scale,\n-            self.scale,\n-            self.sliding_window,\n-            self.alibi_slopes,\n-            self.logits_soft_cap,\n-        )\n+        with torch.inference_mode():\n+            # Move alibi slopes to the same device as query once and cache.\n+            alibi_slopes = self.alibi_slopes\n+            if alibi_slopes is not None and alibi_slopes.device != query.device:\n+                alibi_slopes = alibi_slopes.to(device=query.device)\n+                self.alibi_slopes = alibi_slopes\n+\n+            output = torch.empty_like(query)\n+            torch.ops.vllm.unified_v1_flash_attention(\n+                output,\n+                query,\n+                key,\n+                value,\n+                self.num_heads,\n+                self.head_size,\n+                self.num_kv_heads,\n+                kv_cache,\n+                self.kv_cache_dtype,\n+                k_scale,\n+                v_scale,\n+                self.scale,\n+                self.sliding_window,\n+                alibi_slopes,\n+                self.logits_soft_cap,\n+            )\n         return output\n \n \n@@ -173,54 +180,57 @@ def unified_v1_flash_attention(\n     alibi_slopes: Optional[torch.Tensor] = None,\n     logits_soft_cap: Optional[float] = None,\n ) -> None:\n-    context = get_forward_context()\n-    current_metadata = context.dynamic_forward_context\n-    if current_metadata is None:\n-        # Profiling run.\n-        return\n-\n-    assert current_metadata is not None\n-    assert isinstance(current_metadata, FlashAttentionMetadata)\n-    attn_metadata: FlashAttentionMetadata = current_metadata\n-    num_actual_tokens = attn_metadata.num_actual_tokens\n-\n-    # Reshape the query, key, and value tensors.\n-    query = query.view(-1, num_heads, head_size)\n-    key = key.view(-1, num_kv_heads, head_size)\n-    value = value.view(-1, num_kv_heads, head_size)\n-\n-    # Reshape the input keys and values and store them in the cache.\n-    key_cache = kv_cache[0]\n-    value_cache = kv_cache[1]\n-    torch.ops._C_cache_ops.reshape_and_cache_flash(\n-        key[:num_actual_tokens],\n-        value[:num_actual_tokens],\n-        key_cache,\n-        value_cache,\n-        attn_metadata.slot_mapping,\n-        kv_cache_dtype,\n-        k_scale,\n-        v_scale,\n-    )\n-\n-    attn_output = flash_attn_varlen_func(\n-        q=query[:num_actual_tokens],\n-        k=key_cache,\n-        v=value_cache,\n-        cu_seqlens_q=attn_metadata.query_start_loc,\n-        max_seqlen_q=attn_metadata.max_query_len,\n-        cu_seqlens_k=attn_metadata.seq_start_loc,\n-        max_seqlen_k=attn_metadata.max_seq_len,\n-        softmax_scale=softmax_scale,\n-        causal=True,\n-        alibi_slopes=alibi_slopes,\n-        window_size=window_size,\n-        block_table=attn_metadata.block_table,\n-        softcap=logits_soft_cap,\n-    )\n-    attn_output = attn_output.view(num_actual_tokens, -1)\n-    # TODO(woosuk): Optimize this.\n-    output[:num_actual_tokens].copy_(attn_output)\n+    with torch.inference_mode():\n+        context = get_forward_context()\n+        current_metadata = context.dynamic_forward_context\n+        if current_metadata is None:\n+            # Profiling run.\n+            return\n+\n+        assert current_metadata is not None\n+        assert isinstance(current_metadata, FlashAttentionMetadata)\n+        attn_metadata: FlashAttentionMetadata = current_metadata\n+        num_actual_tokens = attn_metadata.num_actual_tokens\n+\n+        # Reshape the query, key, and value tensors.\n+        query = query.view(-1, num_heads, head_size)\n+        key = key.view(-1, num_kv_heads, head_size)\n+        value = value.view(-1, num_kv_heads, head_size)\n+\n+        # Reshape the input keys and values and store them in the cache.\n+        key_cache = kv_cache[0]\n+        value_cache = kv_cache[1]\n+        k_in = key[:num_actual_tokens]\n+        v_in = value[:num_actual_tokens]\n+        torch.ops._C_cache_ops.reshape_and_cache_flash(\n+            k_in,\n+            v_in,\n+            key_cache,\n+            value_cache,\n+            attn_metadata.slot_mapping,\n+            kv_cache_dtype,\n+            k_scale,\n+            v_scale,\n+        )\n+\n+        attn_output = flash_attn_varlen_func(\n+            q=query[:num_actual_tokens],\n+            k=key_cache,\n+            v=value_cache,\n+            cu_seqlens_q=attn_metadata.query_start_loc,\n+            max_seqlen_q=attn_metadata.max_query_len,\n+            cu_seqlens_k=attn_metadata.seq_start_loc,\n+            max_seqlen_k=attn_metadata.max_seq_len,\n+            softmax_scale=softmax_scale,\n+            causal=True,\n+            alibi_slopes=alibi_slopes,\n+            window_size=window_size,\n+            block_table=attn_metadata.block_table,\n+            softcap=logits_soft_cap,\n+        )\n+        out_slice = output.narrow(0, 0, num_actual_tokens)\n+        # Write directly to the preallocated output slice to avoid an extra tensor.\n+        out_slice.copy_(attn_output.view_as(out_slice), non_blocking=True)\n \n \n def unified_v1_flash_attention_fake(\n", "model_name_or_path": "gpt-5-2025-08-07"}
