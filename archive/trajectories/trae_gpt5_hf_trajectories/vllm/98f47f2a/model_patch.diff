diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index 5f8535e..c127ff8 100644
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -36,7 +36,7 @@ class FlashAttentionBackend(AttentionBackend):
         num_kv_heads: int,
         head_size: int,
     ) -> Tuple[int, ...]:
-        if block_size % 16 != 0:
+        if block_size & 15 != 0:
             raise ValueError("Block size must be a multiple of 16.")
         return (2, num_blocks, block_size, num_kv_heads, head_size)
 
@@ -82,7 +82,7 @@ class FlashAttentionImpl(AttentionImpl):
         self.scale = float(scale)
         self.num_kv_heads = num_kv_heads
         if alibi_slopes is not None:
-            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
+            alibi_slopes = torch.as_tensor(alibi_slopes, dtype=torch.float32)
         self.alibi_slopes = alibi_slopes
         if sliding_window is None:
             self.sliding_window = (-1, -1)
@@ -135,24 +135,31 @@ class FlashAttentionImpl(AttentionImpl):
         assert k_scale == 1.0 and v_scale == 1.0, (
             "key/v_scale is not supported in FlashAttention.")
 
-        output = torch.empty_like(query)
-        torch.ops.vllm.unified_v1_flash_attention(
-            output,
-            query,
-            key,
-            value,
-            self.num_heads,
-            self.head_size,
-            self.num_kv_heads,
-            kv_cache,
-            self.kv_cache_dtype,
-            k_scale,
-            v_scale,
-            self.scale,
-            self.sliding_window,
-            self.alibi_slopes,
-            self.logits_soft_cap,
-        )
+        with torch.inference_mode():
+            # Move alibi slopes to the same device as query once and cache.
+            alibi_slopes = self.alibi_slopes
+            if alibi_slopes is not None and alibi_slopes.device != query.device:
+                alibi_slopes = alibi_slopes.to(device=query.device)
+                self.alibi_slopes = alibi_slopes
+
+            output = torch.empty_like(query)
+            torch.ops.vllm.unified_v1_flash_attention(
+                output,
+                query,
+                key,
+                value,
+                self.num_heads,
+                self.head_size,
+                self.num_kv_heads,
+                kv_cache,
+                self.kv_cache_dtype,
+                k_scale,
+                v_scale,
+                self.scale,
+                self.sliding_window,
+                alibi_slopes,
+                self.logits_soft_cap,
+            )
         return output
 
 
@@ -173,54 +180,57 @@ def unified_v1_flash_attention(
     alibi_slopes: Optional[torch.Tensor] = None,
     logits_soft_cap: Optional[float] = None,
 ) -> None:
-    context = get_forward_context()
-    current_metadata = context.dynamic_forward_context
-    if current_metadata is None:
-        # Profiling run.
-        return
-
-    assert current_metadata is not None
-    assert isinstance(current_metadata, FlashAttentionMetadata)
-    attn_metadata: FlashAttentionMetadata = current_metadata
-    num_actual_tokens = attn_metadata.num_actual_tokens
-
-    # Reshape the query, key, and value tensors.
-    query = query.view(-1, num_heads, head_size)
-    key = key.view(-1, num_kv_heads, head_size)
-    value = value.view(-1, num_kv_heads, head_size)
-
-    # Reshape the input keys and values and store them in the cache.
-    key_cache = kv_cache[0]
-    value_cache = kv_cache[1]
-    torch.ops._C_cache_ops.reshape_and_cache_flash(
-        key[:num_actual_tokens],
-        value[:num_actual_tokens],
-        key_cache,
-        value_cache,
-        attn_metadata.slot_mapping,
-        kv_cache_dtype,
-        k_scale,
-        v_scale,
-    )
-
-    attn_output = flash_attn_varlen_func(
-        q=query[:num_actual_tokens],
-        k=key_cache,
-        v=value_cache,
-        cu_seqlens_q=attn_metadata.query_start_loc,
-        max_seqlen_q=attn_metadata.max_query_len,
-        cu_seqlens_k=attn_metadata.seq_start_loc,
-        max_seqlen_k=attn_metadata.max_seq_len,
-        softmax_scale=softmax_scale,
-        causal=True,
-        alibi_slopes=alibi_slopes,
-        window_size=window_size,
-        block_table=attn_metadata.block_table,
-        softcap=logits_soft_cap,
-    )
-    attn_output = attn_output.view(num_actual_tokens, -1)
-    # TODO(woosuk): Optimize this.
-    output[:num_actual_tokens].copy_(attn_output)
+    with torch.inference_mode():
+        context = get_forward_context()
+        current_metadata = context.dynamic_forward_context
+        if current_metadata is None:
+            # Profiling run.
+            return
+
+        assert current_metadata is not None
+        assert isinstance(current_metadata, FlashAttentionMetadata)
+        attn_metadata: FlashAttentionMetadata = current_metadata
+        num_actual_tokens = attn_metadata.num_actual_tokens
+
+        # Reshape the query, key, and value tensors.
+        query = query.view(-1, num_heads, head_size)
+        key = key.view(-1, num_kv_heads, head_size)
+        value = value.view(-1, num_kv_heads, head_size)
+
+        # Reshape the input keys and values and store them in the cache.
+        key_cache = kv_cache[0]
+        value_cache = kv_cache[1]
+        k_in = key[:num_actual_tokens]
+        v_in = value[:num_actual_tokens]
+        torch.ops._C_cache_ops.reshape_and_cache_flash(
+            k_in,
+            v_in,
+            key_cache,
+            value_cache,
+            attn_metadata.slot_mapping,
+            kv_cache_dtype,
+            k_scale,
+            v_scale,
+        )
+
+        attn_output = flash_attn_varlen_func(
+            q=query[:num_actual_tokens],
+            k=key_cache,
+            v=value_cache,
+            cu_seqlens_q=attn_metadata.query_start_loc,
+            max_seqlen_q=attn_metadata.max_query_len,
+            cu_seqlens_k=attn_metadata.seq_start_loc,
+            max_seqlen_k=attn_metadata.max_seq_len,
+            softmax_scale=softmax_scale,
+            causal=True,
+            alibi_slopes=alibi_slopes,
+            window_size=window_size,
+            block_table=attn_metadata.block_table,
+            softcap=logits_soft_cap,
+        )
+        out_slice = output.narrow(0, 0, num_actual_tokens)
+        # Write directly to the preallocated output slice to avoid an extra tensor.
+        out_slice.copy_(attn_output.view_as(out_slice), non_blocking=True)
 
 
 def unified_v1_flash_attention_fake(
