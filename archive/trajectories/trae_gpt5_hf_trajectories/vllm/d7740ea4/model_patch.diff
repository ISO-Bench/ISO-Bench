diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 000000000..e69de29bb
diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py
index 1f19d2053..da0e29c57 100644
--- a/vllm/model_executor/layers/sampler.py
+++ b/vllm/model_executor/layers/sampler.py
@@ -320,7 +320,7 @@ def _random_sample(
         seq_group has do_sample=False, tuple contains ([], [])
     """
     # Find the maximum best_of value of the prompt phase requests.
-    random_samples = random_samples.cpu()
+    random_samples = random_samples.to('cpu')
     sample_idx = 0
     results: SampleResultType = []
     for seq_group in selected_seq_groups:
@@ -721,7 +721,7 @@ def _get_logprobs(
     next_token_ids: List[int] = []
     # The largest requested number of logprobs. We find logprobs as many as the
     # largest num logprobs in this API.
-    largest_num_logprobs = 1
+    largest_num_logprobs = 0
 
     # Select indices to compute logprob from, ranks of token ids, and the top
     # k token ids from logprobs.
@@ -730,8 +730,7 @@ def _get_logprobs(
         sampling_params = seq_group.sampling_params
 
         # Update indices and tokens for prompt logprobs.
-        if (seq_group.is_prompt
-                and sampling_params.prompt_logprobs is not None):
+        if seq_group.is_prompt and sampling_params.prompt_logprobs is not None:
             largest_num_logprobs = max(largest_num_logprobs,
                                        sampling_params.prompt_logprobs)
             next_prompt_tokens = _get_next_prompt_tokens(seq_group)
@@ -763,15 +762,12 @@ def _get_logprobs(
 
     query_indices_gpu = torch.tensor(query_indices, device=logprobs.device)
     next_token_ids_gpu = torch.tensor(next_token_ids, device=logprobs.device)
+    logprobs_selected = logprobs.index_select(0, query_indices_gpu)
 
-    # (num_selected_query_tokens, num_logprobs). Note that query_indices can
-    # contain duplicates if beam search is enabled.
-    selected_logprobs = logprobs[[
-        query_indices_gpu,
-        next_token_ids_gpu,
-    ]]
+    # (num_selected_query_tokens,)
+    selected_logprobs = logprobs_selected.gather(1, next_token_ids_gpu.unsqueeze(1)).squeeze(1)
     ranks = _get_ranks(
-        logprobs[query_indices_gpu],
+        logprobs_selected,
         next_token_ids_gpu,
     )
     assert selected_logprobs.shape[0] == ranks.shape[0]
@@ -779,16 +775,17 @@ def _get_logprobs(
     # Logprobs of topk tokens for a batch of sequence groups.
     # (num_query_tokens_across_batch).
     if largest_num_logprobs > 0:
-        top_logprobs, top_token_ids = torch.topk(logprobs,
+        top_logprobs, top_token_ids = torch.topk(logprobs_selected,
                                                  largest_num_logprobs,
                                                  dim=-1)
-        top_logprobs = top_logprobs.cpu()
-        top_token_ids = top_token_ids.cpu()
     else:
         top_logprobs, top_token_ids = None, None
 
-    selected_logprobs = selected_logprobs.cpu()
-    ranks = ranks.cpu()
+    selected_logprobs = selected_logprobs.to('cpu')
+    ranks = ranks.to('cpu')
+    if top_logprobs is not None and top_token_ids is not None:
+        top_logprobs = top_logprobs.to('cpu')
+        top_token_ids = top_token_ids.to('cpu')
 
     # Find prompt/sample logprobs.
     prompt_logprobs_per_seq_group: List[Optional[PromptLogprobs]] = []
@@ -828,7 +825,7 @@ def _get_prompt_logprob_if_needed(
 
     # Find prompt logprobs
     prompt_logprobs: Optional[PromptLogprobs] = None
-    if (is_prompt and sampling_params.prompt_logprobs is not None):
+    if is_prompt and sampling_params.prompt_logprobs is not None:
         prompt_logprobs = []
         num_logprobs = sampling_params.prompt_logprobs
         next_prompt_tokens = _get_next_prompt_tokens(seq_group)
