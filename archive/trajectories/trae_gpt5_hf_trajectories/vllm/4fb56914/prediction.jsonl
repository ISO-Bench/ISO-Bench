{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-0df4d9b", "model_patch": "diff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu\nindex 0725a22..e07b9fb 100644\n--- a/csrc/quantization/fp8/common.cu\n+++ b/csrc/quantization/fp8/common.cu\n@@ -91,8 +91,7 @@ void static_scaled_fp8_quant(torch::Tensor& out,          // [..., d]\n   int const block_size = 256;\n   int const num_tokens = input.numel() / input.size(-1);\n   int const num_elems = input.numel();\n-  dim3 const grid(num_tokens);\n-  int const blocks = (num_elems + block_size - 1) / block_size;\n+  dim3 const grid(blocks);\n   dim3 const block(block_size);\n   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\ndiff --git a/model_patch.diff b/model_patch.diff\nindex 454c849..e69de29 100644\n--- a/model_patch.diff\n+++ b/model_patch.diff\n@@ -1,58 +0,0 @@\n-diff --git a/csrc/layernorm_kernels.cu b/csrc/layernorm_kernels.cu\n-index d073dd6..c55c601 100644\n---- a/csrc/layernorm_kernels.cu\n-+++ b/csrc/layernorm_kernels.cu\n-@@ -148,7 +148,8 @@ void rms_norm(torch::Tensor& out,     // [..., hidden_size]\n-   int num_tokens = input.numel() / hidden_size;\n- \n-   dim3 grid(num_tokens);\n--  dim3 block(std::min(hidden_size, 1024));\n-+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n-+  dim3 block(std::min(hidden_size, max_block_size));\n-   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n-   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-   VLLM_DISPATCH_FLOATING_TYPES(input.scalar_type(), \"rms_norm_kernel\", [&] {\n-diff --git a/csrc/layernorm_quant_kernels.cu b/csrc/layernorm_quant_kernels.cu\n-index d595b9e..6b5167b 100644\n---- a/csrc/layernorm_quant_kernels.cu\n-+++ b/csrc/layernorm_quant_kernels.cu\n-@@ -173,7 +173,8 @@ void rms_norm_static_fp8_quant(torch::Tensor& out,     // [..., hidden_size]\n-   int num_tokens = input.numel() / hidden_size;\n- \n-   dim3 grid(num_tokens);\n--  dim3 block(std::min(hidden_size, 1024));\n-+  const int max_block_size = (num_tokens < 256) ? 1024 : 256;\n-+  dim3 block(std::min(hidden_size, max_block_size));\n-   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n-   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-   VLLM_DISPATCH_FLOATING_TYPES(\n-diff --git a/csrc/quantization/fp8/common.cu b/csrc/quantization/fp8/common.cu\n-index f3f9f66..0725a22 100644\n---- a/csrc/quantization/fp8/common.cu\n-+++ b/csrc/quantization/fp8/common.cu\n-@@ -92,6 +92,7 @@ void static_scaled_fp8_quant(torch::Tensor& out,          // [..., d]\n-   int const num_tokens = input.numel() / input.size(-1);\n-   int const num_elems = input.numel();\n-   dim3 const grid(num_tokens);\n-+  int const blocks = (num_elems + block_size - 1) / block_size;\n-   dim3 const block(block_size);\n-   const at::cuda::OptionalCUDAGuard device_guard(device_of(input));\n-   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n-diff --git a/vllm/model_executor/models/deepseek_v2.py b/vllm/model_executor/models/deepseek_v2.py\n-index 5106b99..ba4b11c 100644\n---- a/vllm/model_executor/models/deepseek_v2.py\n-+++ b/vllm/model_executor/models/deepseek_v2.py\n-@@ -822,11 +822,11 @@ class DeepseekV2ForCausalLM(nn.Module, SupportsPP, MixtureOfExperts):\n-             device: torch.device) -> IntermediateTensors:\n-         return IntermediateTensors({\n-             \"hidden_states\":\n--            torch.zeros((batch_size, self.config.hidden_size),\n-+            torch.empty((batch_size, self.config.hidden_size),\n-                         dtype=dtype,\n-                         device=device),\n-             \"residual\":\n--            torch.zeros((batch_size, self.config.hidden_size),\n-+            torch.empty((batch_size, self.config.hidden_size),\n-                         dtype=dtype,\n-                         device=device),\n-         })\n", "model_name_or_path": "gpt-5-2025-08-07"}
