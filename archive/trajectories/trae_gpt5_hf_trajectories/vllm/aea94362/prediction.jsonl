{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-7206ce4", "model_patch": "diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e4134..8cf8b4a 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -2,6 +2,7 @@\n # https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/protocol/openai_api_protocol.py\n import re\n import time\n+from functools import lru_cache\n from argparse import Namespace\n from typing import Any, Dict, List, Literal, Optional, Union\n \n@@ -46,15 +47,17 @@ class OpenAIBaseModel(BaseModel):\n     @classmethod\n     def __log_extra_fields__(cls, data):\n         if isinstance(data, dict):\n-            # Get all class field names and their potential aliases\n-            field_names = set()\n-            for field_name, field in cls.model_fields.items():\n-                field_names.add(field_name)\n-                if hasattr(field, 'alias') and field.alias:\n-                    field_names.add(field.alias)\n-\n-            # Compare against both field names and aliases\n-            extra_fields = data.keys() - field_names\n+            # Cache field names and aliases per-model to avoid recomputation\n+            names = getattr(cls, \"_field_names_cache\", None)\n+            if names is None:\n+                names = set(cls.model_fields.keys())\n+                for field in cls.model_fields.values():\n+                    alias = getattr(field, 'alias', None)\n+                    if alias:\n+                        names.add(alias)\n+                setattr(cls, \"_field_names_cache\", names)\n+\n+            extra_fields = data.keys() - names\n             if extra_fields:\n                 logger.warning(\n                     \"The following fields were present in the request \"\n@@ -166,28 +169,37 @@ class LogitsProcessorConstructor(BaseModel):\n LogitsProcessors = List[Union[str, LogitsProcessorConstructor]]\n \n \n+\n+@lru_cache(maxsize=128)\n+def _get_compiled_pattern(pattern: str):\n+    return re.compile(pattern)\n+\n+\n def get_logits_processors(processors: Optional[LogitsProcessors],\n                           pattern: Optional[str]) -> Optional[List[Any]]:\n-    if processors and pattern:\n+    compiled = _get_compiled_pattern(pattern) if pattern else None\n+    if processors and compiled:\n         logits_processors = []\n+        append_lp = logits_processors.append\n+        resolve = resolve_obj_by_qualname\n         for processor in processors:\n-            qualname = processor if isinstance(processor,\n-                                               str) else processor.qualname\n-            if not re.match(pattern, qualname):\n+            qualname = processor if isinstance(processor, str) else processor.qualname\n+            if not compiled.match(qualname):\n                 raise ValueError(\n                     f\"Logits processor '{qualname}' is not allowed by this \"\n                     \"server. See --logits-processor-pattern engine argument \"\n                     \"for more information.\")\n             try:\n-                logits_processor = resolve_obj_by_qualname(qualname)\n+                logits_processor = resolve(qualname)\n             except Exception as e:\n                 raise ValueError(\n                     f\"Logits processor '{qualname}' could not be resolved: {e}\"\n                 ) from e\n             if isinstance(processor, LogitsProcessorConstructor):\n-                logits_processor = logits_processor(*processor.args or [],\n-                                                    **processor.kwargs or {})\n-            logits_processors.append(logits_processor)\n+                args = processor.args or []\n+                kwargs = processor.kwargs or {}\n+                logits_processor = logits_processor(*args, **kwargs)\n+            append_lp(logits_processor)\n         return logits_processors\n     elif processors:\n         raise ValueError(\ndiff --git a/vllm/envs.py b/vllm/envs.py\nindex 1e68326..690753c 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -480,9 +480,12 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n \n \n def __getattr__(name: str):\n-    # lazy evaluation of environment variables\n+    # lazy evaluation of environment variables with caching to avoid repeated env lookups\n     if name in environment_variables:\n-        return environment_variables[name]()\n+        value = environment_variables[name]()\n+        # Cache the computed value to speed up subsequent attribute access\n+        globals()[name] = value\n+        return value\n     raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n \n \ndiff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e44..5124779 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -208,7 +208,10 @@ class AsyncLLM(EngineClient):\n             while True:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                try:\n+                    out = q.get_nowait()\n+                except asyncio.QueueEmpty:\n+                    out = await q.get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\ndiff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f5..a8f8c87 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -11,7 +11,7 @@ from vllm.v1.engine.detokenizer import (DetokenizerOutput,\n from vllm.v1.metrics.stats import IterationStats\n \n \n-@dataclass\n+@dataclass(slots=True)\n class OutputProcessorOutput:\n \n     request_outputs: List[RequestOutput]\n@@ -21,6 +21,8 @@ class OutputProcessorOutput:\n \n class RequestState:\n \n+    __slots__ = (\"request_id\", \"prompt\", \"prompt_token_ids\", \"prompt_len\", \"detokenizer\", \"is_prefilling\", \"queue\")\n+\n     def __init__(\n         self,\n         request_id: str,\ndiff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 4545016..e397607 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -15,6 +15,14 @@ if TYPE_CHECKING:\n \n class Request:\n \n+    __slots__ = (\n+        \"request_id\", \"sampling_params\", \"eos_token_id\", \"metrics\",\n+        \"lora_request\", \"status\", \"stop_reason\", \"max_tokens\", \"prompt\",\n+        \"prompt_token_ids\", \"num_prompt_tokens\", \"_output_token_ids\",\n+        \"_all_token_ids\", \"num_computed_tokens\", \"mm_positions\",\n+        \"mm_inputs\", \"mm_hashes\", \"_kv_block_hashes\",\n+    )\n+\n     def __init__(\n         self,\n         request_id: str,\n", "model_name_or_path": "gpt-5-2025-08-07"}
