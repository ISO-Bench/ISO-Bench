diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py
index e0169f1..d31008a 100644
--- a/vllm/v1/engine/async_llm.py
+++ b/vllm/v1/engine/async_llm.py
@@ -7,7 +7,6 @@ from collections.abc import AsyncGenerator, Mapping
 from copy import copy
 from typing import Optional, Union
 
-import numpy as np
 
 import vllm.envs as envs
 from vllm.config import ModelConfig, VllmConfig
@@ -25,7 +24,7 @@ from vllm.sampling_params import RequestOutputKind, SamplingParams
 from vllm.transformers_utils.tokenizer import AnyTokenizer
 from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs
 from vllm.usage.usage_lib import UsageContext
-from vllm.utils import Device, cdiv, kill_process_tree
+from vllm.utils import Device, kill_process_tree
 from vllm.v1.engine import EngineCoreRequest
 from vllm.v1.engine.core_client import EngineCoreClient
 from vllm.v1.engine.output_processor import OutputProcessor
@@ -309,14 +308,16 @@ class AsyncLLM(EngineClient):
                 # Split outputs into chunks of at most
                 # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the
                 # event loop for too long.
-                if num_outputs <= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:
-                    slices = (outputs.outputs, )
+                chunk_size = VLLM_V1_OUTPUT_PROC_CHUNK_SIZE
+                if num_outputs <= chunk_size:
+                    num_slices = 1
                 else:
-                    slices = np.array_split(
-                        outputs.outputs,
-                        cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))
+                    num_slices = (num_outputs + chunk_size - 1) // chunk_size
 
-                for i, outputs_slice in enumerate(slices):
+                for i in range(num_slices):
+                    start = i * chunk_size
+                    end = num_outputs if i + 1 == num_slices else start + chunk_size
+                    outputs_slice = outputs.outputs[start:end]
                     # 2) Process EngineCoreOutputs.
                     processed_outputs = self.output_processor.process_outputs(
                         outputs_slice, outputs.timestamp, iteration_stats)
@@ -324,7 +325,7 @@ class AsyncLLM(EngineClient):
                     assert not processed_outputs.request_outputs
 
                     # Allow other asyncio tasks to run between chunks
-                    if i + 1 < len(slices):
+                    if i + 1 < num_slices:
                         await asyncio.sleep(0)
 
                     # 3) Abort any reqs that finished due to stop strings.
diff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py
index 12df341..18bb2f7 100644
--- a/vllm/v1/engine/output_processor.py
+++ b/vllm/v1/engine/output_processor.py
@@ -26,6 +26,24 @@ class OutputProcessorOutput:
 
 class RequestState:
 
+    __slots__ = (
+        "request_id",
+        "parent_req",
+        "request_index",
+        "lora_name",
+        "output_kind",
+        "prompt",
+        "prompt_token_ids",
+        "prompt_len",
+        "logprobs_processor",
+        "detokenizer",
+        "max_tokens_param",
+        "is_prefilling",
+        "queue",
+        "stats",
+    )
+
+
     def __init__(
         self,
         request_id: str,
@@ -163,7 +181,11 @@ class RequestState:
         # Prepare logprobs, based on delta mode
         logprobs = self.logprobs_processor.logprobs
         if delta and logprobs:
-            logprobs = logprobs[-len(token_ids):]
+            if token_ids:
+                logprobs = logprobs[-len(token_ids):]
+            else:
+                # Avoid copying the entire list when no new tokens
+                logprobs = []
 
         return CompletionOutput(
             index=self.request_index,
@@ -178,6 +200,14 @@ class RequestState:
 class OutputProcessor:
     """Process EngineCoreOutputs into RequestOutputs."""
 
+    __slots__ = (
+        "log_stats",
+        "tokenizer",
+        "request_states",
+        "parent_requests",
+        "lora_states",
+    )
+
     def __init__(
         self,
         tokenizer: BaseTokenizerGroup,
@@ -267,17 +297,25 @@ class OutputProcessor:
 
         request_outputs: list[RequestOutput] = []
         reqs_to_abort: list[str] = []
+
+        request_states = self.request_states
+        parent_requests = self.parent_requests
+        lora_states = self.lora_states
+        append_output = request_outputs.append
+        append_abort = reqs_to_abort.append
+        _update_out = self._update_stats_from_output
+        _update_fin = self._update_stats_from_finished
+
         for engine_core_output in engine_core_outputs:
             req_id = engine_core_output.request_id
-            req_state = self.request_states.get(req_id)
+            req_state = request_states.get(req_id)
             if req_state is None:
                 # Ignore output for already-aborted request.
                 continue
 
-            # 1) Compute stats for this iteration.
-            self._update_stats_from_output(req_state, engine_core_output,
-                                           engine_core_timestamp,
-                                           iteration_stats)
+            _update_out(req_state, engine_core_output,
+                         engine_core_timestamp,
+                         iteration_stats)
 
             new_token_ids = engine_core_output.new_token_ids
             finish_reason = engine_core_output.finish_reason
@@ -303,25 +341,24 @@ class OutputProcessor:
                     req_state.queue.put_nowait(request_output)
                 else:
                     # LLMEngine: return list of RequestOutputs.
-                    request_outputs.append(request_output)
+                    append_output(request_output)
 
             # Free completed requests.
             if finish_reason is not None:
-                self.request_states.pop(req_id)
+                request_states.pop(req_id)
                 # Remove parent request if applicable.
                 parent_req = req_state.parent_req
                 if parent_req and not parent_req.child_requests:
-                    self.parent_requests.pop(parent_req.request_id, None)
+                    parent_requests.pop(parent_req.request_id, None)
                 if not engine_core_output.finished:
                     # If req not finished in EngineCore, but Detokenizer
                     # detected stop string, abort needed in EngineCore.
-                    reqs_to_abort.append(req_id)
+                    append_abort(req_id)
 
                 # Track per-request stats
-                self._update_stats_from_finished(req_state, finish_reason,
-                                                 iteration_stats)
+                _update_fin(req_state, finish_reason, iteration_stats)
 
-        self.lora_states.update_iteration_stats(iteration_stats)
+        lora_states.update_iteration_stats(iteration_stats)
 
         return OutputProcessorOutput(
             request_outputs=request_outputs,
