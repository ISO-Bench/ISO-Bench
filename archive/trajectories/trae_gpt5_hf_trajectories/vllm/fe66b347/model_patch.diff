diff --git a/vllm/model_executor/layers/mamba/mamba_mixer2.py b/vllm/model_executor/layers/mamba/mamba_mixer2.py
index b53a540ed..b99dab49c 100644
--- a/vllm/model_executor/layers/mamba/mamba_mixer2.py
+++ b/vllm/model_executor/layers/mamba/mamba_mixer2.py
@@ -440,8 +440,6 @@ class MambaMixer2(CustomOp):
                 query_start_loc=attn_metadata.query_start_loc).transpose(
                     0, 1)[:seq_len]
 
-            # TODO: Why is this needed?
-            hidden_states_B_C = hidden_states_B_C.contiguous()
         else:
             hidden_states_B_C = causal_conv1d_update(
                 hidden_states_B_C,
@@ -466,10 +464,10 @@ class MambaMixer2(CustomOp):
         if has_prefill:
 
             initial_states = None
-            if has_initial_states is not None and any(has_initial_states):
-                for idx in mamba_cache_params.state_indices_tensor[
-                        ~has_initial_states]:
-                    mamba_cache_params.ssm_state[idx].zero_()
+            if has_initial_states is not None and torch.any(has_initial_states):
+                zero_idx = mamba_cache_params.state_indices_tensor[~has_initial_states]
+                if zero_idx.numel() > 0:
+                    mamba_cache_params.ssm_state[zero_idx] = 0
                 initial_states = mamba_cache_params.ssm_state[
                     mamba_cache_params.state_indices_tensor]
 
@@ -495,8 +493,7 @@ class MambaMixer2(CustomOp):
 
             # update ssm states
             # - varlen state is a (batch, nheads, headdim, dstate) tensor
-            for i, idx in enumerate(mamba_cache_params.state_indices_tensor):
-                mamba_cache_params.ssm_state[idx].copy_(varlen_state[i])
+            mamba_cache_params.ssm_state[mamba_cache_params.state_indices_tensor] = varlen_state
 
             # - reshape
             hidden_states = scan_output.view(seq_len, -1)
@@ -504,7 +501,7 @@ class MambaMixer2(CustomOp):
 
             n_groups = self.n_groups // self.tp_size
             A = self.A[:, None, ...][:, :, None].expand(
-                -1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)
+                -1, self.head_dim, self.ssm_state_size)
             dt = dt[:, :, None].expand(-1, -1, self.head_dim)
             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)
             D = self.D[:, None, ...].expand(-1, self.head_dim)
