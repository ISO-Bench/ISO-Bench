{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-005ae9b", "model_patch": "diff --git a/benchmarks/kernels/benchmark_moe_align_block_size.py b/benchmarks/kernels/benchmark_moe_align_block_size.py\nindex 5170ac0..968026d 100644\n--- a/benchmarks/kernels/benchmark_moe_align_block_size.py\n+++ b/benchmarks/kernels/benchmark_moe_align_block_size.py\n@@ -13,12 +13,7 @@ from vllm.triton_utils import triton\n \n \n def get_topk_ids(num_tokens: int, num_experts: int, topk: int) -> torch.Tensor:\n-    return torch.stack(\n-        [\n-            torch.randperm(num_experts, dtype=torch.int32, device=\"cuda\")[:topk]\n-            for _ in range(num_tokens)\n-        ]\n-    )\n+    return torch.randint(0, num_experts, (num_tokens, topk), dtype=torch.int32, device=\"cuda\")\n \n \n def check_correctness(num_tokens, num_experts=256, block_size=256, topk=8):\ndiff --git a/csrc/moe/moe_align_sum_kernels.cu b/csrc/moe/moe_align_sum_kernels.cu\nindex 462dbd1..63c31ea 100644\n--- a/csrc/moe/moe_align_sum_kernels.cu\n+++ b/csrc/moe/moe_align_sum_kernels.cu\n@@ -185,7 +185,7 @@ void moe_align_block_size(torch::Tensor topk_ids, int64_t num_experts,\n         auto options_int =\n             torch::TensorOptions().dtype(torch::kInt).device(topk_ids.device());\n         torch::Tensor cumsum_buffer =\n-            torch::zeros({num_experts + 1}, options_int);\n+            torch::empty({num_experts + 1}, options_int);\n         bool small_batch_expert_mode =\n             (topk_ids.numel() < 1024) && (num_experts <= 64);\n \ndiff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\nindex 3aae183..ff8048f 100644\n--- a/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n+++ b/vllm/model_executor/layers/fused_moe/moe_align_block_size.py\n@@ -107,9 +107,10 @@ def moe_align_block_size_triton(\n     tokens_cnts = torch.zeros((num_experts + 1, num_experts),\n                               dtype=torch.int32,\n                               device=topk_ids.device)\n-    cumsum = torch.zeros((num_experts + 1, ),\n+    cumsum = torch.empty((num_experts + 1, ),\n                          dtype=torch.int32,\n                          device=topk_ids.device)\n+    cumsum[0] = 0\n     tokens_per_thread = cdiv(numel, num_experts)\n \n     moe_align_block_size_stage1[grid](\n@@ -202,10 +203,10 @@ def moe_align_block_size(\n     max_num_tokens_padded = topk_ids.numel() + num_experts * (block_size - 1)\n     if pad_sorted_ids:\n         max_num_tokens_padded = round_up(max_num_tokens_padded, block_size)\n-    sorted_ids = torch.empty((max_num_tokens_padded, ),\n+    sorted_ids = torch.full((max_num_tokens_padded, ),\n+                             topk_ids.numel(),\n                              dtype=torch.int32,\n                              device=topk_ids.device)\n-    sorted_ids.fill_(topk_ids.numel())\n     max_num_m_blocks = triton.cdiv(max_num_tokens_padded, block_size)\n     # Expert ids must be zeroed out to prevent index out of bounds error while\n     # mapping global expert ids to local expert ids in expert parallelism.\n", "model_name_or_path": "gpt-5-2025-08-07"}
