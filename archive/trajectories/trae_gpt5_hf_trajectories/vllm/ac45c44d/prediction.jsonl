{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-d666466", "model_patch": "diff --git a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\nindex 7016ff3..9cce02e 100644\n--- a/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n+++ b/vllm/model_executor/layers/fused_moe/deepep_ht_prepare_finalize.py\n@@ -33,6 +33,12 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # From https://github.com/deepseek-ai/DeepEP/blob/9fe9021f29c9083cd1808ab36b740208524d9f63/deep_ep/buffer.py#L164\n         self.available_rank_configs = [2, 4, 8, 16, 24, 32, 64, 128, 144, 160]\n \n+\n+        # Cache frequently used configs/objects to reduce per-call overhead.\n+        self._dispatch_config: Optional[deep_ep.Config] = None\n+        self._combine_config: Optional[deep_ep.Config] = None\n+        self._contiguous_reducer = TopKWeightAndReduceContiguous()\n+\n     def num_dispatchers(self) -> int:\n         return self.num_dispatchers_\n \n@@ -49,12 +55,16 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n     def _get_dispatch_config(self) -> Optional[deep_ep.Config]:\n         if self.dp_size not in self.available_rank_configs:\n             return None\n-        return deep_ep.Buffer.get_dispatch_config(self.dp_size)\n+        if self._dispatch_config is None:\n+            self._dispatch_config = deep_ep.Buffer.get_dispatch_config(self.dp_size)\n+        return self._dispatch_config\n \n     def _get_combine_config(self) -> Optional[deep_ep.Config]:\n         if self.dp_size not in self.available_rank_configs:\n             return None\n-        return deep_ep.Buffer.get_combine_config(self.dp_size)\n+        if self._combine_config is None:\n+            self._combine_config = deep_ep.Buffer.get_combine_config(self.dp_size)\n+        return self._combine_config\n \n     def _do_dispatch(self, tokens: torch.Tensor,\n                      token_scales: Optional[torch.Tensor],\n@@ -112,10 +122,12 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # DeepEP's topk_ids output refers to the local experts directly. Offset\n         # the topk_ids to move it back to the global experts space so it aligns\n         # with existing vLLM interfaces.\n-        expert_topk_ids = torch.where(\n-            expert_topk_ids == -1,\n-            num_experts - 1 if self.rank_expert_offset == 0 else 0,\n-            expert_topk_ids + self.rank_expert_offset)\n+        if expert_topk_ids.numel() != 0:\n+            invalid_mask = expert_topk_ids.eq(-1)\n+            if self.rank_expert_offset != 0:\n+                expert_topk_ids.add_(self.rank_expert_offset)\n+            fill_val = num_experts - 1 if self.rank_expert_offset == 0 else 0\n+            expert_topk_ids.masked_fill_(invalid_mask, fill_val)\n \n         # Makes a GPU-CPU copy.\n         # TODO (varun): Maybe it is better to re-compute the expert_num_tokens\n@@ -196,9 +208,9 @@ class DeepEPHTPrepareAndFinalize(mk.FusedMoEPrepareAndFinalize):\n         # tokens from the all2all reach this EP rank.\n         if fused_expert_output.numel() != 0:\n             if isinstance(weight_and_reduce_impl, TopKWeightAndReduceDelegate):\n-                weight_and_reduce_impl = TopKWeightAndReduceContiguous()\n+                weight_and_reduce_impl = self._contiguous_reducer\n             fused_expert_output = weight_and_reduce_impl.apply(\n-                output=None,\n+                output=output,\n                 fused_expert_output=fused_expert_output,\n                 topk_weights=topk_weights,\n                 topk_ids=topk_ids,\n", "model_name_or_path": "gpt-5-2025-08-07"}
