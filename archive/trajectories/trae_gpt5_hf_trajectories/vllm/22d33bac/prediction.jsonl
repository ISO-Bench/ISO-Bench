{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-b0e96aa", "model_patch": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex 7978730..566bb21 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -699,7 +699,7 @@ def create_kv_caches_with_random(\n     torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n \n     scale = head_size**-0.5\n-    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n+    x = 16 // get_dtype_size(torch_dtype)\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n     key_caches: list[torch.Tensor] = []\n     for _ in range(num_layers):\n@@ -804,11 +804,24 @@ def make_tensor_with_pad(\n     np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE[dtype]\n     padded_x = make_ndarray_with_pad(x, pad, np_dtype, max_len=max_len)\n \n-    tensor = torch.from_numpy(padded_x).to(device)\n-    if pin_memory:\n-        tensor = tensor.pin_memory()\n-\n-    return tensor\n+    cpu_tensor = torch.from_numpy(padded_x)\n+    # Pin CPU memory before H2D copy only when targeting CUDA\n+    if pin_memory and device is not None:\n+        try:\n+            dest = torch.device(device)\n+        except (TypeError, ValueError):\n+            dest = torch.device(\"cpu\")\n+        if dest.type == \"cuda\":\n+            cpu_tensor = cpu_tensor.pin_memory()\n+\n+    if device is None:\n+        return cpu_tensor\n+\n+    dest_device = torch.device(device)\n+    if dest_device.type == \"cuda\":\n+        return cpu_tensor.to(dest_device, non_blocking=pin_memory)\n+    else:\n+        return cpu_tensor.to(dest_device)\n \n \n def async_tensor_h2d(\n@@ -818,13 +831,16 @@ def async_tensor_h2d(\n     pin_memory: bool,\n ) -> torch.Tensor:\n     \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n-    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n-    return t.to(device=target_device, non_blocking=True)\n+    dest = torch.device(target_device)\n+    do_pin = pin_memory and dest.type == \"cuda\"\n+    t = torch.tensor(data, dtype=dtype, pin_memory=do_pin, device=\"cpu\")\n+    return t.to(device=dest, non_blocking=do_pin)\n \n \n+@lru_cache(maxsize=None)\n def get_dtype_size(dtype: torch.dtype) -> int:\n     \"\"\"Get the size of the data type in bytes.\"\"\"\n-    return torch.tensor([], dtype=dtype).element_size()\n+    return torch.empty((), dtype=dtype).element_size()\n \n \n # `collections` helpers\n@@ -1948,8 +1964,8 @@ class MemorySnapshot:\n         self.torch_peak = torch.cuda.memory_stats().get(\n             \"allocated_bytes.all.peak\", 0)\n \n-        self.cuda_memory = torch.cuda.mem_get_info(\n-        )[1] - torch.cuda.mem_get_info()[0]\n+        free_total = torch.cuda.mem_get_info()\n+        self.cuda_memory = free_total[1] - free_total[0]\n \n         # torch.cuda.memory_reserved() is how many bytes\n         # PyTorch gets from cuda (by calling cudaMalloc, etc.)\n", "model_name_or_path": "gpt-5-2025-08-07"}
