{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-388596c", "model_patch": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 462ba8a..c931b69 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -258,7 +258,7 @@ def scaled_fp8_quant(\n     else:\n         output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n+        scale = torch.empty(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n     else:\n         vllm_ops.static_scaled_fp8_quant(output, input, scale)\ndiff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex bf3a59e..45aaa66 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -204,7 +204,7 @@ class Fp8LinearMethod(LinearMethodBase):\n                                                   layer.weight_scale[idx])\n \n                 layer.weight[start:end, :] = per_tensor_quantize(\n-                    weight_dq, layer.weight_scale.max())\n+                    weight_dq, max_w_scale)\n                 start = end\n             layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n \n@@ -296,19 +296,21 @@ class Fp8KVCacheMethod(QuantizeMethodBase):\n \n def all_close_1d(x: torch.Tensor) -> bool:\n     assert len(x.shape) == 1\n-    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))\n+    return torch.allclose(x, x[0].expand_as(x))\n \n \n def per_tensor_quantize(tensor: torch.Tensor,\n                         inv_scale: Union[float, torch.Tensor]) -> torch.Tensor:\n     finfo = torch.finfo(torch.float8_e4m3fn)\n-    qweight = (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max)\n-    return qweight.to(torch.float8_e4m3fn)\n+    q = torch.empty_like(tensor)\n+    torch.divide(tensor, inv_scale, out=q)\n+    torch.clamp_(q, min=finfo.min, max=finfo.max)\n+    return q.to(torch.float8_e4m3fn)\n \n \n def per_tensor_dequantize(\n         tensor: torch.Tensor, inv_scale: Union[float,\n                                                torch.Tensor]) -> torch.Tensor:\n-    fake_qweight = tensor.to(torch.float16)\n-    dq_weight = fake_qweight * inv_scale\n-    return dq_weight\n+    dq = tensor.to(torch.float16)\n+    torch.mul(dq, inv_scale, out=dq)\n+    return dq\n", "model_name_or_path": "gpt-5-2025-08-07"}
