diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py
index da474be..739fa66 100644
--- a/vllm/model_executor/models/llama.py
+++ b/vllm/model_executor/models/llama.py
@@ -183,11 +183,15 @@ class LlamaAttention(nn.Module):
         kv_cache: torch.Tensor,
         attn_metadata: AttentionMetadata,
     ) -> torch.Tensor:
-        qkv, _ = self.qkv_proj(hidden_states)
+        qkv_proj = self.qkv_proj
+        rotary_emb = self.rotary_emb
+        attn = self.attn
+        o_proj = self.o_proj
+        qkv, _ = qkv_proj(hidden_states)
         q, k, v = qkv.split((self.q_size, self.kv_size, self.kv_size), dim=-1)
-        q, k = self.rotary_emb(positions, q, k)
-        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)
-        output, _ = self.o_proj(attn_output)
+        q, k = rotary_emb(positions, q, k)
+        attn_output = attn(q, k, v, kv_cache, attn_metadata)
+        output, _ = o_proj(attn_output)
         return output
 
 
@@ -575,8 +579,9 @@ class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
         hidden_states: torch.Tensor,
         sampling_metadata: SamplingMetadata,
     ) -> Optional[torch.Tensor]:
-        logits = self.logits_processor(self.lm_head, hidden_states,
-                                       sampling_metadata)
+        proc = self.logits_processor
+        lm_head = self.lm_head
+        logits = proc(lm_head, hidden_states, sampling_metadata)
         return logits
 
     def pooler(
@@ -584,13 +589,14 @@ class LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
         hidden_states: torch.Tensor,
         pooling_metadata: PoolingMetadata,
     ) -> Optional[PoolerOutput]:
+        pool = self._pooler
         logits = self.compute_logits(hidden_states, None)
-        return self._pooler(logits, pooling_metadata)
+        return pool(logits, pooling_metadata)
 
     def sample(self, logits: torch.Tensor,
                sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:
-        next_tokens = self.sampler(logits, sampling_metadata)
-        return next_tokens
+        sample = self.sampler
+        return sample(logits, sampling_metadata)
 
     def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):
         loader = AutoWeightsLoader(
