diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/attention/ops/triton_unified_attention.py b/vllm/attention/ops/triton_unified_attention.py
index c65f095..ee69891 100644
--- a/vllm/attention/ops/triton_unified_attention.py
+++ b/vllm/attention/ops/triton_unified_attention.py
@@ -146,14 +146,14 @@ def kernel_unified_attention_2d(
                               other=0.0)
 
     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)
+    offs_n = tl.arange(0, BLOCK_SIZE)
+
 
     # iterate through tiles
     for j in range(0, num_blocks):
 
         physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)
 
-        offs_n = tl.arange(0, BLOCK_SIZE)
-
         v_offset = (physical_block_idx * stride_v_cache_0 +
                     kv_head_idx * stride_v_cache_2 +
                     offs_d[None, :] * stride_v_cache_3 +
@@ -363,6 +363,8 @@ def kernel_unified_attention_3d(
                               other=0.0)
 
     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)
+    offs_n = tl.arange(0, BLOCK_SIZE)
+
 
     # iterate through tiles within current segment
     for j in range(
@@ -371,8 +373,6 @@ def kernel_unified_attention_3d(
     ):
         physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)
 
-        offs_n = tl.arange(0, BLOCK_SIZE)
-
         v_offset = (physical_block_idx * stride_v_cache_0 +
                     kv_head_idx * stride_v_cache_2 +
                     offs_d[None, :] * stride_v_cache_3 +
@@ -584,8 +584,6 @@ def unified_attention(
         "Block size must be at least 32 for fp8"
 
     use_alibi_slopes = alibi_slopes is not None
-
-    block_size = v.shape[1]
     num_seqs = len(seqused_k)
     num_query_heads = q.shape[1]
     num_kv_heads = k.shape[2]
@@ -593,6 +591,8 @@ def unified_attention(
     head_size = q.shape[2]
 
     BLOCK_M = 16
+    head_size_padded = triton.next_power_of_2(head_size)
+
     BLOCK_Q = BLOCK_M // num_queries_per_kv
 
     # Ideally we would launch with kernel with:
@@ -632,7 +632,7 @@ def unified_attention(
             output_stride_1=out.stride(1),
             BLOCK_SIZE=block_size,
             HEAD_SIZE=head_size,
-            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),
+            HEAD_SIZE_PADDED=head_size_padded,
             USE_ALIBI_SLOPES=use_alibi_slopes,
             USE_SOFTCAP=(softcap > 0),
             SLIDING_WINDOW=(1 + window_size[0]),
@@ -658,7 +658,7 @@ def unified_attention(
             q.shape[0],
             num_query_heads,
             NUM_SEGMENTS,
-            triton.next_power_of_2(head_size),
+            head_size_padded,
             dtype=torch.float32,
             device=q.device,
         )
@@ -699,7 +699,7 @@ def unified_attention(
                 query_stride_1=q.stride(1),
                 BLOCK_SIZE=block_size,
                 HEAD_SIZE=head_size,
-                HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),
+                HEAD_SIZE_PADDED=head_size_padded,
                 USE_ALIBI_SLOPES=use_alibi_slopes,
                 USE_SOFTCAP=(softcap > 0),
                 SLIDING_WINDOW=(1 + window_size[0]),
@@ -731,7 +731,7 @@ def unified_attention(
             block_table_stride=block_table.stride(0),
             BLOCK_SIZE=block_size,
             HEAD_SIZE=head_size,
-            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),
+            HEAD_SIZE_PADDED=head_size_padded,
             query_start_len_ptr=cu_seqlens_q,
             BLOCK_Q=BLOCK_Q,
             NUM_SEGMENTS_PER_SEQ=NUM_SEGMENTS,
