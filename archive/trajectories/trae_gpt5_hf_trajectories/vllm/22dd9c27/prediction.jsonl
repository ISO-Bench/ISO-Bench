{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-a6d795d", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/attention/ops/triton_unified_attention.py b/vllm/attention/ops/triton_unified_attention.py\nindex c65f095..ee69891 100644\n--- a/vllm/attention/ops/triton_unified_attention.py\n+++ b/vllm/attention/ops/triton_unified_attention.py\n@@ -146,14 +146,14 @@ def kernel_unified_attention_2d(\n                               other=0.0)\n \n     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n+    offs_n = tl.arange(0, BLOCK_SIZE)\n+\n \n     # iterate through tiles\n     for j in range(0, num_blocks):\n \n         physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)\n \n-        offs_n = tl.arange(0, BLOCK_SIZE)\n-\n         v_offset = (physical_block_idx * stride_v_cache_0 +\n                     kv_head_idx * stride_v_cache_2 +\n                     offs_d[None, :] * stride_v_cache_3 +\n@@ -363,6 +363,8 @@ def kernel_unified_attention_3d(\n                               other=0.0)\n \n     num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)\n+    offs_n = tl.arange(0, BLOCK_SIZE)\n+\n \n     # iterate through tiles within current segment\n     for j in range(\n@@ -371,8 +373,6 @@ def kernel_unified_attention_3d(\n     ):\n         physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)\n \n-        offs_n = tl.arange(0, BLOCK_SIZE)\n-\n         v_offset = (physical_block_idx * stride_v_cache_0 +\n                     kv_head_idx * stride_v_cache_2 +\n                     offs_d[None, :] * stride_v_cache_3 +\n@@ -584,8 +584,6 @@ def unified_attention(\n         \"Block size must be at least 32 for fp8\"\n \n     use_alibi_slopes = alibi_slopes is not None\n-\n-    block_size = v.shape[1]\n     num_seqs = len(seqused_k)\n     num_query_heads = q.shape[1]\n     num_kv_heads = k.shape[2]\n@@ -593,6 +591,8 @@ def unified_attention(\n     head_size = q.shape[2]\n \n     BLOCK_M = 16\n+    head_size_padded = triton.next_power_of_2(head_size)\n+\n     BLOCK_Q = BLOCK_M // num_queries_per_kv\n \n     # Ideally we would launch with kernel with:\n@@ -632,7 +632,7 @@ def unified_attention(\n             output_stride_1=out.stride(1),\n             BLOCK_SIZE=block_size,\n             HEAD_SIZE=head_size,\n-            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n+            HEAD_SIZE_PADDED=head_size_padded,\n             USE_ALIBI_SLOPES=use_alibi_slopes,\n             USE_SOFTCAP=(softcap > 0),\n             SLIDING_WINDOW=(1 + window_size[0]),\n@@ -658,7 +658,7 @@ def unified_attention(\n             q.shape[0],\n             num_query_heads,\n             NUM_SEGMENTS,\n-            triton.next_power_of_2(head_size),\n+            head_size_padded,\n             dtype=torch.float32,\n             device=q.device,\n         )\n@@ -699,7 +699,7 @@ def unified_attention(\n                 query_stride_1=q.stride(1),\n                 BLOCK_SIZE=block_size,\n                 HEAD_SIZE=head_size,\n-                HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n+                HEAD_SIZE_PADDED=head_size_padded,\n                 USE_ALIBI_SLOPES=use_alibi_slopes,\n                 USE_SOFTCAP=(softcap > 0),\n                 SLIDING_WINDOW=(1 + window_size[0]),\n@@ -731,7 +731,7 @@ def unified_attention(\n             block_table_stride=block_table.stride(0),\n             BLOCK_SIZE=block_size,\n             HEAD_SIZE=head_size,\n-            HEAD_SIZE_PADDED=triton.next_power_of_2(head_size),\n+            HEAD_SIZE_PADDED=head_size_padded,\n             query_start_len_ptr=cu_seqlens_q,\n             BLOCK_Q=BLOCK_Q,\n             NUM_SEGMENTS_PER_SEQ=NUM_SEGMENTS,\n", "model_name_or_path": "gpt-5-2025-08-07"}
