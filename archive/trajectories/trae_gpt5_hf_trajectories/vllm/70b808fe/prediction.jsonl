{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-63d635d", "model_patch": "diff --git a/model_patch.diff b/model_patch.diff\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py\nindex ec5147c..69cadfc 100644\n--- a/vllm/model_executor/models/qwen2_5_vl.py\n+++ b/vllm/model_executor/models/qwen2_5_vl.py\n@@ -395,9 +395,9 @@ class Qwen2_5_VisionPatchEmbed(nn.Module):\n \n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         L, C = x.shape\n-        x = x.view(L, -1, self.temporal_patch_size, self.patch_size,\n+        x = x.reshape(L, -1, self.temporal_patch_size, self.patch_size,\n                    self.patch_size)\n-        x = self.proj(x).view(L, self.hidden_size)\n+        x = self.proj(x).reshape(L, self.hidden_size)\n         return x\n \n \n@@ -553,7 +553,7 @@ class Qwen2_5_VisionTransformer(nn.Module):\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\ndiff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py\nindex f044902..16cf66c 100644\n--- a/vllm/model_executor/models/qwen2_vl.py\n+++ b/vllm/model_executor/models/qwen2_vl.py\n@@ -440,9 +440,9 @@ class Qwen2VisionPatchEmbed(nn.Module):\n \n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n         L, C = x.shape\n-        x = x.view(L, -1, self.temporal_patch_size, self.patch_size,\n+        x = x.reshape(L, -1, self.temporal_patch_size, self.patch_size,\n                    self.patch_size)\n-        x = self.proj(x).view(L, self.embed_dim)\n+        x = self.proj(x).reshape(L, self.embed_dim)\n         return x\n \n \n@@ -595,7 +595,7 @@ class Qwen2VisionTransformer(nn.Module):\n             pos_ids.append(\n                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n         pos_ids = torch.cat(pos_ids, dim=0)\n-        max_grid_size = grid_thw[:, 1:].max()\n+        max_grid_size = int(grid_thw[:, 1:].max().item())\n         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)\n         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n         return rotary_pos_emb\n", "model_name_or_path": "gpt-5-2025-08-07"}
