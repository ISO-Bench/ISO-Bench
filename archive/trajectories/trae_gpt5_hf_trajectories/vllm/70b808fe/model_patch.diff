diff --git a/model_patch.diff b/model_patch.diff
new file mode 100644
index 0000000..e69de29
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index ec5147c..69cadfc 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -395,9 +395,9 @@ class Qwen2_5_VisionPatchEmbed(nn.Module):
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         L, C = x.shape
-        x = x.view(L, -1, self.temporal_patch_size, self.patch_size,
+        x = x.reshape(L, -1, self.temporal_patch_size, self.patch_size,
                    self.patch_size)
-        x = self.proj(x).view(L, self.hidden_size)
+        x = self.proj(x).reshape(L, self.hidden_size)
         return x
 
 
@@ -553,7 +553,7 @@ class Qwen2_5_VisionTransformer(nn.Module):
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
+        max_grid_size = int(grid_thw[:, 1:].max().item())
         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
         return rotary_pos_emb
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index f044902..16cf66c 100644
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -440,9 +440,9 @@ class Qwen2VisionPatchEmbed(nn.Module):
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
         L, C = x.shape
-        x = x.view(L, -1, self.temporal_patch_size, self.patch_size,
+        x = x.reshape(L, -1, self.temporal_patch_size, self.patch_size,
                    self.patch_size)
-        x = self.proj(x).view(L, self.embed_dim)
+        x = self.proj(x).reshape(L, self.embed_dim)
         return x
 
 
@@ -595,7 +595,7 @@ class Qwen2VisionTransformer(nn.Module):
             pos_ids.append(
                 torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))
         pos_ids = torch.cat(pos_ids, dim=0)
-        max_grid_size = grid_thw[:, 1:].max()
+        max_grid_size = int(grid_thw[:, 1:].max().item())
         rotary_pos_emb_full = self.rotary_pos_emb(max_grid_size)
         rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)
         return rotary_pos_emb
