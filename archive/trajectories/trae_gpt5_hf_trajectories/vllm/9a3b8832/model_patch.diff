diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 9de2338..d57e357 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -46,8 +46,10 @@ def _rotate_neox(x: torch.Tensor) -> torch.Tensor:
 def _rotate_gptj(x: torch.Tensor) -> torch.Tensor:
     x1 = x[..., ::2]
     x2 = x[..., 1::2]
-    x = torch.stack((-x2, x1), dim=-1)
-    return x.flatten(-2)
+    out = torch.empty_like(x)
+    out[..., ::2] = -x2
+    out[..., 1::2] = x1
+    return out
 
 
 def _apply_rotary_emb_torch(
@@ -56,8 +58,12 @@ def _apply_rotary_emb_torch(
     sin: torch.Tensor,
     is_neox_style: bool,
 ) -> torch.Tensor:
-    cos = cos.unsqueeze(-2).to(x.dtype)
-    sin = sin.unsqueeze(-2).to(x.dtype)
+    cos = cos.unsqueeze(-2)
+    sin = sin.unsqueeze(-2)
+    if cos.dtype != x.dtype:
+        cos = cos.to(x.dtype)
+    if sin.dtype != x.dtype:
+        sin = sin.to(x.dtype)
     if is_neox_style:
         x1, x2 = torch.chunk(x, 2, dim=-1)
     else:
@@ -68,7 +74,10 @@ def _apply_rotary_emb_torch(
     if is_neox_style:
         return torch.cat((o1, o2), dim=-1)
     else:
-        return torch.stack((o1, o2), dim=-1).flatten(-2)
+        out = torch.empty_like(x)
+        out[..., ::2] = o1
+        out[..., 1::2] = o2
+        return out
 
 
 def _apply_rotary_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor,
@@ -129,7 +138,7 @@ class RotaryEmbedding(CustomOp):
         inv_freq = self._compute_inv_freq(self.base)
         t = torch.arange(self.max_position_embeddings, dtype=torch.float)
 
-        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        freqs = torch.outer(t, inv_freq)
         cos = freqs.cos()
         sin = freqs.sin()
         cache = torch.cat((cos, sin), dim=-1)
@@ -433,7 +442,7 @@ class LinearScalingRotaryEmbedding(RotaryEmbedding):
             t = torch.arange(max_len, dtype=torch.float)
             t = t / scaling_factor
 
-            freqs = torch.einsum("i,j -> ij", t, inv_freq)
+            freqs = torch.outer(t, inv_freq)
             cos = freqs.cos()
             sin = freqs.sin()
             cache = torch.cat((cos, sin), dim=-1)
@@ -524,7 +533,7 @@ class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):
         inv_freq = self._compute_inv_freq(base)
         t = torch.arange(max_len, dtype=torch.float)
 
-        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        freqs = torch.outer(t, inv_freq)
         cos = freqs.cos()
         sin = freqs.sin()
         cache = torch.cat((cos, sin), dim=-1)
@@ -626,7 +635,7 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):
         inv_freq = self._compute_inv_freq(self.scaling_factor)
         t = torch.arange(self.max_position_embeddings * self.scaling_factor,
                          dtype=torch.float32)
-        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        freqs = torch.outer(t, inv_freq)
         cos = (freqs.cos() * self.mscale)
         sin = (freqs.sin() * self.mscale)
         cache = torch.cat((cos, sin), dim=-1)
@@ -711,7 +720,7 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):
     ) -> torch.Tensor:
         inv_freq = self._compute_inv_freq(rescale_factors)
         t = torch.arange(max_position_embeddings, dtype=torch.float)
-        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        freqs = torch.outer(t, inv_freq)
         cos = freqs.cos() * mscale
         sin = freqs.sin() * mscale
         cache = torch.cat((cos, sin), dim=-1)
@@ -822,7 +831,7 @@ class DeepseekScalingRotaryEmbedding(RotaryEmbedding):
         t = torch.arange(self.max_position_embeddings * self.scaling_factor,
                          device=current_platform.device_type,
                          dtype=torch.float32)
-        freqs = torch.einsum("i,j -> ij", t, inv_freq)
+        freqs = torch.outer(t, inv_freq)
         cos = (freqs.cos() * self.mscale)
         sin = (freqs.sin() * self.mscale)
         cache = torch.cat((cos, sin), dim=-1)
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 520d8fb..d341af4 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -280,21 +280,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
         # a faster version of creating a new tensor every time. Thus, we should
         # not make any assumptions about the values in these tensors.
-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+        self.input_ids_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int32,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
-        self.positions_cpu = torch.zeros(self.max_num_tokens,
+        self.positions_cpu = torch.empty(self.max_num_tokens,
                                          dtype=torch.int64,
                                          device="cpu",
                                          pin_memory=self.pin_memory)
         self.positions_np = self.positions_cpu.numpy()
-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,
                                                dtype=torch.int32,
                                                device="cpu",
                                                pin_memory=self.pin_memory)
         self.query_start_loc_np = self.query_start_loc_cpu.numpy()
-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,
                                         dtype=torch.int32,
                                         device="cpu",
                                         pin_memory=self.pin_memory)
