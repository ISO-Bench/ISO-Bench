{"instance_id": "/home/ubuntu/OmniPerf-Bench/vllm__repo-3014c92", "model_patch": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 9de2338..d57e357 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -46,8 +46,10 @@ def _rotate_neox(x: torch.Tensor) -> torch.Tensor:\n def _rotate_gptj(x: torch.Tensor) -> torch.Tensor:\n     x1 = x[..., ::2]\n     x2 = x[..., 1::2]\n-    x = torch.stack((-x2, x1), dim=-1)\n-    return x.flatten(-2)\n+    out = torch.empty_like(x)\n+    out[..., ::2] = -x2\n+    out[..., 1::2] = x1\n+    return out\n \n \n def _apply_rotary_emb_torch(\n@@ -56,8 +58,12 @@ def _apply_rotary_emb_torch(\n     sin: torch.Tensor,\n     is_neox_style: bool,\n ) -> torch.Tensor:\n-    cos = cos.unsqueeze(-2).to(x.dtype)\n-    sin = sin.unsqueeze(-2).to(x.dtype)\n+    cos = cos.unsqueeze(-2)\n+    sin = sin.unsqueeze(-2)\n+    if cos.dtype != x.dtype:\n+        cos = cos.to(x.dtype)\n+    if sin.dtype != x.dtype:\n+        sin = sin.to(x.dtype)\n     if is_neox_style:\n         x1, x2 = torch.chunk(x, 2, dim=-1)\n     else:\n@@ -68,7 +74,10 @@ def _apply_rotary_emb_torch(\n     if is_neox_style:\n         return torch.cat((o1, o2), dim=-1)\n     else:\n-        return torch.stack((o1, o2), dim=-1).flatten(-2)\n+        out = torch.empty_like(x)\n+        out[..., ::2] = o1\n+        out[..., 1::2] = o2\n+        return out\n \n \n def _apply_rotary_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor,\n@@ -129,7 +138,7 @@ class RotaryEmbedding(CustomOp):\n         inv_freq = self._compute_inv_freq(self.base)\n         t = torch.arange(self.max_position_embeddings, dtype=torch.float)\n \n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos()\n         sin = freqs.sin()\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -433,7 +442,7 @@ class LinearScalingRotaryEmbedding(RotaryEmbedding):\n             t = torch.arange(max_len, dtype=torch.float)\n             t = t / scaling_factor\n \n-            freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+            freqs = torch.outer(t, inv_freq)\n             cos = freqs.cos()\n             sin = freqs.sin()\n             cache = torch.cat((cos, sin), dim=-1)\n@@ -524,7 +533,7 @@ class DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):\n         inv_freq = self._compute_inv_freq(base)\n         t = torch.arange(max_len, dtype=torch.float)\n \n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos()\n         sin = freqs.sin()\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -626,7 +635,7 @@ class YaRNScalingRotaryEmbedding(RotaryEmbedding):\n         inv_freq = self._compute_inv_freq(self.scaling_factor)\n         t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                          dtype=torch.float32)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = (freqs.cos() * self.mscale)\n         sin = (freqs.sin() * self.mscale)\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -711,7 +720,7 @@ class Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n     ) -> torch.Tensor:\n         inv_freq = self._compute_inv_freq(rescale_factors)\n         t = torch.arange(max_position_embeddings, dtype=torch.float)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = freqs.cos() * mscale\n         sin = freqs.sin() * mscale\n         cache = torch.cat((cos, sin), dim=-1)\n@@ -822,7 +831,7 @@ class DeepseekScalingRotaryEmbedding(RotaryEmbedding):\n         t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                          device=current_platform.device_type,\n                          dtype=torch.float32)\n-        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n+        freqs = torch.outer(t, inv_freq)\n         cos = (freqs.cos() * self.mscale)\n         sin = (freqs.sin() * self.mscale)\n         cache = torch.cat((cos, sin), dim=-1)\ndiff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 520d8fb..d341af4 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -280,21 +280,21 @@ class GPUModelRunner(LoRAModelRunnerMixin):\n         # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n         # a faster version of creating a new tensor every time. Thus, we should\n         # not make any assumptions about the values in these tensors.\n-        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n+        self.input_ids_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n-        self.positions_cpu = torch.zeros(self.max_num_tokens,\n+        self.positions_cpu = torch.empty(self.max_num_tokens,\n                                          dtype=torch.int64,\n                                          device=\"cpu\",\n                                          pin_memory=self.pin_memory)\n         self.positions_np = self.positions_cpu.numpy()\n-        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n+        self.query_start_loc_cpu = torch.empty(self.max_num_reqs + 1,\n                                                dtype=torch.int32,\n                                                device=\"cpu\",\n                                                pin_memory=self.pin_memory)\n         self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n-        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n+        self.seq_lens_cpu = torch.empty(self.max_num_reqs,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n", "model_name_or_path": "gpt-5-2025-08-07"}
