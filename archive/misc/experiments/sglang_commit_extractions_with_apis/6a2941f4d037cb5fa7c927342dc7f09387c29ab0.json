{
  "commit_hash": "6a2941f4d037cb5fa7c927342dc7f09387c29ab0",
  "pr_url": "https://github.com/sgl-project/sglang/pull/625",
  "pr_date": "2024-07-15",
  "timeline_text": "Copy link Member Ying1123 commented Jul 15, 2024 No description provided. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions wisclmy0611 added 13 commits July 15, 2024 03:20 fix tp worker ce70553 do not asyncio bc30241 refactor tp worker e75ebe4 fix 07b0763 update f7800a5 reduce the overhead of empty recv inputs b18ef09 fill -> prefill 378fa71 fix tp = 1 11c2856 fix multi node tp e7568d2 fix bench one 878f360 fix e15f16e fix cd1068d update 1c0b42d merrymercy merged commit 6a2941f into main Jul 15, 2024 merrymercy deleted the bench branch July 15, 2024 14:10 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Improve tensor parallel performance ( sgl-project#625 ) \u2026 41a9442 Co-authored-by: Mingyi <wisclmy0611@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:49",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NONE",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Improve tensor parallel performance (#625)",
  "commit_message": "Improve tensor parallel performance (#625)\n\nCo-authored-by: Mingyi <wisclmy0611@gmail.com>",
  "commit_date": "2024-07-15T07:10:51-07:00",
  "files_changed": [
    "README.md",
    "benchmark/latency_throughput/bench_one.py",
    "benchmark/latency_throughput/bench_serving.py",
    "python/sglang/README.md",
    "python/sglang/srt/managers/controller/manager_multi.py",
    "python/sglang/srt/managers/controller/manager_single.py",
    "python/sglang/srt/managers/controller/model_runner.py",
    "python/sglang/srt/managers/controller/tp_worker.py",
    "python/sglang/srt/server.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 254,
    "num_files": 10,
    "num_hunks": 22,
    "num_non_test_edited_lines": 254,
    "num_non_test_files": 10,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/README.md b/README.md\nindex 2ac666c6b..90822b176 100644\n--- a/README.md\n+++ b/README.md\n@@ -377,6 +377,14 @@ python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port\n python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --port 30000 --mem-fraction-static 0.7\n ```\n - See [hyperparameter_tuning.md](docs/hyperparameter_tuning.md) on tuning hyperparameters for better performance.\n+- Add `--nnodes 2` to run tensor parallelism on multiple nodes. If you have two nodes with two GPUs on each node and want to run TP=4, let `sgl-dev-1` be the hostname of the first node and `50000` be an available port.\n+```\n+# Node 0\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 0\n+\n+# Node 1\n+python -m sglang.launch_server --model-path meta-llama/Llama-2-7b-chat-hf --tp 4 --nccl-init sgl-dev-1:50000 --nnodes 2 --node-rank 1\n+```\n \n ### Supported Models\n - Llama\ndiff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py\nindex cfd96b54c..0bb26ee15 100644\n--- a/benchmark/latency_throughput/bench_one.py\n+++ b/benchmark/latency_throughput/bench_one.py\n@@ -96,8 +96,11 @@ def run_one_batch_size(bs):\n         ret = response.json()\n     print(ret)\n \n+    input_len = args.input_len if args.input_len else 1\n+    output_len = max_new_tokens\n+\n     output_throughput = bs * max_new_tokens / latency\n-    overall_throughput = bs * (args.input_len + max_new_tokens) / latency\n+    overall_throughput = bs * (input_len + output_len) / latency\n     print(f\"latency: {latency:.2f} s\")\n     print(f\"decode throughput: {output_throughput:.2f} token/s\")\n     print(f\"overall throughput: {overall_throughput:.2f} token/s\")\ndiff --git a/benchmark/latency_throughput/bench_serving.py b/benchmark/latency_throughput/bench_serving.py\nindex 23e8245f2..24816d4bd 100644\n--- a/benchmark/latency_throughput/bench_serving.py\n+++ b/benchmark/latency_throughput/bench_serving.py\n@@ -312,6 +312,9 @@ def main(args: argparse.Namespace):\n         np.sum([output_len for _, output_len, _ in REQUEST_LATENCY]) / benchmark_time\n     )\n \n+    #latencies = [round(latency, 2) for _, _, latency in REQUEST_LATENCY]\n+    #print(latencies)\n+\n     print(f\"Total time: {benchmark_time:.2f} s\")\n     print(f\"Request throughput: {args.num_prompts / benchmark_time:.2f} requests/s\")\n     print(f\"Decoding throughput: {decoding_throughput:.2f} token/s\")\ndiff --git a/python/sglang/README.md b/python/sglang/README.md\nindex c8c093706..2f298c2c3 100644\n--- a/python/sglang/README.md\n+++ b/python/sglang/README.md\n@@ -2,11 +2,10 @@\n \n - `backend`: Various backends for the language interpreter.\n - `lang`: The frontend language.\n-- `srt`: The runtime for running local models.\n+- `srt`: The serving engine for running local models. (SRT = SGLang Runtime).\n - `test`: Test utilities.\n - `api.py`: Public API.\n - `bench_latency.py`: Benchmark utilities.\n - `global_config.py`: The global configs and constants.\n - `launch_server.py`: The entry point of launching local server.\n - `utils.py`: Common utilities.\n-\ndiff --git a/python/sglang/srt/managers/controller/manager_multi.py b/python/sglang/srt/managers/controller/manager_multi.py\nindex 72e3bed80..ea942093a 100644\n--- a/python/sglang/srt/managers/controller/manager_multi.py\n+++ b/python/sglang/srt/managers/controller/manager_multi.py\n@@ -42,6 +42,8 @@ class LoadBalanceMethod(Enum):\n \n \n class Controller:\n+    \"\"\"A controller that manages multiple data parallel workers.\"\"\"\n+\n     def __init__(\n         self,\n         load_balance_method: str,\n@@ -183,9 +185,11 @@ def start_controller_process(\n     except Exception:\n         pipe_writer.send(get_exception_traceback())\n         raise\n-\n     pipe_writer.send(\"init ok\")\n-    loop = asyncio.get_event_loop()\n+\n+    loop = asyncio.new_event_loop()\n+    loop.set_default_executor(ThreadPoolExecutor(max_workers=256))\n+\n     asyncio.set_event_loop(loop)\n     loop.create_task(controller.loop_for_recv_requests())\n     loop.run_until_complete(controller.loop_for_forward())\ndiff --git a/python/sglang/srt/managers/controller/manager_single.py b/python/sglang/srt/managers/controller/manager_single.py\nindex 4c2720733..c2cb922fc 100644\n--- a/python/sglang/srt/managers/controller/manager_single.py\n+++ b/python/sglang/srt/managers/controller/manager_single.py\n@@ -1,28 +1,104 @@\n \"\"\"A controller that manages a group of tensor parallel workers.\"\"\"\n \n-import asyncio\n+import multiprocessing\n import logging\n-from concurrent.futures import ThreadPoolExecutor\n+import os\n+import pickle\n \n-import uvloop\n+import torch\n+import torch.distributed as dist\n import zmq\n import zmq.asyncio\n \n-from sglang.global_config import global_config\n-from sglang.srt.managers.controller.tp_worker import ModelTpClient\n-from sglang.srt.server_args import PortArgs, ServerArgs\n+from sglang.srt.managers.controller.tp_worker import ModelTpServer\n+from sglang.srt.server_args import PortArgs, ServerArgs, ModelPortArgs\n from sglang.srt.utils import kill_parent_process\n from sglang.utils import get_exception_traceback\n \n-asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\n \n logger = logging.getLogger(\"srt.controller\")\n \n \n+def run_tp_server(\n+    gpu_id: int,\n+    tp_rank: int,\n+    server_args: ServerArgs,\n+    model_port_args: ModelPortArgs,\n+    model_overide_args: dict,\n+):\n+    \"\"\"Run a tp server.\"\"\"\n+    try:\n+        model_server = ModelTpServer(\n+            gpu_id,\n+            tp_rank,\n+            server_args,\n+            model_port_args,\n+            model_overide_args,\n+        )\n+        tp_cpu_group = model_server.model_runner.tp_group.cpu_group\n+\n+        while True:\n+            recv_reqs = broadcast_recv_input(None, tp_rank, tp_cpu_group)\n+            model_server.exposed_step(recv_reqs)\n+    except Exception:\n+        logger.error(\"Exception in run_tp_server:\\n\" + get_exception_traceback())\n+        raise\n+\n+\n+def launch_tp_servers(gpu_ids, tp_rank_range, server_args,\n+                      model_port_args, model_overide_args):\n+    \"\"\"Launch multiple tp servers.\"\"\"\n+    procs = []\n+    for i in tp_rank_range:\n+        proc = multiprocessing.Process(target=run_tp_server, args=(\n+            gpu_ids[i], i, server_args, model_port_args, model_overide_args\n+        ))\n+        proc.start()\n+        procs.append(proc)\n+\n+    return procs\n+\n+\n+def broadcast_recv_input(data, rank, dist_group):\n+    \"\"\"Broadcast inputs from rank=0 to all other ranks with torch.dist backend.\"\"\"\n+\n+    if rank == 0:\n+        if len(data) == 0:\n+            tensor_size = torch.tensor([0], dtype=torch.long)\n+            dist.broadcast(tensor_size, src=0, group=dist_group)\n+        else:\n+            serialized_data = pickle.dumps(data)\n+            size = len(serialized_data)\n+            tensor_data = torch.ByteTensor(list(serialized_data))\n+            tensor_size = torch.tensor([size], dtype=torch.long)\n+\n+            dist.broadcast(tensor_size, src=0, group=dist_group)\n+            dist.broadcast(tensor_data, src=0, group=dist_group)\n+    else:\n+        tensor_size = torch.tensor([0], dtype=torch.long)\n+        dist.broadcast(tensor_size, src=0, group=dist_group)\n+        size = tensor_size.item()\n+\n+        if size == 0:\n+            return []\n+\n+        tensor_data = torch.empty(size, dtype=torch.uint8)\n+        dist.broadcast(tensor_data, src=0, group=dist_group)\n+\n+        serialized_data = bytes(tensor_data.tolist())\n+        data = pickle.loads(serialized_data)\n+        return data\n+\n+\n class ControllerSingle:\n-    def __init__(self, model_client: ModelTpClient, port_args: PortArgs):\n+    \"\"\"A controller that manages a group of tensor parallel workers.\"\"\"\n+\n+    def __init__(self, server_args: ServerArgs, port_args: PortArgs, model_overide_args: dict):\n+        # Parse args\n+        self.server_args = server_args\n+\n         # Init communication\n-        context = zmq.asyncio.Context(2)\n+        context = zmq.Context(2)\n         self.recv_from_tokenizer = context.socket(zmq.PULL)\n         self.recv_from_tokenizer.bind(f\"tcp://127.0.0.1:{port_args.router_port}\")\n \n@@ -31,44 +107,52 @@ class ControllerSingle:\n             f\"tcp://127.0.0.1:{port_args.detokenizer_port}\"\n         )\n \n-        # Init status\n-        self.model_client = model_client\n-        self.recv_reqs = []\n-\n-        # Init some configs\n-        self.request_dependency_delay = global_config.request_dependency_delay\n+        # Init model server\n+        tp_size_local = server_args.tp_size // server_args.nnodes\n+        gpu_ids = [i for _ in range(server_args.nnodes) for i in range(tp_size_local)]\n+\n+        # Launch other tp ranks\n+        if tp_size_local > 1:\n+            tp_rank_range = range(1, tp_size_local)\n+            self.tp_procs = launch_tp_servers(\n+                gpu_ids, tp_rank_range, server_args,\n+                port_args.model_port_args[0], model_overide_args)\n+\n+        # Launch tp rank 0\n+        self.tp_server = ModelTpServer(\n+            gpu_ids[0],\n+            0,\n+            server_args,\n+            port_args.model_port_args[0],\n+            model_overide_args,\n+        )\n+        self.tp_cpu_group = self.tp_server.model_runner.tp_group.cpu_group\n \n-    async def loop_for_forward(self):\n+    def loop_for_forward(self):\n         while True:\n-            next_step_input = list(self.recv_reqs)\n-            self.recv_reqs = []\n-            out_pyobjs = await self.model_client.step(next_step_input)\n+            recv_reqs = self.recv_requests()\n+\n+            if self.server_args.tp_size > 1:\n+                broadcast_recv_input(recv_reqs, 0, self.tp_cpu_group)\n+\n+            out_pyobjs = self.tp_server.exposed_step(recv_reqs)\n \n             for obj in out_pyobjs:\n                 self.send_to_detokenizer.send_pyobj(obj)\n \n-            # async sleep for receiving the subsequent request and avoiding cache miss\n-            slept = False\n-            if len(out_pyobjs) != 0:\n-                has_finished = any(\n-                    [obj.finished_reason is not None for obj in out_pyobjs]\n-                )\n-                if has_finished:\n-                    if self.request_dependency_delay > 0:\n-                        slept = True\n-                        await asyncio.sleep(self.request_dependency_delay)\n-\n-            if not slept:\n-                await asyncio.sleep(global_config.wait_for_new_request_delay)\n-\n-    async def loop_for_recv_requests(self):\n+    def recv_requests(self):\n+        recv_reqs = []\n         while True:\n-            recv_req = await self.recv_from_tokenizer.recv_pyobj()\n-            self.recv_reqs.append(recv_req)\n+            try:\n+                recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)\n+                recv_reqs.append(recv_req)\n+            except zmq.ZMQError:\n+                break\n+        return recv_reqs\n \n \n def start_controller_process(\n-    server_args: ServerArgs, port_args: PortArgs, pipe_writer, model_overide_args\n+    server_args: ServerArgs, port_args: PortArgs, pipe_writer, model_overide_args: dict\n ):\n     logging.basicConfig(\n         level=getattr(logging, server_args.log_level.upper()),\n@@ -76,27 +160,18 @@ def start_controller_process(\n     )\n \n     try:\n-        tp_size_local = server_args.tp_size // server_args.nnodes\n-        model_client = ModelTpClient(\n-            [i for _ in range(server_args.nnodes) for i in range(tp_size_local)],\n-            server_args,\n-            port_args.model_port_args[0],\n-            model_overide_args,\n-        )\n-        controller = ControllerSingle(model_client, port_args)\n+        controller = ControllerSingle(server_args, port_args, model_overide_args)\n     except Exception:\n         pipe_writer.send(get_exception_traceback())\n         raise\n \n     pipe_writer.send(\"init ok\")\n \n-    loop = asyncio.new_event_loop()\n-    loop.set_default_executor(ThreadPoolExecutor(max_workers=256))\n-    asyncio.set_event_loop(loop)\n-    loop.create_task(controller.loop_for_recv_requests())\n     try:\n-        loop.run_until_complete(controller.loop_for_forward())\n+        controller.loop_for_forward()\n     except Exception:\n         logger.error(\"Exception in ControllerSingle:\\n\" + get_exception_traceback())\n     finally:\n+        for t in controller.tp_procs:\n+            os.kill(t.pid, 9)\n         kill_parent_process()\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex d68d9af32..80c40e4f5 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -11,7 +11,7 @@ import torch\n import torch.nn as nn\n from vllm.config import DeviceConfig, LoadConfig\n from vllm.config import ModelConfig as VllmModelConfig\n-from vllm.distributed import init_distributed_environment, initialize_model_parallel\n+from vllm.distributed import init_distributed_environment, initialize_model_parallel, get_tp_group\n from vllm.model_executor.model_loader import get_model\n from vllm.model_executor.models import ModelRegistry\n \n@@ -75,6 +75,7 @@ class ModelRunner:\n             distributed_init_method=nccl_init_method,\n         )\n         initialize_model_parallel(tensor_model_parallel_size=self.tp_size)\n+        self.tp_group = get_tp_group()\n         total_gpu_memory = get_available_gpu_memory(\n             self.gpu_id, distributed=self.tp_size > 1\n         )\ndiff --git a/python/sglang/srt/managers/controller/tp_worker.py b/python/sglang/srt/managers/controller/tp_worker.py\nindex 1d22dfdf1..21569c966 100644\n--- a/python/sglang/srt/managers/controller/tp_worker.py\n+++ b/python/sglang/srt/managers/controller/tp_worker.py\n@@ -53,7 +53,7 @@ class ModelTpServer:\n         tp_rank: int,\n         server_args: ServerArgs,\n         model_port_args: ModelPortArgs,\n-        model_overide_args,\n+        model_overide_args: dict,\n     ):\n         server_args, model_port_args = obtain(server_args), obtain(model_port_args)\n         suppress_other_loggers()\n@@ -178,7 +178,7 @@ class ModelTpServer:\n         self.new_token_ratio_recovery = global_config.new_token_ratio_recovery\n \n     def exposed_step(self, recv_reqs):\n-        if self.tp_size * self.dp_size != 1:\n+        if not isinstance(recv_reqs, list):\n             recv_reqs = obtain(recv_reqs)\n \n         try:\n@@ -206,11 +206,11 @@ class ModelTpServer:\n \n     @torch.inference_mode()\n     def forward_step(self):\n-        new_batch = self.get_new_fill_batch()\n+        new_batch = self.get_new_prefill_batch()\n \n         if new_batch is not None:\n-            # Run a new fill batch\n-            self.forward_fill_batch(new_batch)\n+            # Run a new prefill batch\n+            self.forward_prefill_batch(new_batch)\n             self.cache_filled_batch(new_batch)\n \n             if not new_batch.is_empty():\n@@ -219,7 +219,7 @@ class ModelTpServer:\n                 else:\n                     self.running_batch.merge(new_batch)\n         else:\n-            # Run decode batch\n+            # Run a decode batch\n             if self.running_batch is not None:\n                 # Run a few decode batches continuously for reducing overhead\n                 for _ in range(global_config.num_continue_decode_steps):\n@@ -312,7 +312,7 @@ class ModelTpServer:\n         )\n         self.forward_queue.append(req)\n \n-    def get_new_fill_batch(self) -> Optional[Batch]:\n+    def get_new_prefill_batch(self) -> Optional[Batch]:\n         running_bs = (\n             len(self.running_batch.reqs) if self.running_batch is not None else 0\n         )\n@@ -436,7 +436,7 @@ class ModelTpServer:\n         self.forward_queue = [x for x in self.forward_queue if x not in can_run_list]\n         return new_batch\n \n-    def forward_fill_batch(self, batch: Batch):\n+    def forward_prefill_batch(self, batch: Batch):\n         # Build batch tensors\n         batch.prepare_for_extend(\n             self.model_config.vocab_size, self.int_token_logit_bias\n@@ -746,8 +746,8 @@ class ModelTpClient:\n             # Init model\n             assert len(gpu_ids) == 1\n             self.model_server = ModelTpService().exposed_ModelTpServer(\n-                0,\n                 gpu_ids[0],\n+                0,\n                 server_args,\n                 model_port_args,\n                 model_overide_args,\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex 6cda67dea..0a3f53b8b 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -33,9 +33,9 @@ from sglang.srt.managers.controller.manager_multi import (\n     start_controller_process as start_controller_process_multi,\n )\n from sglang.srt.managers.controller.manager_single import (\n+    launch_tp_servers,\n     start_controller_process as start_controller_process_single,\n )\n-from sglang.srt.managers.controller.tp_worker import ModelTpService\n from sglang.srt.managers.detokenizer_manager import start_detokenizer_process\n from sglang.srt.managers.io_struct import GenerateReqInput\n from sglang.srt.managers.tokenizer_manager import TokenizerManager\n@@ -53,7 +53,6 @@ from sglang.srt.utils import (\n     enable_show_time_cost,\n     receive_addrs,\n     send_addrs_to_rank_0,\n-    start_rpyc_service_process,\n )\n from sglang.utils import get_exception_traceback\n \n@@ -192,21 +191,17 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n         model_port_args=model_port_args,\n     )\n \n-    # TODO multi-node dp is not supported\n-    assert not (server_args.dp_size > 1 and server_args.node_rank is not None)\n+    # Handle multi-node tp\n     if server_args.nnodes > 1:\n+        assert server_args.dp_size == 1, \"Multi-node dp is not supported.\"\n+\n         if server_args.node_rank != 0:\n-            send_addrs_to_rank_0(model_port_args[0], server_args)\n-        else:\n-            receive_addrs(model_port_args[0], server_args)\n-        for i in range(tp_size_local):\n-            start_rpyc_service_process(\n-                ModelTpService, model_port_args[0].model_tp_ports[i]\n-            )\n-        if server_args.node_rank != 0:\n-            logger.info(\n-                f\"[node_rank={server_args.node_rank}]: Listen for connections...\"\n-            )\n+            tp_size_local = server_args.tp_size // server_args.nnodes\n+            gpu_ids = [i for _ in range(server_args.nnodes) for i in range(tp_size_local)]\n+            tp_rank_range = list(range(server_args.node_rank * tp_size_local,\n+                                  (server_args.node_rank + 1) * tp_size_local))\n+            procs = launch_tp_servers(gpu_ids, tp_rank_range, server_args,\n+                                      port_args.model_port_args[0], model_overide_args)\n             while True:\n                 pass\n \ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 46dfc25d2..b4f79c066 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -67,10 +67,12 @@ class ServerArgs:\n         if self.tokenizer_path is None:\n             self.tokenizer_path = self.model_path\n         if self.mem_fraction_static is None:\n-            if self.tp_size >= 8:\n+            if self.tp_size >= 16:\n+                self.mem_fraction_static = 0.74\n+            elif self.tp_size >= 8:\n                 self.mem_fraction_static = 0.78\n             elif self.tp_size >= 4:\n-                self.mem_fraction_static = 0.80\n+                self.mem_fraction_static = 0.82\n             elif self.tp_size >= 2:\n                 self.mem_fraction_static = 0.85\n             else:",
  "apis": [
    "sglang.srt.server.launch_server",
    "sglang.srt.managers.controller.ControllerSingle",
    "sglang.srt.managers.controller.tp_worker.ModelTpServer",
    "sglang.srt.server_args.ServerArgs"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit introduces several non-trivial modifications in multiple source code files (README, benchmark scripts, and core modules in the \"python/sglang/srt\" directory). The changes adjust the way tensor parallelism is launched and managed (e.g., introducing new multiprocessing processes for TP servers, updating threadpool executor usage, and modifying batch processing functions to \"prefill\" instead of \"fill\"), as well as configuration parameters (e.g., memory fraction adjustments based on tp_size). The overall intent, as indicated by the commit message \"Improve tensor parallel performance\", is to enhance CPU-side performance for tensor parallel operations through optimized processing and communication mechanisms. These modifications affect high-level APIs and internal execution of the tensor parallel serving, and are performance optimizations rather than simple refactoring, bug fixes, or feature additions.",
  "llm_api_reason": "This commit improves tensor parallel performance by changing several components. In the README the command\u2010line examples are updated with new options specific to multi\u2010node tensor parallelism. In the server and controller code the changes affect the way the server is launched (including new routines that launch tensor parallel servers on different nodes), how the controller for tensor parallel workers (ControllerSingle) coordinates the TP processes, and adjustments in the TP worker\u2019s ModelTpServer (e.g. renaming batch methods to \u201cprefill\u201d style processing). In addition, the ServerArgs configuration logic is modified to adjust memory fraction thresholds based on tensor parallel size. These changes affect high\u2010level public APIs that are used to start and run the serving engine."
}