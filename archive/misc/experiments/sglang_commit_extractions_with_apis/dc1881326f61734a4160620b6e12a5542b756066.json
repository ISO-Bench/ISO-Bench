{
  "commit_hash": "dc1881326f61734a4160620b6e12a5542b756066",
  "pr_url": "https://github.com/sgl-project/sglang/pull/3008",
  "pr_date": "2025-01-20",
  "timeline_text": "Copy link Contributor merrymercy commented Jan 20, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Fix a regression introduced by #2786 python3 -m sglang.bench_offline_throughput --model meta-llama/Llama-3.1-8B-Instruct --dataset-name random --num-p 1 --random-input 32 --random-output 512 --random-range 1 --tp 8 --enable-torch-compile --cuda-graph-max-bs 1 before the fix: 438 token/s after the fix: 449 token/s Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, bjmsong, and yiakwy-xpu-ml-framework-team reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions Fix fp8 kv cache regression 61f0317 merrymercy requested review from Ying1123 , hnyls2002 , zhyncs and ispobock as code owners January 20, 2025 11:32 Hide details View details merrymercy merged commit dc18813 into main Jan 20, 2025 4 of 17 checks passed Uh oh! There was an error while loading. Please reload this page . merrymercy deleted the pr-fix-regression branch January 20, 2025 11:39 zhyncs mentioned this pull request Jan 20, 2025 chore: bump v0.4.1.post7 #3009 Merged 4 tasks merrymercy changed the title Fix perf regression on small batch sizes Fix perf regression on small batch sizes due to kv cache scale Jan 20, 2025 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Fix perf regression on small batch sizes ( sgl-project#3008 ) 211ac33 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:39",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python3 -m sglang.bench_offline_throughput --model meta-llama/Llama-3.1-8B-Instruct --dataset-name random --num-p 1 --random-input 32 --random-output 512 --random-range 1 --tp 8 --enable-torch-compile --cuda-graph-max-bs 1",
  "commit_subject": "Fix perf regression on small batch sizes (#3008)",
  "commit_message": "Fix perf regression on small batch sizes (#3008)",
  "commit_date": "2025-01-20T03:39:49-08:00",
  "files_changed": [
    "python/sglang/srt/layers/radix_attention.py",
    "python/sglang/srt/mem_cache/memory_pool.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 18,
    "num_files": 2,
    "num_hunks": 3,
    "num_non_test_edited_lines": 18,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/radix_attention.py b/python/sglang/srt/layers/radix_attention.py\nindex a449d7188..0d46e7bba 100644\n--- a/python/sglang/srt/layers/radix_attention.py\n+++ b/python/sglang/srt/layers/radix_attention.py\n@@ -47,8 +47,8 @@ class RadixAttention(nn.Module):\n         self.logit_cap = logit_cap\n         self.sliding_window_size = sliding_window_size or -1\n         self.is_cross_attention = is_cross_attention\n-        self.k_scale = 1.0\n-        self.v_scale = 1.0\n+        self.k_scale = None\n+        self.v_scale = None\n \n     def forward(\n         self,\ndiff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py\nindex e30736722..7b9b35611 100644\n--- a/python/sglang/srt/mem_cache/memory_pool.py\n+++ b/python/sglang/srt/mem_cache/memory_pool.py\n@@ -27,7 +27,7 @@ import logging\n import threading\n from enum import IntEnum\n from functools import wraps\n-from typing import List, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import psutil\n@@ -270,13 +270,17 @@ class MHATokenToKVPool(BaseTokenToKVPool):\n         loc: torch.Tensor,\n         cache_k: torch.Tensor,\n         cache_v: torch.Tensor,\n-        k_scale: float = 1.0,\n-        v_scale: float = 1.0,\n+        k_scale: Optional[float] = None,\n+        v_scale: Optional[float] = None,\n     ):\n         layer_id = layer.layer_id\n         if cache_k.dtype != self.dtype:\n-            cache_k = (cache_k / k_scale).to(self.dtype)\n-            cache_v = (cache_v / v_scale).to(self.dtype)\n+            if k_scale is not None:\n+                cache_k.div_(k_scale)\n+            if v_scale is not None:\n+                cache_v.div_(v_scale)\n+            cache_k = cache_k.to(self.dtype)\n+            cache_v = cache_v.to(self.dtype)\n         if self.store_dtype != self.dtype:\n             self.k_buffer[layer_id][loc] = cache_k.view(self.store_dtype)\n             self.v_buffer[layer_id][loc] = cache_v.view(self.store_dtype)",
  "apis": [
    "sglang.srt.layers.radix_attention.RadixAttention",
    "sglang.srt.mem_cache.MHATokenToKVPool.set_kv_buffer"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/radix_attention.py",
    "/path/to/repos/sglang/python/sglang/srt/mem_cache/memory_pool.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit modifies two non-test source files. The changes adjust the behavior of scaling operations in the attention and memory pool modules, conditionally applying division based on the scale values. The commit message \"Fix perf regression on small batch sizes\" further indicates that the changes address a performance issue on CPU for small batches. The modifications are not just trivial refactoring or documentation changes but affect the core processing, thereby optimizing the performance of the existing API. Despite being a fix, it is directly aimed at correcting performance degradation, qualifying it as performance/optimization related.",
  "llm_api_reason": "The commit adjusts how scaling factors are handled in the attention and memory pool code. In RadixAttention, it replaces fixed scale values (1.0) with None to delay or conditionally apply scaling in the forward pass. In MHATokenToKVPool\u2019s set_kv_buffer method, the scaling is applied only if a non\u2010None value is given, avoiding unnecessary division for small batches and thus improving performance. These changes affect how the RadixAttention layer and the KV cache set operation behave when processing small batch sizes."
}