{
  "commit_hash": "6fc175968c3a9fc0521948aa3636887cd6d84107",
  "pr_url": "https://github.com/sgl-project/sglang/pull/5945",
  "pr_date": "2025-05-01",
  "timeline_text": "Copy link Collaborator hebiao064 commented May 1, 2025 Motivation Before: 35us After: 10us Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction hebiao064 added 2 commits May 1, 2025 07:19 Optimize FA3 Decoding Speed 7e7453a Merge branch 'main' into bhe/optimize_pad 303b713 hebiao064 marked this pull request as ready for review May 1, 2025 07:25 hebiao064 requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw and ch-wan as code owners May 1, 2025 07:25 zhyncs self-assigned this May 1, 2025 zhyncs reviewed May 1, 2025 View reviewed changes python/sglang/srt/layers/attention/flashattention_backend.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . trigger ci 48717a2 Hide details View details zhyncs merged commit 6fc1759 into sgl-project : main May 1, 2025 39 of 42 checks passed Uh oh! There was an error while loading. Please reload this page . hebiao064 mentioned this pull request May 1, 2025 Further Speed up FA3 Backend #5810 Closed 9 tasks RunkaiTao pushed a commit\n        to RunkaiTao/sglang\n      that referenced\n      this pull request May 9, 2025 Optimize a pad operation to accelerate 25us ( sgl-project#5945 ) 5b614fc pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request May 23, 2025 Rebase 4_6_post_4 to master_next ( sgl-project#47 ) \u2026 bc7d46c * Use device_id in dist init to reduce NCCL communicator warmup & creation overhead ( sgl-project#5728 )\n\n* [fix] fix potential bumpy throughtput with deepgemm ( sgl-project#5722 )\n\n* Resolves the `404 Not Found` error when running `compile_deep_gemm.py` in multi-node setups ( sgl-project#5720 )\n\n* perf: update H20 fused_moe_triton kernel config to get higher throughput during prefilling ( sgl-project#5716 )\n\n* we fix the non existent access of `decrypted_config_file` ( sgl-project#5685 )\n\n* CI: rewrite test_vision_chunked_prefill to speedup ( sgl-project#5682 )\n\n* Fuse MLA set kv cache kernel ( sgl-project#5748 )\n\n* Update amd docker image to `sglang:v0.4.5.post3-rocm630`. ( sgl-project#5697 )\n\n* [feature] support for roberta embedding models ( sgl-project#5730 )\n\n* [fix] fix bench_one_batch_server ( sgl-project#5607 )\n\n* support for the DeepSeek model by enabling streaming response parsing ( sgl-project#5592 )\n\n* fix: Use `is not None` instead of `!= None` for None checks. ( sgl-project#5687 )\n\n* Add Llama 4 to FA3 test ( sgl-project#5509 )\n\n* [misc] more decode step log for batch_one_batch ( sgl-project#5565 )\n\n* Handle JSONDecodeError while processing request data ( sgl-project#5599 )\n\n* fix(srt): check if sample_indices is not None before usage. ( sgl-project#5633 )\n\n* update llguidance to 0.7.11; adds StructTag ( sgl-project#4870 )\n\n* Use sgl-kernel sgl_per_token_group_quant_int8 ( sgl-project#4971 )\n\n* Add memory_saver check ( sgl-project#4986 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* add switch to disable open api doc ( sgl-project#3744 )\n\nSigned-off-by: congcongke <zhanweidu@163.com>\n\n* Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\" ( sgl-project#5772 )\n\n* Fix eagle test case ( sgl-project#5776 )\n\n* Split local attention test from fa3 test ( sgl-project#5774 )\n\n* Revert \"Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\"\" ( sgl-project#5777 )\n\n* Simplify FA3 tests ( sgl-project#5779 )\n\n* Revert \"[fix] fix bench_one_batch_server\" ( sgl-project#5785 )\n\n* Revert \"Use device_id in dist init to reduce NCCL communicator warmup & creation overhead\" ( sgl-project#5786 )\n\n* [CI] Tune threshold ( sgl-project#5787 )\n\n* [CI] fix port conflicts ( sgl-project#5789 )\n\n* [CI] Fix ci tests ( sgl-project#5769 )\n\n* [PD]Reduce kv transfer threads ( sgl-project#5791 )\n\n* [CI] Fix test case ( sgl-project#5790 )\n\n* Add 8-GPU Test for Deepseek-V3  ( sgl-project#5691 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* Release v0.4.6 ( sgl-project#5795 )\n\n* Update nightly-test.yml ( sgl-project#5797 )\n\n* [CI] Improve github summary & enable fa3 for more models ( sgl-project#5796 )\n\n* [Docs] update grafana setup guide in production metrics ( sgl-project#5643 )\n\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\n\n* [Misc] add structure logging, write to file and log tracing for SGL Router\n\n* Improve overlap scheduling ( sgl-project#5788 )\n\n* Add Cutlass MLA attention backend ( sgl-project#5390 )\n\n* chore: upgrade sgl-kernel 0.1.0 ( sgl-project#5690 )\n\n* Dockerfile.dev pip scikit_build_core ( sgl-project#5807 )\n\n* Add a doc to fix sgl-kernel build link error in py39 with ccache ( sgl-project#5809 )\n\n* Turn on overlap scheduler for multimodal models ( sgl-project#5771 )\n\n* Tiny refactor DefaultModelLoader.Source ( sgl-project#5482 )\n\n* [Docs] Replace lists with tables for cleanup and readability in server_arguments ( sgl-project#5276 )\n\n* Revert \"Tiny refactor DefaultModelLoader.Source\" ( sgl-project#5825 )\n\n* Feat: add support for thinking mode via chat_template_kwargs.enable_t\u2026 ( sgl-project#5551 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: fix the error where the content is None when reasoning and tool \u2026 ( sgl-project#5838 )\n\n* feat: Add fused moe triton config for qwen3 moe on h100 ( sgl-project#5833 )\n\n* fused moe triton tuning script support qwen3 ( sgl-project#5842 )\n\n* feat: Add fused moe triton config for qwen3bf16 moe on h20 ( sgl-project#5839 )\n\n* [PD] support pd fake transfer for warmup ( sgl-project#5726 )\n\n* [config] qwen3moe_tune_h20 fp8 tp4 ( sgl-project#5846 )\n\n* [Doc] Recover history of server_arguments.md ( sgl-project#5851 )\n\n* feat: Add fused moe triton config for qwen3-30b-fp8 moe on h20 ( sgl-project#5850 )\n\n* [CI] test chunked prefill more ( sgl-project#5798 )\n\n* ROCm: update AITER ( sgl-project#5816 )\n\n* [Feat] QWen-1M context support[1/2]: Update block sparse attention backend utils kernel ( sgl-project#5847 )\n\nCo-authored-by: sighingnow <sighingnow@gmail.com>\n\n* [Fix] Missing bootstrap_port field ( sgl-project#5823 )\n\n* feat: update is_fa3_default_architecture ( sgl-project#5854 )\n\n* add fused moe config for qwen3moe fp8/bf16 ( sgl-project#5849 )\n\n* chore: bump v0.4.6.post1 ( sgl-project#5845 )\n\n* Support `max_completion_tokens` for OpenAIChatCompletions ( sgl-project#5857 )\n\n* simplify fused_moe config logging ( sgl-project#5801 )\n\n* [CI] tune the test order to warmup the server ( sgl-project#5860 )\n\n* Cutlass MLA decode - fix dtype error ( sgl-project#5868 )\n\n* cutlass 3.9 supported to improve fp8_blockwise_gemm ( sgl-project#5820 )\n\n* [Feature] support auto chat template ( sgl-project#4949 )\n\n* Feat: support cuda graph for LoRA ( sgl-project#4115 )\n\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\n\n* Add qwen3 30b fused moe config ( sgl-project#5859 )\n\n* [Fix] Fix a bug for flashmla to run R1 model ( sgl-project#5875 )\n\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\n\n* Add A800 fused moe config for qwen3 30b ( sgl-project#5880 )\n\n* [Misc] add service discovery for sgl router\n\n* [fix]: PyO3 macOS linking and consolidate on tracing for logging\n\n* chore: update Dockerfile ( sgl-project#5894 )\n\n* [Docs] Update docs for Qwen3 and Qwen3MoE ( sgl-project#5836 )\n\n* [Doc] Tables instead of bulletpoints for sampling doc ( sgl-project#5841 )\n\n* chore: update CODEOWNERS ( sgl-project#5895 )\n\n* [FEATURE] Enhance platform compatibility for ARM ( sgl-project#5746 )\n\n* [CI] Add test_function_calling.py to run_suite.py ( sgl-project#5896 )\n\n* Auto set draft model path for MTP ( sgl-project#5793 )\n\n* [fix] relax mem_fraction_static for h200 ( sgl-project#5893 )\n\nCo-authored-by: alcanerian <alcanerian@gmail.com>\n\n* feat: support pythonic tool call and index in tool call streaming ( sgl-project#5725 )\n\n* [Bugfix]: fix missing queue_time_start for requests from grammar_queue ( sgl-project#5696 )\n\n* Add AMD MI300x Nightly Testing. ( sgl-project#5861 )\n\n* chore: use torch 2.6 for sgl-kernel build ( sgl-project#5898 )\n\n* Fix check_env script ( sgl-project#5901 )\n\n* [PD] Fix Assertion failed: /DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels sgl-project#134 ( sgl-project#5830 )\n\n* Bump Flashinfer to 0.2.5 ( sgl-project#5870 )\n\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\n\n* [Fix] Unload lora in HF_Runner if needed ( sgl-project#5899 )\n\n* Add A800 fused moe config for qwen3 235b ( sgl-project#5900 )\n\n* Add sm_120 for blackwell ( sgl-project#5903 )\n\n* [Feature] add support kimi vl model ( sgl-project#5383 )\n\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\n\n* support vlm benchmark profile ( sgl-project#5905 )\n\n* [fix] kimi-vl test in test_vision_openai_server.py ( sgl-project#5910 )\n\n* [Misc] use parallel build for cmake in sgl-kernel ( sgl-project#5919 )\n\n* [qwen3] support qwen3 ep moe ( sgl-project#5917 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\n\n* Add TP2 MOE benchmarks for AMD. ( sgl-project#5909 )\n\n* [Feat] Scale up fa3 kernel to sm8x arch ( sgl-project#5912 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* chore: bump sgl-kernel 0.1.1 ( sgl-project#5932 )\n\n* chore: upgrade sgl-kernel 0.1.1 ( sgl-project#5933 )\n\n* Remove unused method `calculate_num_image_tokens` from qwen2_vl.py ( sgl-project#5783 )\n\n* [PP] Add pipeline parallelism ( sgl-project#5724 )\n\n* Fix lora batch processing when input lora_path contains None ( sgl-project#5930 )\n\n* add Thor & Spark ( sgl-project#5915 )\n\n* fix: correct stream response when enable_thinking is set to false ( sgl-project#5881 )\n\n* fix: update model runner ( sgl-project#5934 )\n\n* chore: bump v0.4.6.post2 ( sgl-project#5939 )\n\n* Support XiaomiMiMo/MiMo model inference ( sgl-project#5921 )\n\n* [PD] Vectorise group_concurrent_contiguous in NumPy ( sgl-project#5834 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Remove extra contiguous ( sgl-project#5953 )\n\n* Update ci test and doc for MTP api change ( sgl-project#5952 )\n\n* docs: Fix Qwen model typo ( sgl-project#5944 )\n\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\n\n* Optimize a pad operation to accelerate 25us ( sgl-project#5945 )\n\n* Properly return error response in vertex_generate HTTP endpoint ( sgl-project#5956 )\n\n* feat: add concurrency evaluation logic in mmmu benchmark ( sgl-project#5782 )\n\n* Add 1 gpu perf and 2 gpu accuracy tests for AMD MI300x CI. ( sgl-project#5960 )\n\n* feat: Refactor DeepSeekV3 function call ( sgl-project#5908 )\n\n* Remove token in token out in Native API ( sgl-project#5967 )\n\n* Support InternVL3 ( sgl-project#5350 )\n\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\n\n* Support MMMU benchmark for  InternVL ( sgl-project#5968 )\n\n* FA3 speed up: skip len operation and get batch size directly from forward batch ( sgl-project#5969 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] NIXL backend Prefill TP & Decode TP+DP ( sgl-project#5681 )\n\n* Fix set kv cache multi-stream ( sgl-project#5975 )\n\n* Overlap qk norm with two streams ( sgl-project#5977 )\n\n* fix: only upgrade nccl for cu128 ( sgl-project#5986 )\n\n* Fix Phi3 serving which was broke by earlier change ( sgl-project#5991 )\n\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [perf] H100 DeepSeek-V3 fused moe tuned config ( sgl-project#5998 )\n\n* [Fix] Suppress dynamo logging when using flashinfer backend with torch compile ( sgl-project#5992 )\n\n* [Minor] Fix duplicate method definitions in conversation.py ( sgl-project#6012 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Fix flaky issues of lora and add multi batch tests ( sgl-project#5957 )\n\n* Tool Call: Add `chat_template_kwargs` documentation ( sgl-project#5679 )\n\n* fix: fix broadcast_pyobj breaking VerlEngine ( sgl-project#5997 )\n\n* [PD] Allow customizing reserved tokens to avoid KV cache waste ( sgl-project#6002 )\n\n* Update dev container config to support live code sync and improve docker setup guide   ( sgl-project#6018 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] Optimize disaggregation ib device help info ( sgl-project#5781 )\n\n* [Test] Add flashmla attention backend test ( sgl-project#5587 )\n\n* Fix \"Avoid computing lse in Ragged Prefill when there's no prefix match\" ( sgl-project#5555 )\n\n* feat: Add a unified merge_state API ( sgl-project#5428 )\n\n* feat: append more comprehensive fields in messages instead of merely role and content ( sgl-project#5996 )\n\n* [Security][Bug] Prevent binding to all TCP interfaces ( sgl-project#5752 )\n\n* Fix prefill OOM error in the case of large page size ( sgl-project#5081 )\n\n* Fix problem of large page size with chunked prefill ( sgl-project#6046 )\n\n* docs: add Google Cloud Vertex AI in Adoption and Sponsorship ( sgl-project#6047 )\n\n* docs: add new blog ( sgl-project#6048 )\n\n* Fix not \"import os\" ( sgl-project#6057 )\n\n* Better PD initialization ( sgl-project#5751 )\n\n* fix: deepep dockerfile, use pip install deepep. ( sgl-project#5885 )\n\n* [Fix] Fix and rename flashmla CI test ( sgl-project#6045 )\n\n* chore: upgrade cutlass 3.9.2 ( sgl-project#6004 )\n\nCo-authored-by: yizhang2077 <1109276519@qq.com>\n\n* Fix sgl-kernel build on aarch64 platforms ( sgl-project#6062 )\n\n* Add DeepEP to CI PR Test ( sgl-project#5655 )\n\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\n\n* fix custom_allreduce namespace ( sgl-project#6039 )\n\n* feat: add release workflow for SGLang kernels on aarch64 ( sgl-project#6010 )\n\nCo-authored-by: Qiaolin-Yu <liin1211@outlook.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* [Feature] Support for Ascend NPU backend ( sgl-project#3853 )\n\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\n\n* Fix the timeout for 8 gpu tests ( sgl-project#6084 )\n\n* Hint users DeepEP normal mode is incompatible with CUDA Graph ( sgl-project#5014 )\n\n* Super tiny fix doc ( sgl-project#5233 )\n\n* [Doc]Fix description for dp_size argument ( sgl-project#6063 )\n\n* feat(engine): add bootstrap parameters to generate methods (dynamo) ( sgl-project#6075 )\n\n* [refactor] slightly tidy fp8 module ( sgl-project#5993 )\n\n* Clean up fa3 test from 8 gpus ( sgl-project#6105 )\n\n* Deferring 8 GPU test ( sgl-project#6102 )\n\n* Update doc for MLA attention backends ( sgl-project#6034 )\n\n* Clean logs for DeepSeek-V3 launching ( sgl-project#6079 )\n\n* [CI]Add performance CI for VLM ( sgl-project#6038 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* adding Triton configs for DeepSeekV3 FusedMoE kernel on Blackwell ( sgl-project#6111 )\n\n* optimize pad operations in fa3 to accelarate 100+us ( sgl-project#6077 )\n\n* Overlap shared expert and routed expert computations ( sgl-project#5121 )\n\n* Tiny refactor ModelConfig.from_server_args ( sgl-project#5219 )\n\n* Tiny refactor weight loading logic ( sgl-project#5232 )\n\n* [PD] Add control to slow down a server ( sgl-project#5572 )\n\n* Change AMD test threshold ( sgl-project#6091 )\n\n* DeepEP normal support deepgemm-contiguous ( sgl-project#5626 )\n\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\n\n* [fix] fix pyproject.toml dependencies ( sgl-project#6119 )\n\n* [Feature] Add FlashAttention3 as a backend for VisionAttention ( sgl-project#5764 )\n\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\n\n* [perf] dsv3 bmm fallback to bf16 ( sgl-project#5662 )\n\n* [AMD] switch to custom allreduce regardless of MSCCL setting on ROCm ( sgl-project#6097 )\n\n* [sgl-kernel] fix: fix cu118 compile error ( sgl-project#6123 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* upgrade xgrammar to 0.1.19 ( sgl-project#6129 )\n\n* Remove unecessary is_fa3_supported check ( sgl-project#6112 )\n\n* chore: bump sgl-kernel 0.1.2 ( sgl-project#6131 )\n\n* docs: update README ( sgl-project#6132 )\n\n* [Fix] Incorrect Memory Allocation on CUDA:0 by Non-Zero CUDA Processes in TP/DP ( sgl-project#5745 )\n\n* Cutlass MLA: Disable split kv due to NVIDIA/cutlass#2274 ( sgl-project#6101 )\n\n* opt flashinfer mla cat ( sgl-project#5822 )\n\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\n\n* Update amd nightly concurrency. ( sgl-project#6141 )\n\n* feat: add thinking_budget ( sgl-project#6089 )\n\n* [Bugfix] Fix Llama4 gibberish output with long context and CUDA graph ( sgl-project#6162 )\n\n* fix bug that gpu0 occupies more memory when hicache is turned on ( sgl-project#5778 )\n\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* chore: bump v0.4.6.post3 ( sgl-project#6165 )\n\n* KV\u2011Cache\u202f(MHA, MLA): add missing start_layer\u202f/\u202fend_layer fields to MHATokenToKVPoolHost and MLATokenToKVPoolHost ( sgl-project#6016 )\n\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* [fix] fix determine_n_share_experts_fusion ( sgl-project#6118 )\n\n* Fix and Clean up chat-template requirement for VLM ( sgl-project#6114 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* [Docs]Delete duplicate content ( sgl-project#6146 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* Revert \"feat: add thinking_budget ( sgl-project#6089 )\" ( sgl-project#6181 )\n\n* Added async_encode method to Engine ( sgl-project#4701 )\n\n* Fix data parallel perf regression ( sgl-project#6183 )\n\n* Fix request abortion ( sgl-project#6184 )\n\n* Add typo checker in pre-commit ( sgl-project#6179 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* Remove duplicate IO Struct test ( sgl-project#6180 )\n\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\n\n* [PD] Add simple unit test for disaggregation feature ( sgl-project#5654 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Disabled deepep tests temporarily because it takes too much time. ( sgl-project#6186 )\n\n* feat: support loogle eval ( sgl-project#6190 )\n\n* [fix] remove mixtral from is_fa3_default_architecture ( sgl-project#6191 )\n\n* fix: handle None multimodal_inputs during merging and filtering batches in disaggregation decode mode ( sgl-project#6169 )\n\n* chore: upgrade deepgemm ( sgl-project#6073 )\n\n* chore: bump sgl-kernel v0.1.2.post1 ( sgl-project#6195 )\n\n* chore: upgrade sgl-kernel v0.1.2.post1 ( sgl-project#6196 )\n\nCo-authored-by: alcanderian <alcanderian@gmail.com>\n\n* Handle empty input string for embedding models ( sgl-project#5621 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* doc: fix the erroneous documents and example codes about Alibaba-NLP/gme-Qwen2-VL-2B-Instruct ( sgl-project#6199 )\n\n* [Docs] minor Qwen3 and reasoning parser docs fix ( sgl-project#6032 )\n\n* Improve structured outputs: fix race condition, server crash, metrics and style ( sgl-project#6188 )\n\n* [CI] Reorganize the 8 gpu tests ( sgl-project#6192 )\n\n* Add dev-deepep docker image ( sgl-project#6198 )\n\n* Replace time.time() to time.perf_counter() for benchmarking. ( sgl-project#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Update README.md ( sgl-project#6202 )\n\n* Fix release-docs.yml to not use python 3.9 ( sgl-project#6204 )\n\n* Fix start_profile does not support with_stack and record_shapes ( sgl-project#6043 )\n\n* [doc] add a note for --n-share-experts-fusion args ( sgl-project#6154 )\n\n* Performing Vocabulary Parallelism for LM Head across Attention TP Groups ( sgl-project#5558 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\n\n* Update AMD CI docker to v0.4.6.post3-rocm630. ( sgl-project#6213 )\n\n* Log if cuda graph is used & extend cuda graph capture to cuda-graph-max-bs ( sgl-project#6201 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* [CI] Fix PD mooncake dependency error ( sgl-project#6212 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Re-enable pd disaggregation test ( sgl-project#6231 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* fix some typos ( sgl-project#6209 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* [Docs] Add docs for `SGLANG_` and `SGL_` environment variables ( sgl-project#6206 )\n\n* [PP] Fix init_memory_pool desync & add PP for mixtral ( sgl-project#6223 )\n\n* Revert \"fix some typos\" ( sgl-project#6244 )\n\n* chore: add hf_xet dep ( sgl-project#6243 )\n\n* Update AMD nightly deps. ( sgl-project#6241 )\n\n* [PD] Add support for different TP sizes per DP rank ( sgl-project#5922 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Support incremental streaming of logprob/token_ids between scheduler and detokenizer ( sgl-project#6225 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* fix typo ( sgl-project#6248 )\n\n* Support tuning moe for llama 4 model ( sgl-project#6042 )\n\n* Skip the flaky test_stateful_custom_logit_processor ( sgl-project#6251 )\n\n* [Llama4] Add docs note about enable multimodal ( sgl-project#6235 )\n\n* [VERL Use Case] Add torch_memory_saver into deps ( sgl-project#6247 )\n\n* Fix two issues related to `--moe-dense-tp-size=1` ( sgl-project#5657 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\n\n* model(vlm): pixtral ( sgl-project#5084 )\n\n* [misc] deep_gemm fallback to NVRTC when NVCC not found ( sgl-project#6252 )\n\n* Enable MI325X AMD CI. ( sgl-project#6259 )\n\n* chore: bump v0.4.6.post4 ( sgl-project#6245 )\n\n* formatting fix for the rebased commit for 4.6.0_post4\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix issues in model runner and python packages\n\nfix for following issues:\n> vLLM dependency for xgrammar==0.1.17\n> 'Scheduler' object has no attribute 'device\n> 'pp_proxy_tensors' unexpected arg in HPUGraphRunner\n> TODO: Add pipeline parallelism support in HPUGraphRunner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix formatting in model runner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* base grammar fix for the is_terminated case\n\n>  'OutlinesGrammar' object has no attribute 'is_terminated'\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n---------\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: congcongke <zhanweidu@163.com>\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: Wenxuan Tan <wtan45@wisc.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: vzed <207368749+vincentzed@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DavidBao <121073073+DavidBao03@users.noreply.github.com>\nCo-authored-by: Frankey_8080 <32973306+Frank-Jie@users.noreply.github.com>\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: yan97ao <580776+yan97ao@users.noreply.github.com>\nCo-authored-by: aoshen524 <aoshen524@gmail.com>\nCo-authored-by: Micha\u0142 Moskal <michal@moskal.me>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: zhanweidu <zhanweidu@163.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Huapeng Zhou <73010314+PopSoda2002@users.noreply.github.com>\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\nCo-authored-by: Simo Lin <linsimo.mark@gmail.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: JiLi <leege233@gmail.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: PGFLMG <1106310035@qq.com>\nCo-authored-by: sighingnow <sighingnow@gmail.com>\nCo-authored-by: XTY <xutianyi1999@live.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: woodx <124784234+woodx9@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <qy254@cornell.edu>\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\nCo-authored-by: pengcuo <pengcbupt@163.com>\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Johnny <johnnync13@gmail.com>\nCo-authored-by: alcanerian <alcanerian@gmail.com>\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\nCo-authored-by: zhjunqin <zhjunqin@users.noreply.github.com>\nCo-authored-by: liwenju0 <like4hub@gmail.com>\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\nCo-authored-by: laixin <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: Ying Sheng <sqy1415@gmail.com>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: \u6c5f\u5bb6\u744b <36886416+JiangJiaWei1103@users.noreply.github.com>\nCo-authored-by: KCFindstr <shimakaze@google.com>\nCo-authored-by: xm:D <38322020+xiaomin-D@users.noreply.github.com>\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: Junrong Lin <33685709+ocss884@users.noreply.github.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: Hank Han <54751605+HanHan009527@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <liin1211@outlook.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\nCo-authored-by: Johnny <johnnynuca14@gmail.com>\nCo-authored-by: Song Zhang <70674731+botieking98@users.noreply.github.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\nCo-authored-by: ishandhanani <82981111+ishandhanani@users.noreply.github.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Minglei Zhu <mingleizhu1122@gmail.com>\nCo-authored-by: lukec <118525388+sleepcoo@users.noreply.github.com>\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\nCo-authored-by: Zhu Chen <51010608+Othame@users.noreply.github.com>\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: Yixin Dong <ubospica@gmail.com>\nCo-authored-by: xu-yfei <xu_yfei@qq.com>\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\nCo-authored-by: thyecust <tienhoayu@gmail.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: Simon (Jiyou) Li <Simon-Li@users.noreply.github.com>\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Steven Shimizu <shimizust@gmail.com>\nCo-authored-by: applesaucethebun <113181361+applesaucethebun@users.noreply.github.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nCo-authored-by: Yusong Gao <yusong.gao@gmail.com>\nCo-authored-by: alcanderian <alcanderian@gmail.com>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\nCo-authored-by: Kiv Chen <34561254+KivenChen@users.noreply.github.com> Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Optimize a pad operation to accelerate 25us ( sgl-project#5945 ) 75f87a7 xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Optimize a pad operation to accelerate 25us ( sgl-project#5945 ) 630f33d Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:58:17",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Optimize a pad operation to accelerate 25us (#5945)",
  "commit_message": "Optimize a pad operation to accelerate 25us (#5945)",
  "commit_date": "2025-05-01T10:48:55-07:00",
  "files_changed": [
    "python/sglang/srt/layers/attention/flashattention_backend.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 5,
    "num_files": 1,
    "num_hunks": 1,
    "num_non_test_edited_lines": 5,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 4e8543213..9579b19f2 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1587,8 +1587,9 @@ class FlashAttentionBackend(AttentionBackend):\n                 metadata.max_seq_len_k = max_len\n \n                 metadata.cache_seqlens_int32 = seq_lens.to(torch.int32)\n-                metadata.cu_seqlens_k = torch.nn.functional.pad(\n-                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32), (1, 0)\n+                # Optimize cumulative sequence length calculation\n+                metadata.cu_seqlens_k[1:].copy_(\n+                    torch.cumsum(seq_lens, dim=0, dtype=torch.int32)\n                 )\n \n                 max_seq_pages = (",
  "apis": [
    "sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend.init_forward_metadata"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/attention/flashattention_backend.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a non-test source file and updates a code section to replace a pad operation with an optimized cumulative sum copy, which accelerates the operation by 25 microseconds. The change is non-trivial and directly improves the performance of the cumulative sequence length calculation that affects a high-level API internally. It is a performance optimization rather than a bug fix, simple refactoring, or new feature addition.",
  "llm_api_reason": "The commit replaces a call to torch.nn.functional.pad with an in-place copy operation on the cu_seqlens_k tensor within the init_forward_metadata method of the FlashAttentionBackend class. This change is aimed at optimizing the cumulative sequence length computation for decode operations, thereby reducing a 25\u00b5s overhead. The optimization directly impacts the FlashAttentionBackend.init_forward_metadata API, which is used during model forward passes involving decoding operations."
}