{
  "commit_hash": "62757db6f0f09a6dff15b1ee1ac3029602951509",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1010",
  "pr_date": "2024-08-09",
  "timeline_text": "Copy link Collaborator hnyls2002 commented Aug 9, 2024 Thank you for your contribution, we really appreciate it. The following instructions will help improve your pull request and make it easier to receive feedback. If there are any items you don't understand, don't worry. Just submit the pull request and ask the maintainers for help. Motivation Even if the cache is disabled, there is a loop to iterate every request in the waiting queue. Modification Remove this loop, only do this when the mode is lpm or dfs-weight . Checklist Ensure pre-commit pre-commit run --all-files or other linting tools are used to fix potential lint issues. Confirm that modifications are covered by complete unit tests. If not, please add more unit tests for correctness. Modify documentation as needed, such as docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions hnyls2002 added 4 commits August 9, 2024 22:43 rename 3f3ced6 simplify code ab088b2 remove a loop 6b6e2d7 fix 36a13a7 Hide details View details hnyls2002 merged commit 62757db into main Aug 9, 2024 4 checks passed Uh oh! There was an error while loading. Please reload this page . hnyls2002 deleted the overhead-reduce-cache branch August 9, 2024 23:36 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Reduce the overhead when cache is disabled ( sgl-project#1010 ) 9f240c5 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:42",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NONE",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Reduce the overhead when cache is disabled (#1010)",
  "commit_message": "Reduce the overhead when cache is disabled (#1010)",
  "commit_date": "2024-08-09T16:36:57-07:00",
  "files_changed": [
    "python/sglang/srt/managers/policy_scheduler.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/managers/tp_worker.py",
    "python/sglang/srt/mem_cache/radix_cache.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 78,
    "num_files": 4,
    "num_hunks": 9,
    "num_non_test_edited_lines": 78,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 30a009c2e..a05ba9c9c 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -18,44 +18,40 @@ limitations under the License.\n import random\n from collections import defaultdict\n from contextlib import contextmanager\n+from typing import List\n \n from sglang.srt.managers.schedule_batch import Req, ScheduleBatch\n \n \n class PolicyScheduler:\n-    def __init__(\n-        self,\n-        policy,\n-        max_running_seqs,\n-        max_prefill_num_tokens,\n-        max_total_num_tokens,\n-        tree_cache,\n-    ):\n-        if tree_cache.disable and policy == \"lpm\":\n-            # LMP is meaningless when the tree cache is disabled.\n+    def __init__(self, policy, tree_cache):\n+        if tree_cache.disable and policy in [\"lpm\", \"dfs-weight\"]:\n+            # LPM and DFS-weight is meaningless when the tree cache is disabled.\n             policy = \"fcfs\"\n \n         self.policy = policy\n-        self.max_running_seqs = max_running_seqs\n-        self.max_prefill_num_tokens = max_prefill_num_tokens\n-        self.max_total_num_tokens = max_total_num_tokens\n         self.tree_cache = tree_cache\n \n-    def get_priority_queue(self, waiting_queue):\n+    def calc_priority(self, waiting_queue: List[Req]):\n+        if self.policy in [\"lpm\", \"dfs-weight\"]:\n+            # Compute matched prefix length\n+            for r in waiting_queue:\n+                # NOTE: the prefix_indices must always be aligned with last_node\n+                r.prefix_indices, r.last_node = self.tree_cache.match_prefix(\n+                    rid=r.rid, key=r.adjust_max_prefix_ids()\n+                )\n+\n         if self.policy == \"lpm\":\n-            # longest prefix match\n+            # Longest Prefix Match\n             waiting_queue.sort(key=lambda x: -len(x.prefix_indices))\n-            return waiting_queue\n         elif self.policy == \"fcfs\":\n             # first come first serve\n-            return waiting_queue\n+            pass\n         elif self.policy == \"lof\":\n             # longest output first\n             waiting_queue.sort(key=lambda x: -x.sampling_params.max_new_tokens)\n-            return waiting_queue\n         elif self.policy == \"random\":\n             random.shuffle(waiting_queue)\n-            return waiting_queue\n         elif self.policy == \"dfs-weight\":\n             last_node_to_reqs = defaultdict(list)\n             for req in waiting_queue:\n@@ -66,12 +62,13 @@ class PolicyScheduler:\n                 node_to_weight[node] = len(last_node_to_reqs[node])\n             self.calc_weight(self.tree_cache.root_node, node_to_weight)\n \n-            q = []\n+            waiting_queue.clear()\n             self.get_dfs_priority(\n-                self.tree_cache.root_node, node_to_weight, last_node_to_reqs, q\n+                self.tree_cache.root_node,\n+                node_to_weight,\n+                last_node_to_reqs,\n+                waiting_queue,\n             )\n-            assert len(q) == len(waiting_queue)\n-            return q\n         else:\n             raise ValueError(f\"Unknown schedule_policy: {self.policy}\")\n \n@@ -139,8 +136,6 @@ class PrefillAdder:\n         self.log_input_tokens += extend_input_len\n \n     def add_inflight_req(self, req: Req):\n-        req.input_ids = req.origin_input_ids + req.output_ids\n-        req.extend_input_len = len(req.input_ids) - len(req.prefix_indices)\n         truncated = req.extend_input_len > self.rem_chunk_tokens\n         req.extend_input_len = min(req.extend_input_len, self.rem_chunk_tokens)\n         req.input_ids = req.input_ids[: len(req.prefix_indices) + req.extend_input_len]\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 2489abd5d..278ed006e 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -164,7 +164,12 @@ class Req:\n     def finished(self) -> bool:\n         return self.finished_reason is not None\n \n+    def init_next_round_input(self):\n+        self.input_ids = self.origin_input_ids + self.output_ids\n+        self.extend_input_len = len(self.input_ids) - len(self.prefix_indices)\n+\n     def adjust_max_prefix_ids(self):\n+        self.input_ids = self.origin_input_ids + self.output_ids\n         input_len = len(self.input_ids)\n         max_prefix_len = input_len\n \ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 0228073c7..c66897710 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -165,13 +165,7 @@ class ModelTpServer:\n                 disable=server_args.disable_radix_cache,\n             )\n         self.tree_cache_metrics = {\"total\": 0, \"hit\": 0}\n-        self.scheduler = PolicyScheduler(\n-            self.schedule_policy,\n-            self.max_running_requests,\n-            self.max_prefill_tokens,\n-            self.max_total_num_tokens,\n-            self.tree_cache,\n-        )\n+        self.scheduler = PolicyScheduler(self.schedule_policy, self.tree_cache)\n         self.req_to_token_pool = self.model_runner.req_to_token_pool\n         self.token_to_kv_pool = self.model_runner.token_to_kv_pool\n \n@@ -373,17 +367,8 @@ class ModelTpServer:\n         if running_bs >= self.max_running_requests:\n             return None\n \n-        # Compute matched prefix length\n-        for req in self.waiting_queue:\n-            req.input_ids = req.origin_input_ids + req.output_ids\n-            # NOTE: the prefix_indices must always be aligned with last_node\n-            req.prefix_indices, req.last_node = self.tree_cache.match_prefix(\n-                rid=req.rid, key=req.adjust_max_prefix_ids()\n-            )\n-            req.extend_input_len = len(req.input_ids) - len(req.prefix_indices)\n-\n         # Get priority queue\n-        self.waiting_queue = self.scheduler.get_priority_queue(self.waiting_queue)\n+        self.scheduler.calc_priority(self.waiting_queue)\n \n         adder = PrefillAdder(\n             self.tree_cache,\n@@ -397,12 +382,13 @@ class ModelTpServer:\n \n         has_inflight = self.current_inflight_req is not None\n         if self.current_inflight_req is not None:\n+            self.current_inflight_req.init_next_round_input()\n             self.current_inflight_req = adder.add_inflight_req(\n                 self.current_inflight_req\n             )\n \n         for req in self.waiting_queue:\n-\n+            req.init_next_round_input()\n             res = adder.add_one_req(req)\n             if (\n                 not res\ndiff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py\nindex c23812049..05cbb2c92 100644\n--- a/python/sglang/srt/mem_cache/radix_cache.py\n+++ b/python/sglang/srt/mem_cache/radix_cache.py\n@@ -169,6 +169,9 @@ class RadixCache(BasePrefixCache):\n                 heapq.heappush(leaves, x.parent)\n \n     def inc_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n+\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 0:\n@@ -179,6 +182,9 @@ class RadixCache(BasePrefixCache):\n         return delta\n \n     def dec_lock_ref(self, node: TreeNode):\n+        if self.disable:\n+            return 0\n+\n         delta = 0\n         while node != self.root_node:\n             if node.lock_ref == 1:",
  "apis": [
    "PolicyScheduler.__init__",
    "PolicyScheduler.calc_priority",
    "Req.init_next_round_input",
    "RadixCache.inc_lock_ref",
    "RadixCache.dec_lock_ref"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/managers/tp_worker.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/schedule_batch.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The changes modify non-test source files, adjusting the flow in the PolicyScheduler (renaming get_priority_queue to calc_priority and refactoring the DFS branch) and adding early exits to functions in the radix cache when caching is disabled. These changes avoid unnecessary calculations (e.g., not computing matched prefix length when the tree cache is disabled) and simplify the scheduling logic, reducing overhead. The intent expressed (\"Reduce the overhead when cache is disabled\") indicates a performance optimization for CPU workflows. Overall, the modifications are nontrivial and focus on reducing unnecessary work in the scheduler and cache paths, thus qualifying as performance-related optimizations.",
  "llm_api_reason": "The commit simplifies and standardizes how scheduling and cache management work when the cache is disabled. In the PolicyScheduler class, the constructor\u2019s signature is changed to require fewer parameters, and the old get_priority_queue method is removed in favor of a new calc_priority method that computes prefix matches. In the ScheduleBatch module, a new init_next_round_input method is added to the Req class to update request inputs by concatenating origin and output ids and recalculating the extend length. In the tp_worker module, the scheduler is updated to use the new calc_priority method and to invoke the new init_next_round_input method. Finally, in the RadixCache class, the inc_lock_ref and dec_lock_ref methods are modified to immediately return when caching is disabled, reducing overhead."
}