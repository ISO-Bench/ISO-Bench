{
  "commit_hash": "148254d4db8bf3bffee23710cd1acbd5711ebd1b",
  "pr_url": "https://github.com/sgl-project/sglang/pull/2705",
  "pr_date": "2025-01-02",
  "timeline_text": "Copy link Contributor kkHuang-amd commented Jan 2, 2025 Motivation torch.sum could not use GPU core efficiency, implement specific kernel to enhance the performance Modifications change the base docker image and modify torch.sum to ops.moe_sum in fused_moe.py Checklist [+] Format your code according to the Contributor Guide . [+] Add unit tests as outlined in the Contributor Guide . [+] Update documentation as needed, including docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Improve moe reduce sum kernel performance 1ee403e kkHuang-amd requested review from zhyncs , ispobock and HaiShaw as code owners January 2, 2025 07:35 Copy link Contributor Author kkHuang-amd commented Jan 2, 2025 @HaiShaw : Please help to review it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . HaiShaw approved these changes Jan 2, 2025 View reviewed changes Copy link Collaborator HaiShaw left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM. @kkHuang-amd Thanks! Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Hide details View details HaiShaw merged commit 148254d into sgl-project : main Jan 2, 2025 15 checks passed Uh oh! There was an error while loading. Please reload this page . XiaotongJiang pushed a commit\n        to XiaotongJiang/sglang\n      that referenced\n      this pull request Jan 3, 2025 Improve moe reduce sum kernel performance ( sgl-project#2705 ) \u2026 972a7d5 Co-authored-by: wunhuang <wunhuang@amd.com> timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Improve moe reduce sum kernel performance ( sgl-project#2705 ) \u2026 2fb2ecf Co-authored-by: wunhuang <wunhuang@amd.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:53",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NONE",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Improve moe reduce sum kernel performance (#2705)",
  "commit_message": "Improve moe reduce sum kernel performance (#2705)\n\nCo-authored-by: wunhuang <wunhuang@amd.com>",
  "commit_date": "2025-01-02T01:11:06-08:00",
  "files_changed": [
    "docker/Dockerfile.rocm",
    "python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 18,
    "num_files": 2,
    "num_hunks": 2,
    "num_non_test_edited_lines": 18,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/docker/Dockerfile.rocm b/docker/Dockerfile.rocm\nindex 84ea69cc0..0c0b7e019 100644\n--- a/docker/Dockerfile.rocm\n+++ b/docker/Dockerfile.rocm\n@@ -2,7 +2,7 @@\n #   docker build --build-arg SGL_BRANCH=v0.4.1.post3 -t v0.4.1.post3-rocm620 -f Dockerfile.rocm .\n \n # default base image\n-ARG BASE_IMAGE=\"rocm/vllm-dev:20241031-tuned\"\n+ARG BASE_IMAGE=\"rocmshared/vllm-rocm:20241031-tuned\"\n \n FROM $BASE_IMAGE AS base\n USER root\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\nindex cbacd90c0..2a8080dd3 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\n@@ -854,11 +854,17 @@ def fused_experts_impl(\n             block_shape=block_shape,\n         )\n \n-        torch.sum(\n-            intermediate_cache3.view(*intermediate_cache3.shape),\n-            dim=1,\n-            out=out_hidden_states[begin_chunk_idx:end_chunk_idx],\n-        )\n+        if not_hip:\n+            torch.sum(\n+                intermediate_cache3.view(*intermediate_cache3.shape),\n+                dim=1,\n+                out=out_hidden_states[begin_chunk_idx:end_chunk_idx],\n+            )\n+        else:\n+            ops.moe_sum(\n+                intermediate_cache3.view(*intermediate_cache3.shape),\n+                out_hidden_states[begin_chunk_idx:end_chunk_idx],\n+            )\n     return out_hidden_states",
  "apis": [
    "sglang.srt.layers.moe.fused_moe_triton.fused_experts_impl",
    "sglang.srt.layers.moe.fused_moe_triton.fused_moe"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/sgl-kernel/python/sgl_kernel/fused_moe.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit message clearly states \"Improve moe reduce sum kernel performance\" and the code changes in the file \"python/sglang/srt/layers/moe/fused_moe_triton/fused_moe.py\" replace a call to torch.sum with a branch that uses a different operation (ops.moe_sum) when on HIP. This adjustment is explicitly aiming at enhancing performance for the reduce sum kernel. The changes are made in a non-test source file and directly impact a core computation while being testable on CPU. Hence, the commit satisfies the conditions for a performance optimization change.",
  "llm_api_reason": "This commit updates two parts of the code. First, it changes the base Docker image used for ROCm builds, which does not affect the Python APIs. Second, it improves the performance of the MoE reduce sum kernel in the fused Moe Triton implementation: within the internal function fused_experts_impl, the summing operation is now conditionally executed using either torch.sum (for non-HIP devices) or a custom ops.moe_sum call (for HIP devices). This change affects the behavior of the fused MoE kernel API used during model execution."
}