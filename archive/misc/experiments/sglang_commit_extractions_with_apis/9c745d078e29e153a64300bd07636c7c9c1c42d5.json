{
  "commit_hash": "9c745d078e29e153a64300bd07636c7c9c1c42d5",
  "pr_url": "https://github.com/sgl-project/sglang/pull/2056",
  "pr_date": "2024-11-18",
  "timeline_text": "Copy link Collaborator DarkSharpness commented Nov 17, 2024 Motivation Modifications Update the xgrammar-related constrained decoding part into a new API, including how to allocate vocab_mask , modify the vocab_mask and apply the vocab_mask to the logits . In addition, we now use the custom cuda kernel provided by xgrammar to mask the logits , which brings a significant improvement in e2e performance (see json_schema benchmark ) Checklist Format your code according to the Contributor Guide . Add unit tests as outlined in the Contributor Guide . Update documentation as needed, including docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions fix(xgrammar): update the xgrammar-related constrained decoding part \u2026 \u2026 8082322 \u2026to new API DarkSharpness requested review from merrymercy , hnyls2002 , Ying1123 , zhyncs and ispobock as code owners November 17, 2024 12:51 Merge branch 'main' into dark d16c652 merrymercy reviewed Nov 18, 2024 View reviewed changes python/sglang/srt/constrained/outlines_backend.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/constrained/xgrammar_backend.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/constrained/xgrammar_backend.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . merrymercy added 3 commits November 17, 2024 16:48 Update python/sglang/srt/constrained/outlines_backend.py 4816024 Update python/sglang/srt/constrained/xgrammar_backend.py 6b11834 Update python/sglang/srt/constrained/xgrammar_backend.py 601ece0 merrymercy added\n  the high priority label Nov 18, 2024 Hide details View details merrymercy merged commit 9c745d0 into sgl-project : main Nov 18, 2024 2 of 12 checks passed Uh oh! There was an error while loading. Please reload this page . timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 [Performance] Update xgrammar-related constrained decoding ( sgl-proje\u2026 \u2026 55bd673 \u2026ct#2056 ) DarkSharpness deleted the dark branch June 19, 2025 22:14 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:00",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance] Update xgrammar-related constrained decoding (#2056)",
  "commit_message": "[Performance] Update xgrammar-related constrained decoding (#2056)",
  "commit_date": "2024-11-17T16:58:49-08:00",
  "files_changed": [
    "python/sglang/srt/constrained/outlines_backend.py",
    "python/sglang/srt/constrained/xgrammar_backend.py",
    "python/sglang/srt/model_executor/model_runner.py",
    "python/sglang/srt/sampling/sampling_batch_info.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 70,
    "num_files": 4,
    "num_hunks": 10,
    "num_non_test_edited_lines": 70,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/constrained/outlines_backend.py b/python/sglang/srt/constrained/outlines_backend.py\nindex cc68b97f8..831c1d1a9 100644\n--- a/python/sglang/srt/constrained/outlines_backend.py\n+++ b/python/sglang/srt/constrained/outlines_backend.py\n@@ -81,10 +81,20 @@ class OutlinesGrammar(BaseGrammarObject):\n     ):\n         self.state = next_state\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        return torch.zeros(batch_size, vocab_size, dtype=torch.bool, device=device)\n+\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int) -> None:\n+        vocab_mask = vocab_mask[idx]\n         vocab_mask.fill_(1)\n         vocab_mask[self.guide.get_next_instruction(self.state).tokens] = 0\n \n+    @staticmethod\n+    def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor):\n+        logits.masked_fill_(vocab_mask, float(\"-inf\"))\n+\n     def copy(self):\n         return OutlinesGrammar(self.guide, self.jump_forward_map)\n \ndiff --git a/python/sglang/srt/constrained/xgrammar_backend.py b/python/sglang/srt/constrained/xgrammar_backend.py\nindex ab4df5c98..acaae10c0 100644\n--- a/python/sglang/srt/constrained/xgrammar_backend.py\n+++ b/python/sglang/srt/constrained/xgrammar_backend.py\n@@ -21,7 +21,12 @@ from typing import List, Tuple\n import torch\n \n try:\n-    from xgrammar import CachedGrammarCompiler, CompiledGrammar, GrammarMatcher\n+    from xgrammar import (\n+        CachedGrammarCompiler,\n+        CompiledGrammar,\n+        GrammarMatcher,\n+        TokenizerInfo,\n+    )\n \n     import_error = None\n except ImportError as e:\n@@ -80,19 +85,23 @@ class XGrammarGrammar(BaseGrammarObject):\n         for i in range(k, len(new_output_ids)):\n             assert self.matcher.accept_token(new_output_ids[i])\n \n-    def fill_vocab_mask(self, vocab_mask: torch.Tensor):\n-        # Note that this bitmask is a bitset, not bool\n-        bitmask = self.matcher.get_next_token_bitmask()\n-        # Mask the tokens that are not allowed\n-        vocab_mask[\n-            self.matcher.get_rejected_tokens_from_bitmask(bitmask, self.vocab_size)\n-        ] = 1\n+    def allocate_vocab_mask(\n+        self, vocab_size: int, batch_size: int, device\n+    ) -> torch.Tensor:\n+        return self.matcher.allocate_token_bitmask(vocab_size, batch_size)\n+\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, idx: int) -> None:\n+        self.matcher.fill_next_token_bitmask(vocab_mask, idx)\n+\n+    @staticmethod\n+    def apply_vocab_mask(logits: torch.Tensor, vocab_mask: torch.Tensor) -> None:\n+        GrammarMatcher.apply_token_bitmask_inplace(logits, vocab_mask)\n \n     def copy(self):\n         matcher = GrammarMatcher(\n             self.ctx,\n             max_rollback_tokens=MAX_ROLLBACK_TOKENS,\n-            mask_vocab_size=self.vocab_size,\n+            vocab_size=self.vocab_size,\n         )\n         return XGrammarGrammar(matcher, self.vocab_size, self.ctx)\n \n@@ -112,7 +121,8 @@ class XGrammarGrammarBackend(BaseGrammarBackend):\n             self.grammar_cache = None\n             return\n \n-        self.grammar_cache = CachedGrammarCompiler(tokenizer_or_vocab=tokenizer)\n+        tokenizer_info = TokenizerInfo.from_huggingface(tokenizer)\n+        self.grammar_cache = CachedGrammarCompiler(tokenizer_info=tokenizer_info)\n         self.vocab_size = vocab_size\n \n     def init_value_impl(self, key: Tuple[str, str]) -> XGrammarGrammar:\n@@ -122,9 +132,7 @@ class XGrammarGrammarBackend(BaseGrammarBackend):\n         key_type, key_string = key\n         if key_type == \"json\":\n             try:\n-                ctx = self.grammar_cache.get_compiled_grammar_for_json_schema(\n-                    key_string\n-                )\n+                ctx = self.grammar_cache.compile_json_schema_grammar(schema=key_string)\n             except RuntimeError as e:\n                 logging.warning(\n                     f\"Skip invalid json_schema: json_schema={key_string}, {e=}\"\n@@ -141,7 +149,7 @@ class XGrammarGrammarBackend(BaseGrammarBackend):\n         matcher = GrammarMatcher(\n             ctx,\n             max_rollback_tokens=MAX_ROLLBACK_TOKENS,\n-            mask_vocab_size=self.vocab_size,\n+            vocab_size=self.vocab_size,\n         )\n         return XGrammarGrammar(matcher, self.vocab_size, ctx)\n \ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 02750d5df..8096fec5a 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -645,7 +645,7 @@ class ModelRunner:\n \n         # Apply regex vocab_mask\n         if sampling_info.vocab_mask is not None:\n-            logits = logits.masked_fill(sampling_info.vocab_mask, float(\"-inf\"))\n+            sampling_info.apply_mask(logits=logits, vocab_mask=sampling_info.vocab_mask)\n \n         return logits\n \ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex a341c2b17..61aa341fd 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import dataclasses\n-from typing import TYPE_CHECKING, List, Optional\n+from typing import TYPE_CHECKING, Callable, List, Optional\n \n import torch\n \n@@ -29,7 +29,7 @@ class SamplingBatchInfo:\n     vocab_size: int\n     logit_bias: torch.Tensor = None\n     vocab_mask: Optional[torch.Tensor] = None\n-\n+    apply_mask: Optional[Callable[[torch.Tensor, torch.Tensor], None]] = None\n     grammars: Optional[List] = None\n \n     # Penalizer\n@@ -135,17 +135,23 @@ class SamplingBatchInfo:\n     def update_regex_vocab_mask(self):\n         if not self.grammars or not any(grammar for grammar in self.grammars):\n             self.vocab_mask = None\n+            self.apply_mask = None\n             return\n \n-        self.vocab_mask = torch.zeros(\n-            len(self.temperatures),\n-            self.vocab_size,\n-            dtype=torch.bool,\n+        # find a grammar from the list\n+        grammar = next(grammar for grammar in self.grammars if grammar is not None)\n+\n+        # maybe we can reuse the existing mask?\n+        self.vocab_mask = grammar.allocate_vocab_mask(\n+            vocab_size=self.vocab_size,\n+            batch_size=len(self.temperatures),\n             device=self.device,\n         )\n+        self.apply_mask = type(grammar).apply_vocab_mask  # force to use static method\n+\n         for i, grammar in enumerate(self.grammars):\n             if grammar is not None:\n-                grammar.fill_vocab_mask(self.vocab_mask[i])\n+                grammar.fill_vocab_mask(self.vocab_mask, i)\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:",
  "apis": [
    "sglang.srt.constrained.outlines_backend.OutlinesGrammar.allocate_vocab_mask",
    "sglang.srt.constrained.outlines_backend.OutlinesGrammar.fill_vocab_mask",
    "sglang.srt.constrained.xgrammar_backend.XGrammarGrammar.allocate_vocab_mask",
    "sglang.srt.constrained.xgrammar_backend.XGrammarGrammar.fill_vocab_mask",
    "sglang.srt.sampling.sampling_batch_info.SamplingBatchInfo.update_regex_vocab_mask"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/constrained/outlines_backend.py",
    "/path/to/repos/sglang/python/sglang/srt/constrained/xgrammar_backend.py",
    "/path/to/repos/sglang/python/sglang/srt/model_executor/model_runner.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes modifications to several core source files related to constrained decoding (e.g., outlines_backend.py, xgrammar_backend.py, model_runner.py, and sampling_batch_info.py). The updates include the separation of mask allocation from its filling and the introduction of a dedicated static method for applying the vocab mask, which are changes aimed at the efficiency of the decoding process. Although the commit message simply tags the changes with \"[Performance]\" and does not use explicit optimization keywords, the rewritten routines affect the constrained decoding\u2014typically a performance-critical routine in model inference. The changes are not trivial (they reorganize how vocab masks are handled) and are applied to non-test source code without merely being refactors or bug fixes. Thus, the commit meets the conditions for a performance-related optimization change that influences high-level APIs on the CPU.",
  "llm_api_reason": "This commit improves the constrained decoding functionality by updating how vocabulary masks are allocated and applied in both the outlines and xgrammar backends. It replaces the previous implementation of fill_vocab_mask with a new version that accepts additional parameters (e.g. an index) and introduces a new allocate_vocab_mask method along with a static apply_vocab_mask in both OutlinesGrammar (in outlines_backend.py) and XGrammarGrammar (in xgrammar_backend.py). Additionally, it adjusts calls to masking in the model runner and sampling batch info, ensuring that the new API is used. All of these changes modify the high-level constrained-decoding APIs and how the vocabulary masking logic is exposed to the rest of the system."
}