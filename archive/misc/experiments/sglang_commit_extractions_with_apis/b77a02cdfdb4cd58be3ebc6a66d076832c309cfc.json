{
  "commit_hash": "b77a02cdfdb4cd58be3ebc6a66d076832c309cfc",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1752",
  "pr_date": "2024-10-25",
  "timeline_text": "Copy link Collaborator DarkSharpness commented Oct 22, 2024 Motivation This pull request is a continuation of PR #1680 . This pull request allows users to choose between xgrammar and outlines . Modifications Key modifications include: Added support for both xgrammar and outlines as grammar backends. Users can select their preferred backend by passing the grammar-backend option in the command line. The default backend is set to outlines . Checklist Format your code according to the Contributor Guide . Add unit tests as outlined in the Contributor Guide . Update documentation as needed, including docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, merrymercy, and Ubospica reacted with thumbs up emoji \ud83d\ude80 3 ByronHsu, Ubospica, and roG0d reacted with rocket emoji All reactions \ud83d\udc4d 3 reactions \ud83d\ude80 3 reactions DarkSharpness added 8 commits October 19, 2024 12:12 feat(xgrammar): support xgrammar as one of the grammar backends 8bc804c fix: fix wrongly clearing the vocab_mask of outlines cae33a9 minor: fix the format by running pre-commit 1b17c72 Merge branch 'main' into xgrammar-outlines b23f632 fix: set the object to error when import failed d93f76e minor: set the default grammar backend as outlines ee43065 Merge branch 'main' into xgrammar-outlines 652ef54 Merge branch 'main' into xgrammar-outlines 83d1502 DarkSharpness requested review from merrymercy , hnyls2002 , Ying1123 , zhyncs , ispobock and ByronHsu as code owners October 22, 2024 07:56 merrymercy mentioned this pull request Oct 23, 2024 [Performance] Support xgrammar for faster constrained decoding #1680 Closed 3 tasks merrymercy requested changes Oct 23, 2024 View reviewed changes python/sglang/srt/managers/schedule_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/managers/schedule_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/managers/schedule_batch.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/managers/scheduler.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . DarkSharpness added 2 commits October 23, 2024 15:24 Merge branch 'main' into xgrammar-outlines 5ce813c refactor(constrained): add a new abstraction for constrained decoding b8648dd merrymercy mentioned this pull request Oct 23, 2024 Development Roadmap (2024 Q4) #1487 Closed 37 tasks DarkSharpness and others added 8 commits October 25, 2024 00:00 minor(constrained): set import failure object as None to pass type check e615ce3 fix(constrained): use DummyType to avoid type failure in 'isinstance' cd59ed0 fix(constrained): fix wrong parameter order in initing bnf_cache d01e7af Merge branch 'main' into xgrammar-outlines e1de402 minor: format the code using pre-commit c07cd0d fix(constrained): fix wrong jump-forward assertion 8608c2b minor: format the code using pre-commit cbdca83 Merge branch 'main' into xgrammar-outlines bb0b28d merrymercy approved these changes Oct 25, 2024 View reviewed changes Merge branch 'main' into xgrammar-outlines bed1f3d merrymercy enabled auto-merge (squash) October 25, 2024 21:30 Hide details View details merrymercy merged commit b77a02c into sgl-project : main Oct 25, 2024 12 checks passed Uh oh! There was an error while loading. Please reload this page . zhaochenyang20 mentioned this pull request Mar 3, 2025 Development Roadmap (2025 H1) #4035 Closed 22 tasks timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 [Performance] Support both xgrammar and outlines for constrained deco\u2026 \u2026 9bfefbe \u2026ding ( sgl-project#1752 ) DarkSharpness deleted the xgrammar-outlines branch June 19, 2025 22:14 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:04",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance] Support both xgrammar and outlines for constrained decoding (#1752)",
  "commit_message": "[Performance] Support both xgrammar and outlines for constrained decoding (#1752)",
  "commit_date": "2024-10-25T21:47:02Z",
  "files_changed": [
    "python/sglang/srt/constrained/__init__.py",
    "python/sglang/srt/constrained/bnf_cache.py",
    "python/sglang/srt/constrained/grammar.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/managers/scheduler.py",
    "python/sglang/srt/sampling/sampling_batch_info.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 402,
    "num_files": 7,
    "num_hunks": 28,
    "num_non_test_edited_lines": 402,
    "num_non_test_files": 7,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/constrained/__init__.py b/python/sglang/srt/constrained/__init__.py\nindex c47c5c8dd..a8708dfea 100644\n--- a/python/sglang/srt/constrained/__init__.py\n+++ b/python/sglang/srt/constrained/__init__.py\n@@ -51,6 +51,21 @@ except ImportError:\n         return build_regex_from_schema(schema, whitespace_pattern)\n \n \n+try:\n+    from xgrammar import (\n+        GrammarMatcher,\n+        GrammarMatcherInitContext,\n+        GrammarMatcherInitContextCache,\n+    )\n+except ImportError as e:\n+\n+    class Dummy:\n+        pass\n+\n+    GrammarMatcher = Dummy\n+    GrammarMatcherInitContext = Dummy\n+    GrammarMatcherInitContextCache = Dummy\n+\n __all__ = [\n     \"RegexGuide\",\n     \"FSMInfo\",\n@@ -60,4 +75,7 @@ __all__ = [\n     \"disk_cache\",\n     \"disable_cache\",\n     \"make_byte_level_fsm\",\n+    \"GrammarMatcher\",\n+    \"GrammarMatcherInitContext\",\n+    \"GrammarMatcherInitContextCache\",\n ]\ndiff --git a/python/sglang/srt/constrained/bnf_cache.py b/python/sglang/srt/constrained/bnf_cache.py\nnew file mode 100644\nindex 000000000..19765731b\n--- /dev/null\n+++ b/python/sglang/srt/constrained/bnf_cache.py\n@@ -0,0 +1,61 @@\n+\"\"\"\n+Copyright 2023-2024 SGLang Team\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\"\"\"\n+\n+\"\"\"Cache for the compressed finite state machine.\"\"\"\n+\n+from typing import Tuple\n+\n+from transformers import AutoTokenizer\n+\n+from sglang.srt.constrained import (\n+    GrammarMatcher,\n+    GrammarMatcherInitContext,\n+    GrammarMatcherInitContextCache,\n+)\n+\n+MAX_ROLLBACK_TOKENS = 10\n+\n+\n+class BNFCache:\n+    grammar_cache: GrammarMatcherInitContextCache\n+\n+    def __init__(\n+        self,\n+        tokenizer_path,\n+        tokenizer_args_dict,\n+        skip_tokenizer_init=False,\n+        whitespace_patterns=None,\n+    ):\n+        # TODO(dark): how to deal with whitespace_patterns and skip_tokenizer_init\n+        if skip_tokenizer_init:\n+            return\n+\n+        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, **tokenizer_args_dict)\n+        self.grammar_cache = GrammarMatcherInitContextCache(\n+            tokenizer_or_vocab=tokenizer\n+        )\n+\n+    def get_context(self, key: Tuple[str, str]) -> GrammarMatcherInitContext:\n+        key_type, key_string = key\n+        if key_type == \"json\":\n+            return self.grammar_cache.get_init_context_for_json_schema(key_string)\n+        elif key_type == \"regex\":\n+            raise ValueError(f\"regex hasn't been supported by xgrammar yet\")\n+        else:\n+            raise ValueError(f\"Invalid key_type: {key_type}\")\n+\n+    def query(self, key: Tuple[str, str], vocab_size: int) -> GrammarMatcher:\n+        ctx = self.get_context(key)\n+        return GrammarMatcher(\n+            ctx, max_rollback_tokens=MAX_ROLLBACK_TOKENS, mask_vocab_size=vocab_size\n+        )\ndiff --git a/python/sglang/srt/constrained/grammar.py b/python/sglang/srt/constrained/grammar.py\nnew file mode 100644\nindex 000000000..0281539b8\n--- /dev/null\n+++ b/python/sglang/srt/constrained/grammar.py\n@@ -0,0 +1,190 @@\n+\"\"\"\n+Copyright 2023-2024 SGLang Team\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+    http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\"\"\"\n+\n+\"\"\"Cache for the compressed finite state machine.\"\"\"\n+import logging\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+\n+from sglang.srt.constrained import GrammarMatcher, RegexGuide\n+from sglang.srt.constrained.bnf_cache import BNFCache\n+from sglang.srt.constrained.fsm_cache import FSMCache\n+from sglang.srt.constrained.jump_forward import JumpForwardCache, JumpForwardMap\n+\n+# from sglang.srt.managers.schedule_batch import Req\n+\n+logger = logging.getLogger(__name__)\n+\n+INIT_INCREMENTAL_DETOKENIZATION_OFFSET = 5\n+\n+\n+class XGrammarJump:\n+    pass\n+\n+\n+class JumpHelper:\n+    data: Union[List, str]\n+    state: int\n+    suffix_ids: List[int]\n+\n+    def __init__(\n+        self, data: Union[List, str] = \"\", state: int = -1, suffix_ids=[]\n+    ) -> None:\n+        self.data = data\n+        self.state = state\n+        self.suffix_ids = suffix_ids\n+\n+    def can_jump(self):\n+        return len(self.data) > 0\n+\n+\n+class Grammar:\n+    grammar: Union[GrammarMatcher, Tuple[RegexGuide, int]]\n+    jump_map: Union[XGrammarJump, JumpForwardMap, None]\n+\n+    def __init__(\n+        self,\n+        grammar: Union[GrammarMatcher, Tuple[RegexGuide, int]],\n+        jump_map: Union[XGrammarJump, JumpForwardMap, None],\n+    ) -> None:\n+        self.grammar = grammar\n+        self.jump_map = jump_map\n+\n+    def accept_token(self, token: int):\n+        if isinstance(self.grammar, GrammarMatcher):\n+            assert self.grammar.accept_token(token)\n+        else:\n+            guide, state = self.grammar\n+            self.grammar = guide, guide.get_next_state(state, token)\n+\n+    def try_jump(self, tokenizer) -> JumpHelper:\n+        if isinstance(self.jump_map, XGrammarJump):\n+            assert isinstance(self.grammar, GrammarMatcher)\n+            return JumpHelper(self.grammar.find_jump_forward_string())\n+        elif isinstance(self.jump_map, JumpForwardMap):\n+            assert isinstance(self.grammar, Tuple)\n+\n+            _, state = self.grammar\n+            jump_forward_bytes = self.jump_map.jump_forward_byte(state)\n+            if jump_forward_bytes is None or len(jump_forward_bytes) == 0:\n+                return JumpHelper()  # can't jump\n+\n+            # preprocess the jump forward string\n+            suffix_bytes = []\n+            continuation_range = range(0x80, 0xC0)\n+            cur_state = state\n+            while (\n+                len(jump_forward_bytes)\n+                and jump_forward_bytes[0][0] in continuation_range\n+            ):\n+                # continuation bytes\n+                byte_edge = jump_forward_bytes.pop(0)\n+                suffix_bytes.append(byte_edge[0])\n+                cur_state = byte_edge[1]\n+\n+            suffix_tokens = [f\"<0x{hex(b)[2:].upper()}>\" for b in suffix_bytes]\n+            suffix_ids = tokenizer.convert_tokens_to_ids(suffix_tokens)\n+            return JumpHelper(suffix_ids, cur_state, suffix_bytes)\n+        else:\n+            return JumpHelper()  # can't jump\n+\n+    def jump_forward_str_state(self, helper: JumpHelper) -> Tuple[str, int]:\n+        if isinstance(helper.data, str):\n+            return helper.data, -1\n+        else:\n+            assert isinstance(self.jump_map, JumpForwardMap)\n+            return self.jump_map.jump_forward_symbol(helper.state)\n+\n+    def jump_and_retokenize(\n+        self, old_output_ids: List[int], new_output_ids: List[int], next_state: int\n+    ):\n+        if isinstance(self.grammar, GrammarMatcher):\n+            k = 0\n+            for i, old_id in enumerate(old_output_ids):\n+                if old_id == new_output_ids[i]:\n+                    k = i + 1\n+                else:\n+                    break\n+\n+            # rollback to the last token that is the same\n+            if k < len(old_output_ids):\n+                self.grammar.rollback(len(old_output_ids) - k)\n+\n+            for i in range(k, len(new_output_ids)):\n+                assert self.grammar.accept_token(new_output_ids[i])\n+        else:\n+            self.grammar = self.grammar[0], next_state\n+\n+    def fill_vocab_mask(self, vocab_mask: torch.Tensor, vocab_size: int):\n+        if isinstance(self.grammar, GrammarMatcher):\n+            # Note that this bitmask is a bitset, not bool\n+            bitmask = self.grammar.find_next_token_bitmask()\n+            # Mask the tokens that are not allowed\n+            vocab_mask[\n+                self.grammar.get_rejected_tokens_from_bitmask(bitmask, vocab_size)\n+            ] = 1\n+        else:\n+            guide, state = self.grammar\n+            vocab_mask.fill_(1)\n+            vocab_mask[guide.get_next_instruction(state).tokens] = 0\n+\n+\n+class GrammarCache:\n+    grammar_cache: Union[BNFCache, FSMCache]\n+    jump_cache: Union[XGrammarJump, JumpForwardCache, None]\n+\n+    def __init__(\n+        self,\n+        tokenizer_path,\n+        tokenizer_args_dict,\n+        skip_tokenizer_init=False,\n+        whitespace_patterns=None,\n+        backend=None,\n+        allow_jump=False,\n+    ):\n+        if backend == \"xgrammar\":\n+            self.grammar_cache = BNFCache(\n+                tokenizer_path=tokenizer_path,\n+                tokenizer_args_dict=tokenizer_args_dict,\n+                skip_tokenizer_init=skip_tokenizer_init,\n+                whitespace_patterns=whitespace_patterns,\n+            )\n+            self.jump_cache = XGrammarJump() if allow_jump else None\n+        else:\n+            assert backend == \"outlines\"\n+            self.grammar_cache = FSMCache(\n+                tokenizer_path=tokenizer_path,\n+                tokenizer_args_dict=tokenizer_args_dict,\n+                skip_tokenizer_init=skip_tokenizer_init,\n+                constrained_json_whitespace_pattern=whitespace_patterns,\n+                enable=True,\n+            )\n+            self.jump_cache = JumpForwardCache() if allow_jump else None\n+\n+    def query(self, key: Tuple[str, str], vocab_size: int) -> Grammar:\n+        if isinstance(self.grammar_cache, BNFCache):\n+            assert not isinstance(self.jump_cache, JumpForwardCache)\n+            return Grammar(self.grammar_cache.query(key, vocab_size), self.jump_cache)\n+        else:\n+            jump_map = None\n+            guide, regex = self.grammar_cache.query(key)\n+            if isinstance(self.jump_cache, JumpForwardCache):\n+                jump_map = self.jump_cache.query(regex)\n+            return Grammar((guide, 0), jump_map)\n+\n+    def reset(self):\n+        if isinstance(self.grammar_cache, FSMCache):\n+            self.grammar_cache.reset()\n+        if isinstance(self.jump_cache, JumpForwardCache):\n+            self.jump_cache.reset()\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fcd06d8cc..85ca560a9 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -37,8 +37,7 @@ import torch\n \n from sglang.global_config import global_config\n from sglang.srt.configs.model_config import ModelConfig\n-from sglang.srt.constrained import RegexGuide\n-from sglang.srt.constrained.jump_forward import JumpForwardMap\n+from sglang.srt.constrained.grammar import Grammar\n from sglang.srt.mem_cache.base_prefix_cache import BasePrefixCache\n from sglang.srt.mem_cache.chunk_cache import ChunkCache\n from sglang.srt.mem_cache.memory_pool import BaseTokenToKVPool, ReqToTokenPool\n@@ -247,9 +246,7 @@ class Req:\n         self.embedding = None\n \n         # Constrained decoding\n-        self.regex_fsm: RegexGuide = None\n-        self.regex_fsm_state: int = 0\n-        self.jump_forward_map: JumpForwardMap = None\n+        self.grammar: Optional[Grammar] = None\n \n         # For Qwen2-VL\n         self.mrope_position_delta = []  # use mutable object\n@@ -359,6 +356,8 @@ class Req:\n                     return\n \n     def jump_forward_and_retokenize(self, jump_forward_str, next_state):\n+        assert self.grammar is not None and self.tokenizer is not None\n+\n         if self.origin_input_text is None:\n             # Recovering text can only use unpadded ids\n             self.origin_input_text = self.tokenizer.decode(\n@@ -398,7 +397,8 @@ class Req:\n                 self.surr_offset = self.read_offset - i\n                 break\n \n-        self.regex_fsm_state = next_state\n+        # update the inner state of the grammar\n+        self.grammar.jump_and_retokenize(old_output_ids, self.output_ids, next_state)\n \n         if self.return_logprob:\n             # For fast-forward part's logprobs\n@@ -468,8 +468,8 @@ class ScheduleBatch:\n     # Stream\n     has_stream: bool = False\n \n-    # Has regex\n-    has_regex: bool = False\n+    # Has grammar\n+    has_grammar: bool = False\n \n     # device\n     device: str = \"cuda\"\n@@ -477,7 +477,7 @@ class ScheduleBatch:\n     @classmethod\n     def init_new(\n         cls,\n-        reqs,\n+        reqs: List[Req],\n         req_to_token_pool,\n         token_to_kv_pool,\n         tree_cache,\n@@ -491,7 +491,7 @@ class ScheduleBatch:\n             model_config=model_config,\n             return_logprob=any(req.return_logprob for req in reqs),\n             has_stream=any(req.stream for req in reqs),\n-            has_regex=any(req.regex_fsm for req in reqs),\n+            has_grammar=any(req.grammar for req in reqs),\n             device=req_to_token_pool.device,\n         )\n \n@@ -803,26 +803,10 @@ class ScheduleBatch:\n         keep_indices = set(i for i in range(len(self.reqs)))\n \n         for i, req in enumerate(self.reqs):\n-            if req.jump_forward_map is not None:\n-                jump_forward_bytes = req.jump_forward_map.jump_forward_byte(\n-                    req.regex_fsm_state\n-                )\n-                if jump_forward_bytes is not None and len(jump_forward_bytes) > 1:\n-                    suffix_bytes = []\n-                    continuation_range = range(0x80, 0xC0)\n-                    cur_state = req.regex_fsm_state\n-                    while (\n-                        len(jump_forward_bytes)\n-                        and jump_forward_bytes[0][0] in continuation_range\n-                    ):\n-                        # continuation bytes\n-                        byte_edge = jump_forward_bytes.pop(0)\n-                        suffix_bytes.append(byte_edge[0])\n-                        cur_state = byte_edge[1]\n-\n-                    suffix_tokens = [f\"<0x{hex(b)[2:].upper()}>\" for b in suffix_bytes]\n-                    suffix_ids = req.tokenizer.convert_tokens_to_ids(suffix_tokens)\n-\n+            if req.grammar is not None:\n+                jump_helper = req.grammar.try_jump(req.tokenizer)\n+                if jump_helper.can_jump():\n+                    suffix_ids = jump_helper.suffix_ids\n                     # Current ids, for cache and revert\n                     cur_all_ids = tuple(req.origin_input_ids + req.output_ids)[:-1]\n                     cur_output_ids = req.output_ids\n@@ -836,10 +820,8 @@ class ScheduleBatch:\n                     (\n                         jump_forward_str,\n                         next_state,\n-                    ) = req.jump_forward_map.jump_forward_symbol(cur_state)\n+                    ) = req.grammar.jump_forward_str_state(jump_helper)\n \n-                    # Make the incrementally decoded text part of jump_forward_str\n-                    # so that the UTF-8 will not corrupt\n                     jump_forward_str = new_text + jump_forward_str\n                     if not req.jump_forward_and_retokenize(\n                         jump_forward_str, next_state\n@@ -946,7 +928,7 @@ class ScheduleBatch:\n             self.top_logprobs_nums = None\n \n         self.has_stream = any(req.stream for req in self.reqs)\n-        self.has_regex = any(req.regex_fsm for req in self.reqs)\n+        self.has_grammar = any(req.grammar for req in self.reqs)\n \n         self.sampling_info.filter_batch(keep_indices, new_indices)\n \n@@ -979,7 +961,7 @@ class ScheduleBatch:\n \n         self.return_logprob = self.return_logprob or other.return_logprob\n         self.has_stream = self.has_stream or other.has_stream\n-        self.has_regex = self.has_regex or other.has_regex\n+        self.has_grammar = self.has_grammar or other.has_grammar\n \n     def get_model_worker_batch(self):\n         if self.forward_mode.is_decode():\n@@ -989,13 +971,10 @@ class ScheduleBatch:\n             extend_prefix_lens = self.prefix_lens\n             extend_logprob_start_lens = self.extend_logprob_start_lens\n \n-        if self.has_regex:\n-            self.sampling_info.regex_fsms = [req.regex_fsm for req in self.reqs]\n-            self.sampling_info.regex_fsm_states = [\n-                req.regex_fsm_state for req in self.reqs\n-            ]\n+        if self.has_grammar:\n+            self.sampling_info.grammars = [req.grammar for req in self.reqs]\n         else:\n-            self.sampling_info.regex_fsms = None\n+            self.sampling_info.grammars = None\n \n         global bid\n         bid += 1\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 55b05f846..b1fb96b2a 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -29,8 +29,7 @@ import zmq\n \n from sglang.global_config import global_config\n from sglang.srt.configs.model_config import ModelConfig\n-from sglang.srt.constrained.fsm_cache import FSMCache\n-from sglang.srt.constrained.jump_forward import JumpForwardCache\n+from sglang.srt.constrained.grammar import GrammarCache\n from sglang.srt.hf_transformers_utils import get_processor, get_tokenizer\n from sglang.srt.layers.logits_processor import LogitsProcessorOutput\n from sglang.srt.managers.io_struct import (\n@@ -225,17 +224,20 @@ class Scheduler:\n         )\n \n         # Init the FSM cache for constrained generation\n+        self.grammar_cache = None\n+\n         if not server_args.skip_tokenizer_init:\n-            self.regex_fsm_cache = FSMCache(\n+            self.grammar_cache = GrammarCache(\n                 server_args.tokenizer_path,\n                 {\n                     \"tokenizer_mode\": server_args.tokenizer_mode,\n                     \"trust_remote_code\": server_args.trust_remote_code,\n                 },\n                 skip_tokenizer_init=server_args.skip_tokenizer_init,\n-                constrained_json_whitespace_pattern=server_args.constrained_json_whitespace_pattern,\n+                whitespace_patterns=server_args.constrained_json_whitespace_pattern,\n+                backend=server_args.grammar_backend,\n+                allow_jump=not server_args.disable_regex_jump_forward,\n             )\n-        self.jump_forward_cache = JumpForwardCache()\n \n         # Init new token estimation\n         assert (\n@@ -402,22 +404,20 @@ class Scheduler:\n             # By default, only return the logprobs for output tokens\n             req.logprob_start_len = len(recv_req.input_ids) - 1\n \n-        # Init regex FSM\n+        # Init regex FSM or BNF\n         if (\n             req.sampling_params.json_schema is not None\n             or req.sampling_params.regex is not None\n         ):\n+            assert self.grammar_cache is not None\n             if req.sampling_params.json_schema is not None:\n-                req.regex_fsm, computed_regex_string = self.regex_fsm_cache.query(\n-                    (\"json\", req.sampling_params.json_schema)\n+                req.grammar = self.grammar_cache.query(\n+                    (\"json\", req.sampling_params.json_schema),\n+                    self.model_config.vocab_size,\n                 )\n             elif req.sampling_params.regex is not None:\n-                req.regex_fsm, computed_regex_string = self.regex_fsm_cache.query(\n-                    (\"regex\", req.sampling_params.regex)\n-                )\n-            if not self.disable_regex_jump_forward:\n-                req.jump_forward_map = self.jump_forward_cache.query(\n-                    computed_regex_string\n+                req.grammar = self.grammar_cache.query(\n+                    (\"regex\", req.sampling_params.regex), self.model_config.vocab_size\n                 )\n \n         # Truncate prompts that are too long\n@@ -796,10 +796,8 @@ class Scheduler:\n                     elif not batch.decoding_reqs or req not in batch.decoding_reqs:\n                         self.tree_cache.cache_unfinished_req(req)\n \n-                    if req.regex_fsm is not None:\n-                        req.regex_fsm_state = req.regex_fsm.get_next_state(\n-                            req.regex_fsm_state, next_token_ids[i]\n-                        )\n+                    if req.grammar is not None:\n+                        req.grammar.accept_token(next_token_ids[i])\n \n                     if req.return_logprob:\n                         logprob_pt += self.add_logprob_return_values(\n@@ -855,10 +853,8 @@ class Scheduler:\n             req.output_ids.append(next_token_id)\n             req.check_finished()\n \n-            if req.regex_fsm is not None:\n-                req.regex_fsm_state = req.regex_fsm.get_next_state(\n-                    req.regex_fsm_state, next_token_id\n-                )\n+            if req.grammar is not None:\n+                req.grammar.accept_token(next_token_id)\n \n             if req.finished():\n                 self.tree_cache.cache_finished_req(req)\n@@ -1056,7 +1052,9 @@ class Scheduler:\n         ):\n             self.tree_cache.reset()\n             self.tree_cache_metrics = {\"total\": 0, \"hit\": 0}\n-            self.regex_fsm_cache.reset()\n+            if self.grammar_cache is not None:\n+                self.grammar_cache.reset()\n+            # TODO(dark): reset the bnf cache\n             self.req_to_token_pool.clear()\n             self.token_to_kv_pool.clear()\n             torch.cuda.empty_cache()\ndiff --git a/python/sglang/srt/sampling/sampling_batch_info.py b/python/sglang/srt/sampling/sampling_batch_info.py\nindex 27a2d07fb..6afd48cc8 100644\n--- a/python/sglang/srt/sampling/sampling_batch_info.py\n+++ b/python/sglang/srt/sampling/sampling_batch_info.py\n@@ -6,7 +6,7 @@ from typing import TYPE_CHECKING, List, Optional\n import torch\n \n import sglang.srt.sampling.penaltylib as penaltylib\n-from sglang.srt.constrained import RegexGuide\n+from sglang.srt.constrained.grammar import Grammar\n \n if TYPE_CHECKING:\n     from sglang.srt.managers.schedule_batch import ScheduleBatch\n@@ -29,11 +29,9 @@ class SamplingBatchInfo:\n     # Bias Tensors\n     vocab_size: int\n     logit_bias: torch.Tensor = None\n-    vocab_mask: torch.Tensor = None\n+    vocab_mask: Optional[torch.Tensor] = None\n \n-    # FSM states\n-    regex_fsms: List[RegexGuide] = None\n-    regex_fsm_states: List[int] = None\n+    grammars: Optional[List[Optional[Grammar]]] = None\n \n     # Penalizer\n     penalizer_orchestrator: Optional[penaltylib.BatchedPenalizerOrchestrator] = None\n@@ -136,8 +134,7 @@ class SamplingBatchInfo:\n                 self.linear_penalties = penalizer.apply(self.linear_penalties)\n \n     def update_regex_vocab_mask(self):\n-        has_regex = self.regex_fsms and any(regex_fsm for regex_fsm in self.regex_fsms)\n-        if not has_regex:\n+        if not self.grammars or not any(grammar for grammar in self.grammars):\n             self.vocab_mask = None\n             return\n \n@@ -147,12 +144,9 @@ class SamplingBatchInfo:\n             dtype=torch.bool,\n             device=self.device,\n         )\n-        for i, regex_fsm in enumerate(self.regex_fsms):\n-            if regex_fsm is not None:\n-                self.vocab_mask[i].fill_(1)\n-                self.vocab_mask[i][\n-                    regex_fsm.get_next_instruction(self.regex_fsm_states[i]).tokens\n-                ] = 0\n+        for i, grammar in enumerate(self.grammars):\n+            if grammar is not None:\n+                grammar.fill_vocab_mask(self.vocab_mask[i], self.vocab_size)\n \n     def filter_batch(self, unfinished_indices: List[int], new_indices: torch.Tensor):\n         if self.penalizer_orchestrator:\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 6ccd89185..9cb7c0331 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -102,6 +102,7 @@ class ServerArgs:\n     # Kernel backend\n     attention_backend: Optional[str] = None\n     sampling_backend: Optional[str] = None\n+    grammar_backend: Optional[str] = \"outlines\"\n \n     # Optimization/debug options\n     disable_flashinfer: bool = False\n@@ -537,6 +538,13 @@ class ServerArgs:\n             default=ServerArgs.sampling_backend,\n             help=\"Choose the kernels for sampling layers.\",\n         )\n+        parser.add_argument(\n+            \"--grammar-backend\",\n+            type=str,\n+            choices=[\"xgrammar\", \"outlines\"],\n+            default=ServerArgs.grammar_backend,\n+            help=\"Choose the backend for constrained decoding.\",\n+        )\n \n         # Optimization/debug options\n         parser.add_argument(",
  "apis": [
    "sglang.srt.constrained.GrammarCache.query",
    "sglang.srt.constrained.Grammar.jump_and_retokenize",
    "sglang.srt.managers.schedule_batch.Req.jump_forward_and_retokenize",
    "sglang.srt.server_args.ServerArgs.grammar_backend"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/sgl-kernel/python/sgl_kernel/grammar.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/scheduler.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit introduces significant changes, replacing and extending the regex-based constrained decoding with a new \"grammar\" mechanism that supports two backends (\u201cxgrammar\u201d and \u201coutlines\u201d). It adds new modules and modifies existing non-test source files to integrate and switch between these backends. Although the commit message mentions performance, the changes go beyond mere refactoring by altering the internal constrained decoding API, which is performance-critical. This indicates a performance or optimization focus rather than simple feature addition or bugfix.",
  "llm_api_reason": "This commit refactors and extends the constrained decoding logic by replacing the old regex\u2010based interface with a new unified grammar abstraction that supports both \u201cxgrammar\u201d and \u201coutlines\u201d backends. New classes (such as BNFCache, Grammar, and GrammarCache) are created in the sglang.srt.constrained package, and existing methods in the scheduling and request handling (e.g. jump_forward_and_retokenize in Req and related processing in ScheduleBatch and Scheduler) are updated to use the new grammar API. In addition, a new server argument \u201cgrammar_backend\u201d is introduced to allow selecting between the two backends."
}