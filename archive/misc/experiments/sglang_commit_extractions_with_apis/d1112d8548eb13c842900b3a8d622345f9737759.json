{
  "commit_hash": "d1112d8548eb13c842900b3a8d622345f9737759",
  "pr_url": "https://github.com/sgl-project/sglang/pull/2797",
  "pr_date": "2025-01-08",
  "timeline_text": "Copy link Contributor RinRin-32 commented Jan 8, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation While input_embeds is now supported, I find that for what I'm using it for, normally requesting through /generate wasn't enough. I was spending around 2.5 seconds on average until the request was even handled in tokenizer manager. With this in mind, I suspect that the main issue came around possible overhead and such. To overcome said issue, I figured sending said input_embeds as a file likely would reduce the issue, doing so I managed to shaved off around 2 seconds from my use case. Please contribute or comment your idea of a proper unit test for this application. Modifications Added endpoint in server.py to handle direct file transfer. Checklist Format your code according to the Contributor Guide . Add unit tests as outlined in the Contributor Guide . Update documentation as needed, including docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Add endpoint for file support, purely for input_embeds a533cab RinRin-32 requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock and ByronHsu as code owners January 8, 2025 13:42 RinRin-32 and others added 4 commits January 8, 2025 13:52 reformatted 10988f3 implemented streaming response 05039cd ran pre-commit 28a437a Merge branch 'main' into fastapi_file 5989948 merrymercy requested changes Jan 10, 2025 View reviewed changes Copy link Contributor merrymercy left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment You can add a test case here https://github.com/sgl-project/sglang/blob/main/test/srt/test_input_embeddings.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/server.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author RinRin-32 commented Jan 15, 2025 @merrymercy My use case with this right now is to serve our LLM based service. It's based on Gemma 2 2b where I launch it with the following python -m sglang.launch_server --model-path <model-path> --port 30000 --disable-radix --disable-cuda-graph-padding --disable-jump-forward --disable-mla While it's faster with the new endpoint I can't quit get it as close as how the model can perform on simple implementation. Is there's anything else I can do to get faster response? Perhaps there's something I overlooked while making the changes in this PR, I'm especially curious about whether fastapi implementation made in sglang can be improved. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . RinRin-32 and others added 5 commits January 16, 2025 13:42 added test case 0739b2d remove sampling param since it's not used with the default overlap sc\u2026 \u2026 6cdfe1d \u2026heduler Merge branch 'main' into fastapi_file f616b13 Merge branch 'main' into fastapi_file 7ad0b19 Merge branch 'main' into fastapi_file 2218869 RinRin-32 requested a review\n  from merrymercy January 22, 2025 08:58 Copy link Contributor Author RinRin-32 commented Jan 22, 2025 I skipped checking for param from file name because using the overlap scheduler the sampling params are ignored anyway. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor merrymercy commented Jan 26, 2025 Could you rebase and fix the lint? Sorry for the inconvenience created by our recent refactor #2996 \ud83d\udc4d 1 RinRin-32 reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . RinRin-32 added 3 commits January 28, 2025 02:15 ran pre-commit c240687 added test case b5f627c Merge branch 'fastapi_file' of github.com:RinRin-32/sglang into fasta\u2026 \u2026 755e4a3 \u2026pi_file RinRin-32 requested review from HandH1998 , BBuf , yizhang2077 and HaiShaw as code owners January 28, 2025 02:16 RinRin-32 added 2 commits January 28, 2025 02:19 manually resolving b848172 precommit 1dfee04 RinRin-32 added 2 commits January 28, 2025 09:22 Merge branch 'main' into fastapi_file 790f318 Merge branch 'main' into fastapi_file c835ec3 Hide details View details merrymercy merged commit d1112d8 into sgl-project : main Mar 17, 2025 15 checks passed Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:17",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "google/gemma-2-2b"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=google/gemma-2-2b --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model google/gemma-2-2b --num_prompts 100",
  "commit_subject": "Add endpoint for file support, purely to speed up processing of input_embeds. (#2797)",
  "commit_message": "Add endpoint for file support, purely to speed up processing of input_embeds. (#2797)",
  "commit_date": "2025-03-16T18:30:37-07:00",
  "files_changed": [
    "python/sglang/srt/entrypoints/http_server.py",
    "test/srt/test_input_embeddings.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 67,
    "num_files": 2,
    "num_hunks": 8,
    "num_non_test_edited_lines": 67,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 32a11c15c..5a97072de 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -19,6 +19,7 @@ This file implements HTTP APIs for the inference engine via fastapi.\n \n import asyncio\n import dataclasses\n+import json\n import logging\n import multiprocessing as multiprocessing\n import os\n@@ -259,6 +260,29 @@ async def generate_request(obj: GenerateReqInput, request: Request):\n             return _create_error_response(e)\n \n \n+@app.api_route(\"/generate_from_file\", methods=[\"POST\"])\n+async def generate_from_file_request(file: UploadFile, request: Request):\n+    \"\"\"Handle a generate request, this is purely to work with input_embeds.\"\"\"\n+    content = await file.read()\n+    input_embeds = json.loads(content.decode(\"utf-8\"))\n+\n+    obj = GenerateReqInput(\n+        input_embeds=input_embeds,\n+        sampling_params={\n+            \"repetition_penalty\": 1.2,\n+            \"temperature\": 0.2,\n+            \"max_new_tokens\": 512,\n+        },\n+    )\n+\n+    try:\n+        ret = await _global_state.generate_request(obj, request).__anext__()\n+        return ret\n+    except ValueError as e:\n+        logger.error(f\"Error: {e}\")\n+        return _create_error_response(e)\n+\n+\n @app.api_route(\"/encode\", methods=[\"POST\", \"PUT\"])\n async def encode_request(obj: EmbeddingReqInput, request: Request):\n     \"\"\"Handle an embedding request.\"\"\"\ndiff --git a/test/srt/test_input_embeddings.py b/test/srt/test_input_embeddings.py\nindex 015aabe78..92b643fd3 100644\n--- a/test/srt/test_input_embeddings.py\n+++ b/test/srt/test_input_embeddings.py\n@@ -1,4 +1,6 @@\n import json\n+import os\n+import tempfile\n import unittest\n \n import requests\n@@ -38,7 +40,7 @@ class TestInputEmbeds(unittest.TestCase):\n         return embeddings.squeeze().tolist()  # Convert tensor to a list for API use\n \n     def send_request(self, payload):\n-        \"\"\"Send a POST request to the API and return the response.\"\"\"\n+        \"\"\"Send a POST request to the /generate endpoint and return the response.\"\"\"\n         response = requests.post(\n             self.base_url + \"/generate\",\n             json=payload,\n@@ -50,8 +52,22 @@ class TestInputEmbeds(unittest.TestCase):\n             \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n         }\n \n+    def send_file_request(self, file_path):\n+        \"\"\"Send a POST request to the /generate_from_file endpoint with a file.\"\"\"\n+        with open(file_path, \"rb\") as f:\n+            response = requests.post(\n+                self.base_url + \"/generate_from_file\",\n+                files={\"file\": f},\n+                timeout=30,  # Set a reasonable timeout for the API request\n+            )\n+        if response.status_code == 200:\n+            return response.json()\n+        return {\n+            \"error\": f\"Request failed with status {response.status_code}: {response.text}\"\n+        }\n+\n     def test_text_based_response(self):\n-        \"\"\"Print API response using text-based input.\"\"\"\n+        \"\"\"Test and print API responses using text-based input.\"\"\"\n         for text in self.texts:\n             payload = {\n                 \"model\": self.model,\n@@ -64,7 +80,7 @@ class TestInputEmbeds(unittest.TestCase):\n             )\n \n     def test_embedding_based_response(self):\n-        \"\"\"Print API response using input embeddings.\"\"\"\n+        \"\"\"Test and print API responses using input embeddings.\"\"\"\n         for text in self.texts:\n             embeddings = self.generate_input_embeddings(text)\n             payload = {\n@@ -78,7 +94,7 @@ class TestInputEmbeds(unittest.TestCase):\n             )\n \n     def test_compare_text_vs_embedding(self):\n-        \"\"\"Print responses for both text-based and embedding-based inputs.\"\"\"\n+        \"\"\"Test and compare responses for text-based and embedding-based inputs.\"\"\"\n         for text in self.texts:\n             # Text-based payload\n             text_payload = {\n@@ -106,6 +122,25 @@ class TestInputEmbeds(unittest.TestCase):\n             # This is flaky, so we skip this temporarily\n             # self.assertEqual(text_response[\"text\"], embed_response[\"text\"])\n \n+    def test_generate_from_file(self):\n+        \"\"\"Test the /generate_from_file endpoint using tokenized embeddings.\"\"\"\n+        for text in self.texts:\n+            embeddings = self.generate_input_embeddings(text)\n+            with tempfile.NamedTemporaryFile(\n+                mode=\"w\", suffix=\".json\", delete=False\n+            ) as tmp_file:\n+                json.dump(embeddings, tmp_file)\n+                tmp_file_path = tmp_file.name\n+\n+            try:\n+                response = self.send_file_request(tmp_file_path)\n+                print(\n+                    f\"Text Input: {text}\\nResponse from /generate_from_file: {json.dumps(response, indent=2)}\\n{'-' * 80}\"\n+                )\n+            finally:\n+                # Ensure the temporary file is deleted\n+                os.remove(tmp_file_path)\n+\n     @classmethod\n     def tearDownClass(cls):\n         kill_process_tree(cls.process.pid)",
  "apis": [
    "sglang.srt.entrypoints.http_server.generate_from_file_request"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/entrypoints/http_server.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit modifies a non-test, source code file (\"http_server.py\") by adding a new endpoint (\"/generate_from_file\") which is explicitly introduced \"purely to speed up processing of input_embeds\". The commit targets performance improvement by streamlining processing via file input. Although it is a new endpoint, it is introduced with the intent of improving the performance of handling input embeddings on the CPU rather than simply adding a new feature, bug fix, or trivial refactoring. The changes are testable without requiring GPU support and affect a top-level API.",
  "llm_api_reason": "This commit adds a new HTTP API endpoint \"/generate_from_file\" in the SRT HTTP server by introducing the function generate_from_file_request in http_server.py. The new endpoint reads a file to obtain input_embeds and constructs a GenerateReqInput object to process generation requests. The tests were also updated to send a file request to this endpoint."
}