{
  "commit_hash": "9c064bf78af8558dbc50fbd809f65dcafd6fd965",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1587",
  "pr_date": "2024-10-06",
  "timeline_text": "Copy link Member Ying1123 commented Oct 6, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Use seg_indptr rather than seg_lens . Fix performance bug in adapter swapping. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Ying1123 added 2 commits October 6, 2024 05:16 use indptr rather than lens a7ce434 update 9c1fbf2 Ying1123 force-pushed the lora_fix_1 branch\n    from 558d2cb to 9c1fbf2 Compare October 6, 2024 09:00 merrymercy approved these changes Oct 6, 2024 View reviewed changes Ying1123 enabled auto-merge (squash) October 6, 2024 09:01 Merge branch 'main' into lora_fix_1 dc6a8ee Ying1123 disabled auto-merge October 6, 2024 17:33 Hide details View details Ying1123 merged commit 9c064bf into main Oct 6, 2024 0 of 10 checks passed Uh oh! There was an error while loading. Please reload this page . Ying1123 deleted the lora_fix_1 branch October 6, 2024 17:33 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 [LoRA, Performance] Speedup multi-LoRA serving - Step 1 ( sgl-project#\u2026 \u2026 5ed29a5 \u20261587 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:30",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": false,
  "test_details": "PERF | SERVING",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[LoRA, Performance] Speedup multi-LoRA serving - Step 1 (#1587)",
  "commit_message": "[LoRA, Performance] Speedup multi-LoRA serving - Step 1 (#1587)",
  "commit_date": "2024-10-06T10:33:44-07:00",
  "files_changed": [
    "benchmark/lora/launch_server.py",
    "python/sglang/srt/lora/lora.py",
    "python/sglang/srt/lora/lora_manager.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 66,
    "num_files": 3,
    "num_hunks": 17,
    "num_non_test_edited_lines": 66,
    "num_non_test_files": 3,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/lora/launch_server.py b/benchmark/lora/launch_server.py\nindex 1fa4d7135..f139f0df6 100644\n--- a/benchmark/lora/launch_server.py\n+++ b/benchmark/lora/launch_server.py\n@@ -1,7 +1,7 @@\n import argparse\n import os\n \n-NUM_LORAS = 128\n+NUM_LORAS = 8\n LORA_PATH = {\n     \"base\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n     \"lora\": \"/home/ying/test_lora\",\n@@ -11,12 +11,11 @@ LORA_PATH = {\n def launch_server(args):\n     base_path = LORA_PATH[\"base\"]\n     lora_path = LORA_PATH[\"lora\"]\n-    max_loras_per_batch = 4\n \n     if args.base_only:\n-        cmd = f\"python -m sglang.launch_server --model {base_path} \"\n+        cmd = f\"python3 -m sglang.launch_server --model {base_path} \"\n     else:\n-        cmd = f\"python -m sglang.launch_server --model {base_path} --lora-paths \"\n+        cmd = f\"python3 -m sglang.launch_server --model {base_path} --lora-paths \"\n         for i in range(NUM_LORAS):\n             lora_name = f\"lora{i}\"\n             cmd += f\"{lora_name}={lora_path} \"\n@@ -29,11 +28,6 @@ def launch_server(args):\n \n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"--num-loras\",\n-        type=int,\n-        default=128,\n-    )\n     parser.add_argument(\n         \"--base-only\",\n         action=\"store_true\",\ndiff --git a/python/sglang/srt/lora/lora.py b/python/sglang/srt/lora/lora.py\nindex 379b233bd..85470996f 100644\n--- a/python/sglang/srt/lora/lora.py\n+++ b/python/sglang/srt/lora/lora.py\n@@ -101,12 +101,12 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n     ) -> None:\n         super().__init__(base_layer, segment_gemm, lora_rank, scaling)\n \n-    def set_lora_info(self, A_buffer, B_buffer, bs, seq_lens, weight_indices):\n+    def set_lora_info(self, A_buffer, B_buffer, bs, seg_indptr, weight_indices):\n         self.set_lora = True\n         self.A_buffer = A_buffer\n         self.B_buffer = B_buffer\n         self.bs = bs\n-        self.seq_lens = seq_lens\n+        self.seg_indptr = seg_indptr\n         self.weight_indices = weight_indices\n \n     def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n@@ -115,11 +115,10 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             weights=self.A_buffer,\n             batch_size=self.bs,\n             weight_column_major=True,\n-            seg_lens=self.seq_lens,\n+            seg_indptr=self.seg_indptr,\n             weight_indices=self.weight_indices,\n         )\n         # FIXME\n-        assert lora_a_output.shape[-1] == self.lora_rank * 2\n         lora_output = torch.empty_like(base_output)\n         output_dim = lora_output.shape[-1] // 2\n         for i in range(2):\n@@ -132,7 +131,7 @@ class MergedColumnParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n                 weights=self.B_buffer[:, left:right, :].contiguous(),\n                 batch_size=self.bs,\n                 weight_column_major=True,\n-                seg_lens=self.seq_lens,\n+                seg_indptr=self.seg_indptr,\n                 weight_indices=self.weight_indices,\n             )\n         return base_output + lora_output * self.scaling\n@@ -145,14 +144,14 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n         super().__init__(base_layer, segment_gemm, lora_rank, scaling)\n \n     def set_lora_info(\n-        self, A_buffer_qkv, B_buffer_q, B_buffer_kv, bs, seq_lens, weight_indices\n+        self, A_buffer_qkv, B_buffer_q, B_buffer_kv, bs, seg_indptr, weight_indices\n     ):\n         self.set_lora = True\n         self.A_buffer_qkv = A_buffer_qkv\n         self.B_buffer_q = B_buffer_q\n         self.B_buffer_kv = B_buffer_kv\n         self.bs = bs\n-        self.seq_lens = seq_lens\n+        self.seg_indptr = seg_indptr\n         self.weight_indices = weight_indices\n \n     def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n@@ -161,7 +160,7 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             weights=self.A_buffer_qkv,\n             batch_size=self.bs,\n             weight_column_major=True,\n-            seg_lens=self.seq_lens,\n+            seg_indptr=self.seg_indptr,\n             weight_indices=self.weight_indices,\n         )\n         # FIXME parallelize qkv\n@@ -173,7 +172,7 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n             weights=self.B_buffer_q,\n             batch_size=self.bs,\n             weight_column_major=True,\n-            seg_lens=self.seq_lens,\n+            seg_indptr=self.seg_indptr,\n             weight_indices=self.weight_indices,\n         )\n         # kv\n@@ -189,7 +188,7 @@ class QKVParallelLinearWithLoRA(ColumnParallelLinearWithLoRA):\n                     weights=self.B_buffer_kv[:, left:right, :].contiguous(),\n                     batch_size=self.bs,\n                     weight_column_major=True,\n-                    seg_lens=self.seq_lens,\n+                    seg_indptr=self.seg_indptr,\n                     weight_indices=self.weight_indices,\n                 )\n             )\n@@ -202,12 +201,12 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n     ) -> None:\n         super().__init__(base_layer, segment_gemm, lora_rank, scaling)\n \n-    def set_lora_info(self, A_buffer, B_buffer, bs, seq_lens, weight_indices):\n+    def set_lora_info(self, A_buffer, B_buffer, bs, seg_indptr, weight_indices):\n         self.set_lora = True\n         self.A_buffer = A_buffer\n         self.B_buffer = B_buffer\n         self.bs = bs\n-        self.seq_lens = seq_lens\n+        self.seg_indptr = seg_indptr\n         self.weight_indices = weight_indices\n \n     def apply_lora(self, base_output: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n@@ -216,7 +215,7 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n             weights=self.A_buffer,\n             batch_size=self.bs,\n             weight_column_major=True,\n-            seg_lens=self.seq_lens,\n+            seg_indptr=self.seg_indptr,\n             weight_indices=self.weight_indices,\n         )\n         lora_output = self.segment_gemm.run(\n@@ -224,7 +223,7 @@ class RowParallelLinearWithLoRA(BaseLayerWithLoRA):\n             weights=self.B_buffer,\n             batch_size=self.bs,\n             weight_column_major=True,\n-            seg_lens=self.seq_lens,\n+            seg_indptr=self.seg_indptr,\n             weight_indices=self.weight_indices,\n         )\n         return base_output + lora_output * self.scaling\ndiff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 59cd7e157..dd46212ed 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -274,18 +274,24 @@ class LoRAManager:\n         cur_uids = set(forward_batch.lora_paths)\n         assert len(cur_uids) <= self.max_loras_per_batch\n         i = 0\n+        j = len(self.active_uids)\n         evictable_uids = list(self.active_uids)\n         for uid in cur_uids:\n             if uid not in self.active_uids:\n-                while i < len(evictable_uids) and evictable_uids[i] in cur_uids:\n-                    i += 1\n-                if i < len(evictable_uids):\n+                if j < self.max_loras_per_batch:\n+                    index = j\n+                    j += 1\n+                else:\n+                    while i < len(evictable_uids) and evictable_uids[i] in cur_uids:\n+                        i += 1\n+                    assert i < len(evictable_uids)\n                     self.active_uids.remove(evictable_uids[i])\n                     self.buffer_id.pop(evictable_uids[i])\n-                self.load_lora(uid, i)\n+                    index = i\n+                    i += 1\n+                self.load_lora(uid, index)\n                 self.active_uids.add(uid)\n-                self.buffer_id[uid] = i\n-                i += 1\n+                self.buffer_id[uid] = index\n \n         if cur_uids == set([None]):\n             return\n@@ -295,8 +301,11 @@ class LoRAManager:\n         seg_lens = (\n             forward_batch.extend_seq_lens\n             if forward_batch.forward_mode.is_extend()\n-            else torch.ones(bs)\n+            else torch.ones(bs, device=\"cuda\")\n         )\n+        # FIXME: reuse the data rather than recompute\n+        seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=\"cuda\")\n+        seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n         weight_indices = torch.empty((bs,), dtype=torch.int64, device=\"cuda\")\n         for i, lora_path in enumerate(forward_batch.lora_paths):\n             weight_indices[i] = self.buffer_id[lora_path]\n@@ -310,7 +319,7 @@ class LoRAManager:\n                     self.A_buffer[weight_name][layer_id],\n                     self.B_buffer[weight_name][layer_id],\n                     bs,\n-                    seg_lens,\n+                    seg_indptr,\n                     weight_indices,\n                 )\n             else:\n@@ -319,6 +328,6 @@ class LoRAManager:\n                     self.B_buffer[\"q_proj\"][layer_id],\n                     self.B_buffer[\"kv_proj\"][layer_id],\n                     bs,\n-                    seg_lens,\n+                    seg_indptr,\n                     weight_indices,\n                 )",
  "apis": [
    "sglang.srt.lora.lora.MergedColumnParallelLinearWithLoRA.set_lora_info",
    "sglang.srt.lora.lora.QKVParallelLinearWithLoRA.set_lora_info",
    "sglang.srt.lora.lora.RowParallelLinearWithLoRA.set_lora_info",
    "sglang.srt.lora.lora_manager.LoRAManager.prepare_lora_batch"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py",
    "/path/to/repos/sglang/examples/runtime/lora.py",
    "/path/to/repos/sglang/python/sglang/srt/lora/lora.py",
    "/path/to/repos/sglang/python/sglang/srt/lora/lora_manager.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes non-trivial changes to multiple non-test files (in benchmark and core code under python/sglang) that alter the way the multi-LoRA serving is handled. These include reducing default parameters (NUM_LORAS changed from 128 to 8), switching from seq_lens to a seg_indptr representation (with corresponding changes in function parameters and data processing), and adjusting index management in the LoRAManager. The commit message itself signals a performance speedup effort (\"Speedup multi-LoRA serving - Step 1\") and the adjustments in the algorithm (altering buffer indexing and computing segmentation indices) are intended to enhance the serving performance. Although some device-specific CUDA keywords appear in one section, the overall modifications aim to optimize the internal API that drives multi-LoRA serving, meeting the criteria for performance-related changes. Hence, the commit qualifies as a performance/optimization related commit.",
  "llm_api_reason": "This commit makes two kinds of changes. First, in the benchmark launch_server script, it reduces the number of LoRAs from 128 to 8 and switches the command to use \"python3\" rather than \"python\". Second, in the LoRA module, it renames parameters in the set_lora_info methods for several linear layer classes\u2014changing the parameter from \u201cseq_lens\u201d to \u201cseg_indptr\u201d\u2014and updates the LoRAManager\u2019s batching code to use seg_indptr computed from a cumulative sum. These adjustments affect the APIs used to configure and apply LoRA in the model layers as well as the LoRAManager\u2019s batch preparation."
}