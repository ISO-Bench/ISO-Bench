{
  "commit_hash": "e3ec6bf4b65a50e26e936a96adc7acc618292002",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6814",
  "pr_date": "2025-06-13",
  "timeline_text": "Copy link Collaborator fzyzcjy commented Jun 2, 2025 Motivation Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions cherry pick bfd0cf6 fzyzcjy requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw , ch-wan and BBuf as code owners June 2, 2025 13:20 fzyzcjy changed the title Speed up block_quant_dequant Minor speed up block_quant_dequant Jun 2, 2025 gemini-code-assist bot reviewed Jun 2, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Hello @fzyzcjy , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! Summary of Changes Hello! gemini-code-assist here, providing a summary of this pull request to help everyone get up to speed quickly. This PR aims to significantly speed up the block_quant_dequant function within the FP8 quantization utilities. The primary motivation, based on the title, is performance optimization for this specific operation. The changes involve replacing a manual, loop-based implementation of block dequantization with a more efficient tensor-based approach leveraging the einops library. Highlights Performance Optimization : The core change is a rewrite of the block_quant_dequant function to replace nested Python loops and manual tiling with a single, potentially faster, tensor operation using einops.repeat . Code Simplification : The implementation of block_quant_dequant is simplified by removing the explicit loop structure and tile handling, resulting in more concise code. New Dependency : The einops library is added as a dependency to facilitate the tensor repetition needed for the new dequantization logic. Input Validation : Assertions were added to block_quant_dequant to ensure that the input tensor dimensions are perfectly divisible by the specified block size, which is required for the new einops -based approach. Changelog python/sglang/srt/layers/quantization/fp8_utils.py Removed unused imports os and curses.flash . Added import for the einops library. Rewrote the block_quant_dequant function to use einops.repeat and tensor multiplication for dequantization, replacing the previous loop-based implementation. Added assertions to block_quant_dequant to check for divisibility of input dimensions by block size. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with \ud83d\udc4d and \ud83d\udc4e on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Loops run slow and long, Tensor math makes code strong, Speed up, dequantize! Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . \u21a9 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot suggested changes Jun 2, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request aims to speed up the block_quant_dequant function by replacing a loop-based implementation with a vectorized approach using einops . This is a good optimization strategy and should lead to performance improvements. The main point to clarify is the change in input validation: the new implementation asserts that input tensor dimensions must be perfectly divisible by the block sizes, whereas the previous implementation appeared to handle non-perfectly divisible inputs. Please confirm if this change in behavior is intentional. If so, the function's docstring should be updated to reflect this new precondition. Summary of Findings Change in Input Validation for block_quant_dequant : The function block_quant_dequant now explicitly asserts that the input tensor dimensions n and k must be perfectly divisible by block_n and block_k respectively. The previous implementation appeared to handle non-perfectly divisible inputs. This change needs clarification to ensure it's intentional and to update documentation accordingly if it is. Unused Imports Removed : The imports os and curses.flash were removed. This is a good cleanup as they were unused in the context of the changed function. (Severity: low, not commented due to review settings) Merge Readiness This pull request introduces a good performance optimization by vectorizing the block_quant_dequant function. However, there's a significant change in input validation that needs clarification. If the new assertions for input divisibility are intended and all callers adhere to this, the change is acceptable, but the docstring should be updated. If the previous behavior of handling non-divisible inputs needs to be maintained, the current changes would be breaking. I recommend addressing the point about input validation before merging. I am unable to approve pull requests, so please ensure other reviewers approve this change after the clarification. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/layers/quantization/fp8_utils.py Outdated Comment on lines 381 to 382 assert n % block_n == 0 assert k % block_k == 0 Copy link Contributor gemini-code-assist bot Jun 2, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The new implementation introduces assertions n % block_n == 0 and k % block_k == 0 . This is a stricter requirement compared to the previous version, which used min() in slicing (e.g., min((j + 1) * block_n, n) ) and could handle cases where x_q_block dimensions were not perfectly divisible by block_size (as long as x_s had the correct number of scales for the tiles). Could you clarify if this change in behavior is intentional? If x_q_block is always guaranteed to have dimensions divisible by block_size in all call sites, then these assertions make an implicit precondition explicit, which is good. In this case, it would be beneficial to update the function's docstring (lines 374-378) to mention these new preconditions for x_q_block and x_s shapes. If the function was previously expected to handle non-divisible shapes, this change might break existing callers or alter the function's contract. If the old behavior needs to be preserved, the einops approach might require padding or a different vectorization strategy. This change is important as it affects the function's input contract. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator Alcanderian commented Jun 2, 2025 NO einops for multi-platform compatibility XD All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author fzyzcjy commented Jun 2, 2025 @Alcanderian Ah, that one does not support AMD? I thought it is a wrapper around pytorch... All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned Alcanderian Jun 2, 2025 zhyncs added\n  the high priority label Jun 2, 2025 zhyncs and others added 6 commits June 2, 2025 11:14 Merge branch 'main' into feat/fast_dequant 945f5e9 Merge branch 'main' into feat/fast_dequant 89a7d52 Merge branch 'main' into feat/fast_dequant f3f3542 cherry pick 92fcf44 more 3ec01fb fmt 6b2521b Copy link Collaborator Author fzyzcjy commented Jun 13, 2025 einops removed All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . fzyzcjy and others added 4 commits June 13, 2025 20:31 merge 166f50d more 3cc1bc5 more 15f9a21 Merge branch 'main' into feat/fast_dequant bcfc162 zhyncs approved these changes Jun 13, 2025 View reviewed changes Hide details View details zhyncs merged commit e3ec6bf into sgl-project : main Jun 13, 2025 3 of 48 checks passed Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:34",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Minor speed up block_quant_dequant (#6814)",
  "commit_message": "Minor speed up block_quant_dequant (#6814)",
  "commit_date": "2025-06-13T14:32:46-07:00",
  "files_changed": [
    "python/sglang/srt/layers/quantization/fp8_utils.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 26,
    "num_files": 1,
    "num_hunks": 1,
    "num_non_test_edited_lines": 26,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/quantization/fp8_utils.py b/python/sglang/srt/layers/quantization/fp8_utils.py\nindex 0e1640fcf..86d8155f8 100644\n--- a/python/sglang/srt/layers/quantization/fp8_utils.py\n+++ b/python/sglang/srt/layers/quantization/fp8_utils.py\n@@ -369,27 +369,15 @@ def block_quant_dequant(\n     The output is an unquantized tensor with dtype.\n     \"\"\"\n     block_n, block_k = block_size[0], block_size[1]\n-    n, k = x_q_block.shape\n-    n_tiles = (n + block_n - 1) // block_n\n-    k_tiles = (k + block_k - 1) // block_k\n-    assert n_tiles == x_s.shape[0]\n-    assert k_tiles == x_s.shape[1]\n-\n-    x_dq_block = torch.empty_like(x_q_block, dtype=dtype)\n+    *_, n, k = x_q_block.shape\n \n-    for j in range(n_tiles):\n-        for i in range(k_tiles):\n-            x_q_block_tile = x_q_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile = x_dq_block[\n-                j * block_n : min((j + 1) * block_n, n),\n-                i * block_k : min((i + 1) * block_k, k),\n-            ]\n-            x_dq_block_tile[:, :] = x_q_block_tile.to(torch.float32) * x_s[j][i]\n+    # ... n_scale k_scale -> ... (n_scale block_n) (k_scale block_k)\n+    x_scale_repeat = x_s.repeat_interleave(block_n, dim=-2).repeat_interleave(\n+        block_k, dim=-1\n+    )\n+    x_scale_repeat = x_scale_repeat[..., :n, :k]\n \n-    return x_dq_block\n+    return (x_q_block.to(torch.float32) * x_scale_repeat).to(dtype)\n \n \n def channel_quant_to_tensor_quant(",
  "apis": [
    "sglang.srt.layers.quantization.fp8_utils.block_quant_dequant"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/quantization/fp8.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/quantization/fp8_utils.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes a non-trivial modification to source code (removing nested loops and using vectorized operations) for calculating dequantization, optimizing the performance of the block_quant_dequant function at the CPU level. It is applied on a non-test file and directly impacts the performance of a high-level API. Though the commit message simply states a \"Minor speed up\", the actual code change is a performance optimization. Therefore, the commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "The commit makes performance improvements to the block_quant_dequant function in the fp8_utils module by refactoring the dequantization process and removing an explicit loop, thereby speeding it up. This change affects the public API provided by sglang.srt.layers.quantization.fp8_utils.block_quant_dequant."
}