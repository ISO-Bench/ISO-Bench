{
  "commit_hash": "6f560c761b2fc2f577682d0cfda62630f37a3bb0",
  "pr_url": "https://github.com/sgl-project/sglang/pull/117",
  "pr_date": "2024-01-30",
  "timeline_text": "Copy link Contributor merrymercy commented Jan 30, 2024 No description provided. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions merrymercy added 3 commits January 30, 2024 00:18 fix streaming 01a73b1 improve docs 60d795e Fix test cases dfce4e7 merrymercy merged commit 6f560c7 into main Jan 30, 2024 merrymercy deleted the fix-streaming branch January 30, 2024 01:05 CSEEduanyu mentioned this pull request Jan 26, 2025 [Bug] NCCL Crash with SIGSEGV Frequently when deploying deepseek v3 #2803 Closed 5 tasks timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Improve the control of streaming and improve the first token latency \u2026 \u2026 e04d2df \u2026in streaming ( sgl-project#117 ) NorthmanPKU pushed a commit\n        to NorthmanPKU/sglang\n      that referenced\n      this pull request May 16, 2025 [Transpiler] Adding mechanism to skip invalid transpiled kernels ( sgl\u2026 \u2026 dbbc4f2 \u2026-project#117 )\n\n* Add mechanism to the Transpiler for signaling invalid kernels due to smem usage\n\n* fixes for async transpilation\n\n* fix\n\n* update utils\n\n---------\n\nCo-authored-by: xinhaoc <chengxh_98@163.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:01:18",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Improve the control of streaming and improve the first token latency in streaming (#117)",
  "commit_message": "Improve the control of streaming and improve the first token latency in streaming (#117)",
  "commit_date": "2024-01-29T17:05:42-08:00",
  "files_changed": [
    "python/sglang/srt/managers/router/infer_batch.py",
    "python/sglang/srt/managers/router/manager.py",
    "python/sglang/srt/managers/router/model_rpc.py",
    "python/sglang/srt/managers/router/model_runner.py",
    "python/sglang/srt/managers/tokenizer_manager.py",
    "python/sglang/srt/models/llava.py",
    "python/sglang/srt/server_args.py",
    "test/srt/model/test_llama_extend.py",
    "test/srt/model/test_llava_low_api.py",
    "test/srt/test_httpserver_decode.py",
    "test/srt/test_httpserver_decode_stream.py",
    "test/srt/test_httpserver_llava.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 69,
    "num_files": 12,
    "num_hunks": 23,
    "num_non_test_edited_lines": 69,
    "num_non_test_files": 12,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex 00ada2955..c5aa88615 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -21,14 +21,17 @@ class FinishReason(Enum):\n \n \n class Req:\n-    def __init__(self, rid):\n+    def __init__(self, rid, input_text, input_ids):\n         self.rid = rid\n-        self.input_text = None\n-        self.input_ids = []\n+        self.input_text = input_text\n+        self.input_ids = input_ids\n         self.output_ids = []\n+\n+        # For vision input\n         self.pixel_values = None\n         self.image_size = None\n         self.image_offset = 0\n+\n         self.sampling_params = None\n         self.return_logprob = False\n         self.logprob_start_len = 0\n@@ -46,7 +49,7 @@ class Req:\n         self.logprob = None\n         self.normalized_logprob = None\n \n-        # for constrained decoding\n+        # For constrained decoding\n         self.regex_fsm = None\n         self.regex_fsm_state = 0\n         self.fast_forward_map = None\ndiff --git a/python/sglang/srt/managers/router/manager.py b/python/sglang/srt/managers/router/manager.py\nindex 0732d0fa8..4dc7d1f1c 100644\n--- a/python/sglang/srt/managers/router/manager.py\n+++ b/python/sglang/srt/managers/router/manager.py\n@@ -40,7 +40,7 @@ class RouterManager:\n             for obj in out_pyobjs:\n                 self.send_to_detokenizer.send_pyobj(obj)\n \n-            # async sleep for recving the subsequent request, and avoiding cache miss\n+            # async sleep for receiving the subsequent request and avoiding cache miss\n             if len(out_pyobjs) != 0:\n                 has_finished = any([obj.finished for obj in out_pyobjs])\n                 if has_finished:\ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 199a8974b..eb5fc2f43 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -17,8 +17,8 @@ from sglang.srt.constrained.fsm_cache import FSMCache\n from sglang.srt.hf_transformers_utils import get_processor, get_tokenizer\n from sglang.srt.managers.io_struct import (\n     BatchTokenIDOut,\n-    TokenizedGenerateReqInput,\n     FlushCacheReq,\n+    TokenizedGenerateReqInput,\n )\n from sglang.srt.managers.router.infer_batch import Batch, ForwardMode, Req\n from sglang.srt.managers.router.model_runner import ModelRunner\n@@ -194,6 +194,9 @@ class ModelRpcServer(rpyc.Service):\n                     if self.running_batch.is_empty():\n                         self.running_batch = None\n                         break\n+\n+                    if self.out_pyobjs and self.running_batch.reqs[0].stream:\n+                        break\n             else:\n                 # check the available size\n                 available_size = (\n@@ -208,8 +211,7 @@ class ModelRpcServer(rpyc.Service):\n                     )\n \n         if self.running_batch is not None and self.tp_rank == 0:\n-            if self.decode_forward_ct >= 20:\n-                self.decode_forward_ct = 0\n+            if self.decode_forward_ct % 20 == 0:\n                 num_used = self.max_total_num_token - (\n                     self.token_to_kv_pool.available_size()\n                     + self.tree_cache.evictable_size()\n@@ -225,11 +227,8 @@ class ModelRpcServer(rpyc.Service):\n         self,\n         recv_req: TokenizedGenerateReqInput,\n     ):\n-        req = Req(recv_req.rid)\n-        req.input_text = recv_req.input_text\n-        req.input_ids = recv_req.input_ids\n+        req = Req(recv_req.rid, recv_req.input_text, recv_req.input_ids)\n         req.pixel_values = recv_req.pixel_values\n-        req.image_size = recv_req.image_size\n         if req.pixel_values is not None:\n             pad_value = [\n                 (recv_req.image_hash) % self.model_config.vocab_size,\n@@ -240,6 +239,7 @@ class ModelRpcServer(rpyc.Service):\n             req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\n                 req.input_ids, pad_value, req.pixel_values.shape, req.image_size\n             )\n+            req.image_size = recv_req.image_size\n         req.sampling_params = recv_req.sampling_params\n         req.return_logprob = recv_req.return_logprob\n         req.logprob_start_len = recv_req.logprob_start_len\n@@ -327,9 +327,11 @@ class ModelRpcServer(rpyc.Service):\n                     req.extend_input_len + req.max_new_tokens() + new_batch_total_tokens\n                     < available_size\n                 ):\n+                    # Undo the insertion\n                     delta = self.tree_cache.dec_ref_counter(req.last_node)\n                     available_size += delta\n                 else:\n+                    # Add this request to the running batch\n                     self.token_to_kv_pool.add_refs(req.prefix_indices)\n                     can_run_list.append(req)\n                     new_batch_total_tokens += (\n@@ -421,7 +423,7 @@ class ModelRpcServer(rpyc.Service):\n                 return\n \n         # Update batch tensors\n-        self.decode_forward_ct += 1\n+        self.decode_forward_ct = (self.decode_forward_ct + 1) % (1 << 30)\n         batch.prepare_for_decode()\n \n         # Forward\n@@ -454,7 +456,13 @@ class ModelRpcServer(rpyc.Service):\n                 unfinished_indices.append(i)\n \n             if req.finished or (\n-                req.stream and self.decode_forward_ct % self.stream_interval == 0\n+                (\n+                    req.stream\n+                    and (\n+                        self.decode_forward_ct % self.stream_interval == 0\n+                        or len(req.output_ids) == 1\n+                    )\n+                )\n             ):\n                 output_rids.append(req.rid)\n                 output_tokens.append(req.output_ids)\ndiff --git a/python/sglang/srt/managers/router/model_runner.py b/python/sglang/srt/managers/router/model_runner.py\nindex c85ec534d..7d72c6c70 100644\n--- a/python/sglang/srt/managers/router/model_runner.py\n+++ b/python/sglang/srt/managers/router/model_runner.py\n@@ -7,7 +7,6 @@ from typing import List\n \n import numpy as np\n import torch\n-import sglang\n from sglang.srt.managers.router.infer_batch import Batch, ForwardMode\n from sglang.srt.memory_pool import ReqToTokenPool, TokenToKVPool\n from sglang.srt.utils import is_multimodal_model\n@@ -16,6 +15,8 @@ from vllm.model_executor.layers.quantization.awq import AWQConfig\n from vllm.model_executor.model_loader import _set_default_torch_dtype\n from vllm.model_executor.parallel_utils.parallel_state import initialize_model_parallel\n \n+import sglang\n+\n logger = logging.getLogger(\"model_runner\")\n \n \ndiff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex d08b33634..2213858bf 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -18,9 +18,9 @@ from sglang.srt.hf_transformers_utils import (\n )\n from sglang.srt.managers.io_struct import (\n     BatchStrOut,\n+    FlushCacheReq,\n     GenerateReqInput,\n     TokenizedGenerateReqInput,\n-    FlushCacheReq,\n )\n from sglang.srt.mm_utils import expand2square, process_anyres_image\n from sglang.srt.sampling_params import SamplingParams\ndiff --git a/python/sglang/srt/models/llava.py b/python/sglang/srt/models/llava.py\nindex cd3e93cbd..efc362f59 100644\n--- a/python/sglang/srt/models/llava.py\n+++ b/python/sglang/srt/models/llava.py\n@@ -158,7 +158,7 @@ class LlavaLlamaForCausalLM(nn.Module):\n                                     num_patch_height, num_patch_width, height, width, -1\n                                 )\n                             else:\n-                                raise NotImplementedError\n+                                raise NotImplementedError()\n                             if \"unpad\" in self.mm_patch_merge_type:\n                                 image_feature = image_feature.permute(\n                                     4, 0, 2, 1, 3\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 5fcb6f5c2..17e436d8d 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -19,7 +19,7 @@ class ServerArgs:\n     schedule_heuristic: str = \"lpm\"\n     schedule_conservativeness: float = 1.0\n     random_seed: int = 42\n-    stream_interval: int = 2\n+    stream_interval: int = 8\n     disable_log_stats: bool = False\n     log_stats_interval: int = 10\n     log_level: str = \"info\"\n@@ -132,7 +132,7 @@ class ServerArgs:\n             \"--stream-interval\",\n             type=int,\n             default=ServerArgs.stream_interval,\n-            help=\"The interval in terms of token length for streaming\",\n+            help=\"The interval (or buffer size) for streaming in terms of the token length. A smaller value makes streaming smoother, while a larger value makes the throughput higher\",\n         )\n         parser.add_argument(\n             \"--log-level\",\ndiff --git a/test/srt/model/test_llama_extend.py b/test/srt/model/test_llama_extend.py\nindex b01549878..ae8df9d05 100644\n--- a/test/srt/model/test_llama_extend.py\n+++ b/test/srt/model/test_llama_extend.py\n@@ -28,7 +28,7 @@ def test_generate_worker(model_path, tp_rank, tp_size):\n \n     reqs = []\n     for i in range(len(prompts)):\n-        req = Req(i)\n+        req = Req(i, None, None)\n         req.input_ids = tokenizer.encode(prompts[i])[:cut_num]\n         req.sampling_params = sampling_params\n         reqs.append(req)\ndiff --git a/test/srt/model/test_llava_low_api.py b/test/srt/model/test_llava_low_api.py\nindex 00cdd622f..f6a77a74d 100644\n--- a/test/srt/model/test_llava_low_api.py\n+++ b/test/srt/model/test_llava_low_api.py\n@@ -112,6 +112,7 @@ def test_generate_worker(\n     prefill_params = (\n         torch.tensor(np.array(input_ids)).cuda(),\n         np.array(pixel_values),\n+        [None],\n         [offset],\n         *params,\n     )\ndiff --git a/test/srt/test_httpserver_decode.py b/test/srt/test_httpserver_decode.py\nindex 21ec0be6a..b26eb030d 100644\n--- a/test/srt/test_httpserver_decode.py\n+++ b/test/srt/test_httpserver_decode.py\n@@ -1,5 +1,8 @@\n \"\"\"\n+Usage:\n python3 -m sglang.launch_server --model-path TinyLlama/TinyLlama-1.1B-Chat-v0.4 --port 30000\n+python3 test_httpserver_decode.py\n+\n \n Output:\n The capital of France is Paris.\\nThe capital of the United States is Washington, D.C.\\nThe capital of Canada is Ottawa.\\nThe capital of Japan is Tokyo\ndiff --git a/test/srt/test_httpserver_decode_stream.py b/test/srt/test_httpserver_decode_stream.py\nindex e397f137d..3d63e66cb 100644\n--- a/test/srt/test_httpserver_decode_stream.py\n+++ b/test/srt/test_httpserver_decode_stream.py\n@@ -1,5 +1,7 @@\n \"\"\"\n+Usage:\n python3 -m sglang.launch_server --model-path TinyLlama/TinyLlama-1.1B-Chat-v0.4 --port 30000\n+python3 test_httpserver_decode_stream.py\n \n Output:\n The capital of France is Paris.\\nThe capital of the United States is Washington, D.C.\\nThe capital of Canada is Ottawa.\\nThe capital of Japan is Tokyo\ndiff --git a/test/srt/test_httpserver_llava.py b/test/srt/test_httpserver_llava.py\nindex 042f4229d..25bb79c81 100644\n--- a/test/srt/test_httpserver_llava.py\n+++ b/test/srt/test_httpserver_llava.py\n@@ -1,5 +1,7 @@\n \"\"\"\n+Usage:\n python3 -m sglang.launch_server --model-path liuhaotian/llava-v1.5-7b --tokenizer-path llava-hf/llava-1.5-7b-hf --port 30000\n+python3 test_httpserver_llava.py\n \n Output:\n The image features a man standing on the back of a yellow taxi cab, holding\n@@ -64,9 +66,12 @@ def test_streaming(args):\n     )\n \n     prev = 0\n-    for chunk in response.iter_lines(decode_unicode=False, delimiter=b\"\\0\"):\n-        if chunk:\n-            data = json.loads(chunk.decode())\n+    for chunk in response.iter_lines(decode_unicode=False):\n+        chunk = chunk.decode(\"utf-8\")\n+        if chunk and chunk.startswith(\"data:\"):\n+            if chunk == \"data: [DONE]\":\n+                break\n+            data = json.loads(chunk[5:].strip(\"\\n\"))\n             output = data[\"text\"].strip()\n             print(output[prev:], end=\"\", flush=True)\n             prev = len(output)",
  "apis": [
    "sglang.srt.managers.router.Router",
    "sglang.srt.server_args.ServerArgs"
  ],
  "affected_paths": [],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies several non-test source files to adjust the streaming behavior. It changes how the first token latency is handled (e.g., modifying the decode_forward counter logic and stream_interval parameter) and adds conditions to affect streaming output. These modifications are intended to optimize latency and throughput of the streaming API. It is not just a simple refactoring or a bug-fix; it alters the execution logic to improve performance of a high-level API on CPU. Therefore, the commit meets the performance/optimization criteria.",
  "llm_api_reason": "The commit makes several changes in the router layer and server configuration. In the file infer_batch.py the Req class\u2019s __init__ is updated to accept additional parameters (input_text and input_ids), and in model_rpc.py and manager.py the streaming control logic is adjusted (e.g. changing the condition on decode_forward_ct and modifying the streaming interval). In addition, the ServerArgs configuration is updated to use a different stream_interval along with a more detailed help message. These changes directly affect the operation of the request routing and streaming behavior. Based on the SGLang core API list, the affected high\u2010level APIs are the request routing API (sglang.srt.managers.router.Router) and the server configuration API (sglang.srt.server_args.ServerArgs)."
}