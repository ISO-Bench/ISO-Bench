{
  "commit_hash": "c2f212d672ccaf8a1e5ef09099e981d943600b14",
  "pr_url": "https://github.com/sgl-project/sglang/pull/2966",
  "pr_date": "2025-01-18",
  "timeline_text": "Copy link Collaborator BBuf commented Jan 18, 2025 main branch: lightning-attention-decode-performance:\n    batch_size  seq_len  Original PyTorch Implementation  Triton Implementation\n0          1.0      1.0                       384.128004             312.096000\n1          2.0      1.0                       373.088002             311.919987\n2          3.0      1.0                       374.751985             310.431987\n3          4.0      1.0                       375.216007             314.303994\n4          5.0      1.0                       372.031987             314.911991\n5          6.0      1.0                       372.287989             312.703997\n6          7.0      1.0                       373.600006             315.104008\n7          8.0      1.0                       372.191995             311.648011\n8          9.0      1.0                       373.504013             312.848002\n9         10.0      1.0                       374.944001             312.000006\n10        11.0      1.0                       373.120010             312.096000\n11        12.0      1.0                       371.616006             315.775990\n12        13.0      1.0                       371.807992             314.112008\n13        14.0      1.0                       371.071994             314.080000\n14        15.0      1.0                       371.583998             324.992001\n15        16.0      1.0                       372.447997             338.496000\n16        17.0      1.0                       368.319988             346.047997\n17        18.0      1.0                       365.520000             352.800012\n18        19.0      1.0                       365.935981             362.496018\n19        20.0      1.0                       367.152005             370.368004\n20        21.0      1.0                       369.120002             377.088010\n21        22.0      1.0                       378.975987             387.071997\n22        23.0      1.0                       388.736010             395.359993\n23        24.0      1.0                       396.256000             402.079999\n24        25.0      1.0                       404.736012             410.896003\n25        26.0      1.0                       411.872000             420.320004\n26        27.0      1.0                       420.704007             428.880006\n27        28.0      1.0                       428.063989             437.103987\n28        29.0      1.0                       436.576009             441.280007\n29        30.0      1.0                       446.720004             450.031996\n30        31.0      1.0                       453.967988             458.559990\n31        32.0      1.0                       460.927993             466.399997 pr: lightning-attention-decode-performance:\n    batch_size  seq_len  Original PyTorch Implementation  Triton Implementation\n0          1.0      1.0                       379.135996             297.376007\n1          2.0      1.0                       370.447993             297.248006\n2          3.0      1.0                       376.271993             297.280014\n3          4.0      1.0                       377.391994             297.695994\n4          5.0      1.0                       379.007995             297.695994\n5          6.0      1.0                       371.087998             298.720002\n6          7.0      1.0                       371.183991             298.272014\n7          8.0      1.0                       372.864008             300.576001\n8          9.0      1.0                       373.695999             300.448000\n9         10.0      1.0                       373.439997             298.287988\n10        11.0      1.0                       373.663992             298.848003\n11        12.0      1.0                       372.031987             298.224002\n12        13.0      1.0                       372.943997             300.096005\n13        14.0      1.0                       373.344004             299.216002\n14        15.0      1.0                       370.528013             301.472008\n15        16.0      1.0                       372.736007             300.927997\n16        17.0      1.0                       364.672005             295.264006\n17        18.0      1.0                       367.967993             296.095997\n18        19.0      1.0                       366.560012             295.583993\n19        20.0      1.0                       365.920007             290.399998\n20        21.0      1.0                       370.496005             291.440010\n21        22.0      1.0                       373.279989             295.759976\n22        23.0      1.0                       380.735993             294.784009\n23        24.0      1.0                       392.623991             295.967996\n24        25.0      1.0                       402.000010             293.152004\n25        26.0      1.0                       408.960015             294.559985\n26        27.0      1.0                       419.072002             293.056011\n27        28.0      1.0                       425.024003             293.504000\n28        29.0      1.0                       434.816003             294.719994\n29        30.0      1.0                       444.768012             297.280014\n30        31.0      1.0                       453.664005             305.375993\n31        32.0      1.0                       460.272014             307.711989 By removing explicit padding in lightning_attn_decode triton kernel and directly computing through a mask in the triton kernel, the overhead is significantly reduced. This approach results in a 30%-50% end2end speedup compared to the origin PyTorch MiniMax-Text-01 decoding version. @zhyncs Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 6 zhyncs, Swipe4057, zhaochenyang20, FlamingoPg, antferdom, and kir152 reacted with thumbs up emoji All reactions \ud83d\udc4d 6 reactions optimize MiniMax-Text-01 lightning_attn_decode triton c7039e6 BBuf requested a review\n  from zhyncs January 18, 2025 15:22 zhyncs requested a review\n  from ispobock January 18, 2025 15:24 zhyncs approved these changes Jan 18, 2025 View reviewed changes Hide details View details zhyncs merged commit c2f212d into main Jan 18, 2025 3 checks passed Uh oh! There was an error while loading. Please reload this page . zhyncs deleted the optimize_lighting_attention_decode_triton branch January 18, 2025 15:41 Copy link yzhangcs commented Jan 18, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @BBuf Hi, it looks like BLOCK_SIZE does not actually work in this kernel? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author BBuf commented Jan 19, 2025 @BBuf Hi, it looks BLOCK_SIZE does not actually work in this kernel? Yeah, I'll fix it, thanks. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . This was referenced Jan 19, 2025 fix file name spelling mistake and useless variable in minmax-text-01-lightning_attention #2969 Closed fix file name spelling mistake and useless variable in minmax-text-01-lightning_attention #2971 Merged yzhangcs mentioned this pull request Feb 5, 2025 [Feature Request] Lightning attention support fla-org/flash-linear-attention#164 Closed timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 optimize MiniMax-Text-01 lightning_attn_decode triton ( sgl-project#2966 ) fb38efd Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:46",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "MiniMax-Text-01"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=MiniMax-Text-01 --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model MiniMax-Text-01 --dataset-name random --num-prompts 100",
  "commit_subject": "optimize MiniMax-Text-01 lightning_attn_decode triton (#2966)",
  "commit_message": "optimize MiniMax-Text-01 lightning_attn_decode triton (#2966)",
  "commit_date": "2025-01-18T23:41:01+08:00",
  "files_changed": [
    "benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 69,
    "num_files": 1,
    "num_hunks": 6,
    "num_non_test_edited_lines": 69,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\nindex 1a2036dc0..4ce7f2b49 100644\n--- a/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n+++ b/benchmark/kernels/minmax-text-01-lighting_attention/benchmark_lighting_attention_decode.py\n@@ -23,7 +23,10 @@ def _decode_kernel(\n     h: tl.constexpr,\n     n: tl.constexpr,\n     d: tl.constexpr,\n+    d_original: tl.constexpr,\n     e: tl.constexpr,\n+    e_original: tl.constexpr,\n+    BLOCK_SIZE: tl.constexpr = 32,\n ):\n     off_bh = tl.program_id(0)\n     off_h = off_bh % h\n@@ -39,21 +42,38 @@ def _decode_kernel(\n     d_idx = tl.arange(0, d)\n     e_idx = tl.arange(0, e)\n \n-    q = tl.load(Q + qk_offset + d_idx)\n-    k = tl.load(K + qk_offset + d_idx)\n-    v = tl.load(V + v_offset + e_idx)\n+    # Create masks for original dimensions\n+    d_mask = d_idx < d_original\n+    e_mask = e_idx < e_original\n \n-    kv = tl.load(KV + kv_offset + d_idx[:, None] * e + e_idx[None, :])\n+    # Load with masking\n+    q = tl.load(Q + qk_offset + d_idx, mask=d_mask, other=0.0)\n+    k = tl.load(K + qk_offset + d_idx, mask=d_mask, other=0.0)\n+    v = tl.load(V + v_offset + e_idx, mask=e_mask, other=0.0)\n \n+    # Load KV with 2D masking\n+    kv = tl.load(\n+        KV + kv_offset + d_idx[:, None] * e + e_idx[None, :],\n+        mask=(d_mask[:, None] & e_mask[None, :]),\n+        other=0.0,\n+    )\n+\n+    # Compute outer product using element-wise operations\n     k_v_prod = k[:, None] * v[None, :]\n     kv = ratio * kv + k_v_prod\n \n+    # Store KV with 2D masking\n     tl.store(\n-        KV + kv_offset + d_idx[:, None] * e + e_idx[None, :], kv.to(KV.dtype.element_ty)\n+        KV + kv_offset + d_idx[:, None] * e + e_idx[None, :],\n+        kv.to(KV.dtype.element_ty),\n+        mask=(d_mask[:, None] & e_mask[None, :]),\n     )\n \n+    # Compute matrix-vector multiplication using element-wise operations and reduction\n     o = tl.sum(q[:, None] * kv, axis=0)\n-    tl.store(Out + o_offset + e_idx, o.to(Out.dtype.element_ty))\n+\n+    # Store output with masking\n+    tl.store(Out + o_offset + e_idx, o.to(Out.dtype.element_ty), mask=e_mask)\n \n \n def lightning_attn_decode(q, k, v, kv, s):\n@@ -62,26 +82,27 @@ def lightning_attn_decode(q, k, v, kv, s):\n     e = v.shape[-1]\n     assert n == 1, \"Sequence length must be 1 in decode mode\"\n \n-    # Pad dimensions to power of 2\n+    # Get padded dimensions (power of 2)\n     d_padded = next_power_of_2(d)\n     e_padded = next_power_of_2(e)\n \n-    # Pad inputs\n-    q_padded = F.pad(q, (0, d_padded - d))\n-    k_padded = F.pad(k, (0, d_padded - d))\n-    v_padded = F.pad(v, (0, e_padded - e))\n-    kv_padded = F.pad(kv, (0, e_padded - e, 0, d_padded - d))\n-\n-    # Ensure inputs are contiguous\n-    q_padded = q_padded.contiguous()\n-    k_padded = k_padded.contiguous()\n-    v_padded = v_padded.contiguous()\n-    kv_padded = kv_padded.contiguous().to(torch.float32)\n-    s = s.contiguous()\n-\n     # Create output tensor (padded)\n     o_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n \n+    # Create padded tensors without actually padding the data\n+    q_padded = torch.empty(b, h, n, d_padded, dtype=q.dtype, device=q.device)\n+    k_padded = torch.empty(b, h, n, d_padded, dtype=k.dtype, device=k.device)\n+    v_padded = torch.empty(b, h, n, e_padded, dtype=v.dtype, device=v.device)\n+    kv_padded = torch.empty(\n+        b, h, d_padded, e_padded, dtype=torch.float32, device=kv.device\n+    )\n+\n+    # Copy data to padded tensors\n+    q_padded[..., :d] = q\n+    k_padded[..., :d] = k\n+    v_padded[..., :e] = v\n+    kv_padded[..., :d, :e] = kv\n+\n     # Launch kernel\n     grid = (b * h, 1)\n     _decode_kernel[grid](\n@@ -95,10 +116,12 @@ def lightning_attn_decode(q, k, v, kv, s):\n         h=h,\n         n=n,\n         d=d_padded,\n+        d_original=d,\n         e=e_padded,\n+        e_original=e,\n     )\n \n-    # Remove padding\n+    # Get unpadded outputs\n     o = o_padded[..., :e]\n     kv_out = kv_padded[..., :d, :e]\n \n@@ -351,6 +374,8 @@ def test_lightning_attention_implementations(model_params):\n         msg=\"Lightning attention implementations produce different kv results\",\n     )\n \n+    print(\"\u2705 Two implementations match\")\n+\n \n def _build_slope_tensor(n_attention_heads: int):\n     def get_slopes(n):\n@@ -375,7 +400,7 @@ def _build_slope_tensor(n_attention_heads: int):\n \n \n def get_benchmark():\n-    batch_size_range = [2**i for i in range(0, 12)]  # max 2048\n+    batch_size_range = [i for i in range(1, 33)]  # max 32\n     seq_length_range = [1]  # decode mode sequence length is fixed to 1\n     configs = list(itertools.product(batch_size_range, seq_length_range))",
  "apis": [
    "lightning_attn_decode",
    "triton_lightning_attn_decode",
    "get_benchmark</APIS]"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/benchmark/kernels/minmax-text-01-lightning_attention/benchmark_lightning_attention_prefill.py",
    "/path/to/repos/sglang/sgl-kernel/benchmark/bench_lightning_attention_decode.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a benchmark source file and changes the kernel implementation that handles tensor padding and masking in the lightning attention decode function. The modifications introduce masking and remove extra padding operations, which are non-trivial changes that directly affect the performance of the function by reducing unnecessary computation and memory usage. Although the commit message references \"optimize\", the code modifications indeed focus on performance improvements rather than being a simple refactor or bug fix, and they affect CPU performance. Therefore, these changes meet the given conditions for a performance/optimization-related commit.",
  "llm_api_reason": "The commit refactors and optimizes the Triton-based implementation of Lightning Attention decoding for MiniMax-Text-01. It adds new parameters (e.g. d_original, e_original, BLOCK_SIZE) and applies masking in the kernel to correctly handle input dimensions without explicit padding, and it also adjusts benchmark configuration. These changes affect the high\u2010level Python functions that implement the decode operation via Triton (and its variants) which serve as the public API for performing decode mode attention."
}