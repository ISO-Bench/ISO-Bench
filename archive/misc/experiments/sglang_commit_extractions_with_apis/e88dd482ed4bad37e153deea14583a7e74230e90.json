{
  "commit_hash": "e88dd482ed4bad37e153deea14583a7e74230e90",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6038",
  "pr_date": "2025-05-08",
  "timeline_text": "Copy link Collaborator JustinTong0323 commented May 6, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions JustinTong0323 and others added 4 commits May 6, 2025 02:47 change model for CI \u2026 e2fdca5 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> add performance CI for VLM, not add in github flow yet \u2026 f3fcc2e Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> add test to CI \u2026 00bbbfc Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> Merge branch 'main' into vlm_ci_performance ff01c24 JustinTong0323 marked this pull request as ready for review May 6, 2025 04:06 JustinTong0323 requested review from merrymercy , Ying1123 and zhyncs as code owners May 6, 2025 04:06 JustinTong0323 changed the title [WIP][CI]Add performance CI for VLM [CI]Add performance CI for VLM May 6, 2025 zhaochenyang20 approved these changes May 6, 2025 View reviewed changes Copy link Collaborator zhaochenyang20 commented May 6, 2025 Why all the CI is skipped? \ud83e\udd14 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . minor \u2026 4e803ab Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> JustinTong0323 mentioned this pull request May 6, 2025 [Feature] add more CIs for VLM #5249 Closed 2 tasks zhyncs requested changes May 6, 2025 View reviewed changes python/sglang/bench_serving.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/bench_serving.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . JustinTong0323 and others added 2 commits May 6, 2025 22:34 minor: delete unused debug print and package \u2026 97162e3 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> Merge branch 'main' into vlm_ci_performance ca9fc4d JustinTong0323 mentioned this pull request May 7, 2025 VLM SGLang Tracker zhaochenyang20/Awesome-ML-SYS-Tutorial#111 Open JustinTong0323 added 3 commits May 7, 2025 04:14 minor: clean up codes \u2026 06b7df3 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> update \u2026 57aae3d Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> fix \u2026 ff61cd2 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> mickqian requested changes May 7, 2025 View reviewed changes python/sglang/bench_serving.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . mickqian approved these changes May 8, 2025 View reviewed changes Hide details View details zhaochenyang20 merged commit e88dd48 into sgl-project : main May 8, 2025 47 of 56 checks passed Uh oh! There was an error while loading. Please reload this page . functionstackx mentioned this pull request May 8, 2025 [ROCm][CI]: add VLM PR CI for parity with NVIDIA #6130 Closed 6 tasks RunkaiTao pushed a commit\n        to RunkaiTao/sglang\n      that referenced\n      this pull request May 9, 2025 [CI]Add performance CI for VLM ( sgl-project#6038 ) \u2026 eba4258 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> JustinTong0323 deleted the vlm_ci_performance branch May 11, 2025 06:18 lifuhuang pushed a commit\n        to lifuhuang/sglang\n      that referenced\n      this pull request May 17, 2025 [CI]Add performance CI for VLM ( sgl-project#6038 ) \u2026 2aba29d Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request May 23, 2025 Rebase 4_6_post_4 to master_next ( sgl-project#47 ) \u2026 bc7d46c * Use device_id in dist init to reduce NCCL communicator warmup & creation overhead ( sgl-project#5728 )\n\n* [fix] fix potential bumpy throughtput with deepgemm ( sgl-project#5722 )\n\n* Resolves the `404 Not Found` error when running `compile_deep_gemm.py` in multi-node setups ( sgl-project#5720 )\n\n* perf: update H20 fused_moe_triton kernel config to get higher throughput during prefilling ( sgl-project#5716 )\n\n* we fix the non existent access of `decrypted_config_file` ( sgl-project#5685 )\n\n* CI: rewrite test_vision_chunked_prefill to speedup ( sgl-project#5682 )\n\n* Fuse MLA set kv cache kernel ( sgl-project#5748 )\n\n* Update amd docker image to `sglang:v0.4.5.post3-rocm630`. ( sgl-project#5697 )\n\n* [feature] support for roberta embedding models ( sgl-project#5730 )\n\n* [fix] fix bench_one_batch_server ( sgl-project#5607 )\n\n* support for the DeepSeek model by enabling streaming response parsing ( sgl-project#5592 )\n\n* fix: Use `is not None` instead of `!= None` for None checks. ( sgl-project#5687 )\n\n* Add Llama 4 to FA3 test ( sgl-project#5509 )\n\n* [misc] more decode step log for batch_one_batch ( sgl-project#5565 )\n\n* Handle JSONDecodeError while processing request data ( sgl-project#5599 )\n\n* fix(srt): check if sample_indices is not None before usage. ( sgl-project#5633 )\n\n* update llguidance to 0.7.11; adds StructTag ( sgl-project#4870 )\n\n* Use sgl-kernel sgl_per_token_group_quant_int8 ( sgl-project#4971 )\n\n* Add memory_saver check ( sgl-project#4986 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* add switch to disable open api doc ( sgl-project#3744 )\n\nSigned-off-by: congcongke <zhanweidu@163.com>\n\n* Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\" ( sgl-project#5772 )\n\n* Fix eagle test case ( sgl-project#5776 )\n\n* Split local attention test from fa3 test ( sgl-project#5774 )\n\n* Revert \"Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\"\" ( sgl-project#5777 )\n\n* Simplify FA3 tests ( sgl-project#5779 )\n\n* Revert \"[fix] fix bench_one_batch_server\" ( sgl-project#5785 )\n\n* Revert \"Use device_id in dist init to reduce NCCL communicator warmup & creation overhead\" ( sgl-project#5786 )\n\n* [CI] Tune threshold ( sgl-project#5787 )\n\n* [CI] fix port conflicts ( sgl-project#5789 )\n\n* [CI] Fix ci tests ( sgl-project#5769 )\n\n* [PD]Reduce kv transfer threads ( sgl-project#5791 )\n\n* [CI] Fix test case ( sgl-project#5790 )\n\n* Add 8-GPU Test for Deepseek-V3  ( sgl-project#5691 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* Release v0.4.6 ( sgl-project#5795 )\n\n* Update nightly-test.yml ( sgl-project#5797 )\n\n* [CI] Improve github summary & enable fa3 for more models ( sgl-project#5796 )\n\n* [Docs] update grafana setup guide in production metrics ( sgl-project#5643 )\n\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\n\n* [Misc] add structure logging, write to file and log tracing for SGL Router\n\n* Improve overlap scheduling ( sgl-project#5788 )\n\n* Add Cutlass MLA attention backend ( sgl-project#5390 )\n\n* chore: upgrade sgl-kernel 0.1.0 ( sgl-project#5690 )\n\n* Dockerfile.dev pip scikit_build_core ( sgl-project#5807 )\n\n* Add a doc to fix sgl-kernel build link error in py39 with ccache ( sgl-project#5809 )\n\n* Turn on overlap scheduler for multimodal models ( sgl-project#5771 )\n\n* Tiny refactor DefaultModelLoader.Source ( sgl-project#5482 )\n\n* [Docs] Replace lists with tables for cleanup and readability in server_arguments ( sgl-project#5276 )\n\n* Revert \"Tiny refactor DefaultModelLoader.Source\" ( sgl-project#5825 )\n\n* Feat: add support for thinking mode via chat_template_kwargs.enable_t\u2026 ( sgl-project#5551 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: fix the error where the content is None when reasoning and tool \u2026 ( sgl-project#5838 )\n\n* feat: Add fused moe triton config for qwen3 moe on h100 ( sgl-project#5833 )\n\n* fused moe triton tuning script support qwen3 ( sgl-project#5842 )\n\n* feat: Add fused moe triton config for qwen3bf16 moe on h20 ( sgl-project#5839 )\n\n* [PD] support pd fake transfer for warmup ( sgl-project#5726 )\n\n* [config] qwen3moe_tune_h20 fp8 tp4 ( sgl-project#5846 )\n\n* [Doc] Recover history of server_arguments.md ( sgl-project#5851 )\n\n* feat: Add fused moe triton config for qwen3-30b-fp8 moe on h20 ( sgl-project#5850 )\n\n* [CI] test chunked prefill more ( sgl-project#5798 )\n\n* ROCm: update AITER ( sgl-project#5816 )\n\n* [Feat] QWen-1M context support[1/2]: Update block sparse attention backend utils kernel ( sgl-project#5847 )\n\nCo-authored-by: sighingnow <sighingnow@gmail.com>\n\n* [Fix] Missing bootstrap_port field ( sgl-project#5823 )\n\n* feat: update is_fa3_default_architecture ( sgl-project#5854 )\n\n* add fused moe config for qwen3moe fp8/bf16 ( sgl-project#5849 )\n\n* chore: bump v0.4.6.post1 ( sgl-project#5845 )\n\n* Support `max_completion_tokens` for OpenAIChatCompletions ( sgl-project#5857 )\n\n* simplify fused_moe config logging ( sgl-project#5801 )\n\n* [CI] tune the test order to warmup the server ( sgl-project#5860 )\n\n* Cutlass MLA decode - fix dtype error ( sgl-project#5868 )\n\n* cutlass 3.9 supported to improve fp8_blockwise_gemm ( sgl-project#5820 )\n\n* [Feature] support auto chat template ( sgl-project#4949 )\n\n* Feat: support cuda graph for LoRA ( sgl-project#4115 )\n\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\n\n* Add qwen3 30b fused moe config ( sgl-project#5859 )\n\n* [Fix] Fix a bug for flashmla to run R1 model ( sgl-project#5875 )\n\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\n\n* Add A800 fused moe config for qwen3 30b ( sgl-project#5880 )\n\n* [Misc] add service discovery for sgl router\n\n* [fix]: PyO3 macOS linking and consolidate on tracing for logging\n\n* chore: update Dockerfile ( sgl-project#5894 )\n\n* [Docs] Update docs for Qwen3 and Qwen3MoE ( sgl-project#5836 )\n\n* [Doc] Tables instead of bulletpoints for sampling doc ( sgl-project#5841 )\n\n* chore: update CODEOWNERS ( sgl-project#5895 )\n\n* [FEATURE] Enhance platform compatibility for ARM ( sgl-project#5746 )\n\n* [CI] Add test_function_calling.py to run_suite.py ( sgl-project#5896 )\n\n* Auto set draft model path for MTP ( sgl-project#5793 )\n\n* [fix] relax mem_fraction_static for h200 ( sgl-project#5893 )\n\nCo-authored-by: alcanerian <alcanerian@gmail.com>\n\n* feat: support pythonic tool call and index in tool call streaming ( sgl-project#5725 )\n\n* [Bugfix]: fix missing queue_time_start for requests from grammar_queue ( sgl-project#5696 )\n\n* Add AMD MI300x Nightly Testing. ( sgl-project#5861 )\n\n* chore: use torch 2.6 for sgl-kernel build ( sgl-project#5898 )\n\n* Fix check_env script ( sgl-project#5901 )\n\n* [PD] Fix Assertion failed: /DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels sgl-project#134 ( sgl-project#5830 )\n\n* Bump Flashinfer to 0.2.5 ( sgl-project#5870 )\n\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\n\n* [Fix] Unload lora in HF_Runner if needed ( sgl-project#5899 )\n\n* Add A800 fused moe config for qwen3 235b ( sgl-project#5900 )\n\n* Add sm_120 for blackwell ( sgl-project#5903 )\n\n* [Feature] add support kimi vl model ( sgl-project#5383 )\n\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\n\n* support vlm benchmark profile ( sgl-project#5905 )\n\n* [fix] kimi-vl test in test_vision_openai_server.py ( sgl-project#5910 )\n\n* [Misc] use parallel build for cmake in sgl-kernel ( sgl-project#5919 )\n\n* [qwen3] support qwen3 ep moe ( sgl-project#5917 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\n\n* Add TP2 MOE benchmarks for AMD. ( sgl-project#5909 )\n\n* [Feat] Scale up fa3 kernel to sm8x arch ( sgl-project#5912 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* chore: bump sgl-kernel 0.1.1 ( sgl-project#5932 )\n\n* chore: upgrade sgl-kernel 0.1.1 ( sgl-project#5933 )\n\n* Remove unused method `calculate_num_image_tokens` from qwen2_vl.py ( sgl-project#5783 )\n\n* [PP] Add pipeline parallelism ( sgl-project#5724 )\n\n* Fix lora batch processing when input lora_path contains None ( sgl-project#5930 )\n\n* add Thor & Spark ( sgl-project#5915 )\n\n* fix: correct stream response when enable_thinking is set to false ( sgl-project#5881 )\n\n* fix: update model runner ( sgl-project#5934 )\n\n* chore: bump v0.4.6.post2 ( sgl-project#5939 )\n\n* Support XiaomiMiMo/MiMo model inference ( sgl-project#5921 )\n\n* [PD] Vectorise group_concurrent_contiguous in NumPy ( sgl-project#5834 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Remove extra contiguous ( sgl-project#5953 )\n\n* Update ci test and doc for MTP api change ( sgl-project#5952 )\n\n* docs: Fix Qwen model typo ( sgl-project#5944 )\n\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\n\n* Optimize a pad operation to accelerate 25us ( sgl-project#5945 )\n\n* Properly return error response in vertex_generate HTTP endpoint ( sgl-project#5956 )\n\n* feat: add concurrency evaluation logic in mmmu benchmark ( sgl-project#5782 )\n\n* Add 1 gpu perf and 2 gpu accuracy tests for AMD MI300x CI. ( sgl-project#5960 )\n\n* feat: Refactor DeepSeekV3 function call ( sgl-project#5908 )\n\n* Remove token in token out in Native API ( sgl-project#5967 )\n\n* Support InternVL3 ( sgl-project#5350 )\n\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\n\n* Support MMMU benchmark for  InternVL ( sgl-project#5968 )\n\n* FA3 speed up: skip len operation and get batch size directly from forward batch ( sgl-project#5969 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] NIXL backend Prefill TP & Decode TP+DP ( sgl-project#5681 )\n\n* Fix set kv cache multi-stream ( sgl-project#5975 )\n\n* Overlap qk norm with two streams ( sgl-project#5977 )\n\n* fix: only upgrade nccl for cu128 ( sgl-project#5986 )\n\n* Fix Phi3 serving which was broke by earlier change ( sgl-project#5991 )\n\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [perf] H100 DeepSeek-V3 fused moe tuned config ( sgl-project#5998 )\n\n* [Fix] Suppress dynamo logging when using flashinfer backend with torch compile ( sgl-project#5992 )\n\n* [Minor] Fix duplicate method definitions in conversation.py ( sgl-project#6012 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Fix flaky issues of lora and add multi batch tests ( sgl-project#5957 )\n\n* Tool Call: Add `chat_template_kwargs` documentation ( sgl-project#5679 )\n\n* fix: fix broadcast_pyobj breaking VerlEngine ( sgl-project#5997 )\n\n* [PD] Allow customizing reserved tokens to avoid KV cache waste ( sgl-project#6002 )\n\n* Update dev container config to support live code sync and improve docker setup guide   ( sgl-project#6018 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] Optimize disaggregation ib device help info ( sgl-project#5781 )\n\n* [Test] Add flashmla attention backend test ( sgl-project#5587 )\n\n* Fix \"Avoid computing lse in Ragged Prefill when there's no prefix match\" ( sgl-project#5555 )\n\n* feat: Add a unified merge_state API ( sgl-project#5428 )\n\n* feat: append more comprehensive fields in messages instead of merely role and content ( sgl-project#5996 )\n\n* [Security][Bug] Prevent binding to all TCP interfaces ( sgl-project#5752 )\n\n* Fix prefill OOM error in the case of large page size ( sgl-project#5081 )\n\n* Fix problem of large page size with chunked prefill ( sgl-project#6046 )\n\n* docs: add Google Cloud Vertex AI in Adoption and Sponsorship ( sgl-project#6047 )\n\n* docs: add new blog ( sgl-project#6048 )\n\n* Fix not \"import os\" ( sgl-project#6057 )\n\n* Better PD initialization ( sgl-project#5751 )\n\n* fix: deepep dockerfile, use pip install deepep. ( sgl-project#5885 )\n\n* [Fix] Fix and rename flashmla CI test ( sgl-project#6045 )\n\n* chore: upgrade cutlass 3.9.2 ( sgl-project#6004 )\n\nCo-authored-by: yizhang2077 <1109276519@qq.com>\n\n* Fix sgl-kernel build on aarch64 platforms ( sgl-project#6062 )\n\n* Add DeepEP to CI PR Test ( sgl-project#5655 )\n\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\n\n* fix custom_allreduce namespace ( sgl-project#6039 )\n\n* feat: add release workflow for SGLang kernels on aarch64 ( sgl-project#6010 )\n\nCo-authored-by: Qiaolin-Yu <liin1211@outlook.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* [Feature] Support for Ascend NPU backend ( sgl-project#3853 )\n\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\n\n* Fix the timeout for 8 gpu tests ( sgl-project#6084 )\n\n* Hint users DeepEP normal mode is incompatible with CUDA Graph ( sgl-project#5014 )\n\n* Super tiny fix doc ( sgl-project#5233 )\n\n* [Doc]Fix description for dp_size argument ( sgl-project#6063 )\n\n* feat(engine): add bootstrap parameters to generate methods (dynamo) ( sgl-project#6075 )\n\n* [refactor] slightly tidy fp8 module ( sgl-project#5993 )\n\n* Clean up fa3 test from 8 gpus ( sgl-project#6105 )\n\n* Deferring 8 GPU test ( sgl-project#6102 )\n\n* Update doc for MLA attention backends ( sgl-project#6034 )\n\n* Clean logs for DeepSeek-V3 launching ( sgl-project#6079 )\n\n* [CI]Add performance CI for VLM ( sgl-project#6038 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* adding Triton configs for DeepSeekV3 FusedMoE kernel on Blackwell ( sgl-project#6111 )\n\n* optimize pad operations in fa3 to accelarate 100+us ( sgl-project#6077 )\n\n* Overlap shared expert and routed expert computations ( sgl-project#5121 )\n\n* Tiny refactor ModelConfig.from_server_args ( sgl-project#5219 )\n\n* Tiny refactor weight loading logic ( sgl-project#5232 )\n\n* [PD] Add control to slow down a server ( sgl-project#5572 )\n\n* Change AMD test threshold ( sgl-project#6091 )\n\n* DeepEP normal support deepgemm-contiguous ( sgl-project#5626 )\n\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\n\n* [fix] fix pyproject.toml dependencies ( sgl-project#6119 )\n\n* [Feature] Add FlashAttention3 as a backend for VisionAttention ( sgl-project#5764 )\n\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\n\n* [perf] dsv3 bmm fallback to bf16 ( sgl-project#5662 )\n\n* [AMD] switch to custom allreduce regardless of MSCCL setting on ROCm ( sgl-project#6097 )\n\n* [sgl-kernel] fix: fix cu118 compile error ( sgl-project#6123 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* upgrade xgrammar to 0.1.19 ( sgl-project#6129 )\n\n* Remove unecessary is_fa3_supported check ( sgl-project#6112 )\n\n* chore: bump sgl-kernel 0.1.2 ( sgl-project#6131 )\n\n* docs: update README ( sgl-project#6132 )\n\n* [Fix] Incorrect Memory Allocation on CUDA:0 by Non-Zero CUDA Processes in TP/DP ( sgl-project#5745 )\n\n* Cutlass MLA: Disable split kv due to NVIDIA/cutlass#2274 ( sgl-project#6101 )\n\n* opt flashinfer mla cat ( sgl-project#5822 )\n\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\n\n* Update amd nightly concurrency. ( sgl-project#6141 )\n\n* feat: add thinking_budget ( sgl-project#6089 )\n\n* [Bugfix] Fix Llama4 gibberish output with long context and CUDA graph ( sgl-project#6162 )\n\n* fix bug that gpu0 occupies more memory when hicache is turned on ( sgl-project#5778 )\n\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* chore: bump v0.4.6.post3 ( sgl-project#6165 )\n\n* KV\u2011Cache\u202f(MHA, MLA): add missing start_layer\u202f/\u202fend_layer fields to MHATokenToKVPoolHost and MLATokenToKVPoolHost ( sgl-project#6016 )\n\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* [fix] fix determine_n_share_experts_fusion ( sgl-project#6118 )\n\n* Fix and Clean up chat-template requirement for VLM ( sgl-project#6114 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* [Docs]Delete duplicate content ( sgl-project#6146 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* Revert \"feat: add thinking_budget ( sgl-project#6089 )\" ( sgl-project#6181 )\n\n* Added async_encode method to Engine ( sgl-project#4701 )\n\n* Fix data parallel perf regression ( sgl-project#6183 )\n\n* Fix request abortion ( sgl-project#6184 )\n\n* Add typo checker in pre-commit ( sgl-project#6179 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* Remove duplicate IO Struct test ( sgl-project#6180 )\n\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\n\n* [PD] Add simple unit test for disaggregation feature ( sgl-project#5654 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Disabled deepep tests temporarily because it takes too much time. ( sgl-project#6186 )\n\n* feat: support loogle eval ( sgl-project#6190 )\n\n* [fix] remove mixtral from is_fa3_default_architecture ( sgl-project#6191 )\n\n* fix: handle None multimodal_inputs during merging and filtering batches in disaggregation decode mode ( sgl-project#6169 )\n\n* chore: upgrade deepgemm ( sgl-project#6073 )\n\n* chore: bump sgl-kernel v0.1.2.post1 ( sgl-project#6195 )\n\n* chore: upgrade sgl-kernel v0.1.2.post1 ( sgl-project#6196 )\n\nCo-authored-by: alcanderian <alcanderian@gmail.com>\n\n* Handle empty input string for embedding models ( sgl-project#5621 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* doc: fix the erroneous documents and example codes about Alibaba-NLP/gme-Qwen2-VL-2B-Instruct ( sgl-project#6199 )\n\n* [Docs] minor Qwen3 and reasoning parser docs fix ( sgl-project#6032 )\n\n* Improve structured outputs: fix race condition, server crash, metrics and style ( sgl-project#6188 )\n\n* [CI] Reorganize the 8 gpu tests ( sgl-project#6192 )\n\n* Add dev-deepep docker image ( sgl-project#6198 )\n\n* Replace time.time() to time.perf_counter() for benchmarking. ( sgl-project#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Update README.md ( sgl-project#6202 )\n\n* Fix release-docs.yml to not use python 3.9 ( sgl-project#6204 )\n\n* Fix start_profile does not support with_stack and record_shapes ( sgl-project#6043 )\n\n* [doc] add a note for --n-share-experts-fusion args ( sgl-project#6154 )\n\n* Performing Vocabulary Parallelism for LM Head across Attention TP Groups ( sgl-project#5558 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\n\n* Update AMD CI docker to v0.4.6.post3-rocm630. ( sgl-project#6213 )\n\n* Log if cuda graph is used & extend cuda graph capture to cuda-graph-max-bs ( sgl-project#6201 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* [CI] Fix PD mooncake dependency error ( sgl-project#6212 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Re-enable pd disaggregation test ( sgl-project#6231 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* fix some typos ( sgl-project#6209 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* [Docs] Add docs for `SGLANG_` and `SGL_` environment variables ( sgl-project#6206 )\n\n* [PP] Fix init_memory_pool desync & add PP for mixtral ( sgl-project#6223 )\n\n* Revert \"fix some typos\" ( sgl-project#6244 )\n\n* chore: add hf_xet dep ( sgl-project#6243 )\n\n* Update AMD nightly deps. ( sgl-project#6241 )\n\n* [PD] Add support for different TP sizes per DP rank ( sgl-project#5922 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Support incremental streaming of logprob/token_ids between scheduler and detokenizer ( sgl-project#6225 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* fix typo ( sgl-project#6248 )\n\n* Support tuning moe for llama 4 model ( sgl-project#6042 )\n\n* Skip the flaky test_stateful_custom_logit_processor ( sgl-project#6251 )\n\n* [Llama4] Add docs note about enable multimodal ( sgl-project#6235 )\n\n* [VERL Use Case] Add torch_memory_saver into deps ( sgl-project#6247 )\n\n* Fix two issues related to `--moe-dense-tp-size=1` ( sgl-project#5657 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\n\n* model(vlm): pixtral ( sgl-project#5084 )\n\n* [misc] deep_gemm fallback to NVRTC when NVCC not found ( sgl-project#6252 )\n\n* Enable MI325X AMD CI. ( sgl-project#6259 )\n\n* chore: bump v0.4.6.post4 ( sgl-project#6245 )\n\n* formatting fix for the rebased commit for 4.6.0_post4\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix issues in model runner and python packages\n\nfix for following issues:\n> vLLM dependency for xgrammar==0.1.17\n> 'Scheduler' object has no attribute 'device\n> 'pp_proxy_tensors' unexpected arg in HPUGraphRunner\n> TODO: Add pipeline parallelism support in HPUGraphRunner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix formatting in model runner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* base grammar fix for the is_terminated case\n\n>  'OutlinesGrammar' object has no attribute 'is_terminated'\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n---------\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: congcongke <zhanweidu@163.com>\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: Wenxuan Tan <wtan45@wisc.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: vzed <207368749+vincentzed@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DavidBao <121073073+DavidBao03@users.noreply.github.com>\nCo-authored-by: Frankey_8080 <32973306+Frank-Jie@users.noreply.github.com>\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: yan97ao <580776+yan97ao@users.noreply.github.com>\nCo-authored-by: aoshen524 <aoshen524@gmail.com>\nCo-authored-by: Micha\u0142 Moskal <michal@moskal.me>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: zhanweidu <zhanweidu@163.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Huapeng Zhou <73010314+PopSoda2002@users.noreply.github.com>\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\nCo-authored-by: Simo Lin <linsimo.mark@gmail.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: JiLi <leege233@gmail.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: PGFLMG <1106310035@qq.com>\nCo-authored-by: sighingnow <sighingnow@gmail.com>\nCo-authored-by: XTY <xutianyi1999@live.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: woodx <124784234+woodx9@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <qy254@cornell.edu>\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\nCo-authored-by: pengcuo <pengcbupt@163.com>\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Johnny <johnnync13@gmail.com>\nCo-authored-by: alcanerian <alcanerian@gmail.com>\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\nCo-authored-by: zhjunqin <zhjunqin@users.noreply.github.com>\nCo-authored-by: liwenju0 <like4hub@gmail.com>\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\nCo-authored-by: laixin <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: Ying Sheng <sqy1415@gmail.com>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: \u6c5f\u5bb6\u744b <36886416+JiangJiaWei1103@users.noreply.github.com>\nCo-authored-by: KCFindstr <shimakaze@google.com>\nCo-authored-by: xm:D <38322020+xiaomin-D@users.noreply.github.com>\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: Junrong Lin <33685709+ocss884@users.noreply.github.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: Hank Han <54751605+HanHan009527@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <liin1211@outlook.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\nCo-authored-by: Johnny <johnnynuca14@gmail.com>\nCo-authored-by: Song Zhang <70674731+botieking98@users.noreply.github.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\nCo-authored-by: ishandhanani <82981111+ishandhanani@users.noreply.github.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Minglei Zhu <mingleizhu1122@gmail.com>\nCo-authored-by: lukec <118525388+sleepcoo@users.noreply.github.com>\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\nCo-authored-by: Zhu Chen <51010608+Othame@users.noreply.github.com>\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: Yixin Dong <ubospica@gmail.com>\nCo-authored-by: xu-yfei <xu_yfei@qq.com>\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\nCo-authored-by: thyecust <tienhoayu@gmail.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: Simon (Jiyou) Li <Simon-Li@users.noreply.github.com>\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Steven Shimizu <shimizust@gmail.com>\nCo-authored-by: applesaucethebun <113181361+applesaucethebun@users.noreply.github.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nCo-authored-by: Yusong Gao <yusong.gao@gmail.com>\nCo-authored-by: alcanderian <alcanderian@gmail.com>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\nCo-authored-by: Kiv Chen <34561254+KivenChen@users.noreply.github.com> Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 [CI]Add performance CI for VLM ( sgl-project#6038 ) \u2026 ae204a5 Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 [CI]Add performance CI for VLM ( sgl-project#6038 ) \u2026 96bb8fd Signed-off-by: Xinyuan Tong <justinning0323@outlook.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:58:10",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[CI]Add performance CI for VLM (#6038)",
  "commit_message": "[CI]Add performance CI for VLM (#6038)\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>",
  "commit_date": "2025-05-07T19:20:03-07:00",
  "files_changed": [
    ".github/workflows/pr-test.yml",
    "python/sglang/bench_serving.py",
    "python/sglang/test/test_utils.py",
    "test/srt/test_bench_serving.py",
    "test/srt/test_skip_tokenizer_init.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 223,
    "num_files": 5,
    "num_hunks": 15,
    "num_non_test_edited_lines": 223,
    "num_non_test_files": 5,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/.github/workflows/pr-test.yml b/.github/workflows/pr-test.yml\nindex fb62ac488..45759c0be 100644\n--- a/.github/workflows/pr-test.yml\n+++ b/.github/workflows/pr-test.yml\n@@ -162,6 +162,18 @@ jobs:\n           cd test/srt\n           python3 -m unittest test_bench_serving.TestBenchServing.test_offline_throughput_default_fp8\n \n+      - name: Benchmark VLM offline throughput\n+        timeout-minutes: 10\n+        run: |\n+          cd test/srt\n+          python3 -m unittest test_bench_serving.TestBenchServing.test_vlm_offline_throughput\n+\n+      - name: Benchmark VLM online latency\n+        timeout-minutes: 10\n+        run: |\n+          cd test/srt\n+          python3 -m unittest test_bench_serving.TestBenchServing.test_vlm_online_latency\n+\n   performance-test-2-gpu:\n     if: (github.repository == 'sgl-project/sglang' || github.event_name == 'pull_request') &&\n         github.event.pull_request.draft == false\ndiff --git a/python/sglang/bench_serving.py b/python/sglang/bench_serving.py\nindex 9b2722126..922ba1e39 100644\n--- a/python/sglang/bench_serving.py\n+++ b/python/sglang/bench_serving.py\n@@ -58,6 +58,7 @@ class RequestFuncInput:\n     output_len: int\n     model: str\n     lora_name: str\n+    image_data: str\n     extra_request_body: Dict[str, Any]\n \n \n@@ -347,6 +348,11 @@ async def async_request_sglang_generate(\n             \"logprob_start_len\": -1,\n             **request_func_input.extra_request_body,\n         }\n+\n+        # Add image data if available\n+        if request_func_input.image_data:\n+            payload[\"image_data\"] = request_func_input.image_data\n+\n         headers = get_auth_headers()\n \n         output = RequestFuncOutput()\n@@ -510,6 +516,13 @@ def get_dataset(args, tokenizer):\n             tokenizer=tokenizer,\n             args=args,\n         )\n+    elif args.dataset_name == \"mmmu\":\n+        input_requests = sample_mmmu_requests(\n+            num_requests=args.num_prompts,\n+            tokenizer=tokenizer,\n+            fixed_output_len=args.random_output_len,\n+            random_sample=True,\n+        )\n     else:\n         raise ValueError(f\"Unknown dataset: {args.dataset_name}\")\n     return input_requests\n@@ -597,6 +610,121 @@ def download_and_cache_file(url: str, filename: Optional[str] = None):\n     return filename\n \n \n+def sample_mmmu_requests(\n+    num_requests: int,\n+    tokenizer: PreTrainedTokenizerBase,\n+    fixed_output_len: Optional[int] = None,\n+    random_sample: bool = True,\n+) -> List[Tuple[str, int, int]]:\n+    \"\"\"\n+    Sample requests from the MMMU dataset using HuggingFace datasets.\n+\n+    Args:\n+        num_requests: Number of requests to sample.\n+        tokenizer: Tokenizer to use for token counting.\n+        fixed_output_len: If provided, use this fixed output length for all requests.\n+        random_sample: Whether to randomly sample or take the first N.\n+\n+    Returns:\n+        List of tuples (prompt, prompt_token_len, output_token_len).\n+    \"\"\"\n+    try:\n+        import base64\n+        import io\n+\n+        from datasets import load_dataset\n+    except ImportError:\n+        raise ImportError(\"Please install datasets: pip install datasets\")\n+\n+    print(\"Loading MMMU dataset from HuggingFace...\")\n+\n+    try:\n+        print(\"Attempting to load MMMU Math dataset...\")\n+        mmmu_dataset = load_dataset(\"MMMU/MMMU\", \"Math\", split=\"test\")\n+        print(\n+            f\"Successfully loaded MMMU Math dataset from HuggingFace with {len(mmmu_dataset)} examples\"\n+        )\n+    except Exception as e:\n+        print(f\"Failed to load MMMU Math dataset: {e}\")\n+        raise ValueError(f\"Failed to load MMMU dataset: {e}\")\n+\n+    # Sample from the dataset\n+    if len(mmmu_dataset) > num_requests:\n+        if random_sample:\n+            # Random sample\n+            indices = random.sample(range(len(mmmu_dataset)), num_requests)\n+            sample_dataset = mmmu_dataset.select(indices)\n+        else:\n+            # Take first N\n+            sample_dataset = mmmu_dataset.select(\n+                range(min(num_requests, len(mmmu_dataset)))\n+            )\n+    else:\n+        print(f\"Dataset has less than {num_requests} examples, using all examples\")\n+        sample_dataset = mmmu_dataset\n+\n+    print(f\"Selected {len(sample_dataset)} examples for benchmarking\")\n+\n+    # Create prompts\n+    filtered_dataset = []\n+\n+    for i, example in enumerate(sample_dataset):\n+        try:\n+            # Extract image_1\n+            image = example.get(\"image_1\")\n+\n+            if image is not None:\n+                if hasattr(image, \"save\"):\n+                    # Convert RGBA images to RGB before encoding\n+                    if image.mode == \"RGBA\":\n+                        image = image.convert(\"RGB\")\n+\n+                    # Encode image to base64\n+                    buffered = io.BytesIO()\n+                    image.save(buffered, format=\"JPEG\")\n+                    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n+                    image_path = f\"data:image/jpeg;base64,{img_str}\"\n+                else:\n+                    continue\n+\n+                # Extract the question\n+                question = example.get(\"question\")\n+\n+                # Create the prompt with image, question\n+                prompt = f\"Question: {question}\\n\\nAnswer: \"\n+                prompt = tokenizer.apply_chat_template(\n+                    [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"image_url\", \"image_url\": {\"url\": image_path}},\n+                                {\"type\": \"text\", \"text\": prompt},\n+                            ],\n+                        }\n+                    ],\n+                    add_generation_prompt=True,\n+                    tokenize=False,\n+                )\n+                prompt = f\"<image>{image_path}</image>{prompt}\"\n+\n+                # Calculate token lengths\n+                # Note: This is approximate since we're not rendering the actual image tokens\n+                prompt_token_ids = tokenizer.encode(prompt)\n+                prompt_len = (\n+                    len(prompt_token_ids) + 512\n+                )  # Add estimate for image tokens\n+\n+                output_len = fixed_output_len if fixed_output_len is not None else 256\n+\n+                filtered_dataset.append((prompt, prompt_len, output_len))\n+\n+        except Exception as e:\n+            print(f\"Error processing example {i}: {e}\")\n+\n+    print(f\"\\nCreated {len(filtered_dataset)} MMMU prompts\")\n+    return filtered_dataset\n+\n+\n def sample_sharegpt_requests(\n     dataset_path: str,\n     num_requests: int,\n@@ -1004,6 +1132,15 @@ async def benchmark(\n     else:\n         lora_name = None\n \n+    if \"<image>\" in test_prompt:\n+        import re\n+\n+        image_match = re.search(r\"<image>(.*?)</image>(.*)\", test_prompt)\n+        image_data = image_match.group(1) if image_match else None\n+        test_prompt = image_match.group(2) if image_match else test_prompt\n+    else:\n+        image_data = None\n+\n     # Create the test input once\n     test_input = RequestFuncInput(\n         model=model_id,\n@@ -1012,6 +1149,7 @@ async def benchmark(\n         prompt_len=test_prompt_len,\n         output_len=min(test_output_len, 32),\n         lora_name=lora_name,\n+        image_data=image_data,\n         extra_request_body=extra_request_body,\n     )\n \n@@ -1063,6 +1201,15 @@ async def benchmark(\n         else:\n             lora_name = None\n \n+        if \"<image>\" in prompt:\n+            import re\n+\n+            image_match = re.search(r\"<image>(.*?)</image>(.*)\", prompt)\n+            image_data = image_match.group(1) if image_match else None\n+            prompt = image_match.group(2) if image_match else prompt\n+        else:\n+            image_data = None\n+\n         request_func_input = RequestFuncInput(\n             model=model_id,\n             prompt=prompt,\n@@ -1070,6 +1217,7 @@ async def benchmark(\n             prompt_len=prompt_len,\n             output_len=output_len,\n             lora_name=lora_name,\n+            image_data=image_data,\n             extra_request_body=extra_request_body,\n         )\n         tasks.append(\n@@ -1444,7 +1592,7 @@ if __name__ == \"__main__\":\n         \"--dataset-name\",\n         type=str,\n         default=\"sharegpt\",\n-        choices=[\"sharegpt\", \"random\", \"random-ids\", \"generated-shared-prefix\"],\n+        choices=[\"sharegpt\", \"random\", \"random-ids\", \"generated-shared-prefix\", \"mmmu\"],\n         help=\"Name of the dataset to benchmark on.\",\n     )\n     parser.add_argument(\ndiff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py\nindex 6bcacb427..2c0bf8759 100644\n--- a/python/sglang/test/test_utils.py\n+++ b/python/sglang/test/test_utils.py\n@@ -79,7 +79,8 @@ DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_FP8_TP1 = \"neuralmagic/Meta-Llama-3.1-8B-Ins\n DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_FP8_TP2 = \"neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8,neuralmagic/Mixtral-8x7B-Instruct-v0.1-FP8,neuralmagic/Qwen2-72B-Instruct-FP8,neuralmagic/Qwen2-57B-A14B-Instruct-FP8,neuralmagic/DeepSeek-Coder-V2-Lite-Instruct-FP8\"\n DEFAULT_MODEL_NAME_FOR_NIGHTLY_EVAL_QUANT_TP1 = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4,hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4,hugging-quants/Mixtral-8x7B-Instruct-v0.1-AWQ-INT4\"\n DEFAULT_SMALL_MODEL_NAME_FOR_TEST_QWEN = \"Qwen/Qwen2.5-1.5B-Instruct\"\n-DEFAULT_SMALL_VLM_MODEL_NAME = \"Qwen/Qwen2-VL-2B\"\n+DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n+DEFAULT_VLM_CHAT_TEMPLATE_FOR_TEST = \"qwen2-vl\"\n \n DEFAULT_IMAGE_URL = \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n DEFAULT_VIDEO_URL = \"https://raw.githubusercontent.com/EvolvingLMMs-Lab/sglang/dev/onevision_local/assets/jobs.mp4\"\ndiff --git a/test/srt/test_bench_serving.py b/test/srt/test_bench_serving.py\nindex 032f4fae3..7153b2558 100644\n--- a/test/srt/test_bench_serving.py\n+++ b/test/srt/test_bench_serving.py\n@@ -7,6 +7,8 @@ from sglang.test.test_utils import (\n     DEFAULT_MODEL_NAME_FOR_TEST,\n     DEFAULT_MODEL_NAME_FOR_TEST_FP8,\n     DEFAULT_MOE_MODEL_NAME_FOR_TEST,\n+    DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST,\n+    DEFAULT_VLM_CHAT_TEMPLATE_FOR_TEST,\n     CustomTestCase,\n     is_in_ci,\n     run_bench_serving,\n@@ -148,6 +150,58 @@ class TestBenchServing(CustomTestCase):\n                 self.assertLess(res[\"median_ttft_ms\"], 86)\n             self.assertLess(res[\"median_itl_ms\"], 10)\n \n+    def test_vlm_offline_throughput(self):\n+        res = run_bench_serving(\n+            model=DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST,\n+            num_prompts=200,\n+            request_rate=float(\"inf\"),\n+            other_server_args=[\n+                \"--chat-template\",\n+                DEFAULT_VLM_CHAT_TEMPLATE_FOR_TEST,\n+                \"--mem-fraction-static\",\n+                \"0.7\",\n+            ],\n+            dataset_name=\"mmmu\",\n+        )\n+\n+        if is_in_ci():\n+            write_github_step_summary(\n+                f\"### test_vlm_offline_throughput\\n\"\n+                f'Output throughput: {res[\"output_throughput\"]:.2f} token/s\\n'\n+            )\n+            if os.getenv(\"SGLANG_AMD_CI\") == \"1\":\n+                self.assertGreater(res[\"output_throughput\"], 2000)\n+                # TODO: not set yet, need AMD machine\n+            else:\n+                self.assertGreater(res[\"output_throughput\"], 2500)\n+\n+    def test_vlm_online_latency(self):\n+        res = run_bench_serving(\n+            model=DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST,\n+            num_prompts=50,\n+            request_rate=1,\n+            other_server_args=[\n+                \"--chat-template\",\n+                DEFAULT_VLM_CHAT_TEMPLATE_FOR_TEST,\n+                \"--mem-fraction-static\",\n+                \"0.7\",\n+            ],\n+            dataset_name=\"mmmu\",\n+        )\n+\n+        if is_in_ci():\n+            write_github_step_summary(\n+                f\"### test_vlm_online_latency\\n\"\n+                f'median_e2e_latency_ms: {res[\"median_e2e_latency_ms\"]:.2f} ms\\n'\n+            )\n+            self.assertLess(res[\"median_e2e_latency_ms\"], 16000)\n+            if os.getenv(\"SGLANG_AMD_CI\") == \"1\":\n+                self.assertLess(res[\"median_ttft_ms\"], 150)\n+                # TODO: not set yet, need AMD machine\n+            else:\n+                self.assertLess(res[\"median_ttft_ms\"], 90)\n+            self.assertLess(res[\"median_itl_ms\"], 8)\n+\n     def test_online_latency_eagle(self):\n         res = run_bench_serving(\n             model=DEFAULT_EAGLE_TARGET_MODEL_FOR_TEST,\ndiff --git a/test/srt/test_skip_tokenizer_init.py b/test/srt/test_skip_tokenizer_init.py\nindex a9d27242f..f9eee27b6 100644\n--- a/test/srt/test_skip_tokenizer_init.py\n+++ b/test/srt/test_skip_tokenizer_init.py\n@@ -16,7 +16,7 @@ from sglang.srt.utils import kill_process_tree\n from sglang.test.test_utils import (\n     DEFAULT_IMAGE_URL,\n     DEFAULT_SMALL_MODEL_NAME_FOR_TEST,\n-    DEFAULT_SMALL_VLM_MODEL_NAME,\n+    DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST,\n     DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,\n     DEFAULT_URL_FOR_TEST,\n     CustomTestCase,\n@@ -195,7 +195,7 @@ class TestSkipTokenizerInitVLM(TestSkipTokenizerInit):\n         cls.image_url = DEFAULT_IMAGE_URL\n         response = requests.get(cls.image_url)\n         cls.image = Image.open(BytesIO(response.content))\n-        cls.model = DEFAULT_SMALL_VLM_MODEL_NAME\n+        cls.model = DEFAULT_SMALL_VLM_MODEL_NAME_FOR_TEST\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model, use_fast=False)\n         cls.processor = AutoProcessor.from_pretrained(cls.model, trust_remote_code=True)\n         cls.base_url = DEFAULT_URL_FOR_TEST",
  "apis": [
    "sglang.bench_serving.run_bench_serving",
    "sglang.bench_serving.async_request_sglang_generate",
    "sglang.bench_serving.sample_mmmu_requests"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/benchmark/hicache/bench_serving.py",
    "/path/to/repos/sglang/python/sglang/bench_serving.py",
    "/path/to/repos/sglang/examples/runtime/engine/offline_batch_inference_vlm.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The changes in this commit focus on performance benchmarking for a visual-language model (VLM). The commit adds new CI steps to benchmark both offline throughput and online latency, and introduces corresponding test cases (though many files are test-related, an essential non-test file \u2014 sglang/bench_serving.py \u2014 is also modified). In bench_serving.py, the code is altered in a nontrivial way to support image data in requests and to sample requests from a new \"mmmu\" dataset. These modifications tie directly into measuring the performance of top-level APIs (e.g., run_bench_serving) on CPU. Although the commit message mentions CI and the changes add tests rather than optimize a core algorithm, the entire purpose of the changes is to assess and thereby enhance overall performance in production use cases. Therefore, these changes are performance related. [ANSWER]YES[/ANSWER]",
  "llm_api_reason": "This commit adds new benchmarking steps for VLM in the CI workflow and updates the serving benchmark code. Notably, the RequestFuncInput dataclass is extended to include an \u201cimage_data\u201d field and the async_request_sglang_generate function is updated to check for and include image data in its payload. In addition, support for a new MMMU dataset is added by introducing a new function sample_mmmu_requests and updating get_dataset to recognize \u201cmmmu\u201d as a dataset name. The test suite is also enhanced with new tests for VLM offline throughput and online latency. These changes affect the top\u2010level benchmark APIs."
}