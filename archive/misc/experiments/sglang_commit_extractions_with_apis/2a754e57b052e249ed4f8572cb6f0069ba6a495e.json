{
  "commit_hash": "2a754e57b052e249ed4f8572cb6f0069ba6a495e",
  "pr_url": "https://github.com/sgl-project/sglang/pull/579",
  "pr_date": "2024-07-03",
  "timeline_text": "Copy link Member Ying1123 commented Jul 2, 2024 No description provided. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 2 yzh119 and m0g1cian reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions Ying1123 marked this pull request as draft July 2, 2024 08:47 Ying1123 force-pushed the ying-perf branch\n      6 times, most recently\n    from 081f0b0 to 4387890 Compare July 3, 2024 20:55 Ying1123 force-pushed the main branch\n    from 41ac003 to d530a1c Compare July 3, 2024 21:07 Ying1123 force-pushed the ying-perf branch\n    from 4387890 to 48ee517 Compare July 3, 2024 21:58 Ying1123 force-pushed the main branch\n    from d530a1c to c7709d3 Compare July 3, 2024 21:59 Ying1123 force-pushed the ying-perf branch\n    from 1c64c3d to 5a66b32 Compare July 3, 2024 22:59 Ying1123 marked this pull request as ready for review July 3, 2024 23:00 Ying1123 changed the title Fix performance for large prefill 2x performance improvement for large prefill & Fix workspace conflicts Jul 3, 2024 Ying1123 added 6 commits July 3, 2024 23:04 fix 1b2975a add layer sync 9811764 use ragged prefill kernel ff4efd1 debug 91cbffb fix faa43d1 clean up 64ec9bf Ying1123 force-pushed the ying-perf branch\n    from 5a66b32 to 64ec9bf Compare July 3, 2024 23:05 fix typo 3f69bea Ying1123 merged commit 2a754e5 into main Jul 3, 2024 Ying1123 deleted the ying-perf branch July 3, 2024 23:15 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 2x performance improvement for large prefill & Fix workspace conflicts ( \u2026 ebe7a75 sgl-project#579 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:56",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "2x performance improvement for large prefill & Fix workspace conflicts (#579)",
  "commit_message": "2x performance improvement for large prefill & Fix workspace conflicts (#579)",
  "commit_date": "2024-07-03T16:14:57-07:00",
  "files_changed": [
    "docs/test_process.md",
    "python/sglang/bench_latency.py",
    "python/sglang/global_config.py",
    "python/sglang/srt/layers/radix_attention.py",
    "python/sglang/srt/managers/controller/model_runner.py",
    "python/sglang/srt/server.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 113,
    "num_files": 6,
    "num_hunks": 17,
    "num_non_test_edited_lines": 113,
    "num_non_test_files": 6,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/docs/test_process.md b/docs/test_process.md\nindex e7aff5b5a..18f91c6d4 100644\n--- a/docs/test_process.md\n+++ b/docs/test_process.md\n@@ -1,8 +1,18 @@\n ## SRT Unit Tests\n \n ### Latency Alignment\n+Make sure your changes do not slow down the following benchmarks\n ```\n+# single gpu\n python -m sglang.bench_latency --model-path meta-llama/Llama-2-7b-chat-hf --mem-fraction-static 0.8 --batch 32 --input-len 512 --output-len 256\n+python -m sglang.bench_latency --model-path meta-llama/Llama-2-7b-chat-hf --mem-fraction-static 0.8 --batch 1 --input-len 512 --output-len 256\n+\n+# multiple gpu\n+python -m sglang.bench_latency --model-path meta-llama/Meta-Llama-3-70B --tp 8 --mem-fraction-static 0.6 --batch 32 --input-len 8192 --output-len 1\n+python -m sglang.bench_latency --model-path meta-llama/Meta-Llama-3-70B --tp 8 --mem-fraction-static 0.6 --batch 1 --input-len 8100 --output-len 32\n+\n+# moe model\n+python -m sglang.bench_latency --model-path databricks/dbrx-base --tp 8 --mem-fraction-static 0.6 --batch 4 --input-len 1024 --output-len 32\n ```\n \n ### High-level API\ndiff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex a163cbd30..ca09028f4 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -230,7 +230,7 @@ def latency_test(\n         prefill_latency = time.time() - tic\n         tot_latency += prefill_latency\n         throughput = bench_args.input_len * bench_args.batch_size / prefill_latency\n-        rank_print(f\"Prefill. latency: {prefill_latency:6.5f} ms, throughput: {throughput:9.2f} token/s\")\n+        rank_print(f\"Prefill. latency: {prefill_latency:6.5f} s, throughput: {throughput:9.2f} token/s\")\n \n         # Decode\n         for i in range(output_len):\n@@ -241,13 +241,13 @@ def latency_test(\n             latency = time.time() - tic\n             tot_latency += latency\n             throughput = bench_args.batch_size / latency\n-            if i < 5: rank_print(f\"Decode.  latency: {latency:6.5f} ms, throughput: {throughput:9.2f} token/s\")\n+            if i < 5: rank_print(f\"Decode.  latency: {latency:6.5f} s, throughput: {throughput:9.2f} token/s\")\n         avg_decode_latency = (tot_latency - prefill_latency) / output_len\n         avg_decode_throughput = bench_args.batch_size / avg_decode_latency\n-        rank_print(f\"Decode.  avg latency: {avg_decode_latency:6.5f} ms, avg throughput: {avg_decode_throughput:9.2f} token/s\")\n+        rank_print(f\"Decode.  avg latency: {avg_decode_latency:6.5f} s, avg throughput: {avg_decode_throughput:9.2f} token/s\")\n         \n         throughput = (bench_args.input_len + bench_args.output_len) * bench_args.batch_size / tot_latency\n-        rank_print(f\"Total. latency: {tot_latency:6.3f} ms, throughput: {throughput:9.2f} token/s\")\n+        rank_print(f\"Total. latency: {tot_latency:6.3f} s, throughput: {throughput:9.2f} token/s\")\n \n     # Warm up\n     run_once(4)\ndiff --git a/python/sglang/global_config.py b/python/sglang/global_config.py\nindex 0cc0f747f..377bde82e 100644\n--- a/python/sglang/global_config.py\n+++ b/python/sglang/global_config.py\n@@ -35,5 +35,8 @@ class GlobalConfig:\n         self.new_token_ratio_decay = 0.0001\n         self.new_token_ratio_recovery = 0.05\n \n+        # The threshold (number of tokens) to trigger layer-wise cuda sync.\n+        # This can improve the speed for large batch sizes during prefill.\n+        self.layer_sync_threshold = 8192\n \n global_config = GlobalConfig()\ndiff --git a/python/sglang/srt/layers/radix_attention.py b/python/sglang/srt/layers/radix_attention.py\nindex 66d206082..c46c11237 100644\n--- a/python/sglang/srt/layers/radix_attention.py\n+++ b/python/sglang/srt/layers/radix_attention.py\n@@ -4,6 +4,7 @@ import numpy as np\n import torch\n from torch import nn\n \n+from sglang.global_config import global_config\n from sglang.srt.layers.context_flashattention_nopad import context_attention_fwd\n from sglang.srt.layers.extend_attention import extend_attention_fwd\n from sglang.srt.layers.token_attention import token_attention_fwd\n@@ -103,12 +104,29 @@ class RadixAttention(nn.Module):\n     def prefill_forward_flashinfer(self, q, k, v, input_metadata: InputMetadata):\n         self.store_kv_cache(k, v, input_metadata)\n \n-        o = input_metadata.flashinfer_prefill_wrapper.forward(\n+        o1, s1 = input_metadata.flashinfer_prefill_wrapper_ragged.forward_return_lse(\n             q.contiguous().view(-1, self.tp_q_head_num, self.head_dim),\n-            input_metadata.token_to_kv_pool.kv_data[self.layer_id],\n+            k.contiguous().view(-1, self.tp_k_head_num, self.head_dim),\n+            v.contiguous().view(-1, self.tp_v_head_num, self.head_dim),\n             logits_soft_cap=self.logit_cap,\n         )\n \n+        if input_metadata.no_prefix:\n+            o = o1\n+        else:\n+            o2, s2 = input_metadata.flashinfer_prefill_wrapper_paged.forward_return_lse(\n+                q.contiguous().view(-1, self.tp_q_head_num, self.head_dim),\n+                input_metadata.token_to_kv_pool.kv_data[self.layer_id],\n+                causal=False,\n+                logits_soft_cap=self.logit_cap,\n+            )\n+\n+            from flashinfer.cascade import merge_state\n+            o, _ = merge_state(o1, s1, o2, s2)\n+\n+        if input_metadata.total_num_tokens >= global_config.layer_sync_threshold:\n+            torch.cuda.synchronize()\n+\n         return o.view(-1, self.tp_q_head_num * self.head_dim)\n \n     def decode_forward_flashinfer(self, q, k, v, input_metadata: InputMetadata):\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex e41514706..bded85af9 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -65,23 +65,33 @@ class InputMetadata:\n     kv_indptr: torch.Tensor = None\n     kv_indices: torch.Tensor = None\n     kv_last_page_len: torch.Tensor = None\n-    flashinfer_prefill_wrapper: \"BatchPrefillWithPagedKVCacheWrapper\" = None\n+    flashinfer_prefill_wrapper_ragged: \"BatchPrefillWithRaggedKVCacheWrapper\" = None\n+    flashinfer_prefill_wrapper_paged: \"BatchPrefillWithPagedKVCacheWrapper\" = None\n     flashinfer_decode_wrapper: \"BatchDecodeWithPagedKVCacheWrapper\" = None\n \n     def init_flashinfer_args(self, num_qo_heads, num_kv_heads, head_dim):\n+        if (\n+            self.forward_mode == ForwardMode.PREFILL\n+            or self.forward_mode == ForwardMode.EXTEND\n+        ):\n+            paged_kernel_lens = self.prefix_lens\n+            self.no_prefix = torch.all(self.prefix_lens == 0)\n+        else:\n+            paged_kernel_lens = self.seq_lens\n+\n         self.kv_indptr = torch.zeros(\n             (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n-        self.kv_indptr[1:] = torch.cumsum(self.seq_lens, dim=0)\n+        self.kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n         self.kv_last_page_len = torch.ones(\n             (self.batch_size,), dtype=torch.int32, device=\"cuda\"\n         )\n         req_pool_indices_cpu = self.req_pool_indices.cpu().numpy()\n-        seq_lens_cpu = self.seq_lens.cpu().numpy()\n+        paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()\n         self.kv_indices = torch.cat(\n             [\n                 self.req_to_token_pool.req_to_token[\n-                    req_pool_indices_cpu[i], : seq_lens_cpu[i]\n+                    req_pool_indices_cpu[i], : paged_kernel_lens_cpu[i]\n                 ]\n                 for i in range(self.batch_size)\n             ],\n@@ -92,13 +102,24 @@ class InputMetadata:\n             self.forward_mode == ForwardMode.PREFILL\n             or self.forward_mode == ForwardMode.EXTEND\n         ):\n+            # extend part\n             self.qo_indptr = torch.zeros(\n                 (self.batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n             )\n             self.qo_indptr[1:] = torch.cumsum(self.extend_seq_lens, dim=0)\n \n-            self.flashinfer_prefill_wrapper.end_forward()\n-            self.flashinfer_prefill_wrapper.begin_forward(\n+            self.flashinfer_prefill_wrapper_ragged.end_forward()\n+            self.flashinfer_prefill_wrapper_ragged.begin_forward(\n+                self.qo_indptr,\n+                self.qo_indptr.clone(),\n+                num_qo_heads,\n+                num_kv_heads,\n+                head_dim,\n+            )\n+\n+            # cached part\n+            self.flashinfer_prefill_wrapper_paged.end_forward()\n+            self.flashinfer_prefill_wrapper_paged.begin_forward(\n                 self.qo_indptr,\n                 self.kv_indptr,\n                 self.kv_indices,\n@@ -143,7 +164,8 @@ class InputMetadata:\n         out_cache_cont_end=None,\n         top_logprobs_nums=None,\n         return_logprob=False,\n-        flashinfer_prefill_wrapper=None,\n+        flashinfer_prefill_wrapper_ragged=None,\n+        flashinfer_prefill_wrapper_paged=None,\n         flashinfer_decode_wrapper=None,\n     ):\n         batch_size = len(req_pool_indices)\n@@ -194,7 +216,8 @@ class InputMetadata:\n             other_kv_index=other_kv_index,\n             return_logprob=return_logprob,\n             top_logprobs_nums=top_logprobs_nums,\n-            flashinfer_prefill_wrapper=flashinfer_prefill_wrapper,\n+            flashinfer_prefill_wrapper_ragged=flashinfer_prefill_wrapper_ragged,\n+            flashinfer_prefill_wrapper_paged=flashinfer_prefill_wrapper_paged,\n             flashinfer_decode_wrapper=flashinfer_decode_wrapper,\n         )\n \n@@ -361,6 +384,7 @@ class ModelRunner:\n     def init_flash_infer(self):\n         if not global_server_args_dict.get(\"disable_flashinfer\", False):\n             from flashinfer import (\n+                BatchPrefillWithRaggedKVCacheWrapper,\n                 BatchPrefillWithPagedKVCacheWrapper,\n                 BatchDecodeWithPagedKVCacheWrapper,\n             )\n@@ -373,17 +397,21 @@ class ModelRunner:\n             else:\n                 use_tensor_cores = False\n \n-            workspace_buffer = torch.empty(\n-                128 * 1024 * 1024, dtype=torch.int8, device=\"cuda\"\n+            workspace_buffers = torch.empty(\n+                3, 96 * 1024 * 1024, dtype=torch.uint8, device=\"cuda\"\n+            )\n+            self.flashinfer_prefill_wrapper_ragged = BatchPrefillWithRaggedKVCacheWrapper(\n+                workspace_buffers[0], \"NHD\"\n             )\n-            self.flashinfer_prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n-                workspace_buffer, \"NHD\"\n+            self.flashinfer_prefill_wrapper_paged = BatchPrefillWithPagedKVCacheWrapper(\n+                workspace_buffers[1], \"NHD\"\n             )\n             self.flashinfer_decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(\n-                workspace_buffer, \"NHD\", use_tensor_cores=use_tensor_cores\n+                workspace_buffers[2], \"NHD\", use_tensor_cores=use_tensor_cores\n             )\n         else:\n-            self.flashinfer_prefill_wrapper = self.flashinfer_decode_wrapper = None\n+            self.flashinfer_prefill_wrapper_ragged = self.flashinfer_prefill_wrapper_paged = None\n+            self.flashinfer_decode_wrapper = None\n \n     @torch.inference_mode()\n     def forward_prefill(self, batch: Batch):\n@@ -398,7 +426,8 @@ class ModelRunner:\n             out_cache_loc=batch.out_cache_loc,\n             top_logprobs_nums=batch.top_logprobs_nums,\n             return_logprob=batch.return_logprob,\n-            flashinfer_prefill_wrapper=self.flashinfer_prefill_wrapper,\n+            flashinfer_prefill_wrapper_ragged=self.flashinfer_prefill_wrapper_ragged,\n+            flashinfer_prefill_wrapper_paged=self.flashinfer_prefill_wrapper_paged,\n             flashinfer_decode_wrapper=self.flashinfer_decode_wrapper,\n         )\n         return self.model.forward(\n@@ -418,7 +447,8 @@ class ModelRunner:\n             out_cache_loc=batch.out_cache_loc,\n             top_logprobs_nums=batch.top_logprobs_nums,\n             return_logprob=batch.return_logprob,\n-            flashinfer_prefill_wrapper=self.flashinfer_prefill_wrapper,\n+            flashinfer_prefill_wrapper_ragged=self.flashinfer_prefill_wrapper_ragged,\n+            flashinfer_prefill_wrapper_paged=self.flashinfer_prefill_wrapper_paged,\n             flashinfer_decode_wrapper=self.flashinfer_decode_wrapper,\n         )\n         return self.model.forward(\n@@ -440,7 +470,8 @@ class ModelRunner:\n             out_cache_cont_end=batch.out_cache_cont_end,\n             top_logprobs_nums=batch.top_logprobs_nums,\n             return_logprob=batch.return_logprob,\n-            flashinfer_prefill_wrapper=self.flashinfer_prefill_wrapper,\n+            flashinfer_prefill_wrapper_ragged=self.flashinfer_prefill_wrapper_ragged,\n+            flashinfer_prefill_wrapper_paged=self.flashinfer_prefill_wrapper_paged,\n             flashinfer_decode_wrapper=self.flashinfer_decode_wrapper,\n         )\n         return self.model.forward(\n@@ -460,7 +491,8 @@ class ModelRunner:\n             out_cache_loc=batch.out_cache_loc,\n             top_logprobs_nums=batch.top_logprobs_nums,\n             return_logprob=batch.return_logprob,\n-            flashinfer_prefill_wrapper=self.flashinfer_prefill_wrapper,\n+            flashinfer_prefill_wrapper_ragged=self.flashinfer_prefill_wrapper_ragged,\n+            flashinfer_prefill_wrapper_paged=self.flashinfer_prefill_wrapper_paged,\n             flashinfer_decode_wrapper=self.flashinfer_decode_wrapper,\n         )\n         return self.model.forward(\ndiff --git a/python/sglang/srt/server.py b/python/sglang/srt/server.py\nindex e28530889..78bd2e0d1 100644\n--- a/python/sglang/srt/server.py\n+++ b/python/sglang/srt/server.py\n@@ -152,7 +152,7 @@ def launch_server(server_args: ServerArgs, pipe_finish_writer, model_overide_arg\n     if server_args.disable_disk_cache:\n         disable_cache()\n     if not server_args.disable_flashinfer:\n-        assert_pkg_version(\"flashinfer\", \"0.0.7\")\n+        assert_pkg_version(\"flashinfer\", \"0.0.8\")\n     if server_args.chat_template:\n         # TODO: replace this with huggingface transformers template\n         load_chat_template_for_openai_api(server_args.chat_template)",
  "apis": [
    "RadixAttention.prefill_forward_flashinfer",
    "ModelRunner.init_flash_infer",
    "ModelRunner.forward_prefill",
    "GlobalConfig.__init__",
    "bench_latency.latency_test"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/model_executor/model_runner.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies several non-test source files (latency benchmark code, global configuration, attention layer, model runner, etc.) in a non-trivial manner. The changes include altering latency reporting, introducing a new threshold in the global config that triggers a CUDA sync, and refactoring flashinfer wrappers to improve prefill performance. These modifications directly target performance aspects (improving latency performance and specifically targeting large prefill scenarios mentioned in the commit message), and they affect high-level APIs that are testable on CPU. Therefore, the commit meets the conditions for being performance optimization related.",
  "llm_api_reason": "This commit improves performance for large prefill operations and fixes workspace conflicts during flashinfer usage. In the benchmark test documentation and script (bench_latency.py), the measured latency units were modified (from milliseconds to seconds) to clearly reflect performance. In addition, changes in the global configuration add a new \u201clayer_sync_threshold\u201d to trigger layer\u2010wise CUDA synchronization for efficiency. The RadixAttention module now calls a new flashinfer API (using separate \u201cragged\u201d and \u201cpaged\u201d wrappers) and conditionally synchronizes CUDA when total tokens exceed the threshold. Furthermore, the ModelRunner (in the controller directory) has been updated in its init_flash_infer method (and related methods) to properly instantiate and use the new flashinfer wrappers. These changes affect a few key Python methods that act as high-level APIs for performance critical operations."
}