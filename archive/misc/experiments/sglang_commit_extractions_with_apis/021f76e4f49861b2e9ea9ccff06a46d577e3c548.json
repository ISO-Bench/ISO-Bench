{
  "commit_hash": "021f76e4f49861b2e9ea9ccff06a46d577e3c548",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6994",
  "pr_date": "2025-06-11",
  "timeline_text": "Copy link Collaborator lifuhuang commented Jun 9, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . This PR improves LoRA inference performance by: Eliminating unnecessary CUDA stream synchronizations Reducing redundant computations These changes help us achieve our goal of making the LoRA batch initialization process free of CUDA syncs, as outlined in #6961 . Benchmark results show that this PR, combined with my previous PR ( #6960 ), reduces TTFT (P50) by 31.4% and ITL (P50) by 34.0% . Benchmark Result Benchmark Script python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --disable-radix-cache --lora-paths lora=algoprog/fact-generation-llama-3.1-8b-instruct-lora\npython3 -m sglang.bench_serving --backend sglang  --num-prompt 480 --request-rate 8 --lora-name lora Baseline #6960 (merged) This PR (including #6960 ) ITL@P95 78.42 ms 68.24 ms (-13.0%) 52.51 (-33.0%) ITL@P50 34.36 ms 32.85 ms (-4.4%) 22.68 (-34.0%) TTFT@P50 91.37 ms 85.52 ms (-6.5%) 62.65 (-31.4%) Before (See the huge bubble caused by cudaStreamSynchronize) After (All cudaStreamSynchronize are removed) Comparison between CUDA Graph enabled vs disabled Baseline #6861 (merged) This PR CUDA Graph Enabled CUDA Graph Disabled Modifications (Generated by Copilot..) LoRA Batch Processing Enhancements: Added a new helper function transfer_adapter_info in lora_manager.py to handle asynchronous transfer of adapter metadata (weight indices, LoRA ranks, and scalings) from the host to the CUDA device. This reduces synchronization overhead and improves performance. Improved the initialization of seg_lens and seg_indptr for CUDA graphs in init_cuda_graph_batch_info . These values are now precomputed and remain constant across batches, reducing redundant computations. Memory Management Simplifications: Simplified the get_available_buffer_slot method in mem_pool.py by removing the unnecessary return of evicted LoRA UIDs. Eviction logic is now handled internally, improving code clarity. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83c\udf89 3 Swipe4057, Fridge003, and hebiao064 reacted with hooray emoji All reactions \ud83c\udf89 3 reactions lifuhuang added 3 commits June 8, 2025 00:30 Eliminate cudaStreamSynchronize in QKVParallelLinearWithLoRA. 51d9447 Eliminate all cuda stream syncs in prepare_lora_batch. d6b68de Merge remote-tracking branch 'origin/main' into lifuhuang/lora-param 747adfb lifuhuang requested review from Ying1123 and Fridge003 as code owners June 9, 2025 08:54 lifuhuang had a problem deploying to\n        prod June 9, 2025 08:54 \u2014 with GitHub Actions Error gemini-code-assist bot reviewed Jun 9, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Hello @lifuhuang , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! Summary of Changes Hello! Gemini here, providing a summary of this pull request authored by @lifuhuang . This PR focuses on optimizing the performance of LoRA inference within the SGLang framework. The primary goals are to eliminate performance bottlenecks caused by CUDA stream synchronizations and reduce redundant computations in the LoRA management logic. The description highlights significant improvements in latency metrics (ITL and TTFT), showing reductions of over 30% compared to the baseline, and includes profiling traces demonstrating the successful removal of synchronization bubbles. Highlights Performance Optimization : The core objective is to significantly improve LoRA inference speed by addressing key performance inhibitors. Eliminate Stream Synchronizations : A major focus is removing explicit cudaStreamSynchronize calls, which are identified as causing significant delays and bubbles in the execution timeline, as shown in the profiling results. Reduce Redundant Computations : Optimizes the preparation of batch information, particularly for CUDA graphs, by pre-calculating and reusing constant values. Asynchronous Data Transfer : Introduces the use of pinned memory and non-blocking copies for transferring LoRA adapter metadata (indices, ranks, scalings) from host to device, enabling better overlap of computation and data transfer. Memory Pool Simplification : Refactors the LoRA memory pool's buffer slot allocation logic for improved clarity and efficiency. Changelog python/sglang/srt/lora/lora_manager.py In init_cuda_graph_batch_info , max_len is initialized to 1 instead of 0 (L84). Pre-calculation and initialization of seg_lens and seg_indptr for CUDA graphs are moved into init_cuda_graph_batch_info as these values are constant across batches (L92-101). A new helper function transfer_adapter_info is added to handle the asynchronous transfer of weight_indices , lora_ranks , and scalings using pinned memory and non_blocking copies (L173-210). The logic in prepare_lora_batch for populating weight_indices , lora_ranks , and scalings is replaced by calls to the new transfer_adapter_info function for both CUDA graph (L220-224) and non-CUDA graph paths (L237-241). In the non-CUDA graph path, max_len is now calculated from the CPU copy of extend_seq_lens_cpu to avoid a device-to-host transfer (L249-254). python/sglang/srt/lora/mem_pool.py The get_available_buffer_slot helper function is modified to return only the buffer_id (L135, L141). The logic for popping the evicted LoRA UID from self.uid_to_buffer_id is moved inside the get_available_buffer_slot function itself (L140). The main loop calling get_available_buffer_slot is updated to only expect a single return value (L149). Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with \ud83d\udc4d and \ud83d\udc4e on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . \u21a9 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot suggested changes Jun 9, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This PR brings impressive performance improvements to LoRA inference by strategically eliminating stream synchronizations and pre-computing certain data structures. The new transfer_adapter_info function is a good step towards centralizing asynchronous data transfer logic, and the simplification in mem_pool.py enhances clarity. My main concerns revolve around data type consistency for lora_ranks and weight_indices between the CUDA graph path and the regular path, which could impact correctness if not aligned with kernel expectations. Given the complexity of asynchronous operations and CUDA graph interactions, I'd also recommend considering if additional unit tests are needed to specifically cover the new data transfer and pre-computation logic, even though the existing test suite might provide some coverage. The \"Add unit tests\" item in the PR checklist is currently unchecked. Summary of Findings Data Type Inconsistency for lora_ranks : lora_ranks is torch.int32 in cuda_graph_batch_info but handled as torch.int64 in transfer_adapter_info and the non-CUDA graph path. This could lead to issues if CUDA kernels expect a specific type. Data Type Inconsistency for weight_indices : weight_indices is torch.int32 in cuda_graph_batch_info and for the tensor created in transfer_adapter_info , but initialized as torch.int64 in the non-CUDA graph path. This could cause problems if kernels expect torch.int32 . Unit Testing for Asynchronous Logic : The PR introduces significant changes to data handling with asynchronous operations and CUDA graph interactions. While existing tests might offer some coverage, specific unit tests for this new logic would be beneficial to ensure correctness, especially since the 'Add unit tests' checklist item is unchecked. (No comment added due to review settings). Merge Readiness The pull request demonstrates significant performance gains, which is excellent. However, the identified data type inconsistencies for lora_ranks and weight_indices are high-severity concerns that should be addressed to ensure correctness and prevent potential issues with CUDA kernel execution. Once these are resolved, and perhaps after considering the addition of targeted unit tests for the new asynchronous logic, the PR will be in a much stronger position for merging. As a reviewer, I am not authorized to approve pull requests, but I recommend addressing these points before this WIP PR is finalized and merged. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/lora/lora_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/lora/lora_manager.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Apply suggestions from code review \u2026 6ae874a Co-authored-by: gemini-code-assist[bot] <176961590+gemini-code-assist[bot]@users.noreply.github.com> lifuhuang temporarily deployed to\n        prod June 9, 2025 08:59 \u2014 with GitHub Actions Inactive lifuhuang changed the title [Perf][WIP] Refactor LoRAManager to eliminate stream syncs and redundant computations [Perf] Refactor LoRAManager to eliminate stream syncs and redundant computations Jun 11, 2025 Merge branch 'main' into lifuhuang/lora-param 177527d lifuhuang mentioned this pull request Jun 11, 2025 [Perf] Speed up LoRA Batch Initialization #6961 Closed 3 tasks Fridge003 approved these changes Jun 11, 2025 View reviewed changes Copy link Collaborator Fridge003 left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \u2764\ufe0f 1 lifuhuang reacted with heart emoji All reactions \u2764\ufe0f 1 reaction Merge branch 'main' into lifuhuang/lora-param 0c3cdff hebiao064 reviewed Jun 11, 2025 View reviewed changes python/sglang/srt/lora/lora_manager.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Qiaolin-Yu approved these changes Jun 11, 2025 View reviewed changes Copy link Collaborator Qiaolin-Yu left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Great work! LGTM Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \u2764\ufe0f 1 lifuhuang reacted with heart emoji All reactions \u2764\ufe0f 1 reaction Hide details View details Fridge003 merged commit 021f76e into main Jun 11, 2025 115 of 138 checks passed Uh oh! There was an error while loading. Please reload this page . Fridge003 deleted the lifuhuang/lora-param branch June 11, 2025 23:18 jianan-gu pushed a commit\n        to jianan-gu/sglang\n      that referenced\n      this pull request Jun 12, 2025 [Perf] Refactor LoRAManager to eliminate stream syncs and redundant c\u2026 \u2026 9a1df0a \u2026omputations ( sgl-project#6994 ) lifuhuang mentioned this pull request Jun 16, 2025 [Feature] Phi-4-MM support #6544 Open 7 tasks lifuhuang added performance lora labels Jul 14, 2025 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:41",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct,trust_remote_code=True --tasks gsm8k --num_fewshot 8 --batch_size 16"
  ],
  "perf_command": "python3 -m sglang.bench_serving --backend sglang  --num-prompt 480 --request-rate 8 --lora-name lora",
  "commit_subject": "[Perf] Refactor LoRAManager to eliminate stream syncs and redundant computations  (#6994)",
  "commit_message": "[Perf] Refactor LoRAManager to eliminate stream syncs and redundant computations  (#6994)",
  "commit_date": "2025-06-11T16:18:57-07:00",
  "files_changed": [
    "python/sglang/srt/lora/lora_manager.py",
    "python/sglang/srt/lora/mem_pool.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 122,
    "num_files": 2,
    "num_hunks": 6,
    "num_non_test_edited_lines": 122,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/lora/lora_manager.py b/python/sglang/srt/lora/lora_manager.py\nindex 45050df53..9d0295808 100644\n--- a/python/sglang/srt/lora/lora_manager.py\n+++ b/python/sglang/srt/lora/lora_manager.py\n@@ -81,7 +81,7 @@ class LoRAManager:\n                 seg_indptr=torch.zeros(\n                     self.max_bs_in_cuda_graph + 1, dtype=torch.int32\n                 ),\n-                max_len=0,\n+                max_len=1,\n                 weight_indices=torch.zeros(\n                     self.max_bs_in_cuda_graph, dtype=torch.int32\n                 ),\n@@ -89,6 +89,17 @@ class LoRAManager:\n                 scalings=torch.zeros(self.max_loras_per_batch, dtype=torch.float),\n             )\n \n+            # Initialize seg_lens and seg_indptr for CUDA graph as they remain constant\n+            # across batches.\n+            self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph].fill_(1)\n+            torch.cumsum(\n+                self.cuda_graph_batch_info.seg_lens[: self.max_bs_in_cuda_graph],\n+                dim=0,\n+                out=self.cuda_graph_batch_info.seg_indptr[\n+                    1 : self.max_bs_in_cuda_graph + 1\n+                ],\n+            )\n+\n     def init_loras(self):\n         # Config of each LoRA adapter\n         self.configs: Dict[str, LoRAConfig] = {}\n@@ -159,6 +170,45 @@ class LoRAManager:\n         # set up batch info shared by all lora modules\n         bs = forward_batch.batch_size\n \n+        def transfer_adapter_info(\n+            weight_indices_out: torch.Tensor,\n+            lora_ranks_out: torch.Tensor,\n+            scalings_out: torch.Tensor,\n+        ):\n+            \"\"\"\n+            Transfer adapter metadata (weight indices, LoRA rank, scalings) from host\n+            to device (CUDA) asynchronously.\n+            \"\"\"\n+            weight_indices = [0] * len(forward_batch.lora_paths)\n+            lora_ranks = [0] * self.max_loras_per_batch\n+            scalings = [0] * self.max_loras_per_batch\n+            for i, lora_path in enumerate(forward_batch.lora_paths):\n+                weight_indices[i] = self.memory_pool.get_buffer_id(lora_path)\n+                if lora_path is not None:\n+                    lora = self.loras[lora_path]\n+                    lora_ranks[weight_indices[i]] = lora.config.hf_config[\"r\"]\n+                    scalings[weight_indices[i]] = lora.scaling\n+\n+            # Use pinned memory to avoid synchronizations during host-to-device transfer\n+            weight_indices_tensor = torch.tensor(\n+                weight_indices, dtype=torch.int32, pin_memory=True, device=\"cpu\"\n+            )\n+            lora_ranks_tensor = torch.tensor(\n+                lora_ranks, dtype=torch.int32, pin_memory=True, device=\"cpu\"\n+            )\n+            scalings_tensor = torch.tensor(\n+                scalings, dtype=torch.float, pin_memory=True, device=\"cpu\"\n+            )\n+\n+            # Copy to device tensors asynchronously\n+            weight_indices_out[:bs].copy_(weight_indices_tensor, non_blocking=True)\n+            lora_ranks_out[: self.max_loras_per_batch].copy_(\n+                lora_ranks_tensor, non_blocking=True\n+            )\n+            scalings_out[: self.max_loras_per_batch].copy_(\n+                scalings_tensor, non_blocking=True\n+            )\n+\n         if (\n             hasattr(self, \"max_bs_in_cuda_graph\")\n             and bs <= self.max_bs_in_cuda_graph\n@@ -166,51 +216,46 @@ class LoRAManager:\n         ):\n             # Do in-place updates when CUDA graph is enabled and the batch forward mode\n             # could use CUDA graph.\n-            self.cuda_graph_batch_info.bs = bs\n-            self.cuda_graph_batch_info.seg_lens[:bs].fill_(1)\n-            torch.cumsum(\n-                self.cuda_graph_batch_info.seg_lens[:bs],\n-                dim=0,\n-                out=self.cuda_graph_batch_info.seg_indptr[1 : bs + 1],\n+\n+            transfer_adapter_info(\n+                self.cuda_graph_batch_info.weight_indices,\n+                self.cuda_graph_batch_info.lora_ranks,\n+                self.cuda_graph_batch_info.scalings,\n             )\n-            self.cuda_graph_batch_info.max_len = 1\n \n-            for i, lora_path in enumerate(forward_batch.lora_paths):\n-                self.cuda_graph_batch_info.weight_indices[i] = (\n-                    self.memory_pool.get_buffer_id(lora_path)\n-                )\n-                if lora_path is not None:\n-                    lora = self.loras[lora_path]\n-                    self.cuda_graph_batch_info.lora_ranks[\n-                        self.cuda_graph_batch_info.weight_indices[i]\n-                    ] = lora.config.hf_config[\"r\"]\n-                    self.cuda_graph_batch_info.scalings[\n-                        self.cuda_graph_batch_info.weight_indices[i]\n-                    ] = lora.scaling\n+            self.cuda_graph_batch_info.bs = bs\n+            self.cuda_graph_batch_info.max_len = 1\n             batch_info = self.cuda_graph_batch_info\n         else:\n+            weight_indices = torch.empty((bs,), dtype=torch.int32, device=self.device)\n+            lora_ranks = torch.zeros(\n+                (self.max_loras_per_batch,), dtype=torch.int64, device=self.device\n+            )\n+            scalings = torch.zeros(\n+                (self.max_loras_per_batch,), dtype=torch.float, device=self.device\n+            )\n+            transfer_adapter_info(\n+                weight_indices,\n+                lora_ranks,\n+                scalings,\n+            )\n+\n             seg_lens = (\n                 forward_batch.extend_seq_lens\n                 if forward_batch.forward_mode.is_extend()\n                 else torch.ones(bs, device=self.device)\n             )\n+\n+            max_len = (\n+                # Calculate max_len from the CPU copy to avoid D2H transfer.\n+                max(forward_batch.extend_seq_lens_cpu)\n+                if forward_batch.forward_mode.is_extend()\n+                else 1\n+            )\n+\n             seg_indptr = torch.zeros((bs + 1,), dtype=torch.int32, device=self.device)\n             seg_indptr[1:] = torch.cumsum(seg_lens, dim=0)\n-            max_len = int(torch.max(seg_lens))\n-            weight_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n \n-            lora_ranks = torch.zeros(\n-                (self.max_loras_per_batch,), dtype=torch.int64, device=\"cuda\"\n-            )\n-            scalings = torch.zeros(\n-                (self.max_loras_per_batch,), dtype=torch.float, device=\"cuda\"\n-            )\n-            for i, lora_path in enumerate(forward_batch.lora_paths):\n-                weight_indices[i] = self.memory_pool.get_buffer_id(lora_path)\n-                if lora_path is not None:\n-                    lora = self.loras[lora_path]\n-                    lora_ranks[weight_indices[i]] = lora.config.hf_config[\"r\"]\n-                    scalings[weight_indices[i]] = lora.scaling\n             batch_info = LoRABatchInfo(\n                 bs=bs,\n                 seg_lens=seg_lens,\ndiff --git a/python/sglang/srt/lora/mem_pool.py b/python/sglang/srt/lora/mem_pool.py\nindex 8b8d21332..7e69c4aab 100644\n--- a/python/sglang/srt/lora/mem_pool.py\n+++ b/python/sglang/srt/lora/mem_pool.py\n@@ -132,12 +132,13 @@ class LoRAMemoryPool:\n             for buffer_id in range(self.max_loras_per_batch):\n                 # Prioritize empty slots\n                 if self.buffer_id_to_uid[buffer_id] == \"\":\n-                    return buffer_id, \"\"\n+                    return buffer_id\n \n             for buffer_id in range(self.max_loras_per_batch):\n                 # Evict unneeded lora\n                 if self.buffer_id_to_uid[buffer_id] not in cur_uids:\n-                    return buffer_id, self.buffer_id_to_uid[buffer_id]\n+                    self.uid_to_buffer_id.pop(self.buffer_id_to_uid[buffer_id])\n+                    return buffer_id\n \n             raise ValueError(\n                 \"No available buffer slots found. Please ensure the number of active loras is less than max_loras_per_batch.\"\n@@ -145,9 +146,7 @@ class LoRAMemoryPool:\n \n         for uid in cur_uids:\n             if uid not in self.uid_to_buffer_id:\n-                buffer_id, evicted_lora_uid = get_available_buffer_slot()\n-                if evicted_lora_uid != \"\":\n-                    self.uid_to_buffer_id.pop(evicted_lora_uid)\n+                buffer_id = get_available_buffer_slot()\n                 self.load_lora_weight_to_buffer(\n                     uid, buffer_id, lora_adapters.get(uid, None)\n                 )",
  "apis": [
    "LoRAManager.init_cuda_graph_batch_info",
    "LoRAManager.prepare_lora_batch",
    "LoRAMemoryPool.prepare_lora_batch"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/lora/lora_manager.py",
    "/path/to/repos/sglang/python/sglang/srt/lora/mem_pool.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies non-test source code (both lora_manager.py and mem_pool.py) and introduces changes aimed at eliminating unnecessary stream synchronizations and redundant computations. Specifically, it refactors how adapter metadata is transferred asynchronously using pinned memory, reduces redundant operations by removing extra loops and computations, and optimizes internal data transfer routines, all of which can improve performance. These modifications are non-trivial and target performance improvements of core API functionalities operating on the CPU. Thus, the commit meets the criteria for being performance/optimization related.",
  "llm_api_reason": "The commit refactors parts of the LoRAManager and LoRAMemoryPool classes. In LoRAManager, the initialization of the CUDA graph batch info is changed (setting max_len to 1 instead of 0) and the code is refactored to remove redundant per-batch computations by introducing a helper inline function (transfer_adapter_info) inside prepare_lora_batch. In LoRAMemoryPool, the helper function that searches for an available buffer slot has been refactored to simplify its return signature and remove extra eviction logic. These changes affect the methods that manage CUDA graph batch initialization and the preparation of LoRA batches."
}