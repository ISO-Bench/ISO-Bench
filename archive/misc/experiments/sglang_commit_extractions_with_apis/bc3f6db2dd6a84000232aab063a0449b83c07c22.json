{
  "commit_hash": "bc3f6db2dd6a84000232aab063a0449b83c07c22",
  "pr_url": "https://github.com/sgl-project/sglang/pull/5068",
  "pr_date": "2025-04-09",
  "timeline_text": "Copy link Contributor liz-badada commented Apr 4, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation As title, make DeepEP normal buffer compatible with low_latency buffer, simply tested both intra-node and inter-node can run successfully. Support matrix Type DeepEP Auto DeepEP Normal (disable CUDA Graph) DeepEP Low Latency Intra-node \u2713 \u2713 \u2713 Inter-node \u2713 \u2713 \u2713 performance (1 node, auto mode, H20-3e) single node (H20-3e) # DeepEP MoE (auto) python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 8 --host 0.0.0.0 --port 30000 --enable-deepep-moe --deepep-mode auto \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--cuda-graph-max-bs 128 MoE Version Concurrency Input Output Num Requests Input Throughput(tok/s) Output Throughput (tok/s) Total Throughput (tok/s) DeepEP origin 127.97 1000 1000 512 581.94 581.94 1163.87 DeepEP auto 127.95 1000 1000 512 954.56 954.56 1909.13 EPMoE 127.94 1000 1000 512 862.52 862.52 1725.04 performance (2 nodes, auto mode, H20-3e) # node 0 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 16 --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 0 --enable-deepep-moe \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--cuda-graph-max-bs 128 # node 1 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 16 --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 1 --enable-deepep-moe \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--cuda-graph-max-bs 128 # bench python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 \\\n--random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 \\\n--max-concurrency 128 MoE Version Concurrency Input Output Num Reqs Input (tok/s) Output (tok/s) Total (tok/s) Mean TTFT (ms) Mean ITL (ms) Pure TP 16 127.94 1000 1000 512 1139.01 1139.01 2278.02 9086.90 103.34 DeepEP auto 127.94 1000 1000 512 1146.25 1146.25 2292.50 8164.64 103.56 EPMoE 127.94 1000 1000 512 988.29 988.29 1976.57 19975.66 109.60 Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction DeepEP fix compatibility with low latency 5d2dfe9 liz-badada changed the title DeepEP fix compatibility with low latency [Fix] DeepEP Compatibility with Low Latency Apr 4, 2025 Copy link Contributor Author liz-badada commented Apr 4, 2025 #4734 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan self-assigned this Apr 4, 2025 ch-wan added\n  the high priority label Apr 4, 2025 Copy link Collaborator ch-wan commented Apr 4, 2025 @liz-badada Excellent job! I managed to launch it under a multi-node environment. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan mentioned this pull request Apr 3, 2025 [Roadmap] EP Enhancement #4734 Closed 18 tasks ch-wan and others added 6 commits April 4, 2025 20:49 Merge branch 'pr/liz-badada/5068' into pr-5068 f5eac55 wip: merge 26df9a2 finish refactor b138f03 Merge branch 'pr-5068' into pr/liz-badada/5068 04c41de Merge branch 'main' into Fix_Compatibility_with_Low_Latency cbbe71d Merge branch 'main' into Fix_Compatibility_with_Low_Latency 4a2276f liz-badada marked this pull request as ready for review April 5, 2025 12:53 liz-badada requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock , ByronHsu and HaiShaw as code owners April 5, 2025 12:53 liz-badada and others added 9 commits April 6, 2025 22:35 Merge branch 'main' into Fix_Compatibility_with_Low_Latency 15f5b22 update 5b1309e Merge branch 'main' into Fix_Compatibility_with_Low_Latency 2b9ebf0 Merge branch 'main' into Fix_Compatibility_with_Low_Latency 7d6f844 Merge branch 'main' into Fix_Compatibility_with_Low_Latency f1f3d6a Merge branch 'main' into Fix_Compatibility_with_Low_Latency 88a0850 Merge branch 'main' into Fix_Compatibility_with_Low_Latency 8b95ef6 polish 86ccf9e polish 0c98d14 liz-badada and others added 2 commits April 9, 2025 10:19 Merge branch 'main' into Fix_Compatibility_with_Low_Latency 822cd23 minor 9c6cd18 ch-wan approved these changes Apr 9, 2025 View reviewed changes Hide details View details zhyncs merged commit bc3f6db into sgl-project : main Apr 9, 2025 3 of 19 checks passed Uh oh! There was an error while loading. Please reload this page . ch-wan mentioned this pull request Apr 9, 2025 [Bug] DeepEP Low Latency failed on 2 node (8*H20) #5186 Closed 5 tasks cnwenf pushed a commit\n        to cnwenf/sglang\n      that referenced\n      this pull request Apr 10, 2025 Merge branch 'main' into nixl \u2026 13c355c * main: (29 commits)\n  reduce moe_align_block_size_kernel small batch mode overhead ( sgl-project#5086 )\n  Fix DeepSeek error when using DeepEP mode ( sgl-project#5190 )\n  [metrics] Add in queue metrics ( sgl-project#4444 )\n  fix: log warning when disable cuda graph ( sgl-project#5209 )\n  Add H20 dtype fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5196 )\n  sgl-kernel use cutlass latest version for fp8 blockwise gemm ( sgl-project#5207 )\n  update grok test ( sgl-project#5171 )\n  model: support mllama4 ( sgl-project#5144 )\n  [ci] fix ci test fused_moe op ( sgl-project#5102 )\n  Support Llama4 fp8 inference ( sgl-project#5194 )\n  Optimize topk operation in llama4 ( sgl-project#5128 )\n  Fix ci test \"test_eval_fp8_accuracy\" failed ( sgl-project#5185 )\n  [Misc] clean up vllm in sgl-kernel test ( sgl-project#5189 )\n  Let `bench_one_batch` support `enable_dp_attention` ( sgl-project#4058 )\n  [DeepEP] fix: import buffer error ( sgl-project#5179 )\n  fix: use DeepEPDispatcher on CUDA ( sgl-project#5180 )\n  feat: add DeepGEMM build warning ( sgl-project#5176 )\n  docs: remove the use of Downward API for LWS_WORKER_INDEX ( sgl-project#5110 )\n  [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 )\n  [Bugfix] Fix index out of bounds in local attention with large sequences ( sgl-project#5173 )\n  ...\n\n# Conflicts:\n#\tpython/sglang/srt/disaggregation/mini_lb.py\n#\tpython/sglang/srt/managers/scheduler.py finger92 pushed a commit\n        to protagolabs/sglang\n      that referenced\n      this pull request Apr 10, 2025 [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 ) \u2026 d1fa6d3 Co-authored-by: ch-wan <cwan39@gatech.edu> Oneal65 mentioned this pull request Apr 11, 2025 [Bug] DeepEP Low Latency failed on 2 node (8*H20) #5186 #5285 Closed 5 tasks thyecust pushed a commit\n        to thyecust/sglang\n      that referenced\n      this pull request Apr 11, 2025 [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 ) \u2026 aa007f5 Co-authored-by: ch-wan <cwan39@gatech.edu> jianan-gu pushed a commit\n        to jianan-gu/sglang\n      that referenced\n      this pull request Apr 13, 2025 [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 ) \u2026 304878f Co-authored-by: ch-wan <cwan39@gatech.edu> DiweiSun pushed a commit\n        to DiweiSun/sglang\n      that referenced\n      this pull request Apr 16, 2025 [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 ) \u2026 3deb6bb Co-authored-by: ch-wan <cwan39@gatech.edu> jimoosciuc pushed a commit\n        to Furion-cn/sglang\n      that referenced\n      this pull request Apr 17, 2025 [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 ) \u2026 6114bcd Co-authored-by: ch-wan <cwan39@gatech.edu> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Apr 23, 2025 rebase sglang to tag v0.4.5.post1 ( sgl-project#13 ) \u2026 3ecb4e3 * Support with_stack and record_shapes in profiler ( sgl-project#4740 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* test: reduce `mem_fraction_static` for gemma3 vision test ( sgl-project#4840 )\n\n* Fix CI tests ( sgl-project#4853 )\n\n* Fix fa3 cuda graph page_size > 1 precision and page_size=1 speed ( sgl-project#4855 )\n\n* Revert \"get the python version from env ( sgl-project#4729 )\" ( sgl-project#4863 )\n\n* [Feature] add multi-rank support for Lora ( sgl-project#4492 )\n\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\n\n* Clean up `import vllm` in quantization/__init__.py ( sgl-project#4834 )\n\n* Fix wrong variable name when stopping memory profile ( sgl-project#4772 )\n\n* [Feat] support deepgemm for cmake ( sgl-project#4864 )\n\n* Make torch compile configurable for biased_grouped_topk ( sgl-project#4749 )\n\n* update sgl-kernel test ci ( sgl-project#4866 )\n\n* fix sampling issue ( sgl-project#4871 )\n\n* bump sgl-kernel 0.0.5.post4 ( sgl-project#4768 )\n\n* fix sgl-kernel cu118 build ( sgl-project#4872 )\n\n* [Feature] Support FA3 backend for MLA ( sgl-project#4831 )\n\n* upgrade sgl-kernel 0.0.5.post4 ( sgl-project#4873 )\n\n* update torch compile doc ( sgl-project#4874 )\n\n* bump v0.4.4.post3 ( sgl-project#4878 )\n\n* Fix BadRequestError wrong arguments and remove openai dependency ( sgl-project#4882 )\n\n* Improve stack trace of retry errors ( sgl-project#4845 )\n\n* Tiny fix doc error ( sgl-project#4795 )\n\n* [Docs] Update DeepGEMM at README.md ( sgl-project#4886 )\n\n* Update CODEOWNERS ( sgl-project#4889 )\n\n* Delete test_deep_gemm.py ( sgl-project#4891 )\n\n* Add deepseek style fused moe group gate selection kernel ( sgl-project#4530 )\n\n* quick fix: add default for new kernel ( sgl-project#4898 )\n\n* remove setup for sgl-kernel ( sgl-project#4899 )\n\n* [Misc] Clean m.def and add Development Tips ( sgl-project#4890 )\n\n* fix allreduce test ( sgl-project#4909 )\n\n* Support page size > 1 + eagle ( sgl-project#4908 )\n\n* Fix retract for page size > 1 ( sgl-project#4914 )\n\n* [Feature] use pytest for sgl-kernel ( sgl-project#4896 )\n\n* fix bmm fp8 ( sgl-project#4926 )\n\n* Fix the timeout for unit-test-2-gpu in pr-test.yml ( sgl-project#4927 )\n\n* Fix 2-gpu CI test and suppress some warnings ( sgl-project#4930 )\n\n* [feat] add fa3 in sgl-kernel ( sgl-project#4902 )\n\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\n\n* Fix sglang frontend's incorrect dependency on torch ( sgl-project#4931 )\n\n* [Fix] avoid stream sync and torch compile in prefill for fa3 backend ( sgl-project#4932 )\n\n* cleanup sgl-kernel ( sgl-project#4933 )\n\n* [Fix] Improve Lora tests and reduce CI runtime ( sgl-project#4925 )\n\n* Fix DeepSeek bug causing 2.2% MMLU drop when TP!=DP ( sgl-project#4883 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* [Fix] Add torch compile for torch.clamp back ( sgl-project#4936 )\n\n* Fix oom error for large page size ( sgl-project#4913 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* [feat] interface for platforms abstraction ( sgl-project#4928 )\n\n* [Fix] revert clean m.def for cudagraph ( sgl-project#4944 )\n\n* refactor: multimodal data ( sgl-project#4754 )\n\n* bump sgl-kernel v0.0.6 ( sgl-project#4950 )\n\n* [Build] Fix cuda12.8 build error in nvfp4_scaled_mm_kernels.cu ( sgl-project#4953 )\n\n* use fa3 in sgl-kernel ( sgl-project#4954 )\n\n* Revert PR 4764 & 4813 related to R1 RoPE ( sgl-project#4959 )\n\n* [Feature] Support DeepEP Low Latency ( sgl-project#4767 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* update bench_serving ( sgl-project#4958 )\n\n* Prevent memory leak of retract_decode when page_size > 1 ( sgl-project#4977 )\n\n* [VLM RLHF] Take Image input for verl vlm rollout ( sgl-project#4915 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: GeLee <leege233@gmail.com>\n\n* Large page size aligned hierarchical caching ( sgl-project#4581 )\n\n* bug fix for hicache host eviction ( sgl-project#4989 )\n\n* sgl scaled_fp8_quant support output padding ( sgl-project#4861 )\n\n* Add Eagle Speculative Decoding to FA3 Backend ( sgl-project#4951 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\n\n* Update tokenizer_manager.py ( sgl-project#5008 )\n\n* [sgl-kernel] per token group quant support COLUMN MAJOR ( sgl-project#4817 )\n\n* update cutlass tag ( sgl-project#5011 )\n\n* Feature/revise docs ci ( sgl-project#5009 )\n\n* fix: fix illegal cuda memory access at fused_moe_kernel ( sgl-project#4727 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* [Build] Support build sgl-kernel with ccache ( sgl-project#5020 )\n\n* fix deepgemm as well ( sgl-project#5030 )\n\n* try to fix ci oserror ( sgl-project#5024 )\n\n* Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5005 )\n\n* Small refactor DeepEPMode to clean up code a bit ( sgl-project#4992 )\n\n* [Fix] fix fa3 build at cu118 ( sgl-project#5036 )\n\n* Revert \"Replace enable_flashinfer_mla argument with attention_backend\" ( sgl-project#5048 )\n\n* bump sgl-kernel v0.0.7 ( sgl-project#5046 )\n\n* update eagle-3 docs ( sgl-project#4796 )\n\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\n\n* Add LlavaLlamaForCausaLM in MultiModal Processors ( sgl-project#5039 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* Update the retry count ( sgl-project#5051 )\n\n* upgrade sgl-kernel v0.0.7 ( sgl-project#5049 )\n\n* [2/3] fix dsv3 awq issue  ( sgl-project#4625 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\n\n* Feature/revise docs ci ( sgl-project#5056 )\n\n* Add H20 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5057 )\n\n* [fix] remove `cuda_device_count_stateless` ( sgl-project#5060 )\n\n* Small refactor DeepEPDispatcher into subclasses ( sgl-project#4994 )\n\n* Support async DeepEP by splitting into two stages ( sgl-project#4995 )\n\n* Cleanup unused resources after DeepEP operation ( sgl-project#4996 )\n\n* Add DeepSeek V3/R1 shared experts fusion ( sgl-project#4918 )\n\n* [deepep] fix: shared experts are not initialized when shared experts fusion is enabled ( sgl-project#5072 )\n\n* fix dummy-load deepseekv2 ( sgl-project#4535 )\n\n* support sgl-kernel on blackwell ( sgl-project#5074 )\n\n* FA3 Spec Decoding to support top k = 1 and add cuda graph support ( sgl-project#5050 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Chunan Zeng <zcnrex@gmail.com>\n\n* [Revision] Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5052 )\n\n* upgrade transformers 4.51.0 ( sgl-project#5088 )\n\n* sgl-kernel transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5079 )\n\n* bump sgl-kernel 0.0.8 ( sgl-project#5089 )\n\n* python transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5080 )\n\n* bump v0.4.4.post4 ( sgl-project#5091 )\n\n* Fix: Reduce the number of document ci attempts to avoid long ci running ( sgl-project#5097 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Add Llama4 support ( sgl-project#5092 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Fix refactor error - fp8.py ( sgl-project#5106 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* bump v0.4.5 ( sgl-project#5117 )\n\n* [ci] fix llama4 ci error ( sgl-project#5126 )\n\n* Refactor and Optimize FA3 Code ( sgl-project#5090 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\n\n* Add Llama4 user guide ( sgl-project#5133 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* [Misc] Use pytest.mark.skipif in sgl-kernel test ( sgl-project#5137 )\n\n* feat: disable grammar restrictions within reasoning sections ( sgl-project#4984 )\n\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\n\n* [modelopt] automatically inspect if model is ModelOpt quantized and set quantization method ( sgl-project#5145 )\n\n* [AMD] Fix missing per_token_group_quant_fp8 for ROCm ( sgl-project#5140 )\n\n* fix multimodal hash feature ( sgl-project#5083 )\n\n* Fix run time error in ROCm platform ( sgl-project#5147 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\n\n* [FA3 Feature] Support multi modal Llama-3.2-11B-Vision-Instruct ( sgl-project#5103 )\n\n* Add unit test on page_size > 1 and mla and  integration test for Flash Attention 3 ( sgl-project#4760 )\n\n* Use public model for FA3 speculative decode testing ( sgl-project#5152 )\n\n* Add dummy grok test to amd CI. ( sgl-project#5115 )\n\n* fix empty_cache error in pt_weights_iterator ( sgl-project#5151 )\n\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\n\n* Fix torch compile errors ( sgl-project#5158 )\n\n* Fix loading KV quantization scale; Enable modelopt kv cache ( sgl-project#4686 )\n\nCo-authored-by: qingquansong <ustcsqq@gmail.com>\n\n* [PD] Fix unclosed prefill connection warning of mini_lb ( sgl-project#5155 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Add optimized native kernels in sgl-kernel ( sgl-project#5150 )\n\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\n\n* [PD] Simplify mini LB ( sgl-project#4911 )\n\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\n\n* Small improvement of native api docs ( sgl-project#5139 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* [feat&refactor] Enhance multimodal input support with refactor io_struct ( sgl-project#4938 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Support 2x8xH100 for Llama 4 ( sgl-project#5159 )\n\n* FP4 weight loading and inference (2/2) ( sgl-project#3972 )\n\n* Fix multimodal hashing error ( sgl-project#5174 )\n\n* Tiny disable model that does not work ( sgl-project#5175 )\n\n* [Bugfix] Fix index out of bounds in local attention with large sequences ( sgl-project#5173 )\n\n* [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* docs: remove the use of Downward API for LWS_WORKER_INDEX ( sgl-project#5110 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* feat: add DeepGEMM build warning ( sgl-project#5176 )\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* fix: use DeepEPDispatcher on CUDA ( sgl-project#5180 )\n\n* [DeepEP] fix: import buffer error ( sgl-project#5179 )\n\n* Let `bench_one_batch` support `enable_dp_attention` ( sgl-project#4058 )\n\n* [Misc] clean up vllm in sgl-kernel test ( sgl-project#5189 )\n\n* Fix ci test \"test_eval_fp8_accuracy\" failed ( sgl-project#5185 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\n\n* Optimize topk operation in llama4 ( sgl-project#5128 )\n\n* Support Llama4 fp8 inference ( sgl-project#5194 )\n\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* [ci] fix ci test fused_moe op ( sgl-project#5102 )\n\n* model: support mllama4 ( sgl-project#5144 )\n\n* update grok test ( sgl-project#5171 )\n\n* sgl-kernel use cutlass latest version for fp8 blockwise gemm ( sgl-project#5207 )\n\n* Add H20 dtype fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5196 )\n\n* fix: log warning when disable cuda graph ( sgl-project#5209 )\n\n* [metrics] Add in queue metrics ( sgl-project#4444 )\n\n* Fix DeepSeek error when using DeepEP mode ( sgl-project#5190 )\n\n* reduce moe_align_block_size_kernel small batch mode overhead ( sgl-project#5086 )\n\n* [PD] Support KV transfer with mooncake ( sgl-project#4880 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\n\n* [PD] Add get_contiguous_buf_infos interface for MLATokenToKVPool ( sgl-project#5204 )\n\n* Update deps for mllama4 ( sgl-project#5215 )\n\n* Fix deepseek-v3 with torch.compile in PyTorch 2.6. ( sgl-project#5213 )\n\n* ROCm sgl-kernel: compatible to later torch ( sgl-project#5167 )\n\n* [Misc] Clean sgl-kernel test  ( sgl-project#5216 )\n\n* Update Makefile / build script to avoid installing incompatible torch dependency ( sgl-project#5245 )\n\n* Fix torch.compile cacheing ( sgl-project#5259 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* ROCm/AITER CK_MoE: update 2-stage kernels & support both Activations ( sgl-project#5228 )\n\n* Optimize attention in llama4 ( sgl-project#5127 )\n\n* Optimize GPU memory usage in FlashAttentionBackend's strided indexing ( sgl-project#5262 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* Support `--enable-llama4-multimodal` ( sgl-project#5254 )\n\n* [fix] fix mrope positions not picked up ( sgl-project#5265 )\n\n* doc: nested loop code for offline engine ( sgl-project#5244 )\n\n* fix: examples for token_in_token_out_vlm  ( sgl-project#5193 )\n\n* Fix a 404 link in send_request.ipynb ( sgl-project#5280 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* fix: enable fp4 compilation on cu128 ( sgl-project#5286 )\n\n* feat: add cu128 identifier for sgl-kernel ( sgl-project#5287 )\n\n* chore: relax the torch version restriction for sgl-kernel compilation ( sgl-project#5288 )\n\n* chore: bump sgl-kernel v0.0.8.post1 ( sgl-project#5289 )\n\n* [PD] fix: skip warmup request in disaggregation mode to prevent crash on timeout ( sgl-project#5292 )\n\n* [Docs] Supported Model Docs - Major restructuring ( sgl-project#5290 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* fix: update update_wheel_index for cu128 ( sgl-project#5300 )\n\n* [Docs] Remove the older supported docs section ( sgl-project#5301 )\n\n* remove moe_align_block_size torch.zeros in small batch/expert mode ( sgl-project#5298 )\n\n* feat: add blackwell Dockerfile ( sgl-project#5302 )\n\n* feat: add blackwell workflow ( sgl-project#5303 )\n\n* fix: use fa3 unit test on hopper only ( sgl-project#5304 )\n\n* misc: update blackwell Dockerfile ( sgl-project#5306 )\n\n* fix: remove cublas_grouped_gemm ( sgl-project#5307 )\n\n* fix: update flash attn ( sgl-project#5308 )\n\n* fix: use deepgemm only on hopper ( sgl-project#5310 )\n\n* [VLM] Adopt fast image processor by default ( sgl-project#5065 )\n\n* Adjust ci test threshold ( sgl-project#5271 )\n\n* Blackwell Cutlass MLA kernel ( sgl-project#5142 )\n\n* misc: cleanup 3rdparty ( sgl-project#5311 )\n\n* update variable naming and comments for rocm ( sgl-project#5299 )\n\n* Fix w8a8_int8 model shared experts fusion load weights error ( sgl-project#5120 )\n\n* Add flash_attn_varlen_func to sgl-kernel ( sgl-project#5315 )\n\n* Fix fa3 window size setup ( sgl-project#5316 )\n\n* chore: bump sgl-kernel v0.0.8.post2 ( sgl-project#5317 )\n\n* feat: use fa3 mla by default on hopper ( sgl-project#5210 )\n\nCo-authored-by: yundai424 <yundai424@gmail.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* Fix: docs/backend/structured_outputs.ipynb ( sgl-project#4884 )\n\n* Delete python/sglang/srt/layers/moe/fused_moe_triton/configs/E=257,N=\u2026 ( sgl-project#5321 )\n\n* refine fused_moe tuning docs ( sgl-project#5294 )\n\n* Support server based rollout in Verlengine ( sgl-project#4848 )\n\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\n\n* [Feat] Add sparse attn to sgl-kernel ( sgl-project#5327 )\n\n* fix: solve cu118 issue for cutlass mla ( sgl-project#5331 )\n\n* chore: bump sgl-kernel v0.0.8.post3 ( sgl-project#5332 )\n\n* ci: update release node ( sgl-project#5333 )\n\n* fix: determine if flashinfer is installed ( sgl-project#5336 )\n\n* feat: adapt merge_state ( sgl-project#5337 )\n\n* misc: update sagemaker Dockerfile ( sgl-project#5341 )\n\n* Fix: Ensure tensors for dist.broadcast match NCCL backend device ( sgl-project#5322 )\n\n* docs: update adoption and sponsorship list with Oracle ( sgl-project#5343 )\n\n* chore: upgrade sgl-kernel 0.0.8.post3 ( sgl-project#5342 )\n\n* Fix typo: infight -> inflight ( sgl-project#5357 )\n\n* [PD] Add transfer backend abstraction ( sgl-project#5328 )\n\n* fix MLATokenToKVPoolHost get_size_per_token bug ( sgl-project#5161 )\n\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\n\n* fix sgl-project#5322 ( sgl-project#5359 )\n\n* feat: update experiment_runner ( sgl-project#5360 )\n\n* [DeepEP] Reduce routed scaling overhead ( sgl-project#5277 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Fix DeepSeek DP Attention + torch compile ( sgl-project#5367 )\n\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Support for Qwen2.5-VL Model in bitsandbytes Format ( sgl-project#5003 )\n\n* Fix PD disaggregation bugs ( sgl-project#5326 )\n\n* [PD Bug] fix  MLA get_contiguous_buf_infos error ( sgl-project#5384 )\n\n* [perf] experimental enhance fp8 per-tensor quant ( sgl-project#5370 )\n\n* Apply deepseek cuda rope ( sgl-project#5385 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* apply fused moe gate in ds v3/r1 ( sgl-project#5371 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: update test config ( sgl-project#5392 )\n\n* [Fix] Turn off DeepGEMM by default ( sgl-project#5263 )\n\n* minor clean up of sgl-kernel/CMakeLists.txt ( sgl-project#5393 )\n\n* Add A800 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5368 )\n\n* Add H20 dtype fp8_w8a8 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5291 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* [fix/misc] remove duplicate row in deepseek v2 model ( sgl-project#5279 )\n\n* chore: upgrade DeepGEMM ( sgl-project#5395 )\n\n* fix: update pr-test-sgl-kernel ( sgl-project#5399 )\n\n* kernel: support slightly faster merge_state_v2 cuda kernel ( sgl-project#5381 )\n\n* chore: bump sgl-kernel 0.0.9 ( sgl-project#5400 )\n\n* chore: upgrade sgl-kernel 0.0.9 ( sgl-project#5401 )\n\n* Tiny fix DeepseekScalingRotaryEmbedding always use forward_native ( sgl-project#5406 )\n\n* Fix bench_serving with random-ids ( sgl-project#5214 )\n\n* [misc] fix ci flaky case ( sgl-project#5352 )\n\n* [FIX] Fix concatenation error in capture_bs when open --disable-cuda-graph-padding and without MTP ( sgl-project#5412 )\n\n* Support dynamic connection and TP 16 ( sgl-project#5351 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Fix broadcast use cuda device lead to memory capacity unbalanced ( sgl-project#5416 )\n\n* [PD] Fix dynamic port support and MLA buffer for Mooncake ( sgl-project#5415 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\n\n* Distinguish bootstrap key only in decode server ( sgl-project#5422 )\n\n* [PD] Remove unused bootstrap param and fix port table type ( sgl-project#5423 )\n\n* [minor] cleanup cmakelists.txt ( sgl-project#5420 )\n\n* bugfix: fix merge_state_v2 cuda graph ( sgl-project#5419 )\n\n* chore: bump sgl-kernel v0.0.9.post1 ( sgl-project#5430 )\n\n* fix: solve release issue ( sgl-project#5434 )\n\n* BLackwell cutlass mla: Add check for bad page size/block num combinations ( sgl-project#5431 )\n\n* feat: update model_specific_adjustment ( sgl-project#5344 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* chore: upgrade sgl-kernel 0.0.9.post1 ( sgl-project#5436 )\n\n* Fix ignore_eos parameter when loading a chat template ( sgl-project#5264 )\n\n* add attention backend supporting matrix in the doc ( sgl-project#5211 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\n\n* Support BNB quantization for llama/mllama ( sgl-project#5038 )\n\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\n\n* [Docs] Update start/install.md ( sgl-project#5398 )\n\n* [Minor] Move torch.compile patch to a better place ( sgl-project#5397 )\n\n* [Bug fix] need record start time in pd mode ( sgl-project#5425 )\n\n* Support MHA with chunked prefix cache for DeepSeek chunked prefill ( sgl-project#5113 )\n\n* chore: bump v0.4.5.post1 ( sgl-project#5445 )\n\n* Revert \"[SW-226289] rebase sglang to tag v0.4.5 ( sgl-project#12 )\"\n\nThis reverts commit 0eac714 .\n\n---------\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: chaobo jia <91889375+jcbjcbjc@users.noreply.github.com>\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\nCo-authored-by: Fr4nk1in <sh.fu@outlook.com>\nCo-authored-by: yinfan98 <1106310035@qq.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\nCo-authored-by: SEPLOS <seplos@aliyun.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: GeLee <leege233@gmail.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\nCo-authored-by: Kaiyu Yang <yangky@umich.edu>\nCo-authored-by: renxin <90580890+renxinx@users.noreply.github.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: Tommy Yang <tommyyang0524@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\nCo-authored-by: Yun Dai <yundai424@gmail.com>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: kk <43161300+kkHuang-amd@users.noreply.github.com>\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DangKai <dangkai4u@outlook.com>\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: Ma Mingfei <mingfei.ma@intel.com>\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: grimoire <streetyao@live.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: Zhaoyang Hao <77828610+Muuuchen@users.noreply.github.com>\nCo-authored-by: Teng Ma <805522925@qq.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: Richard Zou <zou3519@users.noreply.github.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: Yusong Gao <yusong.gao@icloud.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: tianlian yi <91449279+yitianlian@users.noreply.github.com>\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\nCo-authored-by: yulei <yuulei12@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Yangcheng Li <bluebluelitchi@hotmail.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\nCo-authored-by: mRSun15 <3150105645@zju.edu.cn>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com> Copy link Xiaofei-fei commented Apr 28, 2025 Hi~I tried the lastest version in main branch and this branch,but I still met the problem: [2025-04-28 07:32:02 TP3] Scheduler hit an exception: Traceback (most recent call last): File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 1999, in run_scheduler_process scheduler = Scheduler(server_args, port_args, gpu_id, tp_rank, dp_rank) File \"/sgl-workspace/sglang/python/sglang/srt/managers/scheduler.py\", line 249, in init self.tp_worker = TpWorkerClass( File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 63, in init self.worker = TpModelWorker(server_args, gpu_id, tp_rank, dp_rank, nccl_port) File \"/sgl-workspace/sglang/python/sglang/srt/managers/tp_worker.py\", line 74, in init self.model_runner = ModelRunner( File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 170, in init self.initialize(min_per_gpu_memory) File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 180, in initialize self.load_model() File \"/sgl-workspace/sglang/python/sglang/srt/model_executor/model_runner.py\", line 381, in load_model self.model = get_model( File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/ init .py\", line 22, in get_model return loader.load_model( File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 365, in load_model model = _initialize_model( File \"/sgl-workspace/sglang/python/sglang/srt/model_loader/loader.py\", line 146, in _initialize_model return model_class( File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1330, in init self.model = DeepseekV2Model( File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1277, in init [ File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1278, in DeepseekV2DecoderLayer( File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 1076, in init self.mlp = DeepseekV2MoE( File \"/sgl-workspace/sglang/python/sglang/srt/models/deepseek_v2.py\", line 259, in init self.deepep_dispatcher = DeepEPDispatcher( File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 136, in init self.buffer_low_latency = get_buffer_low_latency( File \"/sgl-workspace/sglang/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\", line 77, in get_buffer_low_latency _buffer_low_latency = Buffer( File \"/usr/local/lib/python3.10/dist-packages/deep_ep-1.0.0+007fcfc-py3.10-linux-x86_64.egg/deep_ep/buffer.py\", line 88, in init self.runtime.sync(device_ids, ipc_handles, root_unique_id) RuntimeError: Failed: Assertion error /sgl-workspace/DeepEP/csrc/kernels/runtime.cu:56 'nvshmem_team_split_strided(NVSHMEM_TEAM_WORLD, rank % NUM_MAX_NVL_PEERS, NUM_MAX_NVL_PEERS, num_ranks / NUM_MAX_NVL_PEERS, &cpu_rdma_team_config, 0, &cpu_rdma_team) == 0' I don't know why this problem occurred and how to solve it. Could you please give me some suggestions or solutions? The following is the running command I used on a two-node H20 (96G) system. node1: NCCL_IB_GID_INDEX=3 SUPPORT_CUTLASS_BLOCK_FP8=1 NCCL_DEBUG=INFO GLOO_SOCKET_IFNAME=eth0 TP_SOCKET_IFNAME=eth0 NVSHMEM_IB_ENABLE_IBGDA=0 TORCH_DISTRIBUTED_BACKEND=nccl NVSHMEM_IBGDA_NIC_HANDLER=gpu python3 -m sglang.launch_server --model-path model/DeepSeek-V3 --trust-remote-code   --tp 16  --dist-init-addr 172.31.0.4:30000 --nnodes 2 --node-rank 0   --enable-deepep-moe   --deepep-mode auto  --max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output  --cuda-graph-max-bs 128  --host 0.0.0.0 --port 12123 node2: NCCL_IB_GID_INDEX=3 SUPPORT_CUTLASS_BLOCK_FP8=1 NCCL_DEBUG=INFO GLOO_SOCKET_IFNAME=eth0 TP_SOCKET_IFNAME=eth0 NVSHMEM_IB_ENABLE_IBGDA=0 TORCH_DISTRIBUTED_BACKEND=nccl NVSHMEM_IBGDA_NIC_HANDLER=gpu python3 -m sglang.launch_server --model-path model/DeepSeek-V3 --trust-remote-code   --tp 16 --dist-init-addr 172.31.0.4:30000 --nnodes 2 --node-rank 1  --enable-deepep-moe   --deepep-mode auto --max-running-requests 128  --disable-radix-cache  --mem-fraction-static 0.9  --stream-output  --cuda-graph-max-bs 128 btw,I notice the same problem in #5186 , I have tried as comments, it is no works. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:58:53",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 --random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 --max-concurrency 128",
  "commit_subject": "[Fix] DeepEP Compatibility with Low Latency (#5068)",
  "commit_message": "[Fix] DeepEP Compatibility with Low Latency (#5068)\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>",
  "commit_date": "2025-04-08T20:31:31-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
    "python/sglang/srt/model_executor/forward_batch_info.py",
    "python/sglang/srt/models/deepseek_v2.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 268,
    "num_files": 4,
    "num_hunks": 25,
    "num_non_test_edited_lines": 268,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 100fa57fb..b10b1c98b 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -7,6 +7,7 @@ try:\n except ImportError:\n     use_deepep = False\n \n+from enum import IntEnum, auto\n from typing import Optional, Tuple\n \n import torch\n@@ -19,70 +20,95 @@ from sglang.srt.layers.moe.ep_moe.kernels import (\n )\n from sglang.srt.model_executor.forward_batch_info import ForwardMode\n \n-_buffer_normal = None\n-_buffer_low_latency = None\n \n+class DeepEPDispatchMode(IntEnum):\n+    NORMAL = auto()\n+    LOW_LATENCY = auto()\n \n-def _get_buffer_normal(group: dist.ProcessGroup, hidden_bytes: int):\n-    \"\"\"\n-    Copy from DeepEP example usage in model inference prefilling.\n-    https://github.com/deepseek-ai/DeepEP?tab=readme-ov-file#example-use-in-model-training-or-inference-prefilling\n-    \"\"\"\n \n-    global _buffer_normal\n+class DeepEPBuffer:\n \n-    num_nvl_bytes, num_rdma_bytes = 0, 0\n-    for config in (\n-        Buffer.get_dispatch_config(group.size()),\n-        Buffer.get_combine_config(group.size()),\n-    ):\n-        num_nvl_bytes = max(\n-            config.get_nvl_buffer_size_hint(hidden_bytes, group.size()), num_nvl_bytes\n-        )\n-        num_rdma_bytes = max(\n-            config.get_rdma_buffer_size_hint(hidden_bytes, group.size()), num_rdma_bytes\n-        )\n+    _buffer: Optional[Buffer] = None\n+    _dispatch_mode: Optional[DeepEPDispatchMode] = None\n+    _hidden_size: Optional[int] = None\n+    _num_max_dispatch_tokens_per_rank: Optional[int] = None\n+    _num_experts: Optional[int] = None\n \n-    if (\n-        _buffer_normal is None\n-        or _buffer_normal.group != group\n-        or _buffer_normal.num_nvl_bytes < num_nvl_bytes\n-        or _buffer_normal.num_rdma_bytes < num_rdma_bytes\n-    ):\n-        _buffer_normal = Buffer(group, num_nvl_bytes, num_rdma_bytes)\n-    return _buffer_normal\n-\n-\n-def _get_buffer_low_latency(\n-    group: dist.ProcessGroup,\n-    num_max_dispatch_tokens_per_rank: int,\n-    hidden: int,\n-    num_experts: int,\n-):\n-    \"\"\"\n-    Copy from DeepEP example usage in model inference decoding.\n-    https://github.com/deepseek-ai/DeepEP?tab=readme-ov-file#example-use-in-inference-decoding\n-    \"\"\"\n-\n-    global _buffer_low_latency\n-    num_rdma_bytes = Buffer.get_low_latency_rdma_size_hint(\n-        num_max_dispatch_tokens_per_rank, hidden, group.size(), num_experts\n-    )\n-\n-    if (\n-        _buffer_low_latency is None\n-        or _buffer_low_latency.group != group\n-        or not _buffer_low_latency.low_latency_mode\n-        or _buffer_low_latency.num_rdma_bytes < num_rdma_bytes\n+    @classmethod\n+    def get_deepep_buffer(\n+        cls,\n+        group: dist.ProcessGroup,\n+        hidden_size: int,\n+        param_bytes: int,\n+        deepep_mode: DeepEPMode,\n+        num_max_dispatch_tokens_per_rank: int = None,\n+        num_experts: int = None,\n     ):\n-        assert num_experts % group.size() == 0\n-        _buffer_low_latency = Buffer(\n+        if cls._buffer is not None:\n+            return cls._buffer\n+\n+        cls._hidden_size = hidden_size\n+        cls._num_max_dispatch_tokens_per_rank = num_max_dispatch_tokens_per_rank\n+        cls._num_experts = num_experts\n+\n+        num_nvl_bytes, num_rdma_bytes = 0, 0\n+        if deepep_mode.enable_normal():\n+            hidden_bytes = hidden_size * param_bytes\n+            for config in (\n+                Buffer.get_dispatch_config(group.size()),\n+                Buffer.get_combine_config(group.size()),\n+            ):\n+                num_nvl_bytes = max(\n+                    config.get_nvl_buffer_size_hint(hidden_bytes, group.size()),\n+                    num_nvl_bytes,\n+                )\n+                num_rdma_bytes = max(\n+                    config.get_rdma_buffer_size_hint(hidden_bytes, group.size()),\n+                    num_rdma_bytes,\n+                )\n+        if deepep_mode.enable_low_latency():\n+            assert num_max_dispatch_tokens_per_rank is not None\n+            assert num_experts is not None and num_experts % group.size() == 0\n+            num_rdma_bytes = max(\n+                Buffer.get_low_latency_rdma_size_hint(\n+                    num_max_dispatch_tokens_per_rank,\n+                    hidden_size,\n+                    group.size(),\n+                    num_experts,\n+                ),\n+                num_rdma_bytes,\n+            )\n+\n+        cls._buffer = Buffer(\n             group,\n-            num_rdma_bytes=num_rdma_bytes,\n-            low_latency_mode=True,\n-            num_qps_per_rank=num_experts // group.size(),\n+            num_nvl_bytes,\n+            num_rdma_bytes,\n+            low_latency_mode=deepep_mode.enable_low_latency(),\n+            num_qps_per_rank=(\n+                num_experts // group.size() if deepep_mode.enable_low_latency() else 1\n+            ),\n         )\n-    return _buffer_low_latency\n+        return cls._buffer\n+\n+    @classmethod\n+    def clean_buffer(cls):\n+        if not cls._buffer.low_latency_mode:\n+            return\n+        cls._buffer.clean_low_latency_buffer(\n+            cls._num_max_dispatch_tokens_per_rank,\n+            cls._hidden_size,\n+            cls._num_experts,\n+        )\n+\n+    @classmethod\n+    def set_dispatch_mode_as_normal(cls):\n+        cls._dispatch_mode = DeepEPDispatchMode.NORMAL\n+\n+    @classmethod\n+    def set_dispatch_mode_as_low_latency(cls):\n+        if cls._dispatch_mode == DeepEPDispatchMode.NORMAL:\n+            cls.clean_buffer()\n+        cls._dispatch_mode = DeepEPDispatchMode.LOW_LATENCY\n \n \n class _DeepEPDispatcherImplBase:\n@@ -95,6 +121,7 @@ class _DeepEPDispatcherImplBase:\n         num_local_experts: int,\n         hidden_size: int,\n         params_dtype: torch.dtype,\n+        deepep_mode: DeepEPMode,\n     ):\n         if not use_deepep:\n             raise ImportError(\n@@ -109,7 +136,10 @@ class _DeepEPDispatcherImplBase:\n         self.num_local_experts = num_local_experts\n         self.hidden_size = hidden_size\n         self.params_dtype = params_dtype\n+        self.deepep_mode = deepep_mode\n+\n         self.params_bytes = 2\n+        self.num_max_dispatch_tokens_per_rank = 128\n \n         self.handle = None\n \n@@ -118,8 +148,6 @@ class _DeepEPDispatcherImplBase:\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        num_experts: int,\n-        num_max_dispatch_tokens_per_rank: int,\n     ):\n         raise NotImplementedError\n \n@@ -137,14 +165,14 @@ class _DeepEPDispatcherImplBase:\n     def combine_b(self, *args, **kwargs):\n         raise NotImplementedError\n \n+    def _get_buffer(self) -> Buffer:\n+        raise NotImplementedError\n+\n \n class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n     def __init__(self, async_finish: bool, **kwargs):\n         super().__init__(**kwargs)\n \n-        self.buffer_normal = _get_buffer_normal(\n-            self.group, self.hidden_size * self.params_bytes\n-        )\n         self.async_finish = async_finish\n         self.src2dst = None\n \n@@ -153,24 +181,18 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        num_experts: int,\n-        num_max_dispatch_tokens_per_rank: int,\n     ):\n         topk_idx = topk_idx.to(torch.int64)\n         previous_event = Buffer.capture() if self.async_finish else None\n-        return hidden_states, topk_idx, topk_weights, num_experts, previous_event\n+        return hidden_states, topk_idx, topk_weights, previous_event\n \n-    def dispatch_b(\n-        self, hidden_states, topk_idx, topk_weights, num_experts, previous_event\n-    ):\n+    def dispatch_b(self, hidden_states, topk_idx, topk_weights, previous_event):\n         (\n             hidden_states,\n             topk_idx,\n             topk_weights,\n             event,\n-        ) = self._dispatch_core(\n-            hidden_states, topk_idx, topk_weights, num_experts, previous_event\n-        )\n+        ) = self._dispatch_core(hidden_states, topk_idx, topk_weights, previous_event)\n         event.current_stream_wait() if self.async_finish else ()\n         if hidden_states.shape[0] > 0:\n             reorder_topk_ids, seg_indptr, hidden_states = self._deepep_permute(\n@@ -181,7 +203,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n                 (0,), device=hidden_states.device, dtype=torch.int64\n             )\n             seg_indptr = torch.zeros(\n-                (num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n+                (self.num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n             )\n \n         masked_m = expected_m = None\n@@ -201,18 +223,18 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         x: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        num_experts: int,\n         previous_event,\n     ):\n+        buffer = self._get_buffer()\n         (\n             num_tokens_per_rank,\n             num_tokens_per_rdma_rank,\n             num_tokens_per_expert,\n             is_token_in_rank,\n             previous_event,\n-        ) = self.buffer_normal.get_dispatch_layout(\n+        ) = buffer.get_dispatch_layout(\n             topk_idx,\n-            num_experts,\n+            self.num_experts,\n             previous_event=previous_event,\n             async_finish=self.async_finish,\n             allocate_on_comm_stream=previous_event is not None,\n@@ -221,6 +243,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         # FIXME: `handle` should be transmitted with tokens from dispatch to combine.\n         # However, doing this would incur an unknown synchronization error, but keeping\n         # `handle` as a member variable works.\n+\n         (\n             recv_x,\n             recv_topk_idx,\n@@ -228,7 +251,7 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n             _,  # num_recv_tokens_per_expert_list\n             self.handle,\n             event,\n-        ) = self.buffer_normal.dispatch(\n+        ) = buffer.dispatch(\n             x,\n             topk_idx=topk_idx,\n             topk_weights=topk_weights,\n@@ -327,7 +350,8 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         return hidden_states\n \n     def _combine_core(self, x: torch.Tensor, previous_event):\n-        combined_x, _, event = self.buffer_normal.combine(\n+        buffer = self._get_buffer()\n+        combined_x, _, event = buffer.combine(\n             x,\n             self.handle,\n             async_finish=self.async_finish,\n@@ -336,6 +360,17 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n         )\n         return combined_x, event\n \n+    def _get_buffer(self):\n+        DeepEPBuffer.set_dispatch_mode_as_normal()\n+        return DeepEPBuffer.get_deepep_buffer(\n+            self.group,\n+            self.hidden_size,\n+            self.params_bytes,\n+            self.deepep_mode,\n+            self.num_max_dispatch_tokens_per_rank,\n+            self.num_experts,\n+        )\n+\n \n class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n     def __init__(self, return_recv_hook: bool, **kwargs):\n@@ -345,14 +380,6 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n         num_max_dispatch_tokens_per_rank: the actual batch size in the decoding engine should be less than 256\n         https://github.com/deepseek-ai/DeepEP?tab=readme-ov-file#example-use-in-inference-decoding\n         \"\"\"\n-        # TODO(ch-wan): allow users to set this value\n-        self.num_max_dispatch_tokens_per_rank = 128\n-        self.buffer_low_latency = _get_buffer_low_latency(\n-            self.group,\n-            self.num_max_dispatch_tokens_per_rank,\n-            self.hidden_size,\n-            self.num_experts,\n-        )\n         self.return_recv_hook = return_recv_hook\n \n     def dispatch_a(\n@@ -360,21 +387,16 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        num_experts: int,\n-        num_max_dispatch_tokens_per_rank: int,\n     ):\n+        buffer = self._get_buffer()\n         topk_idx = topk_idx.to(torch.int64)\n         expected_m = (\n-            hidden_states.shape[0]\n-            * self.buffer_low_latency.group_size\n-            * topk_idx.shape[1]\n-            + num_experts\n-        ) // num_experts\n+            hidden_states.shape[0] * buffer.group_size * topk_idx.shape[1]\n+            + self.num_experts\n+        ) // self.num_experts\n         hidden_states, masked_m, event, hook = self._dispatch_core(\n             hidden_states,\n             topk_idx,\n-            num_max_dispatch_tokens_per_rank,\n-            num_experts,\n             use_fp8=True,\n         )\n         return (\n@@ -415,8 +437,6 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n         self,\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n-        num_max_dispatch_tokens_per_rank: int,\n-        num_experts: int,\n         use_fp8: bool = False,\n     ):\n         \"\"\"\n@@ -451,13 +471,13 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n \n             const auto num_warps = kNumWarpGroups * kNumWarpsPerGroup;\n         \"\"\"\n-\n+        buffer = self._get_buffer()\n         packed_recv_hidden, packed_recv_count, self.handle, event, hook = (\n-            self.buffer_low_latency.low_latency_dispatch(\n+            buffer.low_latency_dispatch(\n                 hidden_states,\n                 topk_idx,\n-                num_max_dispatch_tokens_per_rank,\n-                num_experts,\n+                self.num_max_dispatch_tokens_per_rank,\n+                self.num_experts,\n                 use_fp8=use_fp8,\n                 async_finish=not self.return_recv_hook,\n                 return_recv_hook=self.return_recv_hook,\n@@ -488,19 +508,29 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n     ):\n-        combined_hidden_states, event, hook = (\n-            self.buffer_low_latency.low_latency_combine(\n-                hidden_states,\n-                topk_idx,\n-                topk_weights,\n-                self.handle,\n-                async_finish=not self.return_recv_hook,\n-                return_recv_hook=self.return_recv_hook,\n-            )\n+        buffer = self._get_buffer()\n+        combined_hidden_states, event, hook = buffer.low_latency_combine(\n+            hidden_states,\n+            topk_idx,\n+            topk_weights,\n+            self.handle,\n+            async_finish=not self.return_recv_hook,\n+            return_recv_hook=self.return_recv_hook,\n         )\n         self.handle = None\n         return combined_hidden_states, event, hook\n \n+    def _get_buffer(self):\n+        DeepEPBuffer.set_dispatch_mode_as_low_latency()\n+        return DeepEPBuffer.get_deepep_buffer(\n+            self.group,\n+            self.hidden_size,\n+            self.params_bytes,\n+            self.deepep_mode,\n+            self.num_max_dispatch_tokens_per_rank,\n+            self.num_experts,\n+        )\n+\n \n class DeepEPDispatcher:\n     def __init__(\n@@ -526,18 +556,19 @@ class DeepEPDispatcher:\n             num_local_experts=num_local_experts,\n             hidden_size=hidden_size,\n             params_dtype=params_dtype,\n+            deepep_mode=deepep_mode,\n         )\n \n-        if self.deepep_mode.enable_normal():\n-            self._normal_dispatcher = _DeepEPDispatcherImplNormal(\n-                async_finish=async_finish,\n-                **common_kwargs,\n-            )\n         if self.deepep_mode.enable_low_latency():\n             self._low_latency_dispatcher = _DeepEPDispatcherImplLowLatency(\n                 return_recv_hook=return_recv_hook,\n                 **common_kwargs,\n             )\n+        if self.deepep_mode.enable_normal():\n+            self._normal_dispatcher = _DeepEPDispatcherImplNormal(\n+                async_finish=async_finish,\n+                **common_kwargs,\n+            )\n \n     def dispatch(self, *args, **kwargs) -> Tuple:\n         self.dispatch_a(*args, **kwargs)\n@@ -548,16 +579,12 @@ class DeepEPDispatcher:\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        num_experts: int,\n-        num_max_dispatch_tokens_per_rank: int = 128,\n         forward_mode: ForwardMode = None,\n     ):\n         inner_state = self._get_impl(forward_mode).dispatch_a(\n             hidden_states=hidden_states,\n             topk_idx=topk_idx,\n             topk_weights=topk_weights,\n-            num_experts=num_experts,\n-            num_max_dispatch_tokens_per_rank=num_max_dispatch_tokens_per_rank,\n         )\n         self._dispatch_intermediate_state = forward_mode, inner_state\n \n@@ -589,7 +616,7 @@ class DeepEPDispatcher:\n         del self._combine_intermediate_state\n         return self._get_impl(forward_mode).combine_b(*inner_state)\n \n-    def _get_impl(self, forward_mode: ForwardMode) -> \"_DeepEPDispatcherImplBase\":\n+    def _get_impl(self, forward_mode: ForwardMode) -> _DeepEPDispatcherImplBase:\n         resolved_deepep_mode = self.deepep_mode.resolve(forward_mode)\n         if resolved_deepep_mode == DeepEPMode.normal:\n             return self._normal_dispatcher\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 96a13a999..ae3e3eb8c 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -72,7 +72,7 @@ class ForwardMode(IntEnum):\n     DUMMY_FIRST = auto()\n \n     def is_prefill(self):\n-        return self == ForwardMode.PREFILL\n+        return self.is_extend()\n \n     def is_extend(self):\n         return (\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex d973f1b88..132210e27 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -324,6 +324,7 @@ class DeepseekV2MoE(nn.Module):\n                 correction_bias=self.correction_bias,\n             )\n         if self.ep_size > 1:\n+            # TODO(ch-wan): allow users to set num_max_dispatch_tokens_per_rank value\n             (\n                 hidden_states,\n                 topk_idx,\n@@ -336,7 +337,6 @@ class DeepseekV2MoE(nn.Module):\n                 hidden_states,\n                 topk_idx,\n                 topk_weights,\n-                self.num_experts,\n                 forward_mode=forward_mode,\n             )\n         final_hidden_states = (\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex a65c90de8..28539dcee 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -1101,6 +1101,7 @@ class ServerArgs:\n             \"--deepep-mode\",\n             type=str,\n             choices=[\"normal\", \"low_latency\", \"auto\"],\n+            default=\"auto\",\n             help=\"Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or `auto`. Default is `auto`, which means `low_latency` for decode batch and `normal` for prefill batch.\",\n         )",
  "apis": [
    "DeepEPBuffer.get_deepep_buffer",
    "DeepEPBuffer.clean_buffer",
    "DeepEPBuffer.set_dispatch_mode_as_normal",
    "DeepEPBuffer.set_dispatch_mode_as_low_latency",
    "ForwardMode.is_prefill"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/models/deepseek_v2.py",
    "/path/to/repos/sglang/python/sglang/srt/server_args.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit makes substantive changes to the token dispatching logic in DeepEP by refactoring the buffer management code and introducing a new class (DeepEPBuffer) that supports both normal and low\u2010latency modes. Its modifications affect how buffer sizes are computed (e.g., num_nvl_bytes and num_rdma_bytes) and how the data is dispatched and combined, which are core performance-critical paths. Although the commit message begins with \u201c[Fix]\u201d, the changes are not mere bug fixes or refactoring for clarity; instead, they directly alter the runtime behavior to support a low-latency (i.e., improved performance) mode. The changes impact high-level APIs and internal dispatch implementations impacting CPU performance, and they meet the criteria for performance/optimization related changes.",
  "llm_api_reason": "This commit refactors the DeepEP token dispatcher to better support low latency mode. A new enum (DeepEPDispatchMode) is introduced and the DeepEPBuffer class has been implemented to encapsulate buffer allocation and cleanup logic based on DeepEP mode (normal versus low latency). In addition, methods to set the dispatch mode (as normal or low latency) and to clean the low latency buffer have been added. The commit also changes the behavior of ForwardMode.is_prefill (now returning is_extend()) and adjusts the dispatch call in the DeepseekV2MoE model so that num_experts is no longer passed explicitly. These modifications ensure that the deep EP MoE dispatching can correctly switch between standard and low latency behavior."
}