{
  "commit_hash": "6b7038babd562de099b583957ff19b78c4689a37",
  "pr_url": "https://github.com/sgl-project/sglang/pull/4695",
  "pr_date": "2025-03-25",
  "timeline_text": "Copy link Collaborator fzyzcjy commented Mar 23, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation For example, suppose we want to run DeepSeek-V3 on 8xH200. Before this change, it takes ~120s to warmup all 8 GPUs, because each GPU warmup are done serially, which is converted into parallel operations. Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions more 950b9d1 fzyzcjy requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock and ByronHsu as code owners March 23, 2025 09:59 Copy link Member zhyncs commented Mar 23, 2025 it takes ~120s to warmup all 8 GPUs How long will it take after this change? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author fzyzcjy commented Mar 24, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @zhyncs Briefly checked the logs, looks like 138s -> 54s. In addition, I am wondering where larger-scale deployment, say 2x8xH100, will be affected by this more, because the original code will need to warmup 16 cards one by one (not done any experiments though). Before [2025-03-23 08:39:08] INFO:     Started server process [874108]\n[2025-03-23 08:41:26] The server is fired up and ready to roll! After [2025-03-23 09:08:22] INFO:     Started server process [893444]\n[2025-03-23 09:09:16] The server is fired up and ready to roll! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details zhyncs merged commit 6b7038b into sgl-project : main Mar 25, 2025 17 of 21 checks passed Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:09",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-V3 --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3",
  "commit_subject": "Speedup warmup when DP > 1 (#4695)",
  "commit_message": "Speedup warmup when DP > 1 (#4695)",
  "commit_date": "2025-03-24T21:08:05-07:00",
  "files_changed": [
    "python/sglang/srt/entrypoints/http_server.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 19,
    "num_files": 1,
    "num_hunks": 2,
    "num_non_test_edited_lines": 19,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/entrypoints/http_server.py b/python/sglang/srt/entrypoints/http_server.py\nindex 5a97072de..62b151162 100644\n--- a/python/sglang/srt/entrypoints/http_server.py\n+++ b/python/sglang/srt/entrypoints/http_server.py\n@@ -730,9 +730,9 @@ def _wait_and_warmup(\n         },\n     }\n     if server_args.skip_tokenizer_init:\n-        json_data[\"input_ids\"] = [10, 11, 12]\n+        json_data[\"input_ids\"] = [[10, 11, 12] for _ in range(server_args.dp_size)]\n     else:\n-        json_data[\"text\"] = \"The capital city of France is\"\n+        json_data[\"text\"] = [\"The capital city of France is\"] * server_args.dp_size\n \n     # Debug dumping\n     if server_args.debug_tensor_dump_input_file:\n@@ -743,14 +743,13 @@ def _wait_and_warmup(\n         json_data[\"sampling_params\"][\"max_new_tokens\"] = 0\n \n     try:\n-        for i in range(server_args.dp_size):\n-            res = requests.post(\n-                url + request_name,\n-                json=json_data,\n-                headers=headers,\n-                timeout=600,\n-            )\n-            assert res.status_code == 200, f\"{res}\"\n+        res = requests.post(\n+            url + request_name,\n+            json=json_data,\n+            headers=headers,\n+            timeout=600,\n+        )\n+        assert res.status_code == 200, f\"{res}\"\n     except Exception:\n         last_traceback = get_exception_traceback()\n         if pipe_finish_writer is not None:",
  "apis": [
    "sglang.srt.entrypoints.http_server.launch_server",
    "sglang.srt.entrypoints.http_server.generate_request"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/entrypoints/http_server.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/python/sglang/srt/warmup.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a non-test source file (http_server.py) to improve the warmup process. It removes the loop that sends multiple HTTP requests (one per data parallel worker) and instead sends a single request with aggregated data, which directly targets the performance of the warmup process when data parallel size is greater than one. The changes are non-trivial as they adjust the payload structure and request handling to speed up warmup, affecting high-level API performance on the CPU. Therefore, the commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "The commit modifies the warmup routine in the HTTP server entrypoint (_wait_and_warmup in http_server.py) so that when the server is initialized with multiple data parallel workers (dp_size > 1), the JSON payload is correctly formed as a list of inputs (for \u201cinput_ids\u201d or \u201ctext\u201d) and the POST request is sent only once instead of iterating over each worker. This change directly affects how the server\u2019s warmup request is constructed and sent, thereby indirectly affecting the generate endpoint and the server launch process."
}