{
  "commit_hash": "f08154193ceaa8cfcc672d9cc312784731ec8312",
  "pr_url": "https://github.com/sgl-project/sglang/pull/5141",
  "pr_date": "2025-04-21",
  "timeline_text": "Copy link Collaborator sundar24295s commented Apr 7, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation The current tokenizer_manager processes batch workloads by sequentially tokenizing each prompt before sending it to the DP. This creates overhead in the tokenization phase. We can improve tokenization performance by leveraging the batch tokenization capabilities of fast tokenizers to process multiple prompts simultaneously before distributing them across DPs. Modifications This PR introduces a batch tokenization option in TokenizerManager, controlled by server_args.enable_tokenizer_batch_encode. When enabled, it tokenizes all text inputs in a single pass for generation requests, while other input types\u2014such as input_ids, input_embeds, or image data\u2014continue to follow the existing sequential process. This optimization is particularly beneficial for prefill-heavy workloads with smaller batch sizes Also, added couple of benchmarking scripts: To benchmark the tokenizer with batch prompts. To benchmark sending batch requests to the /generate endpoint. Future Work The current implementation tokenizes the entire batch at once which is suitable for usecases sending prompts in smaller batch sizes. But, which may not scale well for very large batch sizes (e.g., 1000). Future improvements could include: Splitting large batches into manageable chunks, tokenize the chunk and send them to the DPs These enhancements will be explored in future iterations to support a wider range of use cases Benchmarks All benchmarks are performed on H100s. Batch Tokenization (venv) jobuser [ ~/sglang ]$ python3.10 benchmark/benchmark_batch/benchmark_tokenizer.py\nTokenizer Benchmark: Sequential vs Batch Processing\n------------------------------------------------------------\nTokenizer: /shared/public/sharing/fait360brew/training/models/meta-llama/Llama-3.2-3B\nTokens per prompt: 20000\nNumber of runs per batch size: 5\n------------------------------------------------------------\nGenerating 8 random prompts with 20000 tokens each...\n  Prompt 0: 20905 tokens\n  Prompt 1: 20867 tokens\n  Prompt 2: 20889 tokens\n  Prompt 3: 20882 tokens\n  Prompt 4: 20786 tokens\n  Prompt 5: 20891 tokens\n  Prompt 6: 20876 tokens\n  Prompt 7: 20835 tokens\n\nRunning benchmark...\n.\n.\n.\n============================================================\nSUMMARY OF RESULTS\n============================================================\nBatch Size Sequential (ms)    Batch (ms)         Speedup   \n------------------------------------------------------------\n1          33.23 ms         33.15 ms         1.00x\n2          67.28 ms         39.67 ms         1.70x\n4          159.67 ms         57.98 ms         2.75x\n8          351.50 ms         67.81 ms         5.18x Bechmark Batch prefill The following benchmark is to paint a picture on how much overhead we can save for a batched request if we perform batch tokenization. Launch Server Command (venv) jobuser [ ~/sglang ]$ python -m sglang.launch_server --model-path /models/meta-llama/Llama-3.2-3B --port 30000 --host 0.0.0.0 --disable-radix-cache --disable-cuda-graph --max-prefill-tokens 131072 --chunked-prefill-size 131072 --tp 1 --dp 8 Results using exisitng Sequential Tokenization (venv) jobuser [ ~/sglang ]$ python3.10 benchmark/benchmark_batch/benchmark_batch.py \n.\n.\n.\nGenerated 480 prompts with 32000 tokens each, grouped into 60 requests of 8 prompts.\n\nStarting benchmark: NUM_TOKENS=32000, BATCH_SIZE=8, NUM_REQUESTS=60\n\n[Request] Sending request 1/10 with 8 prompts at 1744061155472\n.\n.\n.\n\nBenchmark Summary:\n  Total requests sent:         10\n  Total prompts sent:          80\n  Successful requests:         10\n  Failed requests:             0\n  Total latency (all requests): 23658.48 ms\n  Avg per request latency:     2365.73 ms\n  Avg per prompt latency:      295.72 ms\n  Throughput:                  3.38 prompts/second Results using exisitng Batch Tokenization Launch Server Command python -m sglang.launch_server --model-path /models/meta-llama/Llama-3.2-3B --port 30000 --host 0.0.0.0 --disable-radix-cache --disable-cuda-graph --max-prefill-tokens 131072 --chunked-prefill-size 131072 --tp 1 --dp 8 --enable-tokenizer-batch-encode (venv) jobuser [ ~/sglang ]$ python3.10 benchmark/benchmark_batch/benchmark_batch.py \n.\n.\n.\nGenerated 480 prompts with 32000 tokens each, grouped into 60 requests of 8 prompts.\n\nStarting benchmark: NUM_TOKENS=32000, BATCH_SIZE=8, NUM_REQUESTS=60\n\n[Request] Sending request 1/10 with 8 prompts at 1744061155472\n.\n.\n.\nBenchmark Summary:\n  Total requests sent:         60\n  Total prompts sent:          480\n  Successful requests:         60\n  Failed requests:             0\n  Total latency (all requests): 126336.63 ms\n  Avg per request latency:     2105.17 ms\n  Avg per prompt latency:      263.15 ms\n  Throughput:                  3.80 prompts/second From the above benchmark we can see a good chunk of 30ms saved per prompt in a batch as measured from the client side. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, hebiao064, and zcnrex reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions sundar24295s added 4 commits April 4, 2025 23:56 Batch Tokenizer 04b52ca Updates 718819d pre-commit checks 533342d Merge branch 'main' into suramach/batchtokenizer f193633 sundar24295s marked this pull request as ready for review April 7, 2025 21:52 sundar24295s requested review from merrymercy , Ying1123 , hnyls2002 , xiezhq-hermann , zhyncs , ispobock and ByronHsu as code owners April 7, 2025 21:52 zhyncs assigned hebiao064 , qingquansong and yundai424 Apr 7, 2025 Copy link Collaborator hebiao064 commented Apr 7, 2025 Is CI down now @zhyncs All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator hebiao064 commented Apr 7, 2025 @sundar24295s qq about this: This optimization is particularly beneficial for prefill-heavy workloads with smaller batch sizes I feel like larger batch size will be more beneficial. Could you please explain a little bit? Thanks All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member zhyncs commented Apr 7, 2025 Is CI down now @zhyncs yeah just wait for @merrymercy \u2764\ufe0f 1 hebiao064 reacted with heart emoji All reactions \u2764\ufe0f 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs added\n  the high priority label Apr 7, 2025 Copy link Collaborator Author sundar24295s commented Apr 7, 2025 @hebiao064 @sundar24295s qq about this: This optimization is particularly beneficial for prefill-heavy workloads with smaller batch sizes I feel like larger batch size will be more beneficial. Could you please explain a little bit? Thanks The current implementation tokenizes the entire batch. If the batch size is larger (say 400 or so - 400 is a rough number, it depends on the model, token length per prompt within a batch etc...), then we wait for entire batch to be tokenized before forwarding the request to GPU. During this time, GPU is idle. The ideal implementation would be split the big batch into smaller batches for tokenizer, tokenize them and send to the DPs which I have called out in the future work. \ud83d\udc4d 2 Swipe4057 and hebiao064 reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator hebiao064 commented Apr 8, 2025 @hebiao064 @sundar24295s qq about this: This optimization is particularly beneficial for prefill-heavy workloads with smaller batch sizes I feel like larger batch size will be more beneficial. Could you please explain a little bit? Thanks The current implementation tokenizes the entire batch. If the batch size is larger (say 400 or so - 400 is a rough number, it depends on the model, token length per prompt within a batch etc...), then we wait for entire batch to be tokenized before forwarding the request to GPU. During this time, GPU is idle. The ideal implementation would be split the big batch into smaller batches for tokenizer, tokenize them and send to the DPs which I have called out in the future work. Thanks, very clear! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'main' into suramach/batchtokenizer cb83faa hebiao064 reviewed Apr 8, 2025 View reviewed changes python/sglang/srt/managers/tokenizer_manager.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . sundar24295s added 2 commits April 8, 2025 09:16 Merge branch 'main' into suramach/batchtokenizer 9db0ac2 Merge branch 'main' into suramach/batchtokenizer dc71b55 hebiao064 approved these changes Apr 8, 2025 View reviewed changes Copy link Collaborator Author sundar24295s commented Apr 8, 2025 3 tests failed, not related to the changes in the current PR, looks flaky. @zhyncs Can you take a look at the PR? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hebiao064 and others added 3 commits April 8, 2025 21:32 Merge branch 'main' into suramach/batchtokenizer 28cd01c Merge branch 'main' into suramach/batchtokenizer 97f30df Merge branch 'main' into suramach/batchtokenizer 38d6341 sundar24295s added 3 commits April 9, 2025 17:31 Merge branch 'main' into suramach/batchtokenizer 9eb7d39 Merge branch 'main' into suramach/batchtokenizer 002602e Merge branch 'main' into suramach/batchtokenizer 0e48daa zhyncs assigned xiezhq-hermann Apr 14, 2025 Merge branch 'main' into suramach/batchtokenizer 3843169 Copy link Collaborator Author sundar24295s commented Apr 14, 2025 There were 3 unrelated unit test failures. @zhyncs / @xiezhq-hermann Can you take a look at this PR? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . sundar24295s added 3 commits April 15, 2025 00:15 Merge branch 'main' into suramach/batchtokenizer 4ca2857 Merge branch 'main' into suramach/batchtokenizer 1f57c49 Merge branch 'main' into suramach/batchtokenizer 58e0f23 Hide details View details merrymercy merged commit f081541 into sgl-project : main Apr 21, 2025 22 of 23 checks passed Uh oh! There was an error while loading. Please reload this page . RunkaiTao pushed a commit\n        to Pb314314/sglang\n      that referenced\n      this pull request Apr 21, 2025 Perform Batch Tokenization. ( sgl-project#5141 ) 691686e pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request May 16, 2025 Rebase_4_6_0_post_1 to master_next ( sgl-project#31 ) \u2026 8ef8859 * fix: update pr-test-sgl-kernel ( sgl-project#5399 )\n\n* kernel: support slightly faster merge_state_v2 cuda kernel ( sgl-project#5381 )\n\n* chore: bump sgl-kernel 0.0.9 ( sgl-project#5400 )\n\n* chore: upgrade sgl-kernel 0.0.9 ( sgl-project#5401 )\n\n* Tiny fix DeepseekScalingRotaryEmbedding always use forward_native ( sgl-project#5406 )\n\n* Fix bench_serving with random-ids ( sgl-project#5214 )\n\n* [misc] fix ci flaky case ( sgl-project#5352 )\n\n* [FIX] Fix concatenation error in capture_bs when open --disable-cuda-graph-padding and without MTP ( sgl-project#5412 )\n\n* Support dynamic connection and TP 16 ( sgl-project#5351 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Fix broadcast use cuda device lead to memory capacity unbalanced ( sgl-project#5416 )\n\n* [PD] Fix dynamic port support and MLA buffer for Mooncake ( sgl-project#5415 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\n\n* Distinguish bootstrap key only in decode server ( sgl-project#5422 )\n\n* [PD] Remove unused bootstrap param and fix port table type ( sgl-project#5423 )\n\n* [minor] cleanup cmakelists.txt ( sgl-project#5420 )\n\n* bugfix: fix merge_state_v2 cuda graph ( sgl-project#5419 )\n\n* chore: bump sgl-kernel v0.0.9.post1 ( sgl-project#5430 )\n\n* fix: solve release issue ( sgl-project#5434 )\n\n* BLackwell cutlass mla: Add check for bad page size/block num combinations ( sgl-project#5431 )\n\n* feat: update model_specific_adjustment ( sgl-project#5344 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* chore: upgrade sgl-kernel 0.0.9.post1 ( sgl-project#5436 )\n\n* Fix ignore_eos parameter when loading a chat template ( sgl-project#5264 )\n\n* add attention backend supporting matrix in the doc ( sgl-project#5211 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\n\n* Support BNB quantization for llama/mllama ( sgl-project#5038 )\n\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\n\n* [Docs] Update start/install.md ( sgl-project#5398 )\n\n* [Minor] Move torch.compile patch to a better place ( sgl-project#5397 )\n\n* [Bug fix] need record start time in pd mode ( sgl-project#5425 )\n\n* Support MHA with chunked prefix cache for DeepSeek chunked prefill ( sgl-project#5113 )\n\n* chore: bump v0.4.5.post1 ( sgl-project#5445 )\n\n* Fix several minor issues in PD disaggregation ( sgl-project#5444 )\n\n* [doc] Update benchmark_and_profiling.md ( sgl-project#5449 )\n\n* Update cutlass dependency. ( sgl-project#5447 )\n\n* add multi-lora feature in README.md ( sgl-project#5463 )\n\n* Clean up imports ( sgl-project#5467 )\n\n* [verl] Modify the update_weights func to align with verl's resharding ( sgl-project#5345 )\n\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\n\n* [Model Support] unsloth/Phi-4-mini bnb model ( sgl-project#4982 )\n\nCo-authored-by: yhyang201 <yhyang201@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* Update attention_backend.md: plural form ( sgl-project#5489 )\n\n* Add test for flash_attn_varlen_func kernel ( sgl-project#5484 )\n\n* Deprecate disable-mla ( sgl-project#5481 )\n\n* Deprecate enable-flashinfer-mla and enable-flashmla ( sgl-project#5480 )\n\n* Feat/support encoder model (like bert) ( sgl-project#4887 )\n\n* Enable local attention during decode ( sgl-project#5479 )\n\n* Refactor DeepSeek decoder layer branches ( sgl-project#5205 )\n\n* Fix a link in sgl-kernel/README.md ( sgl-project#5493 )\n\n* [Bug fix] use correct func path in deepseek ( sgl-project#5496 )\n\nSigned-off-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\n\n* Doc: fix problems of the 'Execute Notebooks / run-all-notebooks' ci caused by the unstability of deepseek-ai/DeepSeek-R1-Distill-Qwen-7B ( sgl-project#5503 )\n\n* [Feat] Update sgl-kernel flashinfer to latest main version ( sgl-project#5500 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* Fix: Incorrect parameters passed to forward_batch_generation ( sgl-project#5506 ) ( sgl-project#5511 )\n\n* Fix: fix the exception 'the memory capacity is unbalanced. Some GPUs \u2026 ( sgl-project#5426 )\n\nCo-authored-by: ocss884 <ocss.lin@gmail.com>\n\n* [docs] Fix several consistency issues in sampling_params.md ( sgl-project#5373 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\n\n* Configuration qwen2_moe.py - qkv_bias now in transformers ( sgl-project#5512 )\n\n* Introduce moe_dense_tp_size to fix dense layer errors in DeepSeek V3 + 4x8xH100 ( sgl-project#4836 )\n\n* Sgl kernel fused_moe_gate support n_shared_experts ( sgl-project#5440 )\n\n* chore: bump sgl-kernel 0.0.9.post2 ( sgl-project#5518 )\n\n* use sglang_per_token_group_quant_fp8 from sgl-kernel instead of trion kernel ( sgl-project#5473 )\n\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\n\n* fix kimi vl running bug after rebase main ( sgl-project#5461 )\n\n* fix bug of VLLM_AVAILABLE not defined ( sgl-project#5497 )\n\n* Avoid computing lse in Ragged Prefill when there's no prefix. ( sgl-project#5476 )\n\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\n\n* [Model] Adding Qwen3 and Qwen3MoE ( sgl-project#4693 )\n\n* fix util import ( sgl-project#5542 )\n\n* Revert \"Avoid computing lse in Ragged Prefill when there's no prefix.\u2026 ( sgl-project#5544 )\n\n* chore: upgrade sgl-kernel 0.0.9.post2 ( sgl-project#5540 )\n\n* Fix DeepGEMM masked cannot be run on groups not being multiple or 4 ( sgl-project#5340 )\n\n* Make profiler output file names consistent ( sgl-project#5548 )\n\n* [PD] Tiny fix timeout error when generate ( sgl-project#5545 )\n\n* [PD] Fix no cache connect for recevier ( sgl-project#5534 )\n\n* feat: use flashinfer jit package ( sgl-project#5547 )\n\n* [PD] Remove the requirement of config file for mooncake backend  ( sgl-project#5460 )\n\n* restruct compressed_tensors_w8a8_fp8 ( sgl-project#5475 )\n\n* simplify the control logic for using shared experts fusion ( sgl-project#5504 )\n\n* Remove one kernel in per_tensor_quant_mla_fp8 ( sgl-project#5549 )\n\n* Fix sampler nan check when calling top_k_top_p_sampling_from_probs ( sgl-project#5546 )\n\n* [PD] Support page size > 1 ( sgl-project#5561 )\n\n* fix hicache write back ( sgl-project#5543 )\n\n* Minor update for ROCm variable style ( sgl-project#5562 )\n\n* Fix bench_one_batch producing unnatural results for expert parallel ( sgl-project#5149 )\n\n* [perf] introduce deep gemm group_gemm_masked as bmm ( sgl-project#5432 )\n\n* [PD] Fix DeepSeek cannot be run on latest master ( sgl-project#5568 )\n\n* Fix BumpAllocator error when no input_ids ( sgl-project#5564 )\n\n* enable DeepSeek V3 shared_experts_fusion in sm90 ( sgl-project#5571 )\n\n* [Fix] fix outlines and xgrammar ( sgl-project#4947 )\n\n* [Doc]Add instruction for profiling with bench_one_batch ( sgl-project#5581 )\n\n* Release v0.4.5.post2 ( sgl-project#5582 )\n\n* Fix bench_serving fail when zero warmup requests ( sgl-project#5574 )\n\n* Fix DeepEP cannot run on latest master ( sgl-project#5567 )\n\n* Fix torch memory saver not enabled in DP scenario ( sgl-project#5560 )\n\n* Super tiny fix typo ( sgl-project#5559 )\n\n* Add document for LoRA serving ( sgl-project#5521 )\n\n* Tiny improve error message ( sgl-project#5526 )\n\n* [PD] Fix server crash when using batch requests ( sgl-project#5531 )\n\n* [Feat] upgrade pytorch2.6 ( sgl-project#5417 )\n\n* Fix enable chunked prefill for Llama4 ( sgl-project#5575 )\n\n* fix: use fa3 for gemma2 ( sgl-project#5586 )\n\n* Fix ChatCompletionMessageGenericParam to allow for None content ( sgl-project#5452 )\n\n* [PD] Fix large page size + chunk prefill ( sgl-project#5588 )\n\n* Add test config yamls for Deepseek v3 ( sgl-project#5433 )\n\n* [Feature] Prefill assistant response - add continue_final_message parameter ( sgl-project#4226 )\n\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\n\n* add function call parser for DeepSeek V3 ( sgl-project#5224 )\n\n* smaller and non gated models for docs ( sgl-project#5378 )\n\n* Feat: Implement JSON Mode (response_format.type=\"json_object\") ( sgl-project#4733 )\n\nCo-authored-by: Kyle Pena <kylepena@kyles-macbook-pro.turkey-marlin.ts.net>\n\n* check marlin format before attempting conversion ( sgl-project#4675 )\n\n* compressed_tensors: port w8a16 fp8 from vllm ( sgl-project#4852 )\n\n* Fix one more issue reported by torchfix ( sgl-project#4859 )\n\n* Add sanity check for max_running_requests ( sgl-project#5016 )\n\n* Correct grafana heatmap. ( sgl-project#5019 )\n\n* Perform Batch Tokenization. ( sgl-project#5141 )\n\n* Speedup shared expert weight construction by avoid cloning ( sgl-project#5188 )\n\n* Tiny add Engine.flush_cache API ( sgl-project#5241 )\n\n* [misc] remove is_cuda_available ( sgl-project#5319 )\n\n* Fix flush cache ( sgl-project#5590 )\n\n* Add Speculative Decoding Eagle3 topk > 1 ( sgl-project#5318 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\n\n* upstream hicache fixes ( sgl-project#5570 )\n\n* Tiny add warning when cannot recognize bool env var ( sgl-project#5348 )\n\n* Modify metrics service endpoint ( sgl-project#3443 )\n\n* Update protocol.py to fix sgl-project#4589 ( sgl-project#4590 )\n\n* [Feat.] Enable grafana to show metrics ( sgl-project#4718 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* [Fix] Enhance DP Attention for IPv6 Compatibility ( sgl-project#4937 )\n\n* Support o1 model on Azure ( sgl-project#4980 )\n\nCo-authored-by: Shan Yu <shanyu1@g.ucla.edu>\n\n* Tiny remove duplicated code ( sgl-project#5021 )\n\n* Tiny update error hint ( sgl-project#5037 )\n\n* Support PD bootstrap fields on /v1/chat/completions endpoint ( sgl-project#5488 )\n\n* [PD] Fix generate endpoint of min_lb for PD ( sgl-project#5598 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [PD] Fix edge case and simplify large page size + chunked prefill ( sgl-project#5589 )\n\n* [PD] Add NIXL transfer backend  ( sgl-project#5477 )\n\n* [PD] Support decode overlap schedule ( sgl-project#5608 )\n\n* [PD] Support prefill overlap + Ensure no race condition ( sgl-project#5609 )\n\n* Enhance GPU memory settings ( sgl-project#5604 )\n\n* [feature] enable pre compile jit deep_gemm ( sgl-project#5580 )\n\n* Clean up mem settings ( sgl-project#5610 )\n\n* Support aiter RMSNorm in AMD ( sgl-project#5510 )\n\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\n\n* chore: bump v0.4.5.post3 ( sgl-project#5611 )\n\n* Remove extra copy in deepseek forward absorb ( sgl-project#5578 )\n\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\n\n* [Doc] Fix a 404 link to llama-405b ( sgl-project#5615 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* [fix] force use deepgemm in compile_deep_gemm ( sgl-project#5618 )\n\n* [fix] fix compile_deep_gemm missing kv_b_proj ( sgl-project#5620 )\n\n* fix: gemma 3 not use softcap ( sgl-project#5622 )\n\n* Fix FA3 DeepSeek prefill performance regression ( sgl-project#5624 )\n\nCo-authored-by: ispobock <ispobaoke@gmail.com>\n\n* [NFC] Remove duplicate `compressed-tensors` ( sgl-project#5640 )\n\n* Fix shared experts fusion error without quantization ( sgl-project#5632 )\n\n* [feature] Add H20 fp8_w8a8 FusedMoE config for --n-share-experts-fusion=16 ( sgl-project#5641 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* fix flashmla bug ( sgl-project#5272 )\n\n* [fix] reduce dp capture bs ( sgl-project#5634 )\n\nCo-authored-by: alcanerian <alcanerian@gmail.com>\n\n* Remove q concat in FA3 backend for DeepSeek decode ( sgl-project#5638 )\n\n* Revert \"Support aiter RMSNorm in AMD\" ( sgl-project#5646 )\n\n* fix: update bench_speculative ( sgl-project#5649 )\n\n* Turn on DeepGemm By Default and Update Doc ( sgl-project#5628 )\n\n* Fuse q_a_proj and kv_a_proj ( sgl-project#5619 )\n\n* Remove unnecessary `torch.full` in DeepSeek ( sgl-project#5601 )\n\n* [1/2] Add FP8 Blockscale MoE CUTLASS kernel for Blackwell ( sgl-project#5281 )\n\n* fix sgl-kernel unit tests ( sgl-project#5666 )\n\n* fix awq_dequantize import ( sgl-project#5669 )\n\n* Integrating PD disaggregation with DP attention and DeepEP ( sgl-project#5435 )\n\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\n\n* fix gemma3 unit test ( sgl-project#5670 )\n\n* fix torchvision::nms not exist ( sgl-project#5671 )\n\n* [PD] Add support for dp attention with mooncake ( sgl-project#5530 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* tune the threshold of gemma-2-27b-it in test_nightly_gsm8k_eval.py ( sgl-project#5677 )\n\n* [Doc] Fix two 404 links caused by sglang typo ( sgl-project#5667 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* fix: update truss bench_serving ( sgl-project#5683 )\n\n* fix: only compile ApplyTokenBitmaskInplace cu124+ ( sgl-project#5686 )\n\n* chore: bump sgl-kernel 0.1.0 ( sgl-project#5688 )\n\n* vlm: enable radix cache for qwen-vl models ( sgl-project#5349 )\n\nCo-authored-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* [BugFix] Fix combination of MTP and `--n-share-experts-fusion`with R1 ( sgl-project#5707 )\n\n* Fix weight loading bug for Deepseek v3+nextn ( sgl-project#5684 )\n\n* Add example to use sgl engine with fastapi ( sgl-project#5648 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* [Doc] Fix a link to Weilin Zhao ( sgl-project#5706 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* Add MMMU benchmark results ( sgl-project#4491 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* [Model] Support `ArcticForCausalLM` architecture (Snowflake/snowflake-arctic-instruct) ( sgl-project#5078 )\n\nCo-authored-by: vincent-4 <vincentzhongy+githubvincent4@gmail.com>\n\n* [PD] Better logs ( sgl-project#5715 )\n\n* [PD] Add kvargs table and thread pool for kvcache sender of mooncake ( sgl-project#5738 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [PD]: Support Muti Prefill in one node ( sgl-project#5704 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Fix: deepseek forward absorb ( sgl-project#5723 )\n\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Pin torch audio to 2.6.0 ( sgl-project#5750 )\n\n* Revert \"[Model] Support `ArcticForCausalLM` architecture (Snowflake/snowflake-arctic-instruct)\" ( sgl-project#5754 )\n\n* Disable flaky eagle tests ( sgl-project#5753 )\n\n* update triton 3.2.0 h200 fused moe triton config and add warning about triton fused_moe_kernel performance degradation due to different Triton versions. ( sgl-project#5740 )\n\n* [Docs] Update runtime/engine/readme.md ( sgl-project#5737 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* Reorder loop in shared expert weight loading ( sgl-project#5719 )\n\n* fix: fix one more bug from merging mm_inputs ( sgl-project#5718 )\n\nCo-authored-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\n\n* [Fix]: support deepseek-vl2-tiny model ( sgl-project#5552 )\n\nCo-authored-by: bppps <zouyu.zzx@alibaba-inc.com>\n\n* Bugfix for minicpmo vision test ( sgl-project#5760 )\n\n* [Minor] fix documentations ( sgl-project#5756 )\n\n* Add an assertion to enhance the robustness of the operator ( sgl-project#5736 )\n\n* fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512 ( sgl-project#5733 )\n\n* Use device_id in dist init to reduce NCCL communicator warmup & creation overhead ( sgl-project#5728 )\n\n* [fix] fix potential bumpy throughtput with deepgemm ( sgl-project#5722 )\n\n* Resolves the `404 Not Found` error when running `compile_deep_gemm.py` in multi-node setups ( sgl-project#5720 )\n\n* perf: update H20 fused_moe_triton kernel config to get higher throughput during prefilling ( sgl-project#5716 )\n\n* we fix the non existent access of `decrypted_config_file` ( sgl-project#5685 )\n\n* CI: rewrite test_vision_chunked_prefill to speedup ( sgl-project#5682 )\n\n* Fuse MLA set kv cache kernel ( sgl-project#5748 )\n\n* Update amd docker image to `sglang:v0.4.5.post3-rocm630`. ( sgl-project#5697 )\n\n* [feature] support for roberta embedding models ( sgl-project#5730 )\n\n* [fix] fix bench_one_batch_server ( sgl-project#5607 )\n\n* support for the DeepSeek model by enabling streaming response parsing ( sgl-project#5592 )\n\n* fix: Use `is not None` instead of `!= None` for None checks. ( sgl-project#5687 )\n\n* Add Llama 4 to FA3 test ( sgl-project#5509 )\n\n* [misc] more decode step log for batch_one_batch ( sgl-project#5565 )\n\n* Handle JSONDecodeError while processing request data ( sgl-project#5599 )\n\n* fix(srt): check if sample_indices is not None before usage. ( sgl-project#5633 )\n\n* update llguidance to 0.7.11; adds StructTag ( sgl-project#4870 )\n\n* Use sgl-kernel sgl_per_token_group_quant_int8 ( sgl-project#4971 )\n\n* Add memory_saver check ( sgl-project#4986 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* add switch to disable open api doc ( sgl-project#3744 )\n\nSigned-off-by: congcongke <zhanweidu@163.com>\n\n* Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\" ( sgl-project#5772 )\n\n* Fix eagle test case ( sgl-project#5776 )\n\n* Split local attention test from fa3 test ( sgl-project#5774 )\n\n* Revert \"Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\"\" ( sgl-project#5777 )\n\n* Simplify FA3 tests ( sgl-project#5779 )\n\n* Revert \"[fix] fix bench_one_batch_server\" ( sgl-project#5785 )\n\n* Revert \"Use device_id in dist init to reduce NCCL communicator warmup & creation overhead\" ( sgl-project#5786 )\n\n* [CI] Tune threshold ( sgl-project#5787 )\n\n* [CI] fix port conflicts ( sgl-project#5789 )\n\n* [CI] Fix ci tests ( sgl-project#5769 )\n\n* [PD]Reduce kv transfer threads ( sgl-project#5791 )\n\n* [CI] Fix test case ( sgl-project#5790 )\n\n* Add 8-GPU Test for Deepseek-V3  ( sgl-project#5691 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* Release v0.4.6 ( sgl-project#5795 )\n\n* Update nightly-test.yml ( sgl-project#5797 )\n\n* [CI] Improve github summary & enable fa3 for more models ( sgl-project#5796 )\n\n* [Docs] update grafana setup guide in production metrics ( sgl-project#5643 )\n\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\n\n* [Misc] add structure logging, write to file and log tracing for SGL Router\n\n* Improve overlap scheduling ( sgl-project#5788 )\n\n* Add Cutlass MLA attention backend ( sgl-project#5390 )\n\n* chore: upgrade sgl-kernel 0.1.0 ( sgl-project#5690 )\n\n* Dockerfile.dev pip scikit_build_core ( sgl-project#5807 )\n\n* Add a doc to fix sgl-kernel build link error in py39 with ccache ( sgl-project#5809 )\n\n* Turn on overlap scheduler for multimodal models ( sgl-project#5771 )\n\n* Tiny refactor DefaultModelLoader.Source ( sgl-project#5482 )\n\n* [Docs] Replace lists with tables for cleanup and readability in server_arguments ( sgl-project#5276 )\n\n* Revert \"Tiny refactor DefaultModelLoader.Source\" ( sgl-project#5825 )\n\n* Feat: add support for thinking mode via chat_template_kwargs.enable_t\u2026 ( sgl-project#5551 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: fix the error where the content is None when reasoning and tool \u2026 ( sgl-project#5838 )\n\n* feat: Add fused moe triton config for qwen3 moe on h100 ( sgl-project#5833 )\n\n* fused moe triton tuning script support qwen3 ( sgl-project#5842 )\n\n* feat: Add fused moe triton config for qwen3bf16 moe on h20 ( sgl-project#5839 )\n\n* [PD] support pd fake transfer for warmup ( sgl-project#5726 )\n\n* [config] qwen3moe_tune_h20 fp8 tp4 ( sgl-project#5846 )\n\n* [Doc] Recover history of server_arguments.md ( sgl-project#5851 )\n\n* feat: Add fused moe triton config for qwen3-30b-fp8 moe on h20 ( sgl-project#5850 )\n\n* [CI] test chunked prefill more ( sgl-project#5798 )\n\n* ROCm: update AITER ( sgl-project#5816 )\n\n* [Feat] QWen-1M context support[1/2]: Update block sparse attention backend utils kernel ( sgl-project#5847 )\n\nCo-authored-by: sighingnow <sighingnow@gmail.com>\n\n* [Fix] Missing bootstrap_port field ( sgl-project#5823 )\n\n* feat: update is_fa3_default_architecture ( sgl-project#5854 )\n\n* add fused moe config for qwen3moe fp8/bf16 ( sgl-project#5849 )\n\n* chore: bump v0.4.6.post1 ( sgl-project#5845 )\n\n* fix for hpu backend in model runner and server args\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* rebase formatting issue\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* [SW-228218]: Fix device mismatch in frequency penalty.\n\nEnsure tensors in BatchedFrequencyPenalizer are on the same device by\nmoving output_ids and frequency_penalties to the device of\ncumulated_frequency_penalties. This resolves a RuntimeError\ncaused by tensors on cpu and hpu:0 during logits subtraction.\n\n---------\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: congcongke <zhanweidu@163.com>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Zhaoyang Hao <77828610+Muuuchen@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: mRSun15 <3150105645@zju.edu.cn>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Ying Sheng <sqy1415@gmail.com>\nCo-authored-by: BearBiscuit <55008898+BearBiscuit05@users.noreply.github.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: eigen <52445717+yyihuang@users.noreply.github.com>\nCo-authored-by: yhyang201 <yhyang201@gmail.com>\nCo-authored-by: Didier Durand <durand.didier@gmail.com>\nCo-authored-by: woodx <124784234+woodx9@users.noreply.github.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: PGFLMG <1106310035@qq.com>\nCo-authored-by: u4lr451 <u4lr451@gmail.com>\nCo-authored-by: ocss884 <ocss.lin@gmail.com>\nCo-authored-by: Michael Feil <63565275+michaelfeil@users.noreply.github.com>\nCo-authored-by: strgrb <zhangkaihong.zkh@antgroup.com>\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\nCo-authored-by: liwenju0 <like4hub@gmail.com>\nCo-authored-by: Wenxuan Tan <wtan45@wisc.edu>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: lukec <118525388+sleepcoo@users.noreply.github.com>\nCo-authored-by: tarinkk <129432511+tarinkk@users.noreply.github.com>\nCo-authored-by: AmadeusW <41280211+Amadeus-Winarto@users.noreply.github.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Yi Zhou <zhouyi920521@gmail.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: kyle-pena-kuzco <kyle.pena@kuzco.xyz>\nCo-authored-by: Kyle Pena <kylepena@kyles-macbook-pro.turkey-marlin.ts.net>\nCo-authored-by: Enrique Shockwave <33002121+qeternity@users.noreply.github.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: mac0ne <mac0ne@users.noreply.github.com>\nCo-authored-by: Sundara Raman Ramachandran <sundar24295@gmail.com>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: moontidef <53668275+relic-yuexi@users.noreply.github.com>\nCo-authored-by: Huapeng Zhou <73010314+PopSoda2002@users.noreply.github.com>\nCo-authored-by: Lucius <souzou@foxmail.com>\nCo-authored-by: Chuyue Sun <33578456+ChuyueSun@users.noreply.github.com>\nCo-authored-by: Shan Yu <shanyu1@g.ucla.edu>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: michael-amd <Michael.Zhang@amd.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: ispobock <ispobaoke@gmail.com>\nCo-authored-by: Connector Switch <c8ef@outlook.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: alcanerian <alcanerian@gmail.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: vincent-4 <vincentzhongy+githubvincent4@gmail.com>\nCo-authored-by: IAN <50618241+hcyz33@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: ZXN <44322223+bppps@users.noreply.github.com>\nCo-authored-by: bppps <zouyu.zzx@alibaba-inc.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: vzed <207368749+vincentzed@users.noreply.github.com>\nCo-authored-by: DavidBao <121073073+DavidBao03@users.noreply.github.com>\nCo-authored-by: Frankey_8080 <32973306+Frank-Jie@users.noreply.github.com>\nCo-authored-by: yan97ao <580776+yan97ao@users.noreply.github.com>\nCo-authored-by: aoshen524 <aoshen524@gmail.com>\nCo-authored-by: Micha\u0142 Moskal <michal@moskal.me>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: zhanweidu <zhanweidu@163.com>\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\nCo-authored-by: Simo Lin <linsimo.mark@gmail.com>\nCo-authored-by: JiLi <leege233@gmail.com>\nCo-authored-by: sighingnow <sighingnow@gmail.com>\nCo-authored-by: XTY <xutianyi1999@live.com>\nCo-authored-by: vikram singh shekhawat <vshekhawat@habana.ai> This was referenced Aug 20, 2025 [Performance] Dynamic Batch Tokenizer #9382 Open [Performance] Batch Send from Tokenizer Manager. #9436 Merged Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:58:36",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3.10 benchmark/benchmark_batch/benchmark_batch.py",
  "commit_subject": "Perform Batch Tokenization. (#5141)",
  "commit_message": "Perform Batch Tokenization. (#5141)",
  "commit_date": "2025-04-20T18:10:37-07:00",
  "files_changed": [
    "benchmark/benchmark_batch/benchmark_batch.py",
    "benchmark/benchmark_batch/benchmark_tokenizer.py",
    "python/sglang/srt/managers/tokenizer_manager.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 454,
    "num_files": 4,
    "num_hunks": 7,
    "num_non_test_edited_lines": 454,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nnew file mode 100644\nindex 000000000..15ef0ab6a\n--- /dev/null\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -0,0 +1,193 @@\n+import concurrent.futures\n+import os\n+import random\n+import time\n+from concurrent.futures import ProcessPoolExecutor\n+from statistics import mean\n+\n+import requests\n+from tqdm import tqdm\n+from transformers import AutoTokenizer\n+\n+from sglang.lang.backend.runtime_endpoint import RuntimeEndpoint\n+\n+###############################################################################\n+# CONFIG\n+###############################################################################\n+ENDPOINT_URL = \"http://127.0.0.1:30000\"\n+TOKENIZER_DIR = \"/models/meta-llama/Llama-3.2-3B\"\n+\n+# Benchmark configurations\n+NUM_REQUESTS = 10  # Total number of requests (each with BATCH_SIZE prompts)\n+NUM_TOKENS = 32000  # Tokens per prompt\n+BATCH_SIZE = 8  # Number of prompts per request\n+GEN_TOKENS = 0  # Tokens to generate per prompt\n+\n+\n+###############################################################################\n+# REQUEST GENERATION (in parallel)\n+###############################################################################\n+def generate_random_prompt(index, tokenizer_dir, num_tokens):\n+    \"\"\"Generate a single random prompt with specified token count.\"\"\"\n+    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\n+    vocab_size = tokenizer.vocab_size\n+\n+    def generate_random_text(num_toks):\n+        random_token_ids = [random.randint(0, vocab_size - 1) for _ in range(num_toks)]\n+        return tokenizer.decode(random_token_ids, clean_up_tokenization_spaces=True)\n+\n+    random_text = generate_random_text(num_tokens)\n+    return f\"Prompt {index}: {random_text}\"\n+\n+\n+def prepare_all_prompts(num_requests, batch_size, num_tokens, tokenizer_dir):\n+    \"\"\"Generate prompts for all requests in parallel.\"\"\"\n+    total_prompts = num_requests * batch_size\n+    all_prompts = [None] * total_prompts\n+    max_workers = min(os.cpu_count() or 1, total_prompts)\n+\n+    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n+        futures = [\n+            executor.submit(generate_random_prompt, i, tokenizer_dir, num_tokens)\n+            for i in range(total_prompts)\n+        ]\n+        for future in tqdm(\n+            concurrent.futures.as_completed(futures),\n+            total=total_prompts,\n+            desc=\"Generating prompts\",\n+        ):\n+            index = futures.index(future)\n+            all_prompts[index] = future.result()\n+\n+    batched_prompts = [\n+        all_prompts[i * batch_size : (i + 1) * batch_size] for i in range(num_requests)\n+    ]\n+\n+    print(\n+        f\"Generated {total_prompts} prompts with {num_tokens} tokens each, grouped into {num_requests} requests of {batch_size} prompts.\\n\"\n+    )\n+    return batched_prompts\n+\n+\n+###############################################################################\n+# HTTP CALLS\n+###############################################################################\n+def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n+    \"\"\"Send a batch of prompts to the /generate endpoint synchronously.\"\"\"\n+    sampling_params = {\n+        \"max_new_tokens\": gen_tokens,\n+        \"temperature\": 0.7,\n+        \"stop\": \"\\n\",\n+    }\n+    data = {\"text\": prompts, \"sampling_params\": sampling_params}\n+\n+    start_time = time.time()\n+    try:\n+        response = requests.post(\n+            endpoint.base_url + \"/generate\", json=data, timeout=3600\n+        )\n+        if response.status_code != 200:\n+            error = response.json()\n+            raise RuntimeError(f\"Request {request_id} failed: {error}\")\n+        result = response.json()\n+        elapsed_time = (time.time() - start_time) * 1000  # Convert to ms\n+        avg_per_prompt = elapsed_time / len(prompts) if prompts else 0\n+        return request_id, elapsed_time, avg_per_prompt, True, len(prompts)\n+    except Exception as e:\n+        print(f\"[Request] Error for request {request_id}: {e}\")\n+        return request_id, 0, 0, False, len(prompts)\n+\n+\n+def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n+    \"\"\"Run the benchmark sequentially.\"\"\"\n+    results = []\n+    num_requests = len(batched_prompts)\n+\n+    # Record start time for total latency\n+    benchmark_start_time = time.time()\n+\n+    for i, batch_prompts in enumerate(batched_prompts):\n+        request_id = i + 1\n+        assert (\n+            len(batch_prompts) == batch_size\n+        ), f\"Request {request_id} should have {batch_size} prompts, got {len(batch_prompts)}\"\n+\n+        print(\n+            f\"[Request] Sending request {request_id}/{num_requests} with {len(batch_prompts)} prompts at {int(time.time()*1000)}\"\n+        )\n+        result = send_batch_request(endpoint, batch_prompts, gen_tokens, request_id)\n+        results.append(result)\n+\n+    # Calculate total latency\n+    total_latency = (time.time() - benchmark_start_time) * 1000  # Convert to ms\n+\n+    return results, total_latency\n+\n+\n+###############################################################################\n+# RESULTS\n+###############################################################################\n+def process_results(results, total_latency, num_requests):\n+    \"\"\"Process and display benchmark results.\"\"\"\n+    total_time = 0\n+    successful_requests = 0\n+    failed_requests = 0\n+    request_latencies = []\n+    per_prompt_latencies = []\n+    total_prompts = 0\n+\n+    for request_id, elapsed_time, avg_per_prompt, success, batch_size in results:\n+        if success:\n+            successful_requests += 1\n+            total_prompts += batch_size\n+            request_latencies.append(elapsed_time)\n+            per_prompt_latencies.append(avg_per_prompt)\n+            total_time += elapsed_time / 1000  # Convert to seconds\n+        else:\n+            failed_requests += 1\n+\n+    avg_request_latency = mean(request_latencies) if request_latencies else 0\n+    avg_per_prompt_latency = mean(per_prompt_latencies) if per_prompt_latencies else 0\n+    throughput = total_prompts / total_time if total_time > 0 else 0\n+\n+    print(\"\\nBenchmark Summary:\")\n+    print(f\"  Total requests sent:         {len(results)}\")\n+    print(f\"  Total prompts sent:          {total_prompts}\")\n+    print(f\"  Successful requests:         {successful_requests}\")\n+    print(f\"  Failed requests:             {failed_requests}\")\n+    print(f\"  Total latency (all requests): {total_latency:.2f} ms\")\n+    print(f\"  Avg per request latency:     {avg_request_latency:.2f} ms\")\n+    print(f\"  Avg per prompt latency:      {avg_per_prompt_latency:.2f} ms\")\n+    print(f\"  Throughput:                  {throughput:.2f} prompts/second\\n\")\n+\n+\n+###############################################################################\n+# MAIN\n+###############################################################################\n+def main():\n+    # Initialize endpoint\n+    endpoint = RuntimeEndpoint(ENDPOINT_URL)\n+\n+    # Generate prompts\n+    batched_prompts = prepare_all_prompts(\n+        NUM_REQUESTS, BATCH_SIZE, NUM_TOKENS, TOKENIZER_DIR\n+    )\n+\n+    # Flush cache before benchmark\n+    # endpoint.flush_cache()\n+\n+    # Run benchmark\n+    print(\n+        f\"Starting benchmark: NUM_TOKENS={NUM_TOKENS}, BATCH_SIZE={BATCH_SIZE}, NUM_REQUESTS={NUM_REQUESTS}\\n\"\n+    )\n+    results, total_latency = run_benchmark(\n+        endpoint, batched_prompts, BATCH_SIZE, GEN_TOKENS\n+    )\n+\n+    # Process and display results\n+    process_results(results, total_latency, NUM_REQUESTS)\n+\n+\n+if __name__ == \"__main__\":\n+    random.seed(0)\n+    main()\ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nnew file mode 100644\nindex 000000000..c00bfb84b\n--- /dev/null\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -0,0 +1,126 @@\n+import random\n+import time\n+from statistics import mean\n+\n+from transformers import AutoTokenizer\n+\n+# CONFIG\n+TOKENIZER_DIR = (\n+    \"/shared/public/sharing/fait360brew/training/models/meta-llama/Llama-3.2-3B\"\n+)\n+NUM_TOKENS = 20000  # Each prompt should contain this many tokens\n+BATCH_SIZES = [1, 2, 4, 8]  # Test different batch sizes\n+NUM_RUNS = 5  # Number of runs for each batch size to get reliable measurements\n+\n+\n+def generate_random_prompts(num_prompts, num_tokens, tokenizer):\n+    \"\"\"Generate random prompts with specified token count.\"\"\"\n+    vocab_size = tokenizer.vocab_size\n+    all_prompts = []\n+\n+    print(f\"Generating {num_prompts} random prompts with {num_tokens} tokens each...\")\n+    for i in range(num_prompts):\n+        # Generate random token IDs - this directly gives us the exact token count\n+        random_token_ids = [\n+            random.randint(0, vocab_size - 1) for _ in range(num_tokens)\n+        ]\n+        random_text = tokenizer.decode(\n+            random_token_ids, clean_up_tokenization_spaces=True\n+        )\n+\n+        prompt = f\"Prompt {i}: {random_text}\"\n+        tokens = tokenizer.encode(prompt)\n+        print(f\"  Prompt {i}: {len(tokens)} tokens\")\n+        all_prompts.append(prompt)\n+\n+    return all_prompts\n+\n+\n+def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n+    \"\"\"Compare sequential vs batch tokenization for a given batch size.\"\"\"\n+\n+    # Sequential tokenization using encode()\n+    sequential_times = []\n+    for run in range(NUM_RUNS):\n+        batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n+\n+        start_time = time.time()\n+        for prompt in batch_prompts:\n+            tokens = tokenizer.encode(prompt)\n+        sequential_time = (time.time() - start_time) * 1000\n+        sequential_times.append(sequential_time)\n+\n+    # Batch tokenization using tokenizer()\n+    batch_times = []\n+    for run in range(NUM_RUNS):\n+        batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n+\n+        start_time = time.time()\n+        tokens = tokenizer(batch_prompts)\n+        batch_time = (time.time() - start_time) * 1000\n+        batch_times.append(batch_time)\n+\n+    return {\n+        \"batch_size\": batch_size,\n+        \"avg_sequential_ms\": mean(sequential_times),\n+        \"avg_batch_ms\": mean(batch_times),\n+        \"speedup_factor\": (\n+            mean(sequential_times) / mean(batch_times) if mean(batch_times) > 0 else 0\n+        ),\n+        \"sequential_runs\": sequential_times,\n+        \"batch_runs\": batch_times,\n+    }\n+\n+\n+def main():\n+    print(\"Tokenizer Benchmark: Sequential vs Batch Processing\")\n+    print(\"-\" * 60)\n+    print(f\"Tokenizer: {TOKENIZER_DIR}\")\n+    print(f\"Tokens per prompt: {NUM_TOKENS}\")\n+    print(f\"Number of runs per batch size: {NUM_RUNS}\")\n+    print(\"-\" * 60)\n+\n+    # Load tokenizer once for all operations\n+    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)\n+\n+    # The largest batch size determines how many prompts we need\n+    max_batch_size = max(BATCH_SIZES)\n+    all_prompts = generate_random_prompts(max_batch_size, NUM_TOKENS, tokenizer)\n+\n+    results = []\n+    print(\"\\nRunning benchmark...\")\n+\n+    for batch_size in BATCH_SIZES:\n+        print(f\"\\nBenchmarking batch size: {batch_size}\")\n+        result = benchmark_sequential_vs_batch(all_prompts, batch_size, tokenizer)\n+        results.append(result)\n+\n+        print(f\"  Sequential tokenization (encode):\")\n+        for i, run_time in enumerate(result[\"sequential_runs\"]):\n+            print(f\"    Run {i+1}: {run_time:.2f} ms\")\n+        print(f\"    Average: {result['avg_sequential_ms']:.2f} ms\")\n+\n+        print(f\"  Batch tokenization (tokenizer):\")\n+        for i, run_time in enumerate(result[\"batch_runs\"]):\n+            print(f\"    Run {i+1}: {run_time:.2f} ms\")\n+        print(f\"    Average: {result['avg_batch_ms']:.2f} ms\")\n+\n+        print(f\"  Speedup factor: {result['speedup_factor']:.2f}x\")\n+\n+    print(\"\\n\" + \"=\" * 60)\n+    print(\"SUMMARY OF RESULTS\")\n+    print(\"=\" * 60)\n+    print(\n+        f\"{'Batch Size':<10} {'Sequential (ms)':<18} {'Batch (ms)':<18} {'Speedup':<10}\"\n+    )\n+    print(\"-\" * 60)\n+\n+    for result in results:\n+        print(\n+            f\"{result['batch_size']:<10} {result['avg_sequential_ms']:.2f} ms{' ' * 8} {result['avg_batch_ms']:.2f} ms{' ' * 8} {result['speedup_factor']:.2f}x\"\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    random.seed(0)\n+    main()\ndiff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex a391dd719..92a6bbafc 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -415,38 +415,60 @@ class TokenizerManager:\n         )\n         if image_inputs and \"input_ids\" in image_inputs:\n             input_ids = image_inputs[\"input_ids\"]\n-        if self.is_generation:\n-            return_logprob = obj.return_logprob\n-            logprob_start_len = obj.logprob_start_len\n-            top_logprobs_num = obj.top_logprobs_num\n-            token_ids_logprob = obj.token_ids_logprob\n-            session_params = (\n-                SessionParams(**obj.session_params) if obj.session_params else None\n-            )\n+\n+        self._validate_token_len(obj, input_ids)\n+        return self._create_tokenized_object(\n+            obj, input_text, input_ids, input_embeds, image_inputs\n+        )\n+\n+    def _validate_token_len(\n+        self, obj: Union[GenerateReqInput, EmbeddingReqInput], input_ids: List[int]\n+    ) -> None:\n+        \"\"\"Validates that the input token count and the requested token count doesn't exceed the model's context length.\"\"\"\n \n         input_token_num = len(input_ids) if input_ids is not None else 0\n+        # Check if input alone exceeds context length\n         if input_token_num >= self.context_len:\n             raise ValueError(\n                 f\"The input ({input_token_num} tokens) is longer than the \"\n                 f\"model's context length ({self.context_len} tokens).\"\n             )\n \n+        # Check total tokens (input + max_new_tokens)\n+        max_new_tokens = obj.sampling_params.get(\"max_new_tokens\")\n         if (\n-            obj.sampling_params.get(\"max_new_tokens\") is not None\n-            and obj.sampling_params.get(\"max_new_tokens\") + input_token_num\n-            >= self.context_len\n+            max_new_tokens is not None\n+            and (max_new_tokens + input_token_num) >= self.context_len\n         ):\n-            raise ValueError(\n+            total_tokens = max_new_tokens + input_token_num\n+            error_msg = (\n                 f\"Requested token count exceeds the model's maximum context length \"\n-                f\"of {self.context_len} tokens. You requested a total of \"\n-                f\"{obj.sampling_params.get('max_new_tokens') + input_token_num} \"\n+                f\"of {self.context_len} tokens. You requested a total of {total_tokens} \"\n                 f\"tokens: {input_token_num} tokens from the input messages and \"\n-                f\"{obj.sampling_params.get('max_new_tokens')} tokens for the \"\n-                f\"completion. Please reduce the number of tokens in the input \"\n-                f\"messages or the completion to fit within the limit.\"\n+                f\"{max_new_tokens} tokens for the completion. Please reduce the number \"\n+                f\"of tokens in the input messages or the completion to fit within the limit.\"\n+            )\n+            raise ValueError(error_msg)\n+\n+    def _create_tokenized_object(\n+        self,\n+        obj: Union[GenerateReqInput, EmbeddingReqInput],\n+        input_text: str,\n+        input_ids: List[int],\n+        input_embeds: Optional[Union[List[float], None]] = None,\n+        image_inputs: Optional[Dict] = None,\n+    ) -> Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]:\n+        \"\"\"Create a tokenized request object from common parameters.\"\"\"\n+\n+        if self.is_generation:\n+            return_logprob = obj.return_logprob\n+            logprob_start_len = obj.logprob_start_len\n+            top_logprobs_num = obj.top_logprobs_num\n+            token_ids_logprob = obj.token_ids_logprob\n+            session_params = (\n+                SessionParams(**obj.session_params) if obj.session_params else None\n             )\n \n-        # Parse sampling parameters\n         sampling_params = SamplingParams(**obj.sampling_params)\n         sampling_params.normalize(self.tokenizer)\n         sampling_params.verify()\n@@ -483,6 +505,50 @@ class TokenizerManager:\n \n         return tokenized_obj\n \n+    async def _batch_tokenize_and_process(\n+        self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput]\n+    ) -> List[Union[TokenizedGenerateReqInput, TokenizedEmbeddingReqInput]]:\n+        \"\"\"Handle batch tokenization for text inputs only.\"\"\"\n+        logger.debug(f\"Starting batch tokenization for {batch_size} text requests\")\n+\n+        # Collect requests and texts\n+        requests = [obj[i] for i in range(batch_size)]\n+        texts = [req.text for req in requests]\n+\n+        # Batch tokenize all texts\n+        encoded = self.tokenizer(texts)\n+        input_ids_list = encoded[\"input_ids\"]\n+\n+        # Process all requests\n+        tokenized_objs = []\n+        for i, req in enumerate(requests):\n+            self._validate_token_len(obj[i], input_ids_list[i])\n+            tokenized_objs.append(\n+                self._create_tokenized_object(\n+                    req, req.text, input_ids_list[i], None, None\n+                )\n+            )\n+        logger.debug(f\"Completed batch processing for {batch_size} requests\")\n+        return tokenized_objs\n+\n+    def _validate_batch_tokenization_constraints(\n+        self, batch_size: int, obj: Union[GenerateReqInput, EmbeddingReqInput]\n+    ) -> None:\n+        \"\"\"Validate constraints for batch tokenization processing.\"\"\"\n+        for i in range(batch_size):\n+            if self.is_generation and obj[i].image_data:\n+                raise ValueError(\n+                    \"For image input processing do not set `enable_tokenizer_batch_encode`.\"\n+                )\n+            if obj[i].input_ids is not None:\n+                raise ValueError(\n+                    \"Batch tokenization is not needed for pre-tokenized input_ids. Do not set `enable_tokenizer_batch_encode`.\"\n+                )\n+            if obj[i].input_embeds is not None:\n+                raise ValueError(\n+                    \"Batch tokenization is not needed for input_embeds. Do not set `enable_tokenizer_batch_encode`.\"\n+                )\n+\n     def _send_one_request(\n         self,\n         obj: Union[GenerateReqInput, EmbeddingReqInput],\n@@ -560,14 +626,27 @@ class TokenizerManager:\n \n         generators = []\n         rids = []\n+\n         if getattr(obj, \"parallel_sample_num\", 1) == 1:\n-            # Send all requests\n-            for i in range(batch_size):\n-                tmp_obj = obj[i]\n-                tokenized_obj = await self._tokenize_one_request(tmp_obj)\n-                self._send_one_request(tmp_obj, tokenized_obj, created_time)\n-                generators.append(self._wait_one_response(tmp_obj, request))\n-                rids.append(tmp_obj.rid)\n+            if self.server_args.enable_tokenizer_batch_encode:\n+                # Validate batch tokenization constraints\n+                self._validate_batch_tokenization_constraints(batch_size, obj)\n+\n+                tokenized_objs = await self._batch_tokenize_and_process(batch_size, obj)\n+\n+                for i, tokenized_obj in enumerate(tokenized_objs):\n+                    tmp_obj = obj[i]\n+                    self._send_one_request(tmp_obj, tokenized_obj, created_time)\n+                    generators.append(self._wait_one_response(tmp_obj, request))\n+                    rids.append(tmp_obj.rid)\n+            else:\n+                # Sequential tokenization and processing\n+                for i in range(batch_size):\n+                    tmp_obj = obj[i]\n+                    tokenized_obj = await self._tokenize_one_request(tmp_obj)\n+                    self._send_one_request(tmp_obj, tokenized_obj, created_time)\n+                    generators.append(self._wait_one_response(tmp_obj, request))\n+                    rids.append(tmp_obj.rid)\n         else:\n             # FIXME: When using batch and parallel_sample_num together, the perf is not optimal.\n             if batch_size > 128:\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex e1768b52e..ddbbdf35d 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -49,6 +49,7 @@ class ServerArgs:\n     tokenizer_path: Optional[str] = None\n     tokenizer_mode: str = \"auto\"\n     skip_tokenizer_init: bool = False\n+    enable_tokenizer_batch_encode: bool = False\n     load_format: str = \"auto\"\n     trust_remote_code: bool = False\n     dtype: str = \"auto\"\n@@ -432,6 +433,11 @@ class ServerArgs:\n             action=\"store_true\",\n             help=\"If set, skip init tokenizer and pass input_ids in generate request\",\n         )\n+        parser.add_argument(\n+            \"--enable-tokenizer-batch-encode\",\n+            action=\"store_true\",\n+            help=\"Enable batch tokenization for improved performance when processing multiple text inputs. Do not use with image inputs, pre-tokenized input_ids, or input_embeds.\",\n+        )\n         parser.add_argument(\n             \"--load-format\",\n             type=str,",
  "apis": [
    "sglang.srt.managers.tokenizer_manager.TokenizerManager.generate_request",
    "sglang.srt.server_args.ServerArgs.enable_tokenizer_batch_encode"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/managers/tokenizer_manager.py",
    "/path/to/repos/sglang/benchmark/benchmark_batch/benchmark_tokenizer.py",
    "/path/to/repos/sglang/python/sglang/srt/server_args.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit adds new batch tokenization code and introduces a command-line flag to enable it. The changes impact the tokenizer manager's behavior, allowing it to process multiple inputs in a batch, which should improve performance in tokenization and overall throughput. The modifications are in production code (non-test files), include new functions for batch processing in the tokenizer manager, and update server arguments to allow its configuration. These changes are clearly intended to optimize the ongoing performance of CPU-based tokenization without targeting GPU-specific improvements. Therefore, the commit meets the conditions for being performance/optimization related.",
  "llm_api_reason": "This commit introduces a new benchmark file for batch processing and, more importantly, modifies the TokenizerManager class to support batch tokenization. It adds new helper functions (_batch_tokenize_and_process and _validate_batch_tokenization_constraints) and updates the logic in _send_one_request to conditionally use batch tokenization based on the new configuration flag enable_tokenizer_batch_encode added to ServerArgs. Thus, the public high\u2010level API for processing generation requests in TokenizerManager (i.e. generate_request) is affected, and the ServerArgs configuration now exposes enable_tokenizer_batch_encode."
}