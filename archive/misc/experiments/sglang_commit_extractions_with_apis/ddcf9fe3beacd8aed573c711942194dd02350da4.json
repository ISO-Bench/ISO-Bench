{
  "commit_hash": "ddcf9fe3beacd8aed573c711942194dd02350da4",
  "pr_url": "https://github.com/sgl-project/sglang/pull/3731",
  "pr_date": "2025-02-20",
  "timeline_text": "Copy link Collaborator ispobock commented Feb 20, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation Skip custom mask for prefix part of triton attention to accelerate target verify stage. python3 -m sglang.launch_server --model meta-llama/Llama-2-7b-chat-hf  --speculative-algo EAGLE --speculative-draft lmzheng/sglang-EAGLE-llama2-chat-7B --speculative-num-steps 2 --speculative-eagle-topk 4 --speculative-num-draft-tokens 4 --disable-radix --attention-backend triton\npython3 benchmark/gsm8k/bench_sglang.py --num-questions 1319 --parallel 1319 # main Accuracy: 0.233\nInvalid: 0.002\nLatency: 187.807 s\nOutput throughput: 796.177 token/s # this pr Accuracy: 0.233\nInvalid: 0.002\nLatency: 105.266 s\nOutput throughput: 1421.813 token/s Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji \ud83c\udf89 1 michaelfeil reacted with hooray emoji All reactions \ud83d\udc4d 1 reaction \ud83c\udf89 1 reaction skip custom mask for prefix part d12cf10 ispobock requested review from merrymercy , Ying1123 and zhyncs as code owners February 20, 2025 16:00 Merge branch 'main' into skip-custom-mask fec6422 Hide details View details zhyncs merged commit ddcf9fe into sgl-project : main Feb 20, 2025 16 of 19 checks passed Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:23",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "meta-llama/Llama-2-7b-chat-hf"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=meta-llama/Llama-2-7b-chat-hf --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python3 benchmark/gsm8k/bench_sglang.py --num-questions 1319 --parallel 1319",
  "commit_subject": "Optimize triton attention custom mask (#3731)",
  "commit_message": "Optimize triton attention custom mask (#3731)",
  "commit_date": "2025-02-21T00:54:41+08:00",
  "files_changed": [
    "python/sglang/srt/layers/attention/triton_ops/extend_attention.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 7,
    "num_files": 1,
    "num_hunks": 5,
    "num_non_test_edited_lines": 7,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\nindex 608f9bab0..079c8cfd9 100644\n--- a/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n+++ b/python/sglang/srt/layers/attention/triton_ops/extend_attention.py\n@@ -74,6 +74,7 @@ def _fwd_kernel(\n     BLOCK_M: tl.constexpr,\n     BLOCK_N: tl.constexpr,\n     USE_CUSTOM_MASK: tl.constexpr,\n+    SKIP_PREFIX_CUSTOM_MASK: tl.constexpr,\n     STORE_TRANSPOSE: tl.constexpr,\n ):\n     cur_seq = tl.program_id(0)\n@@ -160,7 +161,7 @@ def _fwd_kernel(\n         if logit_cap > 0:\n             qk = logit_cap * tanh(qk / logit_cap)\n \n-        if USE_CUSTOM_MASK:\n+        if USE_CUSTOM_MASK and not SKIP_PREFIX_CUSTOM_MASK:\n             custom_mask = tl.load(\n                 mask_ptr\n                 + cur_seq_mask_start_idx\n@@ -302,6 +303,7 @@ def extend_attention_fwd(\n     max_len_extend,\n     sm_scale=None,\n     logit_cap=0.0,\n+    skip_prefix_custom_mask=True,\n ):\n     \"\"\"\n     q_extend, k_extend, v_extend, o_extend: contiguous tensors\n@@ -355,6 +357,8 @@ def extend_attention_fwd(\n     kv_group_num = q_extend.shape[1] // k_extend.shape[1]\n \n     USE_CUSTOM_MASK = custom_mask is not None\n+    # Skip custom mask for prefix part\n+    SKIP_PREFIX_CUSTOM_MASK = skip_prefix_custom_mask\n \n     grid = (batch_size, head_num, triton.cdiv(max_len_extend, BLOCK_M))\n     num_stages = 1\n@@ -398,6 +402,7 @@ def extend_attention_fwd(\n         Lq=Lq,\n         Lv=Lv,\n         USE_CUSTOM_MASK=USE_CUSTOM_MASK,\n+        SKIP_PREFIX_CUSTOM_MASK=SKIP_PREFIX_CUSTOM_MASK,\n         STORE_TRANSPOSE=is_hip_,\n         num_warps=num_warps,\n         num_stages=num_stages,",
  "apis": [
    "sglang.srt.layers.attention.triton_ops.extend_attention.extend_attention_fwd"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/attention/triton_ops/extend_attention.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/python/sglang/srt/custom_op.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes changes in a non-test source file in the triton_ops module, altering the computation of the custom mask in the attention kernel. The change introduces a new parameter that conditionally skips the custom mask computation for the prefix part, which is likely a performance improvement rather than a bug fix or mere refactoring. Although the commit message mentions \"Optimize triton attention custom mask\", the code change itself conditionally bypasses a potentially expensive load operation, directly impacting the performance of this high-level API. Furthermore, the modifications work on CPU-relevant code and meet the performance optimization condition.",
  "llm_api_reason": "The commit introduces a new parameter \"skip_prefix_custom_mask\" in the function \"extend_attention_fwd\" and propagates it down to the Triton kernel. In addition, the use of the custom mask in the kernel (_fwd_kernel) is now gated by checking this new flag. These changes affect the Python API \"extend_attention_fwd\" in the file extend_attention.py."
}