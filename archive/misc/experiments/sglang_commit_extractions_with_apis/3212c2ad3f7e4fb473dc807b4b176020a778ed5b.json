{
  "commit_hash": "3212c2ad3f7e4fb473dc807b4b176020a778ed5b",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6003",
  "pr_date": "2025-07-26",
  "timeline_text": "Copy link Collaborator mickqian commented May 4, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation While extending, a TokenizedGenerateReqInput is expected to be sent between processes (normally TokenizerManager -> Scheduler ). For mllms, when TokenizedGenerateReqInput carries tensors(e.g., pixel_values), tensor pickling with copy can be expensive. Luckily, when the sender and receiver are in the same node, cuda_ipc could save these operations. This pr can reduce TTFT for vlms with multimodal data in request. Modifications TransportableTensor class with serialize/deserialize functions modify the __getstate__ and __setstate__ (which are basically hooks for pickles), to modify the tensor in-place Benchmark mmmu, all accuracy remains the same time base pr OpenGVLab/InternVL2_5-8B (tp==1) 207.7 173.3 OpenGVLab/InternVL2_5-8B (tp==4) 158.1 139.2 TODO support nccl transport for those tensors simplify the logic Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, JustinTong0323, and Swipe4057 reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions mickqian changed the title Optimize req [WIP] zero-copy gpu-tensor from Req May 4, 2025 This comment was marked as outdated. Sign in to view mickqian force-pushed the optimize_req branch\n      2 times, most recently\n    from 0d5abe2 to 78ee211 Compare May 4, 2025 05:20 mickqian changed the title [WIP] zero-copy gpu-tensor from Req [WIP] zero-copy gpu-tensor in Req May 4, 2025 mickqian changed the title [WIP] zero-copy gpu-tensor in Req [WIP] optimize transfer of multimodal data as gpu-tensor in Req May 5, 2025 mickqian force-pushed the optimize_req branch\n      4 times, most recently\n    from 156c31e to 07c6021 Compare June 10, 2025 08:00 mickqian changed the title [WIP] optimize transfer of multimodal data as gpu-tensor in Req vlm: optimize transfer of tensor in TokenizedGenerateReq Jun 10, 2025 mickqian force-pushed the optimize_req branch\n      3 times, most recently\n    from 6ffb637 to 4ee3942 Compare June 11, 2025 04:22 mickqian marked this pull request as ready for review June 11, 2025 08:49 mickqian requested review from merrymercy , Ying1123 , zhyncs , hnyls2002 , ispobock , ByronHsu , zhaochenyang20 and xiezhq-hermann as code owners June 11, 2025 08:49 mickqian marked this pull request as draft June 11, 2025 08:51 mickqian mentioned this pull request Jul 16, 2025 refactor: unify names of the feature field of MultimodalDataItem #8075 Merged 6 tasks mickqian force-pushed the optimize_req branch\n    from 4ee3942 to 3c7d28d Compare July 16, 2025 16:10 mickqian marked this pull request as ready for review July 16, 2025 16:12 mickqian requested a review\n  from JustinTong0323 as a code owner July 16, 2025 16:12 mickqian force-pushed the optimize_req branch\n    from 954ca18 to b7127e8 Compare July 17, 2025 00:56 mickqian changed the title vlm: optimize transfer of tensor in TokenizedGenerateReq vlm: optimize tensor transport Jul 17, 2025 26 hidden items Load more\u2026 mickqian added 13 commits July 21, 2025 14:01 update 9bf97b8 cleanup bd222f5 cleanup 31bb7a2 revert mmmu related ab8cdd3 revert mmmu related 736acfb cleanup 2c2e323 processor 81ba186 TransportableTensor __getstate__ 7a12fbb rename aa5c34d update 2559180 fix 886bd05 upd 56c07c1 upd 631d4b5 mickqian force-pushed the optimize_req branch\n    from 4ced6a7 to 631d4b5 Compare July 21, 2025 06:08 JustinTong0323 and others added 4 commits July 23, 2025 17:35 Merge branch 'main' into optimize_req f2379c4 remove duplicate multimodal_processors/qwen_audio.py a86de63 refactor: update terminology from 'precomputed features' to 'precompu\u2026 \u2026 fded26d \u2026ted embeddings' in relevant classes and tests\n\nSigned-off-by: Xinyuan Tong <xinyuantong.cs@gmail.com> refactor: update constructors in multimodal processors to accept addi\u2026 \u2026 deec266 \u2026tional arguments\n\nUpdated the constructors of various multimodal processor classes to include *args and **kwargs, allowing for more flexible initialization and compatibility with future extensions.\n\nSigned-off-by: Xinyuan Tong <xinyuantong.cs@gmail.com> JustinTong0323 reviewed Jul 24, 2025 View reviewed changes python/sglang/srt/multimodal/processors/base_processor.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . mickqian and others added 3 commits July 24, 2025 20:05 fix ef75f54 fix: rename precomputed_features to precomputed_embeddings 8cd0b75 Merge branch 'main' into optimize_req 54a8f31 JustinTong0323 added Multi-modal multi-modal language model ready-to-merge The PR is ready to merge after the CI is green. labels Jul 26, 2025 Hide details View details ispobock merged commit 3212c2a into sgl-project : main Jul 26, 2025 111 of 120 checks passed Uh oh! There was an error while loading. Please reload this page . JustinTong0323 mentioned this pull request Jul 28, 2025 [Bug] [Multimodal] GPU memory leak #8429 Open 5 tasks kousakawang mentioned this pull request Jul 28, 2025 [Feature] Add cache for Multimoal  input data #8433 Open 2 tasks narutolhy pushed a commit\n        to narutolhy/sglang\n      that referenced\n      this pull request Jul 29, 2025 vlm: optimize tensor transport ( sgl-project#6003 ) \u2026 33e1d8d Co-authored-by: Xinyuan Tong <115166877+JustinTong0323@users.noreply.github.com> ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 vlm: optimize tensor transport ( #6003 ) \u2026 3149977 Co-authored-by: Xinyuan Tong <115166877+JustinTong0323@users.noreply.github.com> ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 vlm: optimize tensor transport ( #6003 ) \u2026 852eae4 Co-authored-by: Xinyuan Tong <115166877+JustinTong0323@users.noreply.github.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:04",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "OpenGVLab/InternVL2_5-8B"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "vlm: optimize tensor transport (#6003)",
  "commit_message": "vlm: optimize tensor transport (#6003)\n\nCo-authored-by: Xinyuan Tong <115166877+JustinTong0323@users.noreply.github.com>",
  "commit_date": "2025-07-26T17:41:01+08:00",
  "files_changed": [
    "python/sglang/srt/managers/mm_utils.py",
    "python/sglang/srt/managers/multimodal_processor.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/managers/tokenizer_manager.py",
    "python/sglang/srt/multimodal/processors/base_processor.py",
    "python/sglang/srt/multimodal/processors/clip.py",
    "python/sglang/srt/multimodal/processors/deepseek_vl_v2.py",
    "python/sglang/srt/multimodal/processors/gemma3.py",
    "python/sglang/srt/multimodal/processors/gemma3n.py",
    "python/sglang/srt/multimodal/processors/internvl.py",
    "python/sglang/srt/multimodal/processors/janus_pro.py",
    "python/sglang/srt/multimodal/processors/kimi_vl.py",
    "python/sglang/srt/multimodal/processors/llava.py",
    "python/sglang/srt/multimodal/processors/minicpm.py",
    "python/sglang/srt/multimodal/processors/mlama.py",
    "python/sglang/srt/multimodal/processors/mllama4.py",
    "python/sglang/srt/multimodal/processors/phi4mm.py",
    "python/sglang/srt/multimodal/processors/pixtral.py",
    "python/sglang/srt/multimodal/processors/qwen_audio.py",
    "python/sglang/srt/multimodal/processors/qwen_vl.py",
    "python/sglang/srt/multimodal/processors/vila.py",
    "python/sglang/utils.py",
    "test/srt/test_vlm_input_format.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 281,
    "num_files": 23,
    "num_hunks": 36,
    "num_non_test_edited_lines": 281,
    "num_non_test_files": 23,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/mm_utils.py b/python/sglang/srt/managers/mm_utils.py\nindex 13ca29c54..78a9762ee 100644\n--- a/python/sglang/srt/managers/mm_utils.py\n+++ b/python/sglang/srt/managers/mm_utils.py\n@@ -3,8 +3,9 @@ Multi-modality utils\n \"\"\"\n \n import hashlib\n+import pickle\n from abc import abstractmethod\n-from typing import Callable, Dict, List, Optional, Tuple\n+from typing import Any, Callable, Dict, List, Literal, Optional, Tuple\n \n import numpy as np\n import torch\n@@ -27,6 +28,130 @@ from sglang.utils import logger\n # propagation that can cause some log messages (like 'server is fired up') to not appear\n # in the console when multimodal support is enabled.\n \n+# TODO(mick): nccl\n+# cuda_ipc: for intranode tensor sharing\n+TensorTransportMode = Literal[\"cuda_ipc\", \"auto\", \"default\"]\n+\n+\n+class TransportProxyTensor(torch.Tensor):\n+    \"\"\"\n+    A convenient torch.Tensor subclass that carries extra metadata and supports\n+    efficient inter-process communications\n+    \"\"\"\n+\n+    @staticmethod\n+    def __new__(\n+        cls,\n+        data: torch.Tensor,\n+        name: Optional[str] = None,\n+        fields: Optional[Dict[str, Any]] = None,\n+        transport_mode: TensorTransportMode = \"default\",\n+        *args,\n+        **kwargs,\n+    ):\n+\n+        if not isinstance(data, torch.Tensor):\n+            raise TypeError(\n+                f\"Input 'data' must be a torch.Tensor, but got {type(data)}\"\n+            )\n+\n+        instance = data.as_subclass(cls)\n+\n+        instance._metadata = {\n+            \"name\": name,\n+            \"fields\": fields if fields is not None else {},\n+            \"transport_mode\": transport_mode,\n+        }\n+\n+        return instance\n+\n+    def __getstate__(self):\n+        \"\"\"\n+        Called during pickling. Implements the serialization logic.\n+        \"\"\"\n+        # acquire all serialize metadata from _metadata\n+        state = {\n+            \"metadata\": self._metadata,\n+            \"tensor_data\": None,\n+            \"ipc_extra\": None,\n+        }\n+\n+        transport_mode = self._metadata.get(\"transport_mode\", \"default\")\n+\n+        if transport_mode == \"cuda_ipc\" and self.is_cuda:\n+            try:\n+                storage = self.untyped_storage()\n+                handle = storage._share_cuda_()\n+\n+                state[\"ipc_extra\"] = {\n+                    \"handle\": handle,\n+                    \"shape\": self.shape,\n+                    \"dtype\": self.dtype,\n+                    \"stride\": self.stride(),\n+                    \"device_index\": self.device.index,\n+                }\n+                state[\"tensor_data\"] = None\n+            except Exception as e:\n+                print_warning_once(\n+                    f\"Warning: Failed to get CUDA IPC handle ({e}). Falling back to default transport.\"\n+                )\n+                state[\"metadata\"][\"transport_mode\"] = \"default\"\n+                state[\"tensor_data\"] = self.as_subclass(torch.Tensor)\n+        else:\n+            state[\"metadata\"][\"transport_mode\"] = \"default\"\n+            state[\"tensor_data\"] = self.as_subclass(torch.Tensor)\n+\n+        return state\n+\n+    def __setstate__(self, state: Dict[str, Any]):\n+        \"\"\"\n+        Called during unpickling. Implements the deserialization logic.\n+        \"\"\"\n+        self._metadata = state[\"metadata\"]\n+\n+        transport_mode = self._metadata.get(\"transport_mode\", \"default\")\n+\n+        if transport_mode == \"cuda_ipc\" and state[\"ipc_extra\"] is not None:\n+            ipc_extra = state[\"ipc_extra\"]\n+            handle, shape, dtype, stride, source_device_index = (\n+                ipc_extra[\"handle\"],\n+                ipc_extra[\"shape\"],\n+                ipc_extra[\"dtype\"],\n+                ipc_extra[\"stride\"],\n+                ipc_extra[\"device_index\"],\n+            )\n+\n+            try:\n+                target_device = torch.device(f\"cuda:{source_device_index}\")\n+                with torch.cuda.device(target_device):\n+                    storage = torch.UntypedStorage._new_shared_cuda(*handle)\n+                    reconstructed_tensor = torch.empty(\n+                        0, dtype=dtype, device=target_device\n+                    ).set_(storage, storage_offset=0, size=shape, stride=stride)\n+                    self.set_(reconstructed_tensor)\n+            except Exception as e:\n+                print(f\"Error: Failed to deserialize from CUDA IPC handle ({e}).\")\n+                raise e\n+\n+        elif state[\"tensor_data\"] is not None:\n+            self.set_(state[\"tensor_data\"])\n+        else:\n+            raise pickle.UnpicklingError(\n+                \"Invalid state for TransportProxyTensor: no tensor data found.\"\n+            )\n+\n+    @property\n+    def name(self) -> Optional[str]:\n+        return self._metadata.get(\"name\")\n+\n+    @property\n+    def fields(self) -> Dict[str, Any]:\n+        return self._metadata.get(\"fields\", {})\n+\n+    @property\n+    def transport_mode(self) -> TensorTransportMode:\n+        return self._metadata.get(\"transport_mode\", \"default\")\n+\n \n class MultiModalityDataPaddingPattern:\n     \"\"\"\ndiff --git a/python/sglang/srt/managers/multimodal_processor.py b/python/sglang/srt/managers/multimodal_processor.py\nindex 76679358a..51b6f3d92 100644\n--- a/python/sglang/srt/managers/multimodal_processor.py\n+++ b/python/sglang/srt/managers/multimodal_processor.py\n@@ -12,18 +12,6 @@ logger = logging.getLogger(__name__)\n PROCESSOR_MAPPING = {}\n \n \n-class DummyMultimodalProcessor(BaseMultimodalProcessor):\n-    def __init__(self):\n-        pass\n-\n-    async def process_mm_data_async(self, *args, **kwargs):\n-        return None\n-\n-\n-def get_dummy_processor():\n-    return DummyMultimodalProcessor()\n-\n-\n def import_processors():\n     package_name = \"sglang.srt.multimodal.processors\"\n     package = importlib.import_module(package_name)\n@@ -49,11 +37,12 @@ def import_processors():\n \n \n def get_mm_processor(\n-    hf_config, server_args: ServerArgs, processor\n+    hf_config, server_args: ServerArgs, processor, transport_mode\n ) -> BaseMultimodalProcessor:\n     for model_cls, processor_cls in PROCESSOR_MAPPING.items():\n         if model_cls.__name__ in hf_config.architectures:\n-            return processor_cls(hf_config, server_args, processor)\n+            return processor_cls(hf_config, server_args, processor, transport_mode)\n+\n     raise ValueError(\n         f\"No processor registered for architecture: {hf_config.architectures}.\\n\"\n         f\"Registered architectures: {[model_cls.__name__ for model_cls in PROCESSOR_MAPPING.keys()]}\"\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex ad8bcf119..283da3394 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -209,10 +209,11 @@ class MultimodalDataItem:\n     hash: int = None\n     pad_value: int = None\n     offsets: Optional[list] = None\n+\n     # the raw features returned by processor, e.g. pixel_values or audio_features\n     feature: Union[torch.Tensor, np.ndarray] = None\n-\n-    # the precomputed embeddings for the modality, e.g. image_emb for image, audio_emb for audio\n+    # the precomputed embeddings, passed as final encoder embeddings\n+    # One and only one of the feature and precomputed_embeddings will be empty\n     precomputed_embeddings: Optional[Union[torch.Tensor, np.ndarray]] = None\n \n     # Model-specific data stored in a dictionary\ndiff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py\nindex 0f65fa925..77c805aac 100644\n--- a/python/sglang/srt/managers/tokenizer_manager.py\n+++ b/python/sglang/srt/managers/tokenizer_manager.py\n@@ -112,6 +112,7 @@ from sglang.srt.managers.io_struct import (\n     UpdateWeightsFromTensorReqInput,\n     UpdateWeightsFromTensorReqOutput,\n )\n+from sglang.srt.managers.mm_utils import TensorTransportMode\n from sglang.srt.managers.multimodal_processor import get_mm_processor, import_processors\n from sglang.srt.metrics.collector import TokenizerMetricsCollector\n from sglang.srt.sampling.sampling_params import SamplingParams\n@@ -166,6 +167,16 @@ class ReqState:\n     output_token_ids_logprobs_idx: List = dataclasses.field(default_factory=list)\n \n \n+def _determine_tensor_transport_mode(server_args: ServerArgs) -> TensorTransportMode:\n+    is_cross_node = server_args.dist_init_addr\n+\n+    if is_cross_node:\n+        # Fallback to default CPU transport for multi-node\n+        return \"default\"\n+    else:\n+        return \"cuda_ipc\"\n+\n+\n class TokenizerManager:\n     \"\"\"TokenizerManager is a process that tokenizes the text.\"\"\"\n \n@@ -216,12 +227,13 @@ class TokenizerManager:\n                 revision=server_args.revision,\n                 use_fast=not server_args.disable_fast_image_processor,\n             )\n+            transport_mode = _determine_tensor_transport_mode(self.server_args)\n \n             # We want to parallelize the image pre-processing so we create an executor for it\n             # We create mm_processor for any skip_tokenizer_init to make sure we still encode\n             # images even with skip_tokenizer_init=False.\n             self.mm_processor = get_mm_processor(\n-                self.model_config.hf_config, server_args, _processor\n+                self.model_config.hf_config, server_args, _processor, transport_mode\n             )\n \n             if server_args.skip_tokenizer_init:\ndiff --git a/python/sglang/srt/multimodal/processors/base_processor.py b/python/sglang/srt/multimodal/processors/base_processor.py\nindex 3d548a19e..3f62a14d1 100644\n--- a/python/sglang/srt/multimodal/processors/base_processor.py\n+++ b/python/sglang/srt/multimodal/processors/base_processor.py\n@@ -12,6 +12,7 @@ import torch\n from PIL import Image\n from transformers import BaseImageProcessorFast\n \n+from sglang.srt.managers.mm_utils import TransportProxyTensor\n from sglang.srt.managers.schedule_batch import Modality, MultimodalDataItem\n from sglang.srt.utils import load_audio, load_image, load_video, logger\n \n@@ -142,11 +143,14 @@ class MultimodalSpecialTokens:\n class BaseMultimodalProcessor(ABC):\n     models = []\n \n-    def __init__(self, hf_config, server_args, _processor):\n+    def __init__(\n+        self, hf_config, server_args, _processor, transport_mode, *args, **kwargs\n+    ):\n         self.hf_config = hf_config\n         self._processor = _processor\n         self.arch = hf_config.architectures[0]\n         self.server_args = server_args\n+        self.transport_mode = transport_mode\n \n         # FIXME: not accurate, model and image specific\n         self.NUM_TOKEN_PER_FRAME = 330\n@@ -217,10 +221,6 @@ class BaseMultimodalProcessor(ABC):\n             return_tensors=\"pt\",\n             **kwargs,\n         )\n-        if \"pixel_values\" in result and isinstance(\n-            result[\"pixel_values\"], torch.Tensor\n-        ):\n-            result[\"pixel_values\"] = result[\"pixel_values\"].to(\"cpu\")\n         return result\n \n     @abstractmethod\n@@ -500,7 +500,6 @@ class BaseMultimodalProcessor(ABC):\n     ) -> List[MultimodalDataItem]:\n         \"\"\"Create mm_items directly from processor output.\"\"\"\n         items: dict[Modality, MultimodalDataItem] = {}\n-\n         for attr_name, value in data_dict.items():\n             if attr_name == \"input_ids\":\n                 continue\n@@ -624,4 +623,19 @@ class BaseMultimodalProcessor(ABC):\n                 mm_token_id=mm_token_id,\n             )\n \n+        # post-process\n+        for item in all_collected_items:\n+            # replace the feature tensor with a proxy\n+            if isinstance(item.feature, torch.Tensor) and item.feature.is_cuda:\n+                item.feature = TransportProxyTensor(\n+                    transport_mode=self.transport_mode, data=item.feature\n+                )\n+            elif (\n+                isinstance(item.precomputed_embeddings, torch.Tensor)\n+                and item.precomputed_embeddings.is_cuda\n+            ):\n+                item.precomputed_embeddings = TransportProxyTensor(\n+                    transport_mode=self.transport_mode, data=item.precomputed_embeddings\n+                )\n+\n         return all_collected_items, input_ids, ret\ndiff --git a/python/sglang/srt/multimodal/processors/clip.py b/python/sglang/srt/multimodal/processors/clip.py\nindex 0925212cb..19ff71e78 100644\n--- a/python/sglang/srt/multimodal/processors/clip.py\n+++ b/python/sglang/srt/multimodal/processors/clip.py\n@@ -10,8 +10,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class ClipImageProcessor(BaseMultimodalProcessor):\n     models = [CLIPModel]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.mm_tokens = MultimodalSpecialTokens(image_token=\"<image>\").build(\n             _processor\n         )\ndiff --git a/python/sglang/srt/multimodal/processors/deepseek_vl_v2.py b/python/sglang/srt/multimodal/processors/deepseek_vl_v2.py\nindex 9847929f7..b09402d0b 100644\n--- a/python/sglang/srt/multimodal/processors/deepseek_vl_v2.py\n+++ b/python/sglang/srt/multimodal/processors/deepseek_vl_v2.py\n@@ -31,8 +31,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class DeepseekVL2ImageProcessor(BaseMultimodalProcessor):\n     models = [DeepseekVL2ForCausalLM]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=\"<image>\", image_token_id=self._processor.image_token_id\n         ).build(_processor)\ndiff --git a/python/sglang/srt/multimodal/processors/gemma3.py b/python/sglang/srt/multimodal/processors/gemma3.py\nindex 9abf172b2..cbfb45e84 100644\n--- a/python/sglang/srt/multimodal/processors/gemma3.py\n+++ b/python/sglang/srt/multimodal/processors/gemma3.py\n@@ -14,8 +14,8 @@ from sglang.srt.multimodal.processors.base_processor import MultimodalSpecialTok\n class Gemma3SGLangImageProcessor(SGLangBaseProcessor):\n     models = [Gemma3ForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.IM_START_TOKEN_ID = hf_config.boi_token_index\n         self.IM_END_TOKEN_ID = hf_config.eoi_token_index\n         self.mm_tokens = MultimodalSpecialTokens(\ndiff --git a/python/sglang/srt/multimodal/processors/gemma3n.py b/python/sglang/srt/multimodal/processors/gemma3n.py\nindex 938819d91..4bfbcaffa 100644\n--- a/python/sglang/srt/multimodal/processors/gemma3n.py\n+++ b/python/sglang/srt/multimodal/processors/gemma3n.py\n@@ -27,8 +27,8 @@ class Gemma3nSGLangProcessor(SGLangBaseProcessor):\n \n     models = [Gemma3nForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n \n         self.IM_START_TOKEN_ID = hf_config.boi_token_id\n         self.IM_END_TOKEN_ID = hf_config.eoi_token_id\ndiff --git a/python/sglang/srt/multimodal/processors/internvl.py b/python/sglang/srt/multimodal/processors/internvl.py\nindex 12823077f..234d57d35 100644\n--- a/python/sglang/srt/multimodal/processors/internvl.py\n+++ b/python/sglang/srt/multimodal/processors/internvl.py\n@@ -16,8 +16,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class InternVLImageProcessor(BaseMultimodalProcessor):\n     models = [InternVLChatModel]\n \n-    def __init__(self, hf_config, server_args, _image_processor):\n-        super().__init__(hf_config, server_args, _image_processor)\n+    def __init__(self, hf_config, server_args, _image_processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _image_processor, *args, **kwargs)\n         image_size = hf_config.force_image_size or hf_config.vision_config.image_size\n         patch_size = hf_config.vision_config.patch_size\n \ndiff --git a/python/sglang/srt/multimodal/processors/janus_pro.py b/python/sglang/srt/multimodal/processors/janus_pro.py\nindex 4dd8c1a84..54d6c1978 100644\n--- a/python/sglang/srt/multimodal/processors/janus_pro.py\n+++ b/python/sglang/srt/multimodal/processors/janus_pro.py\n@@ -11,8 +11,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class JanusProImageProcessor(BaseMultimodalProcessor):\n     models = [MultiModalityCausalLM]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n \n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=_processor.image_token,\ndiff --git a/python/sglang/srt/multimodal/processors/kimi_vl.py b/python/sglang/srt/multimodal/processors/kimi_vl.py\nindex 84c4a5133..541ed5c9e 100644\n--- a/python/sglang/srt/multimodal/processors/kimi_vl.py\n+++ b/python/sglang/srt/multimodal/processors/kimi_vl.py\n@@ -12,8 +12,8 @@ from sglang.srt.multimodal.processors.base_processor import MultimodalSpecialTok\n class KimiVLImageProcessor(SGLangBaseProcessor):\n     models = [KimiVLForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=\"<|media_pad|>\",\n             # TODO: could we convert in MultimodalSpecialTokens?\ndiff --git a/python/sglang/srt/multimodal/processors/llava.py b/python/sglang/srt/multimodal/processors/llava.py\nindex f4504ecea..5031dccbd 100644\n--- a/python/sglang/srt/multimodal/processors/llava.py\n+++ b/python/sglang/srt/multimodal/processors/llava.py\n@@ -30,8 +30,8 @@ class LlavaImageProcessor(BaseMultimodalProcessor):\n         LlavaMistralForCausalLM,\n     ]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n \n     @staticmethod\n     def _process_single_image_task(\n@@ -187,7 +187,7 @@ class LlavaMultimodalProcessor(BaseMultimodalProcessor):\n             f\"Cannot find corresponding multimodal processor registered in sglang for model type `{model_type}`\"\n         )\n \n-    def __init__(self, hf_config, server_args, _processor):\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n         assert hasattr(hf_config, \"vision_config\")\n         assert hasattr(hf_config, \"text_config\")\n         self.vision_config = hf_config.vision_config\n@@ -196,7 +196,7 @@ class LlavaMultimodalProcessor(BaseMultimodalProcessor):\n \n         if vision_type := getattr(self.vision_config, \"model_type\"):\n             self.inner = self._get_sgl_processor_cls(vision_type)(\n-                hf_config, server_args, _processor\n+                hf_config, server_args, _processor, *args, **kwargs\n             )\n         else:\n             raise ValueError(\ndiff --git a/python/sglang/srt/multimodal/processors/minicpm.py b/python/sglang/srt/multimodal/processors/minicpm.py\nindex ed4f86511..9ddbf4fb6 100644\n--- a/python/sglang/srt/multimodal/processors/minicpm.py\n+++ b/python/sglang/srt/multimodal/processors/minicpm.py\n@@ -15,8 +15,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class MiniCPMMultimodalProcessor(BaseMultimodalProcessor):\n     models = [MiniCPMV, MiniCPMO]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         # Collect special token ids\n         tokenizer = self._processor.tokenizer\n         self.slice_start_id = getattr(tokenizer, \"slice_start_id\", None)\n@@ -26,7 +26,6 @@ class MiniCPMMultimodalProcessor(BaseMultimodalProcessor):\n         self.im_start_id = getattr(tokenizer, \"im_start_id\", None)\n         self.im_end_id = getattr(tokenizer, \"im_end_id\", None)\n         self.im_token_id = getattr(tokenizer, \"unk_id\", None)\n-\n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=\"(<image>./</image>)\",\n             audio_token=\"(<audio>./</audio>)\",\ndiff --git a/python/sglang/srt/multimodal/processors/mlama.py b/python/sglang/srt/multimodal/processors/mlama.py\nindex dd3184452..432215a4f 100644\n--- a/python/sglang/srt/multimodal/processors/mlama.py\n+++ b/python/sglang/srt/multimodal/processors/mlama.py\n@@ -10,8 +10,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class MllamaImageProcessor(BaseMultimodalProcessor):\n     models = [MllamaForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=self._processor.image_token,\n             image_token_id=self._processor.image_token_id,\ndiff --git a/python/sglang/srt/multimodal/processors/mllama4.py b/python/sglang/srt/multimodal/processors/mllama4.py\nindex 2d0eba2fd..fd22d3848 100644\n--- a/python/sglang/srt/multimodal/processors/mllama4.py\n+++ b/python/sglang/srt/multimodal/processors/mllama4.py\n@@ -18,8 +18,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class Mllama4ImageProcessor(BaseMultimodalProcessor):\n     models = [Llama4ForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.vision_config = hf_config.vision_config\n         self.text_config = hf_config.text_config\n         self.boi_token_index = hf_config.boi_token_index\ndiff --git a/python/sglang/srt/multimodal/processors/phi4mm.py b/python/sglang/srt/multimodal/processors/phi4mm.py\nindex 720e3c132..1487d2ca2 100644\n--- a/python/sglang/srt/multimodal/processors/phi4mm.py\n+++ b/python/sglang/srt/multimodal/processors/phi4mm.py\n@@ -47,9 +47,9 @@ class Phi4MMProcessorAdapter(ProcessorMixin):\n class Phi4MMMultimodalProcessor(BaseMultimodalProcessor):\n     models = [Phi4MMForCausalLM]\n \n-    def __init__(self, hf_config, server_args, _processor):\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n         self.processor = Phi4MMProcessorAdapter(_processor)\n-        super().__init__(hf_config, server_args, self.processor)\n+        super().__init__(hf_config, server_args, self.processor, *args, **kwargs)\n \n         # the following CONSTANTS come from hugging-face microsoft/Phi-4-multimodal-instruct's processing_phi4mm.py file\n         # ref: https://huggingface.co/microsoft/Phi-4-multimodal-instruct/blob/main/processing_phi4mm.py\ndiff --git a/python/sglang/srt/multimodal/processors/pixtral.py b/python/sglang/srt/multimodal/processors/pixtral.py\nindex fdfd6bd62..af5cedec9 100644\n--- a/python/sglang/srt/multimodal/processors/pixtral.py\n+++ b/python/sglang/srt/multimodal/processors/pixtral.py\n@@ -42,8 +42,8 @@ class PixtralProcessor(BaseMultimodalProcessor):\n \n         return ncols, nrows\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.IM_TOKEN_ID = getattr(\n             hf_config, \"image_token_index\", PixtralVisionModel.DEFAULT_IMAGE_TOKEN_ID\n         )\ndiff --git a/python/sglang/srt/multimodal/processors/qwen_audio.py b/python/sglang/srt/multimodal/processors/qwen_audio.py\nindex 34d440375..b2bb38464 100644\n--- a/python/sglang/srt/multimodal/processors/qwen_audio.py\n+++ b/python/sglang/srt/multimodal/processors/qwen_audio.py\n@@ -11,8 +11,8 @@ from sglang.srt.multimodal.processors.base_processor import (\n class Qwen2AudioMultimodalProcessor(BaseMultimodalProcessor):\n     models = [Qwen2AudioForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.AUDIO_TOKEN = \"<|audio_bos|><|AUDIO|><|audio_eos|>\"\n         self.AUDIO_TOKEN_REGEX = re.compile(\n             r\"<\\|audio_bos\\|>(?:<\\|AUDIO\\|>)+<\\|audio_eos\\|>\"\ndiff --git a/python/sglang/srt/multimodal/processors/qwen_vl.py b/python/sglang/srt/multimodal/processors/qwen_vl.py\nindex 1b1de4369..f67f72b95 100644\n--- a/python/sglang/srt/multimodal/processors/qwen_vl.py\n+++ b/python/sglang/srt/multimodal/processors/qwen_vl.py\n@@ -201,8 +201,8 @@ async def preprocess_video(\n class Qwen2_5VLImageProcessor(SGLangBaseProcessor):\n     models = [Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration]\n \n-    def __init__(self, hf_config, server_args, _processor):\n-        super().__init__(hf_config, server_args, _processor)\n+    def __init__(self, hf_config, server_args, _processor, *args, **kwargs):\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         # The regex that matches expanded image tokens.\n         self.IM_START_TOKEN_ID = hf_config.vision_start_token_id\n         self.IM_END_TOKEN_ID = hf_config.vision_end_token_id\ndiff --git a/python/sglang/srt/multimodal/processors/vila.py b/python/sglang/srt/multimodal/processors/vila.py\nindex 7070dfe73..5f9586b6c 100644\n--- a/python/sglang/srt/multimodal/processors/vila.py\n+++ b/python/sglang/srt/multimodal/processors/vila.py\n@@ -34,8 +34,10 @@ class VILAMultimodalProcessor(BaseMultimodalProcessor):\n         hf_config: PretrainedConfig,\n         server_args: ServerArgs,\n         _processor: VILAProcessor,\n+        *args,\n+        **kwargs,\n     ) -> None:\n-        super().__init__(hf_config, server_args, _processor)\n+        super().__init__(hf_config, server_args, _processor, *args, **kwargs)\n         self.mm_tokens = MultimodalSpecialTokens(\n             image_token=self._processor.tokenizer.image_token,\n             image_token_id=hf_config.image_token_id,\ndiff --git a/python/sglang/utils.py b/python/sglang/utils.py\nindex 83c653232..b7600b1a6 100644\n--- a/python/sglang/utils.py\n+++ b/python/sglang/utils.py\n@@ -14,6 +14,7 @@ import traceback\n import urllib.request\n import weakref\n from concurrent.futures import ThreadPoolExecutor\n+from functools import wraps\n from io import BytesIO\n from json import dumps\n from typing import Any, Callable, List, Optional, Tuple, Type, Union\n@@ -28,6 +29,24 @@ from tqdm import tqdm\n logger = logging.getLogger(__name__)\n \n \n+def execute_once(func):\n+    has_run = None\n+\n+    @wraps(func)\n+    def wrapper(*args, **kwargs):\n+        nonlocal has_run\n+        if not has_run:\n+            func(*args, **kwargs)\n+            has_run = True\n+\n+    return wrapper\n+\n+\n+@execute_once\n+def info_once(message: str):\n+    logger.info(message)\n+\n+\n def convert_json_schema_to_str(json_schema: Union[dict, str, Type[BaseModel]]) -> str:\n     \"\"\"Convert a JSON schema to a string.\n     Parameters\ndiff --git a/test/srt/test_vlm_input_format.py b/test/srt/test_vlm_input_format.py\nindex 79625ee82..b2cf0073d 100644\n--- a/test/srt/test_vlm_input_format.py\n+++ b/test/srt/test_vlm_input_format.py\n@@ -24,7 +24,7 @@ class VLMInputTestBase:\n     model_path = None\n     chat_template = None\n     processor = None\n-    visual = None  # Should be a callable for precomputed features\n+    visual = None  # Should be a callable for precomputed embeddings\n \n     @classmethod\n     def setUpClass(cls):\n@@ -41,7 +41,7 @@ class VLMInputTestBase:\n \n     @classmethod\n     def _init_visual(cls):\n-        \"\"\"Override in subclass to set up cls.visual as a callable for precomputed features.\"\"\"\n+        \"\"\"Override in subclass to set up cls.visual as a callable for precomputed embeddings.\"\"\"\n         raise NotImplementedError\n \n     def setUp(self):",
  "apis": [
    "sglang.srt.managers.mm_utils.TransportProxyTensor",
    "sglang.srt.managers.multimodal_processor.get_mm_processor",
    "sglang.srt.multimodal.processors.base_processor.BaseMultimodalProcessor.__init__"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/managers/multimodal_processor.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/tokenizer_manager.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/mm_utils.py",
    "/path/to/repos/sglang/python/sglang/srt/multimodal/mm_utils.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit introduces a new class (TransportProxyTensor) that extends torch.Tensor with extra metadata and specialized serialization logic for inter-process communication using CUDA IPC when possible. It also propagates a new \"transport_mode\" parameter through various multimodal processor constructors and modifies tensor handling in key high-level APIs. These changes aim to efficiently transport tensor data between processes, potentially reducing overhead. The modifications are made in non-test source files and directly address performance optimization in data transport, not merely a refactor, bug fix, or documentation update. Although the commit message only briefly mentions \u201coptimize tensor transport,\u201d the implementation focuses on enabling efficient inter-process tensor sharing on CPU (with fallback for CUDA, which is testable without reliance on GPU-specific hardware). Therefore, the commit satisfies the performance optimization conditions outlined.",
  "llm_api_reason": "This commit introduces optimizations for tensor transport by adding a new tensor subclass\u2014TransportProxyTensor\u2014in the mm_utils module to carry extra metadata and support inter-process communication. It also updates multimodal processor interfaces by modifying the get_mm_processor API and the BaseMultimodalProcessor constructor to accept a new transport_mode parameter. Additional adjustments include changes to function signatures and minor documentation updates in related modules, all aimed at optimizing VLM tensor handling."
}