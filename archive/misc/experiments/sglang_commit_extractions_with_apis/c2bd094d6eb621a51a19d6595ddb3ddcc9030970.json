{
  "commit_hash": "c2bd094d6eb621a51a19d6595ddb3ddcc9030970",
  "pr_url": "https://github.com/sgl-project/sglang/pull/4643",
  "pr_date": "2025-03-22",
  "timeline_text": "Copy link Collaborator xutizhou commented Mar 21, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation The current performance of DeepEP is suboptimal due to the low efficiency of PyTorch's native permute function, which is used for formatting data before and after DeepEP communication. To address this limitation, we have implemented high-efficiency Triton kernels that significantly improve overall performance. Co-authored-by: @zhou9402 Performance on H20 Single Node Command python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --dp 8 --host 0.0.0.0 --port 30000 --enable-dp-attention --enable-deepep-moe --max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output --disable-cuda-graph\n\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 --random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 --max-concurrency 128 Version Concurrency Input Output Num Requests Input Throughput(tok/s) Output Throughput (tok/s) Total Throughput (tok/s) DeepEP(original) 127.97 1000 1000 512 436.69 436.69 873.38 DeepEP(current) 127.97 1000 1000 512 581.94 581.94 1163.87 Multi Node Command # node 0\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n  --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 0 \\\n  --enable-dp-attention --enable-deepep-moe \\\n  --disable-cuda-graph\n# node 1\npython3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n  --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 1 \\\n  --enable-dp-attention --enable-deepep-moe \\\n  --disable-cuda-graph Version Concurrency Input Output Num Requests Input Throughput(tok/s) Output Throughput (tok/s) Total Throughput (tok/s) DeepEP(current) 255.93 1000 1000 512 956.36 956.36 1912.71 DeepEP(current) 511.31 1000 1000 1024 1711.54 1711.54 3423.09 DeepEP(current) 1023.17 1000 1000 2048 2974.21 2974.21 5948.42 DeepEP(current) 2046.18 1000 1000 4096 3929.73 3929.73 7859.46 EPMoe 255.55 1000 1000 512 868.55 868.55 1737.10 EPMoe 511.85 1000 1000 1024 1694.59 1694.59 3389.18 EPMoe 1022.27 1000 1000 2048 2735.53 2735.53 5471.06 EPMoe 2045.90 1000 1000 4096 3489.57 3489.57 6979.15 Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 5 liz-badada, ch-wan, zhyncs, lambert0312, and Huixxi reacted with thumbs up emoji All reactions \ud83d\udc4d 5 reactions xutizhou added 2 commits March 20, 2025 22:27 add reorder kernels d7f1cfd format code 011a50f zhyncs assigned ch-wan Mar 21, 2025 xutizhou marked this pull request as ready for review March 21, 2025 05:01 xutizhou requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock , ByronHsu and HaiShaw as code owners March 21, 2025 05:01 Merge branch 'main' into optimize_permute_kernel beae218 zhyncs added\n  the high priority label Mar 21, 2025 ch-wan requested changes Mar 21, 2025 View reviewed changes python/sglang/srt/layers/moe/ep_moe/kernels.py Outdated def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int): reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True) seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64) Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It can be init using torch.empty Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction python/sglang/srt/layers/moe/ep_moe/kernels.py Outdated deepep_compute_src2dst_triton_kernel[grid]( reorder_ids, src2dst, topk_ids.numel(), num_minus_one, BLOCK_SIZE ) # src2dst -= num_minus_one Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment debugging code? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction python/sglang/srt/layers/moe/ep_moe/kernels.py Outdated @@ -17,6 +17,116 @@ logger = logging.getLogger(__name__) @triton.jit def compute_src2dst_triton_kernel( Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment compute_src2dst_triton_kernel and deepep_compute_src2dst_triton_kernel are defined twice. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction python/sglang/srt/layers/moe/ep_moe/kernels.py Outdated @triton.jit def deepep_compute_src2dst_triton_kernel( Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Why developing a triton kernel is necessary? Is it faster? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/layers/moe/ep_moe/kernels.py @triton.jit def deepep_permute_triton_kernel( Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It is defined twice. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction python/sglang/srt/layers/moe/ep_moe/kernels.py Outdated @triton.jit def deepep_post_reorder_triton_kernel( Copy link Collaborator ch-wan Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment It is defined twice. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction rm trivial code a7f30d1 ch-wan mentioned this pull request Mar 21, 2025 [Feature] Integrate DeepEP into SGLang #4232 Merged 6 tasks Edenzzzz reviewed Mar 21, 2025 View reviewed changes python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py Outdated Comment on lines 351 to 355 output = torch.zeros( (num_tokens, hidden_states.shape[1]), device=hidden_states.device, dtype=hidden_states.dtype, ) Copy link Contributor Edenzzzz Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Use torch.empty? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc40 1 xutizhou reacted with eyes emoji All reactions \ud83d\udc40 1 reaction Edenzzzz reviewed Mar 21, 2025 View reviewed changes python/sglang/srt/models/deepseek_v2.py @@ -294,7 +294,7 @@ def forward_deepep( correction_bias=self.correction_bias, ) if self.tp_size > 1: recv_hidden_states, topk_idx, topk_weights, tokens_per_expert = ( recv_hidden_states, reorder_topk_ids, seg_indptr = ( Copy link Contributor Edenzzzz Mar 21, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Should we add some short comments on the meaning/examples of reorder_topk_ids and seg_indptr for readability? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions ch-wan and others added 3 commits March 22, 2025 16:39 minor optim 6c25b79 Merge branch 'main' into optimize_permute_kernel 3f21f98 Merge branch 'main' into optimize_permute_kernel 4b39584 Hide details View details zhyncs merged commit c2bd094 into sgl-project : main Mar 22, 2025 0 of 16 checks passed Uh oh! There was an error while loading. Please reload this page . xutizhou deleted the optimize_permute_kernel branch March 23, 2025 03:21 xutizhou restored the optimize_permute_kernel branch March 23, 2025 04:43 Copy link Huixxi commented Mar 24, 2025 Will there be further optimization plans for this permute kernel\uff1f All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author xutizhou commented Mar 24, 2025 Will there be further optimization plans for this permute kernel\uff1f We will continue to optimize the permute kernel, but it is not our top priority at the moment. \ud83d\udc4d 1 Huixxi reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan mentioned this pull request Mar 24, 2025 [Roadmap] EP Enhancement #4734 Closed 18 tasks Copy link Huixxi commented Mar 26, 2025 node 0 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 0 --enable-dp-attention --enable-deepep-moe --disable-cuda-graph node 1 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 1 --enable-dp-attention --enable-deepep-moe --disable-cuda-graph But, it seems that I can't reproduce the performance of deepseek on 2 * H800 x 8 with roce rdma. I don't know why. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author xutizhou commented Mar 26, 2025 node 0 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 0 --enable-dp-attention --enable-deepep-moe --disable-cuda-graph node 1 python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 16 --dp 16  --dist-init-addr 10.6.131.5:5000 --nnodes 2 --node-rank 1 --enable-dp-attention --enable-deepep-moe --disable-cuda-graph But, it seems that I can't reproduce the performance of deepseek on 2 * H800 x 8 with roce rdma. I don't know why. The observed issue could potentially be attributed to ROCE network configuration. To verify this hypothesis, we recommend running the inter-node communication test from DeepEP's validation suite, specifically the internode connectivity check All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan mentioned this pull request Mar 26, 2025 Integrate DeepGemm contiguous group gemm into Fused MoE #4343 Closed 6 tasks Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:13",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-V3,trust_remote_code=True,tp=8 --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 --random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 --max-concurrency 128",
  "commit_subject": "Optimize Permute Kernel in DeepEP (#4643)",
  "commit_message": "Optimize Permute Kernel in DeepEP (#4643)\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>",
  "commit_date": "2025-03-22T14:30:34-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
    "python/sglang/srt/models/deepseek_v2.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 331,
    "num_files": 4,
    "num_hunks": 17,
    "num_non_test_edited_lines": 331,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 6d6c432f8..30c9eb6a7 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -17,52 +17,6 @@ if _is_cuda:\n logger = logging.getLogger(__name__)\n \n \n-@triton.jit\n-def compute_src2dst_triton_kernel(\n-    reorder_ids, src2dst, num_toks, BLOCK_SIZE: tl.constexpr\n-):\n-    pid = tl.program_id(axis=0)\n-    dst_id = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-    mask = dst_id < num_toks\n-    src_id = tl.load(reorder_ids + dst_id, mask=mask)\n-    tl.store(src2dst + src_id, dst_id, mask=mask)\n-\n-\n-@triton.jit\n-def deepep_compute_src2dst_triton_kernel(\n-    reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE: tl.constexpr\n-):\n-    pid = tl.program_id(axis=0)\n-    dst_id = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n-    mask = dst_id < num_toks\n-    src_id = tl.load(reorder_ids + dst_id, mask=mask)\n-    num_invalid = tl.load(num_minus_one)\n-    tl.store(src2dst + src_id, dst_id - num_invalid, mask=mask)\n-\n-\n-def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n-    reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n-    seg_indptr = torch.zeros(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n-    src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int32)\n-\n-    # Find offet\n-    expert_ids = torch.arange(\n-        num_experts + 1, device=topk_ids.device, dtype=reorder_topk_ids.dtype\n-    )\n-    torch.searchsorted(reorder_topk_ids, expert_ids, out=seg_indptr)\n-    num_minus_one = seg_indptr[0]\n-    seg_indptr = seg_indptr - num_minus_one\n-\n-    BLOCK_SIZE = 512\n-    grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)\n-    deepep_compute_src2dst_triton_kernel[grid](\n-        reorder_ids, src2dst, topk_ids.numel(), num_minus_one, BLOCK_SIZE\n-    )\n-\n-    reorder_topk_ids = reorder_topk_ids[num_minus_one:]\n-    return reorder_topk_ids, src2dst, seg_indptr\n-\n-\n @triton.jit\n def deepep_permute_triton_kernel(\n     input_ptr,\n@@ -85,14 +39,13 @@ def deepep_permute_triton_kernel(\n     for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n         offset = start_offset + tl.arange(0, BLOCK_SIZE)\n         mask = offset < hidden_size\n-        in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n+        in_data = tl.load(src_ptr + offset, mask=mask).to(OutDtype)\n \n         for idx in range(topk):\n             dst_idx = tl.load(src2dst_ptr + idx)\n             if dst_idx >= 0:\n                 dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n-                out_data = (in_data).to(OutDtype)\n-                tl.store(dst_ptr + offset, out_data, mask=mask)\n+                tl.store(dst_ptr + offset, in_data, mask=mask)\n \n \n @triton.jit\n@@ -128,6 +81,51 @@ def deepep_post_reorder_triton_kernel(\n         tl.store(store_ptr + offset, sum_vec, mask=mask)\n \n \n+@triton.jit\n+def compute_src2dst_triton_kernel(\n+    reorder_ids, src2dst, num_toks, BLOCK_SIZE: tl.constexpr\n+):\n+    pid = tl.program_id(axis=0)\n+    dst_id = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+    mask = dst_id < num_toks\n+    src_id = tl.load(reorder_ids + dst_id, mask=mask)\n+    tl.store(src2dst + src_id, dst_id, mask=mask)\n+\n+\n+@triton.jit\n+def deepep_compute_src2dst_triton_kernel(\n+    reorder_ids, src2dst, num_toks, num_minus_one, BLOCK_SIZE: tl.constexpr\n+):\n+    pid = tl.program_id(axis=0)\n+    dst_id = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n+    mask = dst_id < num_toks\n+    src_id = tl.load(reorder_ids + dst_id, mask=mask)\n+    num_invalid = tl.load(num_minus_one)\n+    tl.store(src2dst + src_id, dst_id - num_invalid, mask=mask)\n+\n+\n+def deepep_run_moe_deep_preprocess(topk_ids: torch.Tensor, num_experts: int):\n+    reorder_topk_ids, reorder_ids = torch.sort(topk_ids.view(-1), stable=True)\n+    seg_indptr = torch.empty(num_experts + 1, device=topk_ids.device, dtype=torch.int64)\n+    src2dst = torch.empty(topk_ids.numel(), device=topk_ids.device, dtype=torch.int64)\n+\n+    # Find offet\n+    expert_ids = torch.arange(\n+        num_experts + 1, device=topk_ids.device, dtype=reorder_topk_ids.dtype\n+    )\n+    torch.searchsorted(reorder_topk_ids, expert_ids, out=seg_indptr)\n+    num_minus_one = seg_indptr[0]\n+    seg_indptr = seg_indptr - num_minus_one\n+\n+    BLOCK_SIZE = 512\n+    grid = (triton.cdiv(topk_ids.numel(), BLOCK_SIZE),)\n+    deepep_compute_src2dst_triton_kernel[grid](\n+        reorder_ids, src2dst, topk_ids.numel(), num_minus_one, BLOCK_SIZE\n+    )\n+    reorder_topk_ids = reorder_topk_ids[num_minus_one:]\n+    return reorder_topk_ids, src2dst, seg_indptr\n+\n+\n @triton.jit\n def compute_seg_indptr_triton_kernel(reorder_topk_ids, seg_indptr, num_toks):\n     expert = tl.program_id(0)\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex a9b443a75..f0595bfb1 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -831,19 +831,23 @@ class DeepEPMoE(EPMoE):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        tokens_per_expert: torch.Tensor,\n+        reorder_topk_ids: torch.Tensor,\n+        seg_indptr: torch.Tensor,\n         forward_mode: ForwardMode,\n     ):\n         # Todo: use m_grouped_gemm_fp8_fp8_bf16_nt_masked after low_latency dispatch (decode)\n         if True:  # not forward_mode.is_decode():\n-            return self.forward_normal(hidden_states, tokens_per_expert)\n+            return self.forward_normal(hidden_states, reorder_topk_ids, seg_indptr)\n         else:\n-            return self.forward_deepgemm_masked(hidden_states, tokens_per_expert)\n+            return self.forward_deepgemm_masked(\n+                hidden_states, reorder_topk_ids, seg_indptr\n+            )\n \n     def forward_normal(\n         self,\n         hidden_states: torch.Tensor,\n-        tokens_per_expert: torch.Tensor,\n+        reorder_topk_ids: torch.Tensor,\n+        seg_indptr: torch.Tensor,\n     ):\n         assert self.quant_method is not None\n         assert self.activation == \"silu\"\n@@ -851,15 +855,7 @@ class DeepEPMoE(EPMoE):\n             self.grouped_gemm_runner = GroupedGemmRunner(\n                 hidden_states.device, use_flashinfer=False  # TODO: use flashinfer\n             )\n-        seg_indptr_cur_rank = torch.cat(\n-            [\n-                torch.zeros(\n-                    1, device=tokens_per_expert.device, dtype=tokens_per_expert.dtype\n-                ),\n-                torch.cumsum(tokens_per_expert, dim=0),\n-            ]\n-        )\n-        reorder_topk_ids = torch.repeat_interleave(tokens_per_expert)\n+\n         if self.activation_scheme == \"dynamic\" and not self.use_block_quant:\n             max_value = (\n                 torch.max(hidden_states)\n@@ -881,6 +877,7 @@ class DeepEPMoE(EPMoE):\n             device=hidden_states.device,\n             dtype=hidden_states.dtype,\n         )\n+\n         if hidden_states.shape[0] > 0:\n             gateup_output = self.grouped_gemm_runner(\n                 a=hidden_states,\n@@ -888,7 +885,7 @@ class DeepEPMoE(EPMoE):\n                 c=gateup_output,\n                 batch_size=self.num_experts_per_partition,\n                 weight_column_major=True,\n-                seg_indptr=seg_indptr_cur_rank,\n+                seg_indptr=seg_indptr,\n                 weight_indices=weight_indices_cur_rank,\n                 use_fp8_w8a8=self.use_fp8_w8a8,\n                 scale_a=self.w13_input_scale,\n@@ -946,7 +943,7 @@ class DeepEPMoE(EPMoE):\n                 c=down_output,\n                 batch_size=self.num_experts_per_partition,\n                 weight_column_major=True,\n-                seg_indptr=seg_indptr_cur_rank,\n+                seg_indptr=seg_indptr,\n                 weight_indices=weight_indices_cur_rank,\n                 use_fp8_w8a8=self.use_fp8_w8a8,\n                 scale_a=self.w2_input_scale,\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex c91ccd633..6d8605f77 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -12,7 +12,6 @@ import torch\n import torch.distributed as dist\n \n from sglang.srt.layers.moe.ep_moe.kernels import (\n-    compute_src2dst_triton_kernel,\n     deepep_permute_triton_kernel,\n     deepep_post_reorder_triton_kernel,\n     deepep_run_moe_deep_preprocess,\n@@ -86,90 +85,6 @@ def get_buffer_low_latency(\n     return _buffer_low_latency\n \n \n-def permute(\n-    tokens,\n-    routing_map,\n-    num_out_tokens: Optional[int] = None,\n-    fused: bool = False,\n-    drop_and_pad: bool = False,\n-):\n-    \"\"\"\n-    Copy from Megatron-Core moe for token permutation\n-    https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/moe_utils.py\n-    \"\"\"\n-\n-    num_tokens, _ = tokens.shape\n-    num_experts = routing_map.shape[1]\n-    if drop_and_pad and not (num_out_tokens is None):\n-        capacity = num_out_tokens // num_experts\n-        assert not routing_map.requires_grad\n-        routing_map = routing_map.to(dtype=torch.int8).T.contiguous()\n-        sorted_indices = routing_map.argsort(dim=-1, descending=True, stable=True)[\n-            :, :capacity\n-        ].contiguous()\n-        sorted_indices = sorted_indices.view(-1)\n-    else:\n-        routing_map = routing_map.bool().T.contiguous()\n-        token_indices = (\n-            torch.arange(num_tokens, device=routing_map.device)\n-            .unsqueeze(0)\n-            .expand(num_experts, -1)\n-        )\n-        sorted_indices = token_indices.masked_select(routing_map)\n-    permuted_input = tokens.index_select(0, sorted_indices)\n-\n-    return permuted_input, sorted_indices\n-\n-\n-def unpermute(\n-    permuted_tokens: torch.Tensor,\n-    sorted_indices: torch.Tensor,\n-    restore_shape: torch.Size,\n-    probs: torch.Tensor = None,\n-    routing_map: torch.Tensor = None,\n-    fused: bool = False,\n-    drop_and_pad: bool = False,\n-):\n-    \"\"\"\n-    Copy from Megatron-Core moe for token unpermutation\n-    https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/core/transformer/moe/moe_utils.py\n-    \"\"\"\n-\n-    _, hidden = restore_shape\n-\n-    if probs is not None:\n-        assert routing_map is not None, \"Mask must be provided to permute the probs.\"\n-        if drop_and_pad:\n-            num_experts = routing_map.size(1)\n-            num_permuted_tokens = sorted_indices.size(0)\n-            capacity = num_permuted_tokens // num_experts\n-            num_unpermuted_tokens = probs.size(0)\n-\n-            probs_T_1D = probs.T.contiguous().view(-1)\n-\n-            indices_dim0 = torch.arange(\n-                num_experts, device=routing_map.device\n-            ).unsqueeze(-1)\n-            indices_dim1 = sorted_indices.view(num_experts, capacity)\n-            indices_1D = (indices_dim0 * num_unpermuted_tokens + indices_dim1).view(-1)\n-\n-            permuted_probs = probs_T_1D.index_select(0, indices_1D)\n-        else:\n-            permuted_probs = probs.T.contiguous().masked_select(\n-                routing_map.T.contiguous()\n-            )\n-        permuted_tokens = permuted_tokens * permuted_probs.unsqueeze(-1)\n-\n-    output_tokens = torch.zeros(\n-        restore_shape, device=permuted_tokens.device, dtype=permuted_tokens.dtype\n-    )\n-    output_tokens.scatter_add_(\n-        0, sorted_indices.unsqueeze(1).expand(-1, hidden), permuted_tokens\n-    )\n-\n-    return output_tokens\n-\n-\n class DeepEPDispatcher:\n     \"\"\"\n     Copy from Megatron-Core token_dispatcher MoEFlexTokenDispatcher\n@@ -228,16 +143,13 @@ class DeepEPDispatcher:\n \n     def deepep_permute(\n         self,\n-        topk_ids,\n         hidden_states,\n-        num_experts,\n-        top_k,\n-        use_fp8_w8a8,\n-        use_block_quant,\n-        fp8_dtype,\n+        fp8_dtype=None,\n+        use_fp8_w8a8=False,\n+        use_block_quant=False,\n     ):\n         reorder_topk_ids, src2dst, seg_indptr = deepep_run_moe_deep_preprocess(\n-            topk_ids, num_experts\n+            self.topk_idx, self.num_experts\n         )\n         num_total_tokens = reorder_topk_ids.numel()\n         gateup_input = torch.empty(\n@@ -254,9 +166,9 @@ class DeepEPDispatcher:\n             hidden_states,\n             gateup_input,\n             src2dst,\n-            topk_ids,\n+            self.topk_idx,\n             None,\n-            top_k,\n+            self.router_topk,\n             hidden_states.shape[1],\n             BLOCK_SIZE=512,\n         )\n@@ -302,13 +214,21 @@ class DeepEPDispatcher:\n                 )\n             )\n             self.recv_expert_count = recv_expert_count\n-        tokens_per_expert = self.get_number_of_tokens_per_expert()\n         self.handle = handle\n         self.topk_idx = topk_idx\n         self.topk_weights = topk_weights\n         if hidden_states.shape[0] > 0:\n-            hidden_states = self.get_permuted_hidden_states_by_experts(hidden_states)\n-        return hidden_states, topk_idx, topk_weights, tokens_per_expert\n+            reorder_topk_ids, seg_indptr, hidden_states = self.deepep_permute(\n+                hidden_states, fp8_dtype=hidden_states.dtype\n+            )\n+        else:\n+            reorder_topk_ids = torch.empty(\n+                (0,), device=hidden_states.device, dtype=torch.int64\n+            )\n+            seg_indptr = torch.zeros(\n+                (num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n+            )\n+        return hidden_states, reorder_topk_ids, seg_indptr\n \n     def dispatch_normal(\n         self,\n@@ -427,10 +347,29 @@ class DeepEPDispatcher:\n         # Todo: enable low latency combine\n         if True:  # not forward_mode.is_decode():\n             if hidden_states.shape[0] > 0:\n-                hidden_states = self.get_restored_hidden_states_by_experts(\n-                    hidden_states\n+                num_tokens = self.src2dst.shape[0] // self.router_topk\n+                output = torch.empty(\n+                    (num_tokens, hidden_states.shape[1]),\n+                    device=hidden_states.device,\n+                    dtype=hidden_states.dtype,\n+                )\n+                deepep_post_reorder_triton_kernel[(num_tokens,)](\n+                    hidden_states,\n+                    output,\n+                    self.src2dst,\n+                    self.topk_idx,\n+                    self.topk_weights,\n+                    self.router_topk,\n+                    hidden_states.shape[1],\n+                    BLOCK_SIZE=512,\n                 )\n-            hidden_states, event = self.combine_normal(hidden_states, self.handle)\n+            else:\n+                output = torch.zeros(\n+                    (0, hidden_states.shape[1]),\n+                    device=hidden_states.device,\n+                    dtype=hidden_states.dtype,\n+                )\n+            hidden_states, event = self.combine_normal(output, self.handle)\n         else:\n             hidden_states, event, hook = self.combine_low_latency(\n                 hidden_states, self.topk_idx, self.topk_weights, self.handle\n@@ -467,67 +406,3 @@ class DeepEPDispatcher:\n         )\n         # hook()\n         return combined_hidden_states, event_overlap, hook\n-\n-    def _indices_to_multihot(self, indices, probs):\n-        batch_size = indices.shape[0]\n-        multihot_routing_map = torch.zeros(\n-            (batch_size, self.num_local_experts),\n-            dtype=torch.long,\n-            device=indices.device,\n-        )\n-\n-        multihot_probs = torch.zeros(\n-            (batch_size, self.num_local_experts),\n-            dtype=torch.float,\n-            device=indices.device,\n-        )\n-\n-        mask = indices != -1\n-        valid_indices = indices[mask]\n-        row_indices = torch.arange(batch_size, device=indices.device).repeat_interleave(\n-            mask.sum(dim=1)\n-        )\n-        multihot_routing_map[row_indices, valid_indices] = 1\n-        multihot_probs[row_indices, valid_indices] = probs[mask]\n-        return multihot_routing_map.bool(), multihot_probs\n-\n-    def get_dispached_metadata(self) -> torch.Tensor:\n-        return self.topk_idx, self.topk_weights\n-\n-    def get_number_of_tokens_per_expert(self) -> torch.Tensor:\n-        \"\"\"\n-        Get the number of tokens per expert.\n-        \"\"\"\n-        return self.tokens_per_expert\n-\n-    def get_permuted_hidden_states_by_experts(\n-        self, hidden_states: torch.Tensor\n-    ) -> torch.Tensor:\n-        self.dispatched_routing_map, self.topk_weights = self._indices_to_multihot(\n-            self.topk_idx, self.topk_weights\n-        )\n-        self.hidden_shape_before_permute = hidden_states.shape\n-        hidden_states, self.reversed_mapping_for_combine = permute(\n-            hidden_states,\n-            self.dispatched_routing_map,\n-            num_out_tokens=self.tokens_per_expert.sum(),\n-            fused=self.permute_fusion,\n-        )\n-        return hidden_states\n-\n-    def get_restored_hidden_states_by_experts(\n-        self, hidden_states: torch.Tensor\n-    ) -> torch.Tensor:\n-        input_dtype = hidden_states.dtype\n-        assert (\n-            self.topk_weights.dtype == torch.float32\n-        ), \"DeepEP only supports float32 probs\"\n-        hidden_states = unpermute(\n-            hidden_states,\n-            self.reversed_mapping_for_combine,\n-            restore_shape=self.hidden_shape_before_permute,\n-            routing_map=self.dispatched_routing_map,\n-            probs=self.topk_weights,\n-            fused=self.permute_fusion,\n-        )\n-        return hidden_states.to(input_dtype)\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex ffcc9a955..c62dacec9 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -294,7 +294,7 @@ class DeepseekV2MoE(nn.Module):\n                 correction_bias=self.correction_bias,\n             )\n         if self.tp_size > 1:\n-            recv_hidden_states, topk_idx, topk_weights, tokens_per_expert = (\n+            recv_hidden_states, reorder_topk_ids, seg_indptr = (\n                 self.deepep_dispatcher.dispatch(\n                     hidden_states,\n                     topk_idx,\n@@ -306,7 +306,8 @@ class DeepseekV2MoE(nn.Module):\n         final_hidden_states = (\n             self.experts(\n                 hidden_states=recv_hidden_states,\n-                tokens_per_expert=tokens_per_expert,\n+                reorder_topk_ids=reorder_topk_ids,\n+                seg_indptr=seg_indptr,\n                 forward_mode=forward_mode,\n             )\n             * self.routed_scaling_factor",
  "apis": [
    "DeepEPMoE.forward",
    "deepep_run_moe_deep_preprocess",
    "DeepEPDispatcher.deepep_permute",
    "DeepseekV2MoE.forward"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/models/deepseek_v2.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit touches several non-test source files and makes non-trivial changes to performance-critical kernels and data handling. It replaces old Python-based permutation functions with new Triton kernels for token dispatch and reorders the processing flow (changing parameters from tokens_per_expert to reorder_topk_ids and seg_indptr) in order to improve throughput. The modifications in the kernel computations (e.g., adjusting data types and reordering operations) indicate they are intended to optimize runtime performance on the CPU. The changes are not merely refactoring or documentation fixes, but an optimization of the internal processing pipeline of the model\u2019s high-level API. Therefore, the commit meets the performance/optimization criteria described.",
  "llm_api_reason": "This commit refactors and optimizes the token permutation and reordering logic used by the expert-parallel MoE code. In the kernels module the old \u201cpermute\u201d kernel functions are removed and then re\u2010added later with explicit dtypes (changing the src2dst tensor from int32 to int64, and ensuring proper conversions for the permute kernel). In the \u201clayer\u201d module the forward methods of DeepEPMoE are updated so that instead of receiving a \u201ctokens_per_expert\u201d tensor the methods now receive \u201creorder_topk_ids\u201d and \u201cseg_indptr\u201d produced by the updated deepep_run_moe_deep_preprocess function. In the token_dispatcher module the lower\u2010level permute/unpermute functions have been removed and the dispatcher now calls the new preprocessing kernel (via deepep_run_moe_deep_preprocess) to obtain the reordering indices. Finally, in the Deepseek_v2 model the MoE forward invocation is updated accordingly. Overall, the changes affect the high-level MoE forward APIs and its underlying preprocessing routine for token reordering."
}