{
  "commit_hash": "2716830802ae8c2428fdacde7c4041b6f7852d68",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6175",
  "pr_date": "2025-05-17",
  "timeline_text": "Copy link Collaborator fzyzcjy commented May 10, 2025 Motivation test PYTHONUNBUFFERED=1 SGLANG_TORCH_PROFILER_DIR=/host_home/temp_sglang_server2local python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1 --trust-remote-code --dist-init-addr 192.168.0.55:5757 --nnodes 2 --node-rank ${MY_NODE_RANK} --tp-size ${num_gpu} --dp-size ${num_gpu} --enable-dp-attention --mem-fraction-static 0.8 --chunked-prefill-size $((128*${num_gpu})) --max-running-requests $((${num_gpu}*128)) --context-length 4096 --disable-radix-cache --enable-deepep-moe --deepep-mode low_latency --cuda-graph-bs 128 --decode-log-interval 1\n\npython3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-R1 --base-url http://localhost:30000 --batch-size 16 --input-len 1 --output-len 2048 --skip-warmup baseline: 6 tok/s/gpu PR: 29 tok/s/gpu Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 2 sleepcoo and yuan-luo reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions fzyzcjy added 17 commits May 10, 2025 21:22 more 1208fb1 more 4529cc4 more 92330a2 more ae70984 more 95440df more 5dc985f more 39204c6 more 8189122 more 2970122 more d477d4f more a137cee more 1d2f206 fmt 9ef32b4 more ae6a10d more a8f037d more cec1bf5 more eb97a26 fzyzcjy requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock , ByronHsu , HaiShaw and ch-wan as code owners May 10, 2025 13:57 fmt 51247b8 ch-wan reviewed May 10, 2025 View reviewed changes python/sglang/srt/layers/moe/topk.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Update topk.py 3fecc76 ch-wan approved these changes May 11, 2025 View reviewed changes zhyncs added\n  the high priority label May 11, 2025 fzyzcjy marked this pull request as draft May 12, 2025 00:04 fzyzcjy force-pushed the feat/padding_moe branch\n    from 8797942 to 3fecc76 Compare May 12, 2025 00:09 fzyzcjy marked this pull request as ready for review May 12, 2025 00:09 fzyzcjy and others added 9 commits May 12, 2025 08:09 Merge branch 'main' into feat/padding_moe c3fece0 more 9414109 more ed5c4b5 Merge branch 'feat/padding_moe' of https://github.com/fzyzcjy/sglang \u2026 \u2026 bd315ff \u2026into feat/padding_moe more d885df6 more 8e235e2 more 536b595 fmt 67d963d more ab84bc7 Hide details View details zhyncs merged commit 2716830 into sgl-project : main May 17, 2025 113 of 128 checks passed Uh oh! There was an error while loading. Please reload this page . Copy link Contributor lambert0312 commented May 20, 2025 This pr will significantly reduce DeepSeek's inference performance (15%+). Need to look at the specific reasons. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author fzyzcjy commented May 20, 2025 @lambert0312 Looks bad. Could you please show your commands, and would be great to have a profile. My first guess is that, we need to fuse it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . lambert0312 mentioned this pull request May 21, 2025 Fix topk inference performance reduce #6474 Merged 6 tasks Copy link Contributor lambert0312 commented May 21, 2025 @lambert0312 Looks bad. Could you please show your commands, and would be great to have a profile. My first guess is that, we need to fuse it. @fzyzcjy I tried to modify it. You can see the PR I linked above. Thank you. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author fzyzcjy commented May 21, 2025 Interesting, I thought this line already makes no extra kernels are executed. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . fzyzcjy mentioned this pull request May 27, 2025 Speed up when having padding tokens two-batch overlap #6668 Merged 6 tasks Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Speed up when having padding tokens in DeepEP ( sgl-project#6175 ) 26de0da xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Speed up when having padding tokens in DeepEP ( sgl-project#6175 ) 376abc1 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:37",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-R1"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-R1,trust_remote_code=True --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python3 -m sglang.bench_one_batch_server --model-path /dev/shm/DeepSeek-R1 --base-url http://localhost:30000 --batch-size 16 --input-len 1 --output-len 2048 --skip-warmup",
  "commit_subject": "Speed up when having padding tokens in DeepEP (#6175)",
  "commit_message": "Speed up when having padding tokens in DeepEP (#6175)",
  "commit_date": "2025-05-17T16:44:05-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/topk.py",
    "python/sglang/srt/model_executor/cuda_graph_runner.py",
    "python/sglang/srt/model_executor/forward_batch_info.py",
    "python/sglang/srt/models/deepseek_v2.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 62,
    "num_files": 4,
    "num_hunks": 25,
    "num_non_test_edited_lines": 62,
    "num_non_test_files": 4,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex a7af87144..4c065e4e5 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -31,7 +31,6 @@ if _is_cuda:\n if _is_cuda or _is_hip:\n     from sgl_kernel import topk_softmax\n \n-\n expert_distribution_recorder = ExpertDistributionRecorder()\n \n \n@@ -99,6 +98,7 @@ def grouped_topk(\n     topk_group: int = 0,\n     n_share_experts_fusion: int = 0,\n     routed_scaling_factor: Optional[float] = None,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n ):\n     assert hidden_states.shape[0] == gating_output.shape[0], \"Number of tokens mismatch\"\n \n@@ -138,7 +138,9 @@ def grouped_topk(\n         )\n         topk_weights = topk_weights / topk_weights_sum\n \n-    return topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n+    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n+    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    return topk_weights, topk_ids\n \n \n def biased_grouped_topk_impl(\n@@ -151,6 +153,7 @@ def biased_grouped_topk_impl(\n     topk_group: int = 0,\n     n_share_experts_fusion: int = 0,\n     routed_scaling_factor: Optional[float] = None,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n ):\n     assert hidden_states.shape[0] == gating_output.shape[0], \"Number of tokens mismatch\"\n \n@@ -197,13 +200,25 @@ def biased_grouped_topk_impl(\n         )\n         topk_weights = topk_weights / topk_weights_sum\n \n-    return topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n+    topk_weights, topk_ids = topk_weights.to(torch.float32), topk_ids.to(torch.int32)\n+    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    return topk_weights, topk_ids\n \n \n def is_power_of_two(n):\n     return n > 0 and math.log2(n).is_integer()\n \n \n+def _mask_topk_ids_padded_region(\n+    topk_ids: torch.Tensor,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n+):\n+    if num_token_non_padded is None:\n+        return\n+    indices = torch.arange(0, topk_ids.shape[0], device=topk_ids.device)\n+    topk_ids[indices >= num_token_non_padded, :] = -1\n+\n+\n def biased_grouped_topk(\n     hidden_states: torch.Tensor,\n     gating_output: torch.Tensor,\n@@ -215,6 +230,7 @@ def biased_grouped_topk(\n     compiled: bool = True,\n     n_share_experts_fusion: int = 0,\n     routed_scaling_factor: Optional[float] = None,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n ):\n     assert (\n         routed_scaling_factor is not None\n@@ -226,7 +242,7 @@ def biased_grouped_topk(\n         <= 32  # moe_fused_gate kernel ensure that num_experts/num_expert_group does not exceed MAX_VPT=32 now. And when kernel can handle MAX_VPT > 32, we can remove this assertion.\n         and is_power_of_two(correction_bias.shape[0])\n     ):\n-        return moe_fused_gate(\n+        topk_weights, topk_ids = moe_fused_gate(\n             gating_output,\n             correction_bias,\n             num_expert_group,\n@@ -235,6 +251,11 @@ def biased_grouped_topk(\n             n_share_experts_fusion,\n             routed_scaling_factor,\n         )\n+        # TODO will fuse this into kernel, thus use slow manual operation now\n+        torch.compile(\n+            _mask_topk_ids_padded_region, dynamic=True, backend=get_compiler_backend()\n+        )(topk_ids, num_token_non_padded)\n+        return topk_weights, topk_ids\n     else:\n         biased_grouped_topk_fn = (\n             torch.compile(\n@@ -253,6 +274,7 @@ def biased_grouped_topk(\n             topk_group,\n             n_share_experts_fusion=n_share_experts_fusion,\n             routed_scaling_factor=routed_scaling_factor,\n+            num_token_non_padded=num_token_non_padded,\n         )\n \n \n@@ -268,6 +290,7 @@ def select_experts(\n     correction_bias: Optional[torch.Tensor] = None,\n     torch_native: bool = False,\n     routed_scaling_factor: Optional[float] = None,\n+    num_token_non_padded: Optional[torch.Tensor] = None,\n ):\n     n_share_experts_fusion = global_server_args_dict[\"n_share_experts_fusion\"]\n     # DeepSeek V2/V3/R1 series models use grouped_top_k\n@@ -284,6 +307,7 @@ def select_experts(\n                 topk_group=topk_group,\n                 n_share_experts_fusion=n_share_experts_fusion,\n                 routed_scaling_factor=routed_scaling_factor,\n+                num_token_non_padded=num_token_non_padded,\n             )\n         else:\n             topk_weights, topk_ids = biased_grouped_topk(\n@@ -296,8 +320,12 @@ def select_experts(\n                 topk_group=topk_group,\n                 n_share_experts_fusion=n_share_experts_fusion,\n                 routed_scaling_factor=routed_scaling_factor,\n+                num_token_non_padded=num_token_non_padded,\n             )\n     elif torch_native and custom_routing_function is None:\n+        assert (\n+            num_token_non_padded is None\n+        ), \"num_token_non_padded is not yet supported in fused_topk_native\"\n         topk_weights, topk_ids = fused_topk_native(\n             hidden_states=hidden_states,\n             gating_output=router_logits,\n@@ -305,6 +333,9 @@ def select_experts(\n             renormalize=renormalize,\n         )\n     elif custom_routing_function is None:\n+        assert (\n+            num_token_non_padded is None\n+        ), \"num_token_non_padded is not yet supported in fused_topk\"\n         topk_weights, topk_ids = fused_topk(\n             hidden_states=hidden_states,\n             gating_output=router_logits,\n@@ -312,6 +343,9 @@ def select_experts(\n             renormalize=renormalize,\n         )\n     else:\n+        assert (\n+            num_token_non_padded is None\n+        ), \"num_token_non_padded is not yet supported in custom_routing_function\"\n         topk_weights, topk_ids = custom_routing_function(\n             hidden_states=hidden_states,\n             gating_output=router_logits,\ndiff --git a/python/sglang/srt/model_executor/cuda_graph_runner.py b/python/sglang/srt/model_executor/cuda_graph_runner.py\nindex e88022beb..40f136deb 100644\n--- a/python/sglang/srt/model_executor/cuda_graph_runner.py\n+++ b/python/sglang/srt/model_executor/cuda_graph_runner.py\n@@ -240,6 +240,7 @@ class CudaGraphRunner:\n             self.out_cache_loc = torch.zeros((self.max_num_token,), dtype=torch.int64)\n             self.positions = torch.zeros((self.max_num_token,), dtype=torch.int64)\n             self.mrope_positions = torch.zeros((3, self.max_bs), dtype=torch.int64)\n+            self.num_token_non_padded = torch.zeros((1,), dtype=torch.int32)\n \n             # pipeline parallelism\n             if self.pp_size > 1:\n@@ -403,6 +404,7 @@ class CudaGraphRunner:\n         else:\n             encoder_lens = None\n         mrope_positions = self.mrope_positions[:, :bs]\n+        self.num_token_non_padded[...] = num_tokens\n \n         # pipeline parallelism\n         if self.pp_size > 1:\n@@ -461,6 +463,7 @@ class CudaGraphRunner:\n             spec_info=spec_info,\n             capture_hidden_mode=self.capture_hidden_mode,\n             lora_paths=lora_paths,\n+            num_token_non_padded=self.num_token_non_padded,\n         )\n \n         if lora_paths is not None:\n@@ -556,6 +559,7 @@ class CudaGraphRunner:\n         self.seq_lens[:raw_bs].copy_(forward_batch.seq_lens)\n         self.out_cache_loc[:raw_num_token].copy_(forward_batch.out_cache_loc)\n         self.positions[:raw_num_token].copy_(forward_batch.positions)\n+        self.num_token_non_padded[...] = len(forward_batch.input_ids)\n         if forward_batch.seq_lens_cpu is not None:\n             if bs != raw_bs:\n                 self.seq_lens_cpu.fill_(1)\ndiff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py\nindex 5018f92d5..ea64199a5 100644\n--- a/python/sglang/srt/model_executor/forward_batch_info.py\n+++ b/python/sglang/srt/model_executor/forward_batch_info.py\n@@ -247,6 +247,7 @@ class ForwardBatch:\n \n     # For padding\n     padded_static_len: int = -1  # -1 if not padded\n+    num_token_non_padded: Optional[torch.Tensor] = None  # scalar tensor\n \n     # For Qwen2-VL\n     mrope_positions: torch.Tensor = None\n@@ -290,6 +291,9 @@ class ForwardBatch:\n             capture_hidden_mode=batch.capture_hidden_mode,\n             input_embeds=batch.input_embeds,\n             extend_input_logprob_token_ids_gpu=extend_input_logprob_token_ids_gpu,\n+            num_token_non_padded=torch.tensor(\n+                len(batch.input_ids), dtype=torch.int32\n+            ).to(device, non_blocking=True),\n         )\n \n         # For DP attention\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 436e966db..5955332f5 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -165,7 +165,7 @@ class DeepseekV2MLP(nn.Module):\n             )\n         self.act_fn = SiluAndMul()\n \n-    def forward(self, x, forward_mode: Optional[ForwardMode] = None):\n+    def forward(self, x, forward_batch: Optional[ForwardBatch] = None):\n         gate_up, _ = self.gate_up_proj(x)\n         x = self.act_fn(gate_up)\n         x, _ = self.down_proj(x)\n@@ -287,12 +287,12 @@ class DeepseekV2MoE(nn.Module):\n             )\n \n     def forward(\n-        self, hidden_states: torch.Tensor, forward_mode: Optional[ForwardMode] = None\n+        self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None\n     ) -> torch.Tensor:\n         if not global_server_args_dict[\"enable_deepep_moe\"]:\n             return self.forward_normal(hidden_states)\n         else:\n-            return self.forward_deepep(hidden_states, forward_mode)\n+            return self.forward_deepep(hidden_states, forward_batch)\n \n     def forward_normal(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         shared_output = self._forward_shared_experts(hidden_states)\n@@ -309,8 +309,9 @@ class DeepseekV2MoE(nn.Module):\n         return final_hidden_states\n \n     def forward_deepep(\n-        self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n+        self, hidden_states: torch.Tensor, forward_batch: ForwardBatch\n     ) -> torch.Tensor:\n+        forward_mode = forward_batch.forward_mode\n         shared_output = None\n         if (\n             forward_mode is not None\n@@ -330,6 +331,7 @@ class DeepseekV2MoE(nn.Module):\n                 num_expert_group=self.num_expert_group,\n                 correction_bias=self.correction_bias,\n                 routed_scaling_factor=self.routed_scaling_factor,\n+                num_token_non_padded=forward_batch.num_token_non_padded,\n             )\n         else:\n             topk_idx = torch.full(\n@@ -1339,7 +1341,7 @@ class DeepseekV2DecoderLayer(nn.Module):\n             and (not self.info.is_sparse)\n             and hidden_states.shape[0] == 0\n         ):\n-            hidden_states = self.mlp(hidden_states, forward_batch.forward_mode)\n+            hidden_states = self.mlp(hidden_states, forward_batch)\n \n         if self.is_last_layer and self.attn_tp_size != 1:\n             hidden_states += residual",
  "apis": [
    "sglang.srt.layers.moe.topk.grouped_topk",
    "sglang.srt.layers.moe.topk.biased_grouped_topk",
    "sglang.srt.layers.moe.topk.select_experts",
    "sglang.srt.models.deepseek_v2.DeepseekV2MoE.forward",
    "sglang.srt.models.deepseek_v2.DeepseekV2MLP.forward"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/models/deepseek_v2.py",
    "/path/to/repos/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py",
    "/path/to/repos/sglang/python/sglang/srt/model_executor/forward_batch_info.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit adds a new parameter (num_token_non_padded) and integrates a masking function to bypass computations on padded tokens. Multiple core functions and APIs in the production code (non-test files) are modified, leading to a speedup when processing padded inputs. The changes are not just naming a function \u201coptimize\u201d or simple refactoring, but actually modify top-level APIs (e.g., topk selection in MoE layers and parts of the forward passes) which should improve the runtime performance. Therefore, the commit qualifies as a performance optimization, affecting computations on the CPU and testable without GPU-specific features.",
  "llm_api_reason": "This commit improves how padded tokens are handled during expert selection in the MoE (Mixture of Experts) routing. In the topk module, an optional argument \u201cnum_token_non_padded\u201d is added to functions such as grouped_topk, biased_grouped_topk_impl, biased_grouped_topk and select_experts, and a helper (_mask_topk_ids_padded_region) is introduced to mask out padded regions in the topk IDs. Moreover, the forward methods of DeepseekV2MLP and DeepseekV2MoE have been updated to accept a ForwardBatch (which carries padding information) rather than a simple forward_mode, ensuring that the padded tokens are taken into account during inference. These changes help speed up computations when there are padding tokens in the input."
}