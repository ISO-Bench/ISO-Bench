{
  "commit_hash": "2f42749184ca3679d2bb0361903f46632408f9a2",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6474",
  "pr_date": "2025-05-23",
  "timeline_text": "Copy link Contributor lambert0312 commented May 21, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation When the following logic is added to topk.py , the inference performance will be significantly affected: sglang/python/sglang/srt/layers/moe/topk.py Lines 267 to 269\n      in 6632489 torch . compile ( _mask_topk_ids_padded_region , dynamic = True , backend = get_compiler_backend () )( topk_ids , num_token_non_padded ) Run command: python3 -m sglang.launch_server --model-path /path/to/DeepSeek-V3-0324 --trust-remote-code --host 0.0.0.0 --port 30000 --attention-backend flashinfer --n-share-experts-fusion 16 --tp 16 --dist-init-addr IP:20000 --nnodes 2 --node-rank 0 Ref: #6175 Modifications Add num_token_non_padded judgment logic. If it is None, directly return the previous result. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 2 zhyncs and fzyzcjy reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions fix topk inference performance reduce fda3745 lambert0312 requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw , ch-wan and BBuf as code owners May 21, 2025 00:47 Merge branch 'main' into fix_topk_inference_performance_reduce eaa2c42 zhyncs approved these changes May 23, 2025 View reviewed changes Hide details View details zhyncs merged commit 2f42749 into sgl-project : main May 23, 2025 0 of 37 checks passed Uh oh! There was an error while loading. Please reload this page . Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Fix topk inference performance reduce ( sgl-project#6474 ) a4adace xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Fix topk inference performance reduce ( sgl-project#6474 ) d278b57 lambert0312 deleted the fix_topk_inference_performance_reduce branch June 20, 2025 07:14 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:30",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-V3,trust_remote_code=True,tensor_parallel_size=16 --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3 --tp 16",
  "commit_subject": "Fix topk inference performance reduce (#6474)",
  "commit_message": "Fix topk inference performance reduce (#6474)",
  "commit_date": "2025-05-23T02:58:31-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/topk.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 2,
    "num_files": 1,
    "num_hunks": 1,
    "num_non_test_edited_lines": 2,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex 8895e6be6..9d3ae3947 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -264,6 +264,8 @@ def biased_grouped_topk(\n         # TODO merge into kernel for this branch\n         topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n         # TODO will fuse this into kernel, thus use slow manual operation now\n+        if num_token_non_padded is None:\n+            return topk_weights, topk_ids\n         torch.compile(\n             _mask_topk_ids_padded_region, dynamic=True, backend=get_compiler_backend()\n         )(topk_ids, num_token_non_padded)",
  "apis": [
    "sglang.srt.layers.moe.topk.biased_grouped_topk",
    "sglang.srt.layers.moe.topk.select_experts"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/topk.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/examples/runtime/engine/fastapi_engine_inference.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit patch modifies a non-test file (python/sglang/srt/layers/moe/topk.py) and introduces an early return based on a condition (if num_token_non_padded is None). This change alters the control flow for topk inference, bypassing a call to torch.compile that would otherwise be executed. Given the commit message (\u201cFix topk inference performance reduce\u201d), the modification aims at improving performance by avoiding unnecessary computation when certain conditions are met. Although the change is minimal, it directly targets the performance of the inference path in a high-level API. Furthermore, the change is CPU-focused and not simply a bug fix, simple refactoring, or a non-performance feature addition. Therefore, this commit satisfies the conditions for being performance related.",
  "llm_api_reason": "The commit updates the biased_grouped_topk function in topk.py by short-circuiting extra processing (i.e. avoiding the overhead of a torch.compile call) when num_token_non_padded is None. This change impacts the behavior of the biased_grouped_topk function, which is used internally by higher-level routines such as select_experts (and indirectly by TopK\u2019s forward methods). Thus, these functions that form part of the public Python API for routing/expert selection in SGLang are affected."
}