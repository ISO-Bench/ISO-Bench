{
  "commit_hash": "f0653886a5e0fc6a92c879b68ff1cfb30941dd10",
  "pr_url": "https://github.com/sgl-project/sglang/pull/4957",
  "pr_date": "2025-05-20",
  "timeline_text": "Copy link Collaborator fzyzcjy commented Apr 1, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation For EPLB, and also for debugging/knowing details dep: #5219 NOTE: There are enhancements to this, but it currently in branch #5295 and not yet extracted to here. Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\ude80 2 ZJLi2013 and slin1237 reacted with rocket emoji All reactions \ud83d\ude80 2 reactions fzyzcjy added 30 commits April 1, 2025 09:07 more 634f28f more a495780 more 03ae2ed more 7ece402 more 057d740 more e97bd89 more c0d97d6 more bd03337 more de8f68e more a81db0d more 2b7cc46 more cbeae3a more ab7eeef more 3b7b887 more 56f5e09 more d1716ae more 942b8e2 more 1658696 more 6b595d3 more b84e2ca more 29b8f4a more 7a5f544 more de3d02a more f806ac8 more b18639c more c712bbd more 05305b2 more 14fdd55 more b392cb9 more 594b751 247 hidden items Load more\u2026 fzyzcjy and others added 2 commits May 18, 2025 10:43 Merge branch 'main' into feat/expert_distribution_recorder be06021 more ec461ea This was referenced May 18, 2025 Support exporting per-pass expert distribution statistics #6384 Closed Support dispatching logical to physical experts #6385 Merged Support loading weights when physical experts are different from logical experts #6386 Merged Support DeepSeek EPLB algorithm with static distributions #6387 Merged Merge branch 'main' into feat/expert_distribution_recorder f91c9fe fzyzcjy mentioned this pull request May 18, 2025 Support updating expert locations dynamically #6388 Merged 6 tasks Merge branch 'main' into feat/expert_distribution_recorder f5a3908 zhyncs added\n  the high priority label May 18, 2025 fzyzcjy and others added 10 commits May 19, 2025 07:44 Merge branch 'main' into feat/expert_distribution_recorder 6274cd2 Merge branch 'main' into feat/expert_distribution_recorder 5633a60 more 3eef133 Merge branch 'feat/expert_distribution_recorder' of https://github.co\u2026 \u2026 9c6732e \u2026m/fzyzcjy/sglang into feat/expert_distribution_recorder Merge branch 'main' into feat/expert_distribution_recorder 1c1dcdb more 1e32bc8 Merge branch 'feat/expert_distribution_recorder' of https://github.co\u2026 \u2026 a460b75 \u2026m/fzyzcjy/sglang into feat/expert_distribution_recorder Merge branch 'main' into feat/expert_distribution_recorder d04a197 Merge branch 'main' into feat/expert_distribution_recorder 686475f ci 4f15cc5 fzyzcjy mentioned this pull request May 20, 2025 [bugfix] miss forward_batch.forward_mode when refactor deepseek_v2.py #6425 Closed 6 tasks Copy link Collaborator Author fzyzcjy commented May 20, 2025 \u279c  misc (cd /host_home/primary_synced/sglang && SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK=1 python3 test/srt/test_full_deepseek_v3.py)\n\u279c  misc (cd /host_home/primary_synced/sglang && SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK=1 python3 test/srt/test_disaggregation_different_tp.py) All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'main' into feat/expert_distribution_recorder f7905e5 Hide details View details zhyncs merged commit f065388 into sgl-project : main May 20, 2025 1 of 42 checks passed Uh oh! There was an error while loading. Please reload this page . Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Expert distribution recording without overhead for EPLB ( sgl-project#\u2026 \u2026 03a0765 \u20264957 ) xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Expert distribution recording without overhead for EPLB ( sgl-project#\u2026 \u2026 f9b978b \u20264957 ) Yuechguo pushed a commit\n        to Yuechguo/sglang\n      that referenced\n      this pull request Jul 29, 2025 Expert distribution recording without overhead for EPLB ( sgl-project#\u2026 \u2026 ac5c89d \u20264957 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:34",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3",
    "deepseek-ai/DeepSeek-V2"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-V3 --tasks gsm8k --batch_size 8"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-V3 --dataset-name random --num-prompts 100",
  "commit_subject": "Expert distribution recording without overhead for EPLB (#4957)",
  "commit_message": "Expert distribution recording without overhead for EPLB (#4957)",
  "commit_date": "2025-05-19T20:07:43-07:00",
  "files_changed": [
    "docs/backend/native_api.ipynb",
    "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
    "python/sglang/srt/layers/moe/topk.py",
    "python/sglang/srt/managers/expert_distribution.py",
    "python/sglang/srt/managers/expert_location.py",
    "python/sglang/srt/managers/scheduler.py",
    "python/sglang/srt/model_executor/model_runner.py",
    "python/sglang/srt/models/deepseek_v2.py",
    "python/sglang/srt/models/qwen2_moe.py",
    "python/sglang/srt/server_args.py",
    "python/sglang/srt/utils.py",
    "test/srt/test_expert_distribution.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 1309,
    "num_files": 12,
    "num_hunks": 33,
    "num_non_test_edited_lines": 1309,
    "num_non_test_files": 12,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/docs/backend/native_api.ipynb b/docs/backend/native_api.ipynb\nindex 04b8ec0ed..189c678c0 100644\n--- a/docs/backend/native_api.ipynb\n+++ b/docs/backend/native_api.ipynb\n@@ -390,7 +390,7 @@\n    \"outputs\": [],\n    \"source\": [\n     \"expert_record_server_process, port = launch_server_cmd(\\n\",\n-    \"    \\\"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0\\\"\\n\",\n+    \"    \\\"python3 -m sglang.launch_server --model-path Qwen/Qwen1.5-MoE-A2.7B --host 0.0.0.0 --expert-distribution-recorder-mode stat\\\"\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"wait_for_server(f\\\"http://localhost:{port}\\\")\"\n@@ -415,19 +415,7 @@\n     \"print_highlight(response)\\n\",\n     \"\\n\",\n     \"response = requests.post(f\\\"http://localhost:{port}/dump_expert_distribution_record\\\")\\n\",\n-    \"print_highlight(response)\\n\",\n-    \"\\n\",\n-    \"import glob\\n\",\n-    \"\\n\",\n-    \"output_file = glob.glob(\\\"expert_distribution_*.csv\\\")[0]\\n\",\n-    \"with open(output_file, \\\"r\\\") as f:\\n\",\n-    \"    print_highlight(\\\"\\\\n| Layer ID | Expert ID | Count |\\\")\\n\",\n-    \"    print_highlight(\\\"|----------|-----------|--------|\\\")\\n\",\n-    \"    next(f)\\n\",\n-    \"    for i, line in enumerate(f):\\n\",\n-    \"        if i < 9:\\n\",\n-    \"            layer_id, expert_id, count = line.strip().split(\\\",\\\")\\n\",\n-    \"            print_highlight(f\\\"| {layer_id:8} | {expert_id:9} | {count:6} |\\\")\"\n+    \"print_highlight(response)\"\n    ]\n   },\n   {\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 4d165dbd2..b647f456b 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -1,6 +1,9 @@\n import logging\n \n from sglang.srt.layers.quantization.deep_gemm import _ENABLE_JIT_DEEPGEMM\n+from sglang.srt.managers.expert_distribution import (\n+    get_global_expert_distribution_recorder,\n+)\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.utils import DeepEPMode, load_json_config\n \n@@ -326,6 +329,13 @@ class _DeepEPDispatcherImplNormal(_DeepEPDispatcherImplBase):\n             config=_DeepEPConfig.get_instance().normal_dispatch_config,\n         )\n \n+        get_global_expert_distribution_recorder().on_deepep_dispatch_normal(\n+            num_recv_tokens_per_expert_list,\n+            num_tokens_per_rank=num_tokens_per_rank,\n+            num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,\n+            num_tokens_per_expert=num_tokens_per_expert,\n+        )\n+\n         return (\n             recv_x,\n             recv_topk_idx,\n@@ -489,6 +499,10 @@ class _DeepEPDispatcherImplLowLatency(_DeepEPDispatcherImplBase):\n     ):\n         hook() if self.return_recv_hook else event.current_stream_wait()\n \n+        get_global_expert_distribution_recorder().on_deepep_dispatch_low_latency(\n+            masked_m\n+        )\n+\n         reorder_topk_ids = seg_indptr = None\n \n         return (\ndiff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex 4c065e4e5..075587dc0 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -18,7 +18,10 @@ from typing import Callable, Optional\n import torch\n import torch.nn.functional as F\n \n-from sglang.srt.managers.expert_distribution import ExpertDistributionRecorder\n+from sglang.srt.managers.expert_distribution import (\n+    ExpertDistributionRecorder,\n+    get_global_expert_distribution_recorder,\n+)\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.utils import get_compiler_backend, is_cuda, is_hip\n \n@@ -31,8 +34,6 @@ if _is_cuda:\n if _is_cuda or _is_hip:\n     from sgl_kernel import topk_softmax\n \n-expert_distribution_recorder = ExpertDistributionRecorder()\n-\n \n def fused_topk_native(\n     hidden_states: torch.Tensor,\n@@ -353,6 +354,6 @@ def select_experts(\n             renormalize=renormalize,\n         )\n \n-    expert_distribution_recorder.record_new_token(topk_ids)\n+    get_global_expert_distribution_recorder().on_select_experts(topk_ids=topk_ids)\n \n     return topk_weights, topk_ids\ndiff --git a/python/sglang/srt/managers/expert_distribution.py b/python/sglang/srt/managers/expert_distribution.py\nindex 226256ed2..c32cafbb8 100644\n--- a/python/sglang/srt/managers/expert_distribution.py\n+++ b/python/sglang/srt/managers/expert_distribution.py\n@@ -1,81 +1,620 @@\n-import json\n+# Copyright 2023-2024 SGLang Team\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n import logging\n+import os\n import time\n-from collections import defaultdict\n-from typing import Dict, List, Tuple\n+from abc import ABC\n+from contextlib import contextmanager\n+from pathlib import Path\n+from typing import Dict, List, Literal, Optional, Tuple, Type\n \n import torch\n+import torch.distributed\n+\n+from sglang.srt.managers.expert_location import ExpertLocationMetadata\n+from sglang.srt.model_executor.forward_batch_info import ForwardBatch\n+from sglang.srt.server_args import ServerArgs\n+from sglang.srt.utils import Withable\n \n logger = logging.getLogger(__name__)\n \n+# --------------------------------------- Entrypoint -----------------------------------------\n+\n+_OutputMode = Literal[\"file\", \"object\"]\n+\n+\n+class ExpertDistributionRecorder(ABC):\n+    \"\"\"Global expert distribution recording\"\"\"\n+\n+    @staticmethod\n+    def init_new(\n+        server_args: ServerArgs,\n+        expert_location_metadata: \"ExpertLocationMetadata\",\n+        rank: int,\n+    ):\n+        if server_args.expert_distribution_recorder_mode is not None:\n+            return _ExpertDistributionRecorderReal(\n+                server_args, expert_location_metadata, rank\n+            )\n+        else:\n+            return _ExpertDistributionRecorderNoop()\n+\n+    @contextmanager\n+    def with_current_layer(self, layer_idx):\n+        yield\n+\n+    @contextmanager\n+    def with_debug_name(self, debug_name):\n+        yield\n+\n+    @contextmanager\n+    def with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch):\n+        yield\n+\n+    def on_select_experts(self, topk_ids: torch.Tensor):\n+        pass\n+\n+    def on_deepep_dispatch_normal(\n+        self,\n+        local_physical_count_of_layer: List[int],\n+        num_tokens_per_rank,\n+        num_tokens_per_rdma_rank,\n+        num_tokens_per_expert,\n+    ):\n+        pass\n+\n+    def on_deepep_dispatch_low_latency(\n+        self, local_physical_count_of_layer: torch.Tensor\n+    ):\n+        pass\n+\n+    def start_record(self):\n+        self._on_not_implemented()\n+\n+    def stop_record(self):\n+        self._on_not_implemented()\n+\n+    def dump_record(self, output_mode: _OutputMode = \"file\"):\n+        self._on_not_implemented()\n+\n+    def _on_not_implemented(self):\n+        raise Exception(\n+            \"Please set ServerArgs.expert_distribution_recorder_mode to use ExpertDistributionRecorder.\"\n+        )\n+\n+\n+class _ExpertDistributionRecorderNoop(ExpertDistributionRecorder):\n+    pass\n \n-# global expert distribution recording\n-class ExpertDistributionRecorder:\n-    # This class is a singleton class\n-    def __new__(cls):\n-        if not hasattr(cls, \"instance\"):\n-            cls.instance = super(ExpertDistributionRecorder, cls).__new__(cls)\n-        return cls.instance\n \n-    def __init__(self):\n-        # the length of the dictionary is the number of layers\n-        # the length of the list is the number of tokens\n-        # the length of the tuple is topk's k value\n-        self._expert_distribution_record: Dict[int, List[Tuple[int]]] = defaultdict(\n-            list\n+class _ExpertDistributionRecorderReal(ExpertDistributionRecorder):\n+    def __init__(\n+        self,\n+        server_args: ServerArgs,\n+        expert_location_metadata: \"ExpertLocationMetadata\",\n+        rank: int,\n+    ):\n+        self._server_args = server_args\n+        self._expert_location_metadata = expert_location_metadata\n+\n+        self._recording = False\n+        self._current_forward_pass_id = Withable()\n+        self._current_layer_idx = Withable()\n+        self._current_debug_name = Withable()\n+        self._accumulator = _Accumulator.init_new(\n+            server_args, expert_location_metadata, rank\n         )\n-        self._record = False\n-        self._current_layer_id = \"UNKNOWN\"\n+        self._single_pass_gatherers = {\n+            k: _SinglePassGatherer.init_new(server_args, expert_location_metadata, rank)\n+            for k in self._accumulator.get_single_pass_gatherer_keys()\n+        }\n+\n+    def with_current_layer(self, layer_idx):\n+        return self._current_layer_idx.with_value(layer_idx)\n \n-    def set_current_layer(self, layer_idx):\n-        self._current_layer_id = layer_idx\n+    def with_debug_name(self, debug_name):\n+        return self._current_debug_name.with_value(debug_name)\n \n-    def record_new_token(self, topk_ids):\n-        if not self._record:\n+    @contextmanager\n+    def with_forward_pass(self, forward_pass_id: int, forward_batch: ForwardBatch):\n+        with self._current_forward_pass_id.with_value(forward_pass_id):\n+            self._on_forward_pass_start(forward_batch)\n+            try:\n+                yield\n+            finally:\n+                self._on_forward_pass_end(forward_pass_id)\n+\n+    def _on_forward_pass_start(self, forward_batch: ForwardBatch):\n+        if not self._recording:\n             return\n-        topk_ids_list = topk_ids.to(\"cpu\", non_blocking=True).numpy().tolist()\n-        torch.cuda.synchronize()\n-        for i in topk_ids_list:\n-            self._expert_distribution_record[self._current_layer_id].append(tuple(i))\n+        for gatherer_key, gatherer in self._single_pass_gatherers.items():\n+            gatherer.reset()\n+            gatherer.on_forward_pass_start(forward_batch)\n \n-    def reset(self):\n+    def _on_forward_pass_end(self, forward_pass_id: int):\n+        if not self._recording:\n+            return\n+        for gatherer_key, gatherer in self._single_pass_gatherers.items():\n+            single_pass_data = gatherer.collect()\n+            self._accumulator.append(forward_pass_id, gatherer_key, single_pass_data)\n+\n+    def on_select_experts(self, topk_ids: torch.Tensor):\n+        self._on_hook(\"on_select_experts\", topk_ids=topk_ids)\n+\n+    def on_deepep_dispatch_normal(\n+        self,\n+        local_physical_count_of_layer: List[int],\n+        num_tokens_per_rank,\n+        num_tokens_per_rdma_rank,\n+        num_tokens_per_expert,\n+    ):\n+        self._on_hook(\n+            \"on_deepep_dispatch_normal\",\n+            local_physical_count_of_layer=local_physical_count_of_layer,\n+            num_tokens_per_rank=num_tokens_per_rank,\n+            num_tokens_per_rdma_rank=num_tokens_per_rdma_rank,\n+            num_tokens_per_expert=num_tokens_per_expert,\n+        )\n+\n+    def on_deepep_dispatch_low_latency(\n+        self, local_physical_count_of_layer: torch.Tensor\n+    ):\n+        self._on_hook(\n+            \"on_deepep_dispatch_low_latency\",\n+            local_physical_count_of_layer=local_physical_count_of_layer,\n+        )\n+\n+    def _on_hook(self, hook_name: str, **kwargs):\n+        if not (self._recording or torch.cuda.is_current_stream_capturing()):\n+            return\n+        gatherer = self._single_pass_gatherers[\n+            self._accumulator.get_single_pass_gatherer_key(\n+                self._current_debug_name.value\n+            )\n+        ]\n+        getattr(gatherer, hook_name)(layer_idx=self._current_layer_idx.value, **kwargs)\n+\n+    def _reset(self):\n         \"\"\"Reset the expert distribution recorder.\"\"\"\n-        logger.info(\"Resetting expert distribution record...\")\n-        self._record = False\n-        self._expert_distribution_record.clear()\n-        self._current_layer_id = \"UNKNOWN\"\n+        logger.info(\"Resetting ExpertDistributionRecorder...\")\n+        assert (\n+            self._current_layer_idx.value is None\n+        ), f\"{self._current_layer_idx.value=}\"\n+        for gatherer in self._single_pass_gatherers.values():\n+            gatherer.reset()\n+        self._accumulator.reset()\n \n     def start_record(self):\n-        \"\"\"Start recording the expert distribution. Reset the recorder and set the recording flag to True.\"\"\"\n-        if self._record == True:\n+        \"\"\"Start recording the expert distribution.\"\"\"\n+        if self._recording:\n             logger.warning(\n                 \"SGLang server is already recording expert ids. Did you forget to dump the expert ids recorded so far by sending requests to the `/stop_expert_distribution_record` and `/dump_expert_distribution_record` endpoints?\"\n             )\n-        self.reset()\n-        self._record = True\n+        self._reset()\n+        self._recording = True\n \n     def stop_record(self):\n-        \"\"\"Stop recording the expert distribution. Set the recording flag to False.\"\"\"\n-        if self._record == False:\n+        \"\"\"Stop recording the expert distribution.\"\"\"\n+        if not self._recording:\n             logger.warning(\n                 \"SGLang server has not been recording expert ids. Did you forget to start recording by sending request to the `/start_expert_distribution_record` endpoint?\"\n             )\n-        self._record = False\n-\n-    def dump_record(self):\n-        \"\"\"Dump the expert distribution record to a file. Reset the recorder after dumping.\"\"\"\n-        results = {}\n-        for layer_idx, layer_record in self._expert_distribution_record.items():\n-            results[layer_idx] = defaultdict(int)\n-            for token_record in layer_record:\n-                for expert_idx in token_record:\n-                    results[layer_idx][expert_idx] += 1\n-        with open(\n-            f\"expert_distribution_rank{torch.distributed.get_rank()}_timestamp{time.time()}.csv\",\n-            \"w\",\n-        ) as fd:\n-            fd.write(\"layer_id,expert_id,count\\n\")\n-            for layer_idx, layer_results in results.items():\n-                for expert_idx, count in layer_results.items():\n-                    fd.write(f\"{layer_idx},{expert_idx},{count}\\n\")\n-        self.reset()\n+        self._recording = False\n+\n+    def dump_record(self, output_mode: _OutputMode = \"file\"):\n+        \"\"\"Dump the expert distribution record and reset the recorder after dumping.\"\"\"\n+        output = self._accumulator.dump(output_mode=output_mode)\n+        self._reset()\n+        return output\n+\n+\n+_global_expert_distribution_recorder: Optional[ExpertDistributionRecorder] = (\n+    _ExpertDistributionRecorderNoop()\n+)\n+\n+\n+def get_global_expert_distribution_recorder():\n+    return _global_expert_distribution_recorder\n+\n+\n+def set_global_expert_distribution_recorder(value):\n+    global _global_expert_distribution_recorder\n+    _global_expert_distribution_recorder = value\n+\n+\n+# --------------------------------------- SinglePassGatherer -----------------------------------------\n+\n+\n+class _SinglePassGatherer(ABC):\n+    @staticmethod\n+    def init_new(\n+        server_args: ServerArgs,\n+        expert_location_metadata: \"ExpertLocationMetadata\",\n+        rank: int,\n+    ) -> \"_SinglePassGatherer\":\n+        if server_args.expert_distribution_recorder_mode == \"per_token\":\n+            return _DetailSinglePassGatherer(\n+                server_args, expert_location_metadata, rank\n+            )\n+        if server_args.enable_deepep_moe:\n+            if server_args.deepep_mode == \"normal\":\n+                return _DeepepNormalSinglePassGatherer(expert_location_metadata, rank)\n+            elif server_args.deepep_mode == \"low_latency\":\n+                return _DeepepLowLatencySinglePassGatherer(\n+                    expert_location_metadata, rank\n+                )\n+            else:\n+                raise NotImplementedError\n+        return _SelectExpertsSinglePassGatherer(expert_location_metadata, rank)\n+\n+    def __init__(self, expert_location_metadata: \"ExpertLocationMetadata\", rank: int):\n+        self._expert_location_metadata = expert_location_metadata\n+        self._rank = rank\n+\n+    def on_forward_pass_start(self, forward_batch: ForwardBatch):\n+        pass\n+\n+    def on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor):\n+        pass\n+\n+    def on_deepep_dispatch_normal(\n+        self,\n+        layer_idx: int,\n+        local_physical_count_of_layer: List[int],\n+        num_tokens_per_rank,\n+        num_tokens_per_rdma_rank,\n+        num_tokens_per_expert,\n+    ):\n+        pass\n+\n+    def on_deepep_dispatch_low_latency(\n+        self, layer_idx: int, local_physical_count_of_layer: torch.Tensor\n+    ):\n+        pass\n+\n+    def reset(self):\n+        raise NotImplementedError\n+\n+    def collect(self) -> Dict:\n+        raise NotImplementedError\n+\n+\n+class _LayerBasedSinglePassGatherer(_SinglePassGatherer):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._objects_of_layer = {}\n+\n+    def _on_layer_data(self, layer_idx: int, objects: List[int]):\n+        assert 0 <= layer_idx < self._expert_location_metadata.num_layers\n+        if layer_idx in self._objects_of_layer:\n+            self._objects_of_layer[layer_idx] = _list_sum(\n+                self._objects_of_layer[layer_idx], objects\n+            )\n+        else:\n+            self._objects_of_layer[layer_idx] = objects\n+\n+    def reset(self):\n+        self._objects_of_layer.clear()\n+\n+    def _collect_objects(self, pad_len: int) -> torch.Tensor:\n+        data = [\n+            self._objects_of_layer.get(layer_index) or ([0] * pad_len)\n+            for layer_index in range(self._expert_location_metadata.num_layers)\n+        ]\n+        return torch.tensor(data)\n+\n+\n+def _list_sum(a: List, b: List) -> List:\n+    return [x + y for x, y in zip(a, b, strict=True)]\n+\n+\n+class _SelectExpertsSinglePassGatherer(_LayerBasedSinglePassGatherer):\n+    # pretty slow, but we will use the DeepEP Gatherer in production\n+    def on_select_experts(self, layer_idx: int, topk_ids: torch.Tensor):\n+        topk_ids_list = topk_ids.to(\"cpu\", non_blocking=True).numpy().tolist()\n+        torch.cuda.synchronize()\n+\n+        global_physical_count = [\n+            0\n+        ] * self._expert_location_metadata.num_physical_experts\n+        for token_record in topk_ids_list:\n+            for global_physical_expert_idx in token_record:\n+                global_physical_count[global_physical_expert_idx] += 1\n+\n+        self._on_layer_data(layer_idx, global_physical_count)\n+\n+    def collect(self) -> Dict:\n+        global_physical_count = super()._collect_objects(\n+            pad_len=self._expert_location_metadata.num_physical_experts\n+        )\n+        return dict(global_physical_count=global_physical_count)\n+\n+\n+class _DeepepNormalSinglePassGatherer(_LayerBasedSinglePassGatherer):\n+    def on_deepep_dispatch_normal(\n+        self,\n+        layer_idx: int,\n+        local_physical_count_of_layer: List[int],\n+        num_tokens_per_rank,\n+        num_tokens_per_rdma_rank,\n+        num_tokens_per_expert,\n+    ):\n+        assert isinstance(local_physical_count_of_layer, list)\n+        self._on_layer_data(layer_idx, local_physical_count_of_layer)\n+\n+    def collect(self) -> Dict:\n+        local_physical_count = super()._collect_objects(\n+            pad_len=self._expert_location_metadata.num_local_physical_experts\n+        )\n+        global_physical_count = _convert_local_to_global_physical_count(\n+            local_physical_count,\n+            rank=self._rank,\n+            num_local_physical_experts=self._expert_location_metadata.num_local_physical_experts,\n+            num_physical_experts=self._expert_location_metadata.num_physical_experts,\n+        )\n+        return dict(global_physical_count=global_physical_count)\n+\n+\n+class _DeepepLowLatencySinglePassGatherer(_SinglePassGatherer):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._data = torch.zeros(\n+            (\n+                self._expert_location_metadata.num_layers,\n+                self._expert_location_metadata.num_local_physical_experts,\n+            ),\n+            dtype=torch.int,\n+            device=\"cuda\",\n+        )\n+\n+    def on_deepep_dispatch_low_latency(\n+        self, layer_idx: int, local_physical_count_of_layer: torch.Tensor\n+    ):\n+        # Most naive implementation, can optimize later\n+        self._data[layer_idx, :] += local_physical_count_of_layer\n+\n+    def reset(self):\n+        self._data[...] = 0\n+\n+    def collect(self) -> Dict:\n+        # Can optimize if bottleneck\n+        global_physical_count = _convert_local_to_global_physical_count(\n+            self._data,\n+            rank=self._rank,\n+            num_local_physical_experts=self._expert_location_metadata.num_local_physical_experts,\n+            num_physical_experts=self._expert_location_metadata.num_physical_experts,\n+        )\n+        return dict(global_physical_count=global_physical_count)\n+\n+\n+def _convert_local_to_global_physical_count(\n+    local_physical_count: torch.Tensor,\n+    rank: int,\n+    num_local_physical_experts: int,\n+    num_physical_experts: int,\n+) -> torch.Tensor:\n+    dtype = local_physical_count.dtype\n+    device = local_physical_count.device\n+    num_layers, _ = local_physical_count.shape\n+\n+    ans = torch.zeros((num_layers, num_physical_experts), dtype=dtype, device=device)\n+    ans[\n+        :, num_local_physical_experts * rank : num_local_physical_experts * (rank + 1)\n+    ] = local_physical_count\n+    return ans\n+\n+\n+# --------------------------------------- Accumulator -----------------------------------------\n+\n+_SINGLE_PASS_GATHERER_KEY_PRIMARY = \"primary\"\n+\n+\n+class _Accumulator(ABC):\n+    @staticmethod\n+    def init_new(\n+        server_args: ServerArgs,\n+        expert_location_metadata: \"ExpertLocationMetadata\",\n+        rank: int,\n+    ) -> \"_Accumulator\":\n+        return _Accumulator.get_class(server_args)(\n+            server_args, expert_location_metadata, rank\n+        )\n+\n+    @staticmethod\n+    def get_class(server_args: ServerArgs) -> Type[\"_Accumulator\"]:\n+        return {\n+            \"stat\": _StatAccumulator,\n+            # TODO pr-chain: enable this later\n+            # \"per_pass\": _DetailAccumulator,\n+            # \"per_token\": _DetailAccumulator,\n+        }[server_args.expert_distribution_recorder_mode]\n+\n+    def __init__(\n+        self,\n+        server_args: ServerArgs,\n+        expert_location_metadata: \"ExpertLocationMetadata\",\n+        rank: int,\n+    ):\n+        self._server_args = server_args\n+        self._expert_location_metadata = expert_location_metadata\n+        self._rank = rank\n+\n+    def get_single_pass_gatherer_keys(self):\n+        return [_SINGLE_PASS_GATHERER_KEY_PRIMARY]\n+\n+    def get_single_pass_gatherer_key(self, debug_name: Optional[str]):\n+        return _SINGLE_PASS_GATHERER_KEY_PRIMARY\n+\n+    def append(\n+        self,\n+        forward_pass_id: int,\n+        gatherer_key: str,\n+        single_pass_data: Dict,\n+    ):\n+        pass\n+\n+    def reset(self):\n+        pass\n+\n+    def dump(self, output_mode: _OutputMode):\n+        pass\n+\n+\n+class _StatAccumulator(_Accumulator):\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._global_physical_count_of_buffered_step = _Buffer.init_new(\n+            item_shape=(\n+                self._expert_location_metadata.num_layers,\n+                # Cannot use local_physical_count to support select_experts\n+                self._expert_location_metadata.num_physical_experts,\n+            ),\n+            buffer_size=self._server_args.expert_distribution_recorder_buffer_size,\n+            dtype=torch.int32,\n+            device=self._server_args.device,\n+        )\n+\n+    def append(\n+        self,\n+        forward_pass_id: int,\n+        gatherer_key: str,\n+        single_pass_data: Dict,\n+    ):\n+        super().append(forward_pass_id, gatherer_key, single_pass_data)\n+        # Can optimize if overhead here is large\n+        self._global_physical_count_of_buffered_step.append(\n+            single_pass_data[\"global_physical_count\"]\n+        )\n+\n+    def reset(self):\n+        super().reset()\n+        self._global_physical_count_of_buffered_step.reset()\n+\n+    def dump(self, output_mode: _OutputMode):\n+        logical_count_of_buffered_step = _convert_global_physical_count_to_logical_count(\n+            self._global_physical_count_of_buffered_step.get_all(),\n+            num_layers=self._expert_location_metadata.num_layers,\n+            num_logical_experts=self._expert_location_metadata.num_logical_experts,\n+            physical_to_logical_map=self._expert_location_metadata.physical_to_logical_map,\n+        )\n+        torch.distributed.all_reduce(\n+            logical_count_of_buffered_step, op=torch.distributed.ReduceOp.SUM\n+        )\n+        output = dict(\n+            rank=self._rank,\n+            logical_count=logical_count_of_buffered_step,\n+        )\n+\n+        if output_mode == \"file\":\n+            if self._rank == 0:\n+                _dump_to_file(f\"expert_distribution_recorder_{time.time()}.pt\", output)\n+        elif output_mode == \"object\":\n+            return output\n+        else:\n+            raise NotImplementedError\n+\n+\n+def _dump_to_file(name, data):\n+    save_dir = Path(os.environ.get(\"SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR\", \"/tmp\"))\n+    path_output = save_dir / name\n+    logger.info(f\"Write expert distribution to {path_output}\")\n+    if not save_dir.exists():\n+        save_dir.mkdir(parents=True, exist_ok=True)\n+    torch.save(data, str(path_output))\n+\n+\n+class _Buffer:\n+    @staticmethod\n+    def init_new(item_shape: Tuple, buffer_size: int, dtype, device):\n+        if buffer_size < 0:\n+            return _InfiniteBuffer(item_shape, dtype=dtype, device=device)\n+        else:\n+            return _CircularBuffer(item_shape, buffer_size, dtype=dtype, device=device)\n+\n+    def append(self, value: torch.Tensor):\n+        raise NotImplementedError\n+\n+    def get_all(self) -> torch.Tensor:\n+        raise NotImplementedError\n+\n+    def reset(self):\n+        raise NotImplementedError\n+\n+\n+class _CircularBuffer(_Buffer):\n+    def __init__(self, item_shape: Tuple, buffer_size: int, dtype, device):\n+        self._buffer = torch.zeros(\n+            (buffer_size, *item_shape), dtype=dtype, device=device\n+        )\n+        self._curr_index = 0\n+\n+    def append(self, value: torch.Tensor):\n+        self._buffer[self._curr_index] = value\n+        self._curr_index = (self._curr_index + 1) % len(self._buffer)\n+\n+    def get_all(self) -> torch.Tensor:\n+        return self._buffer\n+\n+    def reset(self):\n+        self._buffer[...] = 0\n+\n+\n+class _InfiniteBuffer(_Buffer):\n+    def __init__(self, item_shape: Tuple, dtype, device):\n+        self._item_shape = item_shape\n+        self._buffer = torch.zeros((128, *item_shape), dtype=dtype, device=device)\n+        self._size = 0\n+\n+    def append(self, value: torch.Tensor):\n+        curr_buffer_size = len(self._buffer)\n+        dtype = self._buffer.dtype\n+        device = self._buffer.device\n+\n+        if self._size == curr_buffer_size:\n+            new_buffer = torch.zeros(\n+                (2 * curr_buffer_size, *self._item_shape), dtype=dtype, device=device\n+            )\n+            new_buffer[:curr_buffer_size] = self._buffer\n+            self._buffer = new_buffer\n+\n+        self._buffer[self._size] = value\n+        self._size += 1\n+\n+    def get_all(self) -> torch.Tensor:\n+        return self._buffer[: self._size]\n+\n+    def reset(self):\n+        self._buffer[...] = 0\n+        self._size = 0\n+\n+\n+def _convert_global_physical_count_to_logical_count(\n+    # (whatever, num_layers, num_physical_experts)\n+    global_physical_count: torch.Tensor,\n+    num_layers: int,\n+    num_logical_experts: int,\n+    physical_to_logical_map: torch.Tensor,\n+):\n+    dim_extra, _, _ = global_physical_count.shape\n+    dtype = global_physical_count.dtype\n+    device = global_physical_count.device\n+    logical_count = torch.zeros(\n+        (dim_extra, num_layers, num_logical_experts), dtype=dtype, device=device\n+    )\n+    logical_count.scatter_add_(\n+        dim=2,\n+        index=physical_to_logical_map.unsqueeze(0).expand(dim_extra, -1, -1),\n+        src=global_physical_count,\n+    )\n+    return logical_count\ndiff --git a/python/sglang/srt/managers/expert_location.py b/python/sglang/srt/managers/expert_location.py\nnew file mode 100644\nindex 000000000..44496cdde\n--- /dev/null\n+++ b/python/sglang/srt/managers/expert_location.py\n@@ -0,0 +1,273 @@\n+# Copyright 2023-2024 SGLang Team\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+import json\n+import logging\n+from dataclasses import dataclass\n+from pathlib import Path\n+from typing import Optional\n+\n+import torch\n+import torch.distributed\n+import torch.nn.functional as F\n+\n+from sglang.srt.configs.model_config import ModelConfig\n+from sglang.srt.model_loader import get_model_architecture\n+from sglang.srt.server_args import ServerArgs\n+\n+logger = logging.getLogger(__name__)\n+\n+\n+@dataclass\n+class ExpertLocationMetadata:\n+    physical_to_logical_map: torch.Tensor  # (layers, num_physical_experts)\n+    logical_to_all_physical_map: torch.Tensor  # (layers, num_logical_experts, X)\n+    logical_to_all_physical_map_num_valid: torch.Tensor  # (layers, num_logical_experts)\n+\n+    # -------------------------------- properties ------------------------------------\n+\n+    @property\n+    def num_layers(self) -> int:\n+        return self.physical_to_logical_map.shape[0]\n+\n+    @property\n+    def num_physical_experts(self) -> int:\n+        return self.physical_to_logical_map.shape[1]\n+\n+    @property\n+    def num_local_physical_experts(self) -> int:\n+        ans, remainder = divmod(self.num_physical_experts, self.ep_size)\n+        assert remainder == 0\n+        return ans\n+\n+    @property\n+    def num_logical_experts(self) -> int:\n+        return self.logical_to_all_physical_map.shape[1]\n+\n+    @property\n+    def ep_size(self):\n+        # TODO change when EP size != world size\n+        return torch.distributed.get_world_size()\n+\n+    def __post_init__(self):\n+        num_layers_0, num_physical_experts_0 = self.physical_to_logical_map.shape\n+        num_layers_1, num_logical_experts_0, num_physical_experts_1 = (\n+            self.logical_to_all_physical_map.shape\n+        )\n+        num_layers_2, num_logical_experts_1 = (\n+            self.logical_to_all_physical_map_num_valid.shape\n+        )\n+        # TODO pr-chain: enable this later\n+        # assert num_layers_0 == num_layers_1 == num_layers_2 == num_layers_3\n+        # assert num_logical_experts_0 == num_logical_experts_1 == num_logical_experts_2\n+        assert num_physical_experts_0 == num_physical_experts_1\n+\n+    # -------------------------------- construction ------------------------------------\n+\n+    @staticmethod\n+    def init_trivial(server_args: ServerArgs, model_config: ModelConfig):\n+        \"\"\"Trivial location - logical expert i corresponds to physical expert i\"\"\"\n+        common = ExpertLocationMetadata._init_common(server_args, model_config)\n+        num_physical_experts = common[\"num_physical_experts\"]\n+        model_config_for_expert_location = common[\"model_config_for_expert_location\"]\n+        num_layers = model_config_for_expert_location.num_layers\n+        num_logical_experts = model_config_for_expert_location.num_logical_experts\n+\n+        physical_to_logical_map = (\n+            torch.arange(0, num_physical_experts).repeat(num_layers, 1)\n+            % num_logical_experts\n+        )\n+\n+        return ExpertLocationMetadata.init_by_mapping(\n+            server_args,\n+            model_config,\n+            physical_to_logical_map=physical_to_logical_map,\n+        )\n+\n+    @staticmethod\n+    def init_by_mapping(\n+        server_args: ServerArgs,\n+        model_config: ModelConfig,\n+        physical_to_logical_map,\n+    ):\n+        if not isinstance(physical_to_logical_map, torch.Tensor):\n+            physical_to_logical_map = torch.tensor(physical_to_logical_map)\n+        physical_to_logical_map = physical_to_logical_map.to(server_args.device)\n+\n+        common = ExpertLocationMetadata._init_common(server_args, model_config)\n+        model_config_for_expert_location = common[\"model_config_for_expert_location\"]\n+        logical_to_all_physical_map = _compute_logical_to_all_physical_map(\n+            physical_to_logical_map,\n+            num_logical_experts=model_config_for_expert_location.num_logical_experts,\n+        )\n+\n+        return ExpertLocationMetadata._init_raw(\n+            ep_size=common[\"ep_size\"],\n+            physical_to_logical_map=physical_to_logical_map,\n+            logical_to_all_physical_map=logical_to_all_physical_map,\n+        )\n+\n+    @staticmethod\n+    def _init_common(server_args: ServerArgs, model_config: ModelConfig):\n+        model_config_for_expert_location = (\n+            ModelConfigForExpertLocation.from_model_config(model_config)\n+        )\n+\n+        num_physical_experts = (\n+            model_config_for_expert_location.num_logical_experts\n+            # TODO pr-chain: enable this later\n+            # + server_args.ep_num_redundant_experts\n+        )\n+        ep_size = server_args.ep_size\n+        assert num_physical_experts % ep_size == 0\n+        num_local_physical_experts = num_physical_experts // ep_size\n+\n+        return dict(\n+            model_config_for_expert_location=model_config_for_expert_location,\n+            num_physical_experts=num_physical_experts,\n+            num_local_physical_experts=num_local_physical_experts,\n+            ep_size=ep_size,\n+        )\n+\n+    @staticmethod\n+    def _init_raw(\n+        ep_size: int,\n+        physical_to_logical_map: torch.Tensor,\n+        logical_to_all_physical_map: torch.Tensor,\n+    ):\n+        _, num_physical_experts = physical_to_logical_map.shape\n+\n+        logical_to_all_physical_map_padded = F.pad(\n+            logical_to_all_physical_map,\n+            (0, num_physical_experts - logical_to_all_physical_map.shape[-1]),\n+            value=-1,\n+        )\n+\n+        logical_to_all_physical_map_num_valid = torch.count_nonzero(\n+            logical_to_all_physical_map != -1, dim=-1\n+        )\n+\n+        return ExpertLocationMetadata(\n+            physical_to_logical_map=physical_to_logical_map,\n+            logical_to_all_physical_map=logical_to_all_physical_map_padded,\n+            logical_to_all_physical_map_num_valid=logical_to_all_physical_map_num_valid,\n+        )\n+\n+\n+_global_expert_location_metadata: Optional[ExpertLocationMetadata] = None\n+\n+\n+def get_global_expert_location_metadata():\n+    return _global_expert_location_metadata\n+\n+\n+def set_global_expert_location_metadata(value):\n+    global _global_expert_location_metadata\n+    assert _global_expert_location_metadata is None\n+    _global_expert_location_metadata = value\n+\n+\n+def _compute_logical_to_all_physical_map(\n+    physical_to_logical_map: torch.Tensor, num_logical_experts: int\n+):\n+    # This is rarely called, so we use for loops for maximum clarity\n+\n+    num_layers, num_physical_experts = physical_to_logical_map.shape\n+\n+    logical_to_all_physical_map = [\n+        [[] for _ in range(num_logical_experts)] for _ in range(num_layers)\n+    ]\n+    for layer_id in range(num_layers):\n+        for physical_expert_id in range(num_physical_experts):\n+            logical_expert_id = physical_to_logical_map[\n+                layer_id, physical_expert_id\n+            ].item()\n+            logical_to_all_physical_map[layer_id][logical_expert_id].append(\n+                physical_expert_id\n+            )\n+\n+    logical_to_all_physical_map = _pad_nested_array(\n+        logical_to_all_physical_map, pad_value=-1\n+    )\n+\n+    return torch.tensor(\n+        logical_to_all_physical_map, device=physical_to_logical_map.device\n+    )\n+\n+\n+def _pad_nested_array(arr, pad_value):\n+    max_len = max(len(inner) for outer in arr for inner in outer)\n+    padded = [\n+        [inner + [pad_value] * (max_len - len(inner)) for inner in outer]\n+        for outer in arr\n+    ]\n+    return padded\n+\n+\n+@dataclass\n+class ModelConfigForExpertLocation:\n+    num_layers: int\n+    num_logical_experts: int\n+    num_groups: Optional[int] = None\n+\n+    @staticmethod\n+    def init_dummy():\n+        return ModelConfigForExpertLocation(num_layers=1, num_logical_experts=1)\n+\n+    @staticmethod\n+    def from_model_config(model_config: ModelConfig):\n+        model_class, _ = get_model_architecture(model_config)\n+        if hasattr(model_class, \"get_model_config_for_expert_location\"):\n+            return model_class.get_model_config_for_expert_location(\n+                model_config.hf_config\n+            )\n+        else:\n+            return ModelConfigForExpertLocation.init_dummy()\n+\n+\n+def compute_initial_expert_location_metadata(\n+    server_args: ServerArgs, model_config: ModelConfig\n+) -> ExpertLocationMetadata:\n+    data = server_args.init_expert_location\n+    if data == \"trivial\":\n+        logger.info(\"init_expert_location from trivial\")\n+        return ExpertLocationMetadata.init_trivial(server_args, model_config)\n+\n+    # TODO unify with the utils function\n+    if data.endswith(\".pt\"):\n+        data_dict = torch.load(data, weights_only=True)\n+    elif data.endswith(\".json\"):\n+        data_dict = json.loads(Path(data).read_text())\n+    else:\n+        data_dict = json.loads(data)\n+\n+    if \"physical_to_logical_map\" in data_dict:\n+        logger.info(\n+            \"init_expert_location from init_by_mapping using ServerArgs.init_expert_location\"\n+        )\n+        return ExpertLocationMetadata.init_by_mapping(\n+            server_args, model_config, **data_dict\n+        )\n+    elif \"logical_count\" in data_dict:\n+        # TODO pr-chain: enable this later\n+        raise NotImplementedError\n+        # logger.info(\n+        #     \"init_expert_location from init_by_eplb using ServerArgs.init_expert_location\"\n+        # )\n+        # return ExpertLocationMetadata.init_by_eplb(\n+        #     server_args, model_config, logical_count=data_dict[\"logical_count\"]\n+        # )\n+    else:\n+        raise NotImplementedError(\n+            f\"Unknown init_expert_location format ({list(data_dict.keys())=})\"\n+        )\ndiff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py\nindex 0506460b1..72a3f7246 100644\n--- a/python/sglang/srt/managers/scheduler.py\n+++ b/python/sglang/srt/managers/scheduler.py\n@@ -59,7 +59,10 @@ from sglang.srt.hf_transformers_utils import (\n )\n from sglang.srt.layers.dp_attention import compute_dp_attention_world_info\n from sglang.srt.layers.logits_processor import LogitsProcessorOutput\n-from sglang.srt.managers.expert_distribution import ExpertDistributionRecorder\n+from sglang.srt.managers.expert_distribution import (\n+    ExpertDistributionRecorder,\n+    get_global_expert_distribution_recorder,\n+)\n from sglang.srt.managers.io_struct import (\n     AbortReq,\n     CloseSessionReqInput,\n@@ -142,8 +145,6 @@ from sglang.srt.utils import (\n )\n from sglang.utils import TypeBasedDispatcher, get_exception_traceback\n \n-expert_distribution_recorder = ExpertDistributionRecorder()\n-\n logger = logging.getLogger(__name__)\n \n # Test retract decode for debugging purposes\n@@ -2162,11 +2163,11 @@ class Scheduler(\n \n     def expert_distribution_handle(self, recv_req: ExpertDistributionReq):\n         if recv_req == ExpertDistributionReq.START_RECORD:\n-            expert_distribution_recorder.start_record()\n+            get_global_expert_distribution_recorder().start_record()\n         elif recv_req == ExpertDistributionReq.STOP_RECORD:\n-            expert_distribution_recorder.stop_record()\n+            get_global_expert_distribution_recorder().stop_record()\n         elif recv_req == ExpertDistributionReq.DUMP_RECORD:\n-            expert_distribution_recorder.dump_record()\n+            get_global_expert_distribution_recorder().dump_record()\n         else:\n             raise ValueError(\"Unrecognized ExpertDistributionReq value\")\n         return ExpertDistributionReqOutput()\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex 4ce681c14..78a94a898 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -52,6 +52,16 @@ from sglang.srt.layers.quantization.deep_gemm import (\n from sglang.srt.layers.sampler import Sampler\n from sglang.srt.layers.torchao_utils import apply_torchao_config_to_model\n from sglang.srt.lora.lora_manager import LoRAManager\n+from sglang.srt.managers.expert_distribution import (\n+    ExpertDistributionRecorder,\n+    get_global_expert_distribution_recorder,\n+    set_global_expert_distribution_recorder,\n+)\n+from sglang.srt.managers.expert_location import (\n+    compute_initial_expert_location_metadata,\n+    get_global_expert_location_metadata,\n+    set_global_expert_location_metadata,\n+)\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.mem_cache.memory_pool import (\n     DoubleSparseTokenToKVPool,\n@@ -161,6 +171,8 @@ class ModelRunner:\n         self.use_mla_backend = self.model_config.attention_arch == AttentionArch.MLA\n         self.attention_chunk_size = model_config.attention_chunk_size\n \n+        self.forward_pass_id = 0\n+\n         # Model-specific adjustment\n         self.model_specific_adjustment()\n \n@@ -219,6 +231,25 @@ class ModelRunner:\n             enable=self.server_args.enable_memory_saver\n         )\n \n+        if not self.is_draft_worker:\n+            set_global_expert_location_metadata(\n+                compute_initial_expert_location_metadata(server_args, self.model_config)\n+            )\n+            if self.tp_rank == 0 and get_bool_env_var(\n+                \"SGLANG_LOG_EXPERT_LOCATION_METADATA\"\n+            ):\n+                logger.info(\n+                    f\"Initial expert_location_metadata: {get_global_expert_location_metadata().debug_str()}\"\n+                )\n+\n+            set_global_expert_distribution_recorder(\n+                ExpertDistributionRecorder.init_new(\n+                    server_args,\n+                    get_global_expert_location_metadata(),\n+                    rank=self.tp_rank,\n+                )\n+            )\n+\n         # Load the model\n         self.sampler = Sampler()\n         self.load_model()\n@@ -1093,6 +1124,22 @@ class ModelRunner:\n         forward_batch: ForwardBatch,\n         skip_attn_backend_init: bool = False,\n         pp_proxy_tensors: Optional[PPProxyTensors] = None,\n+    ) -> Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]:\n+        self.forward_pass_id += 1\n+\n+        with get_global_expert_distribution_recorder().with_forward_pass(\n+            self.forward_pass_id,\n+            forward_batch,\n+        ):\n+            return self._forward_raw(\n+                forward_batch, skip_attn_backend_init, pp_proxy_tensors\n+            )\n+\n+    def _forward_raw(\n+        self,\n+        forward_batch: ForwardBatch,\n+        skip_attn_backend_init: bool,\n+        pp_proxy_tensors: Optional[PPProxyTensors],\n     ) -> Tuple[Union[LogitsProcessorOutput, PPProxyTensors], bool]:\n         can_run_cuda_graph = bool(\n             forward_batch.forward_mode.is_cuda_graph()\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex e422a5038..3fb003ff9 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -77,7 +77,11 @@ from sglang.srt.layers.vocab_parallel_embedding import (\n     ParallelLMHead,\n     VocabParallelEmbedding,\n )\n-from sglang.srt.managers.expert_distribution import ExpertDistributionRecorder\n+from sglang.srt.managers.expert_distribution import (\n+    ExpertDistributionRecorder,\n+    get_global_expert_distribution_recorder,\n+)\n+from sglang.srt.managers.expert_location import ModelConfigForExpertLocation\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode\n from sglang.srt.model_loader.weight_utils import default_weight_loader\n@@ -109,8 +113,6 @@ if _is_hip:\n         decode_attention_fwd_grouped_rope,\n     )\n \n-expert_distribution_recorder = ExpertDistributionRecorder()\n-\n logger = logging.getLogger(__name__)\n \n \n@@ -302,6 +304,7 @@ class DeepseekV2MoE(nn.Module):\n     def forward(\n         self, hidden_states: torch.Tensor, forward_batch: Optional[ForwardBatch] = None\n     ) -> torch.Tensor:\n+        forward_mode = forward_batch.forward_mode\n         if (not self._enable_deepep_moe) or is_non_idle_and_non_empty(\n             forward_mode, hidden_states\n         ):\n@@ -1278,7 +1281,7 @@ class DeepseekV2DecoderLayer(nn.Module):\n             )\n \n         # Fully Connected\n-        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.mlp(hidden_states, forward_batch)\n \n         # TODO(ch-wan): use reduce-scatter in MLP to avoid this scatter\n         # Scatter\n@@ -1422,11 +1425,11 @@ class DeepseekV2Model(nn.Module):\n \n         residual = None\n         for i in range(len(self.layers)):\n-            expert_distribution_recorder.set_current_layer(i)\n-            layer = self.layers[i]\n-            hidden_states, residual = layer(\n-                positions, hidden_states, forward_batch, residual, zero_allocator\n-            )\n+            with get_global_expert_distribution_recorder().with_current_layer(i):\n+                layer = self.layers[i]\n+                hidden_states, residual = layer(\n+                    positions, hidden_states, forward_batch, residual, zero_allocator\n+                )\n         if not forward_batch.forward_mode.is_idle():\n             if residual is None:\n                 hidden_states = self.norm(hidden_states)\n@@ -1872,6 +1875,14 @@ class DeepseekV2ForCausalLM(nn.Module):\n         torch.cuda.empty_cache()\n         torch.cuda.synchronize()\n \n+    @classmethod\n+    def get_model_config_for_expert_location(cls, config):\n+        return ModelConfigForExpertLocation(\n+            num_layers=config.num_hidden_layers,\n+            num_logical_experts=config.n_routed_experts,\n+            num_groups=config.n_group,\n+        )\n+\n \n class DeepseekV3ForCausalLM(DeepseekV2ForCausalLM):\n     pass\ndiff --git a/python/sglang/srt/models/qwen2_moe.py b/python/sglang/srt/models/qwen2_moe.py\nindex 525498d5b..261b707d7 100644\n--- a/python/sglang/srt/models/qwen2_moe.py\n+++ b/python/sglang/srt/models/qwen2_moe.py\n@@ -59,14 +59,16 @@ from sglang.srt.layers.vocab_parallel_embedding import (\n     ParallelLMHead,\n     VocabParallelEmbedding,\n )\n-from sglang.srt.managers.expert_distribution import ExpertDistributionRecorder\n+from sglang.srt.managers.expert_distribution import (\n+    ExpertDistributionRecorder,\n+    get_global_expert_distribution_recorder,\n+)\n+from sglang.srt.managers.expert_location import ModelConfigForExpertLocation\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch, PPProxyTensors\n from sglang.srt.model_loader.weight_utils import default_weight_loader\n from sglang.srt.utils import add_prefix, make_layers\n \n-expert_distribution_recorder = ExpertDistributionRecorder()\n-\n logger = logging.getLogger(__name__)\n \n \n@@ -591,11 +593,11 @@ class Qwen2MoeModel(nn.Module):\n             residual = pp_proxy_tensors[\"residual\"]\n \n         for i in range(self.start_layer, self.end_layer):\n-            expert_distribution_recorder.set_current_layer(i)\n-            layer = self.layers[i]\n-            hidden_states, residual = layer(\n-                positions, hidden_states, forward_batch, residual\n-            )\n+            with get_global_expert_distribution_recorder().with_current_layer(i):\n+                layer = self.layers[i]\n+                hidden_states, residual = layer(\n+                    positions, hidden_states, forward_batch, residual\n+                )\n         if not self.pp_group.is_last_rank:\n             return PPProxyTensors(\n                 {\n@@ -752,5 +754,13 @@ class Qwen2MoeForCausalLM(nn.Module):\n                     else:\n                         logger.warning(f\"Parameter {name} not found in params_dict\")\n \n+    @classmethod\n+    def get_model_config_for_expert_location(cls, config):\n+        return ModelConfigForExpertLocation(\n+            num_layers=config.num_hidden_layers,\n+            num_logical_experts=config.num_experts,\n+            num_groups=None,\n+        )\n+\n \n EntryClass = Qwen2MoeForCausalLM\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 1e650fe71..59e8dccc1 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -170,6 +170,11 @@ class ServerArgs:\n     enable_ep_moe: bool = False\n     enable_deepep_moe: bool = False\n     deepep_mode: Optional[Literal[\"auto\", \"normal\", \"low_latency\"]] = \"auto\"\n+    init_expert_location: str = \"trivial\"\n+    expert_distribution_recorder_mode: Optional[\n+        Literal[\"stat\", \"per_pass\", \"per_token\"]\n+    ] = None\n+    expert_distribution_recorder_buffer_size: Optional[int] = None\n     deepep_config: Optional[str] = None\n     enable_torch_compile: bool = False\n     torch_compile_max_bs: int = 32\n@@ -361,6 +366,15 @@ class ServerArgs:\n                 \"Pipeline parallelism is incompatible with overlap schedule.\"\n             )\n \n+        if self.expert_distribution_recorder_buffer_size is None:\n+            # TODO pr-chain: enable this later\n+            # if (x := self.eplb_rebalance_num_iterations) is not None:\n+            #     self.expert_distribution_recorder_buffer_size = x\n+            if False:\n+                pass\n+            elif self.expert_distribution_recorder_mode is not None:\n+                self.expert_distribution_recorder_buffer_size = 1000\n+\n         # Speculative Decoding\n         if self.speculative_algorithm == \"NEXTN\":\n             # NEXTN shares the same implementation of EAGLE\n@@ -1257,6 +1271,24 @@ class ServerArgs:\n             default=\"auto\",\n             help=\"Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or `auto`. Default is `auto`, which means `low_latency` for decode batch and `normal` for prefill batch.\",\n         )\n+        parser.add_argument(\n+            \"--init-expert-location\",\n+            type=str,\n+            default=ServerArgs.init_expert_location,\n+            help=\"Initial location of EP experts.\",\n+        )\n+        parser.add_argument(\n+            \"--expert-distribution-recorder-mode\",\n+            type=str,\n+            default=ServerArgs.expert_distribution_recorder_mode,\n+            help=\"Mode of expert distribution recorder.\",\n+        )\n+        parser.add_argument(\n+            \"--expert-distribution-recorder-buffer-size\",\n+            type=int,\n+            default=ServerArgs.expert_distribution_recorder_buffer_size,\n+            help=\"Circular buffer size of expert distribution recorder. Set to -1 to denote infinite buffer.\",\n+        )\n         parser.add_argument(\n             \"--deepep-config\",\n             type=str,\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 0c16667ef..884e715fa 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -46,7 +46,19 @@ from importlib.util import find_spec\n from io import BytesIO\n from multiprocessing.reduction import ForkingPickler\n from pathlib import Path\n-from typing import Any, Callable, Dict, List, Optional, Protocol, Set, Tuple, Union\n+from typing import (\n+    Any,\n+    Callable,\n+    Dict,\n+    Generic,\n+    List,\n+    Optional,\n+    Protocol,\n+    Set,\n+    Tuple,\n+    TypeVar,\n+    Union,\n+)\n \n import numpy as np\n import psutil\n@@ -2126,3 +2138,25 @@ def load_json_config(data: str):\n \n def dispose_tensor(x: torch.Tensor):\n     x.set_(torch.empty((0,), device=x.device, dtype=x.dtype))\n+\n+\n+T = TypeVar(\"T\")\n+\n+\n+class Withable(Generic[T]):\n+    def __init__(self):\n+        self._value: Optional[T] = None\n+\n+    @property\n+    def value(self) -> T:\n+        return self._value\n+\n+    @contextmanager\n+    def with_value(self, new_value: T):\n+        assert self._value is None\n+        self._value = new_value\n+        try:\n+            yield\n+        finally:\n+            assert self._value is new_value\n+            self._value = None\ndiff --git a/test/srt/test_expert_distribution.py b/test/srt/test_expert_distribution.py\nindex e3826303d..b0efcfb38 100755\n--- a/test/srt/test_expert_distribution.py\n+++ b/test/srt/test_expert_distribution.py\n@@ -1,9 +1,10 @@\n-import csv\n-import glob\n import os\n+import tempfile\n import unittest\n+from pathlib import Path\n \n import requests\n+import torch\n \n from sglang.srt.utils import kill_process_tree\n from sglang.test.test_utils import (\n@@ -16,108 +17,86 @@ from sglang.test.test_utils import (\n \n \n class TestExpertDistribution(CustomTestCase):\n-    def setUp(self):\n-        # Clean up any existing expert distribution files before each test\n-        for f in glob.glob(\"expert_distribution_*.csv\"):\n-            os.remove(f)\n-\n-    def tearDown(self):\n-        # Clean up any expert distribution files after each test\n-        for f in glob.glob(\"expert_distribution_*.csv\"):\n-            os.remove(f)\n-\n     def test_expert_distribution_record(self):\n+        # TODO: Add tests for DeepEP gatherer (currently our CI cannot run that)\n+        for info in [\n+            dict(model_path=\"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"),\n+            dict(model_path=\"Qwen/Qwen1.5-MoE-A2.7B\"),\n+            dict(model_path=\"Qwen/Qwen1.5-MoE-A2.7B\", tp_size=2),\n+            # TODO enable in next PR\n+            # dict(model_path=\"Qwen/Qwen1.5-MoE-A2.7B\", mode=\"per_pass\"),\n+            # dict(model_path=\"Qwen/Qwen1.5-MoE-A2.7B\", mode=\"per_token\"),\n+        ]:\n+            with self.subTest(info=info):\n+                self._execute_core(**info)\n+\n+    def _execute_core(self, model_path: str, mode: str = \"stat\", tp_size: int = 1):\n         \"\"\"Test expert distribution record endpoints\"\"\"\n-        process = popen_launch_server(\n-            # The feature is only implemented in deepseek_v2.py\n-            \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n-            DEFAULT_URL_FOR_TEST,\n-            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,\n-            other_args=[\n-                \"--trust-remote-code\",\n-            ],\n-        )\n-\n-        try:\n-            # Start recording\n-            response = requests.post(\n-                f\"{DEFAULT_URL_FOR_TEST}/start_expert_distribution_record\"\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            os.environ[\"SGLANG_EXPERT_DISTRIBUTION_RECORDER_DIR\"] = tmp_dir\n+\n+            process = popen_launch_server(\n+                model_path,\n+                DEFAULT_URL_FOR_TEST,\n+                timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,\n+                other_args=[\n+                    \"--trust-remote-code\",\n+                    \"--tp-size\",\n+                    str(tp_size),\n+                    \"--expert-distribution-recorder-mode\",\n+                    mode,\n+                    \"--disable-cuda-graph\",\n+                    \"--disable-overlap-schedule\",\n+                ],\n             )\n-            self.assertEqual(response.status_code, 200)\n \n-            # Make some requests to generate expert distribution data\n-            response = requests.post(\n-                f\"{DEFAULT_URL_FOR_TEST}/generate\",\n-                json={\n-                    \"text\": \"The capital of France is\",\n-                    \"sampling_params\": {\n-                        \"temperature\": 0,\n-                        \"max_new_tokens\": 32,\n+            try:\n+                # Start recording\n+                response = requests.post(\n+                    f\"{DEFAULT_URL_FOR_TEST}/start_expert_distribution_record\"\n+                )\n+                self.assertEqual(response.status_code, 200)\n+\n+                # Make some requests to generate expert distribution data\n+                response = requests.post(\n+                    f\"{DEFAULT_URL_FOR_TEST}/generate\",\n+                    json={\n+                        \"text\": \"The capital of France is\",\n+                        \"sampling_params\": {\n+                            \"temperature\": 0,\n+                            \"max_new_tokens\": 32,\n+                        },\n                     },\n-                },\n-            )\n-            self.assertEqual(response.status_code, 200)\n-\n-            # Stop recording\n-            response = requests.post(\n-                f\"{DEFAULT_URL_FOR_TEST}/stop_expert_distribution_record\"\n-            )\n-            self.assertEqual(response.status_code, 200)\n-\n-            # Dump the recorded data\n-            response = requests.post(\n-                f\"{DEFAULT_URL_FOR_TEST}/dump_expert_distribution_record\"\n-            )\n-            self.assertEqual(response.status_code, 200)\n-\n-            # Verify the dumped file exists and has correct format\n-            csv_files = glob.glob(\"expert_distribution_*.csv\")\n-            self.assertEqual(\n-                len(csv_files),\n-                1,\n-                f\"Expected exactly one expert distribution CSV file {csv_files=}\",\n-            )\n+                )\n+                self.assertEqual(response.status_code, 200)\n \n-            # Check CSV file format\n-            with open(csv_files[0], \"r\") as f:\n-                csv_reader = csv.reader(f)\n+                # Stop recording\n+                response = requests.post(\n+                    f\"{DEFAULT_URL_FOR_TEST}/stop_expert_distribution_record\"\n+                )\n+                self.assertEqual(response.status_code, 200)\n \n-                # Check header\n-                header = next(csv_reader)\n-                self.assertEqual(\n-                    header,\n-                    [\"layer_id\", \"expert_id\", \"count\"],\n-                    \"CSV header should be 'layer_id,expert_id,count'\",\n+                # Dump the recorded data\n+                response = requests.post(\n+                    f\"{DEFAULT_URL_FOR_TEST}/dump_expert_distribution_record\"\n                 )\n+                self.assertEqual(response.status_code, 200)\n \n                 # Check data rows\n-                rows = list(csv_reader)\n-                self.assertGreater(len(rows), 0, \"CSV file should contain data rows\")\n-\n-                for row in rows:\n-                    # Verify each row has 3 columns\n-                    self.assertEqual(\n-                        len(row),\n-                        3,\n-                        \"Each row should have layer_id, expert_id and count\",\n-                    )\n+                data = torch.load(\n+                    list(Path(tmp_dir).glob(\"*.pt\"))[0], weights_only=True\n+                )\n+                print(f\"{data=}\")\n \n-                    # Verify data types\n-                    layer_id, expert_id, count = row\n-                    self.assertTrue(\n-                        layer_id.isdigit(),\n-                        f\"layer_id should be an integer {row=} {rows=}\",\n-                    )\n-                    self.assertTrue(\n-                        expert_id.isdigit(),\n-                        f\"expert_id should be an integer {row=} {rows=}\",\n-                    )\n-                    self.assertTrue(\n-                        count.isdigit(), f\"count should be an integer {row=} {rows=}\"\n-                    )\n+                if mode in [\"per_pass\", \"per_token\"]:\n+                    self.assertGreater(len(data), 0, \"Should contain data rows\")\n+                else:\n+                    logical_count = data[\"logical_count\"]\n+                    print(f\"{logical_count.sum()=} {logical_count=}\")\n+                    self.assertTrue(logical_count.sum() > 0)\n \n-        finally:\n-            kill_process_tree(process.pid)\n+            finally:\n+                kill_process_tree(process.pid)\n \n \n if __name__ == \"__main__\":",
  "apis": [
    "sglang.srt.managers.expert_distribution.get_global_expert_distribution_recorder",
    "sglang.srt.managers.expert_distribution.ExpertDistributionRecorder.on_select_experts",
    "sglang.srt.managers.expert_distribution.ExpertDistributionRecorder.on_deepep_dispatch_normal",
    "sglang.srt.managers.expert_distribution.ExpertDistributionRecorder.on_deepep_dispatch_low_latency",
    "sglang.srt.managers.expert_location.ExpertLocationMetadata"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/eplb/expert_distribution.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/scheduler.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes non-trivial modifications in several core modules (e.g. expert distribution recording, gathering, and accumulation) that are used during a model\u2019s forward pass. The changes integrate a new mechanism for recording expert distribution with minimal overhead, which is a performance optimization improvement aimed at reducing runtime costs. The modifications are not limited to tests or refactoring of comments/documentation but impact internal APIs and top-level functions (e.g. in model_runner and scheduler), affecting the performance during inference on CPU. Although the commit message does not directly mention \u201cperformance\u201d but rather focuses on reducing overhead, the nature of the changes (improving expert distribution recorder efficiency) meets the criteria for a performance/optimization commit.",
  "llm_api_reason": "This commit introduces changes to enable expert distribution recording in the server. The documentation is updated to launch the server with the new parameter \"--expert-distribution-recorder-mode stat\". In the codebase, the ExpertDistributionRecorder API is refactored: its methods (on_select_experts, on_deepep_dispatch_normal, on_deepep_dispatch_low_latency) now are invoked via the global getter (get_global_expert_distribution_recorder) instead of using a local instance. In addition, a new module (expert_location.py) is added to handle expert location metadata, which is passed into the recorder during initialization. These modifications affect how expert distribution events are recorded during forward passes and dispatch, ensuring minimal overhead for the EPLB system."
}