{
  "commit_hash": "2ed68d7a6c4737618652cfa0288443a5a5d73b14",
  "pr_url": "https://github.com/sgl-project/sglang/pull/7236",
  "pr_date": "2025-06-16",
  "timeline_text": "Copy link Contributor ssssnow commented Jun 16, 2025 Motivation This PR aims to significantly improve data transfer performance in the mooncake connector by replacing the original simple transfer with a batch transfer interface. Modifications Replaced the simple transfer implementation in the mooncake connector with a batch transfer interface. Performance Improvement In local tests (3 prefill nodes + 9 decode nodes, DeepSeek V3 model, 10K context length): Simple transfer (old implementation in deep_ep branch): max transfer speed ~1.5 GB/s; Batch transfer (this PR): transfer speed increased to 10-15 GB/s, about 8-12x faster; Transfer time reduced from 650ms to 60ms. As shown below, batch transfer greatly improves bandwidth utilization and overall performance: More speed test could be found in kvcache-ai/Mooncake#499 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 Swipe4057, Hongbosherlock, and yiakwy-xpu-ml-framework-team reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions ssssnow requested review from hnyls2002 and ByronHsu as code owners June 16, 2025 12:24 gemini-code-assist bot reviewed Jun 16, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @ssssnow , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request aims to substantially improve the data transfer performance within the mooncake connector by switching from a single-transfer approach to a more efficient batch transfer method. This change involves modifying the KV cache sending logic to utilize a new batch transfer function, which has demonstrated significant speedups in testing. Highlights Performance Improvement : Replaced individual data transfers with a batch transfer mechanism in the mooncake connector, leading to significant performance gains (8-12x faster transfer speed in local tests). Batch Transfer Implementation : Introduced a new batch_transfer_sync method in the transfer engine to facilitate the new batch transfer approach. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with \ud83d\udc4d and \ud83d\udc4e on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configureGemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . \u21a9 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jun 16, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request aims to improve performance by replacing single transfers with batch transfers using the Mooncake engine. The addition of the batch_transfer_sync method in transfer_engine.py is correctly implemented. However, the usage of this new method in conn.py does not currently leverage the batching capability as the batch transfer call and list initialization are still inside the loop processing individual blocks. Addressing this will be crucial to realize the performance gains described. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/disaggregation/mooncake/conn.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . [PD Disaggregation] replace transfer with batch transfer for better p\u2026 \u2026 be13dbb \u2026erformance ssssnow force-pushed the add_batch_transfer branch\n    from 3d96dc6 to be13dbb Compare June 16, 2025 12:28 Copy link Contributor Author ssssnow commented Jun 17, 2025 @binarycrayon @hnyls2002 Hi guys, please help to review this PR, thanks~ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned ShangmingCai Jun 18, 2025 Copy link Collaborator ShangmingCai commented Jun 18, 2025 will review and test this today\uff0c thx \u2764\ufe0f 1 ssssnow reacted with heart emoji All reactions \u2764\ufe0f 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor doujiang24 commented Jun 18, 2025 @ssssnow how many separated(non-contiguous page) chunks in a single layer in the real tests? seems the improvement based on the separated chunk numbers. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author ssssnow commented Jun 18, 2025 @ssssnow how many separated(non-contiguous page) chunks in a single layer in the real tests? seems the improvement based on the separated chunk numbers. When it comes to single big chunk, batch transfer and non-batch transfer have comparable speed. But in the case of DeepSeek MLA, single chunk may be not large enough, so single iteration (61 alyers, 10K inputs) may result in many small chunks. Typical speed test on various chunk size and chunk number could be found here: kvcache-ai/Mooncake#499 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ShangmingCai reviewed Jun 18, 2025 View reviewed changes Copy link Collaborator ShangmingCai left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM. The change is clean and straightforward, but I need to run some tests when kvcache-ai/Mooncake#499 gets merged. Also, we need to wait for a new release v0.3.4 of mooncake, and we can revisit this PR at that time. I will ping someone to speed up the review process. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor doujiang24 commented Jun 18, 2025 @ssssnow I see the performance testing: kvcache-ai/Mooncake#499 , and the number of chunks could be large. I'm just curious, how many chunks in a single layer leads to Transfer time reduced from 650ms to 60ms , in the sglang testing, since the chunks in performance testing start from 5000. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator ShangmingCai commented Jun 18, 2025 @ssssnow Can you share the commands to help reproduce? I think this is a nice work and could be helpful when page size is set to 1. But I am not sure whether it will significantly improve the e2e throughput when the page size is large. Let me run some tests to verify. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author ssssnow commented Jun 18, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @ssssnow Can you share the commands to help reproduce? I think this is a nice work and could be helpful when page size is set to 1. But I am not sure whether it will significantly improve the e2e throughput when the page size is large. Let me run some tests to verify. I tested this in SGLang using 3 Prefill nodes + 9 Decode nodes + DeepSeek V3 model + 10K input length. I add some profiling codes at send_kv_cache . If non-batch transfer is used, it only reach 1.5 GB/s (on old deep_ep branch). Batch transfer could deliver 12 GB/s speed. But I believe this could be tested in Mooncake's repo. Here's the code I used: https://gist.github.com/ssssnow/37f17c9815cd26c3337ceae2729397cb test commands are listed in the file, you could try to reproduce that. \ud83d\udc4d 1 ShangmingCai reacted with thumbs up emoji \ud83d\ude80 1 ShangmingCai reacted with rocket emoji All reactions \ud83d\udc4d 1 reaction \ud83d\ude80 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author ssssnow commented Jun 18, 2025 @ssssnow Can you share the commands to help reproduce? I think this is a nice work and could be helpful when page size is set to 1. But I am not sure whether it will significantly improve the e2e throughput when the page size is large. Let me run some tests to verify. I tested this in SGLang using 3 Prefill nodes + 9 Decode nodes + DeepSeek V3 model + 10K input length. I add some profiling codes at send_kv_cache . If non-batch transfer is used, it only reach 1.5 GB/s (on old deep_ep branch). Batch transfer could deliver 12 GB/s speed. But I believe this could be tested in Mooncake's repo. Here's the code I used: https://gist.github.com/ssssnow/37f17c9815cd26c3337ceae2729397cb test commands are listed in the file, you could try to reproduce that. Not sure if these are enough to help. If I miss anything, just let me know. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ShangmingCai mentioned this pull request Jun 18, 2025 [PD] Add different TP sizes support for no-MLA models #6793 Merged 6 tasks ShangmingCai reviewed Jun 23, 2025 View reviewed changes python/sglang/srt/disaggregation/mooncake/transfer_engine.py peer_buffer_addresses: List[int], lengths: List[int], ) -> int: \"\"\"Synchronously transfer data to the specified address.\"\"\" Copy link Collaborator ShangmingCai Jun 23, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment nit: maybe optimize this comment? like \"Synchronously batch transfer data to the specified addresses.\" Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions ShangmingCai approved these changes Jun 23, 2025 View reviewed changes Copy link Collaborator ShangmingCai left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Collaborator ShangmingCai commented Jun 23, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . I think it is ok to get this merged first. If we get error reports from the users, I can add an env var to let the users decide whether to use this new batch API if its stability or performance improvement turns out to be not guaranteed. Currently, I think the current changes are great. \ud83d\udc4d 1 ssssnow reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ShangmingCai added 2 commits June 23, 2025 18:34 Merge branch 'main' into add_batch_transfer 120938b Merge branch 'main' into add_batch_transfer 4eec830 Hide details View details zhyncs merged commit 2ed68d7 into sgl-project : main Jun 24, 2025 114 of 148 checks passed Uh oh! There was an error while loading. Please reload this page . yilian49 pushed a commit\n        to yilian49/sglang\n      that referenced\n      this pull request Jun 24, 2025 [PD Disaggregation] replace transfer with batch transfer for better p\u2026 \u2026 ffdd8f2 \u2026erformance ( sgl-project#7236 ) HanHan009527 pushed a commit\n        to bytedance-iaas/sglang\n      that referenced\n      this pull request Jun 27, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 99584ad \u2026better performance ( sgl-project#7236 )\"\n\nThis reverts commit 2ed68d7 . fzyzcjy added a commit\n        to fzyzcjy/sglang\n      that referenced\n      this pull request Jul 11, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 44a9234 \u2026better performance ( sgl-project#7236 )\n\n\"\n\nThis reverts commit 2ed68d7 . gemini-code-assist bot mentioned this pull request Jul 12, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for better performance (#7236)\" #7968 Merged zhyncs pushed a commit\n      that referenced\n      this pull request Jul 12, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 880221b \u2026better performance ( #7236 )\" ( #7968 ) ZhengWG pushed a commit\n        to ZhengWG/sglang\n      that referenced\n      this pull request Jul 16, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 b2d555d \u2026better performance ( sgl-project#7236 )\" ( sgl-project#7968 ) chenxijun1029 pushed a commit\n        to chenxijun1029/sglang\n      that referenced\n      this pull request Jul 17, 2025 [PD Disaggregation] replace transfer with batch transfer for better p\u2026 \u2026 acb304a \u2026erformance ( sgl-project#7236 ) chenxijun1029 pushed a commit\n        to chenxijun1029/sglang\n      that referenced\n      this pull request Jul 17, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 d8e434a \u2026better performance ( sgl-project#7236 )\" ( sgl-project#7968 ) pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Jul 17, 2025 Merge 0 4 9 to master next ( sgl-project#80 ) \u2026 8f20122 * Use seq_len_fill_value in the cuda graph runners ( sgl-project#7233 )\n\n* support custom weight loader for model runner ( sgl-project#7122 )\n\nCo-authored-by: kavioyu <kavioyu@tencent.com>\n\n* Fix AMD speculative decoding ( sgl-project#7252 )\n\n* [Refactor] OAI Server components ( sgl-project#7167 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* OAI Server Skeleton & Core Utility Endpoints ( sgl-project#7179 )\n\n* [amd] Opt dsv3 moe ( sgl-project#7160 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\n\n* update ci node for xeon ( sgl-project#7265 )\n\n* feat: mtp support dp-attention ( sgl-project#6081 )\n\nCo-authored-by: austindeng <austindeng@tencent.com>\nCo-authored-by: tianqilin.99 <tianqilin.99@bytedance.com>\nCo-authored-by: Qiaolin Yu <liin1211@outlook.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* support qwen2 running on ascend npu device ( sgl-project#7022 )\n\nCo-authored-by: \u5201\u83b9\u715c <diaoyingyu1@hisilicon.com>\n\n* Fix Deepseek R1 0528 FP4 tensor name mismatch issue during weights loading. ( sgl-project#7164 )\n\n* bugfix(tool call ebnf): Fix EBNF generation for optional function parameters ( sgl-project#7283 )\n\n* Fix AWQ Dequant and Weight Loading of deepseek v2 ( sgl-project#6842 )\n\n* fix: resolve b200 dsv3 mtp issue ( sgl-project#7286 )\n\n* ci: Fix test_ebnf_generate_all_optional_function_params ( sgl-project#7288 )\n\n* fix: only enable flash_attn test on sm80 sm90 ( sgl-project#7289 )\n\n* [PD] Support get local ip from NIC for PD disaggregation ( sgl-project#7237 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [PD] Add custom memory pool option to support Mooncake PD with NVLink  ( sgl-project#7264 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Upstreaming hicache bug fixes ( sgl-project#7267 )\n\n* Update python API of activation, topk, norm and rope and remove vllm dependency ( sgl-project#6614 )\n\nCo-authored-by: Wu, Chunyuan <chunyuan.wu@intel.com>\nCo-authored-by: jianan-gu <jianan.gu@intel.com>\nCo-authored-by: sdp <sdp@gnr799219.jf.intel.com>\n\n* Fix hicache benchmark script bug - some sampled input_request is [] ( sgl-project#7300 )\n\n* chore: change logs from`INFO` to `DEBUG` for dp and add force quit for tokenizer manager ( sgl-project#7251 )\n\n* update invalid link in doc ( sgl-project#7297 )\n\n* Fix mini_lb for PD with long output: limit chunk size of decode response ( sgl-project#7301 )\n\nSigned-off-by: ch-tiger1 <xyz@ch-tech.ip-ddns.com>\nCo-authored-by: ch-tiger1 <xyz@ch-tech.ip-ddns.com>\n\n* Fix profiler error when there are idle passes ( sgl-project#7003 )\n\n* [pd] optimize dockerfile for  pd disaggregation ( sgl-project#7319 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* Merge PDLB (Prefill-Decode Load Balancer) into SGLang Router ( sgl-project#7096 )\n\n* Add more refactored openai test & in CI ( sgl-project#7284 )\n\n* fix: resolve blackwell deepep image issue ( sgl-project#7331 )\n\n* add seed in CPU UTs to avoid flaky failure ( sgl-project#7333 )\n\n* Multi-Stage Awake: Support Resume and Pause KV Cache and Weights separately ( sgl-project#7099 )\n\n* Reintroduce tiny fix sampler error when prob is not contiguous ( sgl-project#7354 )\n\n* [Refactor] Clean up radix cache related API ( sgl-project#7303 )\n\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* Put `_normalize_rid` before other normalization in `io_struct` ( sgl-project#7363 )\n\n* [PD] Transfer hidden states for mtp when disaggregation ( sgl-project#7242 )\n\n* [Bugfix][PD] Set conclude state before clear when failure happens ( sgl-project#7362 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* docs: update installation ( sgl-project#7366 )\n\n* [Docker] optimize dockerfile  remove deepep and blackwell merge it to\u2026 ( sgl-project#7343 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* Clean unused import for mimo mtp model ( sgl-project#7370 )\n\n* [Bugfix]Fix hang bug using dp attention with HiRadixCache ( sgl-project#7159 )\n\nSigned-off-by: huanglong <huanglong@linux.alibaba.com>\n\n* [Doc] add embedding rerank doc ( sgl-project#7364 )\n\n* Fix judgment condition for enabling Deepseek V3/R1 shared expert fusion optimization ( sgl-project#7371 )\n\n* Feat/refactor embedding server ( sgl-project#7322 )\n\n* Purge VerlEngine ( sgl-project#7326 )\n\nSigned-off-by: Ata Fatahi <immrata@gmail.com>\n\n* support return logprobs for pipeline ( sgl-project#7356 )\n\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\n\n* [PD] Optimize custom mem pool usage and bump mooncake version ( sgl-project#7393 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Support THUDM/GLM-4-0414 (GLM-Z1) Glm4ForCausalLM architecture. ( sgl-project#5485 )\n\n* Refine OpenAI serving entrypoint to remove batch requests ( sgl-project#7372 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: Chang Su <csu272@usc.edu>\n\n* [Feature] Comprehensive Hybrid Parallelism Support ( sgl-project#6389 )\n\n* [DeepSeekNextN] fix: residual of head norm can be None ( sgl-project#7398 )\n\n* [OAI refactor] Add rerank and score serving ( sgl-project#7399 )\n\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\n\n* [OAI Server Refactor] [ChatCompletions & Completions] Implement UsageInfo Processor ( sgl-project#7360 )\n\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\n\n* Fix All-Gather under world size one ( sgl-project#7219 )\n\n* Optimize DP attn scheduling for speculative decoding ( sgl-project#7285 )\n\n* Update usage_processor.py ( sgl-project#7402 )\n\n* Fix 7285 Merge Conflicts ( sgl-project#7403 )\n\n* chore: upgrade mooncake-transfer-engine 0.3.4 ( sgl-project#7401 )\n\n* [OAI Server Refactor] [ChatCompletions & Completions] Support Return Hidden State ( sgl-project#7329 )\n\nSigned-off-by: keru <rukeyang@gmail.com>\n\n* Remove batches api in docs & example ( sgl-project#7400 )\n\n* [BugFix]: fix EmbeddingReqInput single input error ( sgl-project#7396 )\n\n* [BugFix]fix qwen25 invoke function call streaming responses with curly braces as the starting indicator ( sgl-project#7394 )\n\n* fix overlap pagecount ( sgl-project#6984 )\n\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* fix: Fix CI test_function_call_parser.py ( sgl-project#7425 )\n\n* Fix CPU offloading for MLA memory pool ( sgl-project#7409 )\n\n* [fix] PD disaggregation when enable mtp and tp!=dp ( sgl-project#7420 )\n\n* feat(oai refactor): Replace `openai_api` with `entrypoints/openai`  ( sgl-project#7351 )\n\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\n\n* Refactor LoRAManager and LoRAMemoryPool state management logic for dynamic LoRA loading support ( sgl-project#7412 )\n\n* refactor(test): reorganize OpenAI test file structure ( sgl-project#7408 )\n\n* [minor] simplify the `TokenToKVPoolAllocator` ( sgl-project#7414 )\n\n* Tiny add logging for GC  ( sgl-project#7406 )\n\n* FlashInfer NVFP4 MoE with EP & 2-stream shared expert ( sgl-project#7327 )\n\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: alcanderian <alcanderian@gmail.com>\n\n* Remove copy after bmm ( sgl-project#7441 )\n\n* Fix torch compile run ( sgl-project#7391 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: Sai Enduri <saimanas.enduri@amd.com>\n\n* [misc] Add PD service discovery support in router ( sgl-project#7361 )\n\n* add fused moe config for qwen3 in triton3.3.1 ( sgl-project#7445 )\n\n* Fix CUDA Graph Check under Deepep with DP FFN ( sgl-project#7451 )\n\n* Update hyperparameter_tuning.md ( sgl-project#7454 )\n\n* feat: integrate deepgemm into EPMoE ( sgl-project#6821 )\n\nCo-authored-by: tianqilin.99 <tianqilin.99@bytedance.com>\nCo-authored-by: TianQiLin666666 <1834987979@qq.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Solve docker build failed in the virtual machine ( sgl-project#7290 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: Sai Enduri <saimanas.enduri@amd.com>\nCo-authored-by: HAI <hixiao@gmail.com>\n\n* Fix a bug in BatchTokenIDOut & Misc style and dependency updates ( sgl-project#7457 )\n\n* [CI] Upgrade mooncake to 0.3.4.post1 to fix 8 gpu tests ( sgl-project#7472 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Fix prefill OOM due to wrong token calculation when page > 1  ( sgl-project#7397 )\n\n* feat(func_call): Add more check in `BaseFormatDetector.parse_streaming_increment` ( sgl-project#7479 )\n\n* Fix dtype for idle input in spec decoding ( sgl-project#7456 )\n\n* update mooncake in dockerfile ( sgl-project#7480 )\n\n* kvcache io kernels and test case ( sgl-project#7382 )\n\n* [perf] slightly imporve DeepSeek-R1-FP4 TP8 ( sgl-project#7481 )\n\n* Quick fix for DeepGemm requant to also cover MTP. ( sgl-project#7378 )\n\n* Support weight loading without mmap ( sgl-project#7469 )\n\n* ci: Revert openai_server related tests in AMD suites ( sgl-project#7449 )\n\n* Perormance: Enable cuda graph for dp idle batch ( sgl-project#7269 )\n\nCo-authored-by: austindeng <austindeng@tencent.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* bugfix: Prevent global mutation of conv.stop_str across requests ( sgl-project#7347 )\n\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\n\n* Fix RequestValidationError response format ( sgl-project#7487 )\n\n* Fix MTP with Deepseek R1 Fp4 ( sgl-project#7376 )\n\n* chore: bump sgl-kernel v0.2.0 ( sgl-project#7490 )\n\n* chore: bump v0.4.8 ( sgl-project#7493 )\n\n* [AMD] add aiter fused moe in DeepEP path ( sgl-project#7268 )\n\n* enable aiter_biased_grouped_topk kernel ( sgl-project#7423 )\n\n* [PD Disaggregation] replace transfer with batch transfer for better performance ( sgl-project#7236 )\n\n* Remove cumsum_buffer initilization ( sgl-project#7439 )\n\n* [benchmark] fbgemm benchmark support bandwidth report and support fbgemm_cutlass_gmm ( sgl-project#7422 )\n\n* Support multi-thread model weight loading ( sgl-project#7277 )\n\n* [PD] NIXL: Register kv args in advance and cleanup finished requests ( sgl-project#6717 )\n\n* fix: Add `--model` as an alias for `--model-path` in server_args ( sgl-project#7505 )\n\n* misc: Improvement to serving_chat.py and add more ut ( sgl-project#7489 )\n\n* Fuse sorted_token_ids padding to moe_align_block_size kernel ( sgl-project#7437 )\n\n* [OAI] patch origin request_id logic ( sgl-project#7508 )\n\n* [PD][Spec] Fix hidden state transfer for spec decode ( sgl-project#7516 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* EPLB support for MTP ( sgl-project#7510 )\n\n* clean duplicate code ( sgl-project#7512 )\n\n* [ci] add router benchmark script and CI ( sgl-project#7498 )\n\n* fix: force synchronization between TP workers when update_weights ( sgl-project#6626 )\n\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\n\n* [CPU] [BF16] Call fused_experts_cpu, weight_packed_linear and bmm_cpu kernel in DeepSeek model ( sgl-project#6641 )\n\nCo-authored-by: Thien Tran <gau.nernst@yahoo.com.sg>\n\n* [CI] Upgrade mooncake to v0.3.4.post2 to fix potential slice failed bug ( sgl-project#7522 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* npu fused op ( sgl-project#7386 )\n\nCo-authored-by: Li Junwen <lijunwen13@hisilicon.com>\n\n* feat: send kvmetrics from sglang scheduler ( sgl-project#6721 )\n\n* [PD] Add different TP sizes support for no-MLA models ( sgl-project#6793 )\n\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* enable aiter fp8 blockscale quant ( sgl-project#7520 )\n\n* take aiter get_rope back ( sgl-project#7521 )\n\n* Fix typo of flash_cache ( sgl-project#7513 )\n\n* feat: add return hidden_states at async generation ( sgl-project#7507 )\n\n* minor: 'role' must be system/assistant/tool, but case insensitive for now ( sgl-project#7499 )\n\n* Fix FP8 KV Cache Support in FA3 Backend ( sgl-project#7148 )\n\n* Fix gathered_buffer issues in tbo ( sgl-project#7531 )\n\n* [PD] Raise error for incompatible mooncake version and some minor fixes ( sgl-project#7527 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CMake] Fix sgl-kernel CMakeLists for Blackwell ( sgl-project#7543 )\n\n* Add Tencent HunYuanMoEV1 model support ( sgl-project#7549 )\n\n* Update seed in CPU UTs to avoid flaky failure with single test ( sgl-project#7544 )\n\n* chore: improve ci bug reporting ( sgl-project#7542 )\n\n* chore: remove vlm unnecessary import ( sgl-project#7541 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: yhyang201 <yhyang201@gmail.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\n\n* chore: bump v0.4.8.post1 ( sgl-project#7559 )\n\n* [PD][NIXL] Set is_sorted=False to fix NIXL_ERR_NOT_FOUND ( sgl-project#7330 )\n\n* [Fix] incorrect assert in EPLB ( sgl-project#7575 )\n\n* Updates Gemma3n MLP layer to adapt latest transformers version ( sgl-project#7573 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Fix MTP error when enabling two-batch overlap  ( sgl-project#7569 )\n\n* Add e2e test for multi instance multi stage memory release/resume occupuation ( sgl-project#7208 )\n\nSigned-off-by: Ata Fatahi <immrata@gmail.com>\n\n* [CI] Add CI Testing for Prefill-Decode Disaggregation with Router ( sgl-project#7540 )\n\n* Updates transformers and timm dependencies ( sgl-project#7577 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* feat: support compatibility between MTP and two-batch-overlap ( sgl-project#7225 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Move multimodal processors into a separate folder ( sgl-project#7581 )\n\n* Fix broken CI TestVILAServer ( sgl-project#7610 )\n\n* [router] add centralized configuration module for sgl-router ( sgl-project#7588 )\n\n* Fix: Minicpm ( sgl-project#7612 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Hybrid kv cache for LLaMA4 ( sgl-project#6563 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: tarinkk <rt572@physics.rutger.edu>\nCo-authored-by: tarinkk <rt572@rutgers.physics.edu>\nCo-authored-by: Hanming Lu <69857889+hanming-lu@users.noreply.github.com>\n\n* [CPU] add optimizations for INT8 and FP8 DeepSeek ( sgl-project#6769 )\n\nCo-authored-by: Zheng, Beilei <beilei.zheng@intel.com>\n\n* Tiny add logs for expert location updater ( sgl-project#7308 )\n\n* Fix flakiness in LoRA batch test. ( sgl-project#7552 )\n\n* [BUG] fix local_rank in initialize_dp_attention ( sgl-project#7584 )\n\n* Support dynamic LoRA loading / unloading in engine/server API ( sgl-project#7446 )\n\n* [PD] Respect sampling_params.max_new_tokens when PD disaggregation is activated ( sgl-project#7598 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* fix unit tests ( sgl-project#7618 )\n\n* Let ep_scatter support arbitrary strides / ue8m0 format ( sgl-project#7309 )\n\n* Let EP prefill support new DeepGEMM ( sgl-project#7310 )\n\n* docs: add gb200 nvl72 and a16z grant ( sgl-project#7620 )\n\n* oai: Adds support for OpenAI chat completions API in bench_serving ( sgl-project#7036 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\n\n* [bugfix] Remove PR comment posting from Rust benchmark workflow ( sgl-project#7625 )\n\n* [Minor] clean up multimodal processor and tokenizer manager ( sgl-project#7624 )\n\n* Add dsv3 fused a gemm to sgl-kernel ( sgl-project#7630 )\n\n* Add @mickqian as the CODEOWNERS of multimodal ( sgl-project#7636 )\n\n* Fix stream reasoning parser and Adds Kimi reasoning parser  ( sgl-project#7432 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Fix sgl-router startup crash ( sgl-project#7619 )\n\n* [bugfix] fix runtime dropping panic in editable ( sgl-project#7628 )\n\n* Move files related to EPLB ( sgl-project#7580 )\n\n* [misc] reduce weird rope_scaling_factor warning ( sgl-project#7176 )\n\n* [AMD] Add unit-test-sgl-kernel-amd to AMD CI ( sgl-project#7539 )\n\n* Update CODEOWNERS ( sgl-project#7640 )\n\n* [EAGLE] remove a wrong adjustment for page_size > 1 & topk > 1 in server_args.py ( sgl-project#7643 )\n\n* [CPU] add c++ kernel to bind CPU cores and memory node ( sgl-project#7524 )\n\n* Improve streaming, log_level, memory report, weight loading, and benchmark script ( sgl-project#7632 )\n\nCo-authored-by: Kan Wu <wukanustc@gmail.com>\n\n* Add dsv3 router gemm kernel ( sgl-project#7627 )\n\n* chore: upgrade flashinfer v0.2.7 jit ( sgl-project#7663 )\n\n* [doc] update lws doc for pd ( sgl-project#7318 )\n\n* Fix: sync prepare_fp8_layer_for_marlin with latest vllm changes ( sgl-project#7648 )\n\n* Add small requirements for benchmark/parse_result tools ( sgl-project#7671 )\n\n* [CPU] remove process_group from inputs of shm_allreduce and shm_allgather ( sgl-project#7486 )\n\n* chore: bump sgl-kernel v0.2.1 ( sgl-project#7675 )\n\n* support llama4 eagle3  ( sgl-project#6985 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Shenggui Li <somerlee.9@gmail.com>\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: yizhang2077 <1109276519@qq.com>\n\n* Refactor mm processors and Enable mixed modality processing ( sgl-project#7629 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* upgrade sgl kernel to 0.2.1 for main ( sgl-project#7676 )\n\n* add description for llama4 eagle3 ( sgl-project#7688 )\n\n* fix(model loader): use safe_open to prevent file handle leaks. ( sgl-project#7684 )\n\n* chore: upgrade flashinfer v0.2.7.post1 ( sgl-project#7698 )\n\n* Improve error handling for requests with unloaded LoRA path(s) ( sgl-project#7642 )\n\n* Apply dsv3_fused_a_gemm kernel ( sgl-project#7635 )\n\n* Fix GPTQMarlinMoE ( sgl-project#7697 )\n\n* [1/n] apply wna16marlin kernel in moe weight only quantization ( sgl-project#7683 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: yych0745 <1398089567@qq.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: \u5f0b\u4e91 <yiyun.wyt@antgroup.com>\nCo-authored-by: walker-ai <2398833647@qq.com>\n\n* Apply dsv3 router gemm kernel for deepseek-r1 fp4 ( sgl-project#7677 )\n\n* [AMD] Temporarily disable test_no_overlap_scheduler and test_vision_chunked_prefill ( sgl-project#7717 )\n\n* [RL] add --skip-warmup ( sgl-project#7416 )\n\n* [RL] support update_weights_from_distributed with different group and multiple weights ( sgl-project#7292 )\n\n* [router] add --log-level to sgl-router ( sgl-project#6512 )\n\n* [b200] support trt-llm allreduce fuse rms_norm_add kernel ( sgl-project#7621 )\n\n* [CPU] Bind threads and numa node for each TP rank ( sgl-project#6549 )\n\nCo-authored-by: srinarayan-srikanthan <srinarayan.srikanthan@intel.com>\n\n* Support non-contiguous query input for extend/decode attention ( sgl-project#7462 )\n\n* Support updating weights at once by stopping all requests ( sgl-project#6698 )\n\nSigned-off-by: Tianyu Zhou <albert.zty@antgroup.com>\nCo-authored-by: Zilin Zhu <zhuzilinallen@gmail.com>\n\n* Fix num_tokens_pre_allocated in disaggregation log ( sgl-project#7714 )\n\n* [CPU] [sgl-kernel] set dispatch key of initialize to CatchAll ( sgl-project#7734 )\n\n* [CPU] fix all_reduce and all_gather ( sgl-project#6770 )\n\nCo-authored-by: blzheng <beilei.zheng@intel.com>\n\n* fix awq and dsv3 fused gemm compatible ( sgl-project#7735 )\n\n* [CI][Router] Fix bench_one_batch_server for pd router test ( sgl-project#7731 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Add CUTLASS FP8 Blockscale MoE kernel for Hopper architecture ( sgl-project#7278 )\n\nCo-authored-by: HydraQYH <QYH820@Outlook.com>\nCo-authored-by: TianQiLin666666 <1834987979@qq.com>\n\n* fix dsv3 fused proj check  ( sgl-project#7738 )\n\n* Ascend attention backend(PA&MLA) ( sgl-project#7722 )\n\nCo-authored-by: Maksim <makcum888e@mail.ru>\nCo-authored-by: VDV1985 <vladdv85@mail.ru>\n\n* [fix] fix dsv3_router_gemm filter ( sgl-project#7750 )\n\n* [CPU] refine CPU integration code ( sgl-project#7647 )\n\n* [CPU] support the case where num_attention_heads or intermediate_size is not divisible by the TP size ( sgl-project#6771 )\n\n* support qwen3 dense model dp attention ( sgl-project#7681 )\n\n* [optimize] add two stream norm for qwen3 ( sgl-project#7740 )\n\nCo-authored-by: ispobock <ispobaoke@gmail.com>\n\n* feat: use D2D instead of H2H in pp ( sgl-project#7673 )\n\nCo-authored-by: alpha-baby <fujianhao1997@qq.com>\n\n* [Bug] add flashinfer bool check for fusedmoe in Qwen moe models ( sgl-project#7723 )\n\n* [fix] put cpu in the first priority in get_device() ( sgl-project#7752 )\n\n* [optimize] fuse renormalize into moe_topk_softmax ( sgl-project#7744 )\n\nCo-authored-by: ispobock <ispobaoke@gmail.com>\n\n* chore: bump sgl-kernel 0.2.2 ( sgl-project#7755 )\n\n* fix CI: update native api ipynb ( sgl-project#7754 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* fuse renormal into moe topk softmax kernel python code ( sgl-project#7751 )\n\nCo-authored-by: ispobock <ispobaoke@gmail.com>\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* Remove type conversion and fix id map in topk ( sgl-project#7759 )\n\n* Add V2-lite model test ( sgl-project#7390 )\n\nCo-authored-by: DiweiSun <105627594+DiweiSun@users.noreply.github.com>\n\n* refactor llama4 dp attention logic ( sgl-project#7729 )\n\n* fix(docs): fix the broken link in `docs/references/production_metrics.md` ( sgl-project#7741 )\n\nSigned-off-by: rudeigerc <rudeigerc@gmail.com>\n\n* [fix] update bench_speculative.py for compatibility ( sgl-project#7764 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* Move mem_fraction_static adjustment for multimodal models to `server_args.py` & Fix session control & Other cleanups ( sgl-project#7748 )\n\n* [RL] Add --nccl-port to prevent port conflict ( sgl-project#7418 )\n\n* [RL] add pause and continue generation for async rl training ( sgl-project#7419 )\n\n* [Fix] Alloc return type error ( sgl-project#7778 )\n\nSigned-off-by: Capronir <839972205@qq.com>\n\n* [feat] Support EAGLE3 for Qwen ( sgl-project#7745 )\n\nCo-authored-by: \u7eac\u676d <ximing.wxm@antgroup.com>\nCo-authored-by: zyksir <zyksir@outlook.com>\n\n* saving hidden_states.clone() ( sgl-project#7705 )\n\n* [1/n]: add cutlass W4A8 moe kernel for hopper architecture ( sgl-project#7772 )\n\nSigned-off-by: yangsijia.614 <yangsijia.614@bytedance.com>\nCo-authored-by: yicwang <yichen.wang@bytedance.com>\n\n* add model: qwen2-audio ( sgl-project#7596 )\n\n* Optimize Hopper CUTLASS FP8 Blockwise Grouped GEMM Kernel in Small K Scenario ( sgl-project#7782 )\n\n* Embedding parallel by attn_tp ( sgl-project#7623 )\n\n* fix: fix apply_shuffle_mul_sum ( sgl-project#7444 )\n\n* chore: bump sgl-kernel v0.2.3 ( sgl-project#7784 )\n\n* fix: use nvidia-nccl-cu12 2.27.5 ( sgl-project#7787 )\n\n* DP Attention with Auto DeepEP Dispatch ( sgl-project#7222 )\n\n* chore: upgrade sgl-kernel v0.2.3 ( sgl-project#7786 )\n\n* Fix incorrect spec_num_draft_tokens in draft_extend ( sgl-project#7757 )\n\n* [fix] fix misusing of is_cuda ( sgl-project#7790 )\n\n* Add treemask mode to build_eagle_tree & release sgl-kernel 0.2.3 ( sgl-project#7756 )\n\nCo-authored-by: Pranjal Shankhdhar <pranjal.ssh@gmail.com>\n\n* chore: bump sgl-kernel v0.2.4 ( sgl-project#7800 )\n\n* ci: fix port args ( sgl-project#7792 )\n\n* Fix CI test OOM issue. ( sgl-project#7799 )\n\n* chore: upgrade sgl-kernel v0.2.4 ( sgl-project#7801 )\n\n* chore: bump v0.4.9 ( sgl-project#7802 )\n\n* fix merge conflict issue\n\n* fix hpu attention nonetyep issue\n\n* fix alignment\n\n* fix alignment2\n\n* Ci failure fixes\n\n* fix attention-backend choices\n\n---------\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: ch-tiger1 <xyz@ch-tech.ip-ddns.com>\nSigned-off-by: huanglong <huanglong@linux.alibaba.com>\nSigned-off-by: Ata Fatahi <immrata@gmail.com>\nSigned-off-by: keru <rukeyang@gmail.com>\nSigned-off-by: Tianyu Zhou <albert.zty@antgroup.com>\nSigned-off-by: rudeigerc <rudeigerc@gmail.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: Capronir <839972205@qq.com>\nSigned-off-by: yangsijia.614 <yangsijia.614@bytedance.com>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: KavioYu <67678385+yukavio@users.noreply.github.com>\nCo-authored-by: kavioyu <kavioyu@tencent.com>\nCo-authored-by: Xinyuan Tong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: kk <43161300+kkHuang-amd@users.noreply.github.com>\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: DiweiSun <105627594+DiweiSun@users.noreply.github.com>\nCo-authored-by: u4lr451 <u4lr451@gmail.com>\nCo-authored-by: austindeng <austindeng@tencent.com>\nCo-authored-by: tianqilin.99 <tianqilin.99@bytedance.com>\nCo-authored-by: Qiaolin Yu <liin1211@outlook.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\nCo-authored-by: Yijie Zhu <762412795@qq.com>\nCo-authored-by: \u5201\u83b9\u715c <diaoyingyu1@hisilicon.com>\nCo-authored-by: Charles Chen <pychen96@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: Wu, Chunyuan <chunyuan.wu@intel.com>\nCo-authored-by: jianan-gu <jianan.gu@intel.com>\nCo-authored-by: sdp <sdp@gnr799219.jf.intel.com>\nCo-authored-by: Binyao Jiang <byjiang1996@gmail.com>\nCo-authored-by: ishandhanani <82981111+ishandhanani@users.noreply.github.com>\nCo-authored-by: linzhuo <15313137931lz@gmail.com>\nCo-authored-by: ch-tiger1 <tiger@ch-tech.ip-ddns.com>\nCo-authored-by: ch-tiger1 <xyz@ch-tech.ip-ddns.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Simo Lin <linsimo.mark@gmail.com>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: DarkSharpness <76582120+DarkSharpness@users.noreply.github.com>\nCo-authored-by: Atream <80757050+Atream@users.noreply.github.com>\nCo-authored-by: Li Hui <lambert80.ios@gmail.com>\nCo-authored-by: Huang Long <121648372+LLLL114@users.noreply.github.com>\nCo-authored-by: woodx <124784234+woodx9@users.noreply.github.com>\nCo-authored-by: Ata Fatahi <immrata@gmail.com>\nCo-authored-by: strgrb <zhangkaihong.zkh@antgroup.com>\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\nCo-authored-by: Wenbo Yang <solrex@users.noreply.github.com>\nCo-authored-by: Chang Su <csu272@usc.edu>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Keyang Ru <rukeyang@gmail.com>\nCo-authored-by: ehuaa <ehuamail@163.com>\nCo-authored-by: pansicheng <sicheng.pan.chn@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: alcanderian <alcanderian@gmail.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Sai Enduri <saimanas.enduri@amd.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: xutizhou <xutingz@nvidia.com>\nCo-authored-by: TianQiLin666666 <1834987979@qq.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: Yuhong Guo <guoyuhong1985@outlook.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: Alex Sun <alex.s@amd.com>\nCo-authored-by: valarLip <103567126+valarLip@users.noreply.github.com>\nCo-authored-by: Francis <38564764+ssssnow@users.noreply.github.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: xianzhiT <xianzhitang@tencent.com>\nCo-authored-by: yilian49 <43861414+yilian49@users.noreply.github.com>\nCo-authored-by: DangKai <dangkai4u@outlook.com>\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\nCo-authored-by: Thien Tran <gau.nernst@yahoo.com.sg>\nCo-authored-by: ll819214 <18801269230@163.com>\nCo-authored-by: Li Junwen <lijunwen13@hisilicon.com>\nCo-authored-by: zixuanzhang226 <zixuanzhang@bytedance.com>\nCo-authored-by: Hongbo Xu <1320612015@qq.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: eigen <52445717+yyihuang@users.noreply.github.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: Ruihang Lai <ruihangl@cs.cmu.edu>\nCo-authored-by: Meng, Peng <pengmeng@tencent.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: yhyang201 <yhyang201@gmail.com>\nCo-authored-by: tarinkk <129432511+tarinkk@users.noreply.github.com>\nCo-authored-by: tarinkk <rt572@physics.rutger.edu>\nCo-authored-by: tarinkk <rt572@rutgers.physics.edu>\nCo-authored-by: Hanming Lu <69857889+hanming-lu@users.noreply.github.com>\nCo-authored-by: Zheng, Beilei <beilei.zheng@intel.com>\nCo-authored-by: Sheng Qi <shengqi2018@pku.edu.cn>\nCo-authored-by: finetune <82650881+finetunej@users.noreply.github.com>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: Kan Wu <wukanustc@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: narutolhy <582909902@qq.com>\nCo-authored-by: lukec <118525388+sleepcoo@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Shenggui Li <somerlee.9@gmail.com>\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Simon_CQK <cqk0100@gmail.com>\nCo-authored-by: Kyungmin Lee <30465912+lkm2835@users.noreply.github.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: yych0745 <1398089567@qq.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: \u5f0b\u4e91 <yiyun.wyt@antgroup.com>\nCo-authored-by: walker-ai <2398833647@qq.com>\nCo-authored-by: Zilin Zhu <zhuzilinallen@gmail.com>\nCo-authored-by: srinarayan-srikanthan <srinarayan.srikanthan@intel.com>\nCo-authored-by: Albert <albert.zty@antgroup.com>\nCo-authored-by: Ziming Huang <1520787127@qq.com>\nCo-authored-by: ayrnb <70835312+ayrnb@users.noreply.github.com>\nCo-authored-by: HydraQYH <QYH820@Outlook.com>\nCo-authored-by: ronnie_zheng <zl19940307@163.com>\nCo-authored-by: Maksim <makcum888e@mail.ru>\nCo-authored-by: VDV1985 <vladdv85@mail.ru>\nCo-authored-by: ispobock <ispobaoke@gmail.com>\nCo-authored-by: TianyuZhang1214 <tianyuzhang1214@163.com>\nCo-authored-by: alpha-baby <fujianhao1997@qq.com>\nCo-authored-by: Yuchen Cheng <rudeigerc@gmail.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: Caproni <40862361+Capronir@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: \u7eac\u676d <ximing.wxm@antgroup.com>\nCo-authored-by: zyksir <zyksir@outlook.com>\nCo-authored-by: SijiaYang <yangsijia.614@bytedance.com>\nCo-authored-by: yicwang <yichen.wang@bytedance.com>\nCo-authored-by: Leng Yue <lengyue@lengyue.me>\nCo-authored-by: Qi Yuhang <45795032+HydraQYH@users.noreply.github.com>\nCo-authored-by: Gang Chen <13298548+MoonBall@users.noreply.github.com>\nCo-authored-by: Pranjal Shankhdhar <pranjal.ssh@gmail.com>\nCo-authored-by: jay <jthakur@habana.ai> DiweiSun pushed a commit\n        to DiweiSun/sglang\n      that referenced\n      this pull request Jul 18, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 05fec4c \u2026better performance ( sgl-project#7236 )\" ( sgl-project#7968 ) shuaills pushed a commit\n        to shuaills/sglang\n      that referenced\n      this pull request Jul 21, 2025 [PD Disaggregation] replace transfer with batch transfer for better p\u2026 \u2026 3ba79c1 \u2026erformance ( sgl-project#7236 ) shuaills pushed a commit\n        to shuaills/sglang\n      that referenced\n      this pull request Jul 21, 2025 Revert \"[PD Disaggregation] replace transfer with batch transfer for \u2026 \u2026 49ae622 \u2026better performance ( sgl-project#7236 )\" ( sgl-project#7968 ) fzyzcjy added a commit\n        to fzyzcjy/sglang\n      that referenced\n      this pull request Jul 22, 2025 Revert \"Revert \"[PD Disaggregation] replace transfer with batch trans\u2026 \u2026 f2e60f6 \u2026fer for better performance ( sgl-project#7236 )\" ( sgl-project#7968 )\"\n\nThis reverts commit 880221b . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:22",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[PD Disaggregation] replace transfer with batch transfer for better performance (#7236)",
  "commit_message": "[PD Disaggregation] replace transfer with batch transfer for better performance (#7236)",
  "commit_date": "2025-06-24T02:12:04-07:00",
  "files_changed": [
    "python/sglang/srt/disaggregation/mooncake/conn.py",
    "python/sglang/srt/disaggregation/mooncake/transfer_engine.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 42,
    "num_files": 2,
    "num_hunks": 3,
    "num_non_test_edited_lines": 42,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/disaggregation/mooncake/conn.py b/python/sglang/srt/disaggregation/mooncake/conn.py\nindex 29e861e9f..92e182dfd 100644\n--- a/python/sglang/srt/disaggregation/mooncake/conn.py\n+++ b/python/sglang/srt/disaggregation/mooncake/conn.py\n@@ -251,17 +251,19 @@ class MooncakeKVManager(BaseKVManager):\n \n         # Worker function for processing a single layer\n         def process_layer(src_ptr: int, dst_ptr: int, item_len: int) -> int:\n+            src_addr_list = []\n+            dst_addr_list = []\n+            length_list = []\n             for prefill_index, decode_index in zip(prefill_kv_blocks, dst_kv_blocks):\n                 src_addr = src_ptr + int(prefill_index[0]) * item_len\n                 dst_addr = dst_ptr + int(decode_index[0]) * item_len\n                 length = item_len * len(prefill_index)\n-\n-                status = self.engine.transfer_sync(\n-                    mooncake_session_id, src_addr, dst_addr, length\n-                )\n-                if status != 0:\n-                    return status\n-            return 0\n+                src_addr_list.append(src_addr)\n+                dst_addr_list.append(dst_addr)\n+                length_list.append(length)\n+            return self.engine.batch_transfer_sync(\n+                mooncake_session_id, src_addr_list, dst_addr_list, length_list\n+            )\n \n         futures = [\n             executor.submit(\ndiff --git a/python/sglang/srt/disaggregation/mooncake/transfer_engine.py b/python/sglang/srt/disaggregation/mooncake/transfer_engine.py\nindex 5643af70b..966f7152c 100644\n--- a/python/sglang/srt/disaggregation/mooncake/transfer_engine.py\n+++ b/python/sglang/srt/disaggregation/mooncake/transfer_engine.py\n@@ -1,7 +1,7 @@\n import json\n import logging\n from dataclasses import dataclass\n-from typing import Optional\n+from typing import List, Optional\n \n logger = logging.getLogger(__name__)\n \n@@ -90,5 +90,29 @@ class MooncakeTransferEngine:\n \n         return ret\n \n+    def batch_transfer_sync(\n+        self,\n+        session_id: str,\n+        buffers: List[int],\n+        peer_buffer_addresses: List[int],\n+        lengths: List[int],\n+    ) -> int:\n+        \"\"\"Synchronously transfer data to the specified address.\"\"\"\n+        try:\n+            ret = self.engine.batch_transfer_sync_write(\n+                session_id, buffers, peer_buffer_addresses, lengths\n+            )\n+        except Exception:\n+            ret = -1\n+\n+        if ret < 0:\n+            logger.debug(\n+                \"Failed to batch transfer data. Buffers: %s, Session: %s, Peer addresses: %s\",\n+                buffers,\n+                session_id,\n+                peer_buffer_addresses,\n+            )\n+        return ret\n+\n     def get_session_id(self):\n         return self.session_id",
  "apis": [
    "MooncakeKVManager.send_kvcache",
    "MooncakeTransferEngine.batch_transfer_sync"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/nixl/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/mooncake/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/ascend/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/common/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/fake/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/base/conn.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/mooncake/transfer_engine.py",
    "/path/to/repos/sglang/python/sglang/srt/disaggregation/ascend/transfer_engine.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The patch replaces individual synchronous transfers with a batch transfer method. Instead of looping over individual transfers and calling self.engine.transfer_sync repeatedly, it accumulates address and length parameters into lists and calls a new API function, batch_transfer_sync, which then internally calls engine.batch_transfer_sync_write. This change is made in source code files (non-test) and is directly aimed at enhancing performance by reducing overhead of multiple synchronous calls. The commit message explicitly mentions better performance, and the change affects key low-level functionality by grouping operations, which should lead to improved performance of the top-level API indirectly. Thus, it qualifies as a performance optimization.",
  "llm_api_reason": "The commit replaces the per-transfer data movement in MooncakeKVManager\u2019s send_kvcache worker loop with a batch transfer approach. Instead of looping and calling transfer_sync for each chunk, it aggregates source addresses, destination addresses, and lengths and then calls a new batch_transfer_sync API exposed in MooncakeTransferEngine. This change should improve performance by minimizing transfer overhead and reducing per-call latency."
}