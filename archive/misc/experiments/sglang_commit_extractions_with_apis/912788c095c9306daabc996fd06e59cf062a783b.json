{
  "commit_hash": "912788c095c9306daabc996fd06e59cf062a783b",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6273",
  "pr_date": "2025-05-14",
  "timeline_text": "Copy link Collaborator CatherineSue commented May 13, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation The current implementation of the local attention mechanism in FlashAttentionBackend allocates significantly more memory than necessary for the local_block_table tensor. This over-allocation becomes particularly problematic for models with large context lengths, leading to unnecessary GPU memory usage. The current allocation creates a tensor with dimensions (max_virtual_batches, max_blocks_per_seq * max_pages_per_block) , but the actual usage in the make_local_attention_virtual_batches function only requires dimensions of (virtual_batches, pages_per_local_batch) . For example, with a context length of 512K and attention chunk size of 8192, the current allocation is 65 times larger than necessary. This is because max_blocks_per_seq scales linearly with the context length. As a result, Llama4-Maverick-FP8 on 8*H100 will OOM as it tries to allocate 20.00 GiB. Modifications Reduce memory usage for local attention by allocating the tensor local_block_table from (max_virtual_batches, max_blocks_per_seq * max_pages_per_block) to (max_virtual_batches, max_pages_per_block) Evaludation Results sglang git:(chang/llama4-opt) \u2717 python3 -m sglang.eval.loogle_eval --api-url=http://127.0.0.1:8080/v1\nRunning benchmark: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1101/1101 [00:00<00:00, 84625.50it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading responses: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1101/1101 [00:00<00:00, 5563.29it/s]\nScoring batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [02:16<00:00,  7.57s/it]\nAverage BERTScore (F1): 84.40% Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, ispobock, and slin1237 reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions perf: optimize local_block_table memory allocation \u2026 5c37691 - Reduce memory usage for local attention by allocating only the\nnecessary tensor size for local_block_table. The previous allocation\nwas using max_blocks_per_seq times more memory than needed.\n\nThis change can significantly reduce GPU memory usage for models with\nlarge context lengths without affecting functionality. CatherineSue requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw and ch-wan as code owners May 13, 2025 23:39 zhyncs assigned zhyncs , ispobock and ch-wan May 13, 2025 zhyncs added\n  the high priority label May 13, 2025 zhyncs approved these changes May 13, 2025 View reviewed changes Hide details View details zhyncs merged commit 912788c into main May 14, 2025 47 of 52 checks passed Uh oh! There was an error while loading. Please reload this page . zhyncs deleted the chang/llama4-opt branch May 14, 2025 00:18 zhyncs pushed a commit\n      that referenced\n      this pull request May 14, 2025 perf: optimize local_block_table memory allocation ( #6273 ) 755fdcb lifuhuang pushed a commit\n        to lifuhuang/sglang\n      that referenced\n      this pull request May 17, 2025 perf: optimize local_block_table memory allocation ( sgl-project#6273 ) c027f54 Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 perf: optimize local_block_table memory allocation ( sgl-project#6273 ) 6a35131 xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 perf: optimize local_block_table memory allocation ( sgl-project#6273 ) 1a8c259 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:44",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "Llama4-Maverick-FP8"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=Llama4-Maverick-FP8,dtype=float8 --tasks gsm8k --limit 100"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model Llama4-Maverick-FP8 --num-prompts 100",
  "commit_subject": "perf: optimize local_block_table memory allocation (#6273)",
  "commit_message": "perf: optimize local_block_table memory allocation (#6273)",
  "commit_date": "2025-05-13T17:18:38-07:00",
  "files_changed": [
    "python/sglang/srt/layers/attention/flashattention_backend.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 3,
    "num_files": 1,
    "num_hunks": 2,
    "num_non_test_edited_lines": 3,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex f200a367b..2f974ea9a 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1165,7 +1165,6 @@ class FlashAttentionBackend(AttentionBackend):\n             max_virtual_batches = max_bs * (\n                 (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             )\n-            max_blocks_per_seq = (max_seq_len + attn_chunk_size - 1) // attn_chunk_size\n             max_pages_per_block = (attn_chunk_size + page_size - 1) // page_size\n \n             self.decode_cuda_graph_local_attn_metadata = {\n@@ -1177,7 +1176,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 ),\n                 \"local_block_table\": torch.zeros(\n                     max_virtual_batches,\n-                    max_blocks_per_seq * max_pages_per_block,\n+                    max_pages_per_block,\n                     dtype=torch.int32,\n                     device=self.device,\n                 ),",
  "apis": [
    "sglang.srt.layers.attention.FlashAttentionBackend"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/attention/flashattention_backend.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a source file in the attention module, which is a non-test file. The change removes an unnecessary multiplication in the dimension of the \"local_block_table\" tensor allocation, thus directly reducing the memory footprint and potentially improving performance by reducing resource usage. Although the commit message mentions performance (\"perf: optimize local_block_table memory allocation\"), the code change indeed refines memory allocation for a key internal API, and it is not merely a refactor or bug fix. Additionally, the change is applicable on CPU and does not target GPU-specific hardware optimizations. Hence, this commit satisfies the conditions for being a performance or optimization-related change.",
  "llm_api_reason": "The commit optimizes memory allocation in the FlashAttentionBackend by reducing the dimensions allocated for the local_block_table within the CUDA graph state initialization. The change removes a multiplication by max_blocks_per_seq, thereby allocating a smaller (and sufficient) tensor. This performance tweak affects the FlashAttentionBackend's initialization logic that benefits the runtime performance when using CUDA graphs for local attention."
}