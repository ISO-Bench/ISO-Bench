{
  "commit_hash": "da47621ccc4f8e8381f3249257489d5fe32aff1b",
  "pr_url": "https://github.com/sgl-project/sglang/pull/7058",
  "pr_date": "2025-06-13",
  "timeline_text": "Copy link Collaborator fzyzcjy commented Jun 10, 2025 (cherry picked from commit c234231 ) Motivation 1.4% speedup in my test case Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions compile topk \u2026 6dec99e (cherry picked from commit c234231 ) fzyzcjy requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw , ch-wan and BBuf as code owners June 10, 2025 15:14 gemini-code-assist bot reviewed Jun 10, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @fzyzcjy , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request aims to achieve a minor speedup in the top-k postprocessing step of the Mixture-of-Experts (MoE) layer. It refactors the existing postprocessing logic into a new function and applies PyTorch's JIT compilation ( @torch.compile ) to this new function, which the author reports yields a 1.4% speedup in their test case. Highlights Performance Optimization : Applied @torch.compile to the postprocessing steps within the biased_grouped_topk function to potentially improve performance. Code Structure : Extracted the postprocessing logic (logical to physical ID conversion and padding mask) into a new dedicated helper function _biased_grouped_topk_postprocess . Refactoring : Modified the main biased_grouped_topk function to call the new compiled helper function conditionally, replacing the previous inline steps. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with \ud83d\udc4d and \ud83d\udc4e on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configureGemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . \u21a9 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jun 10, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces a new compiled function _biased_grouped_topk_postprocess to encapsulate and optimize the post-processing steps for topk_ids in the biased_grouped_topk function. This change aims to improve performance, as indicated by the claimed 1.4% speedup. The code is more modular and the use of torch.compile on the combined operations is a reasonable optimization strategy. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/layers/moe/topk.py Comment on lines +252 to +258 @torch.compile(dynamic=True, backend=get_compiler_backend()) def _biased_grouped_topk_postprocess( topk_ids, expert_location_dispatch_info, num_token_non_padded ): topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info) _mask_topk_ids_padded_region(topk_ids, num_token_non_padded) return topk_ids Copy link Contributor gemini-code-assist bot Jun 10, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Encapsulating the post-processing logic ( topk_ids_logical_to_physical and _mask_topk_ids_padded_region ) into a dedicated function _biased_grouped_topk_postprocess is a good approach. Applying @torch.compile to this combined function should help improve performance by allowing the compiler to optimize the sequence of operations. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/layers/moe/topk.py Comment on lines +295 to +300 if (expert_location_dispatch_info is not None) or ( num_token_non_padded is not None ): topk_ids = _biased_grouped_topk_postprocess( topk_ids, expert_location_dispatch_info, num_token_non_padded ) Copy link Contributor gemini-code-assist bot Jun 10, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Replacing the individual calls to topk_ids_logical_to_physical and _mask_topk_ids_padded_region with a single call to the new compiled _biased_grouped_topk_postprocess function aligns with the goal of optimizing the post-processing steps. The conditional check if (expert_location_dispatch_info is not None) or (num_token_non_padded is not None): correctly determines when post-processing is necessary based on the availability of the required inputs. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Merge branch 'main' into feat/compile_topk_postprocess 2f1a8c9 zhyncs self-assigned this Jun 10, 2025 zhyncs added\n  the high priority label Jun 10, 2025 Merge branch 'main' into feat/compile_topk_postprocess 5af51c8 zhyncs approved these changes Jun 13, 2025 View reviewed changes Hide details View details zhyncs merged commit da47621 into sgl-project : main Jun 13, 2025 119 of 133 checks passed Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:38",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Minor speedup topk postprocessing (#7058)",
  "commit_message": "Minor speedup topk postprocessing (#7058)",
  "commit_date": "2025-06-13T00:50:18-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/topk.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 24,
    "num_files": 1,
    "num_hunks": 2,
    "num_non_test_edited_lines": 24,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/topk.py b/python/sglang/srt/layers/moe/topk.py\nindex f5dceac78..0c3d92b66 100644\n--- a/python/sglang/srt/layers/moe/topk.py\n+++ b/python/sglang/srt/layers/moe/topk.py\n@@ -249,6 +249,15 @@ def _mask_topk_ids_padded_region(\n     topk_ids[indices >= num_token_non_padded, :] = -1\n \n \n+@torch.compile(dynamic=True, backend=get_compiler_backend())\n+def _biased_grouped_topk_postprocess(\n+    topk_ids, expert_location_dispatch_info, num_token_non_padded\n+):\n+    topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n+    _mask_topk_ids_padded_region(topk_ids, num_token_non_padded)\n+    return topk_ids\n+\n+\n def biased_grouped_topk(\n     hidden_states: torch.Tensor,\n     gating_output: torch.Tensor,\n@@ -282,14 +291,13 @@ def biased_grouped_topk(\n             num_fused_shared_experts,\n             routed_scaling_factor,\n         )\n-        # TODO merge into kernel for this branch\n-        topk_ids = topk_ids_logical_to_physical(topk_ids, expert_location_dispatch_info)\n-        # TODO will fuse this into kernel, thus use slow manual operation now\n-        if num_token_non_padded is None:\n-            return topk_weights, topk_ids\n-        torch.compile(\n-            _mask_topk_ids_padded_region, dynamic=True, backend=get_compiler_backend()\n-        )(topk_ids, num_token_non_padded)\n+        # TODO merge into kernel\n+        if (expert_location_dispatch_info is not None) or (\n+            num_token_non_padded is not None\n+        ):\n+            topk_ids = _biased_grouped_topk_postprocess(\n+                topk_ids, expert_location_dispatch_info, num_token_non_padded\n+            )\n         return topk_weights, topk_ids\n     else:\n         biased_grouped_topk_fn = (",
  "apis": [
    "sglang.srt.layers.moe.topk.biased_grouped_topk",
    "sglang.srt.layers.moe.topk._biased_grouped_topk_postprocess"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/topk.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The patch modifies a non-test source file by adding a new function decorated with torch.compile, which is intended to improve the performance of topk postprocessing. The changes are non-trivial and target improving the runtime performance of a critical code path in the model, consistent with performance optimization rather than a bug fix, refactoring, or adding new features. Therefore, the commit is performance related.",
  "llm_api_reason": "The commit speeds up the topk postprocessing in the MOE layer by wrapping the mask operation inside a torch.compile decorated function (_biased_grouped_topk_postprocess) and then using this new function in the biased_grouped_topk branch. This affects the topk postprocessing functions used in selecting experts. The affected high-level Python APIs here are the biased_grouped_topk function and the helper function _biased_grouped_topk_postprocess."
}