{
  "commit_hash": "f06e90c2cfb41f283a22ee705ca20d2f451aab6e",
  "pr_url": "https://github.com/sgl-project/sglang/pull/440",
  "pr_date": "2024-05-25",
  "timeline_text": "Copy link Collaborator hnyls2002 commented May 14, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Deprecated the old style of appending fast-forwarded str directly to input_ids , introducing prev_output_str and prev_output_ids instead. When prefilling, input_ids = origin_input_ids + prev_output_ids and we can still hit cache here. Add the retracted tokens into prev_output_ids instead of discarding them. Make it compatible with logprobs. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 Gintasz reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction hnyls2002 added 4 commits May 14, 2024 07:55 move hyper parameters into global_config 20b3eb3 add prev_output_str 219d1e3 fix 49b01ad Merge branch 'main' into optimize-retract 41005fb Copy link Contributor merrymercy commented May 24, 2024 @hnyls2002 please fix the conflicts. Is this ready for merge? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . hnyls2002 added 6 commits May 25, 2024 03:36 Merge branch 'main' into optimize-retract 2ee129b fix prompt_tokens eed315e minor adjust 7d81436 support logprobs for jump forward 50a2b54 fix f79022e optmize the cache hit rate when jump_forward with logprobs 51b09eb hnyls2002 merged commit f06e90c into main May 25, 2024 hnyls2002 deleted the optimize-retract branch May 25, 2024 16:07 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Optimize retract ( sgl-project#440 ) 0d739fe Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:59",
  "has_lm_eval": false,
  "has_performance": false,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "NONE",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Optimize retract (#440)",
  "commit_message": "Optimize retract (#440)",
  "commit_date": "2024-05-26T00:07:26+08:00",
  "files_changed": [
    "examples/usage/json_logprobs.py",
    "python/sglang/global_config.py",
    "python/sglang/srt/layers/logits_processor.py",
    "python/sglang/srt/managers/detokenizer_manager.py",
    "python/sglang/srt/managers/io_struct.py",
    "python/sglang/srt/managers/router/infer_batch.py",
    "python/sglang/srt/managers/router/model_rpc.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 413,
    "num_files": 7,
    "num_hunks": 30,
    "num_non_test_edited_lines": 413,
    "num_non_test_files": 7,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/examples/usage/json_logprobs.py b/examples/usage/json_logprobs.py\nnew file mode 100644\nindex 000000000..6b5b9c8fc\n--- /dev/null\n+++ b/examples/usage/json_logprobs.py\n@@ -0,0 +1,104 @@\n+# NOTE: Currently this can only be run through HTTP requests.\n+import json\n+from concurrent.futures import ThreadPoolExecutor\n+\n+from json_decode import character_regex\n+\n+from sglang.utils import http_request\n+\n+character_names = [\"Hermione Granger\", \"Ron Weasley\", \"Harry Potter\"]\n+\n+base_url = \"http://localhost:30000\"\n+\n+prompt = \"is a character in Harry Potter. Please fill in the following information about this character.\\n\"\n+\n+\n+def openai_api_request(name):\n+    data = {\n+        \"model\": \"\",\n+        \"prompt\": name + prompt,\n+        \"temperature\": 0,\n+        \"max_tokens\": 128,\n+        \"regex\": character_regex,\n+        \"logprobs\": 3,\n+    }\n+    res = http_request(base_url + \"/v1/completions\", json=data).json()\n+\n+    # with open(f\"json_logprobs_{name.replace(' ', '_')}_tmp.json\", \"w\") as fout:\n+    #     fout.write(json.dumps(res, indent=4))\n+\n+    logprobs = res[\"choices\"][0][\"logprobs\"]\n+    usage = res[\"usage\"]\n+    assert len(logprobs[\"token_logprobs\"]) == len(logprobs[\"tokens\"])\n+    assert len(logprobs[\"token_logprobs\"]) == len(logprobs[\"top_logprobs\"])\n+    assert len(logprobs[\"token_logprobs\"]) == usage[\"completion_tokens\"] - 1\n+\n+    return res\n+\n+\n+def srt_api_request(name):\n+    data = {\n+        \"text\": name + prompt,\n+        \"sampling_params\": {\n+            \"temperature\": 0,\n+            \"max_new_tokens\": 128,\n+            \"regex\": character_regex,\n+        },\n+        \"return_logprob\": True,\n+        \"logprob_start_len\": 0,\n+        \"top_logprobs_num\": 3,\n+        \"return_text_in_logprobs\": True,\n+    }\n+\n+    res = http_request(base_url + \"/generate\", json=data).json()\n+\n+    # with open(f\"json_logprobs_{name.replace(' ', '_')}_tmp.json\", \"w\") as fout:\n+    #     fout.write(json.dumps(res, indent=4))\n+\n+    meta_info = res[\"meta_info\"]\n+    assert len(meta_info[\"prefill_token_logprobs\"]) == len(\n+        meta_info[\"prefill_top_logprobs\"]\n+    )\n+    assert len(meta_info[\"decode_token_logprobs\"]) == len(\n+        meta_info[\"decode_top_logprobs\"]\n+    )\n+    assert len(meta_info[\"prefill_token_logprobs\"]) == meta_info[\"prompt_tokens\"]\n+    assert len(meta_info[\"decode_token_logprobs\"]) == meta_info[\"completion_tokens\"] - 1\n+\n+    return res\n+\n+\n+def pretty_print(res):\n+    meta_info = res[\"meta_info\"]\n+\n+    print(\"\\n\\n\", \"=\" * 30, \"Prefill\", \"=\" * 30)\n+    for i in range(len(meta_info[\"prefill_token_logprobs\"])):\n+        print(f\"{str(meta_info['prefill_token_logprobs'][i][2].encode()): <20}\", end=\"\")\n+        top_ks = (\n+            [str(t[2].encode()) for t in meta_info[\"prefill_top_logprobs\"][i]]\n+            if meta_info[\"prefill_top_logprobs\"][i]\n+            else []\n+        )\n+        for top_k in top_ks:\n+            print(f\"{top_k: <15}\", end=\"\")\n+        print()\n+\n+    print(\"\\n\\n\", \"=\" * 30, \"Decode\", \"=\" * 30)\n+    for i in range(len(meta_info[\"decode_token_logprobs\"])):\n+        print(f\"{str(meta_info['decode_token_logprobs'][i][2].encode()): <20}\", end=\"\")\n+        top_ks = [str(t[2].encode()) for t in meta_info[\"decode_top_logprobs\"][i]]\n+        for top_k in top_ks:\n+            print(f\"{top_k: <15}\", end=\"\")\n+        print()\n+\n+    print(res[\"text\"])\n+\n+\n+if __name__ == \"__main__\":\n+    with ThreadPoolExecutor() as executor:\n+        ress = executor.map(srt_api_request, character_names)\n+\n+    for res in ress:\n+        pretty_print(res)\n+\n+    openai_api_request(\"Hermione Granger\")\ndiff --git a/python/sglang/global_config.py b/python/sglang/global_config.py\nindex 062628bd3..452412bec 100644\n--- a/python/sglang/global_config.py\n+++ b/python/sglang/global_config.py\n@@ -28,5 +28,11 @@ class GlobalConfig:\n         # Request dependency time due to network delay\n         self.request_dependency_time = 0.03\n \n+        # New generation token ratio estimation\n+        self.base_new_token_ratio = 0.4\n+        self.base_min_new_token_ratio = 0.2\n+        self.new_token_ratio_decay = 0.0001\n+        self.new_token_ratio_recovery = 0.05\n+\n \n global_config = GlobalConfig()\ndiff --git a/python/sglang/srt/layers/logits_processor.py b/python/sglang/srt/layers/logits_processor.py\nindex 53d6620e9..e47a286eb 100644\n--- a/python/sglang/srt/layers/logits_processor.py\n+++ b/python/sglang/srt/layers/logits_processor.py\n@@ -50,21 +50,22 @@ class LogitsProcessor(nn.Module):\n             prefill_top_logprobs, decode_top_logprobs = [], []\n             pt = 0\n             # NOTE: the GPU-CPU overhead can be reduced\n-            extend_seq_lens_cpu = input_metadata.extend_seq_lens.cpu().numpy()\n-            for i in range(len(extend_seq_lens_cpu)):\n-                if extend_seq_lens_cpu[i] == 0:\n+            extend_seq_lens_cpu = input_metadata.extend_seq_lens.tolist()\n+            for i, extend_seq_len in enumerate(extend_seq_lens_cpu):\n+                if extend_seq_len == 0:\n                     prefill_top_logprobs.append([])\n                     decode_top_logprobs.append([])\n                     continue\n                 k = input_metadata.top_logprobs_nums[i]\n-                t = all_logprobs[pt : pt + extend_seq_lens_cpu[i]].topk(k)\n+                t = all_logprobs[pt : pt + extend_seq_len].topk(k)\n                 vs_cpu = t.values.tolist()\n                 ps_cpu = t.indices.tolist()\n                 prefill_top_logprobs.append(\n                     [list(zip(vs_cpu[j], ps_cpu[j])) for j in range(len(vs_cpu) - 1)]\n                 )\n                 decode_top_logprobs.append(list(zip(vs_cpu[-1], ps_cpu[-1])))\n-                pt += extend_seq_lens_cpu[i]\n+                pt += extend_seq_len\n+\n             return prefill_top_logprobs, decode_top_logprobs\n \n     def forward(self, input_ids, hidden_states, weight, input_metadata: InputMetadata):\n@@ -145,7 +146,7 @@ class LogitsProcessor(nn.Module):\n                 )\n \n \n-if __name__ == \"__main__\":\n+def test():\n     all_logprobs = torch.tensor(\n         #       s                     s                s\n         [[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6], [4, 5, 6, 7]],\n@@ -173,3 +174,7 @@ if __name__ == \"__main__\":\n     print(\"start\", start)\n     print(\"end\", end)\n     print(\"sum_logp\", sum_logp)\n+\n+\n+if __name__ == \"__main__\":\n+    test()\ndiff --git a/python/sglang/srt/managers/detokenizer_manager.py b/python/sglang/srt/managers/detokenizer_manager.py\nindex eeefbe0ba..4774dba33 100644\n--- a/python/sglang/srt/managers/detokenizer_manager.py\n+++ b/python/sglang/srt/managers/detokenizer_manager.py\n@@ -51,11 +51,6 @@ class DetokenizerManager:\n                 # Trim stop str\n                 # TODO(lmzheng): handle the case where multiple stop strs are hit\n                 for i in range(len(output_strs)):\n-                    if recv_obj.hit_stop_str[i] is not None:\n-                        pos = output_strs[i].find(recv_obj.hit_stop_str[i])\n-                        if pos != -1:\n-                            output_strs[i] = output_strs[i][:pos]\n-\n                     if len(output_tokens[i]) > 0:\n                         first_token = self.tokenizer.convert_ids_to_tokens(\n                             int(output_tokens[i][0])\n@@ -65,9 +60,12 @@ class DetokenizerManager:\n                         if first_token.startswith(\"\u2581\"):\n                             output_strs[i] = \" \" + output_strs[i]\n \n-                    output_strs[i] = (\n-                        recv_obj.output_and_jump_forward_strs[i] + output_strs[i]\n-                    )\n+                    output_strs[i] = recv_obj.prev_output_strs[i] + output_strs[i]\n+\n+                    if recv_obj.hit_stop_str[i] is not None:\n+                        pos = output_strs[i].find(recv_obj.hit_stop_str[i])\n+                        if pos != -1:\n+                            output_strs[i] = output_strs[i][:pos]\n \n                 self.send_to_tokenizer.send_pyobj(\n                     BatchStrOut(\ndiff --git a/python/sglang/srt/managers/io_struct.py b/python/sglang/srt/managers/io_struct.py\nindex 8da2317c1..4e8d6d74a 100644\n--- a/python/sglang/srt/managers/io_struct.py\n+++ b/python/sglang/srt/managers/io_struct.py\n@@ -106,8 +106,8 @@ class TokenizedGenerateReqInput:\n @dataclass\n class BatchTokenIDOut:\n     rids: List[str]\n+    prev_output_strs : List[str]\n     output_tokens: List[List[int]]\n-    output_and_jump_forward_strs: List[str]\n     hit_stop_str: List[Optional[str]]\n     skip_special_tokens: List[bool]\n     spaces_between_special_tokens: List[bool]\ndiff --git a/python/sglang/srt/managers/router/infer_batch.py b/python/sglang/srt/managers/router/infer_batch.py\nindex dbe94371b..20cc662a0 100644\n--- a/python/sglang/srt/managers/router/infer_batch.py\n+++ b/python/sglang/srt/managers/router/infer_batch.py\n@@ -36,15 +36,15 @@ class FinishReason(IntEnum):\n \n \n class Req:\n-    def __init__(self, rid, input_text, input_ids):\n+    def __init__(self, rid, origin_input_text, origin_input_ids):\n         self.rid = rid\n-        self.input_text = input_text\n-        self.input_ids = input_ids\n+        self.origin_input_text = origin_input_text\n+        self.origin_input_ids = origin_input_ids\n+        self.origin_input_ids_unpadded = origin_input_ids  # before image padding\n+        self.prev_output_str = \"\"\n+        self.prev_output_ids = []\n         self.output_ids = []\n-\n-        # Since jump forward may retokenize the prompt with partial outputs,\n-        # we maintain the original prompt length to report the correct usage.\n-        self.prompt_tokens = len(input_ids)\n+        self.input_ids = None  # input_ids = origin_input_ids + prev_output_ids\n \n         # The number of decoded tokens for token usage report. Note that\n         # this does not include the jump forward tokens.\n@@ -76,15 +76,24 @@ class Req:\n         self.top_logprobs_num = 0\n         self.normalized_prompt_logprob = None\n         self.prefill_token_logprobs = None\n-        self.decode_token_logprobs = None\n+        self.decode_token_logprobs = []\n         self.prefill_top_logprobs = None\n-        self.decode_top_logprobs = None\n+        self.decode_top_logprobs = []\n+        # The tokens is prefilled but need to be considered as decode tokens\n+        # and should be updated for the decode logprobs\n+        self.last_update_decode_tokens = 0\n \n         # Constrained decoding\n         self.regex_fsm = None\n         self.regex_fsm_state = 0\n         self.jump_forward_map = None\n-        self.output_and_jump_forward_str = \"\"\n+\n+    def partial_decode(self, ids):\n+        first_token = self.tokenizer.convert_ids_to_tokens(ids[0])\n+        first_token = (\n+            first_token.decode() if isinstance(first_token, bytes) else first_token\n+        )\n+        return (\" \" if first_token.startswith(\"\u2581\") else \"\") + self.tokenizer.decode(ids)\n \n     def max_new_tokens(self):\n         return self.sampling_params.max_new_tokens\n@@ -93,7 +102,10 @@ class Req:\n         if self.finished:\n             return\n \n-        if len(self.output_ids) >= self.sampling_params.max_new_tokens:\n+        if (\n+            len(self.prev_output_ids) + len(self.output_ids)\n+            >= self.sampling_params.max_new_tokens\n+        ):\n             self.finished = True\n             self.finish_reason = FinishReason.LENGTH\n             return\n@@ -112,60 +124,66 @@ class Req:\n             )\n \n             for stop_str in self.sampling_params.stop_strs:\n-                if stop_str in tail_str:\n+                # FIXME: (minor) try incremental match in prev_output_str\n+                if stop_str in tail_str or stop_str in self.prev_output_str:\n                     self.finished = True\n                     self.finish_reason = FinishReason.STOP_STR\n                     self.hit_stop_str = stop_str\n                     return\n \n     def jump_forward_and_retokenize(self, jump_forward_str, next_state):\n-        old_output_str = self.tokenizer.decode(self.output_ids)\n         # FIXME: This logic does not really solve the problem of determining whether\n         # there should be a leading space.\n-        first_token = self.tokenizer.convert_ids_to_tokens(self.output_ids[0])\n-        first_token = (\n-            first_token.decode() if isinstance(first_token, bytes) else first_token\n-        )\n-        if first_token.startswith(\"\u2581\"):\n-            old_output_str = \" \" + old_output_str\n-        if self.input_text is None:\n-            # TODO(lmzheng): This can be wrong. Check with Liangsheng.\n-            self.input_text = self.tokenizer.decode(self.input_ids)\n-        new_input_string = (\n-            self.input_text\n-            + self.output_and_jump_forward_str\n-            + old_output_str\n+        cur_output_str = self.partial_decode(self.output_ids)\n+\n+        # TODO(lsyin): apply re-tokenize only for decode tokens so that we do not need origin_input_text anymore\n+        if self.origin_input_text is None:\n+            # Recovering text can only use unpadded ids\n+            self.origin_input_text = self.tokenizer.decode(\n+                self.origin_input_ids_unpadded\n+            )\n+\n+        all_text = (\n+            self.origin_input_text\n+            + self.prev_output_str\n+            + cur_output_str\n             + jump_forward_str\n         )\n-        new_input_ids = self.tokenizer.encode(new_input_string)\n-        if self.pixel_values is not None:\n-            # NOTE: This is a hack because the old input_ids contains the image padding\n-            jump_forward_tokens_len = len(self.tokenizer.encode(jump_forward_str))\n-        else:\n-            jump_forward_tokens_len = (\n-                len(new_input_ids) - len(self.input_ids) - len(self.output_ids)\n-            )\n+        all_ids = self.tokenizer.encode(all_text)\n+        prompt_tokens = len(self.origin_input_ids_unpadded)\n+        self.origin_input_ids = all_ids[:prompt_tokens]\n+        self.origin_input_ids_unpadded = self.origin_input_ids\n+        # NOTE: the output ids may not strictly correspond to the output text\n+        old_prev_output_ids = self.prev_output_ids\n+        self.prev_output_ids = all_ids[prompt_tokens:]\n+        self.prev_output_str = self.prev_output_str + cur_output_str + jump_forward_str\n+        self.output_ids = []\n+\n+        self.regex_fsm_state = next_state\n+\n+        if self.return_logprob:\n+            # For fast-forward part's logprobs\n+            k = 0\n+            for i, old_id in enumerate(old_prev_output_ids):\n+                if old_id == self.prev_output_ids[i]:\n+                    k = k + 1\n+                else:\n+                    break\n+            self.decode_token_logprobs = self.decode_token_logprobs[:k]\n+            self.decode_top_logprobs = self.decode_top_logprobs[:k]\n+            self.logprob_start_len = prompt_tokens + k\n+            self.last_update_decode_tokens = len(self.prev_output_ids) - k\n \n         # print(\"=\" * 100)\n         # print(f\"Catch jump forward:\\n{jump_forward_str}\")\n         # print(self.tokenizer.convert_ids_to_tokens(self.input_ids))\n         # print(self.tokenizer.convert_ids_to_tokens(new_input_ids))\n \n-        self.input_ids = new_input_ids\n-        self.output_ids = []\n-        self.sampling_params.max_new_tokens = max(\n-            self.sampling_params.max_new_tokens - jump_forward_tokens_len, 0\n-        )\n-        self.regex_fsm_state = next_state\n-        self.output_and_jump_forward_str = (\n-            self.output_and_jump_forward_str + old_output_str + jump_forward_str\n-        )\n-\n         # print(f\"Output and jump forward str:\\n{self.output_and_jump_forward_str}\")\n         # print(\"*\" * 100)\n \n     def __repr__(self):\n-        return f\"rid(n={self.rid}, \" f\"input_ids={self.input_ids}, \"\n+        return f\"rid(n={self.rid}, \" f\"input_ids={self.origin_input_ids}, \"\n \n \n @dataclass\n@@ -336,6 +354,7 @@ class Batch:\n \n     def retract_decode(self):\n         sorted_indices = [i for i in range(len(self.reqs))]\n+        # TODO(lsyin): improve the priority of retraction\n         sorted_indices.sort(\n             key=lambda i: (len(self.reqs[i].output_ids), -len(self.reqs[i].input_ids)),\n             reverse=True,\n@@ -356,18 +375,27 @@ class Batch:\n             ][last_uncached_pos : seq_lens_cpu[idx]]\n             self.token_to_kv_pool.dec_refs(token_indices)\n \n+            # release the last node\n             self.tree_cache.dec_lock_ref(req.last_node)\n+\n+            cur_output_str = req.partial_decode(req.output_ids)\n+            req.prev_output_str = req.prev_output_str + cur_output_str\n+            req.prev_output_ids.extend(req.output_ids)\n+\n             req.prefix_indices = None\n             req.last_node = None\n             req.extend_input_len = 0\n             req.output_ids = []\n-            req.regex_fsm_state = 0\n+\n+            # For incremental logprobs\n+            req.last_update_decode_tokens = 0\n+            req.logprob_start_len = 10**9\n \n         self.filter_batch(sorted_indices)\n \n         return retracted_reqs\n \n-    def check_for_jump_forward(self):\n+    def check_for_jump_forward(self, model_runner):\n         jump_forward_reqs = []\n         filter_indices = [i for i in range(len(self.reqs))]\n \n@@ -397,6 +425,18 @@ class Batch:\n                     # jump-forward\n                     req.jump_forward_and_retokenize(jump_forward_str, next_state)\n \n+                    # re-applying image padding\n+                    if req.pixel_values is not None:\n+                        (\n+                            req.origin_input_ids,\n+                            req.image_offset,\n+                        ) = model_runner.model.pad_input_ids(\n+                            req.origin_input_ids_unpadded,\n+                            req.pad_value,\n+                            req.pixel_values.shape,\n+                            req.image_size,\n+                        )\n+\n                     jump_forward_reqs.append(req)\n                     filter_indices.remove(i)\n \ndiff --git a/python/sglang/srt/managers/router/model_rpc.py b/python/sglang/srt/managers/router/model_rpc.py\nindex 6abb20b25..d52b3767d 100644\n--- a/python/sglang/srt/managers/router/model_rpc.py\n+++ b/python/sglang/srt/managers/router/model_rpc.py\n@@ -4,7 +4,7 @@ import multiprocessing\n import time\n import warnings\n from concurrent.futures import ThreadPoolExecutor\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import List, Optional\n \n import rpyc\n import torch\n@@ -16,6 +16,7 @@ try:\n except ImportError:\n     from vllm.logger import logger as vllm_default_logger\n \n+from sglang.global_config import global_config\n from sglang.srt.constrained.fsm_cache import FSMCache\n from sglang.srt.constrained.jump_forward import JumpForwardCache\n from sglang.srt.hf_transformers_utils import get_processor, get_tokenizer\n@@ -106,7 +107,8 @@ class ModelRpcServer:\n         set_random_seed(server_args.random_seed)\n \n         # Print info\n-        logger.info(f\"[rank={self.tp_rank}] \"\n+        logger.info(\n+            f\"[rank={self.tp_rank}] \"\n             f\"max_total_num_token={self.max_total_num_token}, \"\n             f\"max_prefill_num_token={self.max_prefill_num_token}, \"\n             f\"context_len={self.model_config.context_len}, \"\n@@ -151,9 +153,20 @@ class ModelRpcServer:\n         self.jump_forward_cache = JumpForwardCache()\n \n         # Init new token estimation\n-        self.new_token_ratio = min(0.4 * server_args.schedule_conservativeness, 1.0)\n-        self.min_new_token_ratio = min(0.2 * server_args.schedule_conservativeness, 1.0)\n-        self.new_token_ratio_step = (0.0001, 0.05)  # (down, up)\n+        assert (\n+            server_args.schedule_conservativeness >= 0\n+        ), \"Invalid schedule_conservativeness\"\n+        self.new_token_ratio = min(\n+            global_config.base_new_token_ratio * server_args.schedule_conservativeness,\n+            1.0,\n+        )\n+        self.min_new_token_ratio = min(\n+            global_config.base_min_new_token_ratio\n+            * server_args.schedule_conservativeness,\n+            1.0,\n+        )\n+        self.new_token_ratio_decay = global_config.new_token_ratio_decay\n+        self.new_token_ratio_recovery = global_config.new_token_ratio_recovery\n \n     def exposed_step(self, recv_reqs):\n         if self.tp_size != 1:\n@@ -256,8 +269,13 @@ class ModelRpcServer:\n                 (recv_req.image_hash >> 64) % self.model_config.vocab_size,\n             ]\n             req.image_size = recv_req.image_size\n-            req.input_ids, req.image_offset = self.model_runner.model.pad_input_ids(\n-                req.input_ids, req.pad_value, req.pixel_values.shape, req.image_size\n+            req.origin_input_ids, req.image_offset = (\n+                self.model_runner.model.pad_input_ids(\n+                    req.origin_input_ids_unpadded,\n+                    req.pad_value,\n+                    req.pixel_values.shape,\n+                    req.image_size,\n+                )\n             )\n         req.sampling_params = recv_req.sampling_params\n         req.return_logprob = recv_req.return_logprob\n@@ -275,11 +293,11 @@ class ModelRpcServer:\n                 )\n \n         # Truncate prompts that are too long\n-        req.input_ids = req.input_ids[: self.model_config.context_len - 1]\n+        req.origin_input_ids = req.origin_input_ids[: self.model_config.context_len - 1]\n         req.sampling_params.max_new_tokens = min(\n             req.sampling_params.max_new_tokens,\n-            self.model_config.context_len - 1 - len(req.input_ids),\n-            self.max_total_num_token - 128 - len(req.input_ids),\n+            self.model_config.context_len - 1 - len(req.origin_input_ids),\n+            self.max_total_num_token - 128 - len(req.origin_input_ids),\n         )\n         self.forward_queue.append(req)\n \n@@ -292,6 +310,10 @@ class ModelRpcServer:\n \n         # Compute matched prefix length\n         for req in self.forward_queue:\n+            assert (\n+                len(req.output_ids) == 0\n+            ), \"The output ids should be empty when prefilling\"\n+            req.input_ids = req.origin_input_ids + req.prev_output_ids\n             prefix_indices, last_node = self.tree_cache.match_prefix(req.input_ids)\n             if req.return_logprob:\n                 prefix_indices = prefix_indices[: req.logprob_start_len]\n@@ -319,7 +341,7 @@ class ModelRpcServer:\n             )\n \n         for req in self.forward_queue:\n-            if req.return_logprob:\n+            if req.return_logprob and req.normalized_prompt_logprob is None:\n                 # Need at least two tokens to compute normalized logprob\n                 if req.extend_input_len < 2:\n                     delta = 2 - req.extend_input_len\n@@ -441,28 +463,53 @@ class ModelRpcServer:\n             req.check_finished()\n \n             if req.return_logprob:\n-                req.normalized_prompt_logprob = normalized_prompt_logprobs[i]\n-\n-                # If logprob_start_len > 0, then first logprob_start_len prompt tokens will be ignored.\n-                req.prefill_token_logprobs = list(\n-                    zip(\n-                        prefill_token_logprobs[pt : pt + req.extend_input_len - 1],\n-                        req.input_ids[-req.extend_input_len + 1 :],\n+                if req.normalized_prompt_logprob is None:\n+                    req.normalized_prompt_logprob = normalized_prompt_logprobs[i]\n+\n+                if req.prefill_token_logprobs is None:\n+                    # If logprob_start_len > 0, then first logprob_start_len prompt tokens will be ignored.\n+                    req.prefill_token_logprobs = list(\n+                        zip(\n+                            prefill_token_logprobs[pt : pt + req.extend_input_len - 1],\n+                            req.input_ids[-req.extend_input_len + 1 :],\n+                        )\n                     )\n-                )\n-                if req.logprob_start_len == 0:\n-                    req.prefill_token_logprobs = [\n-                        (None, req.input_ids[0])\n-                    ] + req.prefill_token_logprobs\n-                req.decode_token_logprobs = [\n+                    if req.logprob_start_len == 0:\n+                        req.prefill_token_logprobs = [\n+                            (None, req.input_ids[0])\n+                        ] + req.prefill_token_logprobs\n+\n+                if req.last_update_decode_tokens != 0:\n+                    req.decode_token_logprobs.extend(\n+                        list(\n+                            zip(\n+                                prefill_token_logprobs[\n+                                    pt\n+                                    + req.extend_input_len\n+                                    - req.last_update_decode_tokens : pt\n+                                    + req.extend_input_len\n+                                    - 1\n+                                ],\n+                                req.input_ids[-req.last_update_decode_tokens + 1 :],\n+                            )\n+                        )\n+                    )\n+\n+                req.decode_token_logprobs.append(\n                     (last_token_logprobs[i], next_token_ids[i])\n-                ]\n+                )\n \n             if req.top_logprobs_num > 0:\n-                req.prefill_top_logprobs = prefill_top_logprobs[i]\n-                if req.logprob_start_len == 0:\n-                    req.prefill_top_logprobs = [None] + req.prefill_top_logprobs\n-                req.decode_top_logprobs = [decode_top_logprobs[i]]\n+                if req.prefill_top_logprobs is None:\n+                    req.prefill_top_logprobs = prefill_top_logprobs[i]\n+                    if req.logprob_start_len == 0:\n+                        req.prefill_top_logprobs = [None] + req.prefill_top_logprobs\n+\n+                if req.last_update_decode_tokens != 0:\n+                    req.decode_top_logprobs.extend(\n+                        prefill_top_logprobs[i][-req.last_update_decode_tokens + 1 :]\n+                    )\n+                req.decode_top_logprobs.append(decode_top_logprobs[i])\n \n             pt += req.extend_input_len\n \n@@ -484,7 +531,7 @@ class ModelRpcServer:\n         # check if decode out of memory\n         if not batch.check_decode_mem():\n             old_ratio = self.new_token_ratio\n-            self.new_token_ratio = min(old_ratio + self.new_token_ratio_step[1], 1.0)\n+            self.new_token_ratio = min(old_ratio + self.new_token_ratio_recovery, 1.0)\n \n             retracted_reqs = batch.retract_decode()\n             logger.info(\n@@ -495,26 +542,13 @@ class ModelRpcServer:\n             self.forward_queue.extend(retracted_reqs)\n         else:\n             self.new_token_ratio = max(\n-                self.new_token_ratio - self.new_token_ratio_step[0],\n+                self.new_token_ratio - self.new_token_ratio_decay,\n                 self.min_new_token_ratio,\n             )\n \n         if not self.disable_regex_jump_forward:\n             # check for jump-forward\n-            jump_forward_reqs = batch.check_for_jump_forward()\n-\n-            # check for image jump-forward\n-            for req in jump_forward_reqs:\n-                if req.pixel_values is not None:\n-                    (\n-                        req.input_ids,\n-                        req.image_offset,\n-                    ) = self.model_runner.model.pad_input_ids(\n-                        req.input_ids,\n-                        req.pad_value,\n-                        req.pixel_values.shape,\n-                        req.image_size,\n-                    )\n+            jump_forward_reqs = batch.check_for_jump_forward(self.model_runner)\n \n             self.forward_queue.extend(jump_forward_reqs)\n             if batch.is_empty():\n@@ -557,8 +591,8 @@ class ModelRpcServer:\n \n     def handle_finished_requests(self, batch: Batch):\n         output_rids = []\n+        prev_output_strs = []\n         output_tokens = []\n-        output_and_jump_forward_strs = []\n         output_hit_stop_str = []\n         output_skip_special_tokens = []\n         output_spaces_between_special_tokens = []\n@@ -582,8 +616,8 @@ class ModelRpcServer:\n                 )\n             ):\n                 output_rids.append(req.rid)\n+                prev_output_strs.append(req.prev_output_str)\n                 output_tokens.append(req.output_ids)\n-                output_and_jump_forward_strs.append(req.output_and_jump_forward_str)\n                 output_hit_stop_str.append(req.hit_stop_str)\n                 output_skip_special_tokens.append(\n                     req.sampling_params.skip_special_tokens\n@@ -593,10 +627,8 @@ class ModelRpcServer:\n                 )\n \n                 meta_info = {\n-                    \"prompt_tokens\": req.prompt_tokens,\n-                    \"completion_tokens\": len(req.input_ids)\n-                    + len(req.output_ids)\n-                    - req.prompt_tokens,\n+                    \"prompt_tokens\": len(req.origin_input_ids),\n+                    \"completion_tokens\": len(req.prev_output_ids) + len(req.output_ids),\n                     \"completion_tokens_wo_jump_forward\": req.completion_tokens_wo_jump_forward,\n                     \"finish_reason\": FinishReason.to_str(req.finish_reason),\n                     \"hit_stop_str\": req.hit_stop_str,\n@@ -623,8 +655,8 @@ class ModelRpcServer:\n             self.out_pyobjs.append(\n                 BatchTokenIDOut(\n                     output_rids,\n+                    prev_output_strs,\n                     output_tokens,\n-                    output_and_jump_forward_strs,\n                     output_hit_stop_str,\n                     output_skip_special_tokens,\n                     output_spaces_between_special_tokens,",
  "apis": [
    "openai_api_request",
    "srt_api_request",
    "LogitsProcessor.forward",
    "ModelRpcServer.exposed_step",
    "Req.jump_forward_and_retokenize"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/examples/frontend_language/usage/json_logprobs.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes changes across several non-test source files, modifying core code such as token logprob processing, sequence length handling (changing from a CPU tensor conversion to a Python list conversion), and functions managing retraction and jump-forward logic. These modifications target internal APIs that affect performance, especially by reducing redundant CPU-GPU overhead and by optimizing the retract decode logic (as referenced in the commit message \"Optimize retract (#440)\"). The changes are non-trivial code modifications aimed at performance improvements rather than bug fixes, simple refactoring, or new features, and they impact high-level APIs testable on CPU. Therefore, the commit satisfies the conditions related to performance optimization.",
  "llm_api_reason": "This commit adds a new example \u201cjson_logprobs.py\u201d file with two front\u2010end functions that make HTTP requests using the SGLang backend (\u201copenai_api_request\u201d and \u201csrt_api_request\u201d). It also tweaks several internal components that ultimately affect how the runtime generates and processes outputs. For example, the LogitsProcessor\u2019s forward routine in the attention/logits processing layer is modified to use list\u2010based lengths (improving performance) while the router\u2019s request logic is updated (including a new \u201cjump_forward_and_retokenize\u201d method on the Req class) and the RPC server\u2019s step handling (exposed_step) now uses new token\u2010ratio parameters from the updated global configuration. These changes impact the high\u2010level Python APIs that a user of SGLang might invoke (or that serve as integration points) such as the example \u201copenai_api_request\u201d and \u201csrt_api_request\u201d functions, as well as the core runtime methods in LogitsProcessor, ModelRpcServer, and the router Request (Req) class."
}