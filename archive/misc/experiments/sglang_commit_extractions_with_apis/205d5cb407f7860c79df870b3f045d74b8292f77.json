{
  "commit_hash": "205d5cb407f7860c79df870b3f045d74b8292f77",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6356",
  "pr_date": "2025-05-17",
  "timeline_text": "Copy link Collaborator CatherineSue commented May 16, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation The current implementation allocates maximum sized buffers for local attention metadata during CUDA graph capture, regardless of the actual requirements of the batch being processed. This leads to significant memory overhead and reduced performance. For instance, with Llama-4-Maverick-17B-128E-Instruct-FP8 on 8\u00d7H100 GPUs, we can serve a maximum of 535k tokens. If we set context_len to 524288 and attn_chunk_size to 8192, our theoretical max_virtual_batches would be 64 \u00d7 160 (where 64 is the maximum number of chunks per sequence and 160 is the maximum batch size). It is even bigger for local_block_table tensor. However, it's practically impossible to have 160 concurrent requests each with 524k tokens, as this would far exceed our total token budget of 535k. Without optimized buffer allocation, we would always be sending 65 \u00d7 160 = 10240 shape tensors for local_attn_metadata to the CUDA graph, which wastes significant GPU memory and reduces overall inference throughput. For instance, if there are 160 requests, each has length less than 8192, the actual shape of local_query_start_loc would only be 160, but now we are sending a 10240 shape. As a result, it even takes 10 seconds for the server to finish the warmup /generate request. Modifications Added a new method _update_local_attn_metadata_for_capture that: Calculates the precise dimensions needed for local attention metadata based on the actual batch being processed Creates optimized tensor views with exactly the required sizes Benchmark # Server: 8xH100\npython3 -m sglang.launch_server --model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 --port=8080 --tp-size=8 --context-length=524288 --chat-template=llama-4 # benchmark\npython3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 3000 --random-input 1000 --random-output 1000 --random-range-ratio 1.0 --max-concurrency 64 --port 8080 Before: main branch Output token throughput (tok/s):         3136.87 After: current branch Output token throughput (tok/s):         4165.05 vllm 3164 from blog Evaluation \u279c  sglang git:(chang/opt-local-attn) \u2717 python3 -m sglang.eval.loogle_eval --api-url=http://127.0.0.1:8080/v1\nRunning benchmark: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1101/1101 [00:00<00:00, 85129.39it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nLoading responses: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1101/1101 [00:00<00:00, 5616.48it/s]\nScoring batches: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 18/18 [02:12<00:00,  7.36s/it]\nAverage BERTScore (F1): 84.38% Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 4 ch-wan, YouNeedCryDear, zhyncs, and ispobock reacted with thumbs up emoji \ud83d\ude80 2 YouNeedCryDear and zhyncs reacted with rocket emoji All reactions \ud83d\udc4d 4 reactions \ud83d\ude80 2 reactions Optimize local attention memory allocation in FlashAttentionBackend \u2026 6ce5972 This commit improves memory efficiency in the FlashAttention backend by:\n\n- Add _update_local_attn_metadata_for_capture for CUDA graph capture phase\n- Implementing exact buffer size calculation during CUDA graph capture\n\nThese changes reduce memory overhead by allocating only the necessary buffer\nsizes for local attention metadata, which prevents excessive memory consumption\nin CUDA graphs. Oversized buffers in CUDA graphs not only waste GPU memory but\nalso increase kernel launch overhead and memory transfer times, reducing overall\nthroughput during inference. CatherineSue requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw and ch-wan as code owners May 16, 2025 21:11 Merge branch 'main' into chang/opt-local-attn dc789af zhyncs requested a review\n  from BBuf as a code owner May 16, 2025 23:18 zhyncs self-assigned this May 16, 2025 zhyncs added\n  the high priority label May 16, 2025 Merge branch 'main' into chang/opt-local-attn 0785702 zhyncs approved these changes May 17, 2025 View reviewed changes Hide details View details zhyncs merged commit 205d5cb into main May 17, 2025 35 of 40 checks passed Uh oh! There was an error while loading. Please reload this page . zhyncs deleted the chang/opt-local-attn branch May 17, 2025 08:45 zhyncs pushed a commit\n      that referenced\n      this pull request May 18, 2025 perf: Optimize local attention memory allocation in FlashAttentionBac\u2026 \u2026 d537551 \u2026kend ( #6356 ) Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 perf: Optimize local attention memory allocation in FlashAttentionBac\u2026 \u2026 b1968c8 \u2026kend ( sgl-project#6356 ) xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 perf: Optimize local attention memory allocation in FlashAttentionBac\u2026 \u2026 4582d3c \u2026kend ( sgl-project#6356 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:40",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8,tp_size=8 --tasks hellaswag --batch_size 8"
  ],
  "perf_command": "python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompts 3000 --random-input 1000 --random-output 1000 --random-range-ratio 1.0 --max-concurrency 64 --port 8080",
  "commit_subject": "perf: Optimize local attention memory allocation in FlashAttentionBackend (#6356)",
  "commit_message": "perf: Optimize local attention memory allocation in FlashAttentionBackend (#6356)",
  "commit_date": "2025-05-17T01:45:46-07:00",
  "files_changed": [
    "python/sglang/srt/layers/attention/flashattention_backend.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 70,
    "num_files": 1,
    "num_hunks": 2,
    "num_non_test_edited_lines": 70,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 2f974ea9a..a626ff0d8 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1434,19 +1434,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 self.decode_cuda_graph_metadata[bs] = metadata\n \n                 if self.attention_chunk_size is not None:\n-                    metadata.local_attn_metadata = FlashAttentionMetadata.LocalAttentionMetadata(\n-                        local_query_start_loc=self.decode_cuda_graph_local_attn_metadata[\n-                            \"local_query_start_loc\"\n-                        ],\n-                        local_seqused_k=self.decode_cuda_graph_local_attn_metadata[\n-                            \"local_seqused_k\"\n-                        ],\n-                        local_block_table=self.decode_cuda_graph_local_attn_metadata[\n-                            \"local_block_table\"\n-                        ],\n-                        local_max_query_len=1,\n-                        local_max_seq_len=1,\n-                    )\n+                    self._update_local_attn_metadata_for_capture(metadata, batch_size)\n \n         elif forward_mode.is_target_verify():\n             if self.topk <= 1:\n@@ -1807,6 +1795,62 @@ class FlashAttentionBackend(AttentionBackend):\n         )\n         metadata.local_attn_metadata = local_metadata\n \n+    def _update_local_attn_metadata_for_capture(\n+        self, metadata: FlashAttentionMetadata, bs: int\n+    ):\n+        \"\"\"Update local attention metadata during CUDA graph capture phase.\n+\n+        This method calculates the exact buffer sizes needed for local attention metadata\n+        during the CUDA graph capture phase, optimizing memory usage by creating views of\n+        pre-allocated buffers with exactly the sizes needed.\n+        \"\"\"\n+        seq_lens_capture = metadata.cache_seqlens_int32\n+        max_seq_len = int(seq_lens_capture.max().item())\n+        page_table_capture = metadata.page_table\n+\n+        cu_seqlens_q_np = metadata.cu_seqlens_q.cpu().numpy()\n+        seqlens_np = seq_lens_capture.cpu().numpy()\n+        (\n+            seqlens_q_local_np,\n+            cu_seqlens_q_local_np,\n+            seqlens_k_local_np,\n+            block_table_local_np,\n+        ) = make_local_attention_virtual_batches(\n+            self.attention_chunk_size,\n+            cu_seqlens_q_np,\n+            seqlens_np,\n+            page_table_capture,\n+            self.page_size,\n+        )\n+\n+        # Get exact dimensions from the calculation\n+        q_len = len(cu_seqlens_q_local_np)\n+        k_len = len(seqlens_k_local_np)\n+        b0 = block_table_local_np.shape[0] if block_table_local_np.shape[0] > 0 else bs\n+        b1 = block_table_local_np.shape[1] if block_table_local_np.shape[1] > 0 else 1\n+\n+        # Create views of the pre-allocated buffers with exactly these sizes\n+        # This is the key optimization - we only use the memory we actually need\n+        local_query_start_loc = self.decode_cuda_graph_local_attn_metadata[\n+            \"local_query_start_loc\"\n+        ][:q_len]\n+\n+        local_seqused_k = self.decode_cuda_graph_local_attn_metadata[\"local_seqused_k\"][\n+            :k_len\n+        ]\n+\n+        local_block_table = self.decode_cuda_graph_local_attn_metadata[\n+            \"local_block_table\"\n+        ][:b0, :b1]\n+\n+        metadata.local_attn_metadata = FlashAttentionMetadata.LocalAttentionMetadata(\n+            local_query_start_loc=local_query_start_loc,\n+            local_seqused_k=local_seqused_k,\n+            local_block_table=local_block_table,\n+            local_max_query_len=1,\n+            local_max_seq_len=max_seq_len,\n+        )\n+\n     def _update_local_attn_metadata_for_replay(\n         self, metadata: FlashAttentionMetadata, bs: int\n     ):",
  "apis": [
    "sglang.srt.layers.attention.flashattention_backend.FlashAttentionBackend"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/attention/flashattention_backend.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The patch modifies a non-test Python source file (flashattention_backend.py) and makes non-trivial changes to the way local attention metadata is computed and allocated. Instead of using fixed offsets to extract metadata, the new method calculates the exact sizes needed for local attention and creates views of pre-allocated buffers accordingly. This directly targets memory usage in the CUDA graph capture phase and can affect the performance of the FlashAttentionBackend by optimizing memory allocations. The changes, while not directly labeled as \"performance\" in the commit message, clearly serve to improve runtime memory operations, which is a performance optimization in a high-level API. Therefore, this commit fits the criteria for a performance/optimization-related change.",
  "llm_api_reason": "This commit optimizes the local attention memory allocation for CUDA graph capture within the FlashAttentionBackend. Instead of creating a static LocalAttentionMetadata instance, the code now calls a new helper method (_update_local_attn_metadata_for_capture) that dynamically computes buffer sizes and views, thereby improving memory efficiency during CUDA graph capture. The change affects how the FlashAttentionBackend initializes local attention metadata in its CUDA graph capture path."
}