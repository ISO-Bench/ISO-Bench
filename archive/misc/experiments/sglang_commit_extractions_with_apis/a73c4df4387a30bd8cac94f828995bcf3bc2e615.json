{
  "commit_hash": "a73c4df4387a30bd8cac94f828995bcf3bc2e615",
  "pr_url": "https://github.com/sgl-project/sglang/pull/5150",
  "pr_date": "2025-04-08",
  "timeline_text": "Copy link Collaborator mingfeima commented Apr 8, 2025 Motivation This pull request is a follow up on #2807 to enable and optimize sglang performance on CPU devices. In this patch, optimized C++ kernels are provided including: activations layernorms gemm (bfloat16, int8) extend attention (bfloat16) decode attention (bfloat16) allreduce and allgather moe (bfloat16, int8) rope Specifically, we are are targeting at optimizing DeepSeek R1 617B on CPU devices. And right now the performance on Xeon6 with single batch size and 1024 input tokens and 1024 output tokens are: torch profiler chrome trace saved to Trace_prefill_DS-R1-INT8-TP6-BS1-1024-1024_20250402_batch1_input1024_output1024.trace.json.gz\nPrefill. latency: 2.79226 s, throughput:    366.73 token/s\nDecode.  latency: 0.07376 s, throughput:     13.56 token/s\nDecode.  latency: 0.06361 s, throughput:     15.72 token/s\nDecode.  latency: 0.06163 s, throughput:     16.23 token/s\nDecode.  latency: 0.06307 s, throughput:     15.86 token/s\nDecode.  latency: 0.05998 s, throughput:     16.67 token/s Modifications This PR contains changes in C++ parts from sglang/sgl-kernel/csrc/cpu and we decide to upstream the C++ kernels first so as not to make the PR overwhelming. We will upstream changes to sglang python layers one by one later on. Also please notice that the CPU build now relies on setup_cpu.py . Originally we made modifications on setup.py but I just found out this file has been removed. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 3 zhyncs, gau-nernst, and Swipe4057 reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions chunyuan-w and others added 30 commits April 7, 2025 20:08 Optimize all_reduce by porting the shared memory kernel of deepspeed ( \u2026 62062ff #6 )\n\n* Optimize all_reduce by porting the shm kernel of deepspeed\n\n* Fix rebase: use get_tp_group in sglang.srt.distributed\n\n* Fix rebase: directly modify tensor_model_parallel_all_reduce in sglang add norm kernels for CPU d51b479 add silu_and_mul kernels for CPU 481b1ac add grouped topk kernels for CPU a252d1a add decode attention kernels for CPU 29668f7 add fused moe kernels for CPU 6934dda decode attention: fix non-contiguous k_buffer and v_buffer be7a5c5 add fused moe kernels for CPU: part 2 ea61ccd decode attention: fix seq_len req_pool_indices dtypes using int64_t 8c335dc add extend attention kernel for CPU e51ccd3 extend attention: fix bug in MLA when k_extend and k_buffer have diff\u2026 \u2026 8fe2983 \u2026erent head number fused moe: fix when w13 has OC not multiples of 64 \u2026 da4fe82 w13 has output channel of 1408 and when TP=3, each rank will have 480 after padding\nwhich is 15 * 32. add weight packed linear for bfloat16/float16 on CPU b4db2d8 weight_packed_linear: remove out as input parameter d0b5fc2 convert_weight_packed: use int64_t for stride to avoid overflow 235c6cd add int8_scaled_mm for int8 W8A8 on CPU 33a7009 add biased_grouped_topk for CPU 0df5370 Add record_function for profiling ( #14 ) ba23156 moe: apply avx512-bf16 tinygemm when M is small ebc341a grouped_topk: add support for num_experts = 160, config from DeepSeekV2 b5de4d0 moe: change indexing from int32 to int64 to avoid overflow 1e7ef35 int8_scaled_mm: move dequant to per_token_quant_int8 33b8be8 Add fused_moe int8 w8a8 support for CPU 5edc328 fused_add_rmsnorm: replace at::zeros with at::empty 4096183 mv cpu source files from src/sgl-kernel/csrc/cpu to csrc/cpu 8556c74 biased_grouped_topk: fix correction_bias dtype, should be bfloat16 in\u2026 \u2026 a40b671 \u2026stead of float32 Add bmm AMX and avx512-bf16 kernels on CPU 4497115 Add RECORD_FUNCTION in bmm_cpu, int8 mm, per token quant ( #22 ) 47db4b7 Add rope.cpp and torch_extension_cpu.cpp from 47bc8df 6884f38 Add shared_expert for intel AMX c3e4c89 14 hidden items Load more\u2026 mingfeima mentioned this pull request Apr 8, 2025 [Feature] RFC for adding CPU support for SGLang #2807 Closed 8 tasks zhyncs added\n  the high priority label Apr 8, 2025 Copy link Collaborator FlamingoPg commented Apr 8, 2025 Amazing! \ud83d\ude80 4 zhyncs, gau-nernst, Swipe4057, and p12tic reacted with rocket emoji All reactions \ud83d\ude80 4 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member zhyncs commented Apr 8, 2025 pre-commit run --all-files All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned FlamingoPg Apr 8, 2025 Copy link Collaborator FlamingoPg commented Apr 8, 2025 Also please notice that the CPU build now relies on setup_cpu.py. Originally we made modifications on setup.py but I just found out this file has been removed. That's right, we now use pyproject.toml -> CMakeLists for cpp code compile. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator FlamingoPg commented Apr 8, 2025 @mingfeima This is just some misc issues. If this PR doesn't break the existing functionality, it can be merged first, and the misc issues can be fixed later. However, some unit tests and benchmarks for the CPU kernel need to be added. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author mingfeima commented Apr 8, 2025 @mingfeima This is just some misc issues. If this PR doesn't break the existing functionality, it can be merged first, and the misc issues can be fixed later. However, some unit tests and benchmarks for the CPU kernel need to be added. No, let me correct the formatting. I just have the whole afternoon packed with meetings... Unit tests and benchmarks will be added with the python level change. We have them ready on our local branch. Will PR later one 1 by 1. \ud83d\ude80 3 FlamingoPg, zhyncs, and gau-nernst reacted with rocket emoji All reactions \ud83d\ude80 3 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . apply pre-commit format changes 2ad92c6 Copy link Collaborator Author mingfeima commented Apr 8, 2025 @yinfan98 need your approval to re-run CI! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned hebiao064 Apr 8, 2025 Merge branch 'main' into pr_native_kernels_for_cpu 80e6fe2 zhyncs merged commit a73c4df into sgl-project : main Apr 8, 2025 finger92 pushed a commit\n        to protagolabs/sglang\n      that referenced\n      this pull request Apr 10, 2025 Add optimized native kernels in sgl-kernel ( sgl-project#5150 ) \u2026 907f43c Co-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com> thyecust pushed a commit\n        to thyecust/sglang\n      that referenced\n      this pull request Apr 11, 2025 Add optimized native kernels in sgl-kernel ( sgl-project#5150 ) \u2026 08a3e58 Co-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com> DiweiSun pushed a commit\n        to DiweiSun/sglang\n      that referenced\n      this pull request Apr 16, 2025 Add optimized native kernels in sgl-kernel ( sgl-project#5150 ) \u2026 c859bd6 Co-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com> jimoosciuc pushed a commit\n        to Furion-cn/sglang\n      that referenced\n      this pull request Apr 17, 2025 Add optimized native kernels in sgl-kernel ( sgl-project#5150 ) \u2026 1d59100 Co-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Apr 23, 2025 rebase sglang to tag v0.4.5.post1 ( sgl-project#13 ) \u2026 3ecb4e3 * Support with_stack and record_shapes in profiler ( sgl-project#4740 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* test: reduce `mem_fraction_static` for gemma3 vision test ( sgl-project#4840 )\n\n* Fix CI tests ( sgl-project#4853 )\n\n* Fix fa3 cuda graph page_size > 1 precision and page_size=1 speed ( sgl-project#4855 )\n\n* Revert \"get the python version from env ( sgl-project#4729 )\" ( sgl-project#4863 )\n\n* [Feature] add multi-rank support for Lora ( sgl-project#4492 )\n\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\n\n* Clean up `import vllm` in quantization/__init__.py ( sgl-project#4834 )\n\n* Fix wrong variable name when stopping memory profile ( sgl-project#4772 )\n\n* [Feat] support deepgemm for cmake ( sgl-project#4864 )\n\n* Make torch compile configurable for biased_grouped_topk ( sgl-project#4749 )\n\n* update sgl-kernel test ci ( sgl-project#4866 )\n\n* fix sampling issue ( sgl-project#4871 )\n\n* bump sgl-kernel 0.0.5.post4 ( sgl-project#4768 )\n\n* fix sgl-kernel cu118 build ( sgl-project#4872 )\n\n* [Feature] Support FA3 backend for MLA ( sgl-project#4831 )\n\n* upgrade sgl-kernel 0.0.5.post4 ( sgl-project#4873 )\n\n* update torch compile doc ( sgl-project#4874 )\n\n* bump v0.4.4.post3 ( sgl-project#4878 )\n\n* Fix BadRequestError wrong arguments and remove openai dependency ( sgl-project#4882 )\n\n* Improve stack trace of retry errors ( sgl-project#4845 )\n\n* Tiny fix doc error ( sgl-project#4795 )\n\n* [Docs] Update DeepGEMM at README.md ( sgl-project#4886 )\n\n* Update CODEOWNERS ( sgl-project#4889 )\n\n* Delete test_deep_gemm.py ( sgl-project#4891 )\n\n* Add deepseek style fused moe group gate selection kernel ( sgl-project#4530 )\n\n* quick fix: add default for new kernel ( sgl-project#4898 )\n\n* remove setup for sgl-kernel ( sgl-project#4899 )\n\n* [Misc] Clean m.def and add Development Tips ( sgl-project#4890 )\n\n* fix allreduce test ( sgl-project#4909 )\n\n* Support page size > 1 + eagle ( sgl-project#4908 )\n\n* Fix retract for page size > 1 ( sgl-project#4914 )\n\n* [Feature] use pytest for sgl-kernel ( sgl-project#4896 )\n\n* fix bmm fp8 ( sgl-project#4926 )\n\n* Fix the timeout for unit-test-2-gpu in pr-test.yml ( sgl-project#4927 )\n\n* Fix 2-gpu CI test and suppress some warnings ( sgl-project#4930 )\n\n* [feat] add fa3 in sgl-kernel ( sgl-project#4902 )\n\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\n\n* Fix sglang frontend's incorrect dependency on torch ( sgl-project#4931 )\n\n* [Fix] avoid stream sync and torch compile in prefill for fa3 backend ( sgl-project#4932 )\n\n* cleanup sgl-kernel ( sgl-project#4933 )\n\n* [Fix] Improve Lora tests and reduce CI runtime ( sgl-project#4925 )\n\n* Fix DeepSeek bug causing 2.2% MMLU drop when TP!=DP ( sgl-project#4883 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* [Fix] Add torch compile for torch.clamp back ( sgl-project#4936 )\n\n* Fix oom error for large page size ( sgl-project#4913 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* [feat] interface for platforms abstraction ( sgl-project#4928 )\n\n* [Fix] revert clean m.def for cudagraph ( sgl-project#4944 )\n\n* refactor: multimodal data ( sgl-project#4754 )\n\n* bump sgl-kernel v0.0.6 ( sgl-project#4950 )\n\n* [Build] Fix cuda12.8 build error in nvfp4_scaled_mm_kernels.cu ( sgl-project#4953 )\n\n* use fa3 in sgl-kernel ( sgl-project#4954 )\n\n* Revert PR 4764 & 4813 related to R1 RoPE ( sgl-project#4959 )\n\n* [Feature] Support DeepEP Low Latency ( sgl-project#4767 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* update bench_serving ( sgl-project#4958 )\n\n* Prevent memory leak of retract_decode when page_size > 1 ( sgl-project#4977 )\n\n* [VLM RLHF] Take Image input for verl vlm rollout ( sgl-project#4915 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: GeLee <leege233@gmail.com>\n\n* Large page size aligned hierarchical caching ( sgl-project#4581 )\n\n* bug fix for hicache host eviction ( sgl-project#4989 )\n\n* sgl scaled_fp8_quant support output padding ( sgl-project#4861 )\n\n* Add Eagle Speculative Decoding to FA3 Backend ( sgl-project#4951 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\n\n* Update tokenizer_manager.py ( sgl-project#5008 )\n\n* [sgl-kernel] per token group quant support COLUMN MAJOR ( sgl-project#4817 )\n\n* update cutlass tag ( sgl-project#5011 )\n\n* Feature/revise docs ci ( sgl-project#5009 )\n\n* fix: fix illegal cuda memory access at fused_moe_kernel ( sgl-project#4727 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* [Build] Support build sgl-kernel with ccache ( sgl-project#5020 )\n\n* fix deepgemm as well ( sgl-project#5030 )\n\n* try to fix ci oserror ( sgl-project#5024 )\n\n* Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5005 )\n\n* Small refactor DeepEPMode to clean up code a bit ( sgl-project#4992 )\n\n* [Fix] fix fa3 build at cu118 ( sgl-project#5036 )\n\n* Revert \"Replace enable_flashinfer_mla argument with attention_backend\" ( sgl-project#5048 )\n\n* bump sgl-kernel v0.0.7 ( sgl-project#5046 )\n\n* update eagle-3 docs ( sgl-project#4796 )\n\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\n\n* Add LlavaLlamaForCausaLM in MultiModal Processors ( sgl-project#5039 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* Update the retry count ( sgl-project#5051 )\n\n* upgrade sgl-kernel v0.0.7 ( sgl-project#5049 )\n\n* [2/3] fix dsv3 awq issue  ( sgl-project#4625 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\n\n* Feature/revise docs ci ( sgl-project#5056 )\n\n* Add H20 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5057 )\n\n* [fix] remove `cuda_device_count_stateless` ( sgl-project#5060 )\n\n* Small refactor DeepEPDispatcher into subclasses ( sgl-project#4994 )\n\n* Support async DeepEP by splitting into two stages ( sgl-project#4995 )\n\n* Cleanup unused resources after DeepEP operation ( sgl-project#4996 )\n\n* Add DeepSeek V3/R1 shared experts fusion ( sgl-project#4918 )\n\n* [deepep] fix: shared experts are not initialized when shared experts fusion is enabled ( sgl-project#5072 )\n\n* fix dummy-load deepseekv2 ( sgl-project#4535 )\n\n* support sgl-kernel on blackwell ( sgl-project#5074 )\n\n* FA3 Spec Decoding to support top k = 1 and add cuda graph support ( sgl-project#5050 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Chunan Zeng <zcnrex@gmail.com>\n\n* [Revision] Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5052 )\n\n* upgrade transformers 4.51.0 ( sgl-project#5088 )\n\n* sgl-kernel transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5079 )\n\n* bump sgl-kernel 0.0.8 ( sgl-project#5089 )\n\n* python transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5080 )\n\n* bump v0.4.4.post4 ( sgl-project#5091 )\n\n* Fix: Reduce the number of document ci attempts to avoid long ci running ( sgl-project#5097 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Add Llama4 support ( sgl-project#5092 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Fix refactor error - fp8.py ( sgl-project#5106 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* bump v0.4.5 ( sgl-project#5117 )\n\n* [ci] fix llama4 ci error ( sgl-project#5126 )\n\n* Refactor and Optimize FA3 Code ( sgl-project#5090 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\n\n* Add Llama4 user guide ( sgl-project#5133 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* [Misc] Use pytest.mark.skipif in sgl-kernel test ( sgl-project#5137 )\n\n* feat: disable grammar restrictions within reasoning sections ( sgl-project#4984 )\n\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\n\n* [modelopt] automatically inspect if model is ModelOpt quantized and set quantization method ( sgl-project#5145 )\n\n* [AMD] Fix missing per_token_group_quant_fp8 for ROCm ( sgl-project#5140 )\n\n* fix multimodal hash feature ( sgl-project#5083 )\n\n* Fix run time error in ROCm platform ( sgl-project#5147 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\n\n* [FA3 Feature] Support multi modal Llama-3.2-11B-Vision-Instruct ( sgl-project#5103 )\n\n* Add unit test on page_size > 1 and mla and  integration test for Flash Attention 3 ( sgl-project#4760 )\n\n* Use public model for FA3 speculative decode testing ( sgl-project#5152 )\n\n* Add dummy grok test to amd CI. ( sgl-project#5115 )\n\n* fix empty_cache error in pt_weights_iterator ( sgl-project#5151 )\n\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\n\n* Fix torch compile errors ( sgl-project#5158 )\n\n* Fix loading KV quantization scale; Enable modelopt kv cache ( sgl-project#4686 )\n\nCo-authored-by: qingquansong <ustcsqq@gmail.com>\n\n* [PD] Fix unclosed prefill connection warning of mini_lb ( sgl-project#5155 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Add optimized native kernels in sgl-kernel ( sgl-project#5150 )\n\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\n\n* [PD] Simplify mini LB ( sgl-project#4911 )\n\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\n\n* Small improvement of native api docs ( sgl-project#5139 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* [feat&refactor] Enhance multimodal input support with refactor io_struct ( sgl-project#4938 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Support 2x8xH100 for Llama 4 ( sgl-project#5159 )\n\n* FP4 weight loading and inference (2/2) ( sgl-project#3972 )\n\n* Fix multimodal hashing error ( sgl-project#5174 )\n\n* Tiny disable model that does not work ( sgl-project#5175 )\n\n* [Bugfix] Fix index out of bounds in local attention with large sequences ( sgl-project#5173 )\n\n* [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* docs: remove the use of Downward API for LWS_WORKER_INDEX ( sgl-project#5110 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* feat: add DeepGEMM build warning ( sgl-project#5176 )\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* fix: use DeepEPDispatcher on CUDA ( sgl-project#5180 )\n\n* [DeepEP] fix: import buffer error ( sgl-project#5179 )\n\n* Let `bench_one_batch` support `enable_dp_attention` ( sgl-project#4058 )\n\n* [Misc] clean up vllm in sgl-kernel test ( sgl-project#5189 )\n\n* Fix ci test \"test_eval_fp8_accuracy\" failed ( sgl-project#5185 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\n\n* Optimize topk operation in llama4 ( sgl-project#5128 )\n\n* Support Llama4 fp8 inference ( sgl-project#5194 )\n\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* [ci] fix ci test fused_moe op ( sgl-project#5102 )\n\n* model: support mllama4 ( sgl-project#5144 )\n\n* update grok test ( sgl-project#5171 )\n\n* sgl-kernel use cutlass latest version for fp8 blockwise gemm ( sgl-project#5207 )\n\n* Add H20 dtype fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5196 )\n\n* fix: log warning when disable cuda graph ( sgl-project#5209 )\n\n* [metrics] Add in queue metrics ( sgl-project#4444 )\n\n* Fix DeepSeek error when using DeepEP mode ( sgl-project#5190 )\n\n* reduce moe_align_block_size_kernel small batch mode overhead ( sgl-project#5086 )\n\n* [PD] Support KV transfer with mooncake ( sgl-project#4880 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\n\n* [PD] Add get_contiguous_buf_infos interface for MLATokenToKVPool ( sgl-project#5204 )\n\n* Update deps for mllama4 ( sgl-project#5215 )\n\n* Fix deepseek-v3 with torch.compile in PyTorch 2.6. ( sgl-project#5213 )\n\n* ROCm sgl-kernel: compatible to later torch ( sgl-project#5167 )\n\n* [Misc] Clean sgl-kernel test  ( sgl-project#5216 )\n\n* Update Makefile / build script to avoid installing incompatible torch dependency ( sgl-project#5245 )\n\n* Fix torch.compile cacheing ( sgl-project#5259 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* ROCm/AITER CK_MoE: update 2-stage kernels & support both Activations ( sgl-project#5228 )\n\n* Optimize attention in llama4 ( sgl-project#5127 )\n\n* Optimize GPU memory usage in FlashAttentionBackend's strided indexing ( sgl-project#5262 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* Support `--enable-llama4-multimodal` ( sgl-project#5254 )\n\n* [fix] fix mrope positions not picked up ( sgl-project#5265 )\n\n* doc: nested loop code for offline engine ( sgl-project#5244 )\n\n* fix: examples for token_in_token_out_vlm  ( sgl-project#5193 )\n\n* Fix a 404 link in send_request.ipynb ( sgl-project#5280 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* fix: enable fp4 compilation on cu128 ( sgl-project#5286 )\n\n* feat: add cu128 identifier for sgl-kernel ( sgl-project#5287 )\n\n* chore: relax the torch version restriction for sgl-kernel compilation ( sgl-project#5288 )\n\n* chore: bump sgl-kernel v0.0.8.post1 ( sgl-project#5289 )\n\n* [PD] fix: skip warmup request in disaggregation mode to prevent crash on timeout ( sgl-project#5292 )\n\n* [Docs] Supported Model Docs - Major restructuring ( sgl-project#5290 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* fix: update update_wheel_index for cu128 ( sgl-project#5300 )\n\n* [Docs] Remove the older supported docs section ( sgl-project#5301 )\n\n* remove moe_align_block_size torch.zeros in small batch/expert mode ( sgl-project#5298 )\n\n* feat: add blackwell Dockerfile ( sgl-project#5302 )\n\n* feat: add blackwell workflow ( sgl-project#5303 )\n\n* fix: use fa3 unit test on hopper only ( sgl-project#5304 )\n\n* misc: update blackwell Dockerfile ( sgl-project#5306 )\n\n* fix: remove cublas_grouped_gemm ( sgl-project#5307 )\n\n* fix: update flash attn ( sgl-project#5308 )\n\n* fix: use deepgemm only on hopper ( sgl-project#5310 )\n\n* [VLM] Adopt fast image processor by default ( sgl-project#5065 )\n\n* Adjust ci test threshold ( sgl-project#5271 )\n\n* Blackwell Cutlass MLA kernel ( sgl-project#5142 )\n\n* misc: cleanup 3rdparty ( sgl-project#5311 )\n\n* update variable naming and comments for rocm ( sgl-project#5299 )\n\n* Fix w8a8_int8 model shared experts fusion load weights error ( sgl-project#5120 )\n\n* Add flash_attn_varlen_func to sgl-kernel ( sgl-project#5315 )\n\n* Fix fa3 window size setup ( sgl-project#5316 )\n\n* chore: bump sgl-kernel v0.0.8.post2 ( sgl-project#5317 )\n\n* feat: use fa3 mla by default on hopper ( sgl-project#5210 )\n\nCo-authored-by: yundai424 <yundai424@gmail.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* Fix: docs/backend/structured_outputs.ipynb ( sgl-project#4884 )\n\n* Delete python/sglang/srt/layers/moe/fused_moe_triton/configs/E=257,N=\u2026 ( sgl-project#5321 )\n\n* refine fused_moe tuning docs ( sgl-project#5294 )\n\n* Support server based rollout in Verlengine ( sgl-project#4848 )\n\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\n\n* [Feat] Add sparse attn to sgl-kernel ( sgl-project#5327 )\n\n* fix: solve cu118 issue for cutlass mla ( sgl-project#5331 )\n\n* chore: bump sgl-kernel v0.0.8.post3 ( sgl-project#5332 )\n\n* ci: update release node ( sgl-project#5333 )\n\n* fix: determine if flashinfer is installed ( sgl-project#5336 )\n\n* feat: adapt merge_state ( sgl-project#5337 )\n\n* misc: update sagemaker Dockerfile ( sgl-project#5341 )\n\n* Fix: Ensure tensors for dist.broadcast match NCCL backend device ( sgl-project#5322 )\n\n* docs: update adoption and sponsorship list with Oracle ( sgl-project#5343 )\n\n* chore: upgrade sgl-kernel 0.0.8.post3 ( sgl-project#5342 )\n\n* Fix typo: infight -> inflight ( sgl-project#5357 )\n\n* [PD] Add transfer backend abstraction ( sgl-project#5328 )\n\n* fix MLATokenToKVPoolHost get_size_per_token bug ( sgl-project#5161 )\n\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\n\n* fix sgl-project#5322 ( sgl-project#5359 )\n\n* feat: update experiment_runner ( sgl-project#5360 )\n\n* [DeepEP] Reduce routed scaling overhead ( sgl-project#5277 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Fix DeepSeek DP Attention + torch compile ( sgl-project#5367 )\n\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Support for Qwen2.5-VL Model in bitsandbytes Format ( sgl-project#5003 )\n\n* Fix PD disaggregation bugs ( sgl-project#5326 )\n\n* [PD Bug] fix  MLA get_contiguous_buf_infos error ( sgl-project#5384 )\n\n* [perf] experimental enhance fp8 per-tensor quant ( sgl-project#5370 )\n\n* Apply deepseek cuda rope ( sgl-project#5385 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* apply fused moe gate in ds v3/r1 ( sgl-project#5371 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: update test config ( sgl-project#5392 )\n\n* [Fix] Turn off DeepGEMM by default ( sgl-project#5263 )\n\n* minor clean up of sgl-kernel/CMakeLists.txt ( sgl-project#5393 )\n\n* Add A800 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5368 )\n\n* Add H20 dtype fp8_w8a8 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5291 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* [fix/misc] remove duplicate row in deepseek v2 model ( sgl-project#5279 )\n\n* chore: upgrade DeepGEMM ( sgl-project#5395 )\n\n* fix: update pr-test-sgl-kernel ( sgl-project#5399 )\n\n* kernel: support slightly faster merge_state_v2 cuda kernel ( sgl-project#5381 )\n\n* chore: bump sgl-kernel 0.0.9 ( sgl-project#5400 )\n\n* chore: upgrade sgl-kernel 0.0.9 ( sgl-project#5401 )\n\n* Tiny fix DeepseekScalingRotaryEmbedding always use forward_native ( sgl-project#5406 )\n\n* Fix bench_serving with random-ids ( sgl-project#5214 )\n\n* [misc] fix ci flaky case ( sgl-project#5352 )\n\n* [FIX] Fix concatenation error in capture_bs when open --disable-cuda-graph-padding and without MTP ( sgl-project#5412 )\n\n* Support dynamic connection and TP 16 ( sgl-project#5351 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Fix broadcast use cuda device lead to memory capacity unbalanced ( sgl-project#5416 )\n\n* [PD] Fix dynamic port support and MLA buffer for Mooncake ( sgl-project#5415 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\n\n* Distinguish bootstrap key only in decode server ( sgl-project#5422 )\n\n* [PD] Remove unused bootstrap param and fix port table type ( sgl-project#5423 )\n\n* [minor] cleanup cmakelists.txt ( sgl-project#5420 )\n\n* bugfix: fix merge_state_v2 cuda graph ( sgl-project#5419 )\n\n* chore: bump sgl-kernel v0.0.9.post1 ( sgl-project#5430 )\n\n* fix: solve release issue ( sgl-project#5434 )\n\n* BLackwell cutlass mla: Add check for bad page size/block num combinations ( sgl-project#5431 )\n\n* feat: update model_specific_adjustment ( sgl-project#5344 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* chore: upgrade sgl-kernel 0.0.9.post1 ( sgl-project#5436 )\n\n* Fix ignore_eos parameter when loading a chat template ( sgl-project#5264 )\n\n* add attention backend supporting matrix in the doc ( sgl-project#5211 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\n\n* Support BNB quantization for llama/mllama ( sgl-project#5038 )\n\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\n\n* [Docs] Update start/install.md ( sgl-project#5398 )\n\n* [Minor] Move torch.compile patch to a better place ( sgl-project#5397 )\n\n* [Bug fix] need record start time in pd mode ( sgl-project#5425 )\n\n* Support MHA with chunked prefix cache for DeepSeek chunked prefill ( sgl-project#5113 )\n\n* chore: bump v0.4.5.post1 ( sgl-project#5445 )\n\n* Revert \"[SW-226289] rebase sglang to tag v0.4.5 ( sgl-project#12 )\"\n\nThis reverts commit 0eac714 .\n\n---------\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: chaobo jia <91889375+jcbjcbjc@users.noreply.github.com>\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\nCo-authored-by: Fr4nk1in <sh.fu@outlook.com>\nCo-authored-by: yinfan98 <1106310035@qq.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\nCo-authored-by: SEPLOS <seplos@aliyun.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: GeLee <leege233@gmail.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\nCo-authored-by: Kaiyu Yang <yangky@umich.edu>\nCo-authored-by: renxin <90580890+renxinx@users.noreply.github.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: Tommy Yang <tommyyang0524@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\nCo-authored-by: Yun Dai <yundai424@gmail.com>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: kk <43161300+kkHuang-amd@users.noreply.github.com>\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DangKai <dangkai4u@outlook.com>\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: Ma Mingfei <mingfei.ma@intel.com>\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: grimoire <streetyao@live.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: Zhaoyang Hao <77828610+Muuuchen@users.noreply.github.com>\nCo-authored-by: Teng Ma <805522925@qq.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: Richard Zou <zou3519@users.noreply.github.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: Yusong Gao <yusong.gao@icloud.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: tianlian yi <91449279+yitianlian@users.noreply.github.com>\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\nCo-authored-by: yulei <yuulei12@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Yangcheng Li <bluebluelitchi@hotmail.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\nCo-authored-by: mRSun15 <3150105645@zju.edu.cn>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com> This was referenced May 12, 2025 Add fp8 gemm kernel for CPU in sgl-kernel and add gemm UT #6216 Merged Add fp8 shared_expert kernel for CPU in sgl-kernel and add UT #6339 Merged Add fp8 fused_experts kernel for CPU in sgl-kernel and add UT #6404 Merged yanbing-j mentioned this pull request May 19, 2025 Update extend/decode attention kernel for CPU in sgl-kernel and add UTs #6405 Merged 6 tasks mingfeima added sgl-kernel intel cpu cpu backend performance optimization labels May 21, 2025 blzheng mentioned this pull request May 21, 2025 Add fp8 qkv_proj_with_rope kernel for CPU in sgl-kernel and add UT #6493 Merged 6 tasks jianan-gu mentioned this pull request Jul 11, 2025 [RFC] Intra-node shared memory (SHM) optimizations for communication operators on CPUs pytorch/gloo#455 Open 4 tasks Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:58:57",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-R1"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=deepseek-ai/DeepSeek-R1"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model deepseek-ai/DeepSeek-R1",
  "commit_subject": "Add optimized native kernels in sgl-kernel (#5150)",
  "commit_message": "Add optimized native kernels in sgl-kernel (#5150)\n\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>",
  "commit_date": "2025-04-08T09:37:46-07:00",
  "files_changed": [
    "sgl-kernel/csrc/cpu/activation.cpp",
    "sgl-kernel/csrc/cpu/bmm.cpp",
    "sgl-kernel/csrc/cpu/common.h",
    "sgl-kernel/csrc/cpu/decode.cpp",
    "sgl-kernel/csrc/cpu/extend.cpp",
    "sgl-kernel/csrc/cpu/gemm.cpp",
    "sgl-kernel/csrc/cpu/gemm.h",
    "sgl-kernel/csrc/cpu/gemm_int8.cpp",
    "sgl-kernel/csrc/cpu/interface.cpp",
    "sgl-kernel/csrc/cpu/moe.cpp",
    "sgl-kernel/csrc/cpu/moe_int8.cpp",
    "sgl-kernel/csrc/cpu/norm.cpp",
    "sgl-kernel/csrc/cpu/qkv_proj.cpp",
    "sgl-kernel/csrc/cpu/rope.cpp",
    "sgl-kernel/csrc/cpu/shm.cpp",
    "sgl-kernel/csrc/cpu/shm.h",
    "sgl-kernel/csrc/cpu/topk.cpp",
    "sgl-kernel/csrc/cpu/torch_extension_cpu.cpp",
    "sgl-kernel/csrc/cpu/vec.h",
    "sgl-kernel/setup_cpu.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 7792,
    "num_files": 20,
    "num_hunks": 20,
    "num_non_test_edited_lines": 7792,
    "num_non_test_files": 20,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/sgl-kernel/csrc/cpu/activation.cpp b/sgl-kernel/csrc/cpu/activation.cpp\nnew file mode 100644\nindex 000000000..debf5b244\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/activation.cpp\n@@ -0,0 +1,79 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t, typename func_t, typename vec_func_t>\n+void act_and_mul_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    const scalar_t* __restrict__ input,\n+    int64_t num_tokens,\n+    int64_t dim,\n+    const func_t& f,\n+    const vec_func_t& vf) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  constexpr int64_t kVecSize = bVec::size();\n+  at::parallel_for(0, num_tokens, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t i = begin; i < end; ++i) {\n+      // local ptrs\n+      const scalar_t* __restrict__ input_ptr = input + i * 2 * dim;\n+      const scalar_t* __restrict__ input_other_ptr = input_ptr + dim;\n+      scalar_t* __restrict__ output_ptr = output + i * dim;\n+\n+      int64_t d;\n+#pragma GCC unroll 4\n+      for (d = 0; d <= dim - kVecSize; d += kVecSize) {\n+        bVec x_bvec = bVec::loadu(input_ptr + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        bVec y_bvec = bVec::loadu(input_other_ptr + d);\n+        fVec y_fvec0, y_fvec1;\n+        std::tie(y_fvec0, y_fvec1) = at::vec::convert_to_float(y_bvec);\n+\n+        x_fvec0 = vf(x_fvec0);\n+        x_fvec1 = vf(x_fvec1);\n+\n+        x_fvec0 = x_fvec0 * y_fvec0;\n+        x_fvec1 = x_fvec1 * y_fvec1;\n+\n+        x_bvec = convert_from_float_ext<scalar_t>(x_fvec0, x_fvec1);\n+        x_bvec.store(output_ptr + d);\n+      }\n+#pragma GCC unroll 4\n+      for (; d < dim; ++d) {\n+        float x_val = static_cast<float>(input_ptr[d]);\n+        float y_val = static_cast<float>(input_other_ptr[d]);\n+        output_ptr[d] = f(x_val) * y_val;\n+      }\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// input   : {num_tokens, 2 * d}\n+// output  : {num_tokens, d}\n+at::Tensor silu_and_mul_cpu(at::Tensor& input) {\n+  RECORD_FUNCTION(\"sgl-kernel::silu_and_mul_cpu\", std::vector<c10::IValue>({input}));\n+  auto sizes = input.sizes().vec();\n+  int64_t last_dim = input.ndimension() - 1;\n+  int64_t d = sizes[last_dim] / 2;\n+  sizes[last_dim] = d;\n+  int64_t num_tokens = input.numel() / input.size(-1);\n+  at::Tensor out = at::empty(sizes, input.options());\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(input.scalar_type(), \"silu_and_mul\", [&] {\n+    using Vec = at::vec::Vectorized<float>;\n+    act_and_mul_kernel_impl(\n+        out.data_ptr<scalar_t>(),\n+        input.data_ptr<scalar_t>(),\n+        num_tokens,\n+        d,\n+        [](float x) { return x / (1.f + std::exp(-x)); },\n+        [](Vec x) { return x / (Vec(1.f) + x.neg().exp()); });\n+  });\n+  return out;\n+}\ndiff --git a/sgl-kernel/csrc/cpu/bmm.cpp b/sgl-kernel/csrc/cpu/bmm.cpp\nnew file mode 100644\nindex 000000000..f7377a09c\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/bmm.cpp\n@@ -0,0 +1,122 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t>\n+void bmm_kernel_impl(\n+    scalar_t* __restrict__ out,\n+    const scalar_t* __restrict__ mat1,\n+    const scalar_t* __restrict__ mat2,\n+    int64_t B,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t mat1_strideB,\n+    int64_t mat1_strideM,\n+    int64_t out_strideB,\n+    int64_t out_strideM,\n+    float scale = 0.f) {\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  // mat2 contiguous in [B, N, K]\n+  int64_t mat2_strideB = N * K;\n+  int64_t mat2_strideN = K;\n+\n+  const bool use_brgemm = can_use_brgemm<scalar_t>(M);\n+\n+  // parallel on [B, MB, NB]\n+  at::parallel_for(0, B * MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    int64_t bs{0}, mb{0}, nb{0};\n+    data_index_init(begin, bs, B, mb, MB, nb, NB);\n+\n+    // for brgemm, use float32 for accumulate\n+    alignas(64) float Ctmp[BLOCK_M * BLOCK_N];\n+\n+    for (int i = begin; i < end; ++i) {\n+      UNUSED(i);\n+      int mb_start = mb * BLOCK_M;\n+      int mb_size = std::min(M - mb_start, BLOCK_M);\n+      int nb_start = nb * BLOCK_N;\n+      int nb_size = std::min(N - nb_start, BLOCK_N);\n+\n+      tinygemm_kernel<scalar_t>(\n+          /*   A */ mat1 + bs * mat1_strideB + mb_start * mat1_strideM,\n+          /*   B */ mat2 + bs * mat2_strideB + nb_start * mat2_strideN /* nb * BLOCK_N * K */,\n+          /*   C */ out + bs * out_strideB + mb_start * out_strideM + nb_start,\n+          /* Ctmp*/ Ctmp,\n+          /*   M */ mb_size,\n+          /*   N */ nb_size,\n+          /*   K */ K,\n+          /* lda */ mat1_strideM,\n+          /* ldb */ nb_size,\n+          /* ldc */ out_strideM,\n+          /* brg */ use_brgemm);\n+\n+      // move to the next index\n+      data_index_step(bs, B, mb, MB, nb, NB);\n+    }\n+\n+    if (use_brgemm) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// mat1 : [B, M, K]\n+// mat2 : [B, N, K] or [B, OC, IC]\n+// out  : [B, M, N]\n+// scale: [] 0-dim tensor for per tensor quant\n+//\n+void bmm_cpu(at::Tensor& out, at::Tensor& mat1, at::Tensor& mat2, bool is_vnni, std::optional<at::Tensor>& scale) {\n+  RECORD_FUNCTION(\"sgl-kernel::bmm_cpu\", std::vector<c10::IValue>({out, mat1, mat2}));\n+\n+  auto packed_w = is_vnni ? mat2 : convert_weight_packed(mat2);\n+\n+  // input and out could be non-contiguous\n+  // weight needs to be contiguous in [OC, IC] order\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(mat1);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(out);\n+  CHECK_INPUT(mat2);\n+  CHECK_DIM(3, out);\n+  CHECK_DIM(3, mat1);\n+  CHECK_DIM(3, mat2);\n+\n+  int64_t B = mat1.size(0);\n+  int64_t M = mat1.size(1);\n+  int64_t N = mat2.size(1);\n+  int64_t K = mat1.size(2);\n+\n+  TORCH_CHECK(!scale.has_value(), \"bmm: do not support fp8 weight for now.\")\n+  TORCH_CHECK(N % 32 == 0, \"tinygemm requires N to be 32x.\");\n+\n+  int64_t mat1_strideB = mat1.stride(0);\n+  int64_t mat1_strideM = mat1.stride(1);\n+  int64_t out_strideB = out.stride(0);\n+  int64_t out_strideM = out.stride(1);\n+\n+  // check shapes\n+  TORCH_CHECK(mat2.size(0) == B && mat2.size(2) == K, \"bmm: mat2 shape mismatch!\");\n+  TORCH_CHECK(out.size(0) == B && out.size(1) == M, \"bmm: out shape mismatch!\");\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(mat1.scalar_type(), \"bmm_kernel_impl\", [&] {\n+    bmm_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        mat1.data_ptr<scalar_t>(),\n+        packed_w.data_ptr<scalar_t>(),\n+        B,\n+        M,\n+        N,\n+        K,\n+        mat1_strideB,\n+        mat1_strideM,\n+        out_strideB,\n+        out_strideM);\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/common.h b/sgl-kernel/csrc/cpu/common.h\nnew file mode 100644\nindex 000000000..0d340a756\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/common.h\n@@ -0,0 +1,164 @@\n+#pragma once\n+\n+#include <ATen/ATen.h>\n+#include <ATen/Parallel.h>\n+#include <ATen/record_function.h>\n+\n+#if defined(_OPENMP)\n+#include <omp.h>\n+#endif\n+\n+namespace {\n+\n+// dispatch bool\n+#define AT_DISPATCH_BOOL(BOOL_V, BOOL_NAME, ...) \\\n+  [&] {                                          \\\n+    if (BOOL_V) {                                \\\n+      constexpr bool BOOL_NAME = true;           \\\n+      return __VA_ARGS__();                      \\\n+    } else {                                     \\\n+      constexpr bool BOOL_NAME = false;          \\\n+      return __VA_ARGS__();                      \\\n+    }                                            \\\n+  }()\n+\n+// dispatch: bfloat16, float16, int8_t\n+#define CPU_DISPATCH_PACKED_TYPES(TYPE, ...)                     \\\n+  [&] {                                                          \\\n+    switch (TYPE) {                                              \\\n+      case at::ScalarType::BFloat16: {                           \\\n+        using packed_t = at::BFloat16;                           \\\n+        return __VA_ARGS__();                                    \\\n+      }                                                          \\\n+      case at::ScalarType::Half: {                               \\\n+        using packed_t = at::Half;                               \\\n+        return __VA_ARGS__();                                    \\\n+      }                                                          \\\n+      case at::ScalarType::Char: {                               \\\n+        using packed_t = int8_t;                                 \\\n+        return __VA_ARGS__();                                    \\\n+      }                                                          \\\n+      default:                                                   \\\n+        TORCH_CHECK(false, \"Unsupported floating data type.\\n\"); \\\n+    }                                                            \\\n+  }()\n+\n+#define UNUSED(x) (void)(x)\n+\n+#define CHECK_CPU(x) TORCH_CHECK(x.device().type() == at::kCPU, #x \" must be a CPU tensor\")\n+\n+#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n+#define CHECK_LAST_DIM_CONTIGUOUS(x) \\\n+  TORCH_CHECK(x.strides()[x.strides().size() - 1] == 1, #x \"must be contiguous at last dimention\")\n+\n+#define CHECK_INPUT(x) \\\n+  CHECK_CPU(x);        \\\n+  CHECK_CONTIGUOUS(x)\n+#define CHECK_LAST_DIM_CONTIGUOUS_INPUT(x) \\\n+  CHECK_CPU(x);                            \\\n+  CHECK_LAST_DIM_CONTIGUOUS(x)\n+\n+#define CHECK_DIM(d, x) TORCH_CHECK(x.dim() == d, #x \" must be a \" #d \"D tensor\")\n+\n+#define CHECK_EQ(a, b) TORCH_CHECK((a) == (b), \"CHECK_EQ(\" #a \", \" #b \") failed. \", a, \" vs \", b)\n+\n+// parallel routines\n+constexpr int GRAIN_SIZE = 1024;\n+\n+template <typename T, typename std::enable_if<std::is_integral<T>::value, int>::type = 0>\n+inline T div_up(T x, T y) {\n+  return (x + y - 1) / y;\n+}\n+\n+template <typename T>\n+inline void balance211(T n, T nth, T ith, T& n_start, T& n_end) {\n+#if 0\n+    // onednn partition pattern\n+    T& n_my = n_end;\n+    if (nth <= 1 || n == 0) {\n+        n_start = 0;\n+        n_my = n;\n+    } else {\n+        T n1 = div_up(n, nth);\n+        T n2 = n1 - 1;\n+        T T1 = n - n2 * nth;\n+        n_my = ith < T1 ? n1 : n2;\n+        n_start = ith <= T1 ? ith*n1 : T1 * n1 + (ith - T1) * n2;\n+    }\n+    n_end += n_start;\n+#else\n+  // pytorch aten partition pattern\n+  T n_my = div_up(n, nth);\n+  n_start = ith * n_my;\n+  n_end = std::min(n_start + n_my, n);\n+#endif\n+}\n+\n+template <typename func_t>\n+inline void parallel_for(int n, const func_t& f) {\n+#if defined(_OPENMP)\n+#pragma omp parallel\n+  {\n+    int nth = omp_get_num_threads();\n+    int ith = omp_get_thread_num();\n+    int tbegin, tend;\n+    balance211(n, nth, ith, tbegin, tend);\n+    f(tbegin, tend);\n+  }\n+#else\n+  f(0, n);\n+#endif\n+}\n+\n+// data indexing for dimension collapse\n+template <typename T>\n+inline T data_index_init(T offset) {\n+  return offset;\n+}\n+\n+template <typename T, typename... Args>\n+inline T data_index_init(T offset, T& x, const T& X, Args&&... args) {\n+  offset = data_index_init(offset, std::forward<Args>(args)...);\n+  x = offset % X;\n+  return offset / X;\n+}\n+\n+inline bool data_index_step() {\n+  return true;\n+}\n+\n+template <typename T, typename... Args>\n+inline bool data_index_step(T& x, const T& X, Args&&... args) {\n+  if (data_index_step(std::forward<Args>(args)...)) {\n+    x = ((x + 1) == X) ? 0 : (x + 1);\n+    return x == 0;\n+  }\n+  return false;\n+}\n+\n+// forced unroll for perf critical path\n+\n+#if __has_attribute(always_inline)\n+#define ALWAYS_INLINE __attribute__((__always_inline__)) inline\n+#else\n+#define ALWAYS_INLINE inline\n+#endif\n+\n+template <int n>\n+struct Unroll {\n+  template <typename Func, typename... Args>\n+  ALWAYS_INLINE void operator()(const Func& f, Args... args) const {\n+    Unroll<n - 1>{}(f, args...);\n+    f(std::integral_constant<int, n - 1>{}, args...);\n+  }\n+};\n+\n+template <>\n+struct Unroll<1> {\n+  template <typename Func, typename... Args>\n+  ALWAYS_INLINE void operator()(const Func& f, Args... args) const {\n+    f(std::integral_constant<int, 0>{}, args...);\n+  }\n+};\n+\n+}  // anonymous namespace\ndiff --git a/sgl-kernel/csrc/cpu/decode.cpp b/sgl-kernel/csrc/cpu/decode.cpp\nnew file mode 100644\nindex 000000000..e469ffdc5\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/decode.cpp\n@@ -0,0 +1,1119 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// [NOTE] TODO list for this kernel:\n+//   1. tune the value for BLOCK_N\n+//   2. planning for {batches, num_heads, num_kv_splits}\n+//      and use actual num_kv_splits for small seq length\n+//   3. try fast impl of `.tanh()`\n+//   4. provide amx kernel for index_gemm_kernel_nn when M = 16\n+//\n+\n+inline void fill_stub(float* __restrict__ out, float val, int64_t size) {\n+  using Vec = at::vec::Vectorized<float>;\n+  const Vec data_vec(val);\n+  at::vec::map<float>([data_vec](Vec out) { return out = data_vec; }, out, out, size);\n+}\n+\n+template <typename scalar_t>\n+inline void copy_stub(scalar_t* __restrict__ out, const float* __restrict__ acc, float s, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  const fVec s_fvec = fVec(s);\n+  int64_t d = 0;\n+  for (; d <= size - bVec::size(); d += bVec::size()) {\n+    fVec a_fvec0 = fVec::loadu(acc + d) * s_fvec;\n+    fVec a_fvec1 = fVec::loadu(acc + d + fVec::size()) * s_fvec;\n+    bVec out_bvec = convert_from_float_ext<scalar_t>(a_fvec0, a_fvec1);\n+    out_bvec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(acc[d] * s);\n+  }\n+}\n+\n+// GEMM handles query @ key (indexed) x scale\n+//   A : [M, K]\n+//   B : [N, K] indexed\n+//   C : [M, N]\n+//\n+template <typename scalar_t, typename index_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nt {\n+  static inline void apply(\n+      const scalar_t* __restrict__ A,\n+      const scalar_t* __restrict__ B,\n+      float* __restrict__ C,\n+      const index_t* __restrict__ indices,\n+      float scale,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc,\n+      int64_t K,\n+      int64_t max_tokens) {\n+    for (int64_t m = 0; m < BLOCK_M; ++m) {\n+      for (int64_t n = 0; n < BLOCK_N; ++n) {\n+        float sum = 0.f;\n+        int64_t b_idx = indices[n];\n+        TORCH_CHECK(b_idx < max_tokens, \"token index out of scope!\");\n+        for (int64_t k = 0; k < K; ++k) {\n+          sum += scale * static_cast<float>(A[m * lda + k]) * static_cast<float>(B[b_idx * ldb + k]);\n+        }\n+        C[m * ldc + n] = sum;\n+      }\n+    }\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <typename index_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nt<at::BFloat16, index_t, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const at::BFloat16* __restrict__ A,\n+      const at::BFloat16* __restrict__ B,\n+      float* __restrict__ C,\n+      const index_t* __restrict__ indices,\n+      float scale,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc,\n+      int64_t K,\n+      int64_t max_tokens) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N;\n+\n+    __m512bh va;\n+    __m512bh vb[COLS];\n+    __m512 vc[ROWS * COLS];\n+    __m512 vscale = _mm512_set1_ps(scale);\n+\n+    auto loadc = [&](auto i) { vc[i] = _mm512_setzero_ps(); };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    // for main loop\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = (__m512bh)(_mm512_loadu_si512(A + row * lda + k));\n+      }\n+      if constexpr (row == 0) {\n+        if constexpr (col + 1 < COLS) {\n+          int64_t b_idx_prefetch = indices[col + 1];\n+          _mm_prefetch(B + b_idx_prefetch * ldb + k, _MM_HINT_T0);\n+        }\n+        int64_t b_idx = indices[col];\n+        TORCH_CHECK(b_idx < max_tokens, \"token index out of scope!\");\n+        vb[col] = (__m512bh)(_mm512_loadu_si512(B + b_idx * ldb + k));\n+      }\n+      vc[i] = _mm512_dpbf16_ps(vc[i], va, vb[col]);\n+    };\n+\n+    // for remainder\n+    auto compute2 = [&](auto i, int64_t k, __mmask32 mask) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = (__m512bh)(_mm512_maskz_loadu_epi16(mask, A + row * lda + k));\n+      }\n+      if constexpr (row == 0) {\n+        int64_t b_idx = indices[col];\n+        TORCH_CHECK(b_idx < max_tokens, \"token index out of scope!\");\n+        vb[col] = (__m512bh)(_mm512_maskz_loadu_epi16(mask, B + b_idx * ldb + k));\n+      }\n+      vc[i] = _mm512_dpbf16_ps(vc[i], va, vb[col]);\n+    };\n+\n+    int64_t k = 0;\n+    for (; k <= K - 32; k += 32) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+    int64_t count = K - k;\n+    if (count > 0) {\n+      __mmask32 mask = (1ULL << count) - 1;\n+      Unroll<ROWS * COLS>{}(compute2, k, mask);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      C[row * ldc + col] = _mm512_reduce_add_ps(_mm512_mul_ps(vc[i], vscale));\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NT(MB_SIZE, NB_SIZE)               \\\n+  tinygemm_kernel_nt<scalar_t, index_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda, B, C + mb_start * ldc + nb_start, indices + nb_start, scale, lda, ldb, ldc, K, max_tokens);\n+\n+// this is used when N isn't multiple of 16,\n+// N corresponds to `head_size_v` which should be 16x\n+template <typename scalar_t, typename index_t>\n+inline void tinygemm_kernel_nn_scalar(\n+    const float* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    float* __restrict__ C,\n+    const index_t* __restrict__ indices,\n+    const float* __restrict__ scale,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    int64_t max_tokens) {\n+  for (int64_t m = 0; m < M; ++m) {\n+    for (int64_t n = 0; n < N; ++n) {\n+      C[m * ldc + n] *= scale[m];\n+      for (int64_t k = 0; k < K; ++k) {\n+        int64_t b_idx = indices[k];\n+        TORCH_CHECK(b_idx < max_tokens, \"token index out of scope!\");\n+        C[m * ldc + n] += A[m * lda + k] * static_cast<float>(B[b_idx * ldb + n]);\n+      }\n+    }\n+  }\n+}\n+\n+// GEMM handles v' * scale + attn @ value (indexed)\n+//   A : [M, K]\n+//   B : [K, N] indexed\n+//   C \uff1a[M, N]\n+//\n+template <typename scalar_t, typename index_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn {\n+  static inline void apply(\n+      const float* __restrict__ A,\n+      const scalar_t* __restrict__ B,\n+      float* __restrict__ C,\n+      const index_t* __restrict__ indices,\n+      const float* __restrict__ scale,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc,\n+      int64_t K,\n+      int64_t max_tokens) {\n+    tinygemm_kernel_nn_scalar(A, B, C, indices, scale, BLOCK_M, BLOCK_N, K, lda, ldb, ldc, max_tokens);\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <typename index_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn<at::BFloat16, index_t, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const float* __restrict__ A,\n+      const at::BFloat16* __restrict__ B,\n+      float* __restrict__ C,\n+      const index_t* __restrict__ indices,\n+      const float* __restrict__ scale,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc,\n+      int64_t K,\n+      int64_t max_tokens) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+\n+    __m512 va;\n+    __m512 vb[COLS];\n+    __m512 vc[ROWS * COLS];\n+    __m512 vscale;\n+\n+    auto loadc = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+#pragma GCC diagnostic push\n+#pragma GCC diagnostic ignored \"-Warray-bounds\"\n+      if constexpr (col == 0) {\n+        vscale = _mm512_set1_ps(scale[row]);\n+      }\n+#pragma GCC diagnostic pop\n+      vc[i] = _mm512_loadu_ps(C + row * ldc + col * 16);\n+      vc[i] = _mm512_mul_ps(vc[i], vscale);\n+    };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = _mm512_set1_ps(A[row * lda + k]);\n+      }\n+      if constexpr (row == 0) {\n+        if (k + 1 < K) {\n+          int64_t b_idx_prefetch = indices[k + 1];\n+          _mm_prefetch(B + b_idx_prefetch * ldb + col * 16, _MM_HINT_T0);\n+        }\n+        int64_t b_idx = indices[k];\n+        TORCH_CHECK(b_idx < max_tokens, \"token index out of scope!\");\n+\n+        // for COLS = 2, 4, 6, 8 use 512 bit load\n+        // for COLS = 1, 3, 5, 7 use 256 bit load\n+        if constexpr (COLS % 2 == 0) {\n+          if constexpr (col % 2 == 0) {\n+            __m512i b16 = _mm512_loadu_si512(reinterpret_cast<const __m512i*>(B + b_idx * ldb + col * 16));\n+            vb[col + 0] = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(b16, 0));\n+            vb[col + 1] = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(b16, 1));\n+          }\n+        } else {\n+          __m256i b16 = _mm256_loadu_si256(reinterpret_cast<const __m256i*>(B + b_idx * ldb + col * 16));\n+          vb[col] = CVT_BF16_TO_FP32(b16);\n+        }\n+      }\n+      vc[i] = _mm512_fmadd_ps(va, vb[col], vc[i]);\n+    };\n+\n+    for (int64_t k = 0; k < K; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      _mm512_storeu_ps(C + row * ldc + col * 16, vc[i]);\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NN(MB_SIZE, NB_SIZE)               \\\n+  tinygemm_kernel_nn<scalar_t, index_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda,                                         \\\n+      B + nb_start,                                               \\\n+      C + mb_start * ldc + nb_start,                              \\\n+      indices,                                                    \\\n+      scale + mb_start,                                           \\\n+      lda,                                                        \\\n+      ldb,                                                        \\\n+      ldc,                                                        \\\n+      K,                                                          \\\n+      max_tokens);\n+\n+template <typename scalar_t, typename index_t>\n+void index_gemm_kernel_nt(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    float* __restrict__ C,\n+    const index_t* __restrict__ indices,\n+    float scale,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    int64_t max_tokens) {\n+  // pattern: 1-8-8\n+  if (M == 1) {\n+    constexpr int64_t BLOCK_N = 8;\n+    const int64_t NB = div_up(N, BLOCK_N);\n+    int64_t mb_start = 0, lda = 1, ldc = 1;\n+\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (nb_size) {\n+        case 1:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 1);\n+          break;\n+        case 2:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 2);\n+          break;\n+        case 3:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 3);\n+          break;\n+        case 4:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 4);\n+          break;\n+        case 5:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 5);\n+          break;\n+        case 6:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 6);\n+          break;\n+        case 7:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 7);\n+          break;\n+        case 8:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 8);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, 1x\", \"nb_size\");\n+      }\n+    }\n+    return;\n+  }\n+\n+  // pattern: 1-6-24\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 6;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  for (int64_t mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size) {\n+        // mb_size = 1\n+        case 0x11:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 1);\n+          break;\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 2);\n+          break;\n+        case 0x13:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 3);\n+          break;\n+        case 0x14:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 4);\n+          break;\n+        case 0x15:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 5);\n+          break;\n+        case 0x16:\n+          LAUNCH_TINYGEMM_KERNEL_NT(1, 6);\n+          break;\n+        // mb_size = 2\n+        case 0x21:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 1);\n+          break;\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 2);\n+          break;\n+        case 0x23:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 3);\n+          break;\n+        case 0x24:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 4);\n+          break;\n+        case 0x25:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 5);\n+          break;\n+        case 0x26:\n+          LAUNCH_TINYGEMM_KERNEL_NT(2, 6);\n+          break;\n+        // mb_size = 3\n+        case 0x31:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 1);\n+          break;\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 2);\n+          break;\n+        case 0x33:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 3);\n+          break;\n+        case 0x34:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 4);\n+          break;\n+        case 0x35:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 5);\n+          break;\n+        case 0x36:\n+          LAUNCH_TINYGEMM_KERNEL_NT(3, 6);\n+          break;\n+        // mb_size = 4\n+        case 0x41:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 1);\n+          break;\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 2);\n+          break;\n+        case 0x43:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 3);\n+          break;\n+        case 0x44:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 4);\n+          break;\n+        case 0x45:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 5);\n+          break;\n+        case 0x46:\n+          LAUNCH_TINYGEMM_KERNEL_NT(4, 6);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, typename index_t>\n+void index_gemm_kernel_nn(\n+    const float* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    float* __restrict__ C,\n+    const index_t* __restrict__ indices,\n+    float* __restrict__ scale,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    int64_t max_tokens) {\n+  constexpr int kVecSize = 16;\n+  if ((N & (kVecSize - 1)) != 0) {\n+    tinygemm_kernel_nn_scalar(A, B, C, indices, scale, M, N, K, lda, ldb, ldc, max_tokens);\n+    return;\n+  }\n+\n+  // pattern: 1-8-8\n+  if (M == 1) {\n+    constexpr int64_t BLOCK_N = 8 * kVecSize;\n+    const int64_t NB = div_up(N, BLOCK_N);\n+    int64_t mb_start = 0, lda = 1, ldc = 1;\n+\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (nb_size >> 4) {\n+        case 1:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 16);\n+          break;\n+        case 2:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 32);\n+          break;\n+        case 3:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 48);\n+          break;\n+        case 4:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 64);\n+          break;\n+        case 5:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 80);\n+          break;\n+        case 6:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 96);\n+          break;\n+        case 7:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 112);\n+          break;\n+        case 8:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 128);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, 1x\", \"nb_size\");\n+      }\n+    }\n+    return;\n+  }\n+\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 6 * kVecSize;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  for (int64_t mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        // mb_size = 1\n+        case 0x11:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 16);\n+          break;\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 32);\n+          break;\n+        case 0x13:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 48);\n+          break;\n+        case 0x14:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 64);\n+          break;\n+        case 0x15:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 80);\n+          break;\n+        case 0x16:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 96);\n+          break;\n+        // mb_size = 2\n+        case 0x21:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 16);\n+          break;\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 32);\n+          break;\n+        case 0x23:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 48);\n+          break;\n+        case 0x24:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 64);\n+          break;\n+        case 0x25:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 80);\n+          break;\n+        case 0x26:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 96);\n+          break;\n+        // mb_size = 3\n+        case 0x31:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 16);\n+          break;\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 32);\n+          break;\n+        case 0x33:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 48);\n+          break;\n+        case 0x34:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 64);\n+          break;\n+        case 0x35:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 80);\n+          break;\n+        case 0x36:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 96);\n+          break;\n+        // mb_size = 4\n+        case 0x41:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 16);\n+          break;\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 32);\n+          break;\n+        case 0x43:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 48);\n+          break;\n+        case 0x44:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 64);\n+          break;\n+        case 0x45:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 80);\n+          break;\n+        case 0x46:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 96);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, typename index_t>\n+void decode_attention_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    float* __restrict__ attn_logits,\n+    const scalar_t* __restrict__ query,\n+    const scalar_t* __restrict__ k_buffer,\n+    const scalar_t* __restrict__ v_buffer,\n+    const index_t* __restrict__ req_to_token,\n+    const int64_t* __restrict__ req_pool_indices,\n+    const int64_t* __restrict__ seq_lens,\n+    int64_t batches,\n+    int64_t num_heads,\n+    int64_t head_size,\n+    int64_t head_size_v,\n+    int64_t num_kv_splits,\n+    int64_t k_strideN,\n+    int64_t k_strideH,\n+    int64_t v_strideN,\n+    int64_t v_strideH,\n+    float scaling,\n+    float logit_cap,\n+    int64_t max_num_reqs,\n+    int64_t max_context_len,\n+    int64_t max_total_num_tokens) {\n+  using Vec = at::vec::Vectorized<float>;\n+\n+  // block length for k_buffer and v_buffer\n+  constexpr int64_t BLOCK_N = 256;\n+\n+  // strides\n+  const int64_t q_strideM = num_heads * head_size;\n+  const int64_t q_strideH = head_size;\n+  const int64_t l_stride1 = num_kv_splits * (head_size_v + 1);\n+  const int64_t l_stride2 = head_size_v + 1;\n+\n+  const bool has_logit_cap = logit_cap > 0;\n+  float rlogit_cap = has_logit_cap ? 1 / logit_cap : 0.f;\n+\n+  // parallel on [batches, num_heads, num_kv_splits]\n+  at::parallel_for(0, batches * num_heads * num_kv_splits, 0, [&](int64_t begin, int64_t end) {\n+    int64_t bs{0}, head_id{0}, kv_id{0};\n+    data_index_init(begin, bs, batches, head_id, num_heads, kv_id, num_kv_splits);\n+\n+    // s_prime and s_delta\n+    alignas(64) float s_i[BLOCK_N];\n+    float* __restrict__ s_delta = s_i;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      // get query\n+      const scalar_t* __restrict__ q_ptr = query + bs * q_strideM + head_id * q_strideH;\n+\n+      // get key/value\n+      int64_t seq_len_kv = seq_lens[bs];\n+      int64_t req_pool_id = req_pool_indices[bs];\n+      TORCH_CHECK(seq_len_kv <= max_context_len, \"seq_len_kv out of scope!\");\n+      TORCH_CHECK(req_pool_id < max_num_reqs, \"req_pool_id out of scope!\");\n+\n+      const int64_t SPLIT_SIZE = div_up(seq_len_kv, num_kv_splits);\n+      const int64_t kv_start = kv_id * SPLIT_SIZE;\n+      const int64_t kv_end = std::min(kv_start + SPLIT_SIZE, seq_len_kv);\n+\n+      float m_prime = -std::numeric_limits<float>::infinity();\n+      float s_prime = 0.f;\n+\n+      // get v_prime, and init to zero\n+      float* __restrict__ v_prime = attn_logits + i * (head_size_v + 1);\n+      fill_stub(v_prime, 0.f, head_size_v);\n+\n+      // loop over K and V sequence with BLOCK_N\n+      for (int64_t n = kv_start; n < kv_end; n += BLOCK_N) {\n+        int64_t n_size = std::min(BLOCK_N, kv_end - n);\n+\n+        // calculate s_i <- scale * Q @ K\n+        index_gemm_kernel_nt<scalar_t, index_t>(\n+            /* A   */ q_ptr,\n+            /* B   */ k_buffer + head_id * k_strideH,\n+            /* C   */ s_i,\n+            /* ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /* scl */ scaling,\n+            /* M   */ 1,\n+            /* N   */ n_size,\n+            /* K   */ head_size,\n+            /* lda */ 1,\n+            /* ldb */ k_strideN,\n+            /* ldc */ 1,\n+            /* mtt */ max_total_num_tokens);\n+\n+        // TODO: `tanh` from torch uses sleef u10, going to be slow\n+        if (has_logit_cap) {\n+          at::vec::map<float>(\n+              [logit_cap, rlogit_cap](Vec x) { return Vec(logit_cap) * (x * Vec(rlogit_cap)).tanh(); },\n+              s_i,\n+              s_i,\n+              n_size);\n+        }\n+\n+        // m_i: max value per row\n+        float m_i = at::vec::reduce_all<float>([](Vec& x, Vec& y) { return at::vec::maximum(x, y); }, s_i, n_size);\n+        m_i = std::max(m_i, m_prime);\n+\n+        // m_delta <- exp(m' - m_i)\n+        float m_delta = std::exp(m_prime - m_i);\n+\n+        // s_delta <- exp(s_i - m_i)\n+        at::vec::map<float>([m_i](Vec x) { return (x - Vec(m_i)).exp_u20(); }, s_delta, s_i, n_size);\n+\n+        // s' <- s' * m_delta + sum(s_delta)\n+        s_prime *= m_delta;\n+        s_prime += at::vec::reduce_all<float>([](Vec& x, Vec& y) { return x + y; }, s_delta, n_size);\n+\n+        m_prime = m_i;\n+\n+        // caculate V' <- s_delta @ V + V' * m_delta\n+        index_gemm_kernel_nn<scalar_t, index_t>(\n+            /* A   */ s_delta,\n+            /* B   */ v_buffer + head_id * v_strideH,\n+            /* C   */ v_prime,\n+            /* ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /* scl */ &m_delta,\n+            /* M   */ 1,\n+            /* N   */ head_size_v,\n+            /* K   */ n_size,\n+            /* lda */ 1,\n+            /* ldb */ v_strideN,\n+            /* ldc */ 1,\n+            /* mtt */ max_total_num_tokens);\n+      }  // loop with KV blocks\n+\n+      // only update v' when kv_split_size > 0\n+      if (kv_end > kv_start) {\n+        float s = 1 / s_prime;\n+        at::vec::map<float>([s](Vec out) { return out * Vec(s); }, v_prime, v_prime, head_size_v);\n+\n+        v_prime[head_size_v] = m_prime + std::log(s_prime);\n+      }\n+\n+      // move to the next index\n+      data_index_step(bs, batches, head_id, num_heads, kv_id, num_kv_splits);\n+    }\n+  });\n+\n+  // parallel on [batches, num_heads]\n+  at::parallel_for(0, batches * num_heads, 0, [&](int64_t begin, int64_t end) {\n+    // NB: here we use logits[b][h][0] as acc, since\n+    // for the first kv split (kv_id == 0):\n+    //   m_delta = std::exp(-inf) = 0\n+    //   e_logic = std::exp(0) = 1\n+    //   acc = acc * m_delta + tv * e_logic = tv\n+    for (int64_t i = begin; i < end; ++i) {\n+      float* __restrict__ acc = attn_logits + i * l_stride1;\n+\n+      float s_prime = 0.f;\n+      float m_prime = -std::numeric_limits<scalar_t>::infinity();\n+\n+      // update acc with from each kv_split\n+      for (int64_t kv_id = 0; kv_id < num_kv_splits; ++kv_id) {\n+        float* __restrict__ tv = acc + kv_id * l_stride2;\n+        const float tlogic = (acc + kv_id * l_stride2)[head_size_v];\n+\n+        float m_i = std::max(tlogic, m_prime);\n+        float m_delta = std::exp(m_prime - m_i);\n+        float e_logic = std::exp(tlogic - m_i);\n+        if (kv_id != 0) {\n+          at::vec::map2<float>(\n+              [m_delta, e_logic](Vec x, Vec y) { return x * Vec(m_delta) + y * Vec(e_logic); },\n+              acc,\n+              acc,\n+              tv,\n+              head_size_v);\n+        }\n+\n+        s_prime = s_prime * m_delta + e_logic;\n+        m_prime = m_i;\n+      }\n+\n+      copy_stub<scalar_t>(output + i * head_size_v, acc, 1 / s_prime, head_size_v);\n+    }\n+  });\n+}\n+\n+template <typename scalar_t, typename index_t>\n+void decode_attention_grouped_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    float* __restrict__ attn_logits,\n+    const scalar_t* __restrict__ query,\n+    const scalar_t* __restrict__ k_buffer,\n+    const scalar_t* __restrict__ v_buffer,\n+    const index_t* __restrict__ req_to_token,\n+    const int64_t* __restrict__ req_pool_indices,\n+    const int64_t* __restrict__ seq_lens,\n+    int64_t batches,\n+    int64_t num_heads,\n+    int64_t num_heads_kv,\n+    int64_t head_size,\n+    int64_t head_size_v,\n+    int64_t num_kv_splits,\n+    int64_t k_strideN,\n+    int64_t k_strideH,\n+    int64_t v_strideN,\n+    int64_t v_strideH,\n+    float scaling,\n+    float logit_cap,\n+    int64_t max_num_reqs,\n+    int64_t max_context_len,\n+    int64_t max_total_num_tokens) {\n+  using Vec = at::vec::Vectorized<float>;\n+\n+  // block length for k_buffer and v_buffer\n+  constexpr int64_t BLOCK_N = 256;\n+  // block length for heads\n+  // we parallel on [batches, divup(num_heads, BLOCK_H), num_kv_splits]\n+  // use smaller BLOCK_H when batches is small to utilize all cores\n+  constexpr int64_t kBLOCK_H = 16;\n+  const int64_t BLOCK_H = std::min(4 * batches, kBLOCK_H);\n+\n+  // strides\n+  const int64_t q_strideM = num_heads * head_size;\n+  const int64_t q_strideH = head_size;\n+  const int64_t l_stride0 = num_heads * num_kv_splits * (head_size_v + 1);\n+  const int64_t l_stride1 = num_kv_splits * (head_size_v + 1);\n+  const int64_t l_stride2 = head_size_v + 1;\n+\n+  const bool has_logit_cap = logit_cap > 0;\n+  float rlogit_cap = has_logit_cap ? 1 / logit_cap : 0.f;\n+\n+  // partition the heads into blocks for parallel\n+  const int64_t num_groups = num_heads / num_heads_kv;\n+  const int64_t num_blocks = div_up(num_heads, std::min(BLOCK_H, num_groups));\n+  const int64_t num_groups_per_block = div_up(num_groups, BLOCK_H);\n+  const int64_t num_heads_per_block = std::min(num_groups, BLOCK_H);\n+\n+  // parallel on [batches, num_blocks, num_kv_splits]\n+  at::parallel_for(0, batches * num_blocks * num_kv_splits, 0, [&](int64_t begin, int64_t end) {\n+    int64_t bs{0}, head_id{0}, kv_id{0};\n+    data_index_init(begin, bs, batches, head_id, num_blocks, kv_id, num_kv_splits);\n+\n+    alignas(64) float s_i[BLOCK_H * BLOCK_N];\n+    float* __restrict__ s_delta = s_i;\n+\n+    alignas(64) float s_prime[BLOCK_H];\n+    alignas(64) float m_prime[BLOCK_H];\n+    alignas(64) float m_delta[BLOCK_H];\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      const int64_t h_start = head_id * num_heads_per_block;\n+      const int64_t h_end = std::min(h_start + num_heads_per_block, num_heads);\n+      const int64_t h_size = h_end - h_start;\n+\n+      // get query\n+      const scalar_t* __restrict__ q_ptr = query + bs * q_strideM + h_start * q_strideH;\n+\n+      // kv head id and valid block head size\n+      int64_t head_kv_id = head_id / num_groups_per_block;\n+      int64_t seq_len_kv = seq_lens[bs];\n+      int64_t req_pool_id = req_pool_indices[bs];\n+      TORCH_CHECK(seq_len_kv <= max_context_len, \"seq_len_kv out of scope!\");\n+      TORCH_CHECK(req_pool_id < max_num_reqs, \"req_pool_id out of scope!\");\n+\n+      const int64_t SPLIT_SIZE = div_up(seq_len_kv, num_kv_splits);\n+      const int64_t kv_start = kv_id * SPLIT_SIZE;\n+      const int64_t kv_end = std::min(kv_start + SPLIT_SIZE, seq_len_kv);\n+\n+      fill_stub(s_prime, 0.f, BLOCK_H);\n+      fill_stub(m_prime, -std::numeric_limits<float>::infinity(), BLOCK_H);\n+\n+      // get v_prime, and init to zero\n+      float* __restrict__ v_prime = attn_logits + bs * l_stride0 + h_start * l_stride1 + kv_id * l_stride2;\n+      for (int64_t h = 0; h < h_size; ++h) {\n+        fill_stub(v_prime + h * l_stride1, 0.f, head_size_v);\n+      }\n+\n+      // loop over K and V sequence with BLOCK_N\n+      for (int64_t n = kv_start; n < kv_end; n += BLOCK_N) {\n+        int64_t n_size = std::min(BLOCK_N, kv_end - n);\n+\n+        // calculate Q @ K\n+        index_gemm_kernel_nt<scalar_t, index_t>(\n+            /* A   */ q_ptr,\n+            /* B   */ k_buffer + head_kv_id * k_strideH,\n+            /* C   */ s_i,\n+            /* ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /* scl */ scaling,\n+            /* M   */ h_size,\n+            /* N   */ n_size,\n+            /* K   */ head_size,\n+            /* lda */ q_strideH,\n+            /* ldb */ k_strideN,\n+            /* ldc */ BLOCK_N,\n+            /* mtt */ max_total_num_tokens);\n+\n+        if (has_logit_cap) {\n+          at::vec::map<float>(\n+              [logit_cap, rlogit_cap](Vec x) { return Vec(logit_cap) * (x * Vec(rlogit_cap)).tanh(); },\n+              s_i,\n+              s_i,\n+              n_size);\n+        }\n+\n+        // update the scaling coefficients\n+        for (int64_t h = 0; h < h_size; ++h) {\n+          // m_i: max value per row\n+          float m_i = at::vec::reduce_all<float>(\n+              [](Vec& x, Vec& y) { return at::vec::maximum(x, y); }, s_i + h * BLOCK_N, n_size);\n+          m_i = std::max(m_i, m_prime[h]);\n+\n+          // m_delta <- exp(m' - m_i)\n+          m_delta[h] = std::exp(m_prime[h] - m_i);\n+\n+          // s_delta <- exp(s_i - m_i)\n+          at::vec::map<float>(\n+              [m_i](Vec x) { return (x - Vec(m_i)).exp_u20(); }, s_delta + h * BLOCK_N, s_i + h * BLOCK_N, n_size);\n+\n+          // s' <- s' * m_delta + sum(s_delta)\n+          s_prime[h] *= m_delta[h];\n+          s_prime[h] += at::vec::reduce_all<float>([](Vec& x, Vec& y) { return x + y; }, s_delta + h * BLOCK_N, n_size);\n+\n+          m_prime[h] = m_i;\n+        }\n+\n+        // caculate V' <- s_delta @ V + V' * m_delta\n+        index_gemm_kernel_nn<scalar_t, index_t>(\n+            /* A   */ s_delta,\n+            /* B   */ v_buffer + head_kv_id * v_strideH,\n+            /* C   */ v_prime,\n+            /* ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /* scl */ m_delta,\n+            /* M   */ h_size,\n+            /* N   */ head_size_v,\n+            /* K   */ n_size,\n+            /* lda */ BLOCK_N,\n+            /* ldb */ v_strideN,\n+            /* ldc */ l_stride1,\n+            /* mtt */ max_total_num_tokens);\n+      }  // loop with KV blocks\n+\n+      // only update v' when kv_split_size > 0\n+      if (kv_end > kv_start) {\n+        for (int64_t h = 0; h < h_size; ++h) {\n+          float s = 1 / s_prime[h];\n+          at::vec::map<float>(\n+              [s](Vec out) { return out * Vec(s); }, v_prime + h * l_stride1, v_prime + h * l_stride1, head_size_v);\n+          (v_prime + h * l_stride1)[head_size_v] = m_prime[h] + std::log(s_prime[h]);\n+        }\n+      }\n+\n+      // move to the next index\n+      data_index_step(bs, batches, head_id, num_blocks, kv_id, num_kv_splits);\n+    }\n+  });\n+\n+  // parallel on [batches, num_heads]\n+  at::parallel_for(0, batches * num_heads, 0, [&](int64_t begin, int64_t end) {\n+    // NB: same as above\n+    for (int64_t i = begin; i < end; ++i) {\n+      float* __restrict__ acc = attn_logits + i * l_stride1;\n+\n+      float s_prime = 0.f;\n+      float m_prime = -std::numeric_limits<scalar_t>::infinity();\n+\n+      // update acc with from each kv_split\n+      for (int64_t kv_id = 0; kv_id < num_kv_splits; ++kv_id) {\n+        float* __restrict__ tv = acc + kv_id * l_stride2;\n+        const float tlogic = (acc + kv_id * l_stride2)[head_size_v];\n+\n+        float m_i = std::max(tlogic, m_prime);\n+        float m_delta = std::exp(m_prime - m_i);\n+        float e_logic = std::exp(tlogic - m_i);\n+        if (kv_id != 0) {\n+          at::vec::map2<float>(\n+              [m_delta, e_logic](Vec x, Vec y) { return x * Vec(m_delta) + y * Vec(e_logic); },\n+              acc,\n+              acc,\n+              tv,\n+              head_size_v);\n+        }\n+\n+        s_prime = s_prime * m_delta + e_logic;\n+        m_prime = m_i;\n+      }\n+\n+      copy_stub<scalar_t>(output + i * head_size_v, acc, 1 / s_prime, head_size_v);\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// query:            [num_tokens, num_heads, head_size]\n+// output:           [num_tokens, num_heads, head_size]\n+// k_buffer:         [max_total_num_tokens, num_heads, head_size]\n+// v_buffer:         [max_total_num_tokens, num_heads, head_size_v]\n+// attn_logits:      [num_seqs, num_heads, num_kv_splits, head_size_v + 1]\n+// req_to_token:     [max_num_reqs, max_context_len] int32 or int64\n+// req_pool_indices: [num_seqs] int64\n+// seq_lens:         [num_seqs] int64\n+//\n+void decode_attention_cpu(\n+    at::Tensor& query,\n+    at::Tensor& output,\n+    at::Tensor& k_buffer,\n+    at::Tensor& v_buffer,\n+    at::Tensor& attn_logits,\n+    at::Tensor& req_to_token,\n+    at::Tensor& req_pool_indices,\n+    at::Tensor& seq_lens,\n+    double sm_scale,\n+    double logit_cap) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::decode_attention_cpu\",\n+      std::vector<c10::IValue>(\n+          {query, output, k_buffer, v_buffer, attn_logits, req_to_token, req_pool_indices, seq_lens}));\n+\n+  CHECK_INPUT(query);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(k_buffer);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(v_buffer);\n+  CHECK_DIM(3, query);\n+  CHECK_DIM(3, k_buffer);\n+  CHECK_DIM(3, v_buffer);\n+\n+  int64_t num_seqs = seq_lens.size(0);\n+  int64_t max_num_reqs = req_to_token.size(0);\n+  int64_t max_context_len = req_to_token.size(1);\n+  int64_t max_total_num_tokens = k_buffer.size(0);\n+\n+  int64_t num_heads = query.size(1);\n+  int64_t num_heads_kv = k_buffer.size(1);\n+  int64_t head_size = query.size(2);\n+  int64_t head_size_v = v_buffer.size(2);\n+\n+  int64_t num_kv_splits = attn_logits.size(2);\n+\n+  CHECK_EQ(attn_logits.size(0), num_seqs);\n+  CHECK_EQ(attn_logits.size(1), num_heads);\n+  CHECK_EQ(attn_logits.size(3), head_size_v + 1);\n+  CHECK_EQ(attn_logits.scalar_type(), at::kFloat);\n+\n+  // strides for k_buffer and v_buffer\n+  int64_t k_strideN = k_buffer.stride(0);\n+  int64_t k_strideH = k_buffer.stride(1);\n+  int64_t v_strideN = v_buffer.stride(0);\n+  int64_t v_strideH = v_buffer.stride(1);\n+\n+  // check index data types\n+  const auto index_dtype = req_to_token.scalar_type();\n+  TORCH_CHECK(\n+      index_dtype == at::kInt || index_dtype == at::kLong,\n+      \"decode: expect req_to_token to be int32 or int64, got \",\n+      index_dtype);\n+  TORCH_CHECK(seq_lens.scalar_type() == at::kLong, \"decode: expect req_lens to be int64, got \", seq_lens.scalar_type());\n+  TORCH_CHECK(\n+      req_pool_indices.scalar_type() == at::kLong,\n+      \"decode: expect req_pool_indices to be int64, got \",\n+      req_pool_indices.scalar_type());\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(query.scalar_type(), \"decode_attention_kernel\", [&] {\n+    AT_DISPATCH_INDEX_TYPES(index_dtype, \"decode_attention_indices\", [&] {\n+      if (num_heads == num_heads_kv) {\n+        // MHA\n+        decode_attention_kernel_impl<scalar_t, index_t>(\n+            output.data_ptr<scalar_t>(),\n+            attn_logits.data_ptr<float>(),\n+            query.data_ptr<scalar_t>(),\n+            k_buffer.data_ptr<scalar_t>(),\n+            v_buffer.data_ptr<scalar_t>(),\n+            req_to_token.data_ptr<index_t>(),\n+            req_pool_indices.data_ptr<int64_t>(),\n+            seq_lens.data_ptr<int64_t>(),\n+            num_seqs,\n+            num_heads,\n+            head_size,\n+            head_size_v,\n+            num_kv_splits,\n+            k_strideN,\n+            k_strideH,\n+            v_strideN,\n+            v_strideH,\n+            sm_scale,\n+            logit_cap,\n+            max_num_reqs,\n+            max_context_len,\n+            max_total_num_tokens);\n+      } else {\n+        // GQA/MQA/MLA\n+        decode_attention_grouped_kernel_impl<scalar_t, index_t>(\n+            output.data_ptr<scalar_t>(),\n+            attn_logits.data_ptr<float>(),\n+            query.data_ptr<scalar_t>(),\n+            k_buffer.data_ptr<scalar_t>(),\n+            v_buffer.data_ptr<scalar_t>(),\n+            req_to_token.data_ptr<index_t>(),\n+            req_pool_indices.data_ptr<int64_t>(),\n+            seq_lens.data_ptr<int64_t>(),\n+            num_seqs,\n+            num_heads,\n+            num_heads_kv,\n+            head_size,\n+            head_size_v,\n+            num_kv_splits,\n+            k_strideN,\n+            k_strideH,\n+            v_strideN,\n+            v_strideH,\n+            sm_scale,\n+            logit_cap,\n+            max_num_reqs,\n+            max_context_len,\n+            max_total_num_tokens);\n+      }\n+    });\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/extend.cpp b/sgl-kernel/csrc/cpu/extend.cpp\nnew file mode 100644\nindex 000000000..503cef538\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/extend.cpp\n@@ -0,0 +1,621 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// [NOTE]: extend attention for CPU\n+//   1. tune BLOCK_M and BLOCK_N\n+//   2. can handle non-contiguous k_exttend and v_extend\n+//   3. computes attention for prefix and extend separately\n+//   4. TODO: vectorize `pack_vnni` and `pack_vnni2`\n+//\n+template <typename index_t>\n+inline index_t get_index(index_t* ind, int i) {\n+  return (ind == nullptr) ? (index_t)i : ind[i];\n+}\n+\n+// convert to vnni format\n+// from [N, K/2, 2] to [K/2, N, 2] for bfloat16 and float16\n+template <typename scalar_t, typename index_t>\n+void pack_vnni(\n+    scalar_t* __restrict__ dst,\n+    const scalar_t* __restrict__ src,\n+    const index_t* __restrict__ ind,\n+    int N,\n+    int K,\n+    int ld_src,\n+    int ld_dst) {\n+  for (int n = 0; n < N; ++n) {\n+    index_t index = get_index(ind, n);\n+    for (int k = 0; k < K / 2; ++k) {\n+      for (int d = 0; d < 2; ++d) {\n+        dst[k * ld_dst * 2 + n * 2 + d] = src[index * ld_src + k * 2 + d];\n+      }\n+    }\n+  }\n+}\n+\n+// convert to vnni format\n+// from [K/2, 2, N] to [K/2, N, 2] for bfloat16 and float16\n+template <typename scalar_t, typename index_t>\n+void pack_vnni2(\n+    scalar_t* __restrict__ dst,\n+    const scalar_t* __restrict__ src,\n+    const index_t* __restrict__ ind,\n+    int K,\n+    int N,\n+    int ld_src,\n+    int ld_dst) {\n+  int k = 0;\n+  for (; k < (K >> 1) * 2; k += 2) {\n+    index_t index0 = get_index(ind, k + 0);\n+    index_t index1 = get_index(ind, k + 1);\n+    for (int n = 0; n < N; ++n) {\n+      dst[(k >> 1) * ld_dst * 2 + n * 2 + 0] = src[index0 * ld_src + n];\n+      dst[(k >> 1) * ld_dst * 2 + n * 2 + 1] = src[index1 * ld_src + n];\n+    }\n+  }\n+  if (K % 2 != 0) {\n+    index_t index = get_index(ind, K - 1);\n+    for (int n = 0; n < N; ++n) {\n+      dst[(K >> 1) * ld_dst * 2 + n * 2 + 0] = src[index * ld_src + n];\n+      dst[(K >> 1) * ld_dst * 2 + n * 2 + 1] = 0;\n+    }\n+    k += 2;\n+  }\n+  // TODO: check whether we can skip this!\n+  // const int padded_K = div_up(K, TILE_K) * TILE_K;\n+  // for (; k < padded_K; ++k) {\n+  //   for (int n = 0; n < N; ++n) {\n+  //     dst[k * ld_dst + n] = static_cast<scalar_t>(0);\n+  //   }\n+  // }\n+}\n+\n+template <typename scalar_t>\n+inline void fill_stub(scalar_t* __restrict__ out, float val, int size) {\n+  using Vec = at::vec::Vectorized<scalar_t>;\n+  const Vec data_vec = Vec(static_cast<scalar_t>(val));\n+  int d = 0;\n+  for (; d <= size - Vec::size(); d += Vec::size()) {\n+    data_vec.store(out + d);\n+  }\n+  if (size - d > 0) {\n+    data_vec.store(out + d, size - d);\n+  }\n+}\n+\n+template <typename scalar_t, int BLOCK_N>\n+inline void copy_stub(scalar_t* __restrict__ out, const float* __restrict__ input) {\n+  static_assert(BLOCK_N % 32 == 0);\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  constexpr int COLS = BLOCK_N / 16;\n+  auto store = [&](auto i) {\n+    constexpr int col = i % COLS;\n+    // for COLS = 2, 4 use 512bit store\n+    if constexpr (col % 2 == 0) {\n+      fVec a_fvec0 = fVec::loadu(input + col * 16);\n+      fVec a_fvec1 = fVec::loadu(input + col * 16 + 16);\n+      bVec out_bvec = convert_from_float_ext<scalar_t>(a_fvec0, a_fvec1);\n+      out_bvec.store(out + col * 16);\n+    }\n+  };\n+  Unroll<COLS>{}(store);\n+}\n+\n+template <typename scalar_t>\n+inline void copy_stub(scalar_t* __restrict__ out, const float* __restrict__ acc, float s, int size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  const fVec s_fvec = fVec(s);\n+  int d = 0;\n+  for (; d <= size - bVec::size(); d += bVec::size()) {\n+    fVec a_fvec0 = fVec::loadu(acc + d) * s_fvec;\n+    fVec a_fvec1 = fVec::loadu(acc + d + fVec::size()) * s_fvec;\n+    bVec out_bvec = convert_from_float_ext<scalar_t>(a_fvec0, a_fvec1);\n+    out_bvec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(acc[d] * s);\n+  }\n+}\n+\n+template <typename scalar_t, typename index_t, int BLOCK_M, int BLOCK_N>\n+void extend_attention_kernel_impl(\n+    scalar_t* __restrict__ o_extend,\n+    const scalar_t* __restrict__ q_extend,\n+    const scalar_t* __restrict__ k_extend,\n+    const scalar_t* __restrict__ v_extend,\n+    const scalar_t* __restrict__ k_buffer,\n+    const scalar_t* __restrict__ v_buffer,\n+    const index_t* __restrict__ req_to_token,\n+    const int64_t* __restrict__ req_pool_indices,\n+    const int64_t* __restrict__ seq_lens,\n+    const index_t* __restrict__ extend_seq_lens,\n+    const index_t* __restrict__ extend_start_loc,\n+    const void* __restrict__ buffer,\n+    int batches,\n+    int num_heads,\n+    int num_heads_kv,\n+    int head_size,\n+    int head_size_v,\n+    int ke_strideN,\n+    int ke_strideH,\n+    int ve_strideN,\n+    int ve_strideH,\n+    int k_strideN,\n+    int k_strideH,\n+    int v_strideN,\n+    int v_strideH,\n+    float scaling,\n+    float logit_cap,\n+    int max_num_reqs,\n+    int max_context_len,\n+    int max_total_num_tokens,\n+    int max_len_extend,\n+    int buffer_size_per_thread,\n+    bool is_prefix_skipped) {\n+  using Vec = at::vec::Vectorized<float>;\n+\n+  // strides\n+  const int q_strideM = num_heads * head_size;\n+  const int q_strideH = head_size;\n+  const int o_strideM = num_heads * head_size_v;\n+  const int o_strideH = head_size_v;\n+\n+  // we use same buffer for packed key and value\n+  const int ldb_tmp = std::max(head_size, head_size_v);\n+\n+  const bool has_logit_cap = logit_cap > 0;\n+  float rlogit_cap = has_logit_cap ? 1 / logit_cap : 0.f;\n+\n+  const int num_groups = num_heads / num_heads_kv;\n+  TORCH_CHECK(num_groups * num_heads_kv == num_heads);\n+\n+  // number of blocks along M\n+  int MB = div_up(max_len_extend, BLOCK_M);\n+\n+  // parallel on [batches, num_heads, BM]\n+  at::parallel_for(0, batches * num_heads * MB, 0, [&](int begin, int end) {\n+    int bs{0}, head_id{0}, mb{0};\n+    data_index_init(begin, bs, batches, head_id, num_heads, mb, MB);\n+\n+    int tid = at::get_thread_num();\n+    // s_i and s_delta: [BLOCK_M, BLOCK_N]\n+    float* __restrict__ s_i = reinterpret_cast<float*>((char*)(buffer) + tid * buffer_size_per_thread);\n+    float* __restrict__ s_delta = s_i;\n+\n+    // v_prime: [BLOCK_M, head_size_v]\n+    float* __restrict__ v_prime = s_i + BLOCK_M * BLOCK_N;\n+\n+    // s_delta2: [BLOCK_M, BLOCK_N]; copy of s_delta in scalar_t\n+    scalar_t* __restrict__ s_delta2 = reinterpret_cast<scalar_t*>(v_prime + BLOCK_N * head_size_v);\n+\n+    // Btmp: [BLOCK_N, max(head_size, head_size_v)]\n+    scalar_t* __restrict__ Btmp = s_delta2 + BLOCK_M * BLOCK_N;\n+\n+    // init Btmp just once for each thread to prevent NaN\n+    fill_stub(Btmp, 0.f, BLOCK_N * ldb_tmp);\n+\n+    alignas(64) float s_prime[BLOCK_M];\n+    alignas(64) float m_prime[BLOCK_M];\n+\n+    for (int i = begin; i < end; ++i) {\n+      // seq_len = prefix + extend\n+      int head_kv_id = head_id / num_groups;\n+      int seq_len = seq_lens[bs];\n+      int seq_len_extend = extend_seq_lens[bs];\n+      int seq_len_prefix = seq_len - seq_len_extend;\n+      int seq_extend_start_loc = extend_start_loc[bs];\n+\n+      int req_pool_id = req_pool_indices[bs];\n+      TORCH_CHECK(seq_len_prefix >= 0, \"prefix len < 0!\");\n+      TORCH_CHECK(seq_len <= max_context_len, \"seq_len out of scope!\");\n+      TORCH_CHECK(req_pool_id < max_num_reqs, \"req_pool_id out of scope!\");\n+\n+      if (is_prefix_skipped) {\n+        TORCH_CHECK(seq_len_prefix == 0, \"extend attention: expect seq_len_prefix to be 0, got \", seq_len_prefix);\n+      }\n+\n+      // offset and size in MB\n+      int m = mb * BLOCK_N;\n+      int m_size = std::min(BLOCK_M, seq_len_extend - m);\n+\n+      if (m_size <= 0) {\n+        data_index_step(bs, batches, head_id, num_heads, mb, MB);\n+        continue;\n+      }\n+\n+      // get query\n+      const scalar_t* __restrict__ q_ptr = q_extend + (seq_extend_start_loc + m) * q_strideM + head_id * q_strideH;\n+\n+      // init v', s' and m'\n+      fill_stub(v_prime, 0.f, m_size * head_size_v);\n+      fill_stub(s_prime, 0.f, m_size);\n+      fill_stub(m_prime, -std::numeric_limits<scalar_t>::infinity(), m_size);\n+\n+      // stage 1: compute scores with prefix\n+      for (int n = 0; n < seq_len_prefix; n += BLOCK_N) {\n+        int n_size = std::min(BLOCK_N, seq_len_prefix - n);\n+\n+        // `n_size` is K in 2nd gemm, pad to TILE_K;\n+        const int padded_n_size = div_up(n_size, TILE_K) * TILE_K;\n+\n+        // get key and pack\n+        pack_vnni<scalar_t, index_t>(\n+            /*    dst */ Btmp,\n+            /*    src */ k_buffer + head_kv_id * k_strideH,\n+            /*    ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /*     N  */ n_size,\n+            /*     K  */ head_size,\n+            /* ld_src */ k_strideN,\n+            /* ld_dst */ BLOCK_N);\n+\n+        // calculate s_i <- Q @ K\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ head_size,\n+            /* lda   */ q_strideM,\n+            /* ldb   */ BLOCK_N,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ q_ptr,\n+            /* B     */ Btmp,\n+            /* C     */ s_i);\n+\n+        const Vec scale_vec = Vec(scaling);\n+        for (int row = 0; row < m_size; ++row) {\n+          // s_i <- s_i * scale\n+          at::vec::map<float>(\n+              [scale_vec](Vec x) { return x * scale_vec; }, s_i + row * BLOCK_N, s_i + row * BLOCK_N, n_size);\n+\n+          // TODO: `tanh` from torch uses sleef u10, going to be slow\n+          if (has_logit_cap) {\n+            at::vec::map<float>(\n+                [logit_cap, rlogit_cap](Vec x) { return Vec(logit_cap) * (x * Vec(rlogit_cap)).tanh(); },\n+                s_i + row * BLOCK_N,\n+                s_i + row * BLOCK_N,\n+                n_size);\n+          }\n+\n+          // m_i: max value per row\n+          float m_i = at::vec::reduce_all<float>(\n+              [](Vec& x, Vec& y) { return at::vec::maximum(x, y); }, s_i + row * BLOCK_N, n_size);\n+          m_i = std::max(m_i, m_prime[row]);\n+\n+          // m_delta <- exp(m' - m_i)\n+          float m_delta = std::exp(m_prime[row] - m_i);\n+\n+          // s_delta <- exp(s_i - m_i)\n+          at::vec::map<float>(\n+              [m_i](Vec x) { return (x - Vec(m_i)).exp_u20(); }, s_delta + row * BLOCK_N, s_i + row * BLOCK_N, n_size);\n+\n+          // s' <- s' * m_delta + sum(s_delta)\n+          s_prime[row] *= m_delta;\n+          s_prime[row] +=\n+              at::vec::reduce_all<float>([](Vec& x, Vec& y) { return x + y; }, s_delta + row * BLOCK_N, n_size);\n+\n+          m_prime[row] = m_i;\n+\n+          // v' <- v' * m_delta\n+          at::vec::map<float>(\n+              [m_delta](Vec x) { return x * Vec(m_delta); },\n+              v_prime + row * head_size_v,\n+              v_prime + row * head_size_v,\n+              head_size_v);\n+\n+          // pad s_delta with 0 first and then convert to scalar_t\n+          fill_stub(s_delta + row * BLOCK_N + n_size, 0.f, padded_n_size - n_size);\n+          copy_stub<scalar_t, BLOCK_N>(s_delta2 + row * BLOCK_N, s_delta + row * BLOCK_N);\n+        }\n+\n+        // get value and pack\n+        pack_vnni2<scalar_t, index_t>(\n+            /*    dst */ Btmp,\n+            /*    src */ v_buffer + head_kv_id * v_strideH,\n+            /*    ind */ req_to_token + req_pool_id * max_context_len + n,\n+            /*     K  */ n_size,\n+            /*     N  */ head_size_v,\n+            /* ld_src */ v_strideN,\n+            /* ld_dst */ head_size_v);\n+\n+        // caculate V' <- s_delta @ V + V'\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ head_size_v,\n+            /* K     */ padded_n_size,  // n_size\n+            /* lda   */ BLOCK_N,\n+            /* ldb   */ head_size_v,\n+            /* ldc   */ head_size_v,\n+            /* add_C */ true,\n+            /* A     */ s_delta2,\n+            /* B     */ Btmp,\n+            /* C     */ v_prime);\n+      }  // loop with seq_len_prefix\n+\n+      // stage 2: compute the triangle part\n+      int num_keys = std::min(seq_len_extend, m + BLOCK_M);\n+      for (int n = 0; n < num_keys; n += BLOCK_N) {\n+        int n_size = std::min(BLOCK_N, num_keys - n);\n+\n+        // `n_size` is K in 2nd gemm, pad to TILE_K;\n+        const int padded_n_size = div_up(n_size, TILE_K) * TILE_K;\n+\n+        // get key and pack\n+        pack_vnni<scalar_t, index_t>(\n+            /*    dst */ Btmp,\n+            /*    src */ k_extend + (seq_extend_start_loc + n) * ke_strideN + head_kv_id * ke_strideH,\n+            /*    ind */ nullptr,\n+            /*     N  */ n_size,\n+            /*     K  */ head_size,\n+            /* ld_src */ ke_strideN,\n+            /* ld_dst */ BLOCK_N);\n+\n+        // calculate s_i <- Q @ K\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ head_size,\n+            /* lda   */ q_strideM,\n+            /* ldb   */ BLOCK_N,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ q_ptr,\n+            /* B     */ Btmp,\n+            /* C     */ s_i);\n+\n+        // apply causal mask\n+        if (num_keys - n <= BLOCK_N) {\n+          for (int row = 0; row < m_size; ++row) {\n+            int last_col = m + row - n;\n+            // fill [last_col + 1, n_size) to -inf\n+            float* row_ptr = s_i + row * BLOCK_N;\n+            fill_stub(row_ptr + last_col + 1, -std::numeric_limits<float>::infinity(), n_size - last_col - 1);\n+          }\n+        }\n+\n+        const Vec scale_vec = Vec(scaling);\n+        for (int row = 0; row < m_size; ++row) {\n+          // s_i <- s_i * scale\n+          at::vec::map<float>(\n+              [scale_vec](Vec x) { return x * scale_vec; }, s_i + row * BLOCK_N, s_i + row * BLOCK_N, n_size);\n+\n+          // TODO: `tanh` from torch uses sleef u10, going to be slow\n+          if (has_logit_cap) {\n+            at::vec::map<float>(\n+                [logit_cap, rlogit_cap](Vec x) { return Vec(logit_cap) * (x * Vec(rlogit_cap)).tanh(); },\n+                s_i + row * BLOCK_N,\n+                s_i + row * BLOCK_N,\n+                n_size);\n+          }\n+\n+          // m_i: max value per row\n+          float m_i = at::vec::reduce_all<float>(\n+              [](Vec& x, Vec& y) { return at::vec::maximum(x, y); }, s_i + row * BLOCK_N, n_size);\n+          m_i = std::max(m_i, m_prime[row]);\n+\n+          // m_delta <- exp(m' - m_i)\n+          float m_delta = std::exp(m_prime[row] - m_i);\n+\n+          // s_delta <- exp(s_i - m_i)\n+          at::vec::map<float>(\n+              [m_i](Vec x) { return (x - Vec(m_i)).exp_u20(); }, s_delta + row * BLOCK_N, s_i + row * BLOCK_N, n_size);\n+\n+          // s' <- s' * m_delta + sum(s_delta)\n+          s_prime[row] *= m_delta;\n+          s_prime[row] +=\n+              at::vec::reduce_all<float>([](Vec& x, Vec& y) { return x + y; }, s_delta + row * BLOCK_N, n_size);\n+\n+          m_prime[row] = m_i;\n+\n+          // v' <- v' * m_delta\n+          at::vec::map<float>(\n+              [m_delta](Vec x) { return x * Vec(m_delta); },\n+              v_prime + row * head_size_v,\n+              v_prime + row * head_size_v,\n+              head_size_v);\n+\n+          // pad s_delta with 0 first and then convert to scalar_t\n+          fill_stub(s_delta + row * BLOCK_N + n_size, 0.f, padded_n_size - n_size);\n+          copy_stub<scalar_t, BLOCK_N>(s_delta2 + row * BLOCK_N, s_delta + row * BLOCK_N);\n+        }\n+\n+        // get value and pack\n+        pack_vnni2<scalar_t, index_t>(\n+            /*    dst */ Btmp,\n+            /*    src */ v_extend + (seq_extend_start_loc + n) * ve_strideN + head_kv_id * ve_strideH,\n+            /*    ind */ nullptr,\n+            /*     K  */ n_size,\n+            /*     N  */ head_size_v,\n+            /* ld_src */ ve_strideN,\n+            /* ld_dst */ head_size_v);\n+\n+        // caculate V' <- s_delta @ V + V'\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ head_size_v,\n+            /* K     */ padded_n_size,  // n_size\n+            /* lda   */ BLOCK_N,\n+            /* ldb   */ head_size_v,\n+            /* ldc   */ head_size_v,\n+            /* add_C */ true,\n+            /* A     */ s_delta2,\n+            /* B     */ Btmp,\n+            /* C     */ v_prime);\n+      }  // loop with seq_len_extend\n+\n+      scalar_t* __restrict__ out_ptr = o_extend + (seq_extend_start_loc + m) * o_strideM + head_id * o_strideH;\n+      for (int row = 0; row < m_size; ++row) {\n+        float s = 1 / s_prime[row];\n+        copy_stub<scalar_t>(out_ptr + row * o_strideM, v_prime + row * head_size_v, s, head_size_v);\n+      }\n+\n+      // move to the next index\n+      data_index_step(bs, batches, head_id, num_heads, mb, MB);\n+    }\n+    at::native::cpublas::brgemm_release();\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// q_extend, k_extend, v_extend, o_extend: contiguous tensors\n+// k_buffer, v_buffer: (prefix + extend) tensors in mem_manager\n+//\n+// q_extend: [num_tokens, num_heads, head_size]\n+// k_extend: [num_extend_tokens, num_heads, head_size]\n+// v_extend: [num_extend_tokens, num_heads, head_size]\n+// o_extend: [num_tokens, num_heads, head_size]\n+// k_buffer: [max_total_num_tokens, num_heads, head_size]\n+// v_buffer: [max_total_num_tokens, num_heads, head_size]\n+// req_to_token: [max_num_reqs, max_context_len] int32 or int64\n+// req_pool_indices: [num_seqs] int64\n+// seq_lens: [num_seqs] int64\n+// extend_seq_lens: [num_seqs]\n+// extend_start_loc: [num_seqs]\n+//\n+void extend_attention_cpu(\n+    at::Tensor& q_extend,\n+    at::Tensor& k_extend,\n+    at::Tensor& v_extend,\n+    at::Tensor& o_extend,\n+    at::Tensor& k_buffer,\n+    at::Tensor& v_buffer,\n+    at::Tensor& req_to_token,\n+    at::Tensor& req_pool_indices,\n+    at::Tensor& seq_lens,\n+    at::Tensor& extend_seq_lens,\n+    at::Tensor& extend_start_loc,\n+    int64_t max_len_extend,\n+    double sm_scale,\n+    double logit_cap) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::extend_attention_cpu\",\n+      std::vector<c10::IValue>(\n+          {q_extend,\n+           k_extend,\n+           v_extend,\n+           o_extend,\n+           k_buffer,\n+           v_buffer,\n+           req_to_token,\n+           req_pool_indices,\n+           seq_lens,\n+           extend_seq_lens,\n+           extend_start_loc}));\n+\n+  CHECK_INPUT(q_extend);\n+  CHECK_INPUT(o_extend);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(k_extend);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(v_extend);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(k_buffer);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(v_buffer);\n+\n+  int num_seqs = seq_lens.size(0);\n+  int max_num_reqs = req_to_token.size(0);\n+  int max_context_len = req_to_token.size(1);\n+  int max_total_num_tokens = k_buffer.size(0);\n+\n+  int num_heads = q_extend.size(1);\n+  int num_heads_kv = k_extend.size(1);\n+  int head_size = q_extend.size(2);\n+  int head_size_v = v_extend.size(2);\n+\n+  // strides for k_extend and v_extend\n+  int ke_strideN = k_extend.stride(0);\n+  int ke_strideH = k_extend.stride(1);\n+  int ve_strideN = v_extend.stride(0);\n+  int ve_strideH = v_extend.stride(1);\n+\n+  // strides for k_buffer and v_buffer\n+  int k_strideN = k_buffer.stride(0);\n+  int k_strideH = k_buffer.stride(1);\n+  int v_strideN = v_buffer.stride(0);\n+  int v_strideH = v_buffer.stride(1);\n+\n+  // check sizes\n+  CHECK_EQ(req_pool_indices.size(0), num_seqs);\n+  CHECK_EQ(extend_seq_lens.size(0), num_seqs);\n+  CHECK_EQ(extend_start_loc.size(0), num_seqs);\n+  CHECK_EQ(v_extend.size(1), num_heads_kv);\n+  CHECK_EQ(k_buffer.size(1), v_buffer.size(1));\n+\n+  // MLA will skip prefix part\n+  const bool is_prefix_skipped = k_buffer.size(1) != num_heads_kv;\n+\n+  // check index data types\n+  const auto index_dtype = req_to_token.scalar_type();\n+  TORCH_CHECK(\n+      index_dtype == at::kInt || index_dtype == at::kLong,\n+      \"extend: expect req_to_token to be int32 or int64, got \",\n+      index_dtype);\n+  TORCH_CHECK(seq_lens.scalar_type() == at::kLong, \"extend: expect req_lens to be int64, got \", seq_lens.scalar_type());\n+  TORCH_CHECK(\n+      req_pool_indices.scalar_type() == at::kLong,\n+      \"extend: expect req_pool_indices to be int64, got \",\n+      req_pool_indices.scalar_type());\n+  TORCH_CHECK(\n+      extend_seq_lens.scalar_type() == index_dtype && extend_start_loc.scalar_type() == index_dtype,\n+      \"extend: expect extend_seq_lens and extend_start_loc to have same dtype as req_to_token.\");\n+\n+  // D and DV need to be 32x as we transpose by 512-bit\n+  TORCH_CHECK(head_size % 32 == 0, \"invalid head_size \", head_size);\n+  TORCH_CHECK(head_size_v % 32 == 0, \"invalid head_size_v \", head_size_v);\n+\n+  // block size for query seq length\n+  constexpr int BLOCK_M = 32;\n+  // block size for key/value seq length\n+  constexpr int BLOCK_N = 32;\n+\n+  const int size_per_thread =\n+      /* s_i     */ BLOCK_M * BLOCK_N * sizeof(float) +\n+      /* v_prime */ BLOCK_M * head_size_v * sizeof(float) +\n+      /* s_delta */ BLOCK_M * BLOCK_N * sizeof(uint16_t) +\n+      /* Btmp    */ BLOCK_N * std::max(head_size, head_size_v) * sizeof(uint16_t);\n+\n+  int num_threads = at::get_num_threads();\n+  auto buffer = at::empty({num_threads, size_per_thread}, q_extend.options().dtype(at::kChar));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(q_extend.scalar_type(), \"extend_attention_kernel\", [&] {\n+    AT_DISPATCH_INDEX_TYPES(index_dtype, \"extend_attention_indices\", [&] {\n+      extend_attention_kernel_impl<scalar_t, index_t, BLOCK_M, BLOCK_N>(\n+          o_extend.data_ptr<scalar_t>(),\n+          q_extend.data_ptr<scalar_t>(),\n+          k_extend.data_ptr<scalar_t>(),\n+          v_extend.data_ptr<scalar_t>(),\n+          k_buffer.data_ptr<scalar_t>(),\n+          v_buffer.data_ptr<scalar_t>(),\n+          req_to_token.data_ptr<index_t>(),\n+          req_pool_indices.data_ptr<int64_t>(),\n+          seq_lens.data_ptr<int64_t>(),\n+          extend_seq_lens.data_ptr<index_t>(),\n+          extend_start_loc.data_ptr<index_t>(),\n+          buffer.data_ptr(),\n+          num_seqs,\n+          num_heads,\n+          num_heads_kv,\n+          head_size,\n+          head_size_v,\n+          ke_strideN,\n+          ke_strideH,\n+          ve_strideN,\n+          ve_strideH,\n+          k_strideN,\n+          k_strideH,\n+          v_strideN,\n+          v_strideH,\n+          sm_scale,\n+          logit_cap,\n+          max_num_reqs,\n+          max_context_len,\n+          max_total_num_tokens,\n+          max_len_extend,\n+          size_per_thread,\n+          is_prefix_skipped);\n+    });\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/gemm.cpp b/sgl-kernel/csrc/cpu/gemm.cpp\nnew file mode 100644\nindex 000000000..97c0e7935\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/gemm.cpp\n@@ -0,0 +1,507 @@\n+#include \"gemm.h\"\n+\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// packed   layout:\n+//   quants {N, K}  int8_t\n+//   comp   {N}     int32_t\n+template <int BLOCK_N>\n+inline void s8s8_compensation(int8_t* __restrict__ packed, int K) {\n+#if defined(CPU_CAPABILITY_AVX512)\n+  constexpr int COLS = BLOCK_N / 16;\n+  __m512i vcomp[COLS];\n+\n+  for (int col = 0; col < COLS; ++col) {\n+    vcomp[col] = _mm512_setzero_si512();\n+  }\n+\n+  const int64_t offset = BLOCK_N * K;\n+  const __m512i off = _mm512_set1_epi8(static_cast<char>(0x80));\n+  for (int k = 0; k < K / 4; ++k) {\n+    for (int col = 0; col < COLS; ++col) {\n+      __m512i vb = _mm512_loadu_si512((const __m512i*)(packed + k * BLOCK_N * 4 + col * 64));\n+      vcomp[col] = _mm512_dpbusd_epi32(vcomp[col], off, vb);\n+    }\n+  }\n+\n+  for (int col = 0; col < COLS; ++col) {\n+    _mm512_storeu_si512((__m512i*)(packed + offset + col * 64), vcomp[col]);\n+  }\n+#else\n+  TORCH_CHECK(false, \"s8s8_compensation not implemented!\");\n+#endif\n+}\n+\n+// convert to vnni format\n+// from [N, K] to [K/2, N, 2] for bfloat16 and float16\n+template <typename packed_t>\n+inline void pack_vnni(packed_t* __restrict__ packed, const packed_t* __restrict__ weight, int N, int K) {\n+  const int VNNI_BLK = 2;\n+  for (int n = 0; n < N; ++n) {\n+    for (int k = 0; k < K / VNNI_BLK; ++k) {\n+      for (int d = 0; d < VNNI_BLK; ++d) {\n+        packed[k * N * VNNI_BLK + n * VNNI_BLK + d] = weight[n * K + k * VNNI_BLK + d];\n+      }\n+    }\n+  }\n+}\n+\n+template <>\n+inline void pack_vnni<int8_t>(int8_t* __restrict__ packed, const int8_t* __restrict__ weight, int N, int K) {\n+  constexpr int BLOCK_N = block_size_n();\n+  TORCH_CHECK(N == BLOCK_N);\n+\n+  const int VNNI_BLK = 4;\n+  for (int n = 0; n < N; ++n) {\n+    for (int k = 0; k < K / VNNI_BLK; ++k) {\n+      for (int d = 0; d < VNNI_BLK; ++d) {\n+        packed[k * N * VNNI_BLK + n * VNNI_BLK + d] = weight[n * K + k * VNNI_BLK + d];\n+      }\n+    }\n+  }\n+  s8s8_compensation<BLOCK_N>(packed, K);\n+}\n+\n+template <typename scalar_t>\n+inline void copy_stub(scalar_t* __restrict__ out, const float* __restrict__ input, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec data0 = fVec::loadu(input + d);\n+    fVec data1 = fVec::loadu(input + d + fVec::size());\n+    bVec out_vec = convert_from_float_ext<scalar_t>(data0, data1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d]);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline void copy_add_stub(\n+    scalar_t* __restrict__ out, const float* __restrict__ input, const float* __restrict__ bias, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec data0 = fVec::loadu(input + d) + fVec::loadu(bias + d);\n+    fVec data1 = fVec::loadu(input + d + fVec::size()) + fVec::loadu(bias + d + fVec::size());\n+    bVec out_vec = convert_from_float_ext<scalar_t>(data0, data1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d] + bias[d]);\n+  }\n+}\n+\n+template <typename scalar_t, bool has_bias, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn {\n+  static inline void apply(\n+      const scalar_t* __restrict__ A,\n+      const scalar_t* __restrict__ B,\n+      scalar_t* __restrict__ C,\n+      const float* __restrict__ bias,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <bool has_bias, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn<at::BFloat16, has_bias, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const at::BFloat16* __restrict__ A,\n+      const at::BFloat16* __restrict__ B,\n+      at::BFloat16* __restrict__ C,\n+      const float* __restrict__ bias,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+\n+    // prefetch distance\n+    constexpr int PREFETCH_SIZE_K = 0;\n+\n+    __m512bh va;\n+    __m512bh vb[COLS];\n+    __m512 vc[ROWS * COLS];\n+\n+    auto loadc = [&](auto i) {\n+      constexpr int col = i % COLS;\n+      if constexpr (has_bias) {\n+        vc[i] = _mm512_loadu_ps(bias + col * 16);\n+      } else {\n+        vc[i] = _mm512_set1_ps(0.f);\n+      }\n+    };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K2 = K >> 1;\n+    const int64_t lda2 = lda >> 1;\n+    const int64_t ldb2 = ldb;  // ldb * 2 >> 1;\n+    const float* a_ptr = reinterpret_cast<const float*>(A);\n+    const float* b_ptr = reinterpret_cast<const float*>(B);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = (__m512bh)(_mm512_set1_ps(a_ptr[row * lda2 + k]));\n+      }\n+      if constexpr (row == 0) {\n+        vb[col] = (__m512bh)(_mm512_loadu_si512(b_ptr + k * ldb2 + col * 16));\n+        if constexpr (PREFETCH_SIZE_K > 0) {\n+          _mm_prefetch(b_ptr + (k + PREFETCH_SIZE_K) * ldb2 + col * 16, _MM_HINT_T0);\n+        }\n+      }\n+      vc[i] = _mm512_dpbf16_ps(vc[i], va, vb[col]);\n+    };\n+    for (int64_t k = 0; k < K2; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      // for COLS = 2, 4 use 512bit store\n+      // for COLS = 1, 3 use 256bit store\n+      if constexpr (COLS % 2 == 0) {\n+        if constexpr (col % 2 == 0) {\n+          _mm512_storeu_si512(\n+              reinterpret_cast<__m512i*>((C + row * ldc + col * 16)),\n+              (__m512i)(_mm512_cvtne2ps_pbh(vc[row * COLS + col + 1], vc[row * COLS + col])));\n+        }\n+      } else {\n+        _mm256_storeu_si256(reinterpret_cast<__m256i*>(C + row * ldc + col * 16), (__m256i)(_mm512_cvtneps_pbh(vc[i])));\n+      }\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NN(MB_SIZE, NB_SIZE)                \\\n+  tinygemm_kernel_nn<scalar_t, has_bias, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda,                                          \\\n+      B + nb_start * 2,                                            \\\n+      C + mb_start * ldc + nb_start,                               \\\n+      has_bias ? bias + nb_start : nullptr,                        \\\n+      K,                                                           \\\n+      lda,                                                         \\\n+      ldb,                                                         \\\n+      ldc);\n+\n+template <typename scalar_t, bool has_bias>\n+struct brgemm {\n+  static inline void apply(\n+      const scalar_t* __restrict__ A,\n+      const scalar_t* __restrict__ B,\n+      scalar_t* __restrict__ C,\n+      float* __restrict__ Ctmp,\n+      const float* __restrict__ bias,\n+      int64_t M,\n+      int64_t N,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int BLOCK_N = block_size_n();\n+    at::native::cpublas::brgemm(M, N, K, lda, ldb, BLOCK_N, /* add_C */ false, A, B, Ctmp);\n+\n+    // copy from Ctmp to C\n+    for (int64_t m = 0; m < M; ++m) {\n+      if constexpr (has_bias) {\n+        copy_add_stub(C + m * ldc, Ctmp + m * BLOCK_N, bias, N);\n+      } else {\n+        copy_stub(C + m * ldc, Ctmp + m * BLOCK_N, N);\n+      }\n+    }\n+  }\n+};\n+\n+template <typename scalar_t, bool has_bias>\n+void tinygemm_kernel(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    float* __restrict__ Ctmp,\n+    const float* __restrict__ bias,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg) {\n+  if (brg) {\n+    brgemm<scalar_t, has_bias>::apply(A, B, C, Ctmp, bias, M, N, K, lda, ldb, ldc);\n+    return;\n+  }\n+\n+  // pattern: 1-4-16\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 64;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        // mb_size = 1\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 32);\n+          break;\n+        case 0x14:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 64);\n+          break;\n+        // mb_size = 2\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 32);\n+          break;\n+        case 0x24:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 64);\n+          break;\n+        // mb_size = 3\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 32);\n+          break;\n+        case 0x34:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 64);\n+          break;\n+        // mb_size = 4\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 32);\n+          break;\n+        case 0x44:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 64);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t>\n+void weight_packed_linear_kernel_impl(\n+    scalar_t* __restrict__ out,\n+    const scalar_t* __restrict__ mat1,\n+    const scalar_t* __restrict__ mat2,\n+    const float* __restrict__ bias,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t mat1_strideM,\n+    int64_t out_strideM) {\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  // use avx512-bf16 when a) M is small; b) dtype is bfloat16, otherwise use amx\n+  const bool use_brgemm = (M > 4) || (!std::is_same_v<scalar_t, at::BFloat16>);\n+\n+  // parallel on [MB, NB]\n+  AT_DISPATCH_BOOL(bias != nullptr, has_bias, [&] {\n+    at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+      int64_t mb{0}, nb{0};\n+      data_index_init(begin, mb, MB, nb, NB);\n+\n+      // for brgemm, use float32 for accumulate\n+      alignas(64) float Ctmp[BLOCK_M * BLOCK_N];\n+\n+      for (int64_t i = begin; i < end; ++i) {\n+        UNUSED(i);\n+        int64_t mb_start = mb * BLOCK_M;\n+        int64_t mb_size = std::min(M - mb_start, BLOCK_M);\n+        int64_t nb_start = nb * BLOCK_N;\n+        int64_t nb_size = std::min(N - nb_start, BLOCK_N);\n+\n+        tinygemm_kernel<scalar_t, has_bias>(\n+            /*   A */ mat1 + mb_start * mat1_strideM,\n+            /*   B */ mat2 + nb_start * K /* nb * BLOCK_N * K */,\n+            /*   C */ out + mb_start * out_strideM + nb_start,\n+            /* Ctmp*/ Ctmp,\n+            /* bias*/ bias + nb_start,\n+            /*   M */ mb_size,\n+            /*   N */ nb_size,\n+            /*   K */ K,\n+            /* lda */ mat1_strideM,\n+            /* ldb */ nb_size,\n+            /* ldc */ out_strideM,\n+            /* brg */ use_brgemm);\n+\n+        // move to the next index\n+        data_index_step(mb, MB, nb, NB);\n+      }\n+\n+      if (use_brgemm) {\n+        at::native::cpublas::brgemm_release();\n+      }\n+    });\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// tinygemm interface\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    float* __restrict__ Ctmp,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg) {\n+  tinygemm_kernel<scalar_t, false>(A, B, C, Ctmp, nullptr, M, N, K, lda, ldb, ldc, brg);\n+}\n+\n+#define INSTANTIATE_TINYGEMM_TEMPLATE(TYPE) \\\n+  template void tinygemm_kernel<TYPE>(      \\\n+      const TYPE* __restrict__ A,           \\\n+      const TYPE* __restrict__ B,           \\\n+      TYPE* __restrict__ C,                 \\\n+      float* __restrict__ Ctmp,             \\\n+      int64_t M,                            \\\n+      int64_t N,                            \\\n+      int64_t K,                            \\\n+      int64_t lda,                          \\\n+      int64_t ldb,                          \\\n+      int64_t ldc,                          \\\n+      bool brg)\n+\n+INSTANTIATE_TINYGEMM_TEMPLATE(at::BFloat16);\n+INSTANTIATE_TINYGEMM_TEMPLATE(at::Half);\n+\n+at::Tensor convert_weight_packed(at::Tensor& weight) {\n+  // for 3d moe weights\n+  // weight : [E, OC, IC]\n+  //     w1 : [E, 2N,  K]\n+  //     w2 : [E,  K,  N]\n+  CHECK_INPUT(weight);\n+\n+  const int64_t ndim = weight.ndimension();\n+  TORCH_CHECK(ndim == 2 || ndim == 3, \"expect weight to be 2d or 3d, got \", ndim, \"d tensor.\");\n+  const auto st = weight.scalar_type();\n+  const int64_t E = ndim == 3 ? weight.size(0) : 1;\n+  const int64_t OC = ndim == 3 ? weight.size(1) : weight.size(0);\n+  const int64_t IC = ndim == 3 ? weight.size(2) : weight.size(1);\n+\n+  // we handle 2 TILE_N at a time.\n+  TORCH_CHECK(OC % TILE_N == 0, \"invalid weight out features \", OC);\n+  TORCH_CHECK(IC % TILE_K == 0, \"invalid weight input features \", IC);\n+\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t NB = div_up(OC, BLOCK_N);\n+\n+  // use phony sizes here [E, OC, IC], for each [E], [OC, IC] -> [IC / 2, OC, 2]\n+  auto packed_weight = at::empty({}, weight.options());\n+  const int64_t stride = OC * IC;\n+\n+  TORCH_CHECK(\n+      st == at::kBFloat16 || st == at::kHalf || st == at::kChar, \"expect weight to be bfloat16, float16 or int8.\");\n+\n+  CPU_DISPATCH_PACKED_TYPES(st, [&] {\n+    // adjust most inner dimension size\n+    const int packed_row_size = get_row_size<packed_t>(IC);\n+    auto sizes = weight.sizes().vec();\n+    sizes[ndim - 1] = packed_row_size;\n+    packed_weight.resize_(sizes);\n+\n+    const packed_t* w_data = weight.data_ptr<packed_t>();\n+    packed_t* packed_data = packed_weight.data_ptr<packed_t>();\n+\n+    // parallel on {E, NB}\n+    at::parallel_for(0, E * NB, 0, [&](int64_t begin, int64_t end) {\n+      int64_t e{0}, nb{0};\n+      data_index_init(begin, e, E, nb, NB);\n+\n+      for (int64_t i = begin; i < end; ++i) {\n+        UNUSED(i);\n+\n+        int64_t n = nb * BLOCK_N;\n+        int64_t n_size = std::min(BLOCK_N, OC - n);\n+        pack_vnni<packed_t>(\n+            packed_data + e * OC * packed_row_size + n * packed_row_size, w_data + e * stride + n * IC, n_size, IC);\n+\n+        // move to the next index\n+        data_index_step(e, E, nb, NB);\n+      }\n+    });\n+  });\n+  return packed_weight;\n+}\n+\n+// mat1 : [M, K]\n+// mat2 : [N, K]\n+// bias : [N]\n+// out  : [M, N]\n+//\n+at::Tensor weight_packed_linear(at::Tensor& mat1, at::Tensor& mat2, std::optional<at::Tensor>& bias, bool is_vnni) {\n+  RECORD_FUNCTION(\"sgl-kernel::weight_packed_linear\", std::vector<c10::IValue>({mat1, mat2, bias}));\n+\n+  auto packed_w = is_vnni ? mat2 : convert_weight_packed(mat2);\n+\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(mat1);\n+  CHECK_INPUT(mat2);\n+\n+  int64_t M = mat1.size(0);\n+  int64_t N = mat2.size(0);\n+  int64_t K = mat2.size(1);\n+  CHECK_EQ(mat1.size(1), K);\n+  CHECK_DIM(2, mat1);\n+  CHECK_DIM(2, mat2);\n+\n+  auto out = at::empty({M, N}, mat1.options());\n+\n+  // strides\n+  int64_t mat1_strideM = mat1.stride(0);\n+  int64_t out_strideM = out.stride(0);\n+\n+  const bool has_bias = bias.has_value();\n+  const float* bias_data = nullptr;\n+  if (has_bias) {\n+    CHECK_EQ(bias.value().size(0), N);\n+    bias_data = bias.value().data_ptr<float>();\n+  }\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(mat1.scalar_type(), \"weight_packed_linear_kernel_impl\", [&] {\n+    weight_packed_linear_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        mat1.data_ptr<scalar_t>(),\n+        packed_w.data_ptr<scalar_t>(),\n+        bias_data,\n+        M,\n+        N,\n+        K,\n+        mat1_strideM,\n+        out_strideM);\n+  });\n+\n+  return out;\n+}\ndiff --git a/sgl-kernel/csrc/cpu/gemm.h b/sgl-kernel/csrc/cpu/gemm.h\nnew file mode 100644\nindex 000000000..010f50a0c\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/gemm.h\n@@ -0,0 +1,130 @@\n+#pragma once\n+\n+#include <ATen/native/CPUBlas.h>\n+\n+// amx-bf16\n+#define TILE_M 16\n+#define TILE_N 16\n+#define TILE_K 32\n+\n+// block size for AMX gemm\n+constexpr int block_size_m() {\n+  return 2 * TILE_M;\n+}\n+constexpr int block_size_n() {\n+  return 2 * TILE_N;\n+}\n+\n+// define threshold using brgemm (intel AMX)\n+template <typename T>\n+inline bool can_use_brgemm(int M);\n+template <>\n+inline bool can_use_brgemm<at::BFloat16>(int M) {\n+  return M > 4;\n+}\n+template <>\n+inline bool can_use_brgemm<at::Half>(int M) {\n+  return true;\n+}\n+// TODO: add u8s8 brgemm, this requires PyTorch 2.7\n+template <>\n+inline bool can_use_brgemm<int8_t>(int M) {\n+  return false;\n+}\n+\n+// work around compiler internal error\n+#define BLOCK_K 128  // 4 * TILE_K\n+\n+// adjust leading dimension size for K\n+template <typename T>\n+inline int64_t get_row_size(int64_t K) {\n+  return K;\n+}\n+\n+template <>\n+inline int64_t get_row_size<int8_t>(int64_t K) {\n+  return K + sizeof(int32_t);\n+}\n+\n+inline int64_t get_row_size(int64_t K, bool use_int8_w8a8) {\n+  return use_int8_w8a8 ? K + sizeof(int32_t) : K;\n+}\n+\n+// pack weight to vnni format\n+at::Tensor convert_weight_packed(at::Tensor& weight);\n+\n+// moe implementations for int8 w8a8\n+template <typename scalar_t>\n+void fused_experts_int8_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    scalar_t* __restrict__ ic2,\n+    uint8_t* __restrict__ A_tmp,\n+    float* __restrict__ C_tmp,\n+    uint8_t* __restrict__ Aq_tmp,\n+    float* __restrict__ As_tmp,\n+    const scalar_t* __restrict__ input,\n+    const int8_t* __restrict__ packed_w1,\n+    const int8_t* __restrict__ packed_w2,\n+    const float* __restrict__ w1s,\n+    const float* __restrict__ w2s,\n+    const float* __restrict__ topk_weights,\n+    const int32_t* __restrict__ sorted_ids,\n+    const int32_t* __restrict__ expert_ids,\n+    const int32_t* __restrict__ offsets,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t E,\n+    int64_t topk,\n+    int64_t num_tokens_post_pad);\n+\n+// shared expert implememntation for int8 w8a8\n+template <typename scalar_t>\n+void shared_expert_int8_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    float* __restrict__ C_tmp,\n+    uint8_t* __restrict__ Aq_tmp,\n+    float* __restrict__ As_tmp,\n+    const scalar_t* __restrict__ input,\n+    const int8_t* __restrict__ packed_w1,\n+    const int8_t* __restrict__ packed_w2,\n+    const float* __restrict__ w1s,\n+    const float* __restrict__ w2s,\n+    const scalar_t* __restrict__ fused_experts_out,\n+    float routed_scaling_factor,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K);\n+\n+// tinygemm interface\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    float* __restrict__ Ctmp,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg);\n+\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    int32_t* __restrict__ Ctmp,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg);\ndiff --git a/sgl-kernel/csrc/cpu/gemm_int8.cpp b/sgl-kernel/csrc/cpu/gemm_int8.cpp\nnew file mode 100644\nindex 000000000..ba383076a\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/gemm_int8.cpp\n@@ -0,0 +1,489 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t, bool has_bias, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B,\n+      scalar_t* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs,\n+      const int32_t* __restrict__ Bcomp,\n+      const float* __restrict__ bias,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <bool has_bias, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn<at::BFloat16, has_bias, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B,\n+      at::BFloat16* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs,\n+      const int32_t* __restrict__ Bcomp,\n+      const float* __restrict__ bias,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+    static_assert(COLS % 2 == 0);\n+\n+    // prefetch distance\n+    constexpr int PREFETCH_SIZE_K = 0;\n+\n+    __m512i va;\n+    __m512i vb[COLS];\n+    __m512i vc[ROWS * COLS];\n+    __m512i vcomp[COLS];\n+    __m512 vd0;\n+    __m512 vd1[COLS];\n+\n+    // oops! 4x4 spills but luckly we use 4x2\n+    __m512 vbias[COLS];\n+\n+    // [NOTE]: s8s8 igemm compensation in avx512-vnni\n+    //\n+    // avx512-vnni has no s8s8, so we need to change s8s8 to u8s8 with compensate:\n+    //\n+    //   a * b = (a + 128) * b - 128 * b\n+    //   s   s       u       s    u    s\n+    //\n+    // 1) 128 * b is pre-computed when packing B to vnni formats\n+    // 2) a + 128 is fused when dynamically quantize A\n+    //\n+    auto loadc = [&](auto i) { vc[i] = _mm512_set1_epi32(0); };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K4 = K >> 2;\n+    const int64_t lda4 = lda >> 2;\n+    const int64_t ldb4 = ldb;  // ldb * 4 >> 2;\n+    const int32_t* a_ptr = reinterpret_cast<const int32_t*>(A);\n+    const int32_t* b_ptr = reinterpret_cast<const int32_t*>(B);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = _mm512_set1_epi32(a_ptr[row * lda4 + k]);\n+      }\n+      if constexpr (row == 0) {\n+        vb[col] = _mm512_loadu_si512(b_ptr + k * ldb4 + col * 16);\n+        if constexpr (PREFETCH_SIZE_K > 0) {\n+          _mm_prefetch(b_ptr + (k + PREFETCH_SIZE_K) * ldb4 + col * 16, _MM_HINT_T0);\n+        }\n+      }\n+      vc[i] = _mm512_dpbusd_epi32(vc[i], va, vb[col]);\n+    };\n+    for (int64_t k = 0; k < K4; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      // load a scale\n+      if constexpr (col == 0) {\n+        vd0 = _mm512_set1_ps(As[row]);\n+      }\n+      // load b scale and vcomp per 2 vectors\n+      // also load bias if any\n+      if constexpr (row == 0) {\n+        if constexpr (col % 2 == 0) {\n+          vd1[col + 0] = _mm512_loadu_ps(Bs + col * 16);\n+          vd1[col + 1] = _mm512_loadu_ps(Bs + col * 16 + 16);\n+          vcomp[col + 0] = _mm512_loadu_si512(Bcomp + col * 16);\n+          vcomp[col + 1] = _mm512_loadu_si512(Bcomp + col * 16 + 16);\n+          if constexpr (has_bias) {\n+            vbias[col + 0] = _mm512_loadu_ps(bias + col * 16);\n+            vbias[col + 1] = _mm512_loadu_ps(bias + col * 16 + 16);\n+          }\n+        }\n+      }\n+\n+      // for COLS = 2, 4 use 512bit store\n+      if constexpr (col % 2 == 0) {\n+        __m512 vc0 = _mm512_cvtepi32_ps(_mm512_sub_epi32(vc[row * COLS + col + 0], vcomp[col + 0]));\n+        __m512 vc1 = _mm512_cvtepi32_ps(_mm512_sub_epi32(vc[row * COLS + col + 1], vcomp[col + 1]));\n+        if constexpr (has_bias) {\n+          vc0 = _mm512_fmadd_ps(_mm512_mul_ps(vc0, vd0), vd1[col + 0], vbias[col + 0]);\n+          vc1 = _mm512_fmadd_ps(_mm512_mul_ps(vc1, vd0), vd1[col + 1], vbias[col + 1]);\n+        } else {\n+          vc0 = _mm512_mul_ps(_mm512_mul_ps(vc0, vd0), vd1[col + 0]);\n+          vc1 = _mm512_mul_ps(_mm512_mul_ps(vc1, vd0), vd1[col + 1]);\n+        }\n+\n+        _mm512_storeu_si512(\n+            reinterpret_cast<__m512i*>((C + row * ldc + col * 16)), (__m512i)(_mm512_cvtne2ps_pbh(vc1, vc0)));\n+      }\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NN(MB_SIZE, NB_SIZE)                \\\n+  tinygemm_kernel_nn<scalar_t, has_bias, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda,                                          \\\n+      B + nb_start * 4,                                            \\\n+      C + mb_start * ldc + nb_start,                               \\\n+      As + mb_start,                                               \\\n+      Bs + nb_start,                                               \\\n+      Bcomp + nb_start,                                            \\\n+      has_bias ? bias + nb_start : nullptr,                        \\\n+      K,                                                           \\\n+      lda,                                                         \\\n+      ldb,                                                         \\\n+      ldc);\n+\n+template <typename scalar_t, bool has_bias>\n+void tinygemm_kernel(\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    int32_t* __restrict__ Ctmp,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs,\n+    const float* __restrict__ bias,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg) {\n+  // B compensation\n+  const int32_t* Bcomp = reinterpret_cast<const int32_t*>(B + block_size_n() * K);\n+\n+  // pattern: 1-4-16\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 64;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int64_t mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        // mb_size = 1\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 32);\n+          break;\n+        case 0x14:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 64);\n+          break;\n+        // mb_size = 2\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 32);\n+          break;\n+        case 0x24:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 64);\n+          break;\n+        // mb_size = 3\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 32);\n+          break;\n+        case 0x34:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 64);\n+          break;\n+        // mb_size = 4\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 32);\n+          break;\n+        case 0x44:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 64);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t>\n+void int8_scaled_mm_kernel_impl(\n+    scalar_t* __restrict__ out,\n+    const uint8_t* __restrict__ mat1,\n+    const int8_t* __restrict__ mat2,\n+    const float* __restrict__ scales1,\n+    const float* __restrict__ scales2,\n+    const float* __restrict__ bias,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K) {\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  // TODO: brgemm u8s8 depends on PyTorch 2.7 release.\n+  const bool use_brgemm = false;\n+\n+  // K + 4 after compensation\n+  const int64_t packed_row_size = get_row_size<int8_t>(K);\n+\n+  AT_DISPATCH_BOOL(bias != nullptr, has_bias, [&] {\n+    at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+      int64_t mb{0}, nb{0};\n+      data_index_init(begin, mb, MB, nb, NB);\n+\n+      // for brgemm, use int32_t for accumulate\n+      alignas(64) int32_t Ctmp[BLOCK_M * BLOCK_N];\n+\n+      for (int i = begin; i < end; ++i) {\n+        UNUSED(i);\n+        int mb_start = mb * BLOCK_M;\n+        int mb_size = std::min(M - mb_start, BLOCK_M);\n+        int nb_start = nb * BLOCK_N;\n+        int nb_size = std::min(N - nb_start, BLOCK_N);\n+\n+        tinygemm_kernel<scalar_t, has_bias>(\n+            /*   A */ mat1 + mb_start * K,\n+            /*   B */ mat2 + nb_start * packed_row_size /* nb * BLOCK_N * (K + 4) */,\n+            /*   C */ out + mb_start * N + nb_start,\n+            /* Ctmp*/ Ctmp,\n+            /*  As */ scales1 + mb_start,\n+            /*  Bs */ scales2 + nb_start,\n+            /* bias*/ bias + nb_start,\n+            /*   M */ mb_size,\n+            /*   N */ nb_size,\n+            /*   K */ K,\n+            /* lda */ K,\n+            /* ldb */ nb_size,\n+            /* ldc */ N,\n+            /* brg */ use_brgemm);\n+\n+        // move to the next index\n+        data_index_step(mb, MB, nb, NB);\n+      }\n+\n+      if (use_brgemm) {\n+        at::native::cpublas::brgemm_release();\n+      }\n+    });\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// tinygemm interface\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B,\n+    scalar_t* __restrict__ C,\n+    int32_t* __restrict__ Ctmp,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc,\n+    bool brg) {\n+  tinygemm_kernel<scalar_t, false>(A, B, C, Ctmp, As, Bs, nullptr, M, N, K, lda, ldb, ldc, brg);\n+}\n+\n+#define INSTANTIATE_TINYGEMM_TEMPLATE(TYPE) \\\n+  template void tinygemm_kernel<TYPE>(      \\\n+      const uint8_t* __restrict__ A,        \\\n+      const int8_t* __restrict__ B,         \\\n+      TYPE* __restrict__ C,                 \\\n+      int32_t* __restrict__ Ctmp,           \\\n+      const float* __restrict__ As,         \\\n+      const float* __restrict__ Bs,         \\\n+      int64_t M,                            \\\n+      int64_t N,                            \\\n+      int64_t K,                            \\\n+      int64_t lda,                          \\\n+      int64_t ldb,                          \\\n+      int64_t ldc,                          \\\n+      bool brg)\n+\n+INSTANTIATE_TINYGEMM_TEMPLATE(at::BFloat16);\n+INSTANTIATE_TINYGEMM_TEMPLATE(at::Half);\n+\n+std::tuple<at::Tensor, at::Tensor> per_token_quant_int8_cpu(at::Tensor& A) {\n+  RECORD_FUNCTION(\"sgl-kernel::per_token_quant_int8_cpu\", std::vector<c10::IValue>({A}));\n+\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(A);\n+  CHECK_DIM(2, A);\n+\n+  int64_t M = A.size(0);\n+  int64_t K = A.size(1);\n+  int64_t lda = A.stride(0);\n+\n+  const auto st = A.scalar_type();\n+  TORCH_CHECK(st == at::kBFloat16 || st == at::kHalf, \"per_token_quant_int8: expect A to be bfloat16 or half.\");\n+\n+  auto Aq = at::empty({M, K}, A.options().dtype(at::kByte));\n+  auto As = at::empty({M}, A.options().dtype(at::kFloat));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"per_token_quant_int8\", [&] {\n+    uint8_t* __restrict__ Aq_data = Aq.data_ptr<uint8_t>();\n+    float* __restrict__ As_data = As.data_ptr<float>();\n+    const scalar_t* __restrict__ A_data = A.data_ptr<scalar_t>();\n+\n+    at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+      for (int64_t m = begin; m < end; ++m) {\n+        quantize_row_int8<scalar_t>(Aq_data + m * K, As_data[m], A_data + m * lda, K);\n+      }\n+    });\n+  });\n+  return std::make_tuple(Aq, As);\n+}\n+\n+// weight     :  static, per-channel, symmetric\n+// activation : dynamic,   per-token, symmetric\n+//\n+// mat1    : [M, K]\n+// mat2    : [N, K]\n+// scales1 : [M]\n+// scales2 : [N]\n+// bias    : [N]\n+// out     : [M, N]\n+//\n+at::Tensor int8_scaled_mm_cpu(\n+    at::Tensor& mat1,\n+    at::Tensor& mat2,\n+    at::Tensor& scales1,\n+    at::Tensor& scales2,\n+    std::optional<at::Tensor>& bias,\n+    at::ScalarType out_dtype,\n+    bool is_vnni) {\n+  RECORD_FUNCTION(\"sgl-kernel::int8_scaled_mm_cpu\", std::vector<c10::IValue>({mat1, mat2, scales1, scales2, bias}));\n+\n+  auto packed_w = is_vnni ? mat2 : convert_weight_packed(mat2);\n+\n+  CHECK_INPUT(mat1);\n+  CHECK_INPUT(mat2);\n+  CHECK_INPUT(scales1);\n+  CHECK_INPUT(scales2);\n+  CHECK_DIM(2, mat1);\n+  CHECK_DIM(2, mat2);\n+\n+  int64_t M = mat1.size(0);\n+  int64_t N = mat2.size(0);\n+  int64_t K = mat1.size(1);\n+\n+  // see [NOTE]: s8s8 igemm compensation in avx512-vnni\n+  CHECK_EQ(mat2.size(1), (int64_t)(is_vnni ? K + sizeof(int32_t) : K));\n+  CHECK_EQ(scales1.numel(), M);\n+  CHECK_EQ(scales2.numel(), N);\n+\n+  TORCH_CHECK(mat1.scalar_type() == at::kByte, \"int8_scaled_mm: expect mat1 to be uint8.\");\n+  TORCH_CHECK(mat2.scalar_type() == at::kChar, \"int8_scaled_mm: expect mat2 to be int8.\");\n+  TORCH_CHECK(\n+      scales1.scalar_type() == at::kFloat && scales2.scalar_type() == at::kFloat,\n+      \"int8_scaled_mm: expect scales to be float32.\");\n+\n+  auto out = at::empty({M, N}, mat1.options().dtype(out_dtype));\n+\n+  const bool has_bias = bias.has_value();\n+  const float* bias_data = nullptr;\n+  if (has_bias) {\n+    CHECK_EQ(bias.value().size(0), N);\n+    bias_data = bias.value().data_ptr<float>();\n+  }\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(out_dtype, \"int8_scaled_mm_kernel_impl\", [&] {\n+    int8_scaled_mm_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        mat1.data_ptr<uint8_t>(),\n+        packed_w.data_ptr<int8_t>(),\n+        scales1.data_ptr<float>(),\n+        scales2.data_ptr<float>(),\n+        bias_data,\n+        M,\n+        N,\n+        K);\n+  });\n+  return out;\n+}\n+\n+// fused `per_token_quant_int8_cpu` and `int8_scaled_mm_cpu`\n+at::Tensor int8_scaled_mm_with_quant(\n+    at::Tensor& mat1,\n+    at::Tensor& mat2,\n+    at::Tensor& scales2,\n+    std::optional<at::Tensor>& bias,\n+    at::ScalarType out_dtype,\n+    bool is_vnni) {\n+  RECORD_FUNCTION(\"sgl-kernel::int8_scaled_mm_cpu\", std::vector<c10::IValue>({mat1, mat2, scales2, bias}));\n+\n+  auto packed_w = is_vnni ? mat2 : convert_weight_packed(mat2);\n+\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(mat1);\n+  CHECK_INPUT(mat2);\n+  CHECK_INPUT(scales2);\n+  CHECK_DIM(2, mat1);\n+  CHECK_DIM(2, mat2);\n+\n+  int64_t M = mat1.size(0);\n+  int64_t N = mat2.size(0);\n+  int64_t K = mat1.size(1);\n+  int64_t lda = mat1.stride(0);\n+\n+  // see [NOTE]: s8s8 igemm compensation in avx512-vnni\n+  CHECK_EQ(mat2.size(1), (int64_t)(is_vnni ? K + sizeof(int32_t) : K));\n+  CHECK_EQ(scales2.numel(), N);\n+\n+  const auto st = mat1.scalar_type();\n+  TORCH_CHECK(st == at::kBFloat16 || st == at::kHalf, \"int8_scaled_mm_with_quant: expect A to be bfloat16 or half.\");\n+  TORCH_CHECK(st == out_dtype, \"int8_scaled_mm_with_quant: expect A has same dtype with out_dtype.\");\n+  TORCH_CHECK(mat2.scalar_type() == at::kChar, \"int8_scaled_mm_with_quant: expect mat2 to be int8.\");\n+  TORCH_CHECK(scales2.scalar_type() == at::kFloat, \"int8_scaled_mm_with_quant: expect scales to be float32.\");\n+\n+  const int64_t buffer_size = M * K + M * sizeof(float);\n+  auto buffer = at::empty({buffer_size}, mat1.options().dtype(at::kByte));\n+  auto out = at::empty({M, N}, mat1.options().dtype(out_dtype));\n+\n+  const bool has_bias = bias.has_value();\n+  const float* bias_data = nullptr;\n+  if (has_bias) {\n+    CHECK_EQ(bias.value().size(0), N);\n+    bias_data = bias.value().data_ptr<float>();\n+  }\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(out_dtype, \"int8_scaled_mm_with_quant_kernel_impl\", [&] {\n+    uint8_t* __restrict__ Aq_data = buffer.data_ptr<uint8_t>();\n+    float* __restrict__ As_data = (float*)((void*)(Aq_data + M * K));\n+    const scalar_t* __restrict__ A_data = mat1.data_ptr<scalar_t>();\n+\n+    at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+      for (int64_t m = begin; m < end; ++m) {\n+        quantize_row_int8<scalar_t>(Aq_data + m * K, As_data[m], A_data + m * lda, K);\n+      }\n+    });\n+\n+    int8_scaled_mm_kernel_impl<scalar_t>(\n+        out.data_ptr<scalar_t>(),\n+        Aq_data,\n+        packed_w.data_ptr<int8_t>(),\n+        As_data,\n+        scales2.data_ptr<float>(),\n+        bias_data,\n+        M,\n+        N,\n+        K);\n+  });\n+  return out;\n+}\ndiff --git a/sgl-kernel/csrc/cpu/interface.cpp b/sgl-kernel/csrc/cpu/interface.cpp\nnew file mode 100644\nindex 000000000..cc11c4928\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/interface.cpp\n@@ -0,0 +1,120 @@\n+#include <ATen/record_function.h>\n+#include <torch/extension.h>\n+\n+#include \"shm.h\"\n+\n+// Communication settings\n+static int world_rank = -1;\n+static int world_size = -1;\n+\n+static bool is_initialized = false;\n+\n+static bool all_ranks_local_p = false;\n+\n+void initialize(int size, int rank) {\n+  if (is_initialized) {\n+    return;\n+  }\n+\n+  // Check whether all ranks is on the same physical machine.\n+  // If true, we will use an SHM based low latency allreduce\n+\n+  auto ls_string = std::getenv(\"LOCAL_SIZE\");\n+  int ls = 0;\n+  if (ls_string != NULL) {\n+    ls = std::stoi(std::getenv(\"LOCAL_SIZE\"));\n+  }\n+\n+  if (size >= 1 && size == ls) {\n+    all_ranks_local_p = true;\n+  }\n+\n+  world_size = size;\n+  world_rank = rank;\n+  is_initialized = true;\n+\n+  auto addr_string = std::getenv(\"MASTER_ADDR\");\n+  if (addr_string == NULL) {\n+    addr_string = \"\";\n+  }\n+  auto port_string = std::getenv(\"MASTER_PORT\");\n+  if (port_string == NULL) {\n+    port_string = \"\";\n+  }\n+\n+  if (all_ranks_local_p) {\n+    shm_initialize(size, rank, addr_string, port_string);\n+  }\n+}\n+\n+void shm_allreduce(torch::Tensor& data, c10::intrusive_ptr<c10d::ProcessGroup> process_group, py::object op) {\n+  RECORD_FUNCTION(\"sgl-kernel::shm_allreduce\", std::vector<c10::IValue>({data}));\n+\n+  static py::object ReduceOp = py::module_::import(\"torch.distributed\").attr(\"ReduceOp\");\n+  static auto ReduceOpSum = (int)py::int_(ReduceOp.attr(\"SUM\").attr(\"value\"));\n+  TORCH_CHECK(py::int_(op.attr(\"value\")) == ReduceOpSum, \"Only torch.distributed.ReduceOp.SUM is supported\");\n+\n+  auto numel = data.numel();\n+\n+  int data_size = 0;\n+  bool data_type_fallback = false;\n+\n+  switch (data.scalar_type()) {\n+    case c10::ScalarType::BFloat16:\n+      data_size = numel * 2;\n+      break;\n+    case c10::ScalarType::Float:\n+      data_size = numel * 4;\n+      break;\n+    default:\n+      data_type_fallback = true;\n+  }\n+\n+  if (data_type_fallback || !all_ranks_local_p) {\n+    // Fallback to torch distributed allreduce\n+    std::vector<torch::Tensor> tensors = {data};\n+    process_group->allreduce(tensors)->wait();\n+  } else {\n+    all_reduce_outer_loop(data, numel, data_size);\n+  }\n+\n+  return;\n+}\n+\n+torch::Tensor shm_allgather(torch::Tensor& data, c10::intrusive_ptr<c10d::ProcessGroup> process_group, int dim) {\n+  RECORD_FUNCTION(\"sgl-kernel::shm_allgather\", std::vector<c10::IValue>({data}));\n+\n+  auto numel = data.numel();\n+\n+  int data_size = 0;\n+  bool data_type_fallback = false;\n+\n+  switch (data.scalar_type()) {\n+    case c10::ScalarType::BFloat16:\n+      data_size = numel * 2;\n+      break;\n+    case c10::ScalarType::Float:\n+      data_size = numel * 4;\n+      break;\n+    default:\n+      data_type_fallback = true;\n+  }\n+  if (dim < 0) {\n+    dim += data.dim();\n+  }\n+  if (data_type_fallback || !all_ranks_local_p) {\n+    // Fallback to torch distributed allreduce\n+    std::vector<std::vector<torch::Tensor>> output_tensors(1);\n+    auto world_size = process_group->getSize();\n+    for (int i = 0; i < world_size; i++) {\n+      output_tensors[0].push_back(torch::empty_like(data));\n+    }\n+    std::vector<torch::Tensor> input_tensors = {data};\n+    process_group->allgather(output_tensors, input_tensors)->wait();\n+    return torch::cat(output_tensors[0], dim).contiguous();\n+  }\n+  std::vector<int64_t> result_shape = data.sizes().vec();\n+  result_shape[dim] *= world_size;\n+  torch::Tensor result_tensor = torch::empty(result_shape, data.options());\n+  return all_gather(result_tensor, data, dim, numel, data_size);\n+}\ndiff --git a/sgl-kernel/csrc/cpu/moe.cpp b/sgl-kernel/csrc/cpu/moe.cpp\nnew file mode 100644\nindex 000000000..05825e04f\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/moe.cpp\n@@ -0,0 +1,1247 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// [NOTE]: Fused MoE kernel with AMX\n+//\n+//   This file contains implementations for\n+//     * `moe_align_block_size`\n+//     * `fused_moe`\n+//\n+//   The functionality is identical to triton kernel, excepts:\n+//     * fuse silu_and_mul with gemm1, therefore this kernel\n+//       allocates 2 intermediate_caches instead of 3\n+//     * add `offsets` in `moe_align_block_size` which keeps track\n+//       of starting offset for each M block. this is for keeping\n+//       output of silu_and_mul in sorted order, thus load_A for\n+//       the 2nd gemm would be contiguous, therefore we can directly\n+//       load A from intermediate_cache1.\n+//\n+//  TODO:\n+//     1. tune BLOCK_M and BLOCK_N (BLOCK_N * K fit L2)\n+//     2. add prefetch for load A which is indexed access\n+//     3. abstract at::native::cpublas::brgemm with WoQ gemm (M = 1 & M != 1)\n+//\n+\n+template <typename scalar_t>\n+inline void fill_stub(scalar_t* __restrict__ out, scalar_t val, int64_t size) {\n+  using Vec = at::vec::Vectorized<scalar_t>;\n+  const Vec data_vec(val);\n+  at::vec::map<scalar_t>([data_vec](Vec out) { return out = data_vec; }, out, out, size);\n+}\n+\n+template <typename scalar_t>\n+inline void copy_stub(scalar_t* __restrict__ out, const scalar_t* __restrict__ input, int64_t size) {\n+  using Vec = at::vec::Vectorized<scalar_t>;\n+// no remainder\n+#pragma GCC unroll 4\n+  for (int64_t d = 0; d < size; d += Vec::size()) {\n+    Vec data = Vec::loadu(input + d);\n+    data.store(out + d);\n+  }\n+}\n+\n+template <typename scalar_t>\n+inline void copy_mul_stub(scalar_t* __restrict__ out, const float* __restrict__ input, float weight, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  const fVec weight_vec = fVec(weight);\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec data0 = fVec::loadu(input + d) * weight_vec;\n+    fVec data1 = fVec::loadu(input + d + fVec::size()) * weight_vec;\n+    bVec out_vec = convert_from_float_ext<scalar_t>(data0, data1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d] * weight);\n+  }\n+}\n+\n+// acc from [topk, K] to [K]\n+template <typename scalar_t>\n+inline void sum_stub(scalar_t* __restrict__ out, const scalar_t* __restrict__ input, int64_t topk, int64_t K) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  if (topk == 1) {\n+    // do copy for topk = 1\n+    copy_stub(out, input, K);\n+  } else {\n+    // do sum for topk != 1\n+    int64_t d;\n+#pragma GCC unroll 4\n+    for (d = 0; d <= K - kVecSize; d += kVecSize) {\n+      fVec sum_fvec0 = fVec(0.f);\n+      fVec sum_fvec1 = fVec(0.f);\n+      for (int t = 0; t < topk; ++t) {\n+        bVec x_bvec = bVec::loadu(input + t * K + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        sum_fvec0 += x_fvec0;\n+        sum_fvec1 += x_fvec1;\n+      }\n+      bVec out_bvec = convert_from_float_ext<scalar_t>(sum_fvec0, sum_fvec1);\n+      out_bvec.store(out + d);\n+    }\n+    for (; d < K; ++d) {\n+      float sum_val = 0.f;\n+      for (int t = 0; t < topk; ++t) {\n+        sum_val += static_cast<float>(input[t * K + d]);\n+      }\n+      out[d] = static_cast<scalar_t>(sum_val);\n+    }\n+  }\n+}\n+\n+// out = input + input2 * scale\n+template <typename scalar_t>\n+inline void add_mul_stub(\n+    scalar_t* __restrict__ out,\n+    const float* __restrict__ input,\n+    const scalar_t* __restrict__ input2,\n+    float scale,\n+    int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  const fVec s_vec = fVec(scale);\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec x0 = fVec::loadu(input + d);\n+    fVec x1 = fVec::loadu(input + d + fVec::size());\n+\n+    bVec y_bvec = bVec::loadu(input2 + d);\n+    fVec y0, y1;\n+    std::tie(y0, y1) = at::vec::convert_to_float(y_bvec);\n+\n+    x0 = x0 + y0 * s_vec;\n+    x1 = x1 + y1 * s_vec;\n+    bVec out_vec = convert_from_float_ext<scalar_t>(x0, x1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d] + float(input2[d]) * scale);\n+  }\n+}\n+\n+template <int BLOCK_M>\n+int moe_align_block_size(\n+    int32_t* __restrict__ sorted_ids,\n+    int32_t* __restrict__ expert_ids,\n+    int32_t* __restrict__ topk_ids,\n+    int32_t* __restrict__ total_cnts,\n+    int32_t* __restrict__ cumsums,\n+    int32_t* __restrict__ offsets,\n+    int num_experts,\n+    int numel,\n+    int num_threads) {\n+#define T_INDEX(tt) total_cnts + (tt) * num_experts\n+\n+  // accumulate count of expert ids locally\n+  at::parallel_for(0, numel, 0, [&](int begin, int end) {\n+    int tid = at::get_thread_num();\n+    int32_t* __restrict__ local_cnts = T_INDEX(tid + 1);\n+\n+    for (int i = begin; i < end; ++i) {\n+      local_cnts[topk_ids[i]]++;\n+    }\n+  });\n+\n+  using iVec = at::vec::Vectorized<int32_t>;\n+  for (int t = 0; t < num_threads; ++t) {\n+    at::vec::map2<int32_t>(\n+        [](iVec x, iVec y) { return x + y; }, T_INDEX(t + 1), T_INDEX(t + 1), T_INDEX(t), num_experts);\n+  }\n+\n+  // the last row holds sums of each experts\n+  int32_t* total_cnts_t_1 = T_INDEX(num_threads);\n+\n+  cumsums[0] = 0;\n+  for (int e = 0; e < num_experts; ++e) {\n+    // accumulate `num_tokens_post_pad`, also as the expert offset\n+    cumsums[e + 1] = cumsums[e] + div_up(total_cnts_t_1[e], BLOCK_M) * BLOCK_M;\n+\n+    for (int k = cumsums[e]; k < cumsums[e + 1]; k += BLOCK_M) {\n+      expert_ids[k / BLOCK_M] = e;\n+    }\n+  }\n+  int num_tokens_post_pad = cumsums[num_experts];\n+\n+  at::parallel_for(0, numel, 0, [&](int begin, int end) {\n+    int tid = at::get_thread_num();\n+    // thread tid offsets in `total_cnts`\n+    int32_t* __restrict__ offsets = T_INDEX(tid);\n+\n+    for (int i = begin; i < end; ++i) {\n+      int32_t expert_id = topk_ids[i];\n+      int32_t b_offset = cumsums[expert_id];\n+      int32_t t_offset = offsets[expert_id];\n+      sorted_ids[b_offset + t_offset] = i;\n+      offsets[expert_id]++;\n+    }\n+  });\n+\n+  // debug: the offset for thread t_1 should be identical to t_2\n+  int32_t* total_cnts_t_2 = T_INDEX(num_threads - 1);\n+  for (int e = 0; e < num_experts; ++e) {\n+    TORCH_CHECK(total_cnts_t_1[e] == total_cnts_t_2[e]);\n+  }\n+\n+  // padding value for sorted_ids: numel\n+  auto sorted_id_size = [=](const int32_t* sorted_ids_ptr) {\n+    for (int d = 0; d < BLOCK_M; ++d) {\n+      if (sorted_ids_ptr[d] == numel) {\n+        return d;\n+      }\n+    }\n+    return BLOCK_M;\n+  };\n+\n+  // offsets holds starting offset for each valida M blocks\n+  //   shape : [num_token_blocks + 1]\n+  offsets[0] = 0;\n+  const int num_token_blocks = num_tokens_post_pad / BLOCK_M;\n+  at::parallel_for(0, num_token_blocks, GRAIN_SIZE / BLOCK_M, [&](int begin, int end) {\n+    for (int mb = begin; mb < end; ++mb) {\n+      offsets[mb + 1] = sorted_id_size(sorted_ids + mb * BLOCK_M);\n+    }\n+  });\n+  // TODO: do we need to vecterize this ?\n+  for (int mb = 0; mb < num_token_blocks; ++mb) {\n+    offsets[mb + 1] += offsets[mb];\n+  }\n+  // debug: the last value of offsets should be `numel`\n+  TORCH_CHECK(offsets[num_token_blocks] == numel);\n+\n+  return num_tokens_post_pad;\n+}\n+\n+//   silu :    shape          leading dimension\n+//  input0  [m_size, BLOCK_N]    BLOCK_N\n+//  input1  [m_size, BLOCK_N]    BLOCK_N\n+//  output  [M * topk, N]          N\n+template <typename scalar_t, int BLOCK_N>\n+inline void silu_and_mul(\n+    scalar_t* __restrict__ output,\n+    const float* __restrict__ input0,  // x: x0, x1\n+    const float* __restrict__ input1,  // y: y0, y1\n+    int64_t m_size,\n+    int64_t N) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  const fVec one = fVec(1.f);\n+\n+  // no remainder\n+  for (int64_t m = 0; m < m_size; ++m) {\n+    scalar_t* __restrict__ out = output + m * N;\n+    const float* __restrict__ x = input0 + m * BLOCK_N;\n+    const float* __restrict__ y = input1 + m * BLOCK_N;\n+\n+    for (int64_t d = 0; d < BLOCK_N; d += bVec::size()) {\n+      fVec x0 = fVec::loadu(x + d);\n+      fVec x1 = fVec::loadu(x + d + fVec::size());\n+      fVec y0 = fVec::loadu(y + d);\n+      fVec y1 = fVec::loadu(y + d + fVec::size());\n+      // silu\n+      x0 = x0 / (one + x0.neg().exp_u20());\n+      x1 = x1 / (one + x1.neg().exp_u20());\n+      // mul\n+      x0 = x0 * y0;\n+      x1 = x1 * y1;\n+      // convert\n+      bVec out_vec = convert_from_float_ext<scalar_t>(x0, x1);\n+      out_vec.store(out + d);\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn2 {\n+  static inline void apply(\n+      const scalar_t* __restrict__ A,\n+      const scalar_t* __restrict__ B0,\n+      const scalar_t* __restrict__ B1,\n+      scalar_t* __restrict__ C,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn2<at::BFloat16, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const at::BFloat16* __restrict__ A,\n+      const at::BFloat16* __restrict__ B0,\n+      const at::BFloat16* __restrict__ B1,\n+      at::BFloat16* __restrict__ C,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+\n+    static_assert(COLS % 2 == 0);\n+\n+    // prefetch distance\n+    constexpr int PREFETCH_SIZE_K = 0;\n+\n+    __m512bh va;\n+    __m512bh vb0[COLS];\n+    __m512bh vb1[COLS];\n+    __m512 vc0[ROWS * COLS];\n+    __m512 vc1[ROWS * COLS];\n+\n+    auto loadc = [&](auto i) {\n+      vc0[i] = _mm512_set1_ps(0.f);\n+      vc1[i] = _mm512_set1_ps(0.f);\n+    };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K2 = K >> 1;\n+    const int64_t lda2 = lda >> 1;\n+    const int64_t ldb2 = ldb;  // ldb * 2 >> 1;\n+    const float* a_ptr = reinterpret_cast<const float*>(A);\n+    const float* b0_ptr = reinterpret_cast<const float*>(B0);\n+    const float* b1_ptr = reinterpret_cast<const float*>(B1);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = (__m512bh)(_mm512_set1_ps(a_ptr[row * lda2 + k]));\n+      }\n+      if constexpr (row == 0) {\n+        vb0[col] = (__m512bh)(_mm512_loadu_si512(b0_ptr + k * ldb2 + col * 16));\n+        vb1[col] = (__m512bh)(_mm512_loadu_si512(b1_ptr + k * ldb2 + col * 16));\n+        if constexpr (PREFETCH_SIZE_K > 0) {\n+          _mm_prefetch(b0_ptr + (k + PREFETCH_SIZE_K) * ldb2 + col * 16, _MM_HINT_T0);\n+          _mm_prefetch(b1_ptr + (k + PREFETCH_SIZE_K) * ldb2 + col * 16, _MM_HINT_T0);\n+        }\n+      }\n+      vc0[i] = _mm512_dpbf16_ps(vc0[i], va, vb0[col]);\n+      vc1[i] = _mm512_dpbf16_ps(vc1[i], va, vb1[col]);\n+    };\n+    for (int64_t k = 0; k < K2; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    using Vec = at::vec::Vectorized<float>;\n+    const Vec one = Vec(1.f);\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      // for COLS = 2, 4 use 512bit store\n+      if constexpr (col % 2 == 0) {\n+        Vec x0 = vc0[row * COLS + col + 0];\n+        Vec x1 = vc0[row * COLS + col + 1];\n+        Vec y0 = vc1[row * COLS + col + 0];\n+        Vec y1 = vc1[row * COLS + col + 1];\n+        // silu\n+        x0 = x0 / (one + x0.neg().exp_u20());\n+        x1 = x1 / (one + x1.neg().exp_u20());\n+        // mul\n+        x0 = x0 * y0;\n+        x1 = x1 * y1;\n+\n+        _mm512_storeu_si512(\n+            reinterpret_cast<__m512i*>((C + row * ldc + col * 16)),\n+            (__m512i)(_mm512_cvtne2ps_pbh(__m512(x1), __m512(x0))));\n+      }\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NN(MB_SIZE, NB_SIZE)       \\\n+  tinygemm_kernel_nn2<scalar_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda, B0 + nb_start * 2, B1 + nb_start * 2, C + mb_start * ldc + nb_start, K, lda, ldb, ldc);\n+\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B0,\n+    const scalar_t* __restrict__ B1,\n+    scalar_t* __restrict__ C,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc) {\n+  // pattern: 1-(2+2)-(8+8)\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 32;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        // mb_size = 1\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NN(1, 32);\n+          break;\n+        // mb_size = 2\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NN(2, 32);\n+          break;\n+        // mb_size = 3\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NN(3, 32);\n+          break;\n+        // mb_size = 4\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NN(4, 32);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn {\n+  static inline void apply(\n+      const scalar_t* __restrict__ A,\n+      const scalar_t* __restrict__ B,\n+      float* __restrict__ C,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_nn<at::BFloat16, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const at::BFloat16* __restrict__ A,\n+      const at::BFloat16* __restrict__ B,\n+      float* __restrict__ C,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+\n+    static_assert(COLS % 2 == 0);\n+\n+    // prefetch distance\n+    constexpr int PREFETCH_SIZE_K = 0;\n+\n+    __m512bh va;\n+    __m512bh vb[COLS];\n+    __m512 vc[ROWS * COLS];\n+\n+    auto loadc = [&](auto i) { vc[i] = _mm512_set1_ps(0.f); };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K2 = K >> 1;\n+    const int64_t lda2 = lda >> 1;\n+    const int64_t ldb2 = ldb;  // ldb * 2 >> 1;\n+    const float* a_ptr = reinterpret_cast<const float*>(A);\n+    const float* b_ptr = reinterpret_cast<const float*>(B);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = (__m512bh)(_mm512_set1_ps(a_ptr[row * lda2 + k]));\n+      }\n+      if constexpr (row == 0) {\n+        vb[col] = (__m512bh)(_mm512_loadu_si512(b_ptr + k * ldb2 + col * 16));\n+        if constexpr (PREFETCH_SIZE_K > 0) {\n+          _mm_prefetch(b_ptr + (k + PREFETCH_SIZE_K) * ldb2 + col * 16, _MM_HINT_T0);\n+        }\n+      }\n+      vc[i] = _mm512_dpbf16_ps(vc[i], va, vb[col]);\n+    };\n+    for (int64_t k = 0; k < K2; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      _mm512_storeu_ps(reinterpret_cast<__m512*>(C + row * ldc + col * 16), vc[i]);\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_NN2(MB_SIZE, NB_SIZE)     \\\n+  tinygemm_kernel_nn<scalar_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda, B + nb_start * 2, C + mb_start * ldc + nb_start, K, lda, ldb, ldc);\n+\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B,\n+    float* __restrict__ C,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc) {\n+  // pattern: 1-2-8\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 32;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        // mb_size = 1\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_NN2(1, 32);\n+          break;\n+        // mb_size = 2\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_NN2(2, 32);\n+          break;\n+        // mb_size = 3\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_NN2(3, 32);\n+          break;\n+        // mb_size = 4\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_NN2(4, 32);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+template <typename scalar_t>\n+void fused_experts_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    scalar_t* __restrict__ ic2,\n+    scalar_t* __restrict__ A_tmp,\n+    float* __restrict__ C_tmp,\n+    const scalar_t* __restrict__ input,\n+    const scalar_t* __restrict__ packed_w1,\n+    const scalar_t* __restrict__ packed_w2,\n+    const float* __restrict__ topk_weights,\n+    const int32_t* __restrict__ sorted_ids,\n+    const int32_t* __restrict__ expert_ids,\n+    const int32_t* __restrict__ offsets,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t E,\n+    int64_t topk,\n+    int64_t num_tokens_post_pad) {\n+  // handle 2 tiles per block\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  // stage 1: intermediate_cache1 = silu(hidden_states @ w1)\n+  const int64_t MB = div_up(num_tokens_post_pad, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  // strides for w1: [E, 2N, K]\n+  TORCH_CHECK(N % BLOCK_N == 0, \"Fixme when N is not multiples of \", BLOCK_N);\n+\n+  const int64_t stride_e = 2 * N * K;\n+  const int64_t stride_n = K;\n+\n+  // here we only parallel on half of 2N to fuse silu_and_mul with gemm\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    scalar_t* __restrict__ A = A_tmp + tid * BLOCK_M * K;\n+    float* __restrict__ C0 = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+    float* __restrict__ C1 = C0 + BLOCK_M * BLOCK_N;\n+\n+    bool is_brgemm_used = false;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB;\n+      int64_t nb = i % NB;\n+\n+      // nb0 from top half and nb1 from bottom half\n+      int64_t nb0 = nb, nb1 = nb + NB;\n+      int64_t n_size = std::min(N - nb0 * BLOCK_N, BLOCK_N);\n+\n+      // B shape [K, n_size] in vnni format\n+      int32_t expert_id = expert_ids[mb];\n+      const scalar_t* __restrict__ B0 = packed_w1 + expert_id * stride_e + nb0 * BLOCK_N * stride_n;\n+      const scalar_t* __restrict__ B1 = packed_w1 + expert_id * stride_e + nb1 * BLOCK_N * stride_n;\n+\n+      // 1.a load A\n+      const int32_t* A_ids = sorted_ids + mb * BLOCK_M;\n+      int64_t m_size = offsets[mb + 1] - offsets[mb];\n+\n+      const bool use_brgemm = can_use_brgemm<scalar_t>(m_size);\n+      is_brgemm_used = is_brgemm_used || use_brgemm;\n+\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        int32_t index = A_ids[m] / topk;\n+        copy_stub(A + m * K, input + index * K, K);\n+      }\n+\n+      if (use_brgemm) {\n+        // 1.b gemm: C0 = A @ B0\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B0,\n+            /* C     */ C0);\n+\n+        // 1.c gemm: C1 = A @ B1\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B1,\n+            /* C     */ C1);\n+\n+        // 1.d silu and mul\n+        const int64_t offset = offsets[mb];\n+        silu_and_mul<scalar_t, BLOCK_N>(ic1 + offset * N + nb * BLOCK_N, C0, C1, m_size, N);\n+      } else {\n+        // fused 1.bcd: silu_and_mul(A @ B0, A @ B1)\n+        const int64_t offset = offsets[mb];\n+        tinygemm_kernel(\n+            /* A     */ A,\n+            /* B0    */ B0,\n+            /* B1    */ B1,\n+            /* C     */ ic1 + offset * N + nb * BLOCK_N,\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ N);\n+      }\n+    }\n+\n+    if (is_brgemm_used) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+\n+  // stage 2: intermediate_cache2 = intermediate_cache1 @ w2\n+  //   w2 : [E, K, N] as [E, OC, IC]\n+  const int64_t OC = K;  // rename K as OC\n+  const int64_t IC = N;  // rename N as IC\n+  const int64_t MB2 = MB;\n+  const int64_t NB2 = div_up(OC, BLOCK_N);\n+  const int64_t stride_e2 = OC * IC;\n+  const int64_t stride_oc = IC;\n+\n+  // parallel on [MB2, NB2]\n+  at::parallel_for(0, MB2 * NB2, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    // we won't be using C1 for gemm2\n+    float* __restrict__ C = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+\n+    bool is_brgemm_used = false;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB2;\n+      int64_t nb = i % NB2;\n+\n+      int64_t m_size = offsets[mb + 1] - offsets[mb];\n+      int64_t n_size = std::min(OC - nb * BLOCK_N, BLOCK_N);\n+\n+      const bool use_brgemm = can_use_brgemm<scalar_t>(m_size);\n+      is_brgemm_used = is_brgemm_used || use_brgemm;\n+\n+      // A ptr from ic1 of [M * topk, N] in sorted order\n+      // so as to avoid copy A to tmp buffer again\n+      const scalar_t* __restrict__ A = ic1 + offsets[mb] * N;\n+      const int32_t* A_ids = sorted_ids + mb * BLOCK_M;\n+\n+      // B shape [IC, n_size] in vnni format\n+      int32_t expert_id = expert_ids[mb];\n+      const scalar_t* __restrict__ B = packed_w2 + expert_id * stride_e2 + nb * BLOCK_N * stride_oc;\n+\n+      // 2.a gemm: C = A @ B\n+      if (use_brgemm) {\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ IC,\n+            /* lda   */ IC,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B,\n+            /* C     */ C);\n+      } else {\n+        tinygemm_kernel(\n+            /* A     */ A,\n+            /* B     */ B,\n+            /* C     */ C,\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ IC,\n+            /* lda   */ IC,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N);\n+      }\n+\n+      // 2.b copy from C to ic2 in original order\n+      //   and also mul topk_weights in float32\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        int32_t index = A_ids[m];\n+        float weight = topk_weights[index];\n+        copy_mul_stub(ic2 + index * K + nb * BLOCK_N, C + m * BLOCK_N, weight, n_size);\n+      }\n+    }\n+\n+    if (is_brgemm_used) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+\n+  // stage 3: out = intermediate_cache2.sum(dim=1)\n+  //   from [M, topk, K] to [M, K]\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      sum_stub(output + m * K, ic2 + m * topk * K, topk, K);\n+    }\n+  });\n+}\n+\n+template <typename scalar_t>\n+void shared_expert_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    float* __restrict__ C_tmp,\n+    scalar_t* __restrict__ input,\n+    const scalar_t* __restrict__ packed_w1,\n+    const scalar_t* __restrict__ packed_w2,\n+    const scalar_t* __restrict__ fused_experts_out,\n+    float routed_scaling_factor,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K) {\n+  // handle 2 tiles per block\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  // stage 1: intermediate_cache1 = silu(hidden_states @ w1)\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  TORCH_CHECK(N % BLOCK_N == 0, \"Fixme when N is not multiples of \", BLOCK_N);\n+  const int64_t stride_n = K;\n+\n+  // here we only parallel on half of 2N to fuse silu_and_mul with gemm\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    float* __restrict__ C0 = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+    float* __restrict__ C1 = C0 + BLOCK_M * BLOCK_N;\n+\n+    bool is_brgemm_used = false;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB;\n+      int64_t nb = i % NB;\n+\n+      // nb0 from top half and nb1 from bottom half\n+      int64_t nb0 = nb, nb1 = nb + NB;\n+      int64_t n_size = std::min(N - nb0 * BLOCK_N, BLOCK_N);\n+      int64_t m_size = std::min(M - mb * BLOCK_M, BLOCK_M);\n+\n+      // int64_t mb_start = mb * BLOCK_M;\n+      // int64_t mb_size = std::min(M - mb_start, BLOCK_M);\n+\n+      // A shape [m_size, K]\n+      const scalar_t* A = input + mb * BLOCK_M * K;\n+\n+      // B shape [K, n_size] in vnni format\n+      const scalar_t* __restrict__ B0 = packed_w1 + nb0 * BLOCK_N * stride_n;\n+      const scalar_t* __restrict__ B1 = packed_w1 + nb1 * BLOCK_N * stride_n;\n+\n+      const bool use_brgemm = can_use_brgemm<scalar_t>(m_size);\n+      is_brgemm_used = is_brgemm_used || use_brgemm;\n+\n+      if (use_brgemm) {\n+        // 1.b gemm: C0 = A @ B0\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B0,\n+            /* C     */ C0);\n+\n+        // 1.c gemm: C1 = A @ B1\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B1,\n+            /* C     */ C1);\n+\n+        // 1.d silu and mul\n+        silu_and_mul<scalar_t, BLOCK_N>(ic1 + mb * BLOCK_M * N + nb * BLOCK_N, C0, C1, m_size, N);\n+      } else {\n+        // fused 1.bcd: silu_and_mul(A @ B0, A @ B1)\n+        tinygemm_kernel(\n+            /* A     */ A,\n+            /* B0    */ B0,\n+            /* B1    */ B1,\n+            /* C     */ ic1 + mb * BLOCK_M * N + nb * BLOCK_N,\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ K,\n+            /* lda   */ K,\n+            /* ldb   */ n_size,\n+            /* ldc   */ N);\n+      }\n+    }\n+\n+    if (is_brgemm_used) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+\n+  // stage 2: output = intermediate_cache1 @ w2\n+  //   w2 : [K, N] as [OC, IC]\n+  const int64_t OC = K;  // rename K as OC\n+  const int64_t IC = N;  // rename N as IC\n+  const int64_t MB2 = MB;\n+  const int64_t NB2 = div_up(OC, BLOCK_N);\n+  const int64_t stride_oc = IC;\n+\n+  // parallel on [MB2, NB2]\n+  at::parallel_for(0, MB2 * NB2, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    // we won't be using C1 for gemm2\n+    float* __restrict__ C = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+\n+    bool is_brgemm_used = false;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB2;\n+      int64_t nb = i % NB2;\n+\n+      int64_t m_size = std::min(M - mb * BLOCK_M, BLOCK_M);\n+      int64_t n_size = std::min(OC - nb * BLOCK_N, BLOCK_N);\n+\n+      const bool use_brgemm = can_use_brgemm<scalar_t>(m_size);\n+      is_brgemm_used = is_brgemm_used || use_brgemm;\n+\n+      // A shape [m_size, IC]\n+      const scalar_t* __restrict__ A = ic1 + mb * BLOCK_M * N;\n+\n+      // B shape [IC, n_size] in vnni format\n+      const scalar_t* __restrict__ B = packed_w2 + nb * BLOCK_N * stride_oc;\n+\n+      // 2.a gemm: C = A @ B\n+      if (use_brgemm) {\n+        at::native::cpublas::brgemm(\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ IC,\n+            /* lda   */ IC,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N,\n+            /* add_C */ false,\n+            /* A     */ A,\n+            /* B     */ B,\n+            /* C     */ C);\n+      } else {\n+        tinygemm_kernel(\n+            /* A     */ A,\n+            /* B     */ B,\n+            /* C     */ C,\n+            /* M     */ m_size,\n+            /* N     */ n_size,\n+            /* K     */ IC,\n+            /* lda   */ IC,\n+            /* ldb   */ n_size,\n+            /* ldc   */ BLOCK_N);\n+      }\n+\n+      // 2.b copy from C to output and add fused_experts_out\n+      scalar_t* __restrict__ out = output + mb * BLOCK_M * K + nb * BLOCK_N;\n+      const scalar_t* __restrict__ fused_out = fused_experts_out + mb * BLOCK_M * K + nb * BLOCK_N;\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        add_mul_stub(out + m * K, C + m * BLOCK_N, fused_out + m * K, routed_scaling_factor, n_size);\n+      }\n+    }\n+\n+    if (is_brgemm_used) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// hidden_states: [M, K]\n+// w1: [E, 2N, K]\n+// w2: [E, K, N]\n+// topk_weights: [M, topk]\n+// topk_ids: [M, topk] (int32_t)\n+//\n+at::Tensor fused_experts_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& w1,\n+    at::Tensor& w2,\n+    at::Tensor& topk_weights,\n+    at::Tensor& topk_ids,\n+    bool inplace,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& w1_scale,\n+    std::optional<at::Tensor>& w2_scale,\n+    std::optional<at::Tensor>& a1_scale,\n+    std::optional<at::Tensor>& a2_scale,\n+    bool is_vnni) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::fused_experts_cpu\", std::vector<c10::IValue>({hidden_states, w1, w2, topk_weights, topk_ids}));\n+\n+  auto packed_w1 = is_vnni ? w1 : convert_weight_packed(w1);\n+  auto packed_w2 = is_vnni ? w2 : convert_weight_packed(w2);\n+\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  const auto st = hidden_states.scalar_type();\n+  CHECK_INPUT(hidden_states);\n+  CHECK_INPUT(w1);\n+  CHECK_INPUT(w2);\n+  CHECK_EQ(topk_weights.sizes(), topk_ids.sizes());\n+  CHECK_DIM(2, hidden_states);\n+  CHECK_DIM(3, w1);\n+  CHECK_DIM(3, w2);\n+  CHECK_DIM(2, topk_weights);\n+  CHECK_DIM(2, topk_ids);\n+\n+  CHECK_EQ(topk_ids.scalar_type(), at::kInt);\n+  CHECK_EQ(topk_weights.scalar_type(), at::kFloat);\n+\n+  int64_t M = hidden_states.size(0);\n+  int64_t K = hidden_states.size(1);\n+  int64_t N = w1.size(1) / 2;\n+  int64_t E = w1.size(0);\n+  int64_t topk = topk_weights.size(1);\n+\n+  // we use int32_t compensation for int8 w8a8\n+  int64_t packed_K = get_row_size(K, use_int8_w8a8);\n+  int64_t packed_N = get_row_size(N, use_int8_w8a8);\n+\n+  // check weight shapes\n+  CHECK_EQ(w2.size(0), E);\n+  CHECK_EQ(w2.size(1), K);\n+  CHECK_EQ(packed_w1.size(2), packed_K);\n+  CHECK_EQ(packed_w2.size(2), packed_N);\n+\n+  if (use_int8_w8a8) {\n+    TORCH_CHECK(w1_scale.has_value(), \"missing w1_scale for int8 w8a8.\");\n+    TORCH_CHECK(w2_scale.has_value(), \"missing w2_scale for int8 w8a8.\");\n+    TORCH_CHECK(!a1_scale.has_value(), \"static quantization for activation not supported.\");\n+    TORCH_CHECK(!a2_scale.has_value(), \"static quantization for activation not supported.\");\n+  }\n+\n+  at::Tensor out_hidden_states = inplace ? hidden_states : at::empty_like(hidden_states);\n+\n+  // NB: worst case is each expert holds a block with remainder of 1\n+  //   1. sorted_ids : [M * topk + E * (BLOCK_M - 1)]\n+  //   2. expert_ids : [max_num_blocks]\n+  //   3. total_cnts : [T + 1, E]\n+  //   4. cumsums    : [E + 1]\n+  //   5. offsets    : [max_num_blocks + 1]\n+  //\n+  int num_threads = at::get_num_threads();\n+  int64_t max_num_tokens_padded = M * topk + E * (BLOCK_M - 1);\n+  int64_t max_num_blocks = div_up(max_num_tokens_padded, BLOCK_M);\n+  auto buffer = at::empty(\n+      {max_num_tokens_padded + max_num_blocks + (num_threads + 1) * E + (E + 1) + (max_num_blocks + 1)},\n+      topk_ids.options());\n+\n+  int32_t* __restrict__ sorted_ids = buffer.data_ptr<int32_t>();\n+  int32_t* __restrict__ expert_ids = sorted_ids + max_num_tokens_padded;\n+  int32_t* __restrict__ total_cnts = expert_ids + max_num_blocks;\n+  int32_t* __restrict__ cumsums = total_cnts + (num_threads + 1) * E;\n+  int32_t* __restrict__ offsets = cumsums + (E + 1);\n+\n+  // init sorted_ids with `numel` as the padding number\n+  // init expert_ids with `num_experts`\n+  int64_t numel = M * topk;\n+  at::parallel_for(0, max_num_blocks, GRAIN_SIZE / BLOCK_M, [&](int64_t begin, int64_t end) {\n+    int64_t m_start = begin * BLOCK_M;\n+    int64_t m_size = std::min((end - begin) * BLOCK_M, max_num_tokens_padded - m_start);\n+    fill_stub(sorted_ids + m_start, (int32_t)numel, m_size);\n+    fill_stub(expert_ids + begin, (int32_t)E, end - begin);\n+  });\n+  // zero total_cnts and cumsums\n+  at::parallel_for(0, (num_threads + 1) * E + (E + 1), GRAIN_SIZE, [&](int64_t begin, int64_t end) {\n+    fill_stub(total_cnts + begin, 0, end - begin);\n+  });\n+\n+  // align experts index\n+  int64_t num_tokens_post_pad = moe_align_block_size<BLOCK_M>(\n+      sorted_ids, expert_ids, topk_ids.data_ptr<int32_t>(), total_cnts, cumsums, offsets, E, numel, num_threads);\n+\n+  // unlike triton kernel, we fuse silu with gemm1 so only need 2 intermediate_caches:\n+  //   1. intermediate_cache1 : [M * topk, N]\n+  //   2. intermediate_cache2 : [M * topk, K]\n+  //   3. A_tmp : [T, BLOCK_M * K]\n+  //   4. C_tmp : [T, 2 * BLOCK_M * BLOCK_N]\n+  //\n+  // for int8 w8a8:\n+  //   5. Aq_tmp : [M, K] or [M * topk, N]\n+  //   6. As_tmp : [M * topk]\n+  //\n+  int64_t buffer_size_nbytes = M * topk * N * 2 + M * topk * K * 2 +\n+                               num_threads * BLOCK_M * K * (use_int8_w8a8 ? 1 : 2) +\n+                               num_threads * 2 * BLOCK_M * BLOCK_N * sizeof(float);\n+\n+  if (use_int8_w8a8) {\n+    buffer_size_nbytes += std::max(M * K, M * topk * N) + M * topk * sizeof(float);\n+  }\n+\n+  auto buffer2 = at::empty({buffer_size_nbytes}, hidden_states.options().dtype(at::kChar));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"fused_experts_kernel_impl\", [&] {\n+    scalar_t* __restrict__ intermediate_cache1 = (scalar_t*)((void*)(buffer2.data_ptr<int8_t>()));\n+    scalar_t* __restrict__ intermediate_cache2 = intermediate_cache1 + M * topk * N;\n+\n+    if (use_int8_w8a8) {\n+      uint8_t* __restrict__ A_tmp = (uint8_t*)((void*)(intermediate_cache2 + M * topk * K));\n+      float* __restrict__ C_tmp = (float*)((void*)(A_tmp + num_threads * BLOCK_M * K));\n+      uint8_t* __restrict__ Aq_tmp = (uint8_t*)((void*)(C_tmp + num_threads * 2 * BLOCK_M * BLOCK_N));\n+      float* __restrict__ As_tmp = (float*)((void*)(Aq_tmp + std::max(M * K, M * topk * N)));\n+\n+      auto w1s = w1_scale.value();\n+      auto w2s = w2_scale.value();\n+      TORCH_CHECK(w1s.numel() == E * 2 * N);\n+      TORCH_CHECK(w2s.numel() == E * K);\n+\n+      fused_experts_int8_kernel_impl<scalar_t>(\n+          out_hidden_states.data_ptr<scalar_t>(),\n+          intermediate_cache1,\n+          intermediate_cache2,\n+          A_tmp,\n+          C_tmp,\n+          Aq_tmp,\n+          As_tmp,\n+          hidden_states.data_ptr<scalar_t>(),\n+          packed_w1.data_ptr<int8_t>(),\n+          packed_w2.data_ptr<int8_t>(),\n+          w1s.data_ptr<float>(),\n+          w2s.data_ptr<float>(),\n+          topk_weights.data_ptr<float>(),\n+          sorted_ids,\n+          expert_ids,\n+          offsets,\n+          M,\n+          N,\n+          K,\n+          E,\n+          topk,\n+          num_tokens_post_pad);\n+    } else {\n+      scalar_t* __restrict__ A_tmp = intermediate_cache2 + M * topk * K;\n+      float* __restrict__ C_tmp = (float*)((void*)(A_tmp + num_threads * BLOCK_M * K));\n+\n+      fused_experts_kernel_impl<scalar_t>(\n+          out_hidden_states.data_ptr<scalar_t>(),\n+          intermediate_cache1,\n+          intermediate_cache2,\n+          A_tmp,\n+          C_tmp,\n+          hidden_states.data_ptr<scalar_t>(),\n+          packed_w1.data_ptr<scalar_t>(),\n+          packed_w2.data_ptr<scalar_t>(),\n+          topk_weights.data_ptr<float>(),\n+          sorted_ids,\n+          expert_ids,\n+          offsets,\n+          M,\n+          N,\n+          K,\n+          E,\n+          topk,\n+          num_tokens_post_pad);\n+    }\n+  });\n+  return out_hidden_states;\n+}\n+\n+// shared expert kernel\n+//\n+// hidden_states: [M, K]\n+// w1: [2N, K]\n+// w2: [K, N]\n+// fused_experts_out\n+at::Tensor shared_expert_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& w1,\n+    at::Tensor& w2,\n+    at::Tensor& fused_experts_out,\n+    double routed_scaling_factor,\n+    bool inplace,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& w1_scale,\n+    std::optional<at::Tensor>& w2_scale,\n+    std::optional<at::Tensor>& a1_scale,\n+    std::optional<at::Tensor>& a2_scale,\n+    bool is_vnni) {\n+  RECORD_FUNCTION(\"sgl-kernel::shared_expert_cpu\", std::vector<c10::IValue>({hidden_states, w1, w2}));\n+\n+  auto packed_w1 = is_vnni ? w1 : convert_weight_packed(w1);\n+  auto packed_w2 = is_vnni ? w2 : convert_weight_packed(w2);\n+\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  const auto st = hidden_states.scalar_type();\n+  CHECK_INPUT(hidden_states);\n+  CHECK_INPUT(fused_experts_out);\n+  CHECK_INPUT(w1);\n+  CHECK_INPUT(w2);\n+  CHECK_DIM(2, hidden_states);\n+  CHECK_DIM(2, w1);\n+  CHECK_DIM(2, w2);\n+  CHECK_EQ(hidden_states.sizes(), fused_experts_out.sizes());\n+  CHECK_EQ(hidden_states.scalar_type(), st);\n+\n+  int64_t M = hidden_states.size(0);\n+  int64_t K = hidden_states.size(1);\n+  int64_t N = w1.size(0) / 2;\n+\n+  // we use int32_t compensation for int8 w8a8\n+  int64_t packed_K = get_row_size(K, use_int8_w8a8);\n+  int64_t packed_N = get_row_size(N, use_int8_w8a8);\n+\n+  // check weight shapes\n+  CHECK_EQ(w2.size(0), K);\n+  CHECK_EQ(packed_w1.size(1), packed_K);\n+  CHECK_EQ(packed_w2.size(1), packed_N);\n+\n+  if (use_int8_w8a8) {\n+    TORCH_CHECK(w1_scale.has_value(), \"missing w1_scale for int8 w8a8.\");\n+    TORCH_CHECK(w2_scale.has_value(), \"missing w2_scale for int8 w8a8.\");\n+    TORCH_CHECK(!a1_scale.has_value(), \"static quantization for activation not supported.\");\n+    TORCH_CHECK(!a2_scale.has_value(), \"static quantization for activation not supported.\");\n+  }\n+\n+  at::Tensor out_hidden_states = inplace ? hidden_states : at::empty_like(hidden_states);\n+\n+  // unlike triton kernel, we fuse silu with gemm1 so only need 2 intermediate_caches:\n+  //   1. intermediate_cache1 : [M, N]\n+  //   2. C_tmp : [T, 2 * BLOCK_M * BLOCK_N]\n+  //\n+  // for int8 w8a8:\n+  //   3. Aq_tmp : [M, K] or [M, N]\n+  //   4. As_tmp : [M]\n+  //\n+  int num_threads = at::get_num_threads();\n+  int64_t buffer_size_nbytes = M * N * 2 + num_threads * 2 * BLOCK_M * BLOCK_N * sizeof(float);\n+\n+  if (use_int8_w8a8) {\n+    buffer_size_nbytes += std::max(M * K, M * N) + M * sizeof(float);\n+  }\n+\n+  auto buffer = at::empty({buffer_size_nbytes}, hidden_states.options().dtype(at::kChar));\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"share_experts_kernel_impl\", [&] {\n+    scalar_t* __restrict__ intermediate_cache1 = (scalar_t*)((void*)(buffer.data_ptr<int8_t>()));\n+    float* __restrict__ C_tmp = (float*)((void*)(intermediate_cache1 + M * N));\n+\n+    if (use_int8_w8a8) {\n+      uint8_t* __restrict__ Aq_tmp = (uint8_t*)((void*)(C_tmp + num_threads * 2 * BLOCK_M * BLOCK_N));\n+      float* __restrict__ As_tmp = (float*)((void*)(Aq_tmp + std::max(M * K, M * N)));\n+\n+      auto w1s = w1_scale.value();\n+      auto w2s = w2_scale.value();\n+      TORCH_CHECK(w1s.numel() == 2 * N);\n+      TORCH_CHECK(w2s.numel() == K);\n+\n+      shared_expert_int8_kernel_impl<scalar_t>(\n+          out_hidden_states.data_ptr<scalar_t>(),\n+          intermediate_cache1,\n+          C_tmp,\n+          Aq_tmp,\n+          As_tmp,\n+          hidden_states.data_ptr<scalar_t>(),\n+          packed_w1.data_ptr<int8_t>(),\n+          packed_w2.data_ptr<int8_t>(),\n+          w1s.data_ptr<float>(),\n+          w2s.data_ptr<float>(),\n+          fused_experts_out.data_ptr<scalar_t>(),\n+          routed_scaling_factor,\n+          M,\n+          N,\n+          K);\n+    } else {\n+      shared_expert_kernel_impl<scalar_t>(\n+          out_hidden_states.data_ptr<scalar_t>(),\n+          intermediate_cache1,\n+          C_tmp,\n+          hidden_states.data_ptr<scalar_t>(),\n+          packed_w1.data_ptr<scalar_t>(),\n+          packed_w2.data_ptr<scalar_t>(),\n+          fused_experts_out.data_ptr<scalar_t>(),\n+          routed_scaling_factor,\n+          M,\n+          N,\n+          K);\n+    }\n+  });\n+  return out_hidden_states;\n+}\ndiff --git a/sgl-kernel/csrc/cpu/moe_int8.cpp b/sgl-kernel/csrc/cpu/moe_int8.cpp\nnew file mode 100644\nindex 000000000..e12e5e7cf\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/moe_int8.cpp\n@@ -0,0 +1,830 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t>\n+inline void copy_stub(scalar_t* __restrict__ out, const scalar_t* __restrict__ input, int64_t size) {\n+  using Vec = at::vec::Vectorized<scalar_t>;\n+// no remainder\n+#pragma GCC unroll 4\n+  for (int64_t d = 0; d < size; d += Vec::size()) {\n+    Vec data = Vec::loadu(input + d);\n+    data.store(out + d);\n+  }\n+}\n+\n+template <>\n+inline void copy_stub<uint8_t>(uint8_t* __restrict__ out, const uint8_t* __restrict__ input, int64_t size) {\n+  // size might be 64x + 32\n+  std::memcpy(out, input, size * sizeof(uint8_t));\n+}\n+\n+template <typename scalar_t>\n+inline void copy_mul_stub(scalar_t* __restrict__ out, const float* __restrict__ input, float weight, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  const fVec weight_vec = fVec(weight);\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec data0 = fVec::loadu(input + d) * weight_vec;\n+    fVec data1 = fVec::loadu(input + d + fVec::size()) * weight_vec;\n+    bVec out_vec = convert_from_float_ext<scalar_t>(data0, data1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d] * weight);\n+  }\n+}\n+\n+// acc from [topk, K] to [K]\n+template <typename scalar_t>\n+inline void sum_stub(scalar_t* __restrict__ out, const scalar_t* __restrict__ input, int64_t topk, int64_t K) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  if (topk == 1) {\n+    // do copy for topk = 1\n+    copy_stub(out, input, K);\n+  } else {\n+    // do sum for topk != 1\n+    int64_t d;\n+#pragma GCC unroll 4\n+    for (d = 0; d <= K - kVecSize; d += kVecSize) {\n+      fVec sum_fvec0 = fVec(0.f);\n+      fVec sum_fvec1 = fVec(0.f);\n+      for (int t = 0; t < topk; ++t) {\n+        bVec x_bvec = bVec::loadu(input + t * K + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        sum_fvec0 += x_fvec0;\n+        sum_fvec1 += x_fvec1;\n+      }\n+      bVec out_bvec = convert_from_float_ext<scalar_t>(sum_fvec0, sum_fvec1);\n+      out_bvec.store(out + d);\n+    }\n+    for (; d < K; ++d) {\n+      float sum_val = 0.f;\n+      for (int t = 0; t < topk; ++t) {\n+        sum_val += static_cast<float>(input[t * K + d]);\n+      }\n+      out[d] = static_cast<scalar_t>(sum_val);\n+    }\n+  }\n+}\n+\n+// out = input + input2 * scale\n+template <typename scalar_t>\n+inline void add_mul_stub(\n+    scalar_t* __restrict__ out,\n+    const float* __restrict__ input,\n+    const scalar_t* __restrict__ input2,\n+    float scale,\n+    int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  constexpr int kVecSize = bVec::size();\n+  const fVec s_vec = fVec(scale);\n+  int64_t d;\n+#pragma GCC unroll 4\n+  for (d = 0; d <= size - kVecSize; d += kVecSize) {\n+    fVec x0 = fVec::loadu(input + d);\n+    fVec x1 = fVec::loadu(input + d + fVec::size());\n+\n+    bVec y_bvec = bVec::loadu(input2 + d);\n+    fVec y0, y1;\n+    std::tie(y0, y1) = at::vec::convert_to_float(y_bvec);\n+\n+    x0 = x0 + y0 * s_vec;\n+    x1 = x1 + y1 * s_vec;\n+    bVec out_vec = convert_from_float_ext<scalar_t>(x0, x1);\n+    out_vec.store(out + d);\n+  }\n+  for (; d < size; ++d) {\n+    out[d] = static_cast<scalar_t>(input[d] + float(input2[d]) * scale);\n+  }\n+}\n+\n+/// gemm for w13\n+template <typename scalar_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_vnni {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B0,\n+      const int8_t* __restrict__ B1,\n+      scalar_t* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs0,\n+      const float* __restrict__ Bs1,\n+      const int32_t* __restrict__ Bcomp0,\n+      const int32_t* __restrict__ Bcomp1,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_vnni<at::BFloat16, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B0,\n+      const int8_t* __restrict__ B1,\n+      at::BFloat16* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs0,\n+      const float* __restrict__ Bs1,\n+      const int32_t* __restrict__ Bcomp0,\n+      const int32_t* __restrict__ Bcomp1,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+    static_assert(COLS % 2 == 0);\n+\n+    __m512i va;\n+    __m512i vb0[COLS];\n+    __m512i vb1[COLS];\n+    __m512i vc0[ROWS * COLS];\n+    __m512i vc1[ROWS * COLS];\n+    __m512i vcomp0[COLS];\n+    __m512i vcomp1[COLS];\n+    __m512 vas;\n+    __m512 vbs0[COLS];\n+    __m512 vbs1[COLS];\n+\n+    auto loadc = [&](auto i) {\n+      vc0[i] = _mm512_set1_epi32(0);\n+      vc1[i] = _mm512_set1_epi32(0);\n+    };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K4 = K >> 2;\n+    const int64_t lda4 = lda >> 2;\n+    const int64_t ldb4 = ldb;  // ldb * 4 >> 2;\n+    const int32_t* a_ptr = reinterpret_cast<const int32_t*>(A);\n+    const int32_t* b0_ptr = reinterpret_cast<const int32_t*>(B0);\n+    const int32_t* b1_ptr = reinterpret_cast<const int32_t*>(B1);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = _mm512_set1_epi32(a_ptr[row * lda4 + k]);\n+      }\n+      if constexpr (row == 0) {\n+        vb0[col] = _mm512_loadu_si512(b0_ptr + k * ldb4 + col * 16);\n+        vb1[col] = _mm512_loadu_si512(b1_ptr + k * ldb4 + col * 16);\n+      }\n+      vc0[i] = _mm512_dpbusd_epi32(vc0[i], va, vb0[col]);\n+      vc1[i] = _mm512_dpbusd_epi32(vc1[i], va, vb1[col]);\n+    };\n+    for (int64_t k = 0; k < K4; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto scalec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      // load a scale\n+      if constexpr (col == 0) {\n+        vas = _mm512_set1_ps(As[row]);\n+      }\n+      // load b scale and vcomp\n+      if constexpr (row == 0) {\n+        vbs0[col] = _mm512_loadu_ps(Bs0 + col * 16);\n+        vbs1[col] = _mm512_loadu_ps(Bs1 + col * 16);\n+        vcomp0[col] = _mm512_loadu_si512(Bcomp0 + col * 16);\n+        vcomp1[col] = _mm512_loadu_si512(Bcomp1 + col * 16);\n+      }\n+      __m512 c0 = _mm512_cvtepi32_ps(_mm512_sub_epi32(vc0[i], vcomp0[col]));\n+      __m512 c1 = _mm512_cvtepi32_ps(_mm512_sub_epi32(vc1[i], vcomp1[col]));\n+      vc0[i] = _mm512_castps_si512(_mm512_mul_ps(_mm512_mul_ps(c0, vas), vbs0[col]));\n+      vc1[i] = _mm512_castps_si512(_mm512_mul_ps(_mm512_mul_ps(c1, vas), vbs1[col]));\n+    };\n+    Unroll<ROWS * COLS>{}(scalec);\n+\n+    using Vec = at::vec::Vectorized<float>;\n+    const Vec one = Vec(1.f);\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+      // for COLS = 2, 4 use 512bit store\n+      if constexpr (col % 2 == 0) {\n+        Vec x0 = _mm512_castsi512_ps(vc0[row * COLS + col + 0]);\n+        Vec x1 = _mm512_castsi512_ps(vc0[row * COLS + col + 1]);\n+        Vec y0 = _mm512_castsi512_ps(vc1[row * COLS + col + 0]);\n+        Vec y1 = _mm512_castsi512_ps(vc1[row * COLS + col + 1]);\n+        // silu\n+        x0 = x0 / (one + x0.neg().exp_u20());\n+        x1 = x1 / (one + x1.neg().exp_u20());\n+        // mul\n+        x0 = x0 * y0;\n+        x1 = x1 * y1;\n+\n+        _mm512_storeu_si512(\n+            reinterpret_cast<__m512i*>((C + row * ldc + col * 16)),\n+            (__m512i)(_mm512_cvtne2ps_pbh(__m512(x1), __m512(x0))));\n+      }\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_VNNI(MB_SIZE, NB_SIZE)      \\\n+  tinygemm_kernel_vnni<scalar_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda,                                  \\\n+      B0 + nb_start * 4,                                   \\\n+      B1 + nb_start * 4,                                   \\\n+      C + mb_start * ldc + nb_start,                       \\\n+      As + mb_start,                                       \\\n+      Bs0 + nb_start,                                      \\\n+      Bs1 + nb_start,                                      \\\n+      Bcomp0 + nb_start,                                   \\\n+      Bcomp1 + nb_start,                                   \\\n+      K,                                                   \\\n+      lda,                                                 \\\n+      ldb,                                                 \\\n+      ldc);\n+\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B0,\n+    const int8_t* __restrict__ B1,\n+    scalar_t* __restrict__ C,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs0,\n+    const float* __restrict__ Bs1,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc) {\n+  const int32_t* Bcomp0 = reinterpret_cast<const int32_t*>(B0 + block_size_n() * K);\n+  const int32_t* Bcomp1 = reinterpret_cast<const int32_t*>(B1 + block_size_n() * K);\n+\n+  // pattern: 1-(2+2)-(8+8)\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 32;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI(1, 32);\n+          break;\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI(2, 32);\n+          break;\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI(3, 32);\n+          break;\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI(4, 32);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+/// gemm for w2\n+template <typename scalar_t, int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_vnni2 {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B,\n+      float* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs,\n+      const int32_t* __restrict__ Bcomp,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    TORCH_CHECK(false, \"tinygemm_kernel_nn: scalar path not implemented!\");\n+  }\n+};\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <int BLOCK_M, int BLOCK_N>\n+struct tinygemm_kernel_vnni2<at::BFloat16, BLOCK_M, BLOCK_N> {\n+  static inline void apply(\n+      const uint8_t* __restrict__ A,\n+      const int8_t* __restrict__ B,\n+      float* __restrict__ C,\n+      const float* __restrict__ As,\n+      const float* __restrict__ Bs,\n+      const int32_t* __restrict__ Bcomp,\n+      int64_t K,\n+      int64_t lda,\n+      int64_t ldb,\n+      int64_t ldc) {\n+    constexpr int ROWS = BLOCK_M;\n+    constexpr int COLS = BLOCK_N / 16;\n+    static_assert(COLS % 2 == 0);\n+\n+    __m512i va;\n+    __m512i vb[COLS];\n+    __m512i vc[ROWS * COLS];\n+    __m512i vcomp[COLS];\n+    __m512 vas;\n+    __m512 vbs[COLS];\n+\n+    auto loadc = [&](auto i) { vc[i] = _mm512_set1_epi32(0); };\n+    Unroll<ROWS * COLS>{}(loadc);\n+\n+    const int64_t K4 = K >> 2;\n+    const int64_t lda4 = lda >> 2;\n+    const int64_t ldb4 = ldb;  // ldb * 4 >> 2;\n+    const int32_t* a_ptr = reinterpret_cast<const int32_t*>(A);\n+    const int32_t* b_ptr = reinterpret_cast<const int32_t*>(B);\n+\n+    auto compute = [&](auto i, int64_t k) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      if constexpr (col == 0) {\n+        va = _mm512_set1_epi32(a_ptr[row * lda4 + k]);\n+      }\n+      if constexpr (row == 0) {\n+        vb[col] = _mm512_loadu_si512(b_ptr + k * ldb4 + col * 16);\n+      }\n+      vc[i] = _mm512_dpbusd_epi32(vc[i], va, vb[col]);\n+    };\n+    for (int64_t k = 0; k < K4; ++k) {\n+      Unroll<ROWS * COLS>{}(compute, k);\n+    }\n+\n+    auto storec = [&](auto i) {\n+      constexpr int row = i / COLS;\n+      constexpr int col = i % COLS;\n+\n+      // load a scale\n+      if constexpr (col == 0) {\n+        vas = _mm512_set1_ps(As[row]);\n+      }\n+      // load b scale and vcomp per 2 vectors\n+      // also load bias if any\n+      if constexpr (row == 0) {\n+        if constexpr (col % 2 == 0) {\n+          vbs[col + 0] = _mm512_loadu_ps(Bs + col * 16);\n+          vbs[col + 1] = _mm512_loadu_ps(Bs + col * 16 + 16);\n+          vcomp[col + 0] = _mm512_loadu_si512(Bcomp + col * 16);\n+          vcomp[col + 1] = _mm512_loadu_si512(Bcomp + col * 16 + 16);\n+        }\n+      }\n+      __m512 x = _mm512_cvtepi32_ps(_mm512_sub_epi32(vc[i], vcomp[col]));\n+      x = _mm512_mul_ps(_mm512_mul_ps(x, vas), vbs[col]);\n+      _mm512_storeu_ps(reinterpret_cast<__m512*>(C + row * ldc + col * 16), x);\n+    };\n+    Unroll<ROWS * COLS>{}(storec);\n+  }\n+};\n+#endif\n+\n+#define LAUNCH_TINYGEMM_KERNEL_VNNI2(MB_SIZE, NB_SIZE)      \\\n+  tinygemm_kernel_vnni2<scalar_t, MB_SIZE, NB_SIZE>::apply( \\\n+      A + mb_start * lda,                                   \\\n+      B + nb_start * 4,                                     \\\n+      C + mb_start * ldc + nb_start,                        \\\n+      As + mb_start,                                        \\\n+      Bs + nb_start,                                        \\\n+      Bcomp + nb_start,                                     \\\n+      K,                                                    \\\n+      lda,                                                  \\\n+      ldb,                                                  \\\n+      ldc);\n+\n+template <typename scalar_t>\n+void tinygemm_kernel(\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B,\n+    float* __restrict__ C,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t lda,\n+    int64_t ldb,\n+    int64_t ldc) {\n+  // B compensation\n+  const int32_t* Bcomp = reinterpret_cast<const int32_t*>(B + block_size_n() * K);\n+\n+  // pattern: 1-4-16\n+  constexpr int64_t BLOCK_M = 4;\n+  constexpr int64_t BLOCK_N = 64;\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+  for (int64_t mb = 0; mb < MB; ++mb) {\n+    int64_t mb_start = mb * BLOCK_M;\n+    int64_t mb_size = std::min(BLOCK_M, M - mb_start);\n+    for (int64_t nb = 0; nb < NB; ++nb) {\n+      int64_t nb_start = nb * BLOCK_N;\n+      int64_t nb_size = std::min(BLOCK_N, N - nb_start);\n+\n+      switch (mb_size << 4 | nb_size >> 4) {\n+        case 0x12:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI2(1, 32);\n+          break;\n+        case 0x22:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI2(2, 32);\n+          break;\n+        case 0x32:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI2(3, 32);\n+          break;\n+        case 0x42:\n+          LAUNCH_TINYGEMM_KERNEL_VNNI2(4, 32);\n+          break;\n+        default:\n+          TORCH_CHECK(false, \"Unexpected block size, \", mb_size, \"x\", \"nb_size\");\n+      }\n+    }\n+  }\n+}\n+\n+}  // anonymous namespace\n+\n+template <typename scalar_t>\n+void fused_experts_int8_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    scalar_t* __restrict__ ic2,\n+    uint8_t* __restrict__ A_tmp,\n+    float* __restrict__ C_tmp,\n+    uint8_t* __restrict__ Aq_tmp,\n+    float* __restrict__ As_tmp,\n+    const scalar_t* __restrict__ input,\n+    const int8_t* __restrict__ packed_w1,\n+    const int8_t* __restrict__ packed_w2,\n+    const float* __restrict__ w1s,\n+    const float* __restrict__ w2s,\n+    const float* __restrict__ topk_weights,\n+    const int32_t* __restrict__ sorted_ids,\n+    const int32_t* __restrict__ expert_ids,\n+    const int32_t* __restrict__ offsets,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K,\n+    int64_t E,\n+    int64_t topk,\n+    int64_t num_tokens_post_pad) {\n+  // handle 2 tiles per block\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  // stage 0: quantize input to uint8, [M, K]\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      quantize_row_int8<scalar_t>(Aq_tmp + m * K, As_tmp[m], input + m * K, K);\n+    }\n+  });\n+\n+  // stage 1: intermediate_cache1 = silu(hidden_states @ w1)\n+  const int64_t MB = div_up(num_tokens_post_pad, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  // strides for w1: [E, 2N, K]\n+  TORCH_CHECK(N % BLOCK_N == 0, \"Fixme when N is not multiples of \", BLOCK_N);\n+\n+  // K and N are packed for int8\n+  const int64_t packed_K = get_row_size<int8_t>(K);\n+  const int64_t packed_N = get_row_size<int8_t>(N);\n+\n+  const int64_t stride_e = 2 * N * packed_K;\n+  const int64_t stride_n = packed_K;\n+  // here we only parallel on half of 2N to fuse silu_and_mul with gemm\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    uint8_t* __restrict__ A = A_tmp + tid * BLOCK_M * K;\n+\n+    alignas(64) float As[BLOCK_M];\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB;\n+      int64_t nb = i % NB;\n+\n+      // nb0 from top half and nb1 from bottom half\n+      int64_t nb0 = nb, nb1 = nb + NB;\n+      int64_t n_size = std::min(N - nb0 * BLOCK_N, BLOCK_N);\n+\n+      // B shape [K, n_size] in vnni format\n+      int32_t expert_id = expert_ids[mb];\n+      const int8_t* __restrict__ B0 = packed_w1 + expert_id * stride_e + nb0 * BLOCK_N * stride_n;\n+      const int8_t* __restrict__ B1 = packed_w1 + expert_id * stride_e + nb1 * BLOCK_N * stride_n;\n+      const float* __restrict__ Bs0 = w1s + expert_id * 2 * N + nb0 * BLOCK_N;\n+      const float* __restrict__ Bs1 = w1s + expert_id * 2 * N + nb1 * BLOCK_N;\n+\n+      // 1.a load A\n+      const int32_t* A_ids = sorted_ids + mb * BLOCK_M;\n+      int64_t m_size = offsets[mb + 1] - offsets[mb];\n+\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        int32_t index = A_ids[m] / topk;\n+        copy_stub(A + m * K, Aq_tmp + index * K, K);\n+        As[m] = As_tmp[index];\n+      }\n+\n+      // fused 1.b: silu_and_mul(A @ B0, A @ B1)\n+      const int64_t offset = offsets[mb];\n+      tinygemm_kernel(\n+          /* A     */ A,\n+          /* B0    */ B0,\n+          /* B1    */ B1,\n+          /* C     */ ic1 + offset * N + nb * BLOCK_N,\n+          /* As    */ As,\n+          /* Bs0   */ Bs0,\n+          /* Bs1   */ Bs1,\n+          /* M     */ m_size,\n+          /* N     */ n_size,\n+          /* K     */ K,\n+          /* lda   */ K,\n+          /* ldb   */ n_size,\n+          /* ldc   */ N);\n+    }\n+  });\n+\n+  // stage 1.5: quantize ic1 to uint8, [M * topk, N]\n+  at::parallel_for(0, M * topk, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      quantize_row_int8<scalar_t>(Aq_tmp + m * N, As_tmp[m], ic1 + m * N, N);\n+    }\n+  });\n+\n+  // stage 2: intermediate_cache2 = intermediate_cache1 @ w2\n+  //   w2 : [E, K, N] as [E, OC, IC]\n+  const int64_t OC = K;  // rename K as OC\n+  const int64_t IC = N;  // rename N as IC\n+  const int64_t MB2 = MB;\n+  const int64_t NB2 = div_up(OC, BLOCK_N);\n+  const int64_t stride_e2 = OC * packed_N;\n+  const int64_t stride_oc = packed_N;\n+\n+  // parallel on [MB2, NB2]\n+  at::parallel_for(0, MB2 * NB2, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    // we won't be using C1 for gemm2\n+    float* __restrict__ C = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB2;\n+      int64_t nb = i % NB2;\n+\n+      int64_t m_size = offsets[mb + 1] - offsets[mb];\n+      int64_t n_size = std::min(OC - nb * BLOCK_N, BLOCK_N);\n+\n+      // A ptr from ic1 of [M * topk, N] in sorted order\n+      // so as to avoid copy A to tmp buffer again\n+      const uint8_t* __restrict__ A = Aq_tmp + offsets[mb] * N;\n+      const float* __restrict__ As = As_tmp + offsets[mb];\n+      const int32_t* A_ids = sorted_ids + mb * BLOCK_M;\n+\n+      // B shape [IC, n_size] in vnni format\n+      int32_t expert_id = expert_ids[mb];\n+      const int8_t* __restrict__ B = packed_w2 + expert_id * stride_e2 + nb * BLOCK_N * stride_oc;\n+      const float* __restrict__ Bs = w2s + expert_id * K + nb * BLOCK_N;\n+\n+      // 2.a gemm: C = A @ B\n+      tinygemm_kernel<scalar_t>(\n+          /* A     */ A,\n+          /* B     */ B,\n+          /* C     */ C,\n+          /* As    */ As,\n+          /* Bs    */ Bs,\n+          /* M     */ m_size,\n+          /* N     */ n_size,\n+          /* K     */ IC,\n+          /* lda   */ IC,\n+          /* ldb   */ n_size,\n+          /* ldc   */ BLOCK_N);\n+\n+      // 2.b copy from C to ic2 in original order\n+      //   and also mul topk_weights in float32\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        int32_t index = A_ids[m];\n+        float weight = topk_weights[index];\n+        copy_mul_stub(ic2 + index * K + nb * BLOCK_N, C + m * BLOCK_N, weight, n_size);\n+      }\n+    }\n+  });\n+\n+  // stage 3: out = intermediate_cache2.sum(dim=1)\n+  //   from [M, topk, K] to [M, K]\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      sum_stub(output + m * K, ic2 + m * topk * K, topk, K);\n+    }\n+  });\n+}\n+\n+#define INSTANTIATE_MOE_INT8_TEMPLATE(TYPE)           \\\n+  template void fused_experts_int8_kernel_impl<TYPE>( \\\n+      TYPE* __restrict__ output,                      \\\n+      TYPE* __restrict__ ic1,                         \\\n+      TYPE* __restrict__ ic2,                         \\\n+      uint8_t* __restrict__ A_tmp,                    \\\n+      float* __restrict__ C_tmp,                      \\\n+      uint8_t* __restrict__ Aq_tmp,                   \\\n+      float* __restrict__ As_tmp,                     \\\n+      const TYPE* __restrict__ input,                 \\\n+      const int8_t* __restrict__ packed_w1,           \\\n+      const int8_t* __restrict__ packed_w2,           \\\n+      const float* __restrict__ w1s,                  \\\n+      const float* __restrict__ w2s,                  \\\n+      const float* __restrict__ topk_weights,         \\\n+      const int32_t* __restrict__ sorted_ids,         \\\n+      const int32_t* __restrict__ expert_ids,         \\\n+      const int32_t* __restrict__ offsets,            \\\n+      int64_t M,                                      \\\n+      int64_t N,                                      \\\n+      int64_t K,                                      \\\n+      int64_t E,                                      \\\n+      int64_t topk,                                   \\\n+      int64_t num_tokens_post_pad)\n+\n+INSTANTIATE_MOE_INT8_TEMPLATE(at::BFloat16);\n+INSTANTIATE_MOE_INT8_TEMPLATE(at::Half);\n+\n+template <typename scalar_t>\n+void shared_expert_int8_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    scalar_t* __restrict__ ic1,\n+    float* __restrict__ C_tmp,\n+    uint8_t* __restrict__ Aq_tmp,\n+    float* __restrict__ As_tmp,\n+    const scalar_t* __restrict__ input,\n+    const int8_t* __restrict__ packed_w1,\n+    const int8_t* __restrict__ packed_w2,\n+    const float* __restrict__ w1s,\n+    const float* __restrict__ w2s,\n+    const scalar_t* __restrict__ fused_experts_out,\n+    float routed_scaling_factor,\n+    int64_t M,\n+    int64_t N,\n+    int64_t K) {\n+  // handle 2 tiles per block\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+\n+  // stage 0: quantize input to uint8, [M, K]\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      quantize_row_int8<scalar_t>(Aq_tmp + m * K, As_tmp[m], input + m * K, K);\n+    }\n+  });\n+\n+  // stage 1: intermediate_cache1 = silu(hidden_states @ w1)\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB = div_up(N, BLOCK_N);\n+\n+  TORCH_CHECK(N % BLOCK_N == 0, \"Fixme when N is not multiples of \", BLOCK_N);\n+\n+  // K and N are packed for int8\n+  const int64_t packed_K = get_row_size<int8_t>(K);\n+  const int64_t packed_N = get_row_size<int8_t>(N);\n+  const int64_t stride_n = packed_K;\n+\n+  // here we only parallel on half of 2N to fuse silu_and_mul with gemm\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB;\n+      int64_t nb = i % NB;\n+\n+      // nb0 from top half and nb1 from bottom half\n+      int64_t nb0 = nb, nb1 = nb + NB;\n+      int64_t n_size = std::min(N - nb0 * BLOCK_N, BLOCK_N);\n+      int64_t m_size = std::min(M - mb * BLOCK_M, BLOCK_M);\n+\n+      // A shape [m_size, K]\n+      const uint8_t* A = Aq_tmp + mb * BLOCK_M * K;\n+      const float* As = As_tmp + mb * BLOCK_M;\n+\n+      // B shape [K, n_size] in vnni format\n+      const int8_t* __restrict__ B0 = packed_w1 + nb0 * BLOCK_N * stride_n;\n+      const int8_t* __restrict__ B1 = packed_w1 + nb1 * BLOCK_N * stride_n;\n+      const float* __restrict__ Bs0 = w1s + nb0 * BLOCK_N;\n+      const float* __restrict__ Bs1 = w1s + nb1 * BLOCK_N;\n+\n+      // fused 1.b: silu_and_mul(A @ B0, A @ B1)\n+      tinygemm_kernel(\n+          /* A     */ A,\n+          /* B0    */ B0,\n+          /* B1    */ B1,\n+          /* C     */ ic1 + mb * BLOCK_M * N + nb * BLOCK_N,\n+          /* As    */ As,\n+          /* Bs0   */ Bs0,\n+          /* Bs1   */ Bs1,\n+          /* M     */ m_size,\n+          /* N     */ n_size,\n+          /* K     */ K,\n+          /* lda   */ K,\n+          /* ldb   */ n_size,\n+          /* ldc   */ N);\n+    }\n+  });\n+\n+  // stage 1.5: quantize ic1 to uint8, [M * topk, N]\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      quantize_row_int8<scalar_t>(Aq_tmp + m * N, As_tmp[m], ic1 + m * N, N);\n+    }\n+  });\n+\n+  // stage 2: intermediate_cache2 = intermediate_cache1 @ w2\n+  //   w2 : [K, N] as [OC, IC]\n+  const int64_t OC = K;  // rename K as OC\n+  const int64_t IC = N;  // rename N as IC\n+  const int64_t MB2 = MB;\n+  const int64_t NB2 = div_up(OC, BLOCK_N);\n+  const int64_t stride_oc = packed_N;\n+\n+  // parallel on [MB2, NB2]\n+  at::parallel_for(0, MB2 * NB2, 0, [&](int64_t begin, int64_t end) {\n+    // get local pointers\n+    int tid = at::get_thread_num();\n+    // we won't be using C1 for gemm2\n+    float* __restrict__ C = C_tmp + tid * 2 * BLOCK_M * BLOCK_N;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t mb = i / NB2;\n+      int64_t nb = i % NB2;\n+\n+      int64_t m_size = std::min(M - mb * BLOCK_M, BLOCK_M);\n+      int64_t n_size = std::min(OC - nb * BLOCK_N, BLOCK_N);\n+\n+      // A shape [m_size, IC]\n+      const uint8_t* __restrict__ A = Aq_tmp + mb * BLOCK_M * N;\n+      const float* __restrict__ As = As_tmp + mb * BLOCK_M;\n+\n+      // B shape [IC, n_size] in vnni format\n+      const int8_t* __restrict__ B = packed_w2 + nb * BLOCK_N * stride_oc;\n+      const float* __restrict__ Bs = w2s + nb * BLOCK_N;\n+\n+      // 2.a gemm: C = A @ B\n+      tinygemm_kernel<scalar_t>(\n+          /* A     */ A,\n+          /* B     */ B,\n+          /* C     */ C,\n+          /* As    */ As,\n+          /* Bs    */ Bs,\n+          /* M     */ m_size,\n+          /* N     */ n_size,\n+          /* K     */ IC,\n+          /* lda   */ IC,\n+          /* ldb   */ n_size,\n+          /* ldc   */ BLOCK_N);\n+\n+      // 2.b copy from C to output and add fused_experts_out\n+      scalar_t* __restrict__ out = output + mb * BLOCK_M * K + nb * BLOCK_N;\n+      const scalar_t* __restrict__ fused_out = fused_experts_out + mb * BLOCK_M * K + nb * BLOCK_N;\n+      for (int64_t m = 0; m < m_size; ++m) {\n+        add_mul_stub(out + m * K, C + m * BLOCK_N, fused_out + m * K, routed_scaling_factor, n_size);\n+      }\n+    }\n+  });\n+}\n+\n+#define INSTANTIATE_SHARED_EXPERT_INT8_TEMPLATE(TYPE) \\\n+  template void shared_expert_int8_kernel_impl<TYPE>( \\\n+      TYPE* __restrict__ output,                      \\\n+      TYPE* __restrict__ ic1,                         \\\n+      float* __restrict__ C_tmp,                      \\\n+      uint8_t* __restrict__ Aq_tmp,                   \\\n+      float* __restrict__ As_tmp,                     \\\n+      const TYPE* __restrict__ input,                 \\\n+      const int8_t* __restrict__ packed_w1,           \\\n+      const int8_t* __restrict__ packed_w2,           \\\n+      const float* __restrict__ w1s,                  \\\n+      const float* __restrict__ w2s,                  \\\n+      const TYPE* __restrict__ fused_experts_out,     \\\n+      float routed_scaling_factor,                    \\\n+      int64_t M,                                      \\\n+      int64_t N,                                      \\\n+      int64_t K)\n+\n+INSTANTIATE_SHARED_EXPERT_INT8_TEMPLATE(at::BFloat16);\n+INSTANTIATE_SHARED_EXPERT_INT8_TEMPLATE(at::Half);\ndiff --git a/sgl-kernel/csrc/cpu/norm.cpp b/sgl-kernel/csrc/cpu/norm.cpp\nnew file mode 100644\nindex 000000000..391a0d4e5\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/norm.cpp\n@@ -0,0 +1,221 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// NB: avoid using `at::vec::map<>` on bfloat16 or half\n+template <typename scalar_t>\n+void rmsnorm_kernel_impl(\n+    scalar_t* __restrict__ output,\n+    const scalar_t* __restrict__ input,\n+    const scalar_t* __restrict__ weight,\n+    int64_t batch_size,\n+    int64_t hidden_size,\n+    float eps = 1e-5) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  constexpr int kVecSize = bVec::size();\n+  at::parallel_for(0, batch_size, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t i = begin; i < end; ++i) {\n+      // local ptrs\n+      scalar_t* __restrict__ out_ptr = output + i * hidden_size;\n+      const scalar_t* __restrict__ input_ptr = input + i * hidden_size;\n+\n+      fVec sum_fvec = fVec(float(0));\n+      float sum_val = float(0);\n+\n+      int64_t d;\n+#pragma GCC unroll 4\n+      for (d = 0; d <= hidden_size - kVecSize; d += kVecSize) {\n+        bVec x_bvec = bVec::loadu(input_ptr + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        sum_fvec += x_fvec0 * x_fvec0;\n+        sum_fvec += x_fvec1 * x_fvec1;\n+      }\n+#pragma GCC unroll 4\n+      for (; d < hidden_size; ++d) {\n+        float x_val = static_cast<float>(input_ptr[d]);\n+        sum_val += x_val * x_val;\n+      }\n+\n+      sum_val += vec_reduce_sum(sum_fvec);\n+      float rsqrt_var = float(1) / std::sqrt(sum_val / hidden_size + eps);\n+      const fVec scale_fvec = fVec(rsqrt_var);\n+\n+#pragma GCC unroll 4\n+      for (d = 0; d <= hidden_size - kVecSize; d += kVecSize) {\n+        bVec x_bvec = bVec::loadu(input_ptr + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        bVec w_bvec = bVec::loadu(weight + d);\n+        fVec w_fvec0, w_fvec1;\n+        std::tie(w_fvec0, w_fvec1) = at::vec::convert_to_float(w_bvec);\n+\n+        x_fvec0 = x_fvec0 * scale_fvec * w_fvec0;\n+        x_fvec1 = x_fvec1 * scale_fvec * w_fvec1;\n+\n+        bVec out_bvec = convert_from_float_ext<scalar_t>(x_fvec0, x_fvec1);\n+        out_bvec.store(out_ptr + d);\n+      }\n+#pragma GCC unroll 4\n+      for (; d < hidden_size; ++d) {\n+        float x_val = static_cast<float>(input_ptr[d]);\n+        float w_val = static_cast<float>(weight[d]);\n+        out_ptr[d] = static_cast<scalar_t>(x_val * rsqrt_var * w_val);\n+      }\n+    }\n+  });\n+}\n+\n+template <typename scalar_t>\n+void fused_add_rmsnorm_kernel_impl(\n+    scalar_t* __restrict__ input,\n+    scalar_t* __restrict__ residual,\n+    const scalar_t* __restrict__ weight,\n+    float* __restrict__ buffer,\n+    int64_t batch_size,\n+    int64_t hidden_size,\n+    float eps = 1e-5) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  constexpr int kVecSize = bVec::size();\n+  at::parallel_for(0, batch_size, 0, [&](int64_t begin, int64_t end) {\n+    int tid = at::get_thread_num();\n+    float* __restrict__ buffer_ptr = buffer + tid * hidden_size;\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      // local ptrs\n+      scalar_t* __restrict__ input_ptr = input + i * hidden_size;\n+      scalar_t* __restrict__ residual_ptr = residual + i * hidden_size;\n+\n+      fVec sum_fvec = fVec(float(0));\n+      float sum_val = float(0);\n+\n+      int64_t d;\n+#pragma GCC unroll 4\n+      for (d = 0; d <= hidden_size - kVecSize; d += kVecSize) {\n+        bVec x_bvec = bVec::loadu(input_ptr + d);\n+        fVec x_fvec0, x_fvec1;\n+        std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+        bVec r_bvec = bVec::loadu(residual_ptr + d);\n+        fVec r_fvec0, r_fvec1;\n+        std::tie(r_fvec0, r_fvec1) = at::vec::convert_to_float(r_bvec);\n+\n+        x_fvec0 += r_fvec0;\n+        x_fvec1 += r_fvec1;\n+\n+        bVec out_bvec = convert_from_float_ext<scalar_t>(x_fvec0, x_fvec1);\n+        out_bvec.store(residual_ptr + d);\n+\n+        sum_fvec += x_fvec0 * x_fvec0;\n+        sum_fvec += x_fvec1 * x_fvec1;\n+\n+        x_fvec0.store(buffer_ptr + d);\n+        x_fvec1.store(buffer_ptr + d + fVec::size());\n+      }\n+#pragma GCC unroll 4\n+      for (; d < hidden_size; ++d) {\n+        float x_val = static_cast<float>(input_ptr[d]);\n+        float r_val = static_cast<float>(residual_ptr[d]);\n+\n+        x_val += r_val;\n+        residual_ptr[d] = static_cast<scalar_t>(x_val);\n+\n+        sum_val += x_val * x_val;\n+        buffer_ptr[d] = x_val;\n+      }\n+\n+      sum_val += vec_reduce_sum(sum_fvec);\n+      float rsqrt_var = float(1) / std::sqrt(sum_val / hidden_size + eps);\n+      const fVec scale_fvec = fVec(rsqrt_var);\n+\n+#pragma GCC unroll 4\n+      for (d = 0; d <= hidden_size - kVecSize; d += kVecSize) {\n+        fVec x_fvec0 = fVec::loadu(buffer_ptr + d);\n+        fVec x_fvec1 = fVec::loadu(buffer_ptr + d + fVec::size());\n+\n+        bVec w_bvec = bVec::loadu(weight + d);\n+        fVec w_fvec0, w_fvec1;\n+        std::tie(w_fvec0, w_fvec1) = at::vec::convert_to_float(w_bvec);\n+\n+        x_fvec0 = x_fvec0 * scale_fvec * w_fvec0;\n+        x_fvec1 = x_fvec1 * scale_fvec * w_fvec1;\n+        bVec x_bvec = convert_from_float_ext<scalar_t>(x_fvec0, x_fvec1);\n+        x_bvec.store(input_ptr + d);\n+      }\n+#pragma GCC unroll 4\n+      for (; d < hidden_size; ++d) {\n+        float x_val = buffer_ptr[d] * rsqrt_var * static_cast<float>(weight[d]);\n+        input_ptr[d] = x_val;\n+      }\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+// input : {batch_size, hidden_size}\n+// weight: {hidden_size}\n+at::Tensor rmsnorm_cpu(at::Tensor& input, at::Tensor& weight, double eps) {\n+  RECORD_FUNCTION(\"sgl-kernel::rmsnorm_cpu\", std::vector<c10::IValue>({input, weight}));\n+\n+  CHECK_INPUT(input);\n+  CHECK_INPUT(weight);\n+  CHECK_DIM(2, input);\n+  CHECK_DIM(1, weight);\n+  CHECK_EQ(input.size(1), weight.size(0));\n+  int64_t batch_size = input.size(0);\n+  int64_t hidden_size = input.size(1);\n+  at::Tensor output = at::empty_like(input);\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(input.scalar_type(), \"rmsnorm_kernel\", [&] {\n+    rmsnorm_kernel_impl<scalar_t>(\n+        output.data_ptr<scalar_t>(),\n+        input.data_ptr<scalar_t>(),\n+        weight.data_ptr<scalar_t>(),\n+        batch_size,\n+        hidden_size,\n+        eps);\n+  });\n+  return output;\n+}\n+\n+// input   : {batch_size, hidden_size}\n+// residual: {batch_size, hidden_size}\n+// weight  : {hidden_size}\n+void fused_add_rmsnorm_cpu(at::Tensor& input, at::Tensor& residual, at::Tensor& weight, double eps) {\n+  RECORD_FUNCTION(\"sgl-kernel::fused_add_rmsnorm_cpu\", std::vector<c10::IValue>({input, residual, weight}));\n+  CHECK_INPUT(input);\n+  CHECK_INPUT(residual);\n+  CHECK_INPUT(weight);\n+  CHECK_DIM(2, input);\n+  CHECK_DIM(2, residual);\n+  CHECK_DIM(1, weight);\n+  CHECK_EQ(input.size(0), residual.size(0));\n+  CHECK_EQ(input.size(1), residual.size(1));\n+  CHECK_EQ(input.size(1), weight.size(0));\n+  int64_t batch_size = input.size(0);\n+  int64_t hidden_size = input.size(1);\n+\n+  // allocate temp buffer to store x in float32 per thread\n+  // TODO: implement a singleton for context\n+  int64_t num_threads = at::get_num_threads();\n+  at::Tensor buffer = at::empty({num_threads, hidden_size}, input.options().dtype(at::kFloat));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(input.scalar_type(), \"fused_add_rmsnorm_kernel\", [&] {\n+    fused_add_rmsnorm_kernel_impl<scalar_t>(\n+        input.data_ptr<scalar_t>(),\n+        residual.data_ptr<scalar_t>(),\n+        weight.data_ptr<scalar_t>(),\n+        buffer.data_ptr<float>(),\n+        batch_size,\n+        hidden_size,\n+        eps);\n+  });\n+}\ndiff --git a/sgl-kernel/csrc/cpu/qkv_proj.cpp b/sgl-kernel/csrc/cpu/qkv_proj.cpp\nnew file mode 100644\nindex 000000000..959072878\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/qkv_proj.cpp\n@@ -0,0 +1,504 @@\n+#include \"common.h\"\n+#include \"gemm.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+// [NOTE]: Fused kernel for QKV projection with weight absorption and RoPE\n+//\n+//   1. `q_a_proj` and `kv_a_proj_with_mqa` fused into one gemm,\n+//      otherwise we need to split IC for the 2nd gemm.\n+//   2. `q_a_layernorm` and `kv_a_layernorm` fused into one parallel loop.\n+//   3. k_input and v_input share the same storage, the torch API did\n+//      this in `set_kv_buffer`. No additional memory movement.\n+//\n+\n+// [C0, C1] = A @ [B0, B1]\n+template <typename scalar_t>\n+void segment_gemm_kernel_impl(\n+    scalar_t* __restrict__ C0,\n+    scalar_t* __restrict__ C1,\n+    const scalar_t* __restrict__ A,\n+    const scalar_t* __restrict__ B0,\n+    const scalar_t* __restrict__ B1,\n+    int64_t M,\n+    int64_t N0,\n+    int64_t N1,\n+    int64_t K) {\n+  // convert_weight_packed make sure N0 and N1 are 32x\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB0 = div_up(N0, BLOCK_N);\n+  const int64_t NB1 = div_up(N1, BLOCK_N);\n+  const int64_t NB = NB0 + NB1;\n+\n+  const bool use_brgemm = can_use_brgemm<scalar_t>(M);\n+\n+  // parallel on [MB, NB0 + NB1]\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    int64_t mb{0}, nb{0};\n+    data_index_init(begin, mb, MB, nb, NB);\n+\n+    // for brgemm, use float32 for accumulate\n+    alignas(64) float Ctmp[BLOCK_M * BLOCK_N];\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      UNUSED(i);\n+      int mb_start = mb * BLOCK_M;\n+      int mb_size = std::min(M - mb_start, BLOCK_M);\n+      int nb_start = nb * BLOCK_N;\n+      int nb_size = BLOCK_N;\n+\n+      const scalar_t* __restrict__ B = nb < NB0 ? B0 : B1;\n+      scalar_t* __restrict__ C = nb < NB0 ? C0 : C1;\n+      int64_t ldc = nb < NB0 ? N0 : N1;\n+      int64_t local_nb_start = nb < NB0 ? nb_start : nb_start - N0;\n+\n+      tinygemm_kernel<scalar_t>(\n+          /*   A */ A + mb_start * K,\n+          /*   B */ B + local_nb_start * K /* nb * BLOCK_N * K */,\n+          /*   C */ C + mb_start * ldc + local_nb_start,\n+          /* Ctmp*/ Ctmp,\n+          /*   M */ mb_size,\n+          /*   N */ nb_size,\n+          /*   K */ K,\n+          /* lda */ K,\n+          /* ldb */ nb_size,\n+          /* ldc */ ldc,\n+          /* brg */ use_brgemm);\n+\n+      // move to the next index\n+      data_index_step(mb, MB, nb, NB);\n+    }\n+\n+    if (use_brgemm) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+}\n+\n+// [C0, C1] = A @ [B0, B1]\n+template <typename scalar_t>\n+void segment_gemm_kernel_impl(\n+    scalar_t* __restrict__ C0,\n+    scalar_t* __restrict__ C1,\n+    const uint8_t* __restrict__ A,\n+    const int8_t* __restrict__ B0,\n+    const int8_t* __restrict__ B1,\n+    const float* __restrict__ As,\n+    const float* __restrict__ Bs0,\n+    const float* __restrict__ Bs1,\n+    int64_t M,\n+    int64_t N0,\n+    int64_t N1,\n+    int64_t K) {\n+  constexpr int64_t BLOCK_M = block_size_m();\n+  constexpr int64_t BLOCK_N = block_size_n();\n+  const int64_t MB = div_up(M, BLOCK_M);\n+  const int64_t NB0 = div_up(N0, BLOCK_N);\n+  const int64_t NB1 = div_up(N1, BLOCK_N);\n+  const int64_t NB = NB0 + NB1;\n+\n+  // TODO: brgemm u8s8 depends on PyTorch 2.7 release.\n+  const bool use_brgemm = false;\n+\n+  // K + 4 after compensation\n+  const int64_t packed_row_size = get_row_size<int8_t>(K);\n+\n+  // parallel on [MB, NB0 + NB1]\n+  at::parallel_for(0, MB * NB, 0, [&](int64_t begin, int64_t end) {\n+    int64_t mb{0}, nb{0};\n+    data_index_init(begin, mb, MB, nb, NB);\n+\n+    // for brgemm, use float32 for accumulate\n+    alignas(64) int32_t Ctmp[BLOCK_M * BLOCK_N];\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      UNUSED(i);\n+      int mb_start = mb * BLOCK_M;\n+      int mb_size = std::min(M - mb_start, BLOCK_M);\n+      int nb_start = nb * BLOCK_N;\n+      int nb_size = BLOCK_N;\n+\n+      const int8_t* __restrict__ B = nb < NB0 ? B0 : B1;\n+      const float* __restrict__ Bs = nb < NB0 ? Bs0 : Bs1;\n+      scalar_t* __restrict__ C = nb < NB0 ? C0 : C1;\n+      int64_t ldc = nb < NB0 ? N0 : N1;\n+      int64_t local_nb_start = nb < NB0 ? nb_start : nb_start - N0;\n+\n+      tinygemm_kernel<scalar_t>(\n+          /*   A */ A + mb_start * K,\n+          /*   B */ B + local_nb_start * packed_row_size /* nb * BLOCK_N * (K + 4) */,\n+          /*   C */ C + mb_start * ldc + local_nb_start,\n+          /* Ctmp*/ Ctmp,\n+          /*  As */ As + mb_start,\n+          /*  Bs */ Bs + local_nb_start,\n+          /*   M */ mb_size,\n+          /*   N */ nb_size,\n+          /*   K */ K,\n+          /* lda */ K,\n+          /* ldb */ nb_size,\n+          /* ldc */ ldc,\n+          /* brg */ use_brgemm);\n+\n+      // move to the next index\n+      data_index_step(mb, MB, nb, NB);\n+    }\n+\n+    if (use_brgemm) {\n+      at::native::cpublas::brgemm_release();\n+    }\n+  });\n+}\n+\n+template <typename scalar_t>\n+inline float reduce(const scalar_t* __restrict__ x, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  fVec sum_fvec = fVec(float(0));\n+\n+// no remainder\n+#pragma GCC unroll 4\n+  for (int64_t d = 0; d < size; d += bVec::size()) {\n+    bVec x_bvec = bVec::loadu(x + d);\n+    fVec x_fvec0, x_fvec1;\n+    std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+    sum_fvec += x_fvec0 * x_fvec0;\n+    sum_fvec += x_fvec1 * x_fvec1;\n+  }\n+  return vec_reduce_sum(sum_fvec);\n+}\n+\n+// map2 from aten functional doesn't have fast bf16->fp32 conversion\n+template <typename scalar_t>\n+inline void map2(scalar_t* y, const scalar_t* x, const scalar_t* __restrict__ w, float scale, int64_t size) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  fVec scale_fvec = fVec(scale);\n+\n+// no remainder\n+#pragma GCC unroll 4\n+  for (int64_t d = 0; d < size; d += bVec::size()) {\n+    bVec x_bvec = bVec::loadu(x + d);\n+    fVec x_fvec0, x_fvec1;\n+    std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+    bVec w_bvec = bVec::loadu(w + d);\n+    fVec w_fvec0, w_fvec1;\n+    std::tie(w_fvec0, w_fvec1) = at::vec::convert_to_float(w_bvec);\n+    x_fvec0 = x_fvec0 * scale_fvec * w_fvec0;\n+    x_fvec1 = x_fvec1 * scale_fvec * w_fvec1;\n+    bVec out_bvec = convert_from_float_ext<scalar_t>(x_fvec0, x_fvec1);\n+    out_bvec.store(y + d);\n+  }\n+}\n+\n+template <typename scalar_t>\n+void rms_norm_kernel_impl(\n+    scalar_t* __restrict__ input0,\n+    scalar_t* __restrict__ input1,\n+    const scalar_t* __restrict__ weight0,\n+    const scalar_t* __restrict__ weight1,\n+    int64_t M,\n+    int64_t N0,\n+    int64_t N1,\n+    int64_t stride1,\n+    float eps = 1e-5) {\n+  at::parallel_for(0, M, 0, [&](int64_t begin, int64_t end) {\n+    for (int64_t m = begin; m < end; ++m) {\n+      scalar_t* x0 = input0 + m * N0;\n+      scalar_t* x1 = input1 + m * stride1;\n+      float scale0 = reduce(x0, N0);\n+      float scale1 = reduce(x1, N1);\n+      scale0 = float(1) / std::sqrt(scale0 / N0 + eps);\n+      scale1 = float(1) / std::sqrt(scale1 / N1 + eps);\n+      map2(x0, x0, weight0, scale0, N0);\n+      map2(x1, x1, weight1, scale1, N1);\n+    }\n+  });\n+}\n+\n+template <typename scalar_t>\n+inline void rotary(const scalar_t* input, scalar_t* out, const scalar_t* cos, const scalar_t* sin, int64_t size) {\n+  TORCH_CHECK(false, \"rotary scalar path not implemented.\");\n+}\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <>\n+inline void rotary<at::BFloat16>(\n+    const at::BFloat16* input, at::BFloat16* out, const at::BFloat16* cos, const at::BFloat16* sin, int64_t size) {\n+  // permute indices\n+  const __m512i idx1 = _mm512_set_epi32(30, 28, 26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2, 0);\n+  const __m512i idx2 = _mm512_set_epi32(31, 29, 27, 25, 23, 21, 19, 17, 15, 13, 11, 9, 7, 5, 3, 1);\n+  const __m512i idy1 = _mm512_set_epi32(23, 7, 22, 6, 21, 5, 20, 4, 19, 3, 18, 2, 17, 1, 16, 0);\n+  const __m512i idy2 = _mm512_set_epi32(31, 15, 30, 14, 29, 13, 28, 12, 27, 11, 26, 10, 25, 9, 24, 8);\n+\n+// rotary dim is 64, just 2 iters\n+#pragma GCC unroll 2\n+  for (int64_t d = 0; d < size; d += 32) {\n+    int64_t d2 = d >> 1;\n+    // load coefs\n+    __m512 vcos = CVT_BF16_TO_FP32(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(cos + d2)));\n+    __m512 vsin = CVT_BF16_TO_FP32(_mm256_loadu_si256(reinterpret_cast<const __m256i*>(sin + d2)));\n+    // load input\n+    __m512i a16 = _mm512_loadu_si512(reinterpret_cast<const __m512i*>(input + d));\n+    __m512 a = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(a16, 0));\n+    __m512 b = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(a16, 1));\n+    // from [16, 2] to [2, 16]\n+    __m512 in1 = _mm512_mask_permutex2var_ps(a, 0xffff, idx1, b);\n+    __m512 in2 = _mm512_mask_permutex2var_ps(a, 0xffff, idx2, b);\n+    // out1 = in1 * cos - in2 * sin;\n+    // out2 = in2 * cos + in1 * sin\n+    __m512 out1 = _mm512_sub_ps(_mm512_mul_ps(in1, vcos), _mm512_mul_ps(in2, vsin));\n+    __m512 out2 = _mm512_add_ps(_mm512_mul_ps(in2, vcos), _mm512_mul_ps(in1, vsin));\n+    // from [2, 16] to [16, 2]\n+    a = _mm512_mask_permutex2var_ps(out1, 0xffff, idy1, out2);\n+    b = _mm512_mask_permutex2var_ps(out1, 0xffff, idy2, out2);\n+\n+    _mm512_storeu_si512(reinterpret_cast<__m512i*>((out + d)), (__m512i)(_mm512_cvtne2ps_pbh(b, a)));\n+  }\n+}\n+#endif\n+\n+template <typename scalar_t>\n+void rotary_emb_kernel_impl(\n+    scalar_t* q_pe_out,\n+    scalar_t* k_pe_out,\n+    const scalar_t* q_pe,\n+    const scalar_t* k_pe,\n+    const int64_t* pos,\n+    const scalar_t* cos_sin,\n+    int64_t num_seqs,\n+    int64_t num_heads,\n+    int64_t rotary_dim,\n+    int64_t q_strideB,\n+    int64_t q_strideH,\n+    int64_t k_strideB,\n+    int64_t oq_strideB,\n+    int64_t oq_strideH,\n+    int64_t ok_strideB) {\n+  TORCH_CHECK(rotary_dim % 32 == 0, \"rotary_dim is not 32x.\");\n+  const int64_t rotary_offset = rotary_dim / 2;\n+\n+  // parallel on [num_seqs, num_heads + 1]\n+  // top [num_heads] handle q_pe and bottom [1] handle k_pe\n+  at::parallel_for(0, num_seqs * (num_heads + 1), GRAIN_SIZE / rotary_dim, [&](int64_t begin, int64_t end) {\n+    int64_t seq{0}, head_id{0};\n+    data_index_init(begin, seq, num_seqs, head_id, num_heads + 1);\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      UNUSED(i);\n+      // get cos and sin cache ptr\n+      int64_t index = pos[seq];\n+      const scalar_t* cos = cos_sin + index * rotary_dim;\n+      const scalar_t* sin = cos + rotary_offset;\n+\n+      const scalar_t* input =\n+          (head_id < num_heads) ? q_pe + seq * q_strideB + head_id * q_strideH : k_pe + seq * k_strideB;\n+      scalar_t* out =\n+          (head_id < num_heads) ? q_pe_out + seq * oq_strideB + head_id * oq_strideH : k_pe_out + seq * ok_strideB;\n+      rotary<scalar_t>(input, out, cos, sin, rotary_dim);\n+\n+      // move to the next index\n+      data_index_step(seq, num_seqs, head_id, num_heads + 1);\n+    }\n+  });\n+}\n+\n+}  // anonymous namespace\n+\n+extern at::Tensor\n+weight_packed_linear(at::Tensor& mat1, at::Tensor& mat2, std::optional<at::Tensor>& bias, bool is_vnni);\n+\n+extern at::Tensor int8_scaled_mm_with_quant(\n+    at::Tensor& mat1,\n+    at::Tensor& mat2,\n+    at::Tensor& scales2,\n+    std::optional<at::Tensor>& bias,\n+    at::ScalarType out_dtype,\n+    bool is_vnni);\n+\n+extern void\n+bmm_cpu(at::Tensor& out, at::Tensor& mat1, at::Tensor& mat2, bool is_vnni, std::optional<at::Tensor>& scale);\n+\n+// NB: shapes in DeepDeek R1\n+//\n+//   hidden_states    : [num_seqs, hidden_size] [1, 7168]\n+//   q_a_proj_weight  : [q_lora_rank, hidden_size] [1536, 7168]\n+//   q_b_proj_weight  : [num_heads * qk_head_dim, q_lora_rank] [4224, 1536]\n+//   kv_a_proj_weight : [kv_lora_rank + qk_rope_head_dim, hidden_size] [576, 7168]\n+//   w_kc             : [num_heads, kv_lora_rank, qk_nope_head_dim] [22, 512, 128]\n+//   q_a_layernorm_weight  : [q_lora_rank] [1536]\n+//   kv_a_layernorm_weight : [kv_lora_rank] [512]\n+//\n+std::tuple<at::Tensor, at::Tensor, at::Tensor> qkv_proj_with_rope(\n+    at::Tensor& hidden_states,\n+    at::Tensor& q_a_proj_weight,\n+    at::Tensor& q_b_proj_weight,\n+    at::Tensor& kv_a_proj_weight,\n+    at::Tensor& w_kc,\n+    at::Tensor& q_a_layernorm_weight,\n+    at::Tensor& kv_a_layernorm_weight,\n+    at::Tensor& positions,\n+    at::Tensor& cos_sin_cache,\n+    double eps,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& q_a_proj_scale,\n+    std::optional<at::Tensor>& q_b_proj_scale,\n+    std::optional<at::Tensor>& kv_a_proj_scale,\n+    bool is_vnni) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::qkv_proj_with_rope\",\n+      std::vector<c10::IValue>({hidden_states, q_a_proj_weight, q_b_proj_weight, kv_a_proj_weight, w_kc}));\n+\n+  const auto st = hidden_states.scalar_type();\n+  CHECK_INPUT(hidden_states);\n+  CHECK_INPUT(positions);\n+  CHECK_INPUT(cos_sin_cache);\n+  CHECK_EQ(q_a_layernorm_weight.scalar_type(), st);\n+  CHECK_EQ(kv_a_layernorm_weight.scalar_type(), st);\n+  CHECK_EQ(positions.scalar_type(), at::kLong);\n+  CHECK_EQ(cos_sin_cache.scalar_type(), st);\n+  CHECK_DIM(2, hidden_states);\n+  CHECK_DIM(3, w_kc);\n+  CHECK_DIM(1, q_a_layernorm_weight);\n+  CHECK_DIM(1, kv_a_layernorm_weight);\n+  CHECK_DIM(1, positions);\n+  CHECK_DIM(2, cos_sin_cache);\n+\n+  // skip contiguous checks for weights, expect prepacked\n+  TORCH_CHECK(is_vnni, \"qkv_proj_with_rope: expect weights are prepacked!\");\n+\n+  int64_t num_seqs = hidden_states.size(0);\n+  int64_t hidden_size = hidden_states.size(1);\n+  int64_t q_lora_rank = q_a_proj_weight.size(0);\n+  int64_t num_heads = w_kc.size(0);\n+  int64_t kv_lora_rank = w_kc.size(1);\n+  int64_t qk_head_dim = q_b_proj_weight.size(0) / num_heads;\n+  int64_t qk_nope_head_dim = w_kc.size(2);\n+  int64_t qk_rope_head_dim = kv_a_proj_weight.size(0) - kv_lora_rank;\n+  int64_t rotary_dim = cos_sin_cache.size(1);\n+\n+  CHECK_EQ(positions.numel(), num_seqs);\n+  CHECK_EQ(rotary_dim, qk_rope_head_dim);\n+  CHECK_EQ(q_a_layernorm_weight.numel(), q_lora_rank);\n+  CHECK_EQ(kv_a_layernorm_weight.numel(), kv_lora_rank);\n+\n+  // check the packed dimension\n+  CHECK_EQ(q_a_proj_weight.size(1), get_row_size(hidden_size, use_int8_w8a8));\n+  CHECK_EQ(q_b_proj_weight.size(1), get_row_size(q_lora_rank, use_int8_w8a8));\n+  CHECK_EQ(kv_a_proj_weight.size(1), get_row_size(hidden_size, use_int8_w8a8));\n+\n+  if (use_int8_w8a8) {\n+    TORCH_CHECK(q_a_proj_scale.has_value(), \"missing q_a_proj_scale for int8 w8a8.\");\n+    TORCH_CHECK(q_b_proj_scale.has_value(), \"missing q_b_proj_scale for int8 w8a8.\");\n+    TORCH_CHECK(kv_a_proj_scale.has_value(), \"missing kv_a_proj_scale for int8 w8a8.\");\n+  }\n+\n+  // outputs and temp buffer\n+  const auto options = hidden_states.options();\n+  auto q_input = at::empty({num_seqs, num_heads, kv_lora_rank + qk_rope_head_dim}, options);\n+  auto k_input = at::empty({num_seqs, 1, kv_lora_rank + qk_rope_head_dim}, options);\n+  auto v_input = k_input.narrow(-1, 0, kv_lora_rank);\n+\n+  // outputs of q_a_proj and q_b_proj\n+  auto qa = at::empty({num_seqs, q_lora_rank}, options);\n+\n+  // stage 1: q_a_proj and kv_a_proj\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"qkv_proj_kernel_impl\", [&] {\n+    if (use_int8_w8a8) {\n+      auto q_a_proj_s = q_a_proj_scale.value();\n+      auto kv_a_proj_s = kv_a_proj_scale.value();\n+      TORCH_CHECK(q_a_proj_s.numel() == q_lora_rank);\n+      TORCH_CHECK(kv_a_proj_s.numel() == kv_lora_rank + qk_rope_head_dim);\n+\n+      auto buffer = at::empty({num_seqs * hidden_size + num_seqs * 4}, options.dtype(at::kByte));\n+      uint8_t* __restrict__ Aq_data = buffer.data_ptr<uint8_t>();\n+      float* __restrict__ As_data = (float*)((void*)(Aq_data + num_seqs * hidden_size));\n+      const scalar_t* __restrict__ A_data = hidden_states.data_ptr<scalar_t>();\n+\n+      at::parallel_for(0, num_seqs, 0, [&](int64_t begin, int64_t end) {\n+        for (int64_t m = begin; m < end; ++m) {\n+          quantize_row_int8<scalar_t>(Aq_data + m * hidden_size, As_data[m], A_data + m * hidden_size, hidden_size);\n+        }\n+      });\n+\n+      segment_gemm_kernel_impl<scalar_t>(\n+          qa.data_ptr<scalar_t>(),\n+          k_input.data_ptr<scalar_t>(),\n+          Aq_data,\n+          q_a_proj_weight.data_ptr<int8_t>(),\n+          kv_a_proj_weight.data_ptr<int8_t>(),\n+          As_data,\n+          q_a_proj_s.data_ptr<float>(),\n+          kv_a_proj_s.data_ptr<float>(),\n+          num_seqs,\n+          q_lora_rank,\n+          kv_lora_rank + qk_rope_head_dim,\n+          hidden_size);\n+    } else {\n+      segment_gemm_kernel_impl<scalar_t>(\n+          qa.data_ptr<scalar_t>(),\n+          k_input.data_ptr<scalar_t>(),\n+          hidden_states.data_ptr<scalar_t>(),\n+          q_a_proj_weight.data_ptr<scalar_t>(),\n+          kv_a_proj_weight.data_ptr<scalar_t>(),\n+          num_seqs,\n+          q_lora_rank,\n+          kv_lora_rank + qk_rope_head_dim,\n+          hidden_size);\n+    }\n+  });\n+\n+  // stage 2: apply rmsnorm inplace\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"rms_norm_kernel_impl\", [&] {\n+    rms_norm_kernel_impl<scalar_t>(\n+        qa.data_ptr<scalar_t>(),\n+        v_input.data_ptr<scalar_t>(),\n+        q_a_layernorm_weight.data_ptr<scalar_t>(),\n+        kv_a_layernorm_weight.data_ptr<scalar_t>(),\n+        num_seqs,\n+        q_lora_rank,\n+        kv_lora_rank,\n+        kv_lora_rank + qk_rope_head_dim,\n+        eps);\n+  });\n+\n+  // stage 3: q_b_proj\n+  at::Tensor qb;\n+  std::optional<at::Tensor> bias;\n+  if (use_int8_w8a8) {\n+    qb = int8_scaled_mm_with_quant(qa, q_b_proj_weight, q_b_proj_scale.value(), bias, at::kBFloat16, is_vnni);\n+  } else {\n+    qb = weight_packed_linear(qa, q_b_proj_weight, bias, is_vnni);\n+  }\n+  qb.as_strided_({num_seqs, num_heads, qk_head_dim}, {num_heads * qk_head_dim, qk_head_dim, 1});\n+\n+  // stage 4: bmm\n+  std::optional<at::Tensor> scale;\n+  auto q_nope = qb.narrow(2, 0, qk_nope_head_dim).transpose_(0, 1);\n+  auto q_nope_out = q_input.narrow(2, 0, kv_lora_rank).transpose_(0, 1);\n+  bmm_cpu(q_nope_out, q_nope, w_kc, is_vnni, scale);\n+\n+  // stage 5: rope\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"rotary_emb_kernel_impl\", [&] {\n+    rotary_emb_kernel_impl<scalar_t>(\n+        q_input.data_ptr<scalar_t>() + kv_lora_rank,\n+        k_input.data_ptr<scalar_t>() + kv_lora_rank,\n+        qb.data_ptr<scalar_t>() + qk_nope_head_dim,\n+        k_input.data_ptr<scalar_t>() + kv_lora_rank,\n+        positions.data_ptr<int64_t>(),\n+        cos_sin_cache.data_ptr<scalar_t>(),\n+        num_seqs,\n+        num_heads,\n+        rotary_dim,\n+        num_heads * qk_head_dim,\n+        qk_head_dim,\n+        kv_lora_rank + qk_rope_head_dim,\n+        num_heads * (kv_lora_rank + qk_rope_head_dim),\n+        kv_lora_rank + qk_rope_head_dim,\n+        kv_lora_rank + qk_rope_head_dim);\n+  });\n+\n+  return std::make_tuple(q_input, k_input, v_input);\n+}\ndiff --git a/sgl-kernel/csrc/cpu/rope.cpp b/sgl-kernel/csrc/cpu/rope.cpp\nnew file mode 100644\nindex 000000000..64bc297fe\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/rope.cpp\n@@ -0,0 +1,129 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t>\n+void rope_kernel_impl(\n+    scalar_t* __restrict__ q_pe_out,\n+    scalar_t* __restrict__ k_pe_out,\n+    int64_t* __restrict__ t_pos,\n+    scalar_t* __restrict__ q_pe,\n+    scalar_t* __restrict__ k_pe,\n+    scalar_t* __restrict__ t_emb_pos,\n+    int64_t seq_len,\n+    int64_t num_head,\n+    int64_t rotary_dim,\n+    int64_t HR,\n+    int64_t q_pe_stride_s,\n+    int64_t out_stride_qs,\n+    int64_t out_stride_ks,\n+    int64_t HK,\n+    int64_t k_pe_stride_s,\n+    int64_t q_pe_stride_n,\n+    int64_t out_stride_qn) {\n+  int64_t COFF = HR / 2;\n+  at::parallel_for(0, seq_len * num_head, GRAIN_SIZE / rotary_dim, [&](int64_t begin, int64_t end) {\n+    int64_t seq{0}, head_id{0};\n+    data_index_init(begin, seq, seq_len, head_id, num_head);\n+    for (int64_t i = begin; i < end; ++i) {\n+      int64_t in_offset_q = seq * q_pe_stride_s + head_id * q_pe_stride_n;\n+      int64_t out_offset_q = seq * out_stride_qs + head_id * out_stride_qn;\n+      int64_t out_offset_k = seq * out_stride_ks;\n+      int64_t p = 0;\n+      scalar_t* sin_start = nullptr;\n+      scalar_t* cos_start = nullptr;\n+      // step 0) get the rotary position embedding for the current position\n+      p = t_pos[seq];\n+      sin_start = t_emb_pos + p * HR + COFF;\n+      cos_start = t_emb_pos + p * HR;\n+      // step 1) apply_rotary_pos_emb for the rotary_dim elements in every\n+      // head of query/key\n+      for (int64_t h = 0; h < rotary_dim; h += 2) {\n+        scalar_t cos = cos_start[h >> 1];\n+        scalar_t sin = sin_start[h >> 1];\n+        scalar_t in1 = q_pe[in_offset_q + h];\n+        scalar_t in2 = q_pe[in_offset_q + h + 1];\n+        scalar_t out1 = in1 * cos - in2 * sin;\n+        scalar_t out2 = in2 * cos + in1 * sin;\n+        q_pe_out[out_offset_q + h] = out1;\n+        q_pe_out[out_offset_q + h + 1] = out2;\n+      }\n+      for (int64_t h = 0; h < HK; h += 2) {\n+        scalar_t cos = cos_start[h >> 1];\n+        scalar_t sin = sin_start[h >> 1];\n+        int64_t k_pe_offset = seq * k_pe_stride_s;\n+        scalar_t in1_k = k_pe[k_pe_offset + h];\n+        scalar_t in2_k = k_pe[k_pe_offset + h + 1];\n+        scalar_t out1_k = in1_k * cos - in2_k * sin;\n+        scalar_t out2_k = in2_k * cos + in1_k * sin;\n+        k_pe_out[out_offset_k + h] = out1_k;\n+        k_pe_out[out_offset_k + h + 1] = out2_k;\n+      }\n+      // move to the next index\n+      data_index_step(seq, seq_len, head_id, num_head);\n+    }\n+  });\n+}\n+}  // namespace\n+\n+std::tuple<at::Tensor, at::Tensor>\n+rotary_position_embedding_cpu(at::Tensor& t_pos, at::Tensor& q_pe, at::Tensor& k_pe, at::Tensor& t_emb_pos) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::rotary_position_embedding_cpu\", std::vector<c10::IValue>({t_pos, q_pe, k_pe, t_emb_pos}));\n+  CHECK_INPUT(t_pos);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(q_pe);\n+  CHECK_LAST_DIM_CONTIGUOUS_INPUT(k_pe);\n+  CHECK_INPUT(t_emb_pos);\n+  CHECK_DIM(1, t_pos);\n+  CHECK_DIM(3, q_pe);\n+  CHECK_DIM(3, k_pe);\n+  CHECK_DIM(2, t_emb_pos);\n+\n+  int64_t seq_len = q_pe.size(0);\n+  int64_t num_head = q_pe.size(1);\n+  int64_t rotary_dim = q_pe.size(2);\n+  int64_t HK = k_pe.size(2);\n+  int64_t HR = t_emb_pos.size(1);\n+  CHECK_EQ(HR, rotary_dim);\n+  CHECK_EQ(k_pe.size(0), seq_len);\n+  CHECK_EQ(k_pe.size(1), 1);\n+  CHECK_EQ(t_pos.size(0), seq_len);\n+  CHECK_EQ(HK, rotary_dim);\n+\n+  at::Tensor q_pe_out = at::empty_like(q_pe);\n+  at::Tensor k_pe_out = at::empty_like(k_pe);\n+  int64_t q_pe_stride_s = q_pe.stride(0);\n+  int64_t q_pe_stride_n = q_pe.stride(1);\n+  int64_t k_pe_stride_s = k_pe.stride(0);\n+  int64_t out_stride_qs = q_pe_out.stride(0);\n+  int64_t out_stride_qn = q_pe_out.stride(1);\n+  int64_t out_stride_ks = k_pe_out.stride(0);\n+\n+  const auto input_dtype = q_pe.scalar_type();\n+  TORCH_CHECK(t_pos.scalar_type() == at::kLong, \"expect positions to be int64, got \", t_pos.scalar_type());\n+  TORCH_CHECK(input_dtype == k_pe.scalar_type(), \"q_pe and k_pe must have the same data type\");\n+  TORCH_CHECK(input_dtype == t_emb_pos.scalar_type(), \"q_pe and t_emb_pos must have the same data type\");\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(input_dtype, \"rotary_position_embedding_cpu\", [&] {\n+    rope_kernel_impl<scalar_t>(\n+        q_pe_out.data_ptr<scalar_t>(),\n+        k_pe_out.data_ptr<scalar_t>(),\n+        t_pos.data_ptr<int64_t>(),\n+        q_pe.data_ptr<scalar_t>(),\n+        k_pe.data_ptr<scalar_t>(),\n+        t_emb_pos.data_ptr<scalar_t>(),\n+        seq_len,\n+        num_head,\n+        rotary_dim,\n+        HR,\n+        q_pe_stride_s,\n+        out_stride_qs,\n+        out_stride_ks,\n+        HK,\n+        k_pe_stride_s,\n+        q_pe_stride_n,\n+        out_stride_qn);\n+  });\n+  return std::make_tuple(q_pe_out, k_pe_out);\n+}\ndiff --git a/sgl-kernel/csrc/cpu/shm.cpp b/sgl-kernel/csrc/cpu/shm.cpp\nnew file mode 100644\nindex 000000000..9f7d89df1\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/shm.cpp\n@@ -0,0 +1,659 @@\n+#include \"shm.h\"\n+\n+#include <ATen/ATen.h>\n+#include <errno.h>\n+#include <fcntl.h>\n+#include <immintrin.h>\n+#include <stddef.h>\n+#include <stdio.h>\n+#include <stdlib.h>\n+#include <sys/mman.h>\n+#include <unistd.h>\n+\n+// states for collectives\n+enum coll_state {\n+  coll_begin = 0,\n+  coll_allreduce_naive__copy_in_done,\n+  coll_allreduce_naive__reduce_done,\n+  // alternative state when allreduce is working on alternative buffer\n+  // of the double buffer.\n+  coll_alt1_allreduce_naive__copy_in_done,\n+  coll_alt2_allreduce_naive__copy_in_done,\n+  coll_alt1_allreduce_naive__reduce_done,\n+  coll_allgather_naive__copy_in_done,\n+  coll_alt1_allgather_naive__copy_in_done,\n+  coll_alt2_allgather_naive__copy_in_done,\n+};\n+\n+// SHM building blocks\n+struct SharedData {\n+  const char* name;\n+  int descriptor;\n+  void* bytes;\n+  size_t nbytes;\n+};\n+\n+void shared_open(SharedData* data, const char* name, size_t nbytes) {\n+  int d = shm_open(name, O_RDWR, S_IRUSR | S_IWUSR);\n+  if (d != -1) {\n+    void* bytes = mmap(NULL, nbytes, PROT_READ | PROT_WRITE, MAP_SHARED, d, 0);\n+    data->name = name;\n+    data->descriptor = d;\n+    data->bytes = bytes;\n+    data->nbytes = nbytes;\n+  } else {\n+    if (errno != ENOENT) {\n+      // don't print if shm can not be found because we want to loop over from\n+      // caller again until the other ranks created the shm\n+      printf(\"shared_open %s failed, errno=%d\\n\", name, errno);\n+    }\n+    data->descriptor = -1;\n+  }\n+}\n+\n+void shared_create(SharedData* data, const char* name, void* bytes, size_t nbytes) {\n+  int d = shm_open(name, O_CREAT | O_RDWR, S_IRUSR | S_IWUSR);\n+  if (d != -1) {\n+    if (nbytes = write(d, bytes, nbytes)) {\n+      shared_open(data, name, nbytes);\n+    }\n+  } else {\n+    printf(\"shared_create %s failed\\n\", name);\n+  }\n+}\n+\n+static int world_size;\n+\n+// SHM based allreduce helper functions\n+// buffer that holds shm name\n+#define NAME_BUF_SIZE 1000\n+#define MAX_BUF_SIZE 1048576 * 32\n+#define NAIVE_ALLREDUCE_THRESHOLD 1048576\n+#define SHM_BUFFER_NAME \"deepspeed_allreduce_buffer\"\n+struct allreduce_workspace {\n+  enum coll_state states[2];  // idx=0 -- state for symmetric_naive_all_reduce\n+                              // idx=1 -- state for distributed_naive_all_reduce\n+  // double buffer to avoid syncing between rounds\n+  // offset=0 -- 2*NAIVE_ALLREDUCE_THRESHOLD : buffer for\n+  // symmetric_naive_all_reduce after that : buffer for\n+  // distributed_naive_all_reduce\n+  char buffer[2 * NAIVE_ALLREDUCE_THRESHOLD + 2 * MAX_BUF_SIZE];\n+};\n+\n+#define BUFFER0_OFFSET(current_buffer) current_buffer* NAIVE_ALLREDUCE_THRESHOLD\n+#define BUFFER1_OFFSET(current_buffer) 2 * NAIVE_ALLREDUCE_THRESHOLD + current_buffer* MAX_BUF_SIZE\n+\n+struct allreduce_workspace** workspace;\n+\n+// buffer for small messages, double buffer\n+char** symmetric_buffer[2];\n+// buffer for large messages, double buffer\n+char** distributed_buffer[2];\n+\n+void wait_buffer_state_until_2(int index, enum coll_state state0, enum coll_state state1, int state_group) {\n+  volatile enum coll_state* state_ptr = &(workspace[index]->states[state_group]);\n+\n+  while (1) {\n+    volatile enum coll_state cur_state = *state_ptr;\n+    if (cur_state == state0 || cur_state == state1) break;\n+  }\n+}\n+\n+__m512 cvt_bf16_to_fp32(const __m256i src) __attribute__((target(\"avx512bw\")));\n+inline __m512 cvt_bf16_to_fp32(const __m256i src) {\n+  auto y = _mm512_cvtepu16_epi32(src);\n+  return _mm512_castsi512_ps(_mm512_bslli_epi128(y, 2));\n+}\n+\n+inline __m256i cvt_fp32_to_bf16(const __m512 src) __attribute__((target(\"avx512bw\")));\n+inline __m256i cvt_fp32_to_bf16(const __m512 src) {\n+  __m512i value = _mm512_castps_si512(src);\n+  __m512i nan = _mm512_set1_epi32(0xffff);\n+  auto mask_value = _mm512_cmp_ps_mask(src, src, _CMP_ORD_Q);\n+  __m512i ones = _mm512_set1_epi32(0x1);\n+  __m512i vec_bias = _mm512_set1_epi32(0x7fff);\n+  // uint32_t lsb = (input >> 16) & 1;\n+  auto t_value = _mm512_and_si512(_mm512_srli_epi32(value, 16), ones);\n+  // uint32_t rounding_bias = 0x7fff + lsb;\n+  t_value = _mm512_add_epi32(t_value, vec_bias);\n+  // input += rounding_bias;\n+  t_value = _mm512_add_epi32(t_value, value);\n+  // input = input >> 16;\n+  t_value = _mm512_srli_epi32(t_value, 16);\n+  // Check NaN before converting back to bf16\n+  t_value = _mm512_mask_blend_epi32(mask_value, nan, t_value);\n+  return _mm512_cvtusepi32_epi16(t_value);\n+}\n+\n+__m512 cvt_fp16_to_fp32(const __m256i src) __attribute__((target(\"avx512bw\")));\n+inline __m512 cvt_fp16_to_fp32(const __m256i src) {\n+  return _mm512_cvtph_ps(src);\n+}\n+\n+inline __m256i cvt_fp32_to_fp16(const __m512 src) __attribute__((target(\"avx512bw\")));\n+inline __m256i cvt_fp32_to_fp16(const __m512 src) {\n+  return _mm512_cvtps_ph(src, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n+}\n+\n+void reduce_bf16_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers)\n+    __attribute__((target(\"avx512bw\")));\n+\n+void reduce_fp16_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers)\n+    __attribute__((target(\"avx512bw\")));\n+\n+void reduce_fp32_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers)\n+    __attribute__((target(\"avx512bw\")));\n+\n+void reduce_all_buffers(\n+    int start_elements,\n+    int num_elements,\n+    c10::ScalarType scalar_type,\n+    int to_buffer_idx,\n+    char* to_buffer,\n+    char** buffers) {\n+  switch (scalar_type) {\n+    case c10::ScalarType::BFloat16:\n+      reduce_bf16_buffers(start_elements, num_elements, to_buffer, buffers);\n+      break;\n+    case c10::ScalarType::Half:\n+      reduce_fp16_buffers(start_elements, num_elements, to_buffer, buffers);\n+      break;\n+    case c10::ScalarType::Float:\n+      reduce_fp32_buffers(start_elements, num_elements, to_buffer, buffers);\n+      break;\n+    default:\n+      assert(!\"Should not get here\");\n+  }\n+}\n+\n+#define CVT_ADD_BF16(x)                                                                  \\\n+  do {                                                                                   \\\n+    auto in##x##_val = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[x] + i))); \\\n+    inout_val = _mm512_add_ps(inout_val, in##x##_val);                                   \\\n+  } while (0)\n+\n+// Reduce functions down below use vectorized algorithm, the number of bytes\n+// processed each iteration depends on vector length.  256bit vector ==> 32\n+// bytes, 512bit vector ==> 64 bytes If you change implementation of\n+// reduce_bf16_buffers, etc. , check whether this number needs to be changed\n+#define VECTOR_LENGTH_IN_BYTES 32\n+\n+void reduce_bf16_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers) {\n+  const int element_size = 2;\n+  const int vector_length = VECTOR_LENGTH_IN_BYTES / element_size;\n+  int main_elements = num_elements - (num_elements % vector_length);\n+  int remain_elements = num_elements % vector_length;\n+\n+  // process aligned part\n+#pragma omp parallel for\n+  for (int i = start_elements * element_size; i < (start_elements + main_elements) * element_size;\n+       i += VECTOR_LENGTH_IN_BYTES) {\n+    auto inout_val = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[0] + i)));\n+    switch (world_size) {\n+      case 16:\n+        CVT_ADD_BF16(15);\n+      case 15:\n+        CVT_ADD_BF16(14);\n+      case 14:\n+        CVT_ADD_BF16(13);\n+      case 13:\n+        CVT_ADD_BF16(12);\n+      case 12:\n+        CVT_ADD_BF16(11);\n+      case 11:\n+        CVT_ADD_BF16(10);\n+      case 10:\n+        CVT_ADD_BF16(9);\n+      case 9:\n+        CVT_ADD_BF16(8);\n+      case 8:\n+        CVT_ADD_BF16(7);\n+      case 7:\n+        CVT_ADD_BF16(6);\n+      case 6:\n+        CVT_ADD_BF16(5);\n+      case 5:\n+        CVT_ADD_BF16(4);\n+      case 4:\n+        CVT_ADD_BF16(3);\n+      case 3:\n+        CVT_ADD_BF16(2);\n+      case 2:\n+        CVT_ADD_BF16(1);\n+      case 1:\n+        break;\n+      default:\n+        for (int j = 1; j < world_size; j++) {\n+          auto in_val = cvt_bf16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[j] + i)));\n+          inout_val = _mm512_add_ps(inout_val, in_val);\n+        }\n+    }\n+    _mm256_storeu_si256((__m256i*)(to_buffer + i), cvt_fp32_to_bf16(inout_val));\n+  }\n+\n+  // process remaining part\n+  int i = (start_elements + main_elements) * element_size;\n+  while (remain_elements > 0) {\n+    float val = 0.0f;\n+    for (int j = 0; j < world_size; j++) {\n+      val += *(at::BFloat16*)(buffers[j] + i);\n+    }\n+    *(at::BFloat16*)(to_buffer + i) = val;\n+    remain_elements--;\n+    i += element_size;\n+  }\n+}\n+\n+#define CVT_ADD_FP16(x)                                                                  \\\n+  do {                                                                                   \\\n+    auto in##x##_val = cvt_fp16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[x] + i))); \\\n+    inout_val = _mm512_add_ps(inout_val, in##x##_val);                                   \\\n+  } while (0)\n+\n+void reduce_fp16_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers) {\n+  const int element_size = 2;\n+  const int vector_length = VECTOR_LENGTH_IN_BYTES / element_size;\n+  int main_elements = num_elements - (num_elements % vector_length);\n+  int remain_elements = num_elements % vector_length;\n+\n+  // process aligned part\n+#pragma omp parallel for\n+  for (int i = start_elements * element_size; i < (start_elements + main_elements) * element_size;\n+       i += VECTOR_LENGTH_IN_BYTES) {\n+    auto inout_val = cvt_fp16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[0] + i)));\n+    switch (world_size) {\n+      case 16:\n+        CVT_ADD_FP16(15);\n+      case 15:\n+        CVT_ADD_FP16(14);\n+      case 14:\n+        CVT_ADD_FP16(13);\n+      case 13:\n+        CVT_ADD_FP16(12);\n+      case 12:\n+        CVT_ADD_FP16(11);\n+      case 11:\n+        CVT_ADD_FP16(10);\n+      case 10:\n+        CVT_ADD_FP16(9);\n+      case 9:\n+        CVT_ADD_FP16(8);\n+      case 8:\n+        CVT_ADD_FP16(7);\n+      case 7:\n+        CVT_ADD_FP16(6);\n+      case 6:\n+        CVT_ADD_FP16(5);\n+      case 5:\n+        CVT_ADD_FP16(4);\n+      case 4:\n+        CVT_ADD_FP16(3);\n+      case 3:\n+        CVT_ADD_FP16(2);\n+      case 2:\n+        CVT_ADD_FP16(1);\n+      case 1:\n+        break;\n+      default:\n+        for (int j = 1; j < world_size; j++) {\n+          auto in_val = cvt_fp16_to_fp32(_mm256_loadu_si256((__m256i*)(buffers[j] + i)));\n+          inout_val = _mm512_add_ps(inout_val, in_val);\n+        }\n+    }\n+    _mm256_storeu_si256((__m256i*)(to_buffer + i), cvt_fp32_to_fp16(inout_val));\n+  }\n+\n+  // process remaining part\n+  int i = (start_elements + main_elements) * element_size;\n+  while (remain_elements > 0) {\n+    float val = 0.0f;\n+    for (int j = 0; j < world_size; j++) {\n+      val += *(at::Half*)(buffers[j] + i);\n+    }\n+    *(at::Half*)(to_buffer + i) = val;\n+    remain_elements--;\n+    i += element_size;\n+  }\n+}\n+\n+#define CVT_ADD_F32(x)                                            \\\n+  do {                                                            \\\n+    auto in##x##_val = _mm256_loadu_ps((float*)(buffers[x] + i)); \\\n+    inout_val = _mm256_add_ps(inout_val, in##x##_val);            \\\n+  } while (0)\n+\n+void reduce_fp32_buffers(int start_elements, int num_elements, char* to_buffer, char** buffers) {\n+  const int element_size = 4;\n+  const int vector_length = VECTOR_LENGTH_IN_BYTES / element_size;\n+  int main_elements = num_elements - (num_elements % vector_length);\n+  int remain_elements = num_elements % vector_length;\n+\n+  // process aligned part\n+#pragma omp parallel for\n+  for (int i = start_elements * element_size; i < (start_elements + main_elements) * element_size;\n+       i += VECTOR_LENGTH_IN_BYTES) {\n+    auto inout_val = _mm256_loadu_ps((float*)(buffers[0] + i));\n+    switch (world_size) {\n+      case 16:\n+        CVT_ADD_F32(15);\n+      case 15:\n+        CVT_ADD_F32(14);\n+      case 14:\n+        CVT_ADD_F32(13);\n+      case 13:\n+        CVT_ADD_F32(12);\n+      case 12:\n+        CVT_ADD_F32(11);\n+      case 11:\n+        CVT_ADD_F32(10);\n+      case 10:\n+        CVT_ADD_F32(9);\n+      case 9:\n+        CVT_ADD_F32(8);\n+      case 8:\n+        CVT_ADD_F32(7);\n+      case 7:\n+        CVT_ADD_F32(6);\n+      case 6:\n+        CVT_ADD_F32(5);\n+      case 5:\n+        CVT_ADD_F32(4);\n+      case 4:\n+        CVT_ADD_F32(3);\n+      case 3:\n+        CVT_ADD_F32(2);\n+      case 2:\n+        CVT_ADD_F32(1);\n+      case 1:\n+        break;\n+      default:\n+        for (int j = 1; j < world_size; j++) {\n+          auto in_val = _mm256_loadu_ps((float*)(buffers[j] + i));\n+          inout_val = _mm256_add_ps(inout_val, in_val);\n+        }\n+    }\n+    _mm256_storeu_ps((float*)(to_buffer + i), inout_val);\n+  }\n+\n+  // process remaining part\n+  int i = (start_elements + main_elements) * element_size;\n+  while (remain_elements > 0) {\n+    float val = 0.0f;\n+    for (int j = 0; j < world_size; j++) {\n+      val += *(float*)(buffers[j] + i);\n+    }\n+    *(float*)(to_buffer + i) = val;\n+    remain_elements--;\n+    i += element_size;\n+  }\n+}\n+\n+static bool is_initialized = false;\n+static int world_rank;\n+\n+void shm_initialize(int size, int rank, char* addr_string, char* port_string) {\n+  if (is_initialized) {\n+    return;\n+  }\n+  is_initialized = true;\n+\n+  world_size = size;\n+  world_rank = rank;\n+\n+  char shm_name_prefix[NAME_BUF_SIZE];\n+  char shm_name[NAME_BUF_SIZE];\n+  snprintf(shm_name_prefix, NAME_BUF_SIZE, \"%s_%d_%s_%s\", SHM_BUFFER_NAME, getuid(), addr_string, port_string);\n+  // create shared workspace for SHM based allreduce\n+  SharedData allreduce_buffer;\n+  // allocate workspace_buf for current rank\n+  struct allreduce_workspace* workspace_buf;\n+  struct allreduce_workspace* workspace_buf_other;\n+  workspace_buf = (struct allreduce_workspace*)malloc(sizeof(struct allreduce_workspace));\n+  snprintf(shm_name, NAME_BUF_SIZE, \"%s_%d\", shm_name_prefix, rank);\n+  shared_create(&allreduce_buffer, shm_name, workspace_buf, sizeof(struct allreduce_workspace));\n+  workspace_buf = (struct allreduce_workspace*)allreduce_buffer.bytes;\n+  workspace_buf->states[0] = coll_alt2_allreduce_naive__copy_in_done;\n+  workspace_buf->states[1] = coll_begin;\n+\n+  // create the workspace pointer list\n+  workspace = (struct allreduce_workspace**)malloc(size * sizeof(struct allreduce_workspace*));\n+  symmetric_buffer[0] = (char**)malloc(size * sizeof(char**));\n+  symmetric_buffer[1] = (char**)malloc(size * sizeof(char**));\n+  distributed_buffer[0] = (char**)malloc(size * sizeof(char**));\n+  distributed_buffer[1] = (char**)malloc(size * sizeof(char**));\n+\n+  // map shm of all ranks\n+  for (int i = 0; i < size; i++) {\n+    if (i != rank) {\n+      snprintf(shm_name, NAME_BUF_SIZE, \"%s_%d\", shm_name_prefix, i);\n+      // printf(\"open %s, %d\\n\", shm_name, rank);\n+      do {\n+        shared_open(&allreduce_buffer, shm_name, sizeof(struct allreduce_workspace));\n+      } while (allreduce_buffer.descriptor == -1 && errno == ENOENT);\n+      workspace_buf_other = (struct allreduce_workspace*)allreduce_buffer.bytes;\n+      workspace[i] = workspace_buf_other;\n+    } else {\n+      workspace[i] = workspace_buf;\n+    }\n+    symmetric_buffer[0][i] = workspace[i]->buffer + BUFFER0_OFFSET(0);\n+    symmetric_buffer[1][i] = workspace[i]->buffer + BUFFER0_OFFSET(1);\n+    distributed_buffer[0][i] = workspace[i]->buffer + BUFFER1_OFFSET(0);\n+    distributed_buffer[1][i] = workspace[i]->buffer + BUFFER1_OFFSET(1);\n+  }\n+}\n+\n+static void parallel_memcpy(void* to, void* from, size_t n_bytes) __attribute__((target(\"avx512bw\")));\n+static void parallel_memcpy(void* to, void* from, size_t n_bytes) {\n+  auto aligned_bytes = n_bytes - (n_bytes % VECTOR_LENGTH_IN_BYTES);\n+  // process aligned part\n+#pragma omp parallel for\n+  for (int i = 0; i < aligned_bytes; i += VECTOR_LENGTH_IN_BYTES) {\n+    auto val = _mm256_loadu_si256((__m256i*)((char*)from + i));\n+    _mm256_storeu_si256((__m256i*)((char*)to + i), val);\n+  }\n+\n+  // process remaining part\n+  for (int i = aligned_bytes; i < n_bytes; i++) {\n+    *((char*)to + i) = *((char*)from + i);\n+  }\n+}\n+\n+#define positive_mod(num, mod) ((((num) % (mod)) + (mod)) % (mod))\n+#define rank_mod(rank) positive_mod(rank, world_size)\n+size_t slice_size(size_t chunk_el, int slice_idx) {\n+  size_t slice_size = chunk_el / world_size;\n+  return slice_idx == world_size - 1 ? slice_size + (chunk_el % world_size) : slice_size;\n+}\n+\n+char* slice_data(char* data_ptr, size_t chunk_el, int el_size, int slice_idx) {\n+  size_t slice_size = chunk_el / world_size;\n+  size_t el_offset = slice_size * slice_idx;\n+  return data_ptr + el_offset * el_size;\n+}\n+\n+size_t slice_el_start(size_t chunk_el, int slice_idx) {\n+  size_t slice_size = chunk_el / world_size;\n+  return slice_size * slice_idx;\n+}\n+\n+void symmetric_naive_all_reduce(char* data_ptr, c10::ScalarType scalar_type, size_t chunk_size, size_t chunk_el) {\n+  const int state_group = 0;\n+  static int current_buffer = 0;\n+  static int state_idx = 0;\n+\n+  enum coll_state copy_current, copy_next;\n+\n+  switch (state_idx) {\n+    case 0:\n+      copy_current = coll_allreduce_naive__copy_in_done;\n+      copy_next = coll_alt1_allreduce_naive__copy_in_done;\n+      break;\n+    case 1:\n+      copy_current = coll_alt1_allreduce_naive__copy_in_done;\n+      copy_next = coll_alt2_allreduce_naive__copy_in_done;\n+      break;\n+    case 2:\n+      copy_current = coll_alt2_allreduce_naive__copy_in_done;\n+      copy_next = coll_allreduce_naive__copy_in_done;\n+      break;\n+    default:\n+      assert(!\"Should not get here.\");\n+  }\n+  state_idx = (state_idx + 1) % 3;\n+\n+  parallel_memcpy(symmetric_buffer[current_buffer][world_rank], data_ptr, chunk_size);\n+  std::atomic_thread_fence(std::memory_order_release);\n+  workspace[world_rank]->states[state_group] = copy_current;\n+\n+  for (int i = 0; i < world_size; i++) {\n+    // wait until the other rank copy the buffer\n+    if (i != world_rank) {\n+      wait_buffer_state_until_2(i, copy_current, copy_next, state_group);\n+    }\n+  }\n+\n+  // each rank reduce the buffer independently so therre is no need for\n+  // synchronization afterward\n+  reduce_all_buffers(0, chunk_el, scalar_type, world_rank, data_ptr, symmetric_buffer[current_buffer]);\n+\n+  // switch buffer\n+  current_buffer = 1 - current_buffer;\n+}\n+\n+// naive allreduce distributed, each rank do naive reduce on its slice\n+void distributed_naive_reduce(char* data_ptr, c10::ScalarType scalar_type, size_t chunk_size, size_t chunk_el) {\n+  const int state_group = 1;\n+  static int current_buffer = 0;\n+  static int state_idx = 0;\n+\n+  enum coll_state copy_current, copy_next, reduce_current;\n+\n+  // similar to symmetric_naive_allreduce, but here we only need two sets of\n+  // states, because distributed naive reduce has two barriers in the algorithm\n+  switch (state_idx) {\n+    case 0:\n+      copy_current = coll_allreduce_naive__copy_in_done;\n+      reduce_current = coll_allreduce_naive__reduce_done;\n+      copy_next = coll_alt1_allreduce_naive__copy_in_done;\n+      break;\n+    case 1:\n+      copy_current = coll_alt1_allreduce_naive__copy_in_done;\n+      reduce_current = coll_alt1_allreduce_naive__reduce_done;\n+      copy_next = coll_allreduce_naive__copy_in_done;\n+      break;\n+    default:\n+      assert(!\"Should not get here.\");\n+  }\n+  state_idx = (state_idx + 1) % 2;\n+\n+  int data_size = chunk_size / chunk_el;\n+  parallel_memcpy(distributed_buffer[current_buffer][world_rank], data_ptr, chunk_size);\n+  std::atomic_thread_fence(std::memory_order_release);\n+  workspace[world_rank]->states[state_group] = copy_current;\n+\n+  for (int i = 0; i < world_size; i++) {\n+    // wait until all the other ranks copy the buffer\n+    if (i != world_rank) wait_buffer_state_until_2(i, copy_current, reduce_current, state_group);\n+  }\n+\n+  // reduce scatter\n+  reduce_all_buffers(\n+      slice_el_start(chunk_el, world_rank),\n+      slice_size(chunk_el, world_rank),\n+      scalar_type,\n+      world_rank,\n+      distributed_buffer[current_buffer][world_rank],\n+      distributed_buffer[current_buffer]);\n+  std::atomic_thread_fence(std::memory_order_release);\n+  workspace[world_rank]->states[state_group] = reduce_current;\n+\n+  for (int i = 0; i < world_size; i++) {\n+    // wait until all the other ranks reduce the buffer\n+    if (i != world_rank) wait_buffer_state_until_2(i, reduce_current, copy_next, state_group);\n+  }\n+\n+  for (int i = 0; i < world_size; i++) {\n+    int rank = (i + world_rank) % world_size;\n+    parallel_memcpy(\n+        slice_data(data_ptr, chunk_el, data_size, rank),\n+        slice_data(distributed_buffer[current_buffer][rank], chunk_el, chunk_size / chunk_el, rank),\n+        slice_size(chunk_el, rank) * data_size);\n+  }\n+\n+  current_buffer = 1 - current_buffer;\n+}\n+\n+void all_reduce_outer_loop(torch::Tensor& data, size_t numel, int data_size) {\n+  for (int offset = 0; offset < data_size; offset += MAX_BUF_SIZE) {\n+    auto data_ptr = ((char*)(data.data_ptr()) + offset);\n+    size_t chunk_size = data_size - offset > MAX_BUF_SIZE ? MAX_BUF_SIZE : data_size - offset;\n+    size_t chunk_el = chunk_size / (data_size / numel);\n+    if (chunk_size < NAIVE_ALLREDUCE_THRESHOLD) {\n+      symmetric_naive_all_reduce(data_ptr, data.scalar_type(), chunk_size, chunk_el);\n+    } else {\n+      distributed_naive_reduce(data_ptr, data.scalar_type(), chunk_size, chunk_el);\n+    }\n+  }\n+}\n+\n+void naive_all_gather(char* result_ptr, char* data_ptr, size_t res_stride, size_t chunk_size, size_t chunk_el) {\n+  const int state_group = 1;\n+  static int current_buffer = 0;\n+  static int state_idx = 0;\n+\n+  enum coll_state copy_current, copy_next;\n+\n+  switch (state_idx) {\n+    case 0:\n+      copy_current = coll_allgather_naive__copy_in_done;\n+      copy_next = coll_alt1_allgather_naive__copy_in_done;\n+      break;\n+    case 1:\n+      copy_current = coll_alt1_allgather_naive__copy_in_done;\n+      copy_next = coll_alt2_allgather_naive__copy_in_done;\n+      break;\n+    case 2:\n+      copy_current = coll_alt2_allgather_naive__copy_in_done;\n+      copy_next = coll_allgather_naive__copy_in_done;\n+      break;\n+    default:\n+      assert(!\"Should not get here.\");\n+  }\n+  state_idx = (state_idx + 1) % 3;\n+\n+  int data_size = chunk_size / chunk_el;\n+  parallel_memcpy(distributed_buffer[current_buffer][world_rank], data_ptr, chunk_size);\n+  std::atomic_thread_fence(std::memory_order_release);\n+  workspace[world_rank]->states[state_group] = copy_current;\n+\n+  for (int i = 0; i < world_size; i++) {\n+    // wait until all the other ranks copy the buffer\n+    if (i != world_rank) wait_buffer_state_until_2(i, copy_current, copy_next, state_group);\n+  }\n+  for (int i = 0; i < world_size; i++) {\n+    parallel_memcpy(result_ptr + i * res_stride, distributed_buffer[current_buffer][i], chunk_size);\n+  }\n+  current_buffer = 1 - current_buffer;\n+}\n+\n+torch::Tensor& all_gather(torch::Tensor& result, torch::Tensor& data, int dim, size_t numel, int data_size) {\n+  size_t dim_el = data.stride(dim) * data.size(dim);\n+  int dtype_size = data_size / numel;\n+  size_t dim_size = dim_el * dtype_size;\n+  int dim_count = data_size / dim_size;\n+  auto data_ptr = (char*)(data.data_ptr());\n+  auto result_ptr = (char*)(result.data_ptr());\n+  for (int i = 0; i < dim_count; i++) {\n+    for (int offset = 0; offset < dim_size; offset += MAX_BUF_SIZE) {\n+      size_t chunk_size = dim_size - offset > MAX_BUF_SIZE ? MAX_BUF_SIZE : dim_size - offset;\n+      size_t chunk_el = chunk_size / dtype_size;\n+      naive_all_gather(\n+          result_ptr + i * dim_size * world_size + offset,\n+          data_ptr + i * dim_size + offset,\n+          dim_size,\n+          chunk_size,\n+          chunk_el);\n+    }\n+  }\n+  return result;\n+}\ndiff --git a/sgl-kernel/csrc/cpu/shm.h b/sgl-kernel/csrc/cpu/shm.h\nnew file mode 100644\nindex 000000000..d21fe3d36\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/shm.h\n@@ -0,0 +1,11 @@\n+#include <torch/torch.h>\n+\n+#include <torch/csrc/distributed/c10d/ProcessGroup.hpp>\n+\n+#ifndef __SHM_COLLECTIVES__\n+#define __SHM_COLLECTIVES__\n+#define VECTOR_LENGTH_IN_BYTES 32\n+void shm_initialize(int size, int rank, char* addr_string, char* port_string);\n+void all_reduce_outer_loop(torch::Tensor& data, size_t numel, int data_size);\n+torch::Tensor& all_gather(torch::Tensor& result, torch::Tensor& data, int dim, size_t numel, int data_size);\n+#endif\ndiff --git a/sgl-kernel/csrc/cpu/topk.cpp b/sgl-kernel/csrc/cpu/topk.cpp\nnew file mode 100644\nindex 000000000..6a6b64d12\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/topk.cpp\n@@ -0,0 +1,406 @@\n+#include \"common.h\"\n+#include \"vec.h\"\n+\n+namespace {\n+\n+template <typename scalar_t, int SIZE>\n+inline void softmax(float* __restrict__ out, const scalar_t* __restrict__ input) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  constexpr int kVecSize = bVec::size();\n+\n+  // step 1: get max\n+  fVec max_fvec = fVec(-std::numeric_limits<float>::infinity());\n+  if constexpr (SIZE < kVecSize) {\n+    // SIZE = 1, 2, 4, 8, 16; only the top half is used\n+    bVec x_bvec = bVec::loadu(input, SIZE);\n+    fVec x_fvec0, x_fvec1;\n+    std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+    x_fvec0 = fVec::set(max_fvec, x_fvec0, SIZE);\n+    max_fvec = at::vec::maximum(max_fvec, x_fvec0);\n+    x_fvec0.store(out, SIZE);\n+  } else {\n+    for (int d = 0; d < SIZE; d += kVecSize) {\n+      bVec x_bvec = bVec::loadu(input + d);\n+      fVec x_fvec0, x_fvec1;\n+      std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+      max_fvec = at::vec::maximum(max_fvec, x_fvec0);\n+      max_fvec = at::vec::maximum(max_fvec, x_fvec1);\n+      x_fvec0.store(out + d);\n+      x_fvec1.store(out + d + fVec::size());\n+    }\n+  }\n+  float max_val = vec_reduce_max(max_fvec);\n+  max_fvec = fVec(max_val);\n+\n+  // step 2: sum of (x - max).exp()\n+  fVec sum_fvec = fVec(float(0));\n+  if constexpr (SIZE < fVec::size()) {\n+    // SIZE = 1, 2, 4, 8\n+    fVec x_fvec = (fVec::loadu(out, SIZE) - max_fvec).exp_u20();\n+    x_fvec = fVec::set(sum_fvec, x_fvec, SIZE);\n+    sum_fvec += x_fvec;\n+    x_fvec.store(out, SIZE);\n+  } else {\n+    for (int d = 0; d < SIZE; d += fVec::size()) {\n+      fVec x_fvec = (fVec::loadu(out + d) - max_fvec).exp_u20();\n+      sum_fvec += x_fvec;\n+      x_fvec.store(out + d);\n+    }\n+  }\n+  float sum_val = vec_reduce_sum(sum_fvec);\n+\n+  // step 3: x * (1 / sum)\n+  sum_fvec = fVec(1.f / sum_val);\n+  if constexpr (SIZE < fVec::size()) {\n+    // SIZE = 1, 2, 4, 8\n+    fVec out_fvec = fVec::loadu(out, SIZE) * sum_fvec;\n+    out_fvec.store(out, SIZE);\n+  } else {\n+    for (int d = 0; d < SIZE; d += fVec::size()) {\n+      fVec out_fvec = fVec::loadu(out + d) * sum_fvec;\n+      out_fvec.store(out + d);\n+    }\n+  }\n+}\n+\n+template <typename scalar_t, int NUM_EXPERTS>\n+void grouped_topk_kernel_impl(\n+    float* __restrict__ topk_weights,\n+    int32_t* __restrict__ topk_ids,\n+    const scalar_t* __restrict__ gating_output,\n+    int64_t num_tokens,\n+    int64_t topk,\n+    int64_t num_groups,\n+    int64_t topk_group,\n+    bool renormalize) {\n+  const int64_t num_experts_per_group = NUM_EXPERTS / num_groups;\n+  at::parallel_for(0, num_tokens, 0, [&](int64_t begin, int64_t end) {\n+    alignas(64) float scores[NUM_EXPERTS];\n+\n+    using elem_t = std::pair<float, int32_t>;\n+    std::vector<elem_t> queue(num_groups);\n+    std::vector<elem_t> queue2(topk_group * num_experts_per_group);\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      // do softmax to get scores\n+      softmax<scalar_t, NUM_EXPERTS>(scores, gating_output + i * NUM_EXPERTS);\n+\n+      // find max score per group\n+      for (int64_t g = 0; g < num_groups; ++g) {\n+        float gmax = -std::numeric_limits<float>::infinity();\n+        for (int64_t e = 0; e < num_experts_per_group; ++e) {\n+          gmax = std::max(gmax, scores[g * num_experts_per_group + e]);\n+        }\n+        queue[g] = {gmax, g};\n+      }\n+\n+      // find group topk\n+      std::partial_sort(\n+          queue.begin(), queue.begin() + topk_group, queue.end(), [](const elem_t& x, const elem_t& y) -> bool {\n+            return x.first > y.first;\n+          });\n+\n+      for (int64_t g = 0; g < topk_group; ++g) {\n+        int32_t group_idx = queue[g].second;\n+        for (int64_t e = 0; e < num_experts_per_group; ++e) {\n+          int32_t expert_idx = group_idx * num_experts_per_group + e;\n+          queue2[g * num_experts_per_group + e] = {scores[expert_idx], expert_idx};\n+        }\n+      }\n+\n+      // find global topk\n+      std::partial_sort(\n+          queue2.begin(), queue2.begin() + topk, queue2.end(), [](const elem_t& x, const elem_t& y) -> bool {\n+            return x.first > y.first;\n+          });\n+\n+      for (int64_t j = 0; j < topk; ++j) {\n+        topk_weights[i * topk + j] = queue2[j].first;\n+        topk_ids[i * topk + j] = queue2[j].second;\n+      }\n+\n+      if (renormalize) {\n+        float sum = 0.f;\n+        for (int64_t j = 0; j < topk; ++j) {\n+          sum += topk_weights[i * topk + j];\n+        }\n+        float scale = 1.f / sum;\n+        for (int64_t j = 0; j < topk; ++j) {\n+          topk_weights[i * topk + j] *= scale;\n+        }\n+      }\n+    }\n+  });\n+}\n+\n+template <typename scalar_t, int SIZE>\n+inline void sigmoid(float* __restrict__ out, const scalar_t* __restrict__ input) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+\n+  const fVec one = fVec(1.f);\n+\n+  constexpr int kVecSize = bVec::size();\n+  for (int d = 0; d < SIZE; d += kVecSize) {\n+    bVec x_bvec = bVec::loadu(input + d);\n+    fVec x_fvec0, x_fvec1;\n+    std::tie(x_fvec0, x_fvec1) = at::vec::convert_to_float(x_bvec);\n+\n+    x_fvec0 = one / (one + x_fvec0.neg().exp_u20());\n+    x_fvec1 = one / (one + x_fvec1.neg().exp_u20());\n+\n+    x_fvec0.store(out + d);\n+    x_fvec1.store(out + d + fVec::size());\n+  }\n+}\n+\n+template <typename scalar_t, int SIZE>\n+inline void\n+apply_bias(float* __restrict__ scores2, const float* __restrict__ scores, const scalar_t* __restrict__ bias) {\n+  using bVec = at::vec::Vectorized<scalar_t>;\n+  using fVec = at::vec::Vectorized<float>;\n+  for (int d = 0; d < SIZE; d += bVec::size()) {\n+    bVec bias_vec = bVec::loadu(bias + d);\n+    fVec bias0, bias1;\n+    std::tie(bias0, bias1) = at::vec::convert_to_float(bias_vec);\n+\n+    fVec x0 = fVec::loadu(scores + d) + bias0;\n+    fVec x1 = fVec::loadu(scores + d + fVec::size()) + bias1;\n+    x0.store(scores2 + d);\n+    x1.store(scores2 + d + fVec::size());\n+  }\n+}\n+\n+template <typename scalar_t, int NUM_EXPERTS, int TOPK>\n+void biased_grouped_topk_kernel_impl(\n+    float* __restrict__ topk_weights,\n+    int32_t* __restrict__ topk_ids,\n+    const scalar_t* __restrict__ gating_output,\n+    const scalar_t* __restrict__ bias,\n+    int64_t num_tokens,\n+    int64_t num_groups,\n+    int64_t topk_group,\n+    bool renormalize) {\n+  using Vec = at::vec::Vectorized<float>;\n+\n+  const int64_t num_experts_per_group = NUM_EXPERTS / num_groups;\n+  at::parallel_for(0, num_tokens, 0, [&](int64_t begin, int64_t end) {\n+    // scores: sigmoid\n+    alignas(64) float scores[NUM_EXPERTS];\n+    // scores for choice: sigmoid + bias\n+    alignas(64) float scores2[NUM_EXPERTS];\n+\n+    using elem_t = std::pair<float, int32_t>;\n+    std::vector<elem_t> queue(num_groups);\n+    std::vector<elem_t> queue2(topk_group * num_experts_per_group);\n+\n+    for (int64_t i = begin; i < end; ++i) {\n+      // do sigmoid to get scores\n+      sigmoid<scalar_t, NUM_EXPERTS>(scores, gating_output + i * NUM_EXPERTS);\n+      apply_bias<scalar_t, NUM_EXPERTS>(scores2, scores, bias);\n+\n+      for (int64_t g = 0; g < num_groups; ++g) {\n+        // find the max\n+        float gmax = at::vec::reduce_all<float>(\n+            [](Vec& x, Vec& y) { return at::vec::maximum(x, y); },\n+            scores2 + g * num_experts_per_group,\n+            num_experts_per_group);\n+\n+        // find position of first max,\n+        // note that we may have multiple max values.\n+        int first_max_idx = -1;\n+        for (int64_t e = 0; e < num_experts_per_group; ++e) {\n+          if (scores2[g * num_experts_per_group + e] == gmax) {\n+            first_max_idx = g * num_experts_per_group + e;\n+            break;\n+          }\n+        }\n+\n+        // find the 2nd max\n+        scores2[first_max_idx] = -std::numeric_limits<float>::infinity();\n+        float gmax2 = at::vec::reduce_all<float>(\n+            [](Vec& x, Vec& y) { return at::vec::maximum(x, y); },\n+            scores2 + g * num_experts_per_group,\n+            num_experts_per_group);\n+        // restore scores for choice\n+        scores2[first_max_idx] = gmax;\n+\n+        queue[g] = {gmax + gmax2, g};\n+      }\n+\n+      // find group topk\n+      std::partial_sort(\n+          queue.begin(), queue.begin() + topk_group, queue.end(), [](const elem_t& x, const elem_t& y) -> bool {\n+            return x.first > y.first;\n+          });\n+\n+      for (int64_t g = 0; g < topk_group; ++g) {\n+        int32_t group_idx = queue[g].second;\n+        for (int64_t e = 0; e < num_experts_per_group; ++e) {\n+          int32_t expert_idx = group_idx * num_experts_per_group + e;\n+          queue2[g * num_experts_per_group + e] = {scores2[expert_idx], expert_idx};\n+        }\n+      }\n+\n+      // find global topk\n+      std::partial_sort(\n+          queue2.begin(), queue2.begin() + TOPK, queue2.end(), [](const elem_t& x, const elem_t& y) -> bool {\n+            return x.first > y.first;\n+          });\n+\n+      for (int j = 0; j < TOPK; ++j) {\n+        int32_t index = queue2[j].second;\n+        topk_ids[i * TOPK + j] = index;\n+        topk_weights[i * TOPK + j] = scores[index];\n+      }\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+      if (renormalize) {\n+        __mmask16 mask = (1ULL << TOPK) - 1;\n+        __m512 x = _mm512_maskz_loadu_ps(mask, topk_weights + i * TOPK);\n+        float sum = _mm512_reduce_add_ps(x);\n+        __m512 vscale = _mm512_set1_ps(1.f / sum);\n+        __m512 y = _mm512_mul_ps(x, vscale);\n+        _mm512_mask_storeu_ps(topk_weights + i * TOPK, mask, y);\n+      }\n+#else\n+      if (renormalize) {\n+        float sum = 0.f;\n+        for (int64_t j = 0; j < TOPK; ++j) {\n+          sum += topk_weights[i * TOPK + j];\n+        }\n+        float scale = 1.f / sum;\n+        for (int64_t j = 0; j < TOPK; ++j) {\n+          topk_weights[i * TOPK + j] *= scale;\n+        }\n+      }\n+#endif\n+    }\n+  });\n+}\n+\n+#define LAUNCH_GROUPED_TOPK_KERNEL(NE)    \\\n+  grouped_topk_kernel_impl<scalar_t, NE>( \\\n+      topk_weights.data_ptr<float>(),     \\\n+      topk_ids.data_ptr<int32_t>(),       \\\n+      gating_output.data_ptr<scalar_t>(), \\\n+      num_tokens,                         \\\n+      topk,                               \\\n+      num_expert_group,                   \\\n+      topk_group,                         \\\n+      renormalize);\n+\n+#define LAUNCH_BIASED_GROUPED_TOPK_KERNEL(NE, NTOPK)    \\\n+  biased_grouped_topk_kernel_impl<scalar_t, NE, NTOPK>( \\\n+      topk_weights.data_ptr<float>(),                   \\\n+      topk_ids.data_ptr<int32_t>(),                     \\\n+      gating_output.data_ptr<scalar_t>(),               \\\n+      correction_bias.data_ptr<scalar_t>(),             \\\n+      num_tokens,                                       \\\n+      num_expert_group,                                 \\\n+      topk_group,                                       \\\n+      renormalize);\n+\n+}  // anonymous namespace\n+\n+// grouped topk for DeepSeek V2\n+std::tuple<at::Tensor, at::Tensor> grouped_topk_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& gating_output,\n+    int64_t topk,\n+    bool renormalize,\n+    int64_t num_expert_group,\n+    int64_t topk_group) {\n+  RECORD_FUNCTION(\"sgl-kernel::grouped_topk_cpu\", std::vector<c10::IValue>({hidden_states, gating_output}));\n+  CHECK_INPUT(gating_output);\n+\n+  const auto st = hidden_states.scalar_type();\n+  CHECK_EQ(gating_output.scalar_type(), st);\n+\n+  int64_t num_tokens = hidden_states.size(0);\n+  int64_t num_experts = gating_output.size(1);\n+  TORCH_CHECK(gating_output.size(0) == num_tokens, \"Number of tokens mismatch\");\n+  at::Tensor topk_weights = at::empty({num_tokens, topk}, hidden_states.options().dtype(at::kFloat));\n+  at::Tensor topk_ids = at::empty({num_tokens, topk}, hidden_states.options().dtype(at::kInt));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"grouped_topk_kernel\", [&] {\n+    switch (num_experts) {\n+      case 1:\n+        LAUNCH_GROUPED_TOPK_KERNEL(1);\n+        break;\n+      case 2:\n+        LAUNCH_GROUPED_TOPK_KERNEL(2);\n+        break;\n+      case 4:\n+        LAUNCH_GROUPED_TOPK_KERNEL(4);\n+        break;\n+      case 8:\n+        LAUNCH_GROUPED_TOPK_KERNEL(8);\n+        break;\n+      case 16:\n+        LAUNCH_GROUPED_TOPK_KERNEL(16);\n+        break;\n+      case 32:\n+        LAUNCH_GROUPED_TOPK_KERNEL(32);\n+        break;\n+      case 64:\n+        LAUNCH_GROUPED_TOPK_KERNEL(64);\n+        break;\n+      case 128:\n+        LAUNCH_GROUPED_TOPK_KERNEL(128);\n+        break;\n+      case 160:\n+        LAUNCH_GROUPED_TOPK_KERNEL(160);\n+        break;\n+      case 256:\n+        LAUNCH_GROUPED_TOPK_KERNEL(256);\n+        break;\n+      default:\n+        TORCH_CHECK(false, \"Unexpected num_experts: \", num_experts);\n+    }\n+  });\n+  return std::make_tuple(topk_weights, topk_ids);\n+}\n+\n+// biased grouped topk DeepSeek V3/R1\n+std::tuple<at::Tensor, at::Tensor> biased_grouped_topk_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& gating_output,\n+    at::Tensor& correction_bias,\n+    int64_t topk,\n+    bool renormalize,\n+    int64_t num_expert_group,\n+    int64_t topk_group) {\n+  RECORD_FUNCTION(\n+      \"sgl-kernel::biased_grouped_topk_cpu\", std::vector<c10::IValue>({hidden_states, gating_output, correction_bias}));\n+\n+  CHECK_INPUT(gating_output);\n+  CHECK_INPUT(correction_bias);\n+\n+  const auto st = hidden_states.scalar_type();\n+  CHECK_EQ(gating_output.scalar_type(), st);\n+  CHECK_EQ(correction_bias.scalar_type(), st);\n+\n+  int64_t num_tokens = hidden_states.size(0);\n+  int64_t num_experts = gating_output.size(1);\n+  TORCH_CHECK(gating_output.size(0) == num_tokens, \"Number of tokens mismatch\");\n+  TORCH_CHECK(correction_bias.numel() == num_experts, \"Bias shape mismatch\");\n+  at::Tensor topk_weights = at::empty({num_tokens, topk}, hidden_states.options().dtype(at::kFloat));\n+  at::Tensor topk_ids = at::empty({num_tokens, topk}, hidden_states.options().dtype(at::kInt));\n+\n+  AT_DISPATCH_REDUCED_FLOATING_TYPES(st, \"biased_grouped_topk_kernel\", [&] {\n+    // NOW only support DSv3 configs\n+    TORCH_CHECK(topk == 8, \"Unexpected topk: \", topk);\n+    switch (num_experts) {\n+      case 256:\n+        LAUNCH_BIASED_GROUPED_TOPK_KERNEL(256, 8);\n+        break;\n+      default:\n+        TORCH_CHECK(false, \"Unexpected num_experts: \", num_experts);\n+    }\n+  });\n+  return std::make_tuple(topk_weights, topk_ids);\n+}\ndiff --git a/sgl-kernel/csrc/cpu/torch_extension_cpu.cpp b/sgl-kernel/csrc/cpu/torch_extension_cpu.cpp\nnew file mode 100644\nindex 000000000..6b7cc1d39\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/torch_extension_cpu.cpp\n@@ -0,0 +1,224 @@\n+/* Copyright 2025 SGLang Team. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <ATen/ATen.h>\n+#include <torch/extension.h>\n+#include <torch/library.h>\n+\n+#include \"shm.h\"\n+\n+// silu_and_mul\n+at::Tensor silu_and_mul_cpu(at::Tensor& input);\n+\n+// rmsnorm\n+at::Tensor rmsnorm_cpu(at::Tensor& input, at::Tensor& weight, double eps);\n+\n+// fused_add_rmsnorm\n+void fused_add_rmsnorm_cpu(at::Tensor& input, at::Tensor& residual, at::Tensor& weight, double eps);\n+\n+// topk\n+std::tuple<at::Tensor, at::Tensor> grouped_topk_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& gating_output,\n+    int64_t topk,\n+    bool renormalize,\n+    int64_t num_expert_group,\n+    int64_t topk_group);\n+\n+std::tuple<at::Tensor, at::Tensor> biased_grouped_topk_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& gating_output,\n+    at::Tensor& correction_bias,\n+    int64_t topk,\n+    bool renormalize,\n+    int64_t num_expert_group,\n+    int64_t topk_group);\n+\n+// attention\n+void decode_attention_cpu(\n+    at::Tensor& query,\n+    at::Tensor& output,\n+    at::Tensor& k_cache,\n+    at::Tensor& v_cahce,\n+    at::Tensor& attn_logits,\n+    at::Tensor& req_to_token,\n+    at::Tensor& req_pool_indices,\n+    at::Tensor& seq_lens,\n+    double sm_scale,\n+    double logit_cap);\n+\n+void extend_attention_cpu(\n+    at::Tensor& q_extend,\n+    at::Tensor& k_extend,\n+    at::Tensor& v_extend,\n+    at::Tensor& o_extend,\n+    at::Tensor& k_buffer,\n+    at::Tensor& v_buffer,\n+    at::Tensor& req_to_token,\n+    at::Tensor& req_pool_indices,\n+    at::Tensor& seq_lens,\n+    at::Tensor& extend_seq_lens,\n+    at::Tensor& extend_start_loc,\n+    int64_t max_len_extend,\n+    double sm_scale,\n+    double logit_cap);\n+\n+// weight prepack\n+at::Tensor convert_weight_packed(at::Tensor& weight);\n+\n+// quant\n+std::tuple<at::Tensor, at::Tensor> per_token_quant_int8_cpu(at::Tensor& A);\n+\n+// gemm\n+at::Tensor weight_packed_linear(at::Tensor& mat1, at::Tensor& mat2, std::optional<at::Tensor>& bias, bool is_vnni);\n+\n+// igemm\n+at::Tensor int8_scaled_mm_cpu(\n+    at::Tensor& mat1,\n+    at::Tensor& mat2,\n+    at::Tensor& scales1,\n+    at::Tensor& scales2,\n+    std::optional<at::Tensor>& bias,\n+    at::ScalarType out_dtype,\n+    bool is_vnni);\n+\n+// quant + igemm\n+at::Tensor int8_scaled_mm_with_quant(\n+    at::Tensor& mat1,\n+    at::Tensor& mat2,\n+    at::Tensor& scales2,\n+    std::optional<at::Tensor>& bias,\n+    at::ScalarType out_dtype,\n+    bool is_vnni);\n+\n+// bmm\n+void bmm_cpu(at::Tensor& out, at::Tensor& mat1, at::Tensor& mat2, bool is_vnni, std::optional<at::Tensor>& scale);\n+\n+// fused moe\n+at::Tensor fused_experts_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& w1,\n+    at::Tensor& w2,\n+    at::Tensor& topk_weights,\n+    at::Tensor& topk_ids,\n+    bool inplace,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& w1_scale,\n+    std::optional<at::Tensor>& w2_scale,\n+    std::optional<at::Tensor>& a1_scale,\n+    std::optional<at::Tensor>& a2_scale,\n+    bool is_vnni);\n+\n+at::Tensor shared_expert_cpu(\n+    at::Tensor& hidden_states,\n+    at::Tensor& w1,\n+    at::Tensor& w2,\n+    at::Tensor& fused_experts_out,\n+    double routed_scaling_factor,\n+    bool inplace,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& w1_scale,\n+    std::optional<at::Tensor>& w2_scale,\n+    std::optional<at::Tensor>& a1_scale,\n+    std::optional<at::Tensor>& a2_scale,\n+    bool is_vnni);\n+\n+// weight absorption\n+std::tuple<at::Tensor, at::Tensor, at::Tensor> qkv_proj_with_rope(\n+    at::Tensor& hidden_states,\n+    at::Tensor& q_a_proj_weight,\n+    at::Tensor& q_b_proj_weight,\n+    at::Tensor& kv_a_proj_weight,\n+    at::Tensor& w_kc,\n+    at::Tensor& q_a_layernorm_weight,\n+    at::Tensor& kv_a_layernorm_weight,\n+    at::Tensor& positions,\n+    at::Tensor& cos_sin_cache,\n+    double eps,\n+    bool use_int8_w8a8,\n+    std::optional<at::Tensor>& q_a_proj_scale,\n+    std::optional<at::Tensor>& q_b_proj_scale,\n+    std::optional<at::Tensor>& kv_a_proj_scale,\n+    bool is_vnni);\n+\n+// shared memory init\n+void initialize(int size, int rank);\n+\n+// shared mmeory all_reduce\n+void shm_allreduce(at::Tensor& data, c10::intrusive_ptr<c10d::ProcessGroup> process_group, py::object op);\n+\n+// shared memory all_gather\n+at::Tensor shm_allgather(at::Tensor& data, c10::intrusive_ptr<c10d::ProcessGroup> process_group, int dim);\n+\n+// rope\n+std::tuple<at::Tensor, at::Tensor>\n+rotary_position_embedding_cpu(at::Tensor& t_pos, at::Tensor& q_pe, at::Tensor& k_pe, at::Tensor& t_emb_pos);\n+\n+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n+  // activation\n+  m.def(\"silu_and_mul_cpu\", &silu_and_mul_cpu, \"SiLU and mul for CPU\");\n+\n+  // norm\n+  m.def(\"rmsnorm_cpu\", &rmsnorm_cpu, \"Root mean square normalization for CPU\");\n+  m.def(\"fused_add_rmsnorm_cpu\", &fused_add_rmsnorm_cpu, \"Fused add root mean square normalization for CPU\");\n+\n+  // topk\n+  m.def(\"grouped_topk_cpu\", &grouped_topk_cpu, \"Grouped TopK for CPU\");\n+\n+  // biased group topk\n+  m.def(\"biased_grouped_topk_cpu\", &biased_grouped_topk_cpu, \"Biased Grouped TopK for CPU\");\n+\n+  // decode\n+  m.def(\"decode_attention_cpu\", &decode_attention_cpu, \"Attention decoding for CPU\");\n+\n+  // extend\n+  m.def(\"extend_attention_cpu\", &extend_attention_cpu, \"Attention extend for CPU\");\n+\n+  // weight prepack\n+  m.def(\"convert_weight_packed\", &convert_weight_packed, \"prepack weight to vnni format for intel AMX\");\n+\n+  // quant\n+  m.def(\"per_token_quant_int8_cpu\", &per_token_quant_int8_cpu, \"dynamic quantization for CPU\");\n+\n+  // gemm\n+  m.def(\"weight_packed_linear\", &weight_packed_linear, \"weight packed linear for intel AMX\");\n+\n+  // igemm\n+  m.def(\"int8_scaled_mm_cpu\", &int8_scaled_mm_cpu, \"int8 weight packed linear for intel AMX\");\n+\n+  // quant + igemm\n+  m.def(\n+      \"int8_scaled_mm_with_quant\", &int8_scaled_mm_with_quant, \"fused per row quant and int8 scaled mm for intel AMX\");\n+\n+  // bmm\n+  m.def(\"bmm_cpu\", &bmm_cpu, \"bmm kernel for intel AMX\");\n+\n+  // moe\n+  m.def(\"fused_experts_cpu\", &fused_experts_cpu, \"fused moe kernel for CPU\");\n+\n+  // weight absorption\n+  m.def(\"qkv_proj_with_rope\", &qkv_proj_with_rope, \"fused qkv projection kernel with weight absorption for intel AMX\");\n+\n+  // shared expert\n+  m.def(\"shared_expert_cpu\", &shared_expert_cpu, \"shared expert kernel for CPU\");\n+\n+  // all reduce\n+  m.def(\"initialize\", &initialize, \"shared memory initialization for CPU\");\n+  m.def(\"shm_allreduce\", &shm_allreduce, \"low latency all_reduce implementation for CPU\");\n+  m.def(\"shm_allgather\", &shm_allgather, \"low latency all_gather implementation for CPU\");\n+\n+  // rope\n+  m.def(\"rotary_position_embedding_cpu\", &rotary_position_embedding_cpu, \"rotary position embedding for CPU\");\n+}\ndiff --git a/sgl-kernel/csrc/cpu/vec.h b/sgl-kernel/csrc/cpu/vec.h\nnew file mode 100644\nindex 000000000..e058bd716\n--- /dev/null\n+++ b/sgl-kernel/csrc/cpu/vec.h\n@@ -0,0 +1,115 @@\n+#pragma once\n+\n+#if defined(__AVX512F__) && defined(__AVX512BF16__) && defined(__AMX_BF16__)\n+#define CPU_CAPABILITY_AVX512\n+#endif\n+\n+#include <ATen/cpu/vec/functional.h>\n+#include <ATen/cpu/vec/vec.h>\n+\n+namespace {\n+\n+using namespace at::vec;\n+\n+template <typename scalar_t, typename std::enable_if_t<is_reduced_floating_point_v<scalar_t>, int> = 0>\n+inline Vectorized<scalar_t> convert_from_float_ext(const Vectorized<float>& a, const Vectorized<float>& b) {\n+  return at::vec::convert_from_float<scalar_t>(a, b);\n+}\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+\n+// `at::vec::convert_from_float<>` from PyTorch doesn't have avx512-bf16 intrinsics\n+// use native instruction for bfloat16->float32 conversion\n+template <>\n+inline Vectorized<at::BFloat16>\n+convert_from_float_ext<at::BFloat16>(const Vectorized<float>& a, const Vectorized<float>& b) {\n+  return (__m512i)(_mm512_cvtne2ps_pbh(__m512(b), __m512(a)));\n+}\n+\n+#define CVT_BF16_TO_FP32(a) _mm512_castsi512_ps(_mm512_slli_epi32(_mm512_cvtepu16_epi32(a), 16))\n+\n+#define CVT_FP16_TO_FP32(a) _mm512_cvtps_ph(a, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC))\n+\n+#endif\n+\n+// vector to scalar reduction\n+#if defined(CPU_CAPABILITY_AVX512) && 0\n+inline float vec_reduce_sum(const Vectorized<float>& a) {\n+  return _mm512_reduce_add_ps(__m512(a));\n+}\n+\n+inline float vec_reduce_max(const Vectorized<float>& a) {\n+  return _mm512_reduce_max_ps(__m512(a));\n+}\n+#else\n+inline float vec_reduce_sum(const Vectorized<float>& a) {\n+  return vec_reduce_all([](Vectorized<float>& x, Vectorized<float>& y) { return x + y; }, a);\n+}\n+\n+inline float vec_reduce_max(const Vectorized<float>& a) {\n+  return vec_reduce_all([](Vectorized<float>& x, Vectorized<float>& y) { return maximum(x, y); }, a);\n+}\n+#endif\n+\n+// https://github.com/InternLM/lmdeploy/blob/086481ed84b59bee3b8e4274e5fc69620040c048/lmdeploy/pytorch/kernels/cuda/w8a8_triton_kernels.py#L282\n+template <typename scalar_t>\n+inline void\n+quantize_row_int8(uint8_t* __restrict__ Aq, float& As, const scalar_t* __restrict__ A, int64_t K, float eps = 1e-7) {\n+  float amax = 0.f;  // absolute max\n+  for (int64_t k = 0; k < K; ++k) {\n+    const float val = static_cast<float>(A[k]);\n+    amax = std::max(amax, std::abs(val));\n+  }\n+\n+  amax = std::max(amax, eps);\n+  const float scale = amax / 127;\n+  const float inv_scale = 127 / amax;\n+\n+  for (int64_t k = 0; k < K; ++k) {\n+    const float val = static_cast<float>(A[k]) * inv_scale;\n+    Aq[k] = (uint8_t)(std::round(val)) + 128;\n+  }\n+  As = scale;\n+}\n+\n+#if defined(CPU_CAPABILITY_AVX512)\n+template <>\n+inline void quantize_row_int8<at::BFloat16>(\n+    uint8_t* __restrict__ Aq, float& As, const at::BFloat16* __restrict__ A, int64_t K, float eps) {\n+  const __m512 signBit = _mm512_set1_ps(-0.0f);\n+  const __m512i off = _mm512_set1_epi32(128);\n+\n+  // K is 32x, no remainder\n+  float amax = 0.f;\n+  __m512 vamax0 = _mm512_set1_ps(0.f);\n+  __m512 vamax1 = _mm512_set1_ps(0.f);\n+  for (int64_t k = 0; k < K; k += 32) {\n+    __m512i va = _mm512_loadu_si512((void*)(A + k));\n+    __m512 va0 = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(va, 0));\n+    __m512 va1 = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(va, 1));\n+    vamax0 = _mm512_max_ps(vamax0, _mm512_andnot_ps(signBit, va0));\n+    vamax1 = _mm512_max_ps(vamax1, _mm512_andnot_ps(signBit, va1));\n+  }\n+  amax = _mm512_reduce_max_ps(_mm512_max_ps(vamax0, vamax1));\n+  amax = std::max(amax, eps);\n+  const float scale = amax / 127;\n+  const float inv_scale = 127 / amax;\n+  const __m512 vd = _mm512_set1_ps(inv_scale);\n+\n+  for (int64_t k = 0; k < K; k += 32) {\n+    __m512i va = _mm512_loadu_si512((void*)(A + k));\n+    __m512 va0 = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(va, 0));\n+    __m512 va1 = CVT_BF16_TO_FP32(_mm512_extracti32x8_epi32(va, 1));\n+    va0 = _mm512_mul_ps(va0, vd);\n+    va1 = _mm512_mul_ps(va1, vd);\n+    va0 = _mm512_roundscale_ps(va0, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n+    va1 = _mm512_roundscale_ps(va1, (_MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));\n+    __m128i i0 = _mm512_cvtepi32_epi8(_mm512_add_epi32(_mm512_cvtps_epi32(va0), off));\n+    __m128i i1 = _mm512_cvtepi32_epi8(_mm512_add_epi32(_mm512_cvtps_epi32(va1), off));\n+    _mm256_storeu_si256(reinterpret_cast<__m256i*>(Aq + k), _mm256_set_m128i(i1, i0));\n+  }\n+  As = scale;\n+}\n+#endif\n+\n+}  // anonymous namespace\ndiff --git a/sgl-kernel/setup_cpu.py b/sgl-kernel/setup_cpu.py\nnew file mode 100644\nindex 000000000..04e06cb1a\n--- /dev/null\n+++ b/sgl-kernel/setup_cpu.py\n@@ -0,0 +1,95 @@\n+# Copyright 2025 SGLang Team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+import os\n+import shutil\n+import sys\n+from pathlib import Path\n+\n+import torch\n+from setuptools import find_packages, setup\n+from setuptools.command.build_py import build_py\n+from torch.utils.cpp_extension import BuildExtension, CppExtension\n+\n+root = Path(__file__).parent.resolve()\n+\n+if \"bdist_wheel\" in sys.argv and \"--plat-name\" not in sys.argv:\n+    sys.argv.extend([\"--plat-name\", \"manylinux2014_x86_64\"])\n+\n+\n+def _get_version():\n+    with open(root / \"pyproject.toml\") as f:\n+        for line in f:\n+            if line.startswith(\"version\"):\n+                return line.split(\"=\")[1].strip().strip('\"')\n+\n+\n+operator_namespace = \"sgl_kernel\"\n+include_dirs = []\n+\n+sources = [\n+    \"csrc/cpu/activation.cpp\",\n+    \"csrc/cpu/bmm.cpp\",\n+    \"csrc/cpu/decode.cpp\",\n+    \"csrc/cpu/extend.cpp\",\n+    \"csrc/cpu/gemm.cpp\",\n+    \"csrc/cpu/gemm_int8.cpp\",\n+    \"csrc/cpu/moe.cpp\",\n+    \"csrc/cpu/moe_int8.cpp\",\n+    \"csrc/cpu/norm.cpp\",\n+    \"csrc/cpu/qkv_proj.cpp\",\n+    \"csrc/cpu/topk.cpp\",\n+    \"csrc/cpu/interface.cpp\",\n+    \"csrc/cpu/shm.cpp\",\n+    \"csrc/cpu/torch_extension_cpu.cpp\",\n+]\n+\n+extra_compile_args = {\n+    \"cxx\": [\n+        \"-O3\",\n+        \"-Wno-unknown-pragmas\",\n+        \"-march=native\",\n+        \"-fopenmp\",\n+    ]\n+}\n+libraries = [\"c10\", \"torch\", \"torch_python\"]\n+cmdclass = {\n+    \"build_ext\": BuildExtension.with_options(use_ninja=True),\n+}\n+Extension = CppExtension\n+\n+extra_link_args = [\"-Wl,-rpath,$ORIGIN/../../torch/lib\", \"-L/usr/lib/x86_64-linux-gnu\"]\n+\n+ext_modules = [\n+    Extension(\n+        name=\"sgl_kernel.common_ops\",\n+        sources=sources,\n+        include_dirs=include_dirs,\n+        extra_compile_args=extra_compile_args,\n+        libraries=libraries,\n+        extra_link_args=extra_link_args,\n+        py_limited_api=True,\n+    ),\n+]\n+\n+setup(\n+    name=\"sgl-kernel\",\n+    version=_get_version(),\n+    packages=find_packages(where=\"python\"),\n+    package_dir={\"\": \"python\"},\n+    ext_modules=ext_modules,\n+    cmdclass=cmdclass,\n+    options={\"bdist_wheel\": {\"py_limited_api\": \"cp39\"}},\n+)",
  "apis": [
    "torch.ops.sgl_kernel.silu_and_mul_cpu",
    "torch.ops.sgl_kernel.bmm_cpu"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/sgl-kernel/python/sgl_kernel/gemm.py",
    "/path/to/repos/sglang/sgl-kernel/python/sgl_kernel/moe.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit adds several new optimized native kernel implementations in non-test CPU source files (activation.cpp, bmm.cpp, common.h, and decode.cpp). The additions include performance-critical code (e.g., vectorized operations and parallel processing) and enhance the efficiency of high-level APIs. The commit is clearly aimed at improving CPU performance via optimized low-level kernels \u2013 not merely refactoring, bug fixing, or new feature addition. Therefore, it clearly qualifies as a performance/optimization improvement commit.",
  "llm_api_reason": "This commit adds new native CPU kernels in the sgl-kernel module \u2013 notably an activation routine (silu_and_mul_cpu) and a batched matrix multiplication routine (bmm_cpu) (along with several lower\u2010level helper kernels in common.h and decode.cpp). Although these changes are in C++ code, they will be exposed to Python via torch.ops (i.e. torch.ops.sgl_kernel.silu_and_mul_cpu and torch.ops.sgl_kernel.bmm_cpu), which are the Python-accessible APIs that rely on these new optimized implementations."
}