{
  "commit_hash": "c087ddd6865a52634326a05af66429cb5531cd16",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6627",
  "pr_date": "2025-05-28",
  "timeline_text": "Copy link Contributor yuan-luo commented May 26, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation In ep_moe kernel pre_reorder_triton_kernel and post_reorder_triton_kernel , every inner loop recomputes offset = start_offset + tl.arange(...) The optimization is to create a constant once: vec = tl.arange(0, BLOCK_SIZE) and inside the loop use idx = start_offset + vec The benefit is one less instruction each iteration. The warp scheduler can vectorize the access pattern. Per benchmark result the kernel gains 10-15% performance. Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 2 Alcanderian and Swipe4057 reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions yuan-luo requested review from merrymercy , Ying1123 , zhyncs , ispobock , HaiShaw , ch-wan and BBuf as code owners May 26, 2025 12:52 Copy link Contributor Author yuan-luo commented May 26, 2025 The benchmark script: # test_pre_reorder_triton_kernel.py\n\nimport torch\nimport triton\nimport triton.language as tl\nimport time\n\n@triton.jit\ndef pre_reorder_triton_kernel(\n    input_ptr,\n    gateup_input_ptr,\n    src2dst_ptr,\n    topk_ids_ptr,\n    a1_scales_ptr,\n    start_expert_id,\n    end_expert_id,\n    topk,\n    hidden_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    OutDtype = gateup_input_ptr.dtype.element_ty\n\n    src_idx = tl.program_id(0)\n    src2dst_ptr = src2dst_ptr + src_idx * topk\n    topk_ids_ptr = topk_ids_ptr + src_idx * topk\n    src_ptr = input_ptr + src_idx * hidden_size\n\n    vec = tl.arange(0, BLOCK_SIZE)\n\n    for idx in range(topk):\n        expert_id = tl.load(topk_ids_ptr + idx)\n        if expert_id >= start_expert_id and expert_id <= end_expert_id:\n            if a1_scales_ptr is not None:\n                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)\n            else:\n                scale = 1.0\n\n            dst_idx = tl.load(src2dst_ptr + idx)\n            dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n                offset = start_offset + vec\n                mask = offset < hidden_size\n                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n                out_data = (in_data * scale).to(OutDtype)\n                tl.store(dst_ptr + offset, out_data, mask=mask)\n\ndef main():\n    BATCH = 640\n    TOPK = 2\n    HIDDEN_SIZE = 1024\n    BLOCK_SIZE = 512\n    EXPERT_RANGE = (0, 255)\n\n    input_ptr = torch.randn(BATCH, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    gateup_input_ptr = torch.zeros(BATCH * TOPK, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    src2dst_ptr = torch.randint(0, BATCH * TOPK, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    topk_ids_ptr = torch.randint(EXPERT_RANGE[0], EXPERT_RANGE[1] + 1, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    a1_scales_ptr = torch.rand(EXPERT_RANGE[1] - EXPERT_RANGE[0] + 1, dtype=torch.float32, device='cuda')\n\n    input_ptr = input_ptr.view(-1)\n    gateup_input_ptr = gateup_input_ptr.view(-1)\n    src2dst_ptr = src2dst_ptr.view(-1)\n    topk_ids_ptr = topk_ids_ptr.view(-1)\n\n    def run_kernel():\n        pre_reorder_triton_kernel_fast[(BATCH,)](\n            input_ptr,\n            gateup_input_ptr,\n            src2dst_ptr,\n            topk_ids_ptr,\n            a1_scales_ptr,\n            EXPERT_RANGE[0],\n            EXPERT_RANGE[1],\n            TOPK,\n            HIDDEN_SIZE,\n            BLOCK_SIZE\n        )\n\n    for _ in range(10):\n        run_kernel()\n    torch.cuda.synchronize()\n\n    # Benchmark\n    n_iter = 100000\n    start = time.time()\n    for _ in range(n_iter):\n        run_kernel()\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Average execution time over {n_iter} iterations: {(end - start) / n_iter * 1000:.3f} ms\")\n\nif __name__ == \"__main__\":\n    main() Before the fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.026 ms After fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.024 ms All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . yuan-luo changed the title Refine pre_reorder_triton_kernel sightly to improve performance Refine pre_reorder_triton_kernel slightly to improve performance May 26, 2025 yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from d4703fb to eec86de Compare May 26, 2025 12:56 Copy link Collaborator BBuf commented May 26, 2025 The benchmark script: # test_pre_reorder_triton_kernel.py\n\nimport torch\nimport triton\nimport triton.language as tl\nimport time\n\n@triton.jit\ndef pre_reorder_triton_kernel(\n    input_ptr,\n    gateup_input_ptr,\n    src2dst_ptr,\n    topk_ids_ptr,\n    a1_scales_ptr,\n    start_expert_id,\n    end_expert_id,\n    topk,\n    hidden_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    OutDtype = gateup_input_ptr.dtype.element_ty\n\n    src_idx = tl.program_id(0)\n    src2dst_ptr = src2dst_ptr + src_idx * topk\n    topk_ids_ptr = topk_ids_ptr + src_idx * topk\n    src_ptr = input_ptr + src_idx * hidden_size\n\n    vec = tl.arange(0, BLOCK_SIZE)\n\n    for idx in range(topk):\n        expert_id = tl.load(topk_ids_ptr + idx)\n        if expert_id >= start_expert_id and expert_id <= end_expert_id:\n            if a1_scales_ptr is not None:\n                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)\n            else:\n                scale = 1.0\n\n            dst_idx = tl.load(src2dst_ptr + idx)\n            dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n                offset = start_offset + vec\n                mask = offset < hidden_size\n                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n                out_data = (in_data * scale).to(OutDtype)\n                tl.store(dst_ptr + offset, out_data, mask=mask)\n\ndef main():\n    BATCH = 640\n    TOPK = 2\n    HIDDEN_SIZE = 1024\n    BLOCK_SIZE = 512\n    EXPERT_RANGE = (0, 255)\n\n    input_ptr = torch.randn(BATCH, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    gateup_input_ptr = torch.zeros(BATCH * TOPK, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    src2dst_ptr = torch.randint(0, BATCH * TOPK, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    topk_ids_ptr = torch.randint(EXPERT_RANGE[0], EXPERT_RANGE[1] + 1, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    a1_scales_ptr = torch.rand(EXPERT_RANGE[1] - EXPERT_RANGE[0] + 1, dtype=torch.float32, device='cuda')\n\n    input_ptr = input_ptr.view(-1)\n    gateup_input_ptr = gateup_input_ptr.view(-1)\n    src2dst_ptr = src2dst_ptr.view(-1)\n    topk_ids_ptr = topk_ids_ptr.view(-1)\n\n    def run_kernel():\n        pre_reorder_triton_kernel_fast[(BATCH,)](\n            input_ptr,\n            gateup_input_ptr,\n            src2dst_ptr,\n            topk_ids_ptr,\n            a1_scales_ptr,\n            EXPERT_RANGE[0],\n            EXPERT_RANGE[1],\n            TOPK,\n            HIDDEN_SIZE,\n            BLOCK_SIZE\n        )\n\n    for _ in range(10):\n        run_kernel()\n    torch.cuda.synchronize()\n\n    # Benchmark\n    n_iter = 100000\n    start = time.time()\n    for _ in range(n_iter):\n        run_kernel()\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Average execution time over {n_iter} iterations: {(end - start) / n_iter * 1000:.3f} ms\")\n\nif __name__ == \"__main__\":\n    main() Before the fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.026 ms After fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.024 ms Good job. Can you add the benchmark script to https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton ? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author yuan-luo commented May 26, 2025 The benchmark script: # test_pre_reorder_triton_kernel.py\n\nimport torch\nimport triton\nimport triton.language as tl\nimport time\n\n@triton.jit\ndef pre_reorder_triton_kernel(\n    input_ptr,\n    gateup_input_ptr,\n    src2dst_ptr,\n    topk_ids_ptr,\n    a1_scales_ptr,\n    start_expert_id,\n    end_expert_id,\n    topk,\n    hidden_size,\n    BLOCK_SIZE: tl.constexpr,\n):\n    OutDtype = gateup_input_ptr.dtype.element_ty\n\n    src_idx = tl.program_id(0)\n    src2dst_ptr = src2dst_ptr + src_idx * topk\n    topk_ids_ptr = topk_ids_ptr + src_idx * topk\n    src_ptr = input_ptr + src_idx * hidden_size\n\n    vec = tl.arange(0, BLOCK_SIZE)\n\n    for idx in range(topk):\n        expert_id = tl.load(topk_ids_ptr + idx)\n        if expert_id >= start_expert_id and expert_id <= end_expert_id:\n            if a1_scales_ptr is not None:\n                scale = 1.0 / tl.load(a1_scales_ptr + expert_id - start_expert_id)\n            else:\n                scale = 1.0\n\n            dst_idx = tl.load(src2dst_ptr + idx)\n            dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n            for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n                offset = start_offset + vec\n                mask = offset < hidden_size\n                in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n                out_data = (in_data * scale).to(OutDtype)\n                tl.store(dst_ptr + offset, out_data, mask=mask)\n\ndef main():\n    BATCH = 640\n    TOPK = 2\n    HIDDEN_SIZE = 1024\n    BLOCK_SIZE = 512\n    EXPERT_RANGE = (0, 255)\n\n    input_ptr = torch.randn(BATCH, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    gateup_input_ptr = torch.zeros(BATCH * TOPK, HIDDEN_SIZE, dtype=torch.float16, device='cuda')\n    src2dst_ptr = torch.randint(0, BATCH * TOPK, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    topk_ids_ptr = torch.randint(EXPERT_RANGE[0], EXPERT_RANGE[1] + 1, (BATCH, TOPK), dtype=torch.int32, device='cuda')\n    a1_scales_ptr = torch.rand(EXPERT_RANGE[1] - EXPERT_RANGE[0] + 1, dtype=torch.float32, device='cuda')\n\n    input_ptr = input_ptr.view(-1)\n    gateup_input_ptr = gateup_input_ptr.view(-1)\n    src2dst_ptr = src2dst_ptr.view(-1)\n    topk_ids_ptr = topk_ids_ptr.view(-1)\n\n    def run_kernel():\n        pre_reorder_triton_kernel_fast[(BATCH,)](\n            input_ptr,\n            gateup_input_ptr,\n            src2dst_ptr,\n            topk_ids_ptr,\n            a1_scales_ptr,\n            EXPERT_RANGE[0],\n            EXPERT_RANGE[1],\n            TOPK,\n            HIDDEN_SIZE,\n            BLOCK_SIZE\n        )\n\n    for _ in range(10):\n        run_kernel()\n    torch.cuda.synchronize()\n\n    # Benchmark\n    n_iter = 100000\n    start = time.time()\n    for _ in range(n_iter):\n        run_kernel()\n    torch.cuda.synchronize()\n    end = time.time()\n\n    print(f\"Average execution time over {n_iter} iterations: {(end - start) / n_iter * 1000:.3f} ms\")\n\nif __name__ == \"__main__\":\n    main() Before the fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.026 ms After fix result: [root@decfee1df170 pre_reorder]# python test_pre_reorder_triton_kernel.py \nAverage execution time over 100000 iterations: 0.024 ms Good job. Can you add the benchmark script to https://github.com/sgl-project/sglang/tree/main/benchmark/kernels/fused_moe_triton ? Hi @BBuf , thanks for the comments. I've added benchmark test case in the folder. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author yuan-luo commented May 27, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . [root@decfee1df170 fused_moe_triton]# python benchmark_pre_reorder_triton.py \nINFO 05-27 10:42:57 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-27 10:42:59 [__init__.py:239] Automatically detected platform cuda.\nAverage execution time over 100000 iterations: 0.025 ms [root@decfee1df170 fused_moe_triton]# nvidia-smi \nTue May 27 10:43:11 2025       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA H20                     On  | 00000000:08:00.0 Off |                    0 |\n| N/A   39C    P0             143W / 500W |   1695MiB / 97871MiB |    100%      Default |\n|                                         |                      |             Disabled |\n...... All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from 2509651 to b842504 Compare May 27, 2025 03:38 Copy link Contributor Author yuan-luo commented May 27, 2025 Refactored the test case according to @BBuf 's comments. Before fix, the result is: [root@decfee1df170 fused_moe_triton]# python benchmark_pre_reorder_triton.py --hidden-size 1024\nINFO 05-27 11:35:58 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-27 11:35:59 [__init__.py:239] Automatically detected platform cuda.\npre-reorder-performance:\n   batch_size    TopK=2    TopK=4    TopK=8\n0        64.0  0.007552  0.008512  0.009824\n1       128.0  0.007872  0.008352  0.010112\n2       256.0  0.008224  0.008576  0.010048\n3       512.0  0.008480  0.009344  0.011296\n4       640.0  0.008704  0.009632  0.012224\n5       768.0  0.008896  0.009792  0.012864\n6      1024.0  0.009120  0.010816  0.014144 After fix, the result is: [root@decfee1df170 fused_moe_triton]# python benchmark_pre_reorder_triton.py --hidden-size 1024\nINFO 05-27 11:34:36 [importing.py:53] Triton module has been replaced with a placeholder.\nINFO 05-27 11:34:37 [__init__.py:239] Automatically detected platform cuda.\npre-reorder-performance:\n   batch_size    TopK=2    TopK=4    TopK=8\n0        64.0  0.007360  0.008128  0.009600\n1       128.0  0.007488  0.008224  0.009920\n2       256.0  0.007840  0.008416  0.009920\n3       512.0  0.008224  0.009088  0.011104\n4       640.0  0.008480  0.009184  0.011808\n5       768.0  0.008672  0.009632  0.012608\n6      1024.0  0.008896  0.010592  0.014240 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from b842504 to 6d12d47 Compare May 27, 2025 03:40 BBuf reviewed May 27, 2025 View reviewed changes benchmark/kernels/fused_moe_triton/benchmark_pre_reorder_triton.py Outdated @triton.testing.perf_report( triton.testing.Benchmark( x_names=[\"batch_size\"], x_vals=[64, 128, 256, 512, 640, 768, 1024], Copy link Collaborator BBuf May 27, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The batch size (bs) and topk can be combined using the product function. Refer to https://github.com/sgl-project/sglang/blob/main/sgl-kernel/benchmark/bench_moe_align_block_size.py#L248 for more details. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author yuan-luo May 27, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Revised to use product. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from 6d12d47 to e8aadf8 Compare May 27, 2025 06:24 BBuf approved these changes May 27, 2025 View reviewed changes Copy link Collaborator BBuf left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Please rename benchmark script to benchmark_ep_pre_reorder_triton.py Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from e8aadf8 to 1ff2978 Compare May 27, 2025 07:05 Copy link Contributor Author yuan-luo commented May 27, 2025 Please rename benchmark script to benchmark_ep_pre_reorder_triton.py Done. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator BBuf commented May 27, 2025 Pr is ready now, please have a look when you are free? @zhyncs \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned Alcanderian and BBuf May 27, 2025 zhyncs added\n  the high priority label May 27, 2025 Refine pre_reorder_triton_kernel ea4070d yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from b6475b1 to 0292e03 Compare May 27, 2025 09:53 add benchmark for ep_pre_reorder_triton a8bec42 yuan-luo force-pushed the refine_moe_reorder_trition branch\n    from 0292e03 to a8bec42 Compare May 27, 2025 09:59 Alcanderian approved these changes May 28, 2025 View reviewed changes Hide details View details zhyncs merged commit c087ddd into sgl-project : main May 28, 2025 72 of 78 checks passed Uh oh! There was an error while loading. Please reload this page . Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Refine pre_reorder_triton_kernel slightly to improve performance ( sgl\u2026 \u2026 7189f17 \u2026-project#6627 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com> xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Refine pre_reorder_triton_kernel slightly to improve performance ( sgl\u2026 \u2026 f1557c2 \u2026-project#6627 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com> yuan-luo deleted the refine_moe_reorder_trition branch July 9, 2025 06:35 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:20",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Refine pre_reorder_triton_kernel slightly to improve performance (#6627)",
  "commit_message": "Refine pre_reorder_triton_kernel slightly to improve performance (#6627)\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>",
  "commit_date": "2025-05-28T00:15:23-07:00",
  "files_changed": [
    "benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py",
    "python/sglang/srt/layers/moe/ep_moe/kernels.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 113,
    "num_files": 2,
    "num_hunks": 5,
    "num_non_test_edited_lines": 113,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\nnew file mode 100644\nindex 000000000..c62424357\n--- /dev/null\n+++ b/benchmark/kernels/fused_moe_triton/benchmark_ep_pre_reorder_triton.py\n@@ -0,0 +1,100 @@\n+import argparse\n+import itertools\n+\n+import pandas as pd\n+import torch\n+import triton\n+\n+from sglang.srt.layers.moe.ep_moe.kernels import pre_reorder_triton_kernel\n+\n+\n+def benchmark_pre_reorder(batch_size, topk, model_config):\n+    hidden_size = model_config[\"hidden_size\"]\n+    block_size = model_config[\"block_size\"]\n+    expert_range = model_config[\"expert_range\"]\n+\n+    input_ptr = torch.randn(batch_size, hidden_size, dtype=torch.float16, device=\"cuda\")\n+    gateup_input_ptr = torch.zeros(\n+        batch_size * topk, hidden_size, dtype=torch.float16, device=\"cuda\"\n+    )\n+    src2dst_ptr = torch.randint(\n+        0, batch_size * topk, (batch_size, topk), dtype=torch.int32, device=\"cuda\"\n+    )\n+    topk_ids_ptr = torch.randint(\n+        expert_range[0],\n+        expert_range[1] + 1,\n+        (batch_size, topk),\n+        dtype=torch.int32,\n+        device=\"cuda\",\n+    )\n+    a1_scales_ptr = torch.rand(\n+        expert_range[1] - expert_range[0] + 1, dtype=torch.float32, device=\"cuda\"\n+    )\n+\n+    input_ptr = input_ptr.view(-1)\n+    gateup_input_ptr = gateup_input_ptr.view(-1)\n+    src2dst_ptr = src2dst_ptr.view(-1)\n+    topk_ids_ptr = topk_ids_ptr.view(-1)\n+\n+    def run_kernel():\n+        pre_reorder_triton_kernel[(batch_size,)](\n+            input_ptr,\n+            gateup_input_ptr,\n+            src2dst_ptr,\n+            topk_ids_ptr,\n+            a1_scales_ptr,\n+            expert_range[0],\n+            expert_range[1],\n+            topk,\n+            hidden_size,\n+            block_size,\n+        )\n+\n+    for _ in range(10):\n+        run_kernel()\n+    torch.cuda.synchronize()\n+\n+    ms, _, _ = triton.testing.do_bench(run_kernel, quantiles=[0.5, 0.2, 0.8])\n+    return ms\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--hidden-size\", type=int, required=True)\n+    parser.add_argument(\"--block-size\", type=int, default=512)\n+    args = parser.parse_args()\n+\n+    model_config = {\n+        \"hidden_size\": args.hidden_size,\n+        \"block_size\": args.block_size,\n+        \"expert_range\": (0, 255),\n+    }\n+\n+    batch_sizes = [64, 128, 256, 512, 640, 768, 1024]\n+    topks = [2, 4, 8]\n+    configs = list(itertools.product(batch_sizes, topks))\n+\n+    # Prepare results dict: keys = topk, each row is indexed by batch_size\n+    results_dict = {topk: {} for topk in topks}\n+\n+    for batch_size, topk in configs:\n+        ms = benchmark_pre_reorder(batch_size, topk, model_config)\n+        results_dict[topk][batch_size] = ms\n+\n+    # Build dataframe\n+    df = pd.DataFrame(\n+        {\n+            \"batch_size\": batch_sizes,\n+            **{\n+                f\"TopK={topk}\": [results_dict[topk].get(bs, None) for bs in batch_sizes]\n+                for topk in topks\n+            },\n+        }\n+    )\n+\n+    print(\"\\npre-reorder-performance:\")\n+    print(df.to_string(index=False, float_format=\"%.6f\"))\n+\n+\n+if __name__ == \"__main__\":\n+    main()\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 8c005527a..56c6c7db7 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -184,8 +184,10 @@ def pre_reorder_triton_kernel(\n     src_idx = tl.program_id(0)\n     src2dst_ptr = src2dst_ptr + src_idx * topk\n     topk_ids_ptr = topk_ids_ptr + src_idx * topk\n-\n     src_ptr = input_ptr + src_idx * hidden_size\n+\n+    vec = tl.arange(0, BLOCK_SIZE)\n+\n     for idx in range(topk):\n         expert_id = tl.load(topk_ids_ptr + idx)\n         if expert_id >= start_expert_id and expert_id <= end_expert_id:\n@@ -197,7 +199,7 @@ def pre_reorder_triton_kernel(\n             dst_idx = tl.load(src2dst_ptr + idx)\n             dst_ptr = gateup_input_ptr + dst_idx * hidden_size\n             for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-                offset = start_offset + tl.arange(0, BLOCK_SIZE)\n+                offset = start_offset + vec\n                 mask = offset < hidden_size\n                 in_data = tl.load(src_ptr + offset, mask=mask).to(tl.float32)\n                 out_data = (in_data * scale).to(OutDtype)\n@@ -481,8 +483,11 @@ def post_reorder_triton_kernel(\n \n     computed = False\n     store_ptr = output_ptr + src_idx * hidden_size\n+\n+    vec = tl.arange(0, BLOCK_SIZE)\n+\n     for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-        offset = start_offset + tl.arange(0, BLOCK_SIZE)\n+        offset = start_offset + vec\n         mask = offset < hidden_size\n \n         sum_vec = tl.zeros([BLOCK_SIZE], dtype=InDtype)\n@@ -499,7 +504,7 @@ def post_reorder_triton_kernel(\n \n     if computed == False:\n         for start_offset in tl.range(0, hidden_size, BLOCK_SIZE):\n-            offset = start_offset + tl.arange(0, BLOCK_SIZE)\n+            offset = start_offset + vec\n             mask = offset < hidden_size\n             tl.store(\n                 store_ptr + offset, tl.zeros([BLOCK_SIZE], dtype=InDtype), mask=mask",
  "apis": [
    "sglang.srt.layers.moe.ep_moe.kernels.pre_reorder_triton_kernel",
    "sglang.srt.layers.moe.ep_moe.kernels.post_reorder_triton_kernel"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit alters a Triton kernel in a non-test file (kernels.py) by introducing a precomputed \"vec\" variable to avoid re-computing tl.arange within loops. This modification is targeted at reducing computational overhead in a performance-critical path. Additionally, a benchmark file has been added to evaluate the performance improvements of this kernel, emphasizing the focus on optimizing execution speed. These changes are not trivial nor are they merely a bug fix or refactoring; they are deliberately aimed at enhancing performance on the CPU without relying on GPU/TPU-specific workarounds. Therefore, the commit meets the criteria for performance/optimization related changes.",
  "llm_api_reason": "This commit adds a benchmark script for the fused MoE EP pre-reorder kernel and refines two Triton kernel implementations. In the pre_reorder_triton_kernel and post_reorder_triton_kernel functions, a precomputed vector (vec) is introduced to replace repeated calls to tl.arange, reducing redundant computation in inner loops and thereby improving performance. The benchmark file also provides a way to measure the performance improvement on these kernels."
}