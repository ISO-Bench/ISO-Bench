{
  "commit_hash": "7ce36068914503c3a53ad7be23ab29831fb8aa63",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1738",
  "pr_date": "2024-10-21",
  "timeline_text": "Copy link Contributor merrymercy commented Oct 21, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . This PR improves the order of kernel launch and result fetching. Now the overlap scheduler can bring 10% throughput improvement even when radix cache is turned off. When the radix cache is turned on, we can expect more speedup. Benchmark results Overlap mode:  51.03 req/s python -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --disable-radix --enable-overlap\npython -m sglang.bench_serving --model meta-llama/Llama-3.1-8B-Instruct --num-prompt 3000 ============ Serving Benchmark Result ============\nBackend:                                 sglang\nTraffic request rate:                    inf\nSuccessful requests:                     3000\nBenchmark duration (s):                  58.79\nTotal input tokens:                      673672\nTotal generated tokens:                  581627\nTotal generated tokens (retokenized):    581405\nRequest throughput (req/s):              51.03\nInput token throughput (tok/s):          11459.26\nOutput token throughput (tok/s):         9893.56\n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   28986.97\nMedian E2E Latency (ms):                 29088.28\n---------------Time to First Token----------------\nMean TTFT (ms):                          14495.13\nMedian TTFT (ms):                        11312.61\nP99 TTFT (ms):                           36408.59\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          144.25\nMedian TPOT (ms):                        86.74\nP99 TPOT (ms):                           1081.64\n---------------Inter-token Latency----------------\nMean ITL (ms):                           78.78\nMedian ITL (ms):                         32.48\nP99 ITL (ms):                            529.30\n================================================== Normal mode: 46.06 req/s python -m sglang.launch_server --model meta-llama/Llama-3.1-8B-Instruct --disable-radix \npython -m sglang.bench_serving --model meta-llama/Llama-3.1-8B-Instruct --num-prompt 3000 ============ Serving Benchmark Result ============\nBackend:                                 sglang\nTraffic request rate:                    inf\nSuccessful requests:                     3000\nBenchmark duration (s):                  65.14\nTotal input tokens:                      673672\nTotal generated tokens:                  581627\nTotal generated tokens (retokenized):    581402\nRequest throughput (req/s):              46.06\nInput token throughput (tok/s):          10342.28\nOutput token throughput (tok/s):         8929.19\n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   31574.46\nMedian E2E Latency (ms):                 31581.12\n---------------Time to First Token----------------\nMean TTFT (ms):                          15352.12\nMedian TTFT (ms):                        11615.68\nP99 TTFT (ms):                           39444.51\n-----Time per Output Token (excl. 1st token)------\nMean TPOT (ms):                          157.51\nMedian TPOT (ms):                        96.38\nP99 TPOT (ms):                           1131.20\n---------------Inter-token Latency----------------\nMean ITL (ms):                           87.11\nMedian ITL (ms):                         37.10\nP99 ITL (ms):                            554.28\n================================================== Notes We still only use multi-threading under the limitation of GIL. We can expect a larger improvement if we move to multi-processing or we can turn off GIL. The overlap scheduler is an experimental feature. I verified its accuracy on GSM-8k, and it matches that of the normal scheduler. It works for standard decoding, but it does not support sampling penalizers (e.g., frequency and repetition penalties) or constrained decoding (e.g., regex, JSON). Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 4 zhyncs, austin362667, Mesilenceki, and dotieuthien reacted with thumbs up emoji \ud83d\ude80 3 hnyls2002, austin362667, and ispobock reacted with rocket emoji All reactions \ud83d\udc4d 4 reactions \ud83d\ude80 3 reactions A copy thread 8685533 merrymercy force-pushed the multi-stream branch\n    from 44108b7 to 8685533 Compare October 21, 2024 09:23 merrymercy added 2 commits October 21, 2024 03:34 update b91b56a Add a wait until the batch is launched 97cb23d merrymercy changed the title Launch a copy thread for overlapped scheduler Faster overlap mode scheduler Oct 21, 2024 Fix 53a6acb Hide details View details merrymercy merged commit 7ce3606 into main Oct 21, 2024 9 of 10 checks passed Uh oh! There was an error while loading. Please reload this page . merrymercy deleted the multi-stream branch October 21, 2024 11:30 merrymercy mentioned this pull request Oct 23, 2024 Development Roadmap (2024 Q4) #1487 Closed 37 tasks Copy link fengyang95 commented Nov 10, 2024 @merrymercy Has this been tested on larger models? I tried the deepseek-v2.5 fp8 version, but it doesn't seem to show much improvement. \ud83d\udc40 1 austin362667 reacted with eyes emoji All reactions \ud83d\udc40 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor ykcombat commented Dec 1, 2024 @merrymercy Have you ever tested overlap mode scheduler when receiving requests at a certain request rate rather than sending all the requests at the beginning? When I test it without specifying request rate, everthing goes all right. python -m sglang.bench_serving --backend sglang --num-prompt 10 But when i tried specifying request, letting requests sent in Possion distribution: python -m sglang.bench_serving --backend sglang --num-prompt 10 --request-rate 2 I have encountered a mysterious bugs: CUDA Error: device-side assert triggered (710) /tmp/build-via-sdist-d34cpfe8/flashinfer-0.1.6+cu121torch2.4/include/flashinfer/attention/decode.cuh: line 749 at function cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size) Exception in thread Thread-3 (forward_thread_func): Traceback (most recent call last): File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner self.run() File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/threading.py\", line 953, in run self._target(*self._args, **self._kwargs) File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 99, in forward_thread_func self.forward_thread_func_() File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py\", line 116, in forward_thread_func_ logits_output, next_token_ids = self.worker.forward_batch_generation( File \"/home/ykchen/sglang/python/sglang/srt/managers/tp_worker.py\", line 139, in forward_batch_generation logits_output = self.model_runner.forward(forward_batch) File \"/home/ykchen/sglang/python/sglang/srt/model_executor/model_runner.py\", line 594, in forward return self.forward_decode(forward_batch) File \"/home/ykchen/sglang/python/sglang/srt/model_executor/model_runner.py\", line 565, in forward_decode return self.model.forward( File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context return func(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/models/llama.py\", line 371, in forward hidden_states = self.model(input_ids, positions, forward_batch, input_embeds) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl return forward_call(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/models/llama.py\", line 284, in forward hidden_states, residual = layer( File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl return forward_call(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/models/llama.py\", line 234, in forward hidden_states = self.self_attn( File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl return forward_call(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/models/llama.py\", line 171, in forward attn_output = self.attn(q, k, v, forward_batch) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl return self._call_impl(*args, **kwargs) File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl return forward_call(*args, **kwargs) File \"/home/ykchen/sglang/python/sglang/srt/layers/radix_attention.py\", line 60, in forward return forward_batch.attn_backend.forward(q, k, v, self, forward_batch) File \"/home/ykchen/sglang/python/sglang/srt/layers/attention/__init__.py\", line 58, in forward return self.forward_decode(q, k, v, layer, forward_batch) File \"/home/ykchen/sglang/python/sglang/srt/layers/attention/flashinfer_backend.py\", line 284, in forward_decode o = decode_wrapper.forward( File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/flashinfer/decode.py\", line 589, in forward return self.run( File \"/state/partition/ykchen/conda/envs/sglang/lib/python3.10/site-packages/flashinfer/decode.py\", line 673, in run out = self._wrapper.run( RuntimeError: BatchDecodeWithPagedKVCache failed with error device-side assert triggered This script works with normal scheduler. Did I make any mistakes or it's a bug for overlap mode scheduler? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author merrymercy commented Dec 2, 2024 @ykcombat Did you try it with the latest main branch? If the error is still there, please open a new issue with reproducible instructions. We will fix it very soon if we can reproduce that. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor ykcombat commented Dec 2, 2024 @ykcombat Did you try it with the latest main branch? If the error is still there, please open a new issue with reproducible instructions. We will fix it very soon if we can reproduce that. @merrymercy Thanks for your quick reply! I tried it with the latest main branch but it seems that the error is still there. I have opened a new issue at #2312 . All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhaochenyang20 mentioned this pull request Mar 3, 2025 Development Roadmap (2025 H1) #4035 Closed 22 tasks timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Faster overlap mode scheduler ( sgl-project#1738 ) 60f666f Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:15",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python -m sglang.bench_serving --model meta-llama/Llama-3.1-8B-Instruct --num-prompt 3000",
  "commit_subject": "Faster overlap mode scheduler (#1738)",
  "commit_message": "Faster overlap mode scheduler (#1738)",
  "commit_date": "2024-10-21T04:30:52-07:00",
  "files_changed": [
    "python/sglang/srt/managers/tp_worker_overlap_thread.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 30,
    "num_files": 1,
    "num_hunks": 5,
    "num_non_test_edited_lines": 30,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/tp_worker_overlap_thread.py b/python/sglang/srt/managers/tp_worker_overlap_thread.py\nindex 5d78b97ce..8b27d2a69 100644\n--- a/python/sglang/srt/managers/tp_worker_overlap_thread.py\n+++ b/python/sglang/srt/managers/tp_worker_overlap_thread.py\n@@ -55,7 +55,7 @@ class TpModelWorkerClient:\n             (self.max_running_requests * 5,), dtype=torch.int32, device=self.device\n         )\n \n-        # Launch a thread\n+        # Launch threads\n         self.input_queue = Queue()\n         self.output_queue = Queue()\n         self.forward_stream = torch.cuda.Stream()\n@@ -64,6 +64,12 @@ class TpModelWorkerClient:\n         )\n         self.forward_thread.start()\n \n+        self.copy_queue = Queue()\n+        self.copy_thread = threading.Thread(\n+            target=self.copy_thread_func,\n+        )\n+        self.copy_thread.start()\n+\n     def get_worker_info(self):\n         return self.worker.get_worker_info()\n \n@@ -86,7 +92,10 @@ class TpModelWorkerClient:\n     @torch.inference_mode()\n     def forward_thread_func_(self):\n         while True:\n+            self.has_inflight_batch = False\n             model_worker_batch, future_token_ids_ct = self.input_queue.get()\n+            self.has_inflight_batch = True\n+            self.launch_event = threading.Event()\n \n             # Resolve future tokens in the input\n             input_ids = model_worker_batch.input_ids\n@@ -100,6 +109,7 @@ class TpModelWorkerClient:\n             logits_output, next_token_ids = self.worker.forward_batch_generation(\n                 model_worker_batch\n             )\n+            self.launch_event.set()\n \n             # Update the future token ids map\n             bs = len(model_worker_batch.seq_lens)\n@@ -113,13 +123,23 @@ class TpModelWorkerClient:\n                 torch.int32\n             )\n \n-            # Set the result\n-            next_token_ids = next_token_ids.tolist()\n-            assert logits_output.next_token_logprobs is None, \"Not supported\"\n-            self.output_queue.put((None, next_token_ids))\n+            next_token_ids = next_token_ids.to(\"cpu\", non_blocking=True)\n+            copy_event = torch.cuda.Event(blocking=True)\n+            copy_event.record()\n+            self.copy_queue.put((copy_event, next_token_ids))\n+\n+    def copy_thread_func(self):\n+        while True:\n+            copy_event, next_token_ids = self.copy_queue.get()\n+            while not copy_event.query():\n+                time.sleep(1e-5)\n+            self.output_queue.put((None, next_token_ids.tolist()))\n \n     def resulve_batch_result(self, bid: int):\n         logits_output, next_token_ids = self.output_queue.get()\n+        if self.has_inflight_batch:\n+            # Wait until the batch is launched\n+            self.launch_event.wait()\n         return logits_output, next_token_ids\n \n     def forward_batch_generation(self, model_worker_batch: ModelWorkerBatch):",
  "apis": [
    "sglang.srt.managers.tp_worker_overlap_thread.TpModelWorkerClient.__init__",
    "sglang.srt.managers.tp_worker_overlap_thread.TpModelWorkerClient.forward_thread_func_",
    "sglang.srt.managers.tp_worker_overlap_thread.TpModelWorkerClient.copy_thread_func",
    "sglang.srt.managers.tp_worker_overlap_thread.TpModelWorkerClient.forward_batch_generation",
    "sglang.srt.managers.tp_worker_overlap_thread.TpModelWorkerClient.resulve_batch_result"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/managers/tp_worker_overlap_thread.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/python/sglang/srt/entrypoints/http_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a non-test source code file by adding a new thread and additional queue handling to perform asynchronous copying of data from CUDA. This change modifies how the scheduler overlaps work by introducing a new thread for copying, potentially reducing idle time and improving performance. The commit is not merely a refactoring or bug fix but targets improved execution performance via scheduling optimizations that are testable on a CPU. Therefore, it meets the criteria for performance/optimization-related changes.",
  "llm_api_reason": "The commit improves the overlap mode scheduler by adding an additional thread to handle the copying of tensor data asynchronously. In the TpModelWorkerClient class, a new copy_queue and copy_thread (with its copy_thread_func) are introduced to execute copy events separately. In addition, the forward_thread_func_ is modified to set a launch event, and resulve_batch_result now waits on that event to ensure the batch has been correctly launched. These changes enhance performance by overlapping computation and data movement."
}