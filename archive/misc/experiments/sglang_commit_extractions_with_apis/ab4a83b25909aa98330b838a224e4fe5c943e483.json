{
  "commit_hash": "ab4a83b25909aa98330b838a224e4fe5c943e483",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1339",
  "pr_date": "2024-09-05",
  "timeline_text": "Copy link Collaborator hnyls2002 commented Sep 5, 2024 Motivation Modifications Checklist Format your code according to the Contributor Guide . Add unit tests as outlined in the Contributor Guide . Update documentation as needed, including docstrings or example tutorials. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions hnyls2002 added 5 commits September 5, 2024 16:18 update 6ccea63 update 8a35b55 reduce overhead a9c5f0a reduce overhead a3ce927 fix inflight 37bf108 Hide details View details merrymercy merged commit ab4a83b into main Sep 5, 2024 9 checks passed Uh oh! There was an error while loading. Please reload this page . merrymercy deleted the optimize-schedule branch September 5, 2024 21:30 merrymercy mentioned this pull request Sep 13, 2024 Development  Roadmap (2024 Q3) #634 Closed 29 tasks Copy link Contributor hxer7963 commented Sep 21, 2024 hi, @hnyls2002 @merrymercy . I have been exploring the source code of the PrefillAdder class and the scheduler module within ModelTpServer::get_new_prefill_batch . It seems that the implementation reserves the maximum possible output token slots based on the estimated new_token_ratio before scheduling prefill requests. However, I am curious about the motivation behind the scheduling strategy used by PrefillAdder and how it contributes to optimizing scheduling performance. Could you provide some insights into these aspects? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Optimize schedule ( sgl-project#1339 ) 568eea5 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:36",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Optimize schedule (#1339)",
  "commit_message": "Optimize schedule (#1339)",
  "commit_date": "2024-09-05T14:30:26-07:00",
  "files_changed": [
    "python/sglang/srt/managers/policy_scheduler.py",
    "python/sglang/srt/managers/tp_worker.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 131,
    "num_files": 2,
    "num_hunks": 13,
    "num_non_test_edited_lines": 131,
    "num_non_test_files": 2,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/policy_scheduler.py b/python/sglang/srt/managers/policy_scheduler.py\nindex 04169e808..3a70bfe54 100644\n--- a/python/sglang/srt/managers/policy_scheduler.py\n+++ b/python/sglang/srt/managers/policy_scheduler.py\n@@ -108,18 +108,24 @@ class PrefillAdder:\n     def __init__(\n         self,\n         tree_cache: BasePrefixCache,\n+        running_batch: ScheduleBatch,\n+        new_token_ratio: float,\n         rem_total_tokens: int,\n         rem_input_tokens: int,\n         rem_chunk_tokens: Optional[int],\n         mixed_with_decode_tokens: int = 0,\n     ):\n         self.tree_cache = tree_cache\n+        self.running_batch = running_batch\n+        self.new_token_ratio = new_token_ratio\n         self.rem_total_tokens = rem_total_tokens - mixed_with_decode_tokens\n+        self.total_tokens = rem_total_tokens\n         self.rem_input_tokens = rem_input_tokens - mixed_with_decode_tokens\n         self.rem_chunk_tokens = rem_chunk_tokens\n         if self.rem_chunk_tokens is not None:\n             self.rem_chunk_tokens -= mixed_with_decode_tokens\n \n+        self.req_states = None\n         self.can_run_list = []\n         self.new_inflight_req = None\n         self.log_hit_tokens = 0\n@@ -136,16 +142,14 @@ class PrefillAdder:\n             )\n         )\n \n-    def remove_running_tokens(\n-        self, running_batch: ScheduleBatch, new_token_ratio: float\n-    ):\n+    def remove_running_tokens(self, running_batch: ScheduleBatch):\n         self.rem_total_tokens -= sum(\n             [\n                 min(\n                     (r.sampling_params.max_new_tokens - len(r.output_ids)),\n                     CLIP_MAX_NEW_TOKENS,\n                 )\n-                * new_token_ratio\n+                * self.new_token_ratio\n                 for r in running_batch.reqs\n             ]\n         )\n@@ -161,7 +165,29 @@ class PrefillAdder:\n         self.log_hit_tokens += prefix_len\n         self.log_input_tokens += extend_input_len\n \n+    def add_inflight_req_ignore_eos(self, req: Req):\n+        truncated = req.extend_input_len > self.rem_chunk_tokens\n+        req.extend_input_len = min(req.extend_input_len, self.rem_chunk_tokens)\n+        req.fill_ids = req.fill_ids[: len(req.prefix_indices) + req.extend_input_len]\n+        self.can_run_list.append(req)\n+\n+        self._prefill_one_req(\n+            0,\n+            req.extend_input_len,\n+            (\n+                min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS)\n+                if not truncated\n+                else 0\n+            ),\n+        )\n+\n+        # Return if chunked prefill not finished\n+        return req if truncated else None\n+\n     def add_inflight_req(self, req: Req):\n+        if req.sampling_params.ignore_eos:\n+            return self.add_inflight_req_ignore_eos(req)\n+\n         truncated = req.extend_input_len > self.rem_chunk_tokens\n         req.extend_input_len = min(req.extend_input_len, self.rem_chunk_tokens)\n         req.fill_ids = req.fill_ids[: len(req.prefix_indices) + req.extend_input_len]\n@@ -190,7 +216,81 @@ class PrefillAdder:\n             delta = self.tree_cache.dec_lock_ref(last_node)\n             self.rem_total_tokens += delta\n \n+    def add_one_req_ignore_eos(self, req: Req):\n+        def get_req_state(r):\n+            new_token_ratio = (\n+                1.0 if r.sampling_params.ignore_eos else self.new_token_ratio\n+            )\n+            tokens_left = r.sampling_params.max_new_tokens * new_token_ratio - len(\n+                r.output_ids\n+            )\n+            tokens_occupied = len(r.origin_input_ids) + len(r.output_ids)\n+\n+            if tokens_left > 0:\n+                return (tokens_left, tokens_occupied)\n+\n+            return None\n+\n+        if self.req_states is None:\n+            self.req_states = []\n+            if self.running_batch is not None:\n+                for r in self.running_batch.reqs:\n+                    state = get_req_state(r)\n+                    if state is not None:\n+                        self.req_states.append(state)\n+            for r in self.can_run_list:\n+                state = get_req_state(r)\n+                if state is not None:\n+                    self.req_states.append(state)\n+            state = get_req_state(req)\n+            if state is not None:\n+                self.req_states.append(state)\n+\n+            self.req_states.sort(key=lambda x: x[0])\n+        else:\n+            state = get_req_state(req)\n+            if state is not None:\n+                for i, (tokens_left, tokens_occupied) in enumerate(self.req_states):\n+                    if tokens_left >= state[0]:\n+                        self.req_states.insert(i, state)\n+                        break\n+                else:\n+                    self.req_states.append(state)\n+\n+        tokens_freed = 0\n+        for i, (tokens_left, tokens_occupied) in enumerate(self.req_states):\n+            decode_steps = (\n+                self.req_states[i + 1][0]\n+                if i + 1 < len(self.req_states)\n+                else tokens_left\n+            )\n+            bs = len(self.req_states) - i\n+            if self.total_tokens + tokens_freed - decode_steps * bs <= 0:\n+                return False\n+            tokens_freed += tokens_occupied\n+\n+        if req.extend_input_len <= self.rem_chunk_tokens:\n+            self.can_run_list.append(req)\n+            self._prefill_one_req(\n+                0,\n+                req.extend_input_len,\n+                min(req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS),\n+            )\n+        else:\n+            # Chunked prefill\n+            trunc_len = self.rem_chunk_tokens\n+            req.extend_input_len = trunc_len\n+            req.fill_ids = req.fill_ids[:trunc_len]\n+            self.can_run_list.append(req)\n+            self.new_inflight_req = req\n+            self._prefill_one_req(0, trunc_len, 0)\n+\n+        return True\n+\n     def add_one_req(self, req: Req):\n+        if req.sampling_params.ignore_eos and self.tree_cache.disable:\n+            return self.add_one_req_ignore_eos(req)\n+\n         total_tokens = req.extend_input_len + min(\n             req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS\n         )\n@@ -233,4 +333,4 @@ class PrefillAdder:\n                 self.tree_cache.inc_lock_ref(req.last_node)\n                 self._prefill_one_req(prefix_len, trunc_len, 0)\n \n-        return True\n+        return True and not self.no_remaining_tokens()\ndiff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py\nindex 8fc03b859..d914a71c2 100644\n--- a/python/sglang/srt/managers/tp_worker.py\n+++ b/python/sglang/srt/managers/tp_worker.py\n@@ -221,6 +221,7 @@ class ModelTpServer:\n         )\n         self.new_token_ratio = self.min_new_token_ratio\n         self.new_token_ratio_decay = global_config.new_token_ratio_decay\n+        self.do_not_get_new_batch = False\n \n     def exposed_step(self, recv_reqs: List):\n         try:\n@@ -253,7 +254,13 @@ class ModelTpServer:\n \n     @torch.inference_mode()\n     def forward_step(self):\n-        new_batch = self.get_new_prefill_batch()\n+        if self.current_inflight_req is not None:\n+            self.do_not_get_new_batch = False\n+\n+        new_batch = (\n+            self.get_new_prefill_batch() if not self.do_not_get_new_batch else None\n+        )\n+        self.do_not_get_new_batch = False\n \n         if new_batch is not None:\n             # Run a new prefill batch\n@@ -409,6 +416,8 @@ class ModelTpServer:\n \n         adder = PrefillAdder(\n             self.tree_cache,\n+            self.running_batch,\n+            self.new_token_ratio,\n             self.token_to_kv_pool.available_size() + self.tree_cache.evictable_size(),\n             self.max_prefill_tokens,\n             self.chunked_prefill_size,\n@@ -416,7 +425,7 @@ class ModelTpServer:\n         )\n \n         if self.running_batch is not None:\n-            adder.remove_running_tokens(self.running_batch, self.new_token_ratio)\n+            adder.remove_running_tokens(self.running_batch)\n \n         has_inflight = self.current_inflight_req is not None\n         if self.current_inflight_req is not None:\n@@ -428,11 +437,12 @@ class ModelTpServer:\n             )\n \n         for req in self.waiting_queue:\n+            if adder.no_remaining_tokens():\n+                break\n             req.init_next_round_input(None if prefix_computed else self.tree_cache)\n             res = adder.add_one_req(req)\n             if (\n                 not res\n-                or adder.no_remaining_tokens()\n                 or running_bs + len(adder.can_run_list) >= self.max_running_requests\n             ):\n                 break\n@@ -700,6 +710,7 @@ class ModelTpServer:\n         next_token_ids = next_token_ids.tolist()\n \n         # Check finish condition\n+        has_finished = False\n         for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):\n             req.completion_tokens_wo_jump_forward += 1\n             req.output_ids.append(next_token_id)\n@@ -712,6 +723,7 @@ class ModelTpServer:\n \n             if req.finished():\n                 self.tree_cache.cache_finished_req(req)\n+                has_finished = True\n \n             if req.return_logprob:\n                 req.output_token_logprobs.append(\n@@ -720,6 +732,9 @@ class ModelTpServer:\n                 if req.top_logprobs_num > 0:\n                     req.output_top_logprobs.append(logits_output.output_top_logprobs[i])\n \n+        if not has_finished:\n+            self.do_not_get_new_batch = True\n+\n         self.handle_finished_requests(batch)\n \n     def handle_finished_requests(self, batch: ScheduleBatch):",
  "apis": [
    "None"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/managers/tp_worker.py",
    "/path/to/repos/sglang/python/sglang/api.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies two non-test source files and makes non-trivial changes in the scheduling logic that governs prefill token management and request processing. The changes introduce new functions (e.g., handling of in-flight requests with ignore_eos, refactoring of remove_running_tokens, and adjustments in batch processing in ModelTpServer) and conditions that impact how scheduling decisions are made. Although the commit message is \"Optimize schedule\", the modifications improve the scheduling mechanism\u2019s efficiency rather than simply refactoring or fixing a bug. The changes are intended to affect the performance of top-level APIs running on the CPU, making the handling of running batches and tokens more efficient. Hence, these modifications align with performance/optimization improvements.",
  "llm_api_reason": "The commit introduces optimization changes for scheduling prefill batches in tensor parallel workers. In particular, it modifies the internal PrefillAdder class by adding new methods to handle requests with ignore_eos enabled and adjusts token accounting. It also changes the ModelTpServer class to control prefill batch retrieval and scheduling logic. These improvements are internal to the scheduling and batching process and do not change any of the public or high\u2010level Python APIs exposed by SGLang. [APIS] None [/APIS]"
}