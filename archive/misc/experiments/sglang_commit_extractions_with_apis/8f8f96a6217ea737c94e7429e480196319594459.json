{
  "commit_hash": "8f8f96a6217ea737c94e7429e480196319594459",
  "pr_url": "https://github.com/sgl-project/sglang/pull/1773",
  "pr_date": "2024-10-23",
  "timeline_text": "Copy link Contributor merrymercy commented Oct 23, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . #1766 introduced a pref regression on the test python3 -m unittest test_bench_serving.TestBenchServing.test_online_latency_default This PR fixed it. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions merrymercy requested review from hnyls2002 , Ying1123 , zhyncs , ispobock and ByronHsu as code owners October 23, 2024 22:34 merrymercy added 8 commits October 23, 2024 16:28 Fix the perf regression introduced by additional_stop_token_ids e1add99 Fix perf regression bf4a666 Fix cc35463 Fix min tokens fbe60da Fix d3ef107 Fix 5d3c781 Fix 9b40b47 Fix f0a22a5 merrymercy force-pushed the fix-perf-reg branch\n    from 421befd to f0a22a5 Compare October 23, 2024 23:28 Hide details View details merrymercy merged commit 8f8f96a into main Oct 23, 2024 5 of 10 checks passed Uh oh! There was an error while loading. Please reload this page . merrymercy deleted the fix-perf-reg branch October 23, 2024 23:45 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Fix the perf regression due to additional_stop_token_ids ( sgl-project\u2026 \u2026 ec40365 \u2026#1773 ) Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:08",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Fix the perf regression due to additional_stop_token_ids (#1773)",
  "commit_message": "Fix the perf regression due to additional_stop_token_ids (#1773)",
  "commit_date": "2024-10-23T16:45:21-07:00",
  "files_changed": [
    "python/sglang/srt/hf_transformers_utils.py",
    "python/sglang/srt/layers/sampler.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py",
    "python/sglang/srt/sampling/sampling_params.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 36,
    "num_files": 5,
    "num_hunks": 7,
    "num_non_test_edited_lines": 36,
    "num_non_test_files": 5,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/hf_transformers_utils.py b/python/sglang/srt/hf_transformers_utils.py\nindex 56d7c8a1f..6a2582e42 100644\n--- a/python/sglang/srt/hf_transformers_utils.py\n+++ b/python/sglang/srt/hf_transformers_utils.py\n@@ -164,7 +164,7 @@ def get_tokenizer(\n             \"slowdown. Consider using a fast tokenizer instead.\"\n         )\n \n-    handle_additional_stop_token_ids(tokenizer)\n+    attach_additional_stop_token_ids(tokenizer)\n     return tokenizer\n \n \n@@ -184,11 +184,11 @@ def get_processor(\n         **kwargs,\n     )\n \n-    handle_additional_stop_token_ids(processor.tokenizer)\n+    attach_additional_stop_token_ids(processor.tokenizer)\n     return processor\n \n \n-def handle_additional_stop_token_ids(tokenizer):\n+def attach_additional_stop_token_ids(tokenizer):\n     # Special handling for stop token <|eom_id|> generated by llama 3 tool use.\n     if \"<|eom_id|>\" in tokenizer.get_added_vocab():\n         tokenizer.additional_stop_token_ids = set(\ndiff --git a/python/sglang/srt/layers/sampler.py b/python/sglang/srt/layers/sampler.py\nindex 9ae5801cc..a5afcab51 100644\n--- a/python/sglang/srt/layers/sampler.py\n+++ b/python/sglang/srt/layers/sampler.py\n@@ -42,11 +42,11 @@ class Sampler(nn.Module):\n         logits = logits.contiguous()\n \n         if self.use_nan_detectioin and torch.any(torch.isnan(logits)):\n-            exit(1) if crash_on_warning else None\n             logger.warning(\"Detected errors during sampling! NaN in the logits.\")\n             logits = torch.where(\n                 torch.isnan(logits), torch.full_like(logits, -1e5), logits\n             )\n+            exit(1) if crash_on_warning else None\n \n         if sampling_info.is_all_greedy:\n             # Use torch.argmax if all requests use greedy sampling\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex fac008d3f..fcd06d8cc 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -334,15 +334,20 @@ class Req:\n \n         last_token_id = self.output_ids[-1]\n \n-        matched_eos = last_token_id in self.sampling_params.stop_token_ids\n+        matched_eos = False\n \n+        # Check stop token ids\n+        if self.sampling_params.stop_token_ids:\n+            matched_eos = last_token_id in self.sampling_params.stop_token_ids\n         if self.tokenizer is not None:\n             matched_eos |= last_token_id == self.tokenizer.eos_token_id\n-\n+            if self.tokenizer.additional_stop_token_ids:\n+                matched_eos |= last_token_id in self.tokenizer.additional_stop_token_ids\n         if matched_eos and not self.sampling_params.ignore_eos:\n             self.finished_reason = FINISH_MATCHED_TOKEN(matched=last_token_id)\n             return\n \n+        # Check stop strings\n         if len(self.sampling_params.stop_strs) > 0:\n             tail_str = self.tokenizer.decode(\n                 self.output_ids[-(self.sampling_params.stop_str_max_len + 1) :]\ndiff --git a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\nindex c9e0f078e..cc97a2eac 100644\n--- a/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n+++ b/python/sglang/srt/sampling/penaltylib/penalizers/min_new_tokens.py\n@@ -31,9 +31,12 @@ class BatchedMinNewTokensPenalizer(_BatchedPenalizer):\n         padded_stop_token_ids = torch.nn.utils.rnn.pad_sequence(\n             sequences=[\n                 torch.tensor(\n-                    data=list(\n-                        req.sampling_params.stop_token_ids\n-                        | {req.tokenizer.eos_token_id}\n+                    data=(\n+                        list(\n+                            (req.sampling_params.stop_token_ids or set())\n+                            | (req.tokenizer.additional_stop_token_ids or set())\n+                            | {req.tokenizer.eos_token_id}\n+                        )\n                     ),\n                     dtype=torch.int64,\n                     device=self.orchestrator.device,\ndiff --git a/python/sglang/srt/sampling/sampling_params.py b/python/sglang/srt/sampling/sampling_params.py\nindex b0863b557..fbe90ba0f 100644\n--- a/python/sglang/srt/sampling/sampling_params.py\n+++ b/python/sglang/srt/sampling/sampling_params.py\n@@ -50,10 +50,10 @@ class SamplingParams:\n         self.presence_penalty = presence_penalty\n         self.repetition_penalty = repetition_penalty\n         self.stop_strs = stop\n-        if stop_token_ids is None:\n-            self.stop_token_ids = set()\n-        else:\n+        if stop_token_ids:\n             self.stop_token_ids = set(stop_token_ids)\n+        else:\n+            self.stop_token_ids = None\n         self.max_new_tokens = max_new_tokens\n         self.min_new_tokens = min_new_tokens\n         self.ignore_eos = ignore_eos\n@@ -134,10 +134,6 @@ class SamplingParams:\n                     stop_str_max_len = max(stop_str_max_len, len(stop_str))\n             self.stop_str_max_len = stop_str_max_len\n \n-        # Process stop token ids\n-        if tokenizer and tokenizer.additional_stop_token_ids:\n-            self.stop_token_ids.update(tokenizer.additional_stop_token_ids)\n-\n     def to_srt_kwargs(self):\n         return {\n             \"max_new_tokens\": self.max_new_tokens,",
  "apis": [
    "sglang.srt.hf_transformers_utils.get_tokenizer",
    "sglang.srt.hf_transformers_utils.get_processor",
    "sglang.srt.sampling.sampling_params.SamplingParams",
    "sglang.srt.sampling.penaltylib.penalizers.BatchedMinNewTokensPenalizer"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/hf_transformers_utils.py",
    "/path/to/repos/sglang/python/sglang/srt/managers/schedule_batch.py",
    "/path/to/repos/sglang/python/sglang/srt/sampling/sampling_params.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit touches several non-test source files and addresses performance issues around handling additional_stop_token_ids. Despite the changes including some renaming and condition reordering, the commit message indicates that it fixes a performance regression, and the modifications alter internal APIs used by key high-level functions (like sampling and scheduling) that are performance-critical on the CPU. This meets our criteria for performance/optimization-related changes rather than being merely a bug fix or simple refactoring.",
  "llm_api_reason": "The commit fixes performance regressions involving handling of additional stop token IDs. In the HuggingFace utilities, the helper function formerly named \u201chandle_additional_stop_token_ids\u201d has been renamed to \u201cattach_additional_stop_token_ids\u201d and is now consistently called in both the get_tokenizer and get_processor functions. In the scheduling and sampling code, the logic for checking EOS conditions now also considers the additional stop token IDs, and the penalizer in min_new_tokens has been updated to include these additional stop tokens in its union of stop token IDs. Additionally, SamplingParams now avoids automatically updating the stop token IDs. These changes improve performance by streamlining the additional stop token id handling."
}