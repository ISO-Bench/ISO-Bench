{
  "commit_hash": "564a898ad975192b593be81387d11faf15cb1d3e",
  "pr_url": "https://github.com/sgl-project/sglang/pull/619",
  "pr_date": "2024-07-14",
  "timeline_text": "Copy link Collaborator hnyls2002 commented Jul 14, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . This #418 can promise that the ref count of token_to_kv is at max 1. Just avoid the torch operation when computing the available size, and use an int var instead, which can bring improvement from 88.1 token / s to 88.8 token / s , approximately 0.8% Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions hnyls2002 added 2 commits July 14, 2024 06:19 update 054e612 format f6cc6d2 hnyls2002 merged commit 564a898 into main Jul 14, 2024 hnyls2002 deleted the optimize-mem-idx branch July 14, 2024 06:39 timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Optimize mem indices mangement ( sgl-project#619 ) a99cad1 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:00:52",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": false,
  "test_details": "PERF",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Optimize mem indices mangement (#619)",
  "commit_message": "Optimize mem indices mangement (#619)",
  "commit_date": "2024-07-13T23:39:37-07:00",
  "files_changed": [
    "benchmark/latency_throughput/bench_one.py",
    "python/sglang/backend/runtime_endpoint.py",
    "python/sglang/bench_latency.py",
    "python/sglang/global_config.py",
    "python/sglang/lang/chat_template.py",
    "python/sglang/lang/ir.py",
    "python/sglang/srt/managers/controller/cuda_graph_runner.py",
    "python/sglang/srt/managers/controller/infer_batch.py",
    "python/sglang/srt/managers/controller/model_runner.py",
    "python/sglang/srt/managers/controller/radix_cache.py",
    "python/sglang/srt/managers/controller/tp_worker.py",
    "python/sglang/srt/memory_pool.py",
    "python/sglang/srt/models/minicpm.py",
    "python/sglang/srt/models/qwen2_moe.py",
    "python/sglang/srt/utils.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 435,
    "num_files": 15,
    "num_hunks": 59,
    "num_non_test_edited_lines": 435,
    "num_non_test_files": 15,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/latency_throughput/bench_one.py b/benchmark/latency_throughput/bench_one.py\nindex b912d3a02..36ae8a436 100644\n--- a/benchmark/latency_throughput/bench_one.py\n+++ b/benchmark/latency_throughput/bench_one.py\n@@ -17,7 +17,8 @@ def run_one_batch_size(bs):\n \n     if args.input_len:\n         input_ids = [\n-            [int(x) for x in np.random.randint(0, high=16384, size=(args.input_len,))] for _ in range(bs)\n+            [int(x) for x in np.random.randint(0, high=16384, size=(args.input_len,))]\n+            for _ in range(bs)\n         ]\n     else:\n         text = [f\"{i, }\" for i in range(bs)]\n@@ -116,9 +117,11 @@ if __name__ == \"__main__\":\n     parser.add_argument(\"--port\", type=int, default=None)\n     parser.add_argument(\"--backend\", type=str, default=\"srt\")\n     parser.add_argument(\"--input-len\", type=int, default=None)\n-    parser.add_argument(\"--batch-size\", type=int, nargs='*', default=[1])\n+    parser.add_argument(\"--batch-size\", type=int, nargs=\"*\", default=[1])\n     parser.add_argument(\"--max-tokens\", type=int, default=256)\n-    parser.add_argument(\"--vllm-model-name\", type=str, default=\"meta-llama/Meta-Llama-3-70B\")\n+    parser.add_argument(\n+        \"--vllm-model-name\", type=str, default=\"meta-llama/Meta-Llama-3-70B\"\n+    )\n     args = parser.parse_args()\n \n     if args.port is None:\ndiff --git a/python/sglang/backend/runtime_endpoint.py b/python/sglang/backend/runtime_endpoint.py\nindex da27a57e9..d845e8116 100644\n--- a/python/sglang/backend/runtime_endpoint.py\n+++ b/python/sglang/backend/runtime_endpoint.py\n@@ -12,7 +12,6 @@ from sglang.utils import http_request\n \n \n class RuntimeEndpoint(BaseBackend):\n-\n     def __init__(\n         self,\n         base_url: str,\n@@ -38,7 +37,8 @@ class RuntimeEndpoint(BaseBackend):\n         self.model_info = res.json()\n \n         self.chat_template = get_chat_template_by_model_path(\n-            self.model_info[\"model_path\"])\n+            self.model_info[\"model_path\"]\n+        )\n \n     def get_model_name(self):\n         return self.model_info[\"model_path\"]\n@@ -124,7 +124,12 @@ class RuntimeEndpoint(BaseBackend):\n         else:\n             raise RuntimeError(f\"Invalid dtype: {sampling_params.dtype}\")\n \n-        for item in [\"return_logprob\", \"logprob_start_len\", \"top_logprobs_num\", \"return_text_in_logprobs\"]:\n+        for item in [\n+            \"return_logprob\",\n+            \"logprob_start_len\",\n+            \"top_logprobs_num\",\n+            \"return_text_in_logprobs\",\n+        ]:\n             value = getattr(sampling_params, item, None)\n             if value is not None:\n                 data[item] = value\n@@ -171,7 +176,12 @@ class RuntimeEndpoint(BaseBackend):\n         else:\n             raise RuntimeError(f\"Invalid dtype: {sampling_params.dtype}\")\n \n-        for item in [\"return_logprob\", \"logprob_start_len\", \"top_logprobs_num\", \"return_text_in_logprobs\"]:\n+        for item in [\n+            \"return_logprob\",\n+            \"logprob_start_len\",\n+            \"top_logprobs_num\",\n+            \"return_text_in_logprobs\",\n+        ]:\n             value = getattr(sampling_params, item, None)\n             if value is not None:\n                 data[item] = value\ndiff --git a/python/sglang/bench_latency.py b/python/sglang/bench_latency.py\nindex 23ec11a34..c4c6d0ecf 100644\n--- a/python/sglang/bench_latency.py\n+++ b/python/sglang/bench_latency.py\n@@ -32,7 +32,6 @@ import logging\n import multiprocessing\n import time\n \n-\n import numpy as np\n import torch\n import torch.distributed as dist\ndiff --git a/python/sglang/global_config.py b/python/sglang/global_config.py\nindex 662cb4a6f..ba2895a9d 100644\n--- a/python/sglang/global_config.py\n+++ b/python/sglang/global_config.py\n@@ -44,4 +44,5 @@ class GlobalConfig:\n         # adjust_cache: Adjust the position embedding of KV cache.\n         self.concate_and_append_mode = \"no_adjust\"\n \n+\n global_config = GlobalConfig()\ndiff --git a/python/sglang/lang/chat_template.py b/python/sglang/lang/chat_template.py\nindex 273eb8c3b..bfde4bbdb 100644\n--- a/python/sglang/lang/chat_template.py\n+++ b/python/sglang/lang/chat_template.py\n@@ -84,7 +84,7 @@ register_chat_template(\n             \"system\": (\"SYSTEM:\", \"\\n\"),\n             \"user\": (\"USER:\", \"\\n\"),\n             \"assistant\": (\"ASSISTANT:\", \"\\n\"),\n-        }\n+        },\n     )\n )\n \n@@ -177,7 +177,7 @@ register_chat_template(\n             \"assistant\": (\"\", \"<|im_end|>\\n\"),\n         },\n         style=ChatTemplateStyle.PLAIN,\n-        stop_str=(\"<|im_end|>\",)\n+        stop_str=(\"<|im_end|>\",),\n     )\n )\n \ndiff --git a/python/sglang/lang/ir.py b/python/sglang/lang/ir.py\nindex 83c6f79b0..e5d5e837a 100644\n--- a/python/sglang/lang/ir.py\n+++ b/python/sglang/lang/ir.py\n@@ -24,9 +24,9 @@ class SglSamplingParams:\n     presence_penalty: float = 0.0\n     ignore_eos: bool = False\n     return_logprob: Optional[bool] = None\n-    logprob_start_len: Optional[int] = None,\n-    top_logprobs_num: Optional[int] = None,\n-    return_text_in_logprobs: Optional[bool] = None,\n+    logprob_start_len: Optional[int] = (None,)\n+    top_logprobs_num: Optional[int] = (None,)\n+    return_text_in_logprobs: Optional[bool] = (None,)\n \n     # for constrained generation, not included in to_xxx_kwargs\n     dtype: Optional[str] = None\ndiff --git a/python/sglang/srt/managers/controller/cuda_graph_runner.py b/python/sglang/srt/managers/controller/cuda_graph_runner.py\nindex 2e37e55b5..7218936be 100644\n--- a/python/sglang/srt/managers/controller/cuda_graph_runner.py\n+++ b/python/sglang/srt/managers/controller/cuda_graph_runner.py\n@@ -8,7 +8,10 @@ from vllm.distributed.parallel_state import graph_capture\n from sglang.global_config import global_config\n from sglang.srt.layers.logits_processor import LogitProcessorOutput\n from sglang.srt.managers.controller.infer_batch import (\n-    Batch, ForwardMode, InputMetadata, init_flashinfer_args\n+    Batch,\n+    ForwardMode,\n+    InputMetadata,\n+    init_flashinfer_args,\n )\n \n \n@@ -24,18 +27,28 @@ class CudaGraphRunner:\n         # Common inputs\n         self.max_bs = max_batch_size_to_capture\n         self.input_ids = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.req_pool_indices = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.req_pool_indices = torch.zeros(\n+            (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n+        )\n         self.seq_lens = torch.ones((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.position_ids_offsets = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n-        self.out_cache_loc = torch.zeros((self.max_bs,), dtype=torch.int32, device=\"cuda\")\n+        self.position_ids_offsets = torch.zeros(\n+            (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n+        )\n+        self.out_cache_loc = torch.zeros(\n+            (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n+        )\n \n         # FlashInfer inputs\n-        self.flashinfer_workspace_buffer = self.model_runner.flashinfer_workspace_buffers[0]\n+        self.flashinfer_workspace_buffer = (\n+            self.model_runner.flashinfer_workspace_buffers[0]\n+        )\n         self.flashinfer_kv_indptr = torch.zeros(\n             (self.max_bs + 1,), dtype=torch.int32, device=\"cuda\"\n         )\n         self.flashinfer_kv_indices = torch.zeros(\n-            (self.max_bs * model_runner.model_config.context_len,), dtype=torch.int32, device=\"cuda\"\n+            (self.max_bs * model_runner.model_config.context_len,),\n+            dtype=torch.int32,\n+            device=\"cuda\",\n         )\n         self.flashinfer_kv_last_page_len = torch.ones(\n             (self.max_bs,), dtype=torch.int32, device=\"cuda\"\n@@ -49,7 +62,12 @@ class CudaGraphRunner:\n         with graph_capture() as graph_capture_context:\n             self.stream = graph_capture_context.stream\n             for bs in batch_size_list:\n-                graph, input_buffers, output_buffers, flashinfer_handler = self.capture_one_batch_size(bs)\n+                (\n+                    graph,\n+                    input_buffers,\n+                    output_buffers,\n+                    flashinfer_handler,\n+                ) = self.capture_one_batch_size(bs)\n                 self.graphs[bs] = graph\n                 self.input_buffers[bs] = input_buffers\n                 self.output_buffers[bs] = output_buffers\n@@ -71,17 +89,19 @@ class CudaGraphRunner:\n \n         # FlashInfer inputs\n         if not _grouped_size_compiled_for_decode_kernels(\n-            self.model_runner.model_config.num_attention_heads // self.model_runner.tp_size,\n+            self.model_runner.model_config.num_attention_heads\n+            // self.model_runner.tp_size,\n             self.model_runner.model_config.get_num_kv_heads(self.model_runner.tp_size),\n         ):\n             use_tensor_cores = True\n         else:\n             use_tensor_cores = False\n         flashinfer_decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(\n-            self.flashinfer_workspace_buffer, \"NHD\",\n+            self.flashinfer_workspace_buffer,\n+            \"NHD\",\n             use_cuda_graph=True,\n             use_tensor_cores=use_tensor_cores,\n-            paged_kv_indptr_buffer=self.flashinfer_kv_indptr[:bs+1],\n+            paged_kv_indptr_buffer=self.flashinfer_kv_indptr[: bs + 1],\n             paged_kv_indices_buffer=self.flashinfer_kv_indices,\n             paged_kv_last_page_len_buffer=self.flashinfer_kv_last_page_len[:bs],\n         )\n@@ -163,10 +183,14 @@ class CudaGraphRunner:\n         else:\n             output = LogitProcessorOutput(\n                 next_token_logits=output.next_token_logits[:raw_bs],\n-                next_token_logprobs=output.next_token_logprobs[:raw_bs] if output.next_token_logprobs is not None else None,\n+                next_token_logprobs=output.next_token_logprobs[:raw_bs]\n+                if output.next_token_logprobs is not None\n+                else None,\n                 normalized_prompt_logprobs=None,\n                 prefill_token_logprobs=None,\n                 prefill_top_logprobs=None,\n-                decode_top_logprobs=output.decode_top_logprobs[:raw_bs] if output.decode_top_logprobs is not None else None,\n+                decode_top_logprobs=output.decode_top_logprobs[:raw_bs]\n+                if output.decode_top_logprobs is not None\n+                else None,\n             )\n         return output\ndiff --git a/python/sglang/srt/managers/controller/infer_batch.py b/python/sglang/srt/managers/controller/infer_batch.py\nindex d89e9786e..375ec6eeb 100644\n--- a/python/sglang/srt/managers/controller/infer_batch.py\n+++ b/python/sglang/srt/managers/controller/infer_batch.py\n@@ -668,7 +668,9 @@ class Batch:\n             sampled_index = torch.multinomial(probs_sort, num_samples=1)\n         except RuntimeError as e:\n             warnings.warn(f\"Ignore errors in sampling: {e}\")\n-            sampled_index = torch.ones(probs_sort.shape[:-1] + (1,), dtype=torch.int64, device=probs.device)\n+            sampled_index = torch.ones(\n+                probs_sort.shape[:-1] + (1,), dtype=torch.int64, device=probs.device\n+            )\n         batch_next_token_ids = torch.gather(probs_idx, dim=1, index=sampled_index).view(\n             -1\n         )\n@@ -749,8 +751,14 @@ class InputMetadata:\n         skip_flashinfer_init=False,\n     ):\n         if not skip_flashinfer_init and not model_runner.server_args.disable_flashinfer:\n-            init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens, prefix_lens,\n-                                 model_runner.flashinfer_decode_wrapper)\n+            init_flashinfer_args(\n+                forward_mode,\n+                model_runner,\n+                req_pool_indices,\n+                seq_lens,\n+                prefix_lens,\n+                model_runner.flashinfer_decode_wrapper,\n+            )\n \n         batch_size = len(req_pool_indices)\n \n@@ -807,16 +815,24 @@ class InputMetadata:\n         )\n \n         if model_runner.server_args.disable_flashinfer:\n-            (ret.triton_max_seq_len,\n-             ret.triton_max_extend_len,\n-             ret.triton_start_loc,\n-             ret.triton_prefix_lens) = init_triton_args(forward_mode, seq_lens, prefix_lens)\n+            (\n+                ret.triton_max_seq_len,\n+                ret.triton_max_extend_len,\n+                ret.triton_start_loc,\n+                ret.triton_prefix_lens,\n+            ) = init_triton_args(forward_mode, seq_lens, prefix_lens)\n \n         return ret\n \n \n-def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens, prefix_lens,\n-                         flashinfer_decode_wrapper):\n+def init_flashinfer_args(\n+    forward_mode,\n+    model_runner,\n+    req_pool_indices,\n+    seq_lens,\n+    prefix_lens,\n+    flashinfer_decode_wrapper,\n+):\n     num_qo_heads = model_runner.model_config.num_attention_heads // model_runner.tp_size\n     num_kv_heads = model_runner.model_config.get_num_kv_heads(model_runner.tp_size)\n     head_dim = model_runner.model_config.head_dim\n@@ -827,9 +843,7 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n     else:\n         paged_kernel_lens = prefix_lens\n \n-    kv_indptr = torch.zeros(\n-        (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n-    )\n+    kv_indptr = torch.zeros((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n     kv_indptr[1:] = torch.cumsum(paged_kernel_lens, dim=0)\n     req_pool_indices_cpu = req_pool_indices.cpu().numpy()\n     paged_kernel_lens_cpu = paged_kernel_lens.cpu().numpy()\n@@ -842,9 +856,7 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n         ],\n         dim=0,\n     ).contiguous()\n-    kv_last_page_len = torch.ones(\n-        (batch_size,), dtype=torch.int32, device=\"cuda\"\n-    )\n+    kv_last_page_len = torch.ones((batch_size,), dtype=torch.int32, device=\"cuda\")\n \n     if forward_mode == ForwardMode.DECODE:\n         flashinfer_decode_wrapper.end_forward()\n@@ -859,9 +871,7 @@ def init_flashinfer_args(forward_mode, model_runner, req_pool_indices, seq_lens,\n         )\n     else:\n         # extend part\n-        qo_indptr = torch.zeros(\n-            (batch_size + 1,), dtype=torch.int32, device=\"cuda\"\n-        )\n+        qo_indptr = torch.zeros((batch_size + 1,), dtype=torch.int32, device=\"cuda\")\n         qo_indptr[1:] = torch.cumsum(seq_lens - prefix_lens, dim=0)\n \n         model_runner.flashinfer_prefill_wrapper_ragged.end_forward()\ndiff --git a/python/sglang/srt/managers/controller/model_runner.py b/python/sglang/srt/managers/controller/model_runner.py\nindex 315dd4d66..d68d9af32 100644\n--- a/python/sglang/srt/managers/controller/model_runner.py\n+++ b/python/sglang/srt/managers/controller/model_runner.py\n@@ -16,7 +16,12 @@ from vllm.model_executor.model_loader import get_model\n from vllm.model_executor.models import ModelRegistry\n \n from sglang.global_config import global_config\n-from sglang.srt.managers.controller.infer_batch import Batch, ForwardMode, InputMetadata, global_server_args_dict\n+from sglang.srt.managers.controller.infer_batch import (\n+    Batch,\n+    ForwardMode,\n+    InputMetadata,\n+    global_server_args_dict,\n+)\n from sglang.srt.memory_pool import ReqToTokenPool, TokenToKVPool\n from sglang.srt.server_args import ServerArgs\n from sglang.srt.utils import (\n@@ -83,7 +88,9 @@ class ModelRunner:\n \n         # Set some global args\n         global_server_args_dict[\"disable_flashinfer\"] = server_args.disable_flashinfer\n-        global_server_args_dict[\"attention_reduce_in_fp32\"] = server_args.attention_reduce_in_fp32\n+        global_server_args_dict[\n+            \"attention_reduce_in_fp32\"\n+        ] = server_args.attention_reduce_in_fp32\n \n         # Load the model and create memory pool\n         self.load_model()\n@@ -217,7 +224,9 @@ class ModelRunner:\n             self.flashinfer_workspace_buffers[1], \"NHD\"\n         )\n         self.flashinfer_decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(\n-            self.flashinfer_workspace_buffers[0], \"NHD\", use_tensor_cores=use_tensor_cores\n+            self.flashinfer_workspace_buffers[0],\n+            \"NHD\",\n+            use_tensor_cores=use_tensor_cores,\n         )\n \n     def init_cuda_graphs(self):\n@@ -229,7 +238,9 @@ class ModelRunner:\n \n         logger.info(f\"[gpu_id={self.gpu_id}] Capture cuda graph begin.\")\n         batch_size_list = [1, 2, 4] + [i * 8 for i in range(1, 16)]\n-        self.cuda_graph_runner = CudaGraphRunner(self, max_batch_size_to_capture=max(batch_size_list))\n+        self.cuda_graph_runner = CudaGraphRunner(\n+            self, max_batch_size_to_capture=max(batch_size_list)\n+        )\n         self.cuda_graph_runner.capture(batch_size_list)\n \n     @torch.inference_mode()\ndiff --git a/python/sglang/srt/managers/controller/radix_cache.py b/python/sglang/srt/managers/controller/radix_cache.py\nindex ab8d6b446..bc7b758dd 100644\n--- a/python/sglang/srt/managers/controller/radix_cache.py\n+++ b/python/sglang/srt/managers/controller/radix_cache.py\n@@ -125,7 +125,8 @@ class RadixCache:\n             if x.lock_ref > 0:\n                 continue\n \n-            num_evicted += evict_callback(x.value)\n+            evict_callback(x.value)\n+            num_evicted += len(x.value)\n             self._delete_leaf(x)\n \n             if len(x.parent.children) == 0:\ndiff --git a/python/sglang/srt/managers/controller/tp_worker.py b/python/sglang/srt/managers/controller/tp_worker.py\nindex b572e120e..12c278fd5 100644\n--- a/python/sglang/srt/managers/controller/tp_worker.py\n+++ b/python/sglang/srt/managers/controller/tp_worker.py\n@@ -314,7 +314,9 @@ class ModelTpServer:\n         self.forward_queue.append(req)\n \n     def get_new_fill_batch(self) -> Optional[Batch]:\n-        running_bs = len(self.running_batch.reqs) if self.running_batch is not None else 0\n+        running_bs = (\n+            len(self.running_batch.reqs) if self.running_batch is not None else 0\n+        )\n         if running_bs >= self.max_running_requests:\n             return\n \ndiff --git a/python/sglang/srt/memory_pool.py b/python/sglang/srt/memory_pool.py\nindex 245e6ef08..46010ccf7 100644\n--- a/python/sglang/srt/memory_pool.py\n+++ b/python/sglang/srt/memory_pool.py\n@@ -39,10 +39,12 @@ class ReqToTokenPool:\n class TokenToKVPool:\n     def __init__(self, size, dtype, head_num, head_dim, layer_num):\n         self.size = size\n-        # mem_state is the reference counter.\n+        # This can be promised:\n+        # assert torch.all(mem_state <= 1) and torch.all(mem_state >= 0)\n         # We also add one slot. This slot is used for writing dummy output from padded tokens.\n-        self.mem_state = torch.zeros((self.size + 1,), dtype=torch.int16, device=\"cuda\")\n-        self.total_ref_ct = 0\n+        self.mem_state = torch.zeros((self.size + 1,), dtype=torch.bool, device=\"cuda\")\n+        self.total_size = self.size\n+        self.total_alloc = 0\n \n         # [size, key/value, head_num, head_dim] for each layer\n         self.kv_data = [\n@@ -71,7 +73,9 @@ class TokenToKVPool:\n \n         addition_size = need_size - buffer_len\n         alloc_size = max(addition_size, self.prefetch_chunk_size)\n-        select_index = torch.nonzero(self.mem_state == 0).squeeze(1)[:alloc_size].to(torch.int32)\n+        select_index = (\n+            torch.nonzero(self.mem_state == 0).squeeze(1)[:alloc_size].to(torch.int32)\n+        )\n \n         if select_index.shape[0] < addition_size:\n             return None\n@@ -105,26 +109,22 @@ class TokenToKVPool:\n         return select_index.to(torch.int32), start_loc, start_loc + need_size\n \n     def used_size(self):\n-        return len(torch.nonzero(self.mem_state).squeeze(1))\n+        return self.total_alloc\n \n     def available_size(self):\n-        return torch.sum(self.mem_state == 0).item() + len(self.prefetch_buffer)\n+        return self.total_size - self.total_alloc + len(self.prefetch_buffer)\n \n     def add_refs(self, token_index: torch.Tensor):\n-        self.total_ref_ct += len(token_index)\n-        self.mem_state[token_index] += 1\n+        self.total_alloc += len(token_index)\n+        self.mem_state[token_index] ^= True\n \n     def dec_refs(self, token_index: torch.Tensor):\n-        self.total_ref_ct -= len(token_index)\n-        self.mem_state[token_index] -= 1\n-\n-        num_freed = torch.sum(self.mem_state[token_index] == 0)\n-\n-        return num_freed\n+        self.total_alloc -= len(token_index)\n+        self.mem_state[token_index] ^= True\n \n     def clear(self):\n         self.mem_state.fill_(0)\n-        self.total_ref_ct = 0\n+        self.total_alloc = 0\n \n         # We also add one slot. This slot is used for writing dummy output from padded tokens.\n-        self.add_refs(torch.tensor([0], dtype=torch.int32))\n+        self.mem_state[0] = True\ndiff --git a/python/sglang/srt/models/minicpm.py b/python/sglang/srt/models/minicpm.py\nindex 072bf99ab..3f16c95f9 100644\n--- a/python/sglang/srt/models/minicpm.py\n+++ b/python/sglang/srt/models/minicpm.py\n@@ -5,12 +5,9 @@ from typing import Any, Dict, Iterable, Optional, Tuple\n \n import torch\n from torch import nn\n-\n from vllm.config import CacheConfig\n from vllm.distributed import get_tensor_model_parallel_world_size\n-\n from vllm.model_executor.layers.activation import SiluAndMul\n-\n from vllm.model_executor.layers.layernorm import RMSNorm\n from vllm.model_executor.layers.linear import (\n     MergedColumnParallelLinear,\n@@ -31,7 +28,6 @@ from sglang.srt.managers.controller.model_runner import InputMetadata\n \n \n class MiniCPMMLP(nn.Module):\n-\n     def __init__(\n         self,\n         hidden_size: int,\n@@ -67,7 +63,6 @@ class MiniCPMMLP(nn.Module):\n \n \n class MiniCPMAttention(nn.Module):\n-\n     def __init__(\n         self,\n         hidden_size: int,\n@@ -152,7 +147,6 @@ class MiniCPMAttention(nn.Module):\n \n \n class MiniCPMDecoderLayer(nn.Module):\n-\n     def __init__(\n         self,\n         config,\n@@ -217,7 +211,6 @@ class MiniCPMDecoderLayer(nn.Module):\n \n \n class MiniCPMModel(nn.Module):\n-\n     def __init__(\n         self,\n         config,\n@@ -274,7 +267,7 @@ class MiniCPMForCausalLM(nn.Module):\n     ) -> None:\n         super().__init__()\n         self.config = config\n-        \n+\n         self.num_experts = getattr(self.config, \"num_experts\", 0)\n         self.quant_config = quant_config\n         self.model = MiniCPMModel(config, quant_config=quant_config)\ndiff --git a/python/sglang/srt/models/qwen2_moe.py b/python/sglang/srt/models/qwen2_moe.py\nindex 79187cd43..072002c6f 100644\n--- a/python/sglang/srt/models/qwen2_moe.py\n+++ b/python/sglang/srt/models/qwen2_moe.py\n@@ -8,24 +8,28 @@ import torch\n import torch.nn.functional as F\n from torch import nn\n from transformers import PretrainedConfig\n-\n from vllm.config import CacheConfig\n-from vllm.distributed import (get_tensor_model_parallel_world_size,\n-                              tensor_model_parallel_all_reduce)\n+from vllm.distributed import (\n+    get_tensor_model_parallel_world_size,\n+    tensor_model_parallel_all_reduce,\n+)\n from vllm.model_executor.layers.activation import SiluAndMul\n from vllm.model_executor.layers.fused_moe import FusedMoE\n from vllm.model_executor.layers.layernorm import RMSNorm\n-from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n-                                               QKVParallelLinear,\n-                                               ReplicatedLinear,\n-                                               RowParallelLinear)\n+from vllm.model_executor.layers.linear import (\n+    MergedColumnParallelLinear,\n+    QKVParallelLinear,\n+    ReplicatedLinear,\n+    RowParallelLinear,\n+)\n from vllm.model_executor.layers.logits_processor import LogitsProcessor\n-from vllm.model_executor.layers.quantization.base_config import (\n-    QuantizationConfig)\n+from vllm.model_executor.layers.quantization.base_config import QuantizationConfig\n from vllm.model_executor.layers.rotary_embedding import get_rope\n from vllm.model_executor.layers.sampler import Sampler\n from vllm.model_executor.layers.vocab_parallel_embedding import (\n-    ParallelLMHead, VocabParallelEmbedding)\n+    ParallelLMHead,\n+    VocabParallelEmbedding,\n+)\n from vllm.model_executor.model_loader.weight_utils import default_weight_loader\n from vllm.model_executor.sampling_metadata import SamplingMetadata\n from vllm.sequence import IntermediateTensors, SamplerOutput\n@@ -34,8 +38,8 @@ from sglang.srt.layers.logits_processor import LogitsProcessor\n from sglang.srt.layers.radix_attention import RadixAttention\n from sglang.srt.managers.controller.model_runner import InputMetadata\n \n-class Qwen2MoeMLP(nn.Module):\n \n+class Qwen2MoeMLP(nn.Module):\n     def __init__(\n         self,\n         hidden_size: int,\n@@ -46,17 +50,20 @@ class Qwen2MoeMLP(nn.Module):\n     ) -> None:\n         super().__init__()\n         self.gate_up_proj = MergedColumnParallelLinear(\n-            hidden_size, [intermediate_size] * 2,\n+            hidden_size, [intermediate_size] * 2, bias=False, quant_config=quant_config\n+        )\n+        self.down_proj = RowParallelLinear(\n+            intermediate_size,\n+            hidden_size,\n             bias=False,\n-            quant_config=quant_config)\n-        self.down_proj = RowParallelLinear(intermediate_size,\n-                                           hidden_size,\n-                                           bias=False,\n-                                           quant_config=quant_config,\n-                                           reduce_results=reduce_results)\n+            quant_config=quant_config,\n+            reduce_results=reduce_results,\n+        )\n         if hidden_act != \"silu\":\n-            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n-                             \"Only silu is supported for now.\")\n+            raise ValueError(\n+                f\"Unsupported activation: {hidden_act}. \"\n+                \"Only silu is supported for now.\"\n+            )\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n@@ -67,7 +74,6 @@ class Qwen2MoeMLP(nn.Module):\n \n \n class Qwen2MoeSparseMoeBlock(nn.Module):\n-\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -79,20 +85,22 @@ class Qwen2MoeSparseMoeBlock(nn.Module):\n         if self.tp_size > config.num_experts:\n             raise ValueError(\n                 f\"Tensor parallel size {self.tp_size} is greater than \"\n-                f\"the number of experts {config.num_experts}.\")\n-\n-        self.experts = FusedMoE(num_experts=config.num_experts,\n-                                top_k=config.num_experts_per_tok,\n-                                hidden_size=config.hidden_size,\n-                                intermediate_size=config.moe_intermediate_size,\n-                                reduce_results=False,\n-                                renormalize=config.norm_topk_prob,\n-                                quant_config=quant_config)\n-\n-        self.gate = ReplicatedLinear(config.hidden_size,\n-                                     config.num_experts,\n-                                     bias=False,\n-                                     quant_config=None)\n+                f\"the number of experts {config.num_experts}.\"\n+            )\n+\n+        self.experts = FusedMoE(\n+            num_experts=config.num_experts,\n+            top_k=config.num_experts_per_tok,\n+            hidden_size=config.hidden_size,\n+            intermediate_size=config.moe_intermediate_size,\n+            reduce_results=False,\n+            renormalize=config.norm_topk_prob,\n+            quant_config=quant_config,\n+        )\n+\n+        self.gate = ReplicatedLinear(\n+            config.hidden_size, config.num_experts, bias=False, quant_config=None\n+        )\n         if config.shared_expert_intermediate_size > 0:\n             self.shared_expert = Qwen2MoeMLP(\n                 hidden_size=config.hidden_size,\n@@ -103,9 +111,7 @@ class Qwen2MoeSparseMoeBlock(nn.Module):\n             )\n         else:\n             self.shared_expert = None\n-        self.shared_expert_gate = torch.nn.Linear(config.hidden_size,\n-                                                  1,\n-                                                  bias=False)\n+        self.shared_expert_gate = torch.nn.Linear(config.hidden_size, 1, bias=False)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         num_tokens, hidden_dim = hidden_states.shape\n@@ -114,24 +120,24 @@ class Qwen2MoeSparseMoeBlock(nn.Module):\n         if self.shared_expert is not None:\n             shared_output = self.shared_expert(hidden_states)\n             if self.shared_expert_gate is not None:\n-                shared_output = F.sigmoid(\n-                    self.shared_expert_gate(hidden_states)) * shared_output\n+                shared_output = (\n+                    F.sigmoid(self.shared_expert_gate(hidden_states)) * shared_output\n+                )\n \n         # router_logits: (num_tokens, n_experts)\n         router_logits, _ = self.gate(hidden_states)\n-        final_hidden_states = self.experts(hidden_states=hidden_states,\n-                                           router_logits=router_logits)\n+        final_hidden_states = self.experts(\n+            hidden_states=hidden_states, router_logits=router_logits\n+        )\n         if shared_output is not None:\n             final_hidden_states = final_hidden_states + shared_output\n         if self.tp_size > 1:\n-            final_hidden_states = tensor_model_parallel_all_reduce(\n-                final_hidden_states)\n+            final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)\n \n         return final_hidden_states.view(num_tokens, hidden_dim)\n \n \n class Qwen2MoeAttention(nn.Module):\n-\n     def __init__(\n         self,\n         hidden_size: int,\n@@ -190,17 +196,19 @@ class Qwen2MoeAttention(nn.Module):\n             base=rope_theta,\n             rope_scaling=rope_scaling,\n         )\n-        self.attn = RadixAttention(self.num_heads,\n-                                   self.head_dim,\n-                                   self.scaling,\n-                                   num_kv_heads=self.num_kv_heads,\n-                                   layer_id=layer_id)\n+        self.attn = RadixAttention(\n+            self.num_heads,\n+            self.head_dim,\n+            self.scaling,\n+            num_kv_heads=self.num_kv_heads,\n+            layer_id=layer_id,\n+        )\n \n     def forward(\n         self,\n         positions: torch.Tensor,\n         hidden_states: torch.Tensor,\n-        input_metadata: InputMetadata\n+        input_metadata: InputMetadata,\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n         q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n@@ -211,7 +219,6 @@ class Qwen2MoeAttention(nn.Module):\n \n \n class Qwen2MoeDecoderLayer(nn.Module):\n-\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -223,8 +230,7 @@ class Qwen2MoeDecoderLayer(nn.Module):\n         self.hidden_size = config.hidden_size\n         rope_theta = getattr(config, \"rope_theta\", 10000)\n         rope_scaling = getattr(config, \"rope_scaling\", None)\n-        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n-                                          8192)\n+        max_position_embeddings = getattr(config, \"max_position_embeddings\", 8192)\n         self.self_attn = Qwen2MoeAttention(\n             hidden_size=self.hidden_size,\n             num_heads=config.num_attention_heads,\n@@ -239,13 +245,13 @@ class Qwen2MoeDecoderLayer(nn.Module):\n \n         # Note: Qwen/Qwen2-57B-A14B-Instruct does not have\n         # `mlp_only_layers` in the config.\n-        mlp_only_layers = ([] if not hasattr(config, \"mlp_only_layers\") else\n-                           config.mlp_only_layers)\n+        mlp_only_layers = (\n+            [] if not hasattr(config, \"mlp_only_layers\") else config.mlp_only_layers\n+        )\n         if (layer_id not in mlp_only_layers) and (\n-                config.num_experts > 0 and\n-            (layer_id + 1) % config.decoder_sparse_step == 0):\n-            self.mlp = Qwen2MoeSparseMoeBlock(config=config,\n-                                              quant_config=quant_config)\n+            config.num_experts > 0 and (layer_id + 1) % config.decoder_sparse_step == 0\n+        ):\n+            self.mlp = Qwen2MoeSparseMoeBlock(config=config, quant_config=quant_config)\n         else:\n             self.mlp = Qwen2MoeMLP(\n                 hidden_size=config.hidden_size,\n@@ -253,10 +259,10 @@ class Qwen2MoeDecoderLayer(nn.Module):\n                 hidden_act=config.hidden_act,\n                 quant_config=quant_config,\n             )\n-        self.input_layernorm = RMSNorm(config.hidden_size,\n-                                       eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n-                                                eps=config.rms_norm_eps)\n+        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = RMSNorm(\n+            config.hidden_size, eps=config.rms_norm_eps\n+        )\n \n     def forward(\n         self,\n@@ -270,23 +276,20 @@ class Qwen2MoeDecoderLayer(nn.Module):\n             residual = hidden_states\n             hidden_states = self.input_layernorm(hidden_states)\n         else:\n-            hidden_states, residual = self.input_layernorm(\n-                hidden_states, residual)\n+            hidden_states, residual = self.input_layernorm(hidden_states, residual)\n         hidden_states = self.self_attn(\n             positions=positions,\n             hidden_states=hidden_states,\n-            input_metadata=input_metadata\n+            input_metadata=input_metadata,\n         )\n \n         # Fully Connected\n-        hidden_states, residual = self.post_attention_layernorm(\n-            hidden_states, residual)\n+        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)\n         hidden_states = self.mlp(hidden_states)\n         return hidden_states, residual\n \n \n class Qwen2MoeModel(nn.Module):\n-\n     def __init__(\n         self,\n         config: PretrainedConfig,\n@@ -301,13 +304,14 @@ class Qwen2MoeModel(nn.Module):\n             config.vocab_size,\n             config.hidden_size,\n         )\n-        self.layers = nn.ModuleList([\n-            Qwen2MoeDecoderLayer(config,\n-                                 layer_id,\n-                                 cache_config,\n-                                 quant_config=quant_config)\n-            for layer_id in range(config.num_hidden_layers)\n-        ])\n+        self.layers = nn.ModuleList(\n+            [\n+                Qwen2MoeDecoderLayer(\n+                    config, layer_id, cache_config, quant_config=quant_config\n+                )\n+                for layer_id in range(config.num_hidden_layers)\n+            ]\n+        )\n         self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n@@ -315,7 +319,7 @@ class Qwen2MoeModel(nn.Module):\n         input_ids: torch.Tensor,\n         positions: torch.Tensor,\n         input_metadata: InputMetadata,\n-        input_embeds: torch.Tensor = None\n+        input_embeds: torch.Tensor = None,\n     ) -> torch.Tensor:\n         if input_embeds is None:\n             hidden_states = self.embed_tokens(input_ids)\n@@ -324,10 +328,9 @@ class Qwen2MoeModel(nn.Module):\n         residual = None\n         for i in range(len(self.layers)):\n             layer = self.layers[i]\n-            hidden_states, residual = layer(positions,\n-                                            hidden_states,\n-                                            input_metadata,\n-                                            residual)\n+            hidden_states, residual = layer(\n+                positions, hidden_states, input_metadata, residual\n+            )\n         hidden_states, _ = self.norm(hidden_states, residual)\n         return hidden_states\n \n@@ -346,9 +349,9 @@ class Qwen2MoeForCausalLM(nn.Module):\n         self.config = config\n         self.quant_config = quant_config\n         self.model = Qwen2MoeModel(config, cache_config, quant_config)\n-        self.lm_head = ParallelLMHead(config.vocab_size,\n-                                      config.hidden_size,\n-                                      quant_config=quant_config)\n+        self.lm_head = ParallelLMHead(\n+            config.vocab_size, config.hidden_size, quant_config=quant_config\n+        )\n         self.logits_processor = LogitsProcessor(config)\n         self.sampler = Sampler()\n \n@@ -357,17 +360,22 @@ class Qwen2MoeForCausalLM(nn.Module):\n         input_ids: torch.Tensor,\n         positions: torch.Tensor,\n         input_metadata: InputMetadata,\n-        input_embeds: torch.Tensor = None\n+        input_embeds: torch.Tensor = None,\n+    ) -> torch.Tensor:\n+        hidden_states = self.model(input_ids, positions, input_metadata, input_embeds)\n+        return self.logits_processor(\n+            input_ids, hidden_states, self.lm_head.weight, input_metadata\n+        )\n+\n+    def compute_logits(\n+        self,\n+        input_ids: torch.Tensor,\n+        hidden_states: torch.Tensor,\n+        input_metadata: InputMetadata,\n     ) -> torch.Tensor:\n-        hidden_states = self.model(input_ids, positions, input_metadata,\n-                                   input_embeds)\n-        return self.logits_processor(input_ids, hidden_states, self.lm_head.weight,\n-                                     input_metadata)\n-\n-    def compute_logits(self, input_ids: torch.Tensor, hidden_states: torch.Tensor,\n-                       input_metadata: InputMetadata) -> torch.Tensor:\n-        logits = self.logits_processor(input_ids, hidden_states, self.lm_head.weight,\n-                                       input_metadata)\n+        logits = self.logits_processor(\n+            input_ids, hidden_states, self.lm_head.weight, input_metadata\n+        )\n         return logits\n \n     def sample(\n@@ -391,11 +399,18 @@ class Qwen2MoeForCausalLM(nn.Module):\n         expert_params_mapping = [\n             # These are the weights for the experts\n             # (param_name, weight_name, expert_id, shard_id)\n-            (\"experts.w13_weight\" if weight_name in [\"gate_proj\", \"up_proj\"]\n-             else \"experts.w2_weight\",\n-             f\"experts.{expert_id}.{weight_name}.weight\", expert_id, shard_id)\n-            for expert_id in range(self.config.num_experts) for shard_id,\n-            weight_name in enumerate([\"gate_proj\", \"down_proj\", \"up_proj\"])\n+            (\n+                \"experts.w13_weight\"\n+                if weight_name in [\"gate_proj\", \"up_proj\"]\n+                else \"experts.w2_weight\",\n+                f\"experts.{expert_id}.{weight_name}.weight\",\n+                expert_id,\n+                shard_id,\n+            )\n+            for expert_id in range(self.config.num_experts)\n+            for shard_id, weight_name in enumerate(\n+                [\"gate_proj\", \"down_proj\", \"up_proj\"]\n+            )\n         ]\n \n         params_dict = dict(self.named_parameters())\n@@ -433,11 +448,13 @@ class Qwen2MoeForCausalLM(nn.Module):\n                     name = name.replace(weight_name, param_name)\n                     param = params_dict[name]\n                     weight_loader = param.weight_loader\n-                    weight_loader(param,\n-                                  loaded_weight,\n-                                  weight_name,\n-                                  shard_id=shard_id,\n-                                  expert_id=expert_id)\n+                    weight_loader(\n+                        param,\n+                        loaded_weight,\n+                        weight_name,\n+                        shard_id=shard_id,\n+                        expert_id=expert_id,\n+                    )\n                     break\n                 else:\n                     # Skip loading extra bias for GPTQ models.\n@@ -447,8 +464,10 @@ class Qwen2MoeForCausalLM(nn.Module):\n                         continue\n \n                     param = params_dict[name]\n-                    weight_loader = getattr(param, \"weight_loader\",\n-                                            default_weight_loader)\n+                    weight_loader = getattr(\n+                        param, \"weight_loader\", default_weight_loader\n+                    )\n                     weight_loader(param, loaded_weight)\n \n+\n EntryClass = Qwen2MoeForCausalLM\ndiff --git a/python/sglang/srt/utils.py b/python/sglang/srt/utils.py\nindex 03a2d60ab..981b5e218 100644\n--- a/python/sglang/srt/utils.py\n+++ b/python/sglang/srt/utils.py\n@@ -474,9 +474,9 @@ def monkey_patch_vllm_dummy_weight_loader():\n         DummyModelLoader,\n         LoRAConfig,\n         ModelConfig,\n+        MultiModalConfig,\n         ParallelConfig,\n         SchedulerConfig,\n-        MultiModalConfig,\n         _initialize_model,\n         initialize_dummy_weights,\n         nn,",
  "apis": [
    "RuntimeEndpoint",
    "SglSamplingParams",
    "TokenToKVPool",
    "CudaGraphRunner",
    "Qwen2MoeForCausalLM"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/lang/backend/runtime_endpoint.py",
    "/path/to/repos/sglang/python/sglang/srt/model_executor/model_runner.py",
    "/path/to/repos/sglang/python/sglang/srt/models/qwen2_moe.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit touches multiple non-test source files and includes non-trivial changes, particularly in the memory pool management (in python/sglang/srt/memory_pool.py). The modifications change how memory reference counters are handled (switching from integer-based counters to a boolean flag with a separate total allocation counter) and adjust memory indexing operations, which are likely intended to optimize performance. While many changes are stylistic or for code formatting, the alterations in the memory indices management represent performance optimizations that affect internal APIs and overall efficiency on CPU. Therefore, this commit satisfies the conditions for being performance/optimization related.",
  "llm_api_reason": "This commit makes several cleanup and formatting changes across various modules while also refactoring parts related to memory index management. In particular, it adjusts how token\u2010to\u2010KV cache indices are allocated (in TokenToKVPool), updates default values in sampling parameters (SglSamplingParams in IR), and reorganizes code in runtime endpoint, CUDA graph runner and a Qwen2MoE model. These changes optimize memory management and improve code clarity in core runtime and model components."
}