{
  "commit_hash": "6e2da5156176ed2d7fe2445b7c7316bc1650b20a",
  "pr_url": "https://github.com/sgl-project/sglang/pull/6178",
  "pr_date": "2025-05-11",
  "timeline_text": "Copy link Collaborator lifuhuang commented May 10, 2025 Motivation Currently, SGL measures duration using wall clock (time.time()), which is not recommended for interval measuring purpose as it does not guarantee monotonicity (e.g., due to NTP sync) and has lower resolution. perf_counter should be used instead for perf measurement puropses. More details can be read here: PEP 418 . Example in vllm: benchmark_latency.py#L93 Modifications Replace all usage of time.time() in benchmark and tests to time.perf_counter(). There are also potentially suspicious usage of wall clock in inferencing code. E.g., in RadixCache , arguably time.monotonic() should be used instead of time.time() for resilience against NTP sync / daylight saving adjustment, etc. But to be extra cautious, I intentionally scoped this PR to only test and benchmark but not inferencing code to make sure changes to core logic are tested/benchmarked in a controlled setup. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 4 Swipe4057, Alcanderian, b8zhong, and hebiao064 reacted with thumbs up emoji All reactions \ud83d\udc4d 4 reactions Replace time.time() to time.perf_counter() for benchmarking. \u2026 a2fd019 Signed-off-by: Lifu Huang <lifu.hlf@gmail.com> lifuhuang requested review from ByronHsu , Ying1123 , slin1237 , merrymercy and zhyncs as code owners May 10, 2025 19:42 lifuhuang mentioned this pull request May 10, 2025 [Misc] Use monotonic time for interval measurement #6177 Closed 4 tasks zhyncs approved these changes May 11, 2025 View reviewed changes Hide details View details zhyncs merged commit 6e2da51 into sgl-project : main May 11, 2025 29 of 31 checks passed Uh oh! There was an error while loading. Please reload this page . lifuhuang added a commit\n        to lifuhuang/sglang\n      that referenced\n      this pull request May 17, 2025 Replace time.time() to time.perf_counter() for benchmarking. ( sgl-pro\u2026 \u2026 a0e85c4 \u2026ject#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request May 23, 2025 Rebase 4_6_post_4 to master_next ( sgl-project#47 ) \u2026 bc7d46c * Use device_id in dist init to reduce NCCL communicator warmup & creation overhead ( sgl-project#5728 )\n\n* [fix] fix potential bumpy throughtput with deepgemm ( sgl-project#5722 )\n\n* Resolves the `404 Not Found` error when running `compile_deep_gemm.py` in multi-node setups ( sgl-project#5720 )\n\n* perf: update H20 fused_moe_triton kernel config to get higher throughput during prefilling ( sgl-project#5716 )\n\n* we fix the non existent access of `decrypted_config_file` ( sgl-project#5685 )\n\n* CI: rewrite test_vision_chunked_prefill to speedup ( sgl-project#5682 )\n\n* Fuse MLA set kv cache kernel ( sgl-project#5748 )\n\n* Update amd docker image to `sglang:v0.4.5.post3-rocm630`. ( sgl-project#5697 )\n\n* [feature] support for roberta embedding models ( sgl-project#5730 )\n\n* [fix] fix bench_one_batch_server ( sgl-project#5607 )\n\n* support for the DeepSeek model by enabling streaming response parsing ( sgl-project#5592 )\n\n* fix: Use `is not None` instead of `!= None` for None checks. ( sgl-project#5687 )\n\n* Add Llama 4 to FA3 test ( sgl-project#5509 )\n\n* [misc] more decode step log for batch_one_batch ( sgl-project#5565 )\n\n* Handle JSONDecodeError while processing request data ( sgl-project#5599 )\n\n* fix(srt): check if sample_indices is not None before usage. ( sgl-project#5633 )\n\n* update llguidance to 0.7.11; adds StructTag ( sgl-project#4870 )\n\n* Use sgl-kernel sgl_per_token_group_quant_int8 ( sgl-project#4971 )\n\n* Add memory_saver check ( sgl-project#4986 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* add switch to disable open api doc ( sgl-project#3744 )\n\nSigned-off-by: congcongke <zhanweidu@163.com>\n\n* Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\" ( sgl-project#5772 )\n\n* Fix eagle test case ( sgl-project#5776 )\n\n* Split local attention test from fa3 test ( sgl-project#5774 )\n\n* Revert \"Revert \"fix: import vllm_rotary_embedding error when head_size not in 64, 128, 256, 512\"\" ( sgl-project#5777 )\n\n* Simplify FA3 tests ( sgl-project#5779 )\n\n* Revert \"[fix] fix bench_one_batch_server\" ( sgl-project#5785 )\n\n* Revert \"Use device_id in dist init to reduce NCCL communicator warmup & creation overhead\" ( sgl-project#5786 )\n\n* [CI] Tune threshold ( sgl-project#5787 )\n\n* [CI] fix port conflicts ( sgl-project#5789 )\n\n* [CI] Fix ci tests ( sgl-project#5769 )\n\n* [PD]Reduce kv transfer threads ( sgl-project#5791 )\n\n* [CI] Fix test case ( sgl-project#5790 )\n\n* Add 8-GPU Test for Deepseek-V3  ( sgl-project#5691 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* Release v0.4.6 ( sgl-project#5795 )\n\n* Update nightly-test.yml ( sgl-project#5797 )\n\n* [CI] Improve github summary & enable fa3 for more models ( sgl-project#5796 )\n\n* [Docs] update grafana setup guide in production metrics ( sgl-project#5643 )\n\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\n\n* [Misc] add structure logging, write to file and log tracing for SGL Router\n\n* Improve overlap scheduling ( sgl-project#5788 )\n\n* Add Cutlass MLA attention backend ( sgl-project#5390 )\n\n* chore: upgrade sgl-kernel 0.1.0 ( sgl-project#5690 )\n\n* Dockerfile.dev pip scikit_build_core ( sgl-project#5807 )\n\n* Add a doc to fix sgl-kernel build link error in py39 with ccache ( sgl-project#5809 )\n\n* Turn on overlap scheduler for multimodal models ( sgl-project#5771 )\n\n* Tiny refactor DefaultModelLoader.Source ( sgl-project#5482 )\n\n* [Docs] Replace lists with tables for cleanup and readability in server_arguments ( sgl-project#5276 )\n\n* Revert \"Tiny refactor DefaultModelLoader.Source\" ( sgl-project#5825 )\n\n* Feat: add support for thinking mode via chat_template_kwargs.enable_t\u2026 ( sgl-project#5551 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: fix the error where the content is None when reasoning and tool \u2026 ( sgl-project#5838 )\n\n* feat: Add fused moe triton config for qwen3 moe on h100 ( sgl-project#5833 )\n\n* fused moe triton tuning script support qwen3 ( sgl-project#5842 )\n\n* feat: Add fused moe triton config for qwen3bf16 moe on h20 ( sgl-project#5839 )\n\n* [PD] support pd fake transfer for warmup ( sgl-project#5726 )\n\n* [config] qwen3moe_tune_h20 fp8 tp4 ( sgl-project#5846 )\n\n* [Doc] Recover history of server_arguments.md ( sgl-project#5851 )\n\n* feat: Add fused moe triton config for qwen3-30b-fp8 moe on h20 ( sgl-project#5850 )\n\n* [CI] test chunked prefill more ( sgl-project#5798 )\n\n* ROCm: update AITER ( sgl-project#5816 )\n\n* [Feat] QWen-1M context support[1/2]: Update block sparse attention backend utils kernel ( sgl-project#5847 )\n\nCo-authored-by: sighingnow <sighingnow@gmail.com>\n\n* [Fix] Missing bootstrap_port field ( sgl-project#5823 )\n\n* feat: update is_fa3_default_architecture ( sgl-project#5854 )\n\n* add fused moe config for qwen3moe fp8/bf16 ( sgl-project#5849 )\n\n* chore: bump v0.4.6.post1 ( sgl-project#5845 )\n\n* Support `max_completion_tokens` for OpenAIChatCompletions ( sgl-project#5857 )\n\n* simplify fused_moe config logging ( sgl-project#5801 )\n\n* [CI] tune the test order to warmup the server ( sgl-project#5860 )\n\n* Cutlass MLA decode - fix dtype error ( sgl-project#5868 )\n\n* cutlass 3.9 supported to improve fp8_blockwise_gemm ( sgl-project#5820 )\n\n* [Feature] support auto chat template ( sgl-project#4949 )\n\n* Feat: support cuda graph for LoRA ( sgl-project#4115 )\n\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\n\n* Add qwen3 30b fused moe config ( sgl-project#5859 )\n\n* [Fix] Fix a bug for flashmla to run R1 model ( sgl-project#5875 )\n\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\n\n* Add A800 fused moe config for qwen3 30b ( sgl-project#5880 )\n\n* [Misc] add service discovery for sgl router\n\n* [fix]: PyO3 macOS linking and consolidate on tracing for logging\n\n* chore: update Dockerfile ( sgl-project#5894 )\n\n* [Docs] Update docs for Qwen3 and Qwen3MoE ( sgl-project#5836 )\n\n* [Doc] Tables instead of bulletpoints for sampling doc ( sgl-project#5841 )\n\n* chore: update CODEOWNERS ( sgl-project#5895 )\n\n* [FEATURE] Enhance platform compatibility for ARM ( sgl-project#5746 )\n\n* [CI] Add test_function_calling.py to run_suite.py ( sgl-project#5896 )\n\n* Auto set draft model path for MTP ( sgl-project#5793 )\n\n* [fix] relax mem_fraction_static for h200 ( sgl-project#5893 )\n\nCo-authored-by: alcanerian <alcanerian@gmail.com>\n\n* feat: support pythonic tool call and index in tool call streaming ( sgl-project#5725 )\n\n* [Bugfix]: fix missing queue_time_start for requests from grammar_queue ( sgl-project#5696 )\n\n* Add AMD MI300x Nightly Testing. ( sgl-project#5861 )\n\n* chore: use torch 2.6 for sgl-kernel build ( sgl-project#5898 )\n\n* Fix check_env script ( sgl-project#5901 )\n\n* [PD] Fix Assertion failed: /DeepEP/csrc/kernels/internode.cu:483, condition: ibgda_get_state()->num_rc_per_pe >= num_channels sgl-project#134 ( sgl-project#5830 )\n\n* Bump Flashinfer to 0.2.5 ( sgl-project#5870 )\n\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\n\n* [Fix] Unload lora in HF_Runner if needed ( sgl-project#5899 )\n\n* Add A800 fused moe config for qwen3 235b ( sgl-project#5900 )\n\n* Add sm_120 for blackwell ( sgl-project#5903 )\n\n* [Feature] add support kimi vl model ( sgl-project#5383 )\n\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\n\n* support vlm benchmark profile ( sgl-project#5905 )\n\n* [fix] kimi-vl test in test_vision_openai_server.py ( sgl-project#5910 )\n\n* [Misc] use parallel build for cmake in sgl-kernel ( sgl-project#5919 )\n\n* [qwen3] support qwen3 ep moe ( sgl-project#5917 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\n\n* Add TP2 MOE benchmarks for AMD. ( sgl-project#5909 )\n\n* [Feat] Scale up fa3 kernel to sm8x arch ( sgl-project#5912 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* chore: bump sgl-kernel 0.1.1 ( sgl-project#5932 )\n\n* chore: upgrade sgl-kernel 0.1.1 ( sgl-project#5933 )\n\n* Remove unused method `calculate_num_image_tokens` from qwen2_vl.py ( sgl-project#5783 )\n\n* [PP] Add pipeline parallelism ( sgl-project#5724 )\n\n* Fix lora batch processing when input lora_path contains None ( sgl-project#5930 )\n\n* add Thor & Spark ( sgl-project#5915 )\n\n* fix: correct stream response when enable_thinking is set to false ( sgl-project#5881 )\n\n* fix: update model runner ( sgl-project#5934 )\n\n* chore: bump v0.4.6.post2 ( sgl-project#5939 )\n\n* Support XiaomiMiMo/MiMo model inference ( sgl-project#5921 )\n\n* [PD] Vectorise group_concurrent_contiguous in NumPy ( sgl-project#5834 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Remove extra contiguous ( sgl-project#5953 )\n\n* Update ci test and doc for MTP api change ( sgl-project#5952 )\n\n* docs: Fix Qwen model typo ( sgl-project#5944 )\n\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\n\n* Optimize a pad operation to accelerate 25us ( sgl-project#5945 )\n\n* Properly return error response in vertex_generate HTTP endpoint ( sgl-project#5956 )\n\n* feat: add concurrency evaluation logic in mmmu benchmark ( sgl-project#5782 )\n\n* Add 1 gpu perf and 2 gpu accuracy tests for AMD MI300x CI. ( sgl-project#5960 )\n\n* feat: Refactor DeepSeekV3 function call ( sgl-project#5908 )\n\n* Remove token in token out in Native API ( sgl-project#5967 )\n\n* Support InternVL3 ( sgl-project#5350 )\n\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\n\n* Support MMMU benchmark for  InternVL ( sgl-project#5968 )\n\n* FA3 speed up: skip len operation and get batch size directly from forward batch ( sgl-project#5969 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] NIXL backend Prefill TP & Decode TP+DP ( sgl-project#5681 )\n\n* Fix set kv cache multi-stream ( sgl-project#5975 )\n\n* Overlap qk norm with two streams ( sgl-project#5977 )\n\n* fix: only upgrade nccl for cu128 ( sgl-project#5986 )\n\n* Fix Phi3 serving which was broke by earlier change ( sgl-project#5991 )\n\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [perf] H100 DeepSeek-V3 fused moe tuned config ( sgl-project#5998 )\n\n* [Fix] Suppress dynamo logging when using flashinfer backend with torch compile ( sgl-project#5992 )\n\n* [Minor] Fix duplicate method definitions in conversation.py ( sgl-project#6012 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Fix flaky issues of lora and add multi batch tests ( sgl-project#5957 )\n\n* Tool Call: Add `chat_template_kwargs` documentation ( sgl-project#5679 )\n\n* fix: fix broadcast_pyobj breaking VerlEngine ( sgl-project#5997 )\n\n* [PD] Allow customizing reserved tokens to avoid KV cache waste ( sgl-project#6002 )\n\n* Update dev container config to support live code sync and improve docker setup guide   ( sgl-project#6018 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* [PD] Optimize disaggregation ib device help info ( sgl-project#5781 )\n\n* [Test] Add flashmla attention backend test ( sgl-project#5587 )\n\n* Fix \"Avoid computing lse in Ragged Prefill when there's no prefix match\" ( sgl-project#5555 )\n\n* feat: Add a unified merge_state API ( sgl-project#5428 )\n\n* feat: append more comprehensive fields in messages instead of merely role and content ( sgl-project#5996 )\n\n* [Security][Bug] Prevent binding to all TCP interfaces ( sgl-project#5752 )\n\n* Fix prefill OOM error in the case of large page size ( sgl-project#5081 )\n\n* Fix problem of large page size with chunked prefill ( sgl-project#6046 )\n\n* docs: add Google Cloud Vertex AI in Adoption and Sponsorship ( sgl-project#6047 )\n\n* docs: add new blog ( sgl-project#6048 )\n\n* Fix not \"import os\" ( sgl-project#6057 )\n\n* Better PD initialization ( sgl-project#5751 )\n\n* fix: deepep dockerfile, use pip install deepep. ( sgl-project#5885 )\n\n* [Fix] Fix and rename flashmla CI test ( sgl-project#6045 )\n\n* chore: upgrade cutlass 3.9.2 ( sgl-project#6004 )\n\nCo-authored-by: yizhang2077 <1109276519@qq.com>\n\n* Fix sgl-kernel build on aarch64 platforms ( sgl-project#6062 )\n\n* Add DeepEP to CI PR Test ( sgl-project#5655 )\n\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\n\n* fix custom_allreduce namespace ( sgl-project#6039 )\n\n* feat: add release workflow for SGLang kernels on aarch64 ( sgl-project#6010 )\n\nCo-authored-by: Qiaolin-Yu <liin1211@outlook.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* [Feature] Support for Ascend NPU backend ( sgl-project#3853 )\n\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\n\n* Fix the timeout for 8 gpu tests ( sgl-project#6084 )\n\n* Hint users DeepEP normal mode is incompatible with CUDA Graph ( sgl-project#5014 )\n\n* Super tiny fix doc ( sgl-project#5233 )\n\n* [Doc]Fix description for dp_size argument ( sgl-project#6063 )\n\n* feat(engine): add bootstrap parameters to generate methods (dynamo) ( sgl-project#6075 )\n\n* [refactor] slightly tidy fp8 module ( sgl-project#5993 )\n\n* Clean up fa3 test from 8 gpus ( sgl-project#6105 )\n\n* Deferring 8 GPU test ( sgl-project#6102 )\n\n* Update doc for MLA attention backends ( sgl-project#6034 )\n\n* Clean logs for DeepSeek-V3 launching ( sgl-project#6079 )\n\n* [CI]Add performance CI for VLM ( sgl-project#6038 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* adding Triton configs for DeepSeekV3 FusedMoE kernel on Blackwell ( sgl-project#6111 )\n\n* optimize pad operations in fa3 to accelarate 100+us ( sgl-project#6077 )\n\n* Overlap shared expert and routed expert computations ( sgl-project#5121 )\n\n* Tiny refactor ModelConfig.from_server_args ( sgl-project#5219 )\n\n* Tiny refactor weight loading logic ( sgl-project#5232 )\n\n* [PD] Add control to slow down a server ( sgl-project#5572 )\n\n* Change AMD test threshold ( sgl-project#6091 )\n\n* DeepEP normal support deepgemm-contiguous ( sgl-project#5626 )\n\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\n\n* [fix] fix pyproject.toml dependencies ( sgl-project#6119 )\n\n* [Feature] Add FlashAttention3 as a backend for VisionAttention ( sgl-project#5764 )\n\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\n\n* [perf] dsv3 bmm fallback to bf16 ( sgl-project#5662 )\n\n* [AMD] switch to custom allreduce regardless of MSCCL setting on ROCm ( sgl-project#6097 )\n\n* [sgl-kernel] fix: fix cu118 compile error ( sgl-project#6123 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* upgrade xgrammar to 0.1.19 ( sgl-project#6129 )\n\n* Remove unecessary is_fa3_supported check ( sgl-project#6112 )\n\n* chore: bump sgl-kernel 0.1.2 ( sgl-project#6131 )\n\n* docs: update README ( sgl-project#6132 )\n\n* [Fix] Incorrect Memory Allocation on CUDA:0 by Non-Zero CUDA Processes in TP/DP ( sgl-project#5745 )\n\n* Cutlass MLA: Disable split kv due to NVIDIA/cutlass#2274 ( sgl-project#6101 )\n\n* opt flashinfer mla cat ( sgl-project#5822 )\n\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\n\n* Update amd nightly concurrency. ( sgl-project#6141 )\n\n* feat: add thinking_budget ( sgl-project#6089 )\n\n* [Bugfix] Fix Llama4 gibberish output with long context and CUDA graph ( sgl-project#6162 )\n\n* fix bug that gpu0 occupies more memory when hicache is turned on ( sgl-project#5778 )\n\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* chore: bump v0.4.6.post3 ( sgl-project#6165 )\n\n* KV\u2011Cache\u202f(MHA, MLA): add missing start_layer\u202f/\u202fend_layer fields to MHATokenToKVPoolHost and MLATokenToKVPoolHost ( sgl-project#6016 )\n\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\n\n* [fix] fix determine_n_share_experts_fusion ( sgl-project#6118 )\n\n* Fix and Clean up chat-template requirement for VLM ( sgl-project#6114 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* [Docs]Delete duplicate content ( sgl-project#6146 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* Revert \"feat: add thinking_budget ( sgl-project#6089 )\" ( sgl-project#6181 )\n\n* Added async_encode method to Engine ( sgl-project#4701 )\n\n* Fix data parallel perf regression ( sgl-project#6183 )\n\n* Fix request abortion ( sgl-project#6184 )\n\n* Add typo checker in pre-commit ( sgl-project#6179 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* Remove duplicate IO Struct test ( sgl-project#6180 )\n\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\n\n* [PD] Add simple unit test for disaggregation feature ( sgl-project#5654 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Disabled deepep tests temporarily because it takes too much time. ( sgl-project#6186 )\n\n* feat: support loogle eval ( sgl-project#6190 )\n\n* [fix] remove mixtral from is_fa3_default_architecture ( sgl-project#6191 )\n\n* fix: handle None multimodal_inputs during merging and filtering batches in disaggregation decode mode ( sgl-project#6169 )\n\n* chore: upgrade deepgemm ( sgl-project#6073 )\n\n* chore: bump sgl-kernel v0.1.2.post1 ( sgl-project#6195 )\n\n* chore: upgrade sgl-kernel v0.1.2.post1 ( sgl-project#6196 )\n\nCo-authored-by: alcanderian <alcanderian@gmail.com>\n\n* Handle empty input string for embedding models ( sgl-project#5621 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* doc: fix the erroneous documents and example codes about Alibaba-NLP/gme-Qwen2-VL-2B-Instruct ( sgl-project#6199 )\n\n* [Docs] minor Qwen3 and reasoning parser docs fix ( sgl-project#6032 )\n\n* Improve structured outputs: fix race condition, server crash, metrics and style ( sgl-project#6188 )\n\n* [CI] Reorganize the 8 gpu tests ( sgl-project#6192 )\n\n* Add dev-deepep docker image ( sgl-project#6198 )\n\n* Replace time.time() to time.perf_counter() for benchmarking. ( sgl-project#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\n\n* Update README.md ( sgl-project#6202 )\n\n* Fix release-docs.yml to not use python 3.9 ( sgl-project#6204 )\n\n* Fix start_profile does not support with_stack and record_shapes ( sgl-project#6043 )\n\n* [doc] add a note for --n-share-experts-fusion args ( sgl-project#6154 )\n\n* Performing Vocabulary Parallelism for LM Head across Attention TP Groups ( sgl-project#5558 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\n\n* Update AMD CI docker to v0.4.6.post3-rocm630. ( sgl-project#6213 )\n\n* Log if cuda graph is used & extend cuda graph capture to cuda-graph-max-bs ( sgl-project#6201 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* [CI] Fix PD mooncake dependency error ( sgl-project#6212 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* [CI] Re-enable pd disaggregation test ( sgl-project#6231 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* fix some typos ( sgl-project#6209 )\n\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* [Docs] Add docs for `SGLANG_` and `SGL_` environment variables ( sgl-project#6206 )\n\n* [PP] Fix init_memory_pool desync & add PP for mixtral ( sgl-project#6223 )\n\n* Revert \"fix some typos\" ( sgl-project#6244 )\n\n* chore: add hf_xet dep ( sgl-project#6243 )\n\n* Update AMD nightly deps. ( sgl-project#6241 )\n\n* [PD] Add support for different TP sizes per DP rank ( sgl-project#5922 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Support incremental streaming of logprob/token_ids between scheduler and detokenizer ( sgl-project#6225 )\n\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\n\n* fix typo ( sgl-project#6248 )\n\n* Support tuning moe for llama 4 model ( sgl-project#6042 )\n\n* Skip the flaky test_stateful_custom_logit_processor ( sgl-project#6251 )\n\n* [Llama4] Add docs note about enable multimodal ( sgl-project#6235 )\n\n* [VERL Use Case] Add torch_memory_saver into deps ( sgl-project#6247 )\n\n* Fix two issues related to `--moe-dense-tp-size=1` ( sgl-project#5657 )\n\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\n\n* model(vlm): pixtral ( sgl-project#5084 )\n\n* [misc] deep_gemm fallback to NVRTC when NVCC not found ( sgl-project#6252 )\n\n* Enable MI325X AMD CI. ( sgl-project#6259 )\n\n* chore: bump v0.4.6.post4 ( sgl-project#6245 )\n\n* formatting fix for the rebased commit for 4.6.0_post4\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix issues in model runner and python packages\n\nfix for following issues:\n> vLLM dependency for xgrammar==0.1.17\n> 'Scheduler' object has no attribute 'device\n> 'pp_proxy_tensors' unexpected arg in HPUGraphRunner\n> TODO: Add pipeline parallelism support in HPUGraphRunner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* fix formatting in model runner\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n* base grammar fix for the is_terminated case\n\n>  'OutlinesGrammar' object has no attribute 'is_terminated'\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n---------\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: congcongke <zhanweidu@163.com>\nSigned-off-by: JiangJiaWei1103 <waynechuang97@gmail.com>\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>\nSigned-off-by: Song Zhang <gepin.zs@antgroup.com>\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: Wenxuan Tan <wtan45@wisc.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: vzed <207368749+vincentzed@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DavidBao <121073073+DavidBao03@users.noreply.github.com>\nCo-authored-by: Frankey_8080 <32973306+Frank-Jie@users.noreply.github.com>\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: yan97ao <580776+yan97ao@users.noreply.github.com>\nCo-authored-by: aoshen524 <aoshen524@gmail.com>\nCo-authored-by: Micha\u0142 Moskal <michal@moskal.me>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: zhanweidu <zhanweidu@163.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: Huapeng Zhou <73010314+PopSoda2002@users.noreply.github.com>\nCo-authored-by: NoahM <88418672+zhudianGG@users.noreply.github.com>\nCo-authored-by: Simo Lin <linsimo.mark@gmail.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: JiLi <leege233@gmail.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: PGFLMG <1106310035@qq.com>\nCo-authored-by: sighingnow <sighingnow@gmail.com>\nCo-authored-by: XTY <xutianyi1999@live.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: woodx <124784234+woodx9@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <qy254@cornell.edu>\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\nCo-authored-by: pengcuo <pengcbupt@163.com>\nCo-authored-by: pengcuo <dgpengcuo@gmail.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Johnny <johnnync13@gmail.com>\nCo-authored-by: alcanerian <alcanerian@gmail.com>\nCo-authored-by: Yuhao Chen <yxckeis8@gmail.com>\nCo-authored-by: zhjunqin <zhjunqin@users.noreply.github.com>\nCo-authored-by: liwenju0 <like4hub@gmail.com>\nCo-authored-by: wenju.li <wenju.li@deepctr.cn>\nCo-authored-by: laixin <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: Ying Sheng <sqy1415@gmail.com>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: \u6c5f\u5bb6\u744b <36886416+JiangJiaWei1103@users.noreply.github.com>\nCo-authored-by: KCFindstr <shimakaze@google.com>\nCo-authored-by: xm:D <38322020+xiaomin-D@users.noreply.github.com>\nCo-authored-by: Lifu Huang <lifu.hlf@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: Junrong Lin <33685709+ocss884@users.noreply.github.com>\nCo-authored-by: shangmingc <caishangming@linux.alibaba.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: Hank Han <54751605+HanHan009527@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <liin1211@outlook.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: Jinyan Chen <jinyanc@nvidia.com>\nCo-authored-by: Johnny <johnnynuca14@gmail.com>\nCo-authored-by: Song Zhang <70674731+botieking98@users.noreply.github.com>\nCo-authored-by: 22dimensions <waitingwind@foxmail.com>\nCo-authored-by: ishandhanani <82981111+ishandhanani@users.noreply.github.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: Minglei Zhu <mingleizhu1122@gmail.com>\nCo-authored-by: lukec <118525388+sleepcoo@users.noreply.github.com>\nCo-authored-by: Yingyi Huang <yingyihuang2000@outlook.com>\nCo-authored-by: Xuting Zhou <xutingz@nvidia.com>\nCo-authored-by: ZhengHSI <zhenghsi@qq.com>\nCo-authored-by: Zhu Chen <51010608+Othame@users.noreply.github.com>\nCo-authored-by: othame <chenzhu_912@zju.edu.cn>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: Yixin Dong <ubospica@gmail.com>\nCo-authored-by: xu-yfei <xu_yfei@qq.com>\nCo-authored-by: xuyongfei.xyf <xuyongfei.xyf@antgroup.com>\nCo-authored-by: thyecust <tienhoayu@gmail.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: Simon (Jiyou) Li <Simon-Li@users.noreply.github.com>\nCo-authored-by: \u7ee7\u4f18 <jiyou.ljy@alibaba-inc.com>\nCo-authored-by: chus-chus <chus-chus@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Steven Shimizu <shimizust@gmail.com>\nCo-authored-by: applesaucethebun <113181361+applesaucethebun@users.noreply.github.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Emmanuel Ferdman <emmanuelferdman@gmail.com>\nCo-authored-by: Yusong Gao <yusong.gao@gmail.com>\nCo-authored-by: alcanderian <alcanderian@gmail.com>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: liusy58 <liusy58@linux.alibaba.com>\nCo-authored-by: SangBin Cho <rkooo567@gmail.com>\nCo-authored-by: \u9889\u6c86 <xiehang.lsy@alibaba-inc.com>\nCo-authored-by: Kiv Chen <34561254+KivenChen@users.noreply.github.com> Layssy pushed a commit\n        to Layssy/sglang-iaas\n      that referenced\n      this pull request Jun 9, 2025 Replace time.time() to time.perf_counter() for benchmarking. ( sgl-pro\u2026 \u2026 fa9ceda \u2026ject#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com> xwu-intel pushed a commit\n        to xwu-intel/sglang\n      that referenced\n      this pull request Jun 17, 2025 Replace time.time() to time.perf_counter() for benchmarking. ( sgl-pro\u2026 \u2026 1bcfee4 \u2026ject#6178 )\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:57:51",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "Replace time.time() to time.perf_counter() for benchmarking. (#6178)",
  "commit_message": "Replace time.time() to time.perf_counter() for benchmarking. (#6178)\n\nSigned-off-by: Lifu Huang <lifu.hlf@gmail.com>",
  "commit_date": "2025-05-11T14:32:49-07:00",
  "files_changed": [
    "benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py",
    "benchmark/benchmark_batch/benchmark_batch.py",
    "benchmark/benchmark_batch/benchmark_tokenizer.py",
    "benchmark/generative_agents/bench_other.py",
    "benchmark/generative_agents/bench_sglang.py",
    "benchmark/gsm8k/bench_other.py",
    "benchmark/gsm8k/bench_sglang.py",
    "benchmark/hellaswag/bench_other.py",
    "benchmark/hellaswag/bench_sglang.py",
    "benchmark/hicache/bench_multiturn.py",
    "benchmark/json_decode_regex/bench_other.py",
    "benchmark/json_decode_regex/bench_sglang.py",
    "benchmark/json_jump_forward/bench_other.py",
    "benchmark/json_jump_forward/bench_sglang.py",
    "benchmark/json_schema/bench_sglang.py",
    "benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py",
    "benchmark/kernels/quantization/tuning_block_wise_kernel.py",
    "benchmark/line_retrieval/bench_sglang.py",
    "benchmark/llava_bench/bench_sglang.py",
    "benchmark/llm_judge/bench_other.py",
    "benchmark/llm_judge/bench_sglang.py",
    "benchmark/long_json_decode/bench_other.py",
    "benchmark/long_json_decode/bench_sglang.py",
    "benchmark/mmlu/bench_other.py",
    "benchmark/mmlu/bench_sglang.py",
    "benchmark/mmmu/bench_sglang.py",
    "benchmark/mtbench/bench_other.py",
    "benchmark/mtbench/bench_sglang.py",
    "benchmark/mtbench/bench_sglang_eagle.py",
    "benchmark/multi_chain_reasoning/bench_other.py",
    "benchmark/multi_chain_reasoning/bench_sglang.py",
    "benchmark/multi_document_qa/bench_other.py",
    "benchmark/multi_document_qa/bench_sglang.py",
    "benchmark/multi_turn_chat/bench_other.py",
    "benchmark/multi_turn_chat/bench_sglang.py",
    "benchmark/multi_turn_chat/long_prompt_multi_turn.py",
    "benchmark/react/bench_other.py",
    "benchmark/react/bench_sglang.py",
    "benchmark/reasoning_benchmark/bench_sglang.py",
    "benchmark/tip_suggestion/bench_other.py",
    "benchmark/tip_suggestion/bench_sglang.py",
    "benchmark/tree_of_thought_deep/bench_other.py",
    "benchmark/tree_of_thought_deep/bench_sglang.py",
    "benchmark/tree_of_thought_v0/bench_other.py",
    "benchmark/tree_of_thought_v0/bench_sglang.py",
    "python/sglang/test/few_shot_gsm8k.py",
    "python/sglang/test/few_shot_gsm8k_engine.py",
    "python/sglang/test/run_eval.py",
    "python/sglang/test/test_programs.py",
    "python/sglang/test/test_utils.py",
    "sgl-router/py_test/test_launch_server.py",
    "test/srt/experiment_runner.py",
    "test/srt/models/test_encoder_embedding_models.py",
    "test/srt/test_gptqmodel_dynamic.py",
    "test/srt/test_release_memory_occupation.py",
    "test/srt/test_torch_compile.py",
    "test/srt/test_torch_compile_moe.py",
    "test/srt/test_torchao.py",
    "test/srt/test_update_weights_from_distributed.py",
    "test/srt/test_update_weights_from_tensor.py",
    "test/srt/test_w8a8_quantization.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 316,
    "num_files": 61,
    "num_hunks": 121,
    "num_non_test_edited_lines": 316,
    "num_non_test_files": 61,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\nindex 86648e5ff..282097112 100644\n--- a/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n+++ b/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py\n@@ -64,11 +64,11 @@ def test_batch_by_batch(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         text_qa.run_batch(\n             list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))),\n         )\n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -78,13 +78,13 @@ def test_batch_by_batch_with_hint(all_prompts, gen_len):\n \n     tot_time = 0\n     for i in range(len(all_prompts)):\n-        tic = time.time()\n+        tic = time.perf_counter()\n         # Send a hint to cache the prefix\n         text_qa.run_batch(list(zip(all_prompts[i][:1], [gen_len])))\n         # Send the batch\n         text_qa.run_batch(list(zip(all_prompts[i], [gen_len] * len(all_prompts[i]))))\n \n-        tot_time += time.time() - tic\n+        tot_time += time.perf_counter() - tic\n \n     return tot_time\n \n@@ -94,11 +94,11 @@ def test_send_all(all_prompts, gen_len):\n \n     all_prompts = [x for prompt_list in all_prompts for x in prompt_list]\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     text_qa.run_batch(\n         list(zip(all_prompts, [gen_len] * len(all_prompts))),\n     )\n-    tot_time = time.time() - tic\n+    tot_time = time.perf_counter() - tic\n \n     return tot_time\n \ndiff --git a/benchmark/benchmark_batch/benchmark_batch.py b/benchmark/benchmark_batch/benchmark_batch.py\nindex 15ef0ab6a..a8592d48a 100644\n--- a/benchmark/benchmark_batch/benchmark_batch.py\n+++ b/benchmark/benchmark_batch/benchmark_batch.py\n@@ -81,7 +81,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n     }\n     data = {\"text\": prompts, \"sampling_params\": sampling_params}\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     try:\n         response = requests.post(\n             endpoint.base_url + \"/generate\", json=data, timeout=3600\n@@ -90,7 +90,7 @@ def send_batch_request(endpoint, prompts, gen_tokens, request_id):\n             error = response.json()\n             raise RuntimeError(f\"Request {request_id} failed: {error}\")\n         result = response.json()\n-        elapsed_time = (time.time() - start_time) * 1000  # Convert to ms\n+        elapsed_time = (time.perf_counter() - start_time) * 1000  # Convert to ms\n         avg_per_prompt = elapsed_time / len(prompts) if prompts else 0\n         return request_id, elapsed_time, avg_per_prompt, True, len(prompts)\n     except Exception as e:\n@@ -104,7 +104,7 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n     num_requests = len(batched_prompts)\n \n     # Record start time for total latency\n-    benchmark_start_time = time.time()\n+    benchmark_start_time = time.perf_counter()\n \n     for i, batch_prompts in enumerate(batched_prompts):\n         request_id = i + 1\n@@ -119,7 +119,7 @@ def run_benchmark(endpoint, batched_prompts, batch_size, gen_tokens):\n         results.append(result)\n \n     # Calculate total latency\n-    total_latency = (time.time() - benchmark_start_time) * 1000  # Convert to ms\n+    total_latency = (time.perf_counter() - benchmark_start_time) * 1000  # Convert to ms\n \n     return results, total_latency\n \ndiff --git a/benchmark/benchmark_batch/benchmark_tokenizer.py b/benchmark/benchmark_batch/benchmark_tokenizer.py\nindex c00bfb84b..88a5820b6 100644\n--- a/benchmark/benchmark_batch/benchmark_tokenizer.py\n+++ b/benchmark/benchmark_batch/benchmark_tokenizer.py\n@@ -44,10 +44,10 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         for prompt in batch_prompts:\n             tokens = tokenizer.encode(prompt)\n-        sequential_time = (time.time() - start_time) * 1000\n+        sequential_time = (time.perf_counter() - start_time) * 1000\n         sequential_times.append(sequential_time)\n \n     # Batch tokenization using tokenizer()\n@@ -55,9 +55,9 @@ def benchmark_sequential_vs_batch(prompts, batch_size, tokenizer):\n     for run in range(NUM_RUNS):\n         batch_prompts = prompts[:batch_size]  # Use same prompts for fair comparison\n \n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         tokens = tokenizer(batch_prompts)\n-        batch_time = (time.time() - start_time) * 1000\n+        batch_time = (time.perf_counter() - start_time) * 1000\n         batch_times.append(batch_time)\n \n     return {\ndiff --git a/benchmark/generative_agents/bench_other.py b/benchmark/generative_agents/bench_other.py\nindex 48f6ebc40..c0b3a3406 100644\n--- a/benchmark/generative_agents/bench_other.py\n+++ b/benchmark/generative_agents/bench_other.py\n@@ -39,7 +39,7 @@ def main(args):\n         answer = await call_generate(**arg, temperature=0)\n         states.append(answer)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     # we always sequentially execute agent calls to maintain its dependency\n     if args.backend != \"lmql\":\n         for arg in tqdm(arguments):\n@@ -50,7 +50,7 @@ def main(args):\n         loop = asyncio.get_event_loop()\n         for arg in tqdm(arguments):\n             loop.run_until_complete(get_one_answer_async(arg))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/generative_agents/bench_sglang.py b/benchmark/generative_agents/bench_sglang.py\nindex b42a32b44..034b16591 100644\n--- a/benchmark/generative_agents/bench_sglang.py\n+++ b/benchmark/generative_agents/bench_sglang.py\n@@ -35,14 +35,14 @@ def main(args):\n \n     states = []\n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     for a in arguments:\n         # only a single key in the dict\n         for func, arg in a.items():\n             result = func.run(**arg)\n         result.sync()\n         states.append(result)\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/gsm8k/bench_other.py b/benchmark/gsm8k/bench_other.py\nindex a8bbcfb5c..6dcb9ad7c 100644\n--- a/benchmark/gsm8k/bench_other.py\n+++ b/benchmark/gsm8k/bench_other.py\n@@ -75,7 +75,7 @@ def main(args):\n             )\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -106,9 +106,9 @@ def main(args):\n                 for j in range(len(rets)):\n                     states[i + j] = rets[j]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/gsm8k/bench_sglang.py b/benchmark/gsm8k/bench_sglang.py\nindex b6bdbef09..05ac0beb1 100644\n--- a/benchmark/gsm8k/bench_sglang.py\n+++ b/benchmark/gsm8k/bench_sglang.py\n@@ -84,14 +84,14 @@ def main(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_gsm8k.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/hellaswag/bench_other.py b/benchmark/hellaswag/bench_other.py\nindex 04be4569a..cde0794bb 100644\n--- a/benchmark/hellaswag/bench_other.py\n+++ b/benchmark/hellaswag/bench_other.py\n@@ -57,7 +57,7 @@ def main(args):\n                 context=few_shot_examples + questions[i], choices=choices[i]\n             )\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -82,10 +82,10 @@ def main(args):\n                 for j in range(len(rets)):\n                     preds[i + j] = rets[j]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     acc = np.mean(np.array(preds) == np.array(labels))\ndiff --git a/benchmark/hellaswag/bench_sglang.py b/benchmark/hellaswag/bench_sglang.py\nindex 798521f97..6345a453b 100644\n--- a/benchmark/hellaswag/bench_sglang.py\n+++ b/benchmark/hellaswag/bench_sglang.py\n@@ -68,7 +68,7 @@ def main(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -76,7 +76,7 @@ def main(args):\n         progress_bar=True,\n     )\n     preds = [choices[i].index(rets[i][\"answer\"]) for i in range(len(rets))]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     acc = np.mean(np.array(preds) == np.array(labels))\ndiff --git a/benchmark/hicache/bench_multiturn.py b/benchmark/hicache/bench_multiturn.py\nindex 6bd0bd99e..a2a88b634 100644\n--- a/benchmark/hicache/bench_multiturn.py\n+++ b/benchmark/hicache/bench_multiturn.py\n@@ -261,7 +261,7 @@ class WorkloadGenerator:\n             client_id, payload = item\n             response = await async_request_sglang_generate(payload, self.url, self.pbar)\n             if self.pbar.n == self.pbar.total:\n-                self.finished_time = time.time()\n+                self.finished_time = time.perf_counter()\n             self.response_queue.put((client_id, response))\n         except Exception as e:\n             print(f\"Request failed: {e}\")\n@@ -334,7 +334,7 @@ class WorkloadGenerator:\n         request_thread = threading.Thread(target=self.request_sender, daemon=True)\n         response_thread = threading.Thread(target=self.response_handler, daemon=True)\n \n-        self.start_time = time.time()\n+        self.start_time = time.perf_counter()\n         request_thread.start()\n         response_thread.start()\n \ndiff --git a/benchmark/json_decode_regex/bench_other.py b/benchmark/json_decode_regex/bench_other.py\nindex d80ea1de7..87051ea82 100644\n--- a/benchmark/json_decode_regex/bench_other.py\n+++ b/benchmark/json_decode_regex/bench_other.py\n@@ -53,7 +53,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = json_decode(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -68,7 +68,7 @@ def main(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/json_decode_regex/bench_sglang.py b/benchmark/json_decode_regex/bench_sglang.py\nindex 4139ebf8a..9aab11e43 100644\n--- a/benchmark/json_decode_regex/bench_sglang.py\n+++ b/benchmark/json_decode_regex/bench_sglang.py\n@@ -63,11 +63,11 @@ def main(args):\n     json_warm_up.run().sync()\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = json_decode.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/json_jump_forward/bench_other.py b/benchmark/json_jump_forward/bench_other.py\nindex 9eb5c58b3..a64e950d7 100644\n--- a/benchmark/json_jump_forward/bench_other.py\n+++ b/benchmark/json_jump_forward/bench_other.py\n@@ -175,7 +175,7 @@ def bench_character(args):\n     else:\n         raise ValueError(f\"Invalid backend: {args.backend}\")\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n         if args.parallel == 1:\n@@ -202,7 +202,7 @@ def bench_character(args):\n                 asyncio.gather(*[get_one_answer_async(i) for i in bt])\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \n@@ -236,7 +236,7 @@ def bench_city_doc(args):\n     else:\n         raise ValueError(f\"Invalid backend: {args.backend}\")\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -246,7 +246,7 @@ def bench_city_doc(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \ndiff --git a/benchmark/json_jump_forward/bench_sglang.py b/benchmark/json_jump_forward/bench_sglang.py\nindex 10cf2699b..29f635f75 100644\n--- a/benchmark/json_jump_forward/bench_sglang.py\n+++ b/benchmark/json_jump_forward/bench_sglang.py\n@@ -67,14 +67,14 @@ def bench_city_doc(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = city_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \n@@ -91,14 +91,14 @@ def bench_character(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = character_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     return states, latency\n \ndiff --git a/benchmark/json_schema/bench_sglang.py b/benchmark/json_schema/bench_sglang.py\nindex 4693baae3..55365ff2e 100644\n--- a/benchmark/json_schema/bench_sglang.py\n+++ b/benchmark/json_schema/bench_sglang.py\n@@ -85,14 +85,14 @@ def bench_schema(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = schema_gen.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Check if the outputs are valid\n     indexes = []\ndiff --git a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\nindex a3ead1eca..be349e456 100644\n--- a/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n+++ b/benchmark/kernels/fused_moe_triton/tuning_fused_moe_triton.py\n@@ -487,7 +487,7 @@ def main(args: argparse.Namespace):\n             ]\n         print(f\"Start tuning over {len(search_space)} configurations...\")\n \n-        start = time.time()\n+        start = time.perf_counter()\n         configs = _distribute(\n             \"tune\",\n             [\n@@ -522,7 +522,7 @@ def main(args: argparse.Namespace):\n             use_int8_w8a16,\n             block_shape,\n         )\n-        end = time.time()\n+        end = time.perf_counter()\n         print(f\"Tuning took {end - start:.2f} seconds\")\n     else:\n         outputs = _distribute(\ndiff --git a/benchmark/kernels/quantization/tuning_block_wise_kernel.py b/benchmark/kernels/quantization/tuning_block_wise_kernel.py\nindex 7b0dfb47a..1b51e54b7 100644\n--- a/benchmark/kernels/quantization/tuning_block_wise_kernel.py\n+++ b/benchmark/kernels/quantization/tuning_block_wise_kernel.py\n@@ -359,7 +359,7 @@ def tune_on_gpu(args_dict):\n         config for config in search_space if block_k % config[\"BLOCK_SIZE_K\"] == 0\n     ]\n \n-    start = time.time()\n+    start = time.perf_counter()\n     results = {}\n     for shape in tqdm(weight_shapes, desc=f\"GPU {gpu_id} - Shapes\"):\n         N, K = shape[0], shape[1]\n@@ -379,7 +379,7 @@ def tune_on_gpu(args_dict):\n         best_configs = {M: config for M, config in zip(batch_sizes, benchmark_results)}\n         save_configs(N, K, block_n, block_k, best_configs, save_path, input_type)\n \n-    end = time.time()\n+    end = time.perf_counter()\n     print(f\"Tuning on GPU {gpu_id} took {end - start:.2f} seconds\")\n \n \ndiff --git a/benchmark/line_retrieval/bench_sglang.py b/benchmark/line_retrieval/bench_sglang.py\nindex 922d5009d..e974e7dd3 100644\n--- a/benchmark/line_retrieval/bench_sglang.py\n+++ b/benchmark/line_retrieval/bench_sglang.py\n@@ -70,7 +70,7 @@ def eval_model(args, line_obj, num_hoops, src_indices, dst_percents):\n     # Select backend\n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = line_retrieval.run_batch(\n         arguments,\n         temperature=0,\n@@ -78,7 +78,7 @@ def eval_model(args, line_obj, num_hoops, src_indices, dst_percents):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     corrects = []\n     for i in range(len(arguments)):\ndiff --git a/benchmark/llava_bench/bench_sglang.py b/benchmark/llava_bench/bench_sglang.py\nindex f84c8a90f..b9e8c1405 100644\n--- a/benchmark/llava_bench/bench_sglang.py\n+++ b/benchmark/llava_bench/bench_sglang.py\n@@ -41,7 +41,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm.tqdm(range(len(lines))):\n             image_file = arguments[i][\"image_file\"]\n@@ -52,7 +52,7 @@ def main(args):\n         states = image_qa.run_batch(\n             arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n         )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/llm_judge/bench_other.py b/benchmark/llm_judge/bench_other.py\nindex 2231bcdbb..8e6029067 100644\n--- a/benchmark/llm_judge/bench_other.py\n+++ b/benchmark/llm_judge/bench_other.py\n@@ -85,7 +85,7 @@ def main(args):\n     call_generate = partial(get_call_generate(args), temperature=0)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n \n@@ -120,7 +120,7 @@ def main(args):\n                 asyncio.gather(*[get_one_answer_async(i) for i in bt])\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/llm_judge/bench_sglang.py b/benchmark/llm_judge/bench_sglang.py\nindex 38c95974e..97e6c3979 100644\n--- a/benchmark/llm_judge/bench_sglang.py\n+++ b/benchmark/llm_judge/bench_sglang.py\n@@ -59,7 +59,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_dimension_judge.run_batch(\n         arguments,\n         temperature=0,\n@@ -67,7 +67,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/long_json_decode/bench_other.py b/benchmark/long_json_decode/bench_other.py\nindex a83c797c4..0ad38a014 100644\n--- a/benchmark/long_json_decode/bench_other.py\n+++ b/benchmark/long_json_decode/bench_other.py\n@@ -45,7 +45,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = json_decode(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(arguments))):\n             get_one_answer(i)\n@@ -58,7 +58,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/long_json_decode/bench_sglang.py b/benchmark/long_json_decode/bench_sglang.py\nindex 6e19a732f..8394cfc2e 100644\n--- a/benchmark/long_json_decode/bench_sglang.py\n+++ b/benchmark/long_json_decode/bench_sglang.py\n@@ -46,11 +46,11 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = json_decode.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/mmlu/bench_other.py b/benchmark/mmlu/bench_other.py\nindex c5d48dac6..f1b166c2b 100644\n--- a/benchmark/mmlu/bench_other.py\n+++ b/benchmark/mmlu/bench_other.py\n@@ -76,7 +76,7 @@ def evaluate(args, subject, dev_df, test_df, call_generate):\n             pred = call_generate(prompts[i], temperature=0, max_tokens=max_tokens)\n             preds[i] = pred.strip()[0]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in range(len(prompts)):\n                 get_one_answer(i)\n@@ -94,9 +94,9 @@ def evaluate(args, subject, dev_df, test_df, call_generate):\n                 for j in range(len(rets)):\n                     preds[i + j] = rets[j].strip()[0]\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         asyncio.run(batched_call(batch_size=args.parallel))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     cors = [pred == label for pred, label in zip(preds, labels)]\ndiff --git a/benchmark/mmlu/bench_sglang.py b/benchmark/mmlu/bench_sglang.py\nindex 210b6111e..0bae7b6e4 100644\n--- a/benchmark/mmlu/bench_sglang.py\n+++ b/benchmark/mmlu/bench_sglang.py\n@@ -116,7 +116,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_mmlu.run_batch(\n         arguments,\n         temperature=0,\n@@ -128,7 +128,7 @@ def main(args):\n     preds = [\n         s[\"answer\"].strip()[0] if len(s[\"answer\"].strip()) > 0 else \"\" for s in states\n     ]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     cors = [pred == label for pred, label in zip(preds, labels)]\ndiff --git a/benchmark/mmmu/bench_sglang.py b/benchmark/mmmu/bench_sglang.py\nindex 58a4039ef..a177fd137 100644\n--- a/benchmark/mmmu/bench_sglang.py\n+++ b/benchmark/mmmu/bench_sglang.py\n@@ -119,7 +119,7 @@ async def eval_mmmu(args) -> None:\n         api_key=\"sk\", base_url=f\"http://127.0.0.1:{args.port}/v1\"\n     )\n     semaphore = asyncio.Semaphore(args.concurrency)\n-    start = time.time()\n+    start = time.perf_counter()\n     base_url = f\"http://127.0.0.1:{args.port}\"\n \n     if args.profile:\n@@ -147,7 +147,7 @@ async def eval_mmmu(args) -> None:\n         if profile_output.success:\n             print(\"Profiler stopped\")\n \n-    print(f\"Benchmark time: {time.time() - start}\")\n+    print(f\"Benchmark time: {time.perf_counter() - start}\")\n     args.output_path = f\"./val_sglang.json\"\n     save_json(args.output_path, out_samples)\n     eval_result(model_answer_path=args.output_path, answer_dict=answer_dict)\ndiff --git a/benchmark/mtbench/bench_other.py b/benchmark/mtbench/bench_other.py\nindex 2c321e8a1..5e579e9a6 100644\n--- a/benchmark/mtbench/bench_other.py\n+++ b/benchmark/mtbench/bench_other.py\n@@ -66,7 +66,7 @@ def main(args):\n         answers[i] = cur_answers\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(questions))):\n             get_answer(i)\n@@ -79,7 +79,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"#questions: {len(questions)}, Latency: {latency:.2f}\")\n \ndiff --git a/benchmark/mtbench/bench_sglang.py b/benchmark/mtbench/bench_sglang.py\nindex b57d1647d..0d0545b3a 100644\n--- a/benchmark/mtbench/bench_sglang.py\n+++ b/benchmark/mtbench/bench_sglang.py\n@@ -57,7 +57,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = answer_mt_bench.run_batch(\n         arguments,\n         temperature=0,\n@@ -66,7 +66,7 @@ def main(args):\n         progress_bar=True,\n     )\n     answers = [[s[\"answer_1\"], s[\"answer_2\"]] for s in rets]\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"#questions: {len(questions)}, Latency: {latency:.2f}\")\n \ndiff --git a/benchmark/mtbench/bench_sglang_eagle.py b/benchmark/mtbench/bench_sglang_eagle.py\nindex e1207afe1..3eb6036c7 100644\n--- a/benchmark/mtbench/bench_sglang_eagle.py\n+++ b/benchmark/mtbench/bench_sglang_eagle.py\n@@ -68,7 +68,7 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = answer_mt_bench.run_batch(\n         arguments,\n         temperature=0,\n@@ -78,7 +78,7 @@ def main(args):\n     )\n     answers = [[s[\"answer_1\"], s[\"answer_2\"]] for s in rets]\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     num_output_tokens = sum(\n         s.get_meta_info(\"answer_1\")[\"completion_tokens\"]\n         + s.get_meta_info(\"answer_2\")[\"completion_tokens\"]\ndiff --git a/benchmark/multi_chain_reasoning/bench_other.py b/benchmark/multi_chain_reasoning/bench_other.py\nindex e0ff2be45..f361496ad 100644\n--- a/benchmark/multi_chain_reasoning/bench_other.py\n+++ b/benchmark/multi_chain_reasoning/bench_other.py\n@@ -113,7 +113,7 @@ def main(args):\n             answer = multi_chain_gsm8k(questions[i], args.num_chains, call_generate)\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         if args.parallel == 1:\n             for i in tqdm(range(len(questions))):\n                 get_one_answer(i)\n@@ -134,7 +134,7 @@ def main(args):\n             )\n             states[i] = answer\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         loop = asyncio.get_event_loop()\n         batches = [\n             list(range(i, min(i + args.parallel, len(questions))))\n@@ -144,7 +144,7 @@ def main(args):\n             tasks = [get_one_answer_asyncio(k) for k in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/multi_chain_reasoning/bench_sglang.py b/benchmark/multi_chain_reasoning/bench_sglang.py\nindex 98a6b511e..1d3129db2 100644\n--- a/benchmark/multi_chain_reasoning/bench_sglang.py\n+++ b/benchmark/multi_chain_reasoning/bench_sglang.py\n@@ -90,7 +90,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_chain_gsm8k.run_batch(\n         arguments,\n         temperature=0,\n@@ -98,7 +98,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/benchmark/multi_document_qa/bench_other.py b/benchmark/multi_document_qa/bench_other.py\nindex 6f0addcb7..627837c5c 100644\n--- a/benchmark/multi_document_qa/bench_other.py\n+++ b/benchmark/multi_document_qa/bench_other.py\n@@ -61,7 +61,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = multi_document_qa(generate=call_generate, **arguments[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(labels))):\n             get_one_answer(i)\n@@ -74,7 +74,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(states)\ndiff --git a/benchmark/multi_document_qa/bench_sglang.py b/benchmark/multi_document_qa/bench_sglang.py\nindex 645520166..0b4b0dbc6 100644\n--- a/benchmark/multi_document_qa/bench_sglang.py\n+++ b/benchmark/multi_document_qa/bench_sglang.py\n@@ -49,11 +49,11 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_document_qa.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print([s[\"answer\"] for s in states])\ndiff --git a/benchmark/multi_turn_chat/bench_other.py b/benchmark/multi_turn_chat/bench_other.py\nindex 81d67ab7b..9189af5be 100644\n--- a/benchmark/multi_turn_chat/bench_other.py\n+++ b/benchmark/multi_turn_chat/bench_other.py\n@@ -35,7 +35,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = multi_turns(generate=call_generate, **multi_qas[i])\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(multi_qas))):\n             get_one_answer(i)\n@@ -50,7 +50,7 @@ def main(args):\n             for _ in rets:\n                 pass\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/multi_turn_chat/bench_sglang.py b/benchmark/multi_turn_chat/bench_sglang.py\nindex 7feaced73..1051bf19e 100644\n--- a/benchmark/multi_turn_chat/bench_sglang.py\n+++ b/benchmark/multi_turn_chat/bench_sglang.py\n@@ -27,7 +27,7 @@ def main(args):\n \n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_turns.run_batch(\n         multi_qas,\n         temperature=0,\n@@ -35,7 +35,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/multi_turn_chat/long_prompt_multi_turn.py b/benchmark/multi_turn_chat/long_prompt_multi_turn.py\nindex 20f6dd5e3..bda5bb9cc 100644\n--- a/benchmark/multi_turn_chat/long_prompt_multi_turn.py\n+++ b/benchmark/multi_turn_chat/long_prompt_multi_turn.py\n@@ -84,7 +84,7 @@ def main(args):\n \n     backend = select_sglang_backend(args)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = multi_turns.run_batch(\n         multi_qas,\n         temperature=0,\n@@ -92,7 +92,7 @@ def main(args):\n         num_threads=\"auto\",\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/react/bench_other.py b/benchmark/react/bench_other.py\nindex 91c5546f1..08666662b 100644\n--- a/benchmark/react/bench_other.py\n+++ b/benchmark/react/bench_other.py\n@@ -146,7 +146,7 @@ def main(args):\n \n             states.append(answer)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     if args.backend != \"lmql\":\n         if args.parallel == 1:\n@@ -173,7 +173,7 @@ def main(args):\n             tasks = [run_single_agent_async(arg) for arg in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     print(f\"Latency: {latency:.3f}\")\n \ndiff --git a/benchmark/react/bench_sglang.py b/benchmark/react/bench_sglang.py\nindex b07105e2c..331638e9f 100644\n--- a/benchmark/react/bench_sglang.py\n+++ b/benchmark/react/bench_sglang.py\n@@ -115,14 +115,14 @@ def main(args):\n     sgl.set_default_backend(backend)\n \n     states = []\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = webthink.run_batch(\n         arguments,\n         temperature=0,\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/reasoning_benchmark/bench_sglang.py b/benchmark/reasoning_benchmark/bench_sglang.py\nindex c83204960..ccbff9d17 100644\n--- a/benchmark/reasoning_benchmark/bench_sglang.py\n+++ b/benchmark/reasoning_benchmark/bench_sglang.py\n@@ -51,7 +51,7 @@ def main(args):\n     )\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = reasoning_gen.run_batch(\n         questions,\n         num_threads=args.parallel,\n@@ -60,7 +60,7 @@ def main(args):\n         max_new_tokens=32768,\n         top_p=0.95,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Extract results and record outcomes in a list.\n     outcomes = []\ndiff --git a/benchmark/tip_suggestion/bench_other.py b/benchmark/tip_suggestion/bench_other.py\nindex fcc4fd624..2630081bd 100644\n--- a/benchmark/tip_suggestion/bench_other.py\n+++ b/benchmark/tip_suggestion/bench_other.py\n@@ -68,7 +68,7 @@ def main(args):\n     call_generate = partial(get_call_generate(args), temperature=0)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.backend != \"lmql\":\n \n         def get_one_answer(i):\n@@ -102,7 +102,7 @@ def main(args):\n             loop.run_until_complete(\n                 asyncio.gather(*[get_one_answer_async(i) for i in batch])\n             )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/tip_suggestion/bench_sglang.py b/benchmark/tip_suggestion/bench_sglang.py\nindex 6d17821bc..86c476f97 100644\n--- a/benchmark/tip_suggestion/bench_sglang.py\n+++ b/benchmark/tip_suggestion/bench_sglang.py\n@@ -65,11 +65,11 @@ def main(args):\n     sgl.set_default_backend(select_sglang_backend(args))\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = suggest_tips.run_batch(\n         arguments, temperature=0, num_threads=args.parallel, progress_bar=True\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     print(f\"Latency: {latency:.3f}\")\ndiff --git a/benchmark/tree_of_thought_deep/bench_other.py b/benchmark/tree_of_thought_deep/bench_other.py\nindex 21c7df351..0ef8c6360 100644\n--- a/benchmark/tree_of_thought_deep/bench_other.py\n+++ b/benchmark/tree_of_thought_deep/bench_other.py\n@@ -138,7 +138,7 @@ def main(args):\n     # Run requests\n     states = [None] * len(questions)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.backend != \"lmql\":\n \n         def get_one_answer(i):\n@@ -177,7 +177,7 @@ def main(args):\n             tasks = [get_one_answer_async(k) for k in bt]\n             loop.run_until_complete(asyncio.gather(*tasks))\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     answers_text = []\n     for s in states:\ndiff --git a/benchmark/tree_of_thought_deep/bench_sglang.py b/benchmark/tree_of_thought_deep/bench_sglang.py\nindex bfb2a4113..bcdb6e54d 100644\n--- a/benchmark/tree_of_thought_deep/bench_sglang.py\n+++ b/benchmark/tree_of_thought_deep/bench_sglang.py\n@@ -119,7 +119,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = tree_search.run_batch(\n         arguments,\n         temperature=0,\n@@ -127,7 +127,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     answers_text = []\n     for s in states:\n         answers_text.append([x for xs in s.ret_value for x in xs])\ndiff --git a/benchmark/tree_of_thought_v0/bench_other.py b/benchmark/tree_of_thought_v0/bench_other.py\nindex 86e133577..703ecd7f4 100644\n--- a/benchmark/tree_of_thought_v0/bench_other.py\n+++ b/benchmark/tree_of_thought_v0/bench_other.py\n@@ -121,7 +121,7 @@ def main(args):\n     def get_one_answer(i):\n         states[i] = tree_search(**arguments[i], call_generate=call_generate)\n \n-    tic = time.time()\n+    tic = time.perf_counter()\n     if args.parallel == 1:\n         for i in tqdm(range(len(questions))):\n             get_one_answer(i)\n@@ -134,7 +134,7 @@ def main(args):\n                 )\n             )\n \n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     answers_text = []\n     for s in states:\ndiff --git a/benchmark/tree_of_thought_v0/bench_sglang.py b/benchmark/tree_of_thought_v0/bench_sglang.py\nindex f0d130778..6d7575f36 100644\n--- a/benchmark/tree_of_thought_v0/bench_sglang.py\n+++ b/benchmark/tree_of_thought_v0/bench_sglang.py\n@@ -107,7 +107,7 @@ def main(args):\n     backend = select_sglang_backend(args)\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = tree_search.run_batch(\n         arguments,\n         temperature=0,\n@@ -115,7 +115,7 @@ def main(args):\n         num_threads=args.parallel,\n         progress_bar=True,\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n     answers_text = []\n     for s in states:\n         answers_text.append([x for xs in s[\"answer\"] for x in xs])\ndiff --git a/python/sglang/test/few_shot_gsm8k.py b/python/sglang/test/few_shot_gsm8k.py\nindex 4f655eb60..5aac87bd2 100644\n--- a/python/sglang/test/few_shot_gsm8k.py\n+++ b/python/sglang/test/few_shot_gsm8k.py\n@@ -90,7 +90,7 @@ def run_eval(args):\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     states = few_shot_gsm8k.run_batch(\n         arguments,\n         temperature=args.temperature if hasattr(args, \"temperature\") else 0,\n@@ -99,7 +99,7 @@ def run_eval(args):\n         return_logprob=getattr(args, \"return_logprob\", None),\n         logprob_start_len=getattr(args, \"logprob_start_len\", None),\n     )\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     preds = []\n     for i in range(len(states)):\ndiff --git a/python/sglang/test/few_shot_gsm8k_engine.py b/python/sglang/test/few_shot_gsm8k_engine.py\nindex 67844e2f1..2453a91e4 100644\n--- a/python/sglang/test/few_shot_gsm8k_engine.py\n+++ b/python/sglang/test/few_shot_gsm8k_engine.py\n@@ -89,7 +89,7 @@ def run_eval(args):\n     }\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n \n     loop = asyncio.get_event_loop()\n \n@@ -98,7 +98,7 @@ def run_eval(args):\n     )\n \n     # End requests\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Shutdown the engine\n     engine.shutdown()\ndiff --git a/python/sglang/test/run_eval.py b/python/sglang/test/run_eval.py\nindex fe88171ce..51743be09 100644\n--- a/python/sglang/test/run_eval.py\n+++ b/python/sglang/test/run_eval.py\n@@ -71,9 +71,9 @@ def run_eval(args):\n     )\n \n     # Run eval\n-    tic = time.time()\n+    tic = time.perf_counter()\n     result = eval_obj(sampler)\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Dump reports\n     metrics = result.metrics | {\"score\": result.score}\ndiff --git a/python/sglang/test/test_programs.py b/python/sglang/test/test_programs.py\nindex 262637eed..6756f2dd7 100644\n--- a/python/sglang/test/test_programs.py\n+++ b/python/sglang/test/test_programs.py\n@@ -503,7 +503,7 @@ def test_hellaswag_select():\n     #####################################\n \n     # Run requests\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -514,13 +514,13 @@ def test_hellaswag_select():\n     preds = []\n     for i, ret in enumerate(rets):\n         preds.append(choices[i].index(ret[\"answer\"]))\n-    latency = time.time() - tic\n+    latency = time.perf_counter() - tic\n \n     # Compute accuracy\n     accuracy = np.mean(np.array(preds) == np.array(labels))\n \n     # Test generator style of run_batch\n-    tic = time.time()\n+    tic = time.perf_counter()\n     rets = few_shot_hellaswag.run_batch(\n         arguments,\n         temperature=0,\n@@ -531,7 +531,7 @@ def test_hellaswag_select():\n     preds_gen = []\n     for i, ret in enumerate(rets):\n         preds_gen.append(choices[i].index(ret[\"answer\"]))\n-    latency_gen = time.time() - tic\n+    latency_gen = time.perf_counter() - tic\n \n     # Compute accuracy\n     accuracy_gen = np.mean(np.array(preds_gen) == np.array(labels))\ndiff --git a/python/sglang/test/test_utils.py b/python/sglang/test/test_utils.py\nindex 1e78d6dc1..150f385c9 100644\n--- a/python/sglang/test/test_utils.py\n+++ b/python/sglang/test/test_utils.py\n@@ -449,9 +449,9 @@ def popen_launch_server(\n     else:\n         process = subprocess.Popen(command, stdout=None, stderr=None, env=env)\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     with requests.Session() as session:\n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 headers = {\n                     \"Content-Type\": \"application/json; charset=utf-8\",\n@@ -584,7 +584,7 @@ class TestFile:\n \n \n def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n-    tic = time.time()\n+    tic = time.perf_counter()\n     success = True\n \n     for i, file in enumerate(files):\n@@ -599,13 +599,13 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n                 f\".\\n.\\nBegin ({i}/{len(files) - 1}):\\npython3 {filename}\\n.\\n.\\n\",\n                 flush=True,\n             )\n-            tic = time.time()\n+            tic = time.perf_counter()\n \n             process = subprocess.Popen(\n                 [\"python3\", filename], stdout=None, stderr=None, env=os.environ\n             )\n             process.wait()\n-            elapsed = time.time() - tic\n+            elapsed = time.perf_counter() - tic\n \n             print(\n                 f\".\\n.\\nEnd ({i}/{len(files) - 1}):\\n{filename=}, {elapsed=:.0f}, {estimated_time=}\\n.\\n.\\n\",\n@@ -631,9 +631,9 @@ def run_unittest_files(files: List[TestFile], timeout_per_file: float):\n             break\n \n     if success:\n-        print(f\"Success. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Success. Time elapsed: {time.perf_counter() - tic:.2f}s\", flush=True)\n     else:\n-        print(f\"Fail. Time elapsed: {time.time() - tic:.2f}s\", flush=True)\n+        print(f\"Fail. Time elapsed: {time.perf_counter() - tic:.2f}s\", flush=True)\n \n     return 0 if success else -1\n \ndiff --git a/sgl-router/py_test/test_launch_server.py b/sgl-router/py_test/test_launch_server.py\nindex 33dd3e854..afffe334f 100644\n--- a/sgl-router/py_test/test_launch_server.py\n+++ b/sgl-router/py_test/test_launch_server.py\n@@ -92,9 +92,9 @@ def popen_launch_router(\n \n     process = subprocess.Popen(command, stdout=None, stderr=None)\n \n-    start_time = time.time()\n+    start_time = time.perf_counter()\n     with requests.Session() as session:\n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 response = session.get(f\"{base_url}/health\")\n                 if response.status_code == 200:\n@@ -155,11 +155,11 @@ def terminate_and_wait(process, timeout=300):\n         return\n \n     process.terminate()\n-    start_time = time.time()\n+    start_time = time.perf_counter()\n \n     while process.poll() is None:\n         print(f\"Terminating process {process.pid}\")\n-        if time.time() - start_time > timeout:\n+        if time.perf_counter() - start_time > timeout:\n             raise TimeoutError(\n                 f\"Process {process.pid} failed to terminate within {timeout}s\"\n             )\ndiff --git a/test/srt/experiment_runner.py b/test/srt/experiment_runner.py\nindex 7feeef1aa..f32f61d3b 100644\n--- a/test/srt/experiment_runner.py\n+++ b/test/srt/experiment_runner.py\n@@ -184,9 +184,9 @@ class ExperimentRunner:\n         self.logger = logging.getLogger(__name__)\n \n     def wait_for_server(self, port: int, timeout: int = 300) -> bool:\n-        start_time = time.time()\n+        start_time = time.perf_counter()\n \n-        while time.time() - start_time < timeout:\n+        while time.perf_counter() - start_time < timeout:\n             try:\n                 response = requests.get(f\"http://localhost:{port}/health\")\n                 if response.status_code == 200:\n@@ -197,7 +197,7 @@ class ExperimentRunner:\n         return False\n \n     def run_task(self, config: TaskConfig) -> TaskResult:\n-        start_time = time.time()\n+        start_time = time.perf_counter()\n         client_output = []\n \n         try:\n@@ -247,7 +247,7 @@ class ExperimentRunner:\n                 name=config.name,\n                 success=True,\n                 output=formatted_output,\n-                runtime=time.time() - start_time,\n+                runtime=time.perf_counter() - start_time,\n                 timestamp=datetime.now().isoformat(),\n             )\n \n@@ -256,7 +256,7 @@ class ExperimentRunner:\n                 name=config.name,\n                 success=False,\n                 output=str(e),\n-                runtime=time.time() - start_time,\n+                runtime=time.perf_counter() - start_time,\n                 timestamp=datetime.now().isoformat(),\n             )\n \ndiff --git a/test/srt/models/test_encoder_embedding_models.py b/test/srt/models/test_encoder_embedding_models.py\nindex 5202917c4..bea5d4aff 100644\n--- a/test/srt/models/test_encoder_embedding_models.py\n+++ b/test/srt/models/test_encoder_embedding_models.py\n@@ -79,9 +79,9 @@ class TestEncoderEmbeddingModels(CustomTestCase):\n             # warm up\n             hf_outputs = hf_runner.forward(truncated_prompts)\n \n-            st_start_time = time.time()\n+            st_start_time = time.perf_counter()\n             hf_outputs = hf_runner.forward(truncated_prompts)\n-            st_end_time = time.time()\n+            st_end_time = time.perf_counter()\n \n         with SRTRunner(\n             model_path,\n@@ -95,9 +95,9 @@ class TestEncoderEmbeddingModels(CustomTestCase):\n             # warm up\n             srt_outputs = srt_runner.forward(truncated_prompts)\n \n-            sgl_start_time = time.time()\n+            sgl_start_time = time.perf_counter()\n             srt_outputs = srt_runner.forward(truncated_prompts)\n-            sgl_end_time = time.time()\n+            sgl_end_time = time.perf_counter()\n \n         transformer_time = st_end_time - st_start_time\n         sgl_time = sgl_end_time - sgl_start_time\ndiff --git a/test/srt/test_gptqmodel_dynamic.py b/test/srt/test_gptqmodel_dynamic.py\nindex 27ccd9a4b..284465b8b 100644\n--- a/test/srt/test_gptqmodel_dynamic.py\n+++ b/test/srt/test_gptqmodel_dynamic.py\n@@ -130,9 +130,9 @@ class TestGPTQModelDynamic(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         result = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n \n         print(f\"result = `{result}`\")\n \n@@ -185,9 +185,9 @@ class TestGPTQModelDynamicWithMarlin(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         result = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n \n         print(f\"result = `{result}`\")\n \ndiff --git a/test/srt/test_release_memory_occupation.py b/test/srt/test_release_memory_occupation.py\nindex 7ccd9f1f7..7a7659280 100644\n--- a/test/srt/test_release_memory_occupation.py\n+++ b/test/srt/test_release_memory_occupation.py\n@@ -42,10 +42,10 @@ class TestReleaseMemoryOccupation(CustomTestCase):\n         )\n \n         print(\"release_memory_occupation start\")\n-        t = time.time()\n+        t = time.perf_counter()\n         engine.release_memory_occupation()\n         if _DEBUG_EXTRA:\n-            print(\"release_memory_occupation\", time.time() - t)\n+            print(\"release_memory_occupation\", time.perf_counter() - t)\n \n         if _DEBUG_EXTRA:\n             time.sleep(5)\n@@ -60,10 +60,10 @@ class TestReleaseMemoryOccupation(CustomTestCase):\n             time.sleep(5)\n \n         print(\"resume_memory_occupation start\")\n-        t = time.time()\n+        t = time.perf_counter()\n         engine.resume_memory_occupation()\n         if _DEBUG_EXTRA:\n-            print(\"resume_memory_occupation\", time.time() - t)\n+            print(\"resume_memory_occupation\", time.perf_counter() - t)\n \n         self.assertEqual(\n             _try_allocate_big_tensor(),\ndiff --git a/test/srt/test_torch_compile.py b/test/srt/test_torch_compile.py\nindex 760cec84b..904e49f9d 100644\n--- a/test/srt/test_torch_compile.py\n+++ b/test/srt/test_torch_compile.py\n@@ -62,9 +62,9 @@ class TestTorchCompile(CustomTestCase):\n         res = self.run_decode(16)\n \n         max_tokens = 256\n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(f\"{res=}\")\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")\ndiff --git a/test/srt/test_torch_compile_moe.py b/test/srt/test_torch_compile_moe.py\nindex 42415b155..63423af43 100644\n--- a/test/srt/test_torch_compile_moe.py\n+++ b/test/srt/test_torch_compile_moe.py\n@@ -62,9 +62,9 @@ class TestTorchCompileMoe(CustomTestCase):\n         res = self.run_decode(16)\n \n         max_tokens = 256\n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(f\"{res=}\")\n         throughput = max_tokens / (tok - tic)\n         self.assertGreaterEqual(throughput, 285)\ndiff --git a/test/srt/test_torchao.py b/test/srt/test_torchao.py\nindex 77ec0a570..13c7b60b5 100644\n--- a/test/srt/test_torchao.py\n+++ b/test/srt/test_torchao.py\n@@ -61,9 +61,9 @@ class TestTorchAO(CustomTestCase):\n \n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(res[\"text\"])\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")\ndiff --git a/test/srt/test_update_weights_from_distributed.py b/test/srt/test_update_weights_from_distributed.py\nindex e558a56e3..064406703 100644\n--- a/test/srt/test_update_weights_from_distributed.py\n+++ b/test/srt/test_update_weights_from_distributed.py\n@@ -164,7 +164,7 @@ def init_process_hf(\n     )\n     dist.barrier(group=group, device_ids=[rank])\n     torch.cuda.synchronize()\n-    time_begin_broadcast = time.time()\n+    time_begin_broadcast = time.perf_counter()\n \n     # The last parameter is lm_head.weight, which is tied\n     # with embed_tokens.weight. Actually, we only need\n@@ -182,7 +182,7 @@ def init_process_hf(\n             group=group,\n         )\n     torch.cuda.synchronize()\n-    time_end_broadcast = time.time()\n+    time_end_broadcast = time.perf_counter()\n \n     # Measure the latency of broadcasting/weights update.\n     broadcast_time = time_end_broadcast - time_begin_broadcast\n@@ -282,7 +282,7 @@ def init_process_sgl(\n         )\n \n     torch.cuda.synchronize()\n-    time_begin_update = time.time()\n+    time_begin_update = time.perf_counter()\n \n     # The last parameter is lm_head.weight, which is tied\n     # with embed_tokens.weight. Actually, we only need\n@@ -312,7 +312,7 @@ def init_process_sgl(\n                 },\n             )\n     torch.cuda.synchronize()\n-    time_end_update = time.time()\n+    time_end_update = time.perf_counter()\n \n     # Measure the latency of broadcast/weights update.\n     update_time = time_end_update - time_begin_update\ndiff --git a/test/srt/test_update_weights_from_tensor.py b/test/srt/test_update_weights_from_tensor.py\nindex 1f3592447..38187652b 100644\n--- a/test/srt/test_update_weights_from_tensor.py\n+++ b/test/srt/test_update_weights_from_tensor.py\n@@ -21,9 +21,9 @@ def test_update_weights_from_tensor(tp_size):\n     memory_before = torch.cuda.memory_allocated()\n     new_tensor = torch.full((16384, 2048), 1.5, device=\"cuda\")\n \n-    time_start = time.time()\n+    time_start = time.perf_counter()\n     engine.update_weights_from_tensor([(x, new_tensor) for x in param_names])\n-    print(f\"Time delta: {time.time() - time_start:.03f}\")\n+    print(f\"Time delta: {time.perf_counter() - time_start:.03f}\")\n \n     for param_name in param_names[:3]:\n         _check_param(engine, param_name, [1.5] * 5)\ndiff --git a/test/srt/test_w8a8_quantization.py b/test/srt/test_w8a8_quantization.py\nindex 2cb2fa073..3d4ce1afa 100644\n--- a/test/srt/test_w8a8_quantization.py\n+++ b/test/srt/test_w8a8_quantization.py\n@@ -62,9 +62,9 @@ class TestW8A8(CustomTestCase):\n     def test_throughput(self):\n         max_tokens = 256\n \n-        tic = time.time()\n+        tic = time.perf_counter()\n         res = self.run_decode(max_tokens)\n-        tok = time.time()\n+        tok = time.perf_counter()\n         print(res[\"text\"])\n         throughput = max_tokens / (tok - tic)\n         print(f\"Throughput: {throughput} tokens/s\")",
  "apis": [
    "time.perf_counter"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/benchmark/bench_in_batch_prefix/bench_in_batch_prefix.py",
    "/path/to/repos/sglang/benchmark/benchmark_batch/benchmark_batch.py",
    "/path/to/repos/sglang/benchmark/benchmark_batch/benchmark_tokenizer.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The diff replaces all calls to time.time() with time.perf_counter() in numerous benchmark files and test scripts. The changes affect non-test files in the benchmark folder (and possibly other parts), altering how performance metrics (latency measurements) are computed. This modification is clearly aimed at improving measurement precision (thus indirectly optimizing performance evaluation) by switching to a more precise timer. The commit is not merely a comment or renaming change, but a direct modification of the benchmarking code that affects performance measurement of existing APIs, which are testable on CPU. Therefore, the commit meets the criteria of a performance/optimization related change.",
  "llm_api_reason": "The commit replaces all occurrences of time.time() with time.perf_counter() in various benchmarking and testing scripts. This change affects the Python standard library timing API used for performance measurements in these benchmarks. The affected high-level API is the time.perf_counter() function, which is now used in place of time.time() across the repo."
}