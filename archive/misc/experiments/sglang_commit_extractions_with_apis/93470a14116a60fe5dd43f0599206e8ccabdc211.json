{
  "commit_hash": "93470a14116a60fe5dd43f0599206e8ccabdc211",
  "pr_url": "https://github.com/sgl-project/sglang/pull/5090",
  "pr_date": "2025-04-07",
  "timeline_text": "Copy link Collaborator hebiao064 commented Apr 5, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation Accelerate Spec Decoding + FA3 + CUDA Graph Easier to maintain Follow up on #5050 Benchmark for Llama 8B (Before and After) Iterations GSM Latency on H200 Send One on H200 Main Branch with Flash Infer 2118.780 token/s 229.67 token/s Main Branch  with FA3 2000.892 token/s 235.28 token/s FA3 After commit: 21bf3fe and eb77c88 2428.587 token/s 235.00 token/s GSM Accuracy is Accuracy: 0.775 with spec decoding. Sanity Check without Spec Decoding for Llama 8B Accuracy: 0.775 Invalid: 0.000 Latency: 4.473 s Accuracy: 0.775 Invalid: 0.000 Latency: 3.993 s Output throughput: 4255.736 token/s Sanity Check with Deepseek V3 (Send One) Deepseek V3 with flashinfer + cuda graph: speed=41.59 token/s Deepseek V3 with flashinfer + disabled cuda graph: speed=4.60 token/s Deepseek V3 with flashinfer + cuda graph + spec decode: acc_length=2.59, speed=62.73 token/s Deepseek V3 with flashinfer + disabled cuda graph + spec decode:  acc_length=2.59, speed=11.09 token/s Deepseek V3 with fa3 + cuda graph: speed=44.93 token/s, result is good Deepseek V3 with fa3 + disabled cuda graph: speed=4.22 token/s token/s, result is good Deepseek V3 with fa3 + cuda graph + spec decode: acc_length=2.64, speed=66.66 token/s Deepseek V3 with fa3 + disabled cuda graph + spec decode: acc_length=2.69, speed=11.72 token/s Sanity Check with Deepseek V3 (GSM8K) Deepseek V3 with flashinfer + cuda graph: Accuracy: 0.960 Invalid: 0.000 Latency: 45.092 s Output throughput: 439.656 token/s Deepseek V3 with flashinfer + disabled cuda graph: Accuracy: 0.970 Invalid: 0.000 Latency: 154.613 s Output throughput: 126.697 token/s Deepseek V3 with flashinfer + cuda graph + spec decode: Accuracy: 0.970 Invalid: 0.000 Latency: 42.325 s Output throughput: 461.995 token/s Deepseek V3 with flashinfer + disabled cuda graph + spec decode: Accuracy: 0.965 Invalid: 0.000 Latency: 93.745 s Output throughput: 209.964 token/s Deepseek V3 with fa3 + cuda graph: Accuracy: 0.965 Invalid: 0.000 Latency: 43.330 s Output throughput: 463.840 token/s Deepseek V3 with fa3 + disabled cuda graph: Accuracy: 0.950 Invalid: 0.000 Latency: 178.272 s Output throughput: 112.082 token/s Deepseek V3 with fa3 + cuda graph + spec decode: Accuracy: 0.955 Invalid: 0.000 Latency: 36.043 s Output throughput: 550.533 token/s Deepseek V3 with fa3 + disabled cuda graph + spec decode: Accuracy: 0.960 Invalid: 0.000 Latency: 134.789 s Output throughput: 148.343 token/s Modifications See all commits msg, they are clear Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 5 BBuf, zhyncs, zcnrex, FlamingoPg, and hubertlu-tw reacted with thumbs up emoji All reactions \ud83d\udc4d 5 reactions hebiao064 added 4 commits April 5, 2025 21:59 Optimize FA3 Code: Renaming Params 4a39559 Polish the init_forward_metadata, reduced one tensor assignment 9f62ca5 Polish forward_extend and forward_decode d18caf2 Optimized CUDA GRAPH: Target Verify by precalculate some metadata in \u2026 \u2026 21bf3fe \u2026capture hebiao064 force-pushed the optimize_fa3_code branch\n    from eb77c88 to bc73017 Compare April 5, 2025 23:47 hebiao064 marked this pull request as ready for review April 5, 2025 23:48 hebiao064 requested review from merrymercy , Ying1123 , zhyncs , ispobock and HaiShaw as code owners April 5, 2025 23:48 Optimized CUDA GRAPH: Draft Decode by removed duplicate code \u2026 e300570 Co-authored-by: Qingquan Song <ustcsqq@gmail.com> hebiao064 force-pushed the optimize_fa3_code branch\n    from bc73017 to e300570 Compare April 5, 2025 23:49 hebiao064 mentioned this pull request Apr 5, 2025 [Roadmap] FlashAttention3 Support as SGLang Attention Backend #4709 Closed 15 tasks Merge branch 'main' into optimize_fa3_code 6cc636f Fridge003 reviewed Apr 6, 2025 View reviewed changes python/sglang/srt/layers/attention/flashattention_backend.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Fridge003 reviewed Apr 6, 2025 View reviewed changes python/sglang/srt/layers/attention/flashattention_backend.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/attention/flashattention_backend.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/attention/flashattention_backend.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/attention/flashattention_backend.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . zhyncs and others added 3 commits April 6, 2025 12:43 Merge branch 'main' into optimize_fa3_code 2f7862c merge conflict 708cdc9 isort 7c843f7 Hide details View details zhyncs merged commit 93470a1 into sgl-project : main Apr 7, 2025 35 of 37 checks passed Uh oh! There was an error while loading. Please reload this page . hebiao064 mentioned this pull request Apr 8, 2025 Support Page Size > 1 (when top k = 1) for FA3 Spec Decode #5168 Closed 6 tasks finger92 pushed a commit\n        to protagolabs/sglang\n      that referenced\n      this pull request Apr 10, 2025 Refactor and Optimize FA3 Code ( sgl-project#5090 ) \u2026 6ba74e2 Co-authored-by: Qingquan Song <ustcsqq@gmail.com> thyecust pushed a commit\n        to thyecust/sglang\n      that referenced\n      this pull request Apr 11, 2025 Refactor and Optimize FA3 Code ( sgl-project#5090 ) \u2026 804e840 Co-authored-by: Qingquan Song <ustcsqq@gmail.com> jianan-gu pushed a commit\n        to jianan-gu/sglang\n      that referenced\n      this pull request Apr 13, 2025 Refactor and Optimize FA3 Code ( sgl-project#5090 ) \u2026 2dddcfe Co-authored-by: Qingquan Song <ustcsqq@gmail.com> jimoosciuc pushed a commit\n        to Furion-cn/sglang\n      that referenced\n      this pull request Apr 17, 2025 Refactor and Optimize FA3 Code ( sgl-project#5090 ) \u2026 8c0123a Co-authored-by: Qingquan Song <ustcsqq@gmail.com> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Apr 23, 2025 rebase sglang to tag v0.4.5.post1 ( sgl-project#13 ) \u2026 3ecb4e3 * Support with_stack and record_shapes in profiler ( sgl-project#4740 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* test: reduce `mem_fraction_static` for gemma3 vision test ( sgl-project#4840 )\n\n* Fix CI tests ( sgl-project#4853 )\n\n* Fix fa3 cuda graph page_size > 1 precision and page_size=1 speed ( sgl-project#4855 )\n\n* Revert \"get the python version from env ( sgl-project#4729 )\" ( sgl-project#4863 )\n\n* [Feature] add multi-rank support for Lora ( sgl-project#4492 )\n\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\n\n* Clean up `import vllm` in quantization/__init__.py ( sgl-project#4834 )\n\n* Fix wrong variable name when stopping memory profile ( sgl-project#4772 )\n\n* [Feat] support deepgemm for cmake ( sgl-project#4864 )\n\n* Make torch compile configurable for biased_grouped_topk ( sgl-project#4749 )\n\n* update sgl-kernel test ci ( sgl-project#4866 )\n\n* fix sampling issue ( sgl-project#4871 )\n\n* bump sgl-kernel 0.0.5.post4 ( sgl-project#4768 )\n\n* fix sgl-kernel cu118 build ( sgl-project#4872 )\n\n* [Feature] Support FA3 backend for MLA ( sgl-project#4831 )\n\n* upgrade sgl-kernel 0.0.5.post4 ( sgl-project#4873 )\n\n* update torch compile doc ( sgl-project#4874 )\n\n* bump v0.4.4.post3 ( sgl-project#4878 )\n\n* Fix BadRequestError wrong arguments and remove openai dependency ( sgl-project#4882 )\n\n* Improve stack trace of retry errors ( sgl-project#4845 )\n\n* Tiny fix doc error ( sgl-project#4795 )\n\n* [Docs] Update DeepGEMM at README.md ( sgl-project#4886 )\n\n* Update CODEOWNERS ( sgl-project#4889 )\n\n* Delete test_deep_gemm.py ( sgl-project#4891 )\n\n* Add deepseek style fused moe group gate selection kernel ( sgl-project#4530 )\n\n* quick fix: add default for new kernel ( sgl-project#4898 )\n\n* remove setup for sgl-kernel ( sgl-project#4899 )\n\n* [Misc] Clean m.def and add Development Tips ( sgl-project#4890 )\n\n* fix allreduce test ( sgl-project#4909 )\n\n* Support page size > 1 + eagle ( sgl-project#4908 )\n\n* Fix retract for page size > 1 ( sgl-project#4914 )\n\n* [Feature] use pytest for sgl-kernel ( sgl-project#4896 )\n\n* fix bmm fp8 ( sgl-project#4926 )\n\n* Fix the timeout for unit-test-2-gpu in pr-test.yml ( sgl-project#4927 )\n\n* Fix 2-gpu CI test and suppress some warnings ( sgl-project#4930 )\n\n* [feat] add fa3 in sgl-kernel ( sgl-project#4902 )\n\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\n\n* Fix sglang frontend's incorrect dependency on torch ( sgl-project#4931 )\n\n* [Fix] avoid stream sync and torch compile in prefill for fa3 backend ( sgl-project#4932 )\n\n* cleanup sgl-kernel ( sgl-project#4933 )\n\n* [Fix] Improve Lora tests and reduce CI runtime ( sgl-project#4925 )\n\n* Fix DeepSeek bug causing 2.2% MMLU drop when TP!=DP ( sgl-project#4883 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* [Fix] Add torch compile for torch.clamp back ( sgl-project#4936 )\n\n* Fix oom error for large page size ( sgl-project#4913 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* [feat] interface for platforms abstraction ( sgl-project#4928 )\n\n* [Fix] revert clean m.def for cudagraph ( sgl-project#4944 )\n\n* refactor: multimodal data ( sgl-project#4754 )\n\n* bump sgl-kernel v0.0.6 ( sgl-project#4950 )\n\n* [Build] Fix cuda12.8 build error in nvfp4_scaled_mm_kernels.cu ( sgl-project#4953 )\n\n* use fa3 in sgl-kernel ( sgl-project#4954 )\n\n* Revert PR 4764 & 4813 related to R1 RoPE ( sgl-project#4959 )\n\n* [Feature] Support DeepEP Low Latency ( sgl-project#4767 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* update bench_serving ( sgl-project#4958 )\n\n* Prevent memory leak of retract_decode when page_size > 1 ( sgl-project#4977 )\n\n* [VLM RLHF] Take Image input for verl vlm rollout ( sgl-project#4915 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: GeLee <leege233@gmail.com>\n\n* Large page size aligned hierarchical caching ( sgl-project#4581 )\n\n* bug fix for hicache host eviction ( sgl-project#4989 )\n\n* sgl scaled_fp8_quant support output padding ( sgl-project#4861 )\n\n* Add Eagle Speculative Decoding to FA3 Backend ( sgl-project#4951 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\n\n* Update tokenizer_manager.py ( sgl-project#5008 )\n\n* [sgl-kernel] per token group quant support COLUMN MAJOR ( sgl-project#4817 )\n\n* update cutlass tag ( sgl-project#5011 )\n\n* Feature/revise docs ci ( sgl-project#5009 )\n\n* fix: fix illegal cuda memory access at fused_moe_kernel ( sgl-project#4727 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* [Build] Support build sgl-kernel with ccache ( sgl-project#5020 )\n\n* fix deepgemm as well ( sgl-project#5030 )\n\n* try to fix ci oserror ( sgl-project#5024 )\n\n* Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5005 )\n\n* Small refactor DeepEPMode to clean up code a bit ( sgl-project#4992 )\n\n* [Fix] fix fa3 build at cu118 ( sgl-project#5036 )\n\n* Revert \"Replace enable_flashinfer_mla argument with attention_backend\" ( sgl-project#5048 )\n\n* bump sgl-kernel v0.0.7 ( sgl-project#5046 )\n\n* update eagle-3 docs ( sgl-project#4796 )\n\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\n\n* Add LlavaLlamaForCausaLM in MultiModal Processors ( sgl-project#5039 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* Update the retry count ( sgl-project#5051 )\n\n* upgrade sgl-kernel v0.0.7 ( sgl-project#5049 )\n\n* [2/3] fix dsv3 awq issue  ( sgl-project#4625 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\n\n* Feature/revise docs ci ( sgl-project#5056 )\n\n* Add H20 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5057 )\n\n* [fix] remove `cuda_device_count_stateless` ( sgl-project#5060 )\n\n* Small refactor DeepEPDispatcher into subclasses ( sgl-project#4994 )\n\n* Support async DeepEP by splitting into two stages ( sgl-project#4995 )\n\n* Cleanup unused resources after DeepEP operation ( sgl-project#4996 )\n\n* Add DeepSeek V3/R1 shared experts fusion ( sgl-project#4918 )\n\n* [deepep] fix: shared experts are not initialized when shared experts fusion is enabled ( sgl-project#5072 )\n\n* fix dummy-load deepseekv2 ( sgl-project#4535 )\n\n* support sgl-kernel on blackwell ( sgl-project#5074 )\n\n* FA3 Spec Decoding to support top k = 1 and add cuda graph support ( sgl-project#5050 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Chunan Zeng <zcnrex@gmail.com>\n\n* [Revision] Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5052 )\n\n* upgrade transformers 4.51.0 ( sgl-project#5088 )\n\n* sgl-kernel transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5079 )\n\n* bump sgl-kernel 0.0.8 ( sgl-project#5089 )\n\n* python transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5080 )\n\n* bump v0.4.4.post4 ( sgl-project#5091 )\n\n* Fix: Reduce the number of document ci attempts to avoid long ci running ( sgl-project#5097 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Add Llama4 support ( sgl-project#5092 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Fix refactor error - fp8.py ( sgl-project#5106 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* bump v0.4.5 ( sgl-project#5117 )\n\n* [ci] fix llama4 ci error ( sgl-project#5126 )\n\n* Refactor and Optimize FA3 Code ( sgl-project#5090 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\n\n* Add Llama4 user guide ( sgl-project#5133 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* [Misc] Use pytest.mark.skipif in sgl-kernel test ( sgl-project#5137 )\n\n* feat: disable grammar restrictions within reasoning sections ( sgl-project#4984 )\n\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\n\n* [modelopt] automatically inspect if model is ModelOpt quantized and set quantization method ( sgl-project#5145 )\n\n* [AMD] Fix missing per_token_group_quant_fp8 for ROCm ( sgl-project#5140 )\n\n* fix multimodal hash feature ( sgl-project#5083 )\n\n* Fix run time error in ROCm platform ( sgl-project#5147 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\n\n* [FA3 Feature] Support multi modal Llama-3.2-11B-Vision-Instruct ( sgl-project#5103 )\n\n* Add unit test on page_size > 1 and mla and  integration test for Flash Attention 3 ( sgl-project#4760 )\n\n* Use public model for FA3 speculative decode testing ( sgl-project#5152 )\n\n* Add dummy grok test to amd CI. ( sgl-project#5115 )\n\n* fix empty_cache error in pt_weights_iterator ( sgl-project#5151 )\n\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\n\n* Fix torch compile errors ( sgl-project#5158 )\n\n* Fix loading KV quantization scale; Enable modelopt kv cache ( sgl-project#4686 )\n\nCo-authored-by: qingquansong <ustcsqq@gmail.com>\n\n* [PD] Fix unclosed prefill connection warning of mini_lb ( sgl-project#5155 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Add optimized native kernels in sgl-kernel ( sgl-project#5150 )\n\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\n\n* [PD] Simplify mini LB ( sgl-project#4911 )\n\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\n\n* Small improvement of native api docs ( sgl-project#5139 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* [feat&refactor] Enhance multimodal input support with refactor io_struct ( sgl-project#4938 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Support 2x8xH100 for Llama 4 ( sgl-project#5159 )\n\n* FP4 weight loading and inference (2/2) ( sgl-project#3972 )\n\n* Fix multimodal hashing error ( sgl-project#5174 )\n\n* Tiny disable model that does not work ( sgl-project#5175 )\n\n* [Bugfix] Fix index out of bounds in local attention with large sequences ( sgl-project#5173 )\n\n* [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* docs: remove the use of Downward API for LWS_WORKER_INDEX ( sgl-project#5110 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* feat: add DeepGEMM build warning ( sgl-project#5176 )\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* fix: use DeepEPDispatcher on CUDA ( sgl-project#5180 )\n\n* [DeepEP] fix: import buffer error ( sgl-project#5179 )\n\n* Let `bench_one_batch` support `enable_dp_attention` ( sgl-project#4058 )\n\n* [Misc] clean up vllm in sgl-kernel test ( sgl-project#5189 )\n\n* Fix ci test \"test_eval_fp8_accuracy\" failed ( sgl-project#5185 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\n\n* Optimize topk operation in llama4 ( sgl-project#5128 )\n\n* Support Llama4 fp8 inference ( sgl-project#5194 )\n\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* [ci] fix ci test fused_moe op ( sgl-project#5102 )\n\n* model: support mllama4 ( sgl-project#5144 )\n\n* update grok test ( sgl-project#5171 )\n\n* sgl-kernel use cutlass latest version for fp8 blockwise gemm ( sgl-project#5207 )\n\n* Add H20 dtype fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5196 )\n\n* fix: log warning when disable cuda graph ( sgl-project#5209 )\n\n* [metrics] Add in queue metrics ( sgl-project#4444 )\n\n* Fix DeepSeek error when using DeepEP mode ( sgl-project#5190 )\n\n* reduce moe_align_block_size_kernel small batch mode overhead ( sgl-project#5086 )\n\n* [PD] Support KV transfer with mooncake ( sgl-project#4880 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\n\n* [PD] Add get_contiguous_buf_infos interface for MLATokenToKVPool ( sgl-project#5204 )\n\n* Update deps for mllama4 ( sgl-project#5215 )\n\n* Fix deepseek-v3 with torch.compile in PyTorch 2.6. ( sgl-project#5213 )\n\n* ROCm sgl-kernel: compatible to later torch ( sgl-project#5167 )\n\n* [Misc] Clean sgl-kernel test  ( sgl-project#5216 )\n\n* Update Makefile / build script to avoid installing incompatible torch dependency ( sgl-project#5245 )\n\n* Fix torch.compile cacheing ( sgl-project#5259 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* ROCm/AITER CK_MoE: update 2-stage kernels & support both Activations ( sgl-project#5228 )\n\n* Optimize attention in llama4 ( sgl-project#5127 )\n\n* Optimize GPU memory usage in FlashAttentionBackend's strided indexing ( sgl-project#5262 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* Support `--enable-llama4-multimodal` ( sgl-project#5254 )\n\n* [fix] fix mrope positions not picked up ( sgl-project#5265 )\n\n* doc: nested loop code for offline engine ( sgl-project#5244 )\n\n* fix: examples for token_in_token_out_vlm  ( sgl-project#5193 )\n\n* Fix a 404 link in send_request.ipynb ( sgl-project#5280 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* fix: enable fp4 compilation on cu128 ( sgl-project#5286 )\n\n* feat: add cu128 identifier for sgl-kernel ( sgl-project#5287 )\n\n* chore: relax the torch version restriction for sgl-kernel compilation ( sgl-project#5288 )\n\n* chore: bump sgl-kernel v0.0.8.post1 ( sgl-project#5289 )\n\n* [PD] fix: skip warmup request in disaggregation mode to prevent crash on timeout ( sgl-project#5292 )\n\n* [Docs] Supported Model Docs - Major restructuring ( sgl-project#5290 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* fix: update update_wheel_index for cu128 ( sgl-project#5300 )\n\n* [Docs] Remove the older supported docs section ( sgl-project#5301 )\n\n* remove moe_align_block_size torch.zeros in small batch/expert mode ( sgl-project#5298 )\n\n* feat: add blackwell Dockerfile ( sgl-project#5302 )\n\n* feat: add blackwell workflow ( sgl-project#5303 )\n\n* fix: use fa3 unit test on hopper only ( sgl-project#5304 )\n\n* misc: update blackwell Dockerfile ( sgl-project#5306 )\n\n* fix: remove cublas_grouped_gemm ( sgl-project#5307 )\n\n* fix: update flash attn ( sgl-project#5308 )\n\n* fix: use deepgemm only on hopper ( sgl-project#5310 )\n\n* [VLM] Adopt fast image processor by default ( sgl-project#5065 )\n\n* Adjust ci test threshold ( sgl-project#5271 )\n\n* Blackwell Cutlass MLA kernel ( sgl-project#5142 )\n\n* misc: cleanup 3rdparty ( sgl-project#5311 )\n\n* update variable naming and comments for rocm ( sgl-project#5299 )\n\n* Fix w8a8_int8 model shared experts fusion load weights error ( sgl-project#5120 )\n\n* Add flash_attn_varlen_func to sgl-kernel ( sgl-project#5315 )\n\n* Fix fa3 window size setup ( sgl-project#5316 )\n\n* chore: bump sgl-kernel v0.0.8.post2 ( sgl-project#5317 )\n\n* feat: use fa3 mla by default on hopper ( sgl-project#5210 )\n\nCo-authored-by: yundai424 <yundai424@gmail.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* Fix: docs/backend/structured_outputs.ipynb ( sgl-project#4884 )\n\n* Delete python/sglang/srt/layers/moe/fused_moe_triton/configs/E=257,N=\u2026 ( sgl-project#5321 )\n\n* refine fused_moe tuning docs ( sgl-project#5294 )\n\n* Support server based rollout in Verlengine ( sgl-project#4848 )\n\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\n\n* [Feat] Add sparse attn to sgl-kernel ( sgl-project#5327 )\n\n* fix: solve cu118 issue for cutlass mla ( sgl-project#5331 )\n\n* chore: bump sgl-kernel v0.0.8.post3 ( sgl-project#5332 )\n\n* ci: update release node ( sgl-project#5333 )\n\n* fix: determine if flashinfer is installed ( sgl-project#5336 )\n\n* feat: adapt merge_state ( sgl-project#5337 )\n\n* misc: update sagemaker Dockerfile ( sgl-project#5341 )\n\n* Fix: Ensure tensors for dist.broadcast match NCCL backend device ( sgl-project#5322 )\n\n* docs: update adoption and sponsorship list with Oracle ( sgl-project#5343 )\n\n* chore: upgrade sgl-kernel 0.0.8.post3 ( sgl-project#5342 )\n\n* Fix typo: infight -> inflight ( sgl-project#5357 )\n\n* [PD] Add transfer backend abstraction ( sgl-project#5328 )\n\n* fix MLATokenToKVPoolHost get_size_per_token bug ( sgl-project#5161 )\n\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\n\n* fix sgl-project#5322 ( sgl-project#5359 )\n\n* feat: update experiment_runner ( sgl-project#5360 )\n\n* [DeepEP] Reduce routed scaling overhead ( sgl-project#5277 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Fix DeepSeek DP Attention + torch compile ( sgl-project#5367 )\n\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Support for Qwen2.5-VL Model in bitsandbytes Format ( sgl-project#5003 )\n\n* Fix PD disaggregation bugs ( sgl-project#5326 )\n\n* [PD Bug] fix  MLA get_contiguous_buf_infos error ( sgl-project#5384 )\n\n* [perf] experimental enhance fp8 per-tensor quant ( sgl-project#5370 )\n\n* Apply deepseek cuda rope ( sgl-project#5385 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* apply fused moe gate in ds v3/r1 ( sgl-project#5371 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: update test config ( sgl-project#5392 )\n\n* [Fix] Turn off DeepGEMM by default ( sgl-project#5263 )\n\n* minor clean up of sgl-kernel/CMakeLists.txt ( sgl-project#5393 )\n\n* Add A800 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5368 )\n\n* Add H20 dtype fp8_w8a8 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5291 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* [fix/misc] remove duplicate row in deepseek v2 model ( sgl-project#5279 )\n\n* chore: upgrade DeepGEMM ( sgl-project#5395 )\n\n* fix: update pr-test-sgl-kernel ( sgl-project#5399 )\n\n* kernel: support slightly faster merge_state_v2 cuda kernel ( sgl-project#5381 )\n\n* chore: bump sgl-kernel 0.0.9 ( sgl-project#5400 )\n\n* chore: upgrade sgl-kernel 0.0.9 ( sgl-project#5401 )\n\n* Tiny fix DeepseekScalingRotaryEmbedding always use forward_native ( sgl-project#5406 )\n\n* Fix bench_serving with random-ids ( sgl-project#5214 )\n\n* [misc] fix ci flaky case ( sgl-project#5352 )\n\n* [FIX] Fix concatenation error in capture_bs when open --disable-cuda-graph-padding and without MTP ( sgl-project#5412 )\n\n* Support dynamic connection and TP 16 ( sgl-project#5351 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Fix broadcast use cuda device lead to memory capacity unbalanced ( sgl-project#5416 )\n\n* [PD] Fix dynamic port support and MLA buffer for Mooncake ( sgl-project#5415 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\n\n* Distinguish bootstrap key only in decode server ( sgl-project#5422 )\n\n* [PD] Remove unused bootstrap param and fix port table type ( sgl-project#5423 )\n\n* [minor] cleanup cmakelists.txt ( sgl-project#5420 )\n\n* bugfix: fix merge_state_v2 cuda graph ( sgl-project#5419 )\n\n* chore: bump sgl-kernel v0.0.9.post1 ( sgl-project#5430 )\n\n* fix: solve release issue ( sgl-project#5434 )\n\n* BLackwell cutlass mla: Add check for bad page size/block num combinations ( sgl-project#5431 )\n\n* feat: update model_specific_adjustment ( sgl-project#5344 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* chore: upgrade sgl-kernel 0.0.9.post1 ( sgl-project#5436 )\n\n* Fix ignore_eos parameter when loading a chat template ( sgl-project#5264 )\n\n* add attention backend supporting matrix in the doc ( sgl-project#5211 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\n\n* Support BNB quantization for llama/mllama ( sgl-project#5038 )\n\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\n\n* [Docs] Update start/install.md ( sgl-project#5398 )\n\n* [Minor] Move torch.compile patch to a better place ( sgl-project#5397 )\n\n* [Bug fix] need record start time in pd mode ( sgl-project#5425 )\n\n* Support MHA with chunked prefix cache for DeepSeek chunked prefill ( sgl-project#5113 )\n\n* chore: bump v0.4.5.post1 ( sgl-project#5445 )\n\n* Revert \"[SW-226289] rebase sglang to tag v0.4.5 ( sgl-project#12 )\"\n\nThis reverts commit 0eac714 .\n\n---------\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: chaobo jia <91889375+jcbjcbjc@users.noreply.github.com>\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\nCo-authored-by: Fr4nk1in <sh.fu@outlook.com>\nCo-authored-by: yinfan98 <1106310035@qq.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\nCo-authored-by: SEPLOS <seplos@aliyun.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: GeLee <leege233@gmail.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\nCo-authored-by: Kaiyu Yang <yangky@umich.edu>\nCo-authored-by: renxin <90580890+renxinx@users.noreply.github.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: Tommy Yang <tommyyang0524@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\nCo-authored-by: Yun Dai <yundai424@gmail.com>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: kk <43161300+kkHuang-amd@users.noreply.github.com>\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DangKai <dangkai4u@outlook.com>\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: Ma Mingfei <mingfei.ma@intel.com>\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: grimoire <streetyao@live.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: Zhaoyang Hao <77828610+Muuuchen@users.noreply.github.com>\nCo-authored-by: Teng Ma <805522925@qq.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: Richard Zou <zou3519@users.noreply.github.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: Yusong Gao <yusong.gao@icloud.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: tianlian yi <91449279+yitianlian@users.noreply.github.com>\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\nCo-authored-by: yulei <yuulei12@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Yangcheng Li <bluebluelitchi@hotmail.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\nCo-authored-by: mRSun15 <3150105645@zju.edu.cn>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:01",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "meta-llama/Llama-3.1-8B-Instruct",
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": [
    "lm_eval --model sglang --model_args pretrained=meta-llama/Llama-3.1-8B-Instruct --tasks gsm8k --batch_size 1"
  ],
  "perf_command": "python benchmarks/benchmark_serving.py --model meta-llama/Llama-3.1-8B-Instruct",
  "commit_subject": "Refactor and Optimize FA3 Code (#5090)",
  "commit_message": "Refactor and Optimize FA3 Code (#5090)\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>",
  "commit_date": "2025-04-07T11:52:42-07:00",
  "files_changed": [
    "python/sglang/srt/layers/attention/flashattention_backend.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 236,
    "num_files": 1,
    "num_hunks": 20,
    "num_non_test_edited_lines": 236,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/attention/flashattention_backend.py b/python/sglang/srt/layers/attention/flashattention_backend.py\nindex 62604fe56..45e64c45e 100644\n--- a/python/sglang/srt/layers/attention/flashattention_backend.py\n+++ b/python/sglang/srt/layers/attention/flashattention_backend.py\n@@ -1,24 +1,16 @@\n from __future__ import annotations\n \n-import numpy as np\n-\n-from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n-\n-\"\"\"\n-Support different attention backends.\n-Now there are three backends: FlashInfer, Triton and FlashAttention.\n-Each backend supports two operators: extend (i.e. prefill with cached prefix) and decode.\n-\"\"\"\n-\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Optional, Union\n \n+import numpy as np\n import torch\n \n from sglang.srt.configs.model_config import AttentionArch\n from sglang.srt.layers.attention.base_attn_backend import AttentionBackend\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_executor.forward_batch_info import ForwardBatch, ForwardMode\n+from sglang.srt.speculative.eagle_utils import EagleDraftInput, EagleVerifyInput\n \n if TYPE_CHECKING:\n     from sglang.srt.layers.radix_attention import RadixAttention\n@@ -30,22 +22,25 @@ from sgl_kernel.flash_attn import flash_attn_with_kvcache\n @dataclass\n class FlashAttentionMetadata:\n     \"\"\"Metadata to be init once in the model forward pass,\n-    each layer's forward pass can reuse the metadata.\"\"\"\n+    each layer's forward pass can reuse the metadata.\n \n-    # Cumulative sequence lengths for query\n-    cu_seqlens_q: torch.Tensor = None\n-    # Cumulative sequence lengths for key\n-    cu_seqlens_k: torch.Tensor = None\n+    For each init metadata function, we will try set up them in below order\n+    \"\"\"\n+\n+    # Sequence lengths for the forward batch\n+    cache_seqlens_int32: torch.Tensor = None\n     # Maximum sequence length for query\n     max_seq_len_q: int = 0\n     # Maximum sequence length for key\n     max_seq_len_k: int = 0\n+    # Cumulative sequence lengths for query\n+    cu_seqlens_q: torch.Tensor = None\n+    # Cumulative sequence lengths for key\n+    cu_seqlens_k: torch.Tensor = None\n     # Window size (typically used by Gemma)\n     window_size: tuple = (-1, -1)\n     # Page table, the index of KV Cache Tables/Blocks\n     page_table: torch.Tensor = None\n-    # Sequence lengths for the forward batch\n-    cache_seqlens_int32: torch.Tensor = None\n \n     @dataclass\n     class LocalAttentionMetadata:\n@@ -270,9 +265,9 @@ class FlashAttentionBackend(AttentionBackend):\n         self,\n         model_runner: ModelRunner,\n         skip_prefill: bool = False,\n+        speculative_step_id=0,\n         topk=0,\n         speculative_num_steps=0,\n-        step_id=0,\n     ):\n         super().__init__()\n \n@@ -293,14 +288,12 @@ class FlashAttentionBackend(AttentionBackend):\n         ) and (not global_server_args_dict[\"disable_mla\"])\n         self.skip_prefill = skip_prefill\n \n-        # TODO: Support Topk > 1 for FlashAttentionBackend Spec Decoding\n-        assert (\n-            topk <= 1\n-        ), \"topk must be 1 (if spec decoding) or 0 (if no spec decoding) for FlashAttentionBackend\"\n-\n-        self.topk = 1\n-        self.step_id = step_id\n+        self.topk = topk\n         self.speculative_num_steps = speculative_num_steps\n+        self.speculative_num_draft_tokens = (\n+            model_runner.server_args.speculative_num_draft_tokens\n+        )\n+        self.speculative_step_id = speculative_step_id\n \n         # Local attention settings\n         self.attention_chunk_size = (\n@@ -310,71 +303,59 @@ class FlashAttentionBackend(AttentionBackend):\n         )\n \n     def init_forward_metadata(self, forward_batch: ForwardBatch):\n-        \"\"\"Initialize forward metadata to cache repetitive calculations.\"\"\"\n+        \"\"\"Initialize forward metadata hence all layers in the forward pass can reuse it.\"\"\"\n         metadata = FlashAttentionMetadata()\n         seqlens_in_batch = forward_batch.seq_lens\n         batch_size = len(seqlens_in_batch)\n         device = seqlens_in_batch.device\n+\n         if forward_batch.forward_mode.is_decode():\n-            # Skip Prefill or Draft Decode\n-            # Note: Draft Decode will be ran on the Draft Worker\n+            # Draft Decode\n             if forward_batch.spec_info is not None:\n+                metadata.cache_seqlens_int32 = (\n+                    seqlens_in_batch + (self.speculative_step_id + 1)\n+                ).to(torch.int32)\n+                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item() + (\n+                    self.speculative_step_id + 1\n+                )\n                 metadata.cu_seqlens_q = torch.arange(\n                     0, batch_size + 1, dtype=torch.int32, device=device\n                 )\n-                seq_lens_with_decode = seqlens_in_batch + (self.step_id + 1)\n-                metadata.cache_seqlens_int32 = seq_lens_with_decode.to(torch.int32)\n                 metadata.cu_seqlens_k = torch.nn.functional.pad(\n                     torch.cumsum(\n                         metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     ),\n                     (1, 0),\n                 )\n-                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item() + (\n-                    self.step_id + 1\n-                )\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n-                cache_loc = forward_batch.out_cache_loc.view(\n-                    self.speculative_num_steps, -1\n-                ).T\n-\n-                for idx, single_seq_len in enumerate(seq_lens_with_decode):\n-                    real_bsz_start_idx = idx\n-                    real_bsz_end_idx = idx + 1\n-                    metadata.page_table[\n-                        real_bsz_start_idx:real_bsz_end_idx,\n-                        (single_seq_len - (self.step_id + 1)) : single_seq_len,\n-                    ] = cache_loc[\n-                        real_bsz_start_idx:real_bsz_end_idx, : (self.step_id + 1)\n-                    ]\n-            else:  # Normal Decode without Spec Decoding\n+            else:\n+                # Normal Decode\n                 metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n+                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n+                metadata.cu_seqlens_q = torch.arange(\n+                    0, batch_size + 1, dtype=torch.int32, device=device\n+                )\n                 metadata.cu_seqlens_k = torch.nn.functional.pad(\n                     torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n                 )\n-                metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n                 metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                     forward_batch.req_pool_indices, : metadata.max_seq_len_k\n                 ]\n-                metadata.cu_seqlens_q = torch.arange(\n-                    0, batch_size + 1, dtype=torch.int32, device=device\n-                )\n         elif forward_batch.forward_mode.is_target_verify():\n-            # Note: Target Verify will be ran on the Target Worker\n-            draft_token_num = forward_batch.spec_info.draft_token_num\n             metadata.cache_seqlens_int32 = (\n-                forward_batch.seq_lens + draft_token_num\n+                forward_batch.seq_lens + self.speculative_num_draft_tokens\n             ).to(torch.int32)\n-            metadata.max_seq_len_q = draft_token_num\n+            metadata.max_seq_len_q = self.speculative_num_draft_tokens\n             metadata.max_seq_len_k = (\n-                forward_batch.seq_lens_cpu.max().item() + draft_token_num\n+                forward_batch.seq_lens_cpu.max().item()\n+                + self.speculative_num_draft_tokens\n             )\n             metadata.cu_seqlens_q = torch.arange(\n                 0,\n-                batch_size * draft_token_num + 1,\n-                draft_token_num,\n+                batch_size * self.speculative_num_draft_tokens + 1,\n+                self.speculative_num_draft_tokens,\n                 dtype=torch.int32,\n                 device=device,\n             )\n@@ -387,31 +368,27 @@ class FlashAttentionBackend(AttentionBackend):\n             ]\n \n         elif forward_batch.forward_mode.is_extend_or_draft_extend():\n-            # Normal or Draft Extend (Both of them will be ran on the Target Worker)\n             metadata.cache_seqlens_int32 = seqlens_in_batch.to(torch.int32)\n+            metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n             metadata.cu_seqlens_k = torch.nn.functional.pad(\n                 torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0)\n             )\n-            # Precompute maximum sequence length\n-            metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()\n-            # Precompute page table\n             metadata.page_table = forward_batch.req_to_token_pool.req_to_token[\n                 forward_batch.req_pool_indices, : metadata.max_seq_len_k\n             ]\n \n-            # Precompute cumulative sequence lengths\n             if (\n                 any(forward_batch.extend_prefix_lens_cpu)\n                 or forward_batch.forward_mode == ForwardMode.DRAFT_EXTEND\n             ):\n                 extend_seq_lens = forward_batch.extend_seq_lens\n+                metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)\n                 metadata.cu_seqlens_q = torch.nn.functional.pad(\n                     torch.cumsum(extend_seq_lens, dim=0, dtype=torch.int32), (1, 0)\n                 )\n-                metadata.max_seq_len_q = max(forward_batch.extend_seq_lens_cpu)\n             else:\n-                metadata.cu_seqlens_q = metadata.cu_seqlens_k\n                 metadata.max_seq_len_q = metadata.max_seq_len_k\n+                metadata.cu_seqlens_q = metadata.cu_seqlens_k\n \n             # Setup local attention if enabled\n             if (\n@@ -458,7 +435,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 )\n                 metadata.local_attn_metadata = local_metadata\n \n-        # Precompute strided indices\n+        # Convert the page table to a strided format which is needed by FA3 API\n         if self.page_size > 1:\n             self.strided_indices = torch.arange(\n                 0, metadata.page_table.shape[1], self.page_size, device=self.device\n@@ -498,7 +475,7 @@ class FlashAttentionBackend(AttentionBackend):\n                         v,\n                     )\n \n-        # Use precomputed metadata\n+        # Use precomputed metadata across all layers\n         metadata = self.forward_metadata\n \n         # Calculate window size (can be moved to metadata if layer properties don't change)\n@@ -606,8 +583,6 @@ class FlashAttentionBackend(AttentionBackend):\n         forward_batch: ForwardBatch,\n         save_kv_cache=True,\n     ) -> torch.Tensor:\n-        \"\"\"Forward pass with FlashAttention using precomputed metadata.\"\"\"\n-        # Save KV cache if needed\n         if k is not None:\n             assert v is not None\n             if save_kv_cache:\n@@ -628,7 +603,7 @@ class FlashAttentionBackend(AttentionBackend):\n                         v,\n                     )\n \n-        # Use precomputed metadata\n+        # Use precomputed metadata across all layers\n         metadata = self.forward_metadata\n \n         # Calculate window size (can be moved to metadata if layer properties don't change)\n@@ -639,12 +614,9 @@ class FlashAttentionBackend(AttentionBackend):\n             if layer.sliding_window_size is not None\n             else (-1, -1)\n         )\n-        page_table = metadata.page_table\n \n         if not self.use_mla:\n             # Do multi-head attention\n-\n-            # Get KV cache\n             kv_cache = forward_batch.token_to_kv_pool.get_kv_buffer(layer.layer_id)\n             key_cache, value_cache = kv_cache[0], kv_cache[1]\n             key_cache = key_cache.view(\n@@ -654,13 +626,12 @@ class FlashAttentionBackend(AttentionBackend):\n                 -1, self.page_size, layer.tp_v_head_num, layer.head_dim\n             )\n \n-            # Pre-reshape query tensor\n             q_reshaped = q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim)\n             o = flash_attn_with_kvcache(\n                 q=q_reshaped,\n                 k_cache=key_cache,\n                 v_cache=value_cache,\n-                page_table=page_table,\n+                page_table=metadata.page_table,\n                 cache_seqlens=metadata.cache_seqlens_int32,\n                 cu_seqlens_q=metadata.cu_seqlens_q,\n                 cu_seqlens_k_new=metadata.cu_seqlens_k,\n@@ -696,7 +667,7 @@ class FlashAttentionBackend(AttentionBackend):\n                 k_cache=k_rope_cache,\n                 v_cache=c_kv_cache,\n                 qv=q_nope,\n-                page_table=page_table,\n+                page_table=metadata.page_table,\n                 cache_seqlens=metadata.cache_seqlens_int32,\n                 cu_seqlens_q=metadata.cu_seqlens_q,\n                 cu_seqlens_k_new=metadata.cu_seqlens_k,\n@@ -719,7 +690,13 @@ class FlashAttentionBackend(AttentionBackend):\n         to avoid memory allocations.\n         \"\"\"\n         self.decode_cuda_graph_metadata = {\n-            # Page table for token mapping (batch_size, max_context_len)\n+            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n+            \"cu_seqlens_q\": torch.arange(\n+                0, max_bs + 1, dtype=torch.int32, device=self.device\n+            ),\n+            \"cu_seqlens_k\": torch.zeros(\n+                max_bs + 1, dtype=torch.int32, device=self.device\n+            ),\n             \"page_table\": torch.zeros(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n@@ -735,30 +712,22 @@ class FlashAttentionBackend(AttentionBackend):\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n+        }\n+\n+        self.target_verify_metadata = {\n             \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.arange(\n-                0, max_bs + 128, dtype=torch.int32, device=self.device\n+            \"cu_seqlens_q\": torch.zeros(\n+                max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n             \"cu_seqlens_k\": torch.zeros(\n-                max_bs + 128, dtype=torch.int32, device=self.device\n+                max_bs + 1, dtype=torch.int32, device=self.device\n             ),\n-        }\n-\n-        self.target_verify_metadata = {\n             \"page_table\": torch.zeros(\n                 max_bs,\n                 (self.max_context_len + self.page_size - 1) // self.page_size,\n                 dtype=torch.int32,\n                 device=self.device,\n             ),\n-            \"cache_seqlens\": torch.zeros(max_bs, dtype=torch.int32, device=self.device),\n-            \"cu_seqlens_q\": torch.zeros(\n-                max_bs + 128, dtype=torch.int32, device=self.device\n-            ),\n-            \"cu_seqlens_k\": torch.zeros(\n-                max_bs + 128, dtype=torch.int32, device=self.device\n-            ),\n-            \"max_seqlen_q\": 0,\n             \"strided_indices\": torch.arange(\n                 0, self.max_context_len, self.page_size, device=self.device\n             ),\n@@ -780,24 +749,21 @@ class FlashAttentionBackend(AttentionBackend):\n         if forward_mode.is_decode():\n             if spec_info is not None:\n                 # Draft Decode\n-                metadata.cu_seqlens_q = torch.arange(\n-                    0, bs + 1, dtype=torch.int32, device=device\n-                )\n                 metadata.cache_seqlens_int32 = self.decode_cuda_graph_metadata[\n                     \"cache_seqlens\"\n                 ][:bs]\n-\n+                metadata.max_seq_len_k = seq_lens.max().item() + (\n+                    self.speculative_step_id + 1\n+                )\n                 metadata.cu_seqlens_q = self.decode_cuda_graph_metadata[\"cu_seqlens_q\"][\n                     : bs + 1\n                 ]\n-\n                 metadata.cu_seqlens_k = torch.nn.functional.pad(\n                     torch.cumsum(\n                         metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n                     ),\n                     (1, 0),\n                 )\n-                metadata.max_seq_len_k = seq_lens.max().item() + (self.step_id + 1)\n                 metadata.page_table = self.decode_cuda_graph_metadata[\n                     \"page_table_draft_decode\"\n                 ][req_pool_indices, :]\n@@ -822,37 +788,30 @@ class FlashAttentionBackend(AttentionBackend):\n                 )\n             self.decode_cuda_graph_metadata[bs] = metadata\n         elif forward_mode.is_target_verify():\n-            draft_token_num = spec_info.draft_token_num\n-\n             metadata.cache_seqlens_int32 = self.target_verify_metadata[\"cache_seqlens\"][\n                 :bs\n             ]\n             metadata.cache_seqlens_int32.copy_(\n-                (seq_lens + draft_token_num).to(torch.int32)\n+                (seq_lens + self.speculative_num_draft_tokens).to(torch.int32)\n             )\n \n-            metadata.max_seq_len_q = draft_token_num\n-            metadata.max_seq_len_k = seq_lens.max().item() + draft_token_num\n+            metadata.max_seq_len_q = self.speculative_num_draft_tokens\n+            metadata.max_seq_len_k = (\n+                seq_lens.max().item() + self.speculative_num_draft_tokens\n+            )\n \n-            metadata.cu_seqlens_q = self.target_verify_metadata[\"cu_seqlens_q\"][\n-                torch.arange(\n-                    0,\n-                    bs * draft_token_num + 1,\n-                    draft_token_num,\n-                    dtype=torch.int32,\n-                    device=device,\n-                )\n-            ]\n-            cu_k = self.target_verify_metadata[\"cu_seqlens_k\"][: (bs + 1)]\n-            cu_k.copy_(\n-                torch.nn.functional.pad(\n-                    torch.cumsum(\n-                        metadata.cache_seqlens_int32, dim=0, dtype=torch.int32\n-                    ),\n-                    (1, 0),\n-                )\n+            metadata.cu_seqlens_q = torch.arange(\n+                0,\n+                bs * self.speculative_num_draft_tokens + 1,\n+                self.speculative_num_draft_tokens,\n+                dtype=torch.int32,\n+                device=device,\n             )\n-            metadata.cu_seqlens_k = cu_k\n+\n+            metadata.cu_seqlens_k = self.target_verify_metadata[\"cu_seqlens_k\"][\n+                : (bs + 1)\n+            ]\n+\n             metadata.page_table = self.target_verify_metadata[\"page_table\"][\n                 req_pool_indices, :\n             ]\n@@ -874,24 +833,21 @@ class FlashAttentionBackend(AttentionBackend):\n         out_cache_loc: torch.Tensor = None,\n     ):\n         # \"\"\"Initialize forward metadata for replaying CUDA graph.\"\"\"\n-        device = seq_lens.device\n         seq_lens = seq_lens[:bs]\n-        req_pool_indices = req_pool_indices[:bs]\n         seq_lens_cpu = seq_lens_cpu[:bs]\n+        req_pool_indices = req_pool_indices[:bs]\n         if forward_mode.is_decode():\n             metadata = self.decode_cuda_graph_metadata[bs]\n \n             if spec_info is not None:\n                 # Draft Decode\n-                max_len = seq_lens_cpu.max().item()\n-                metadata.max_seq_len_k = max_len + (self.step_id + 1)\n-\n                 metadata.cache_seqlens_int32.copy_(\n-                    (seq_lens + (self.step_id + 1)).to(torch.int32)\n+                    (seq_lens + (self.speculative_step_id + 1)).to(torch.int32)\n                 )\n \n-                metadata.max_seq_len_k = seq_lens_cpu.max().item() + (self.step_id + 1)\n-\n+                metadata.max_seq_len_k = seq_lens_cpu.max().item() + (\n+                    self.speculative_step_id + 1\n+                )\n                 metadata.cu_seqlens_k.copy_(\n                     torch.nn.functional.pad(\n                         torch.cumsum(\n@@ -929,22 +885,13 @@ class FlashAttentionBackend(AttentionBackend):\n \n         elif forward_mode.is_target_verify():\n             metadata = self.target_verify_metadata[bs]\n-            draft_token_num = spec_info.draft_token_num\n-\n-            metadata.cu_seqlens_q.copy_(\n-                torch.arange(\n-                    0,\n-                    bs * draft_token_num + 1,\n-                    draft_token_num,\n-                    dtype=torch.int32,\n-                    device=device,\n-                )\n-            )\n             metadata.cache_seqlens_int32.copy_(\n-                (seq_lens + draft_token_num).to(torch.int32)\n+                (seq_lens + self.speculative_num_draft_tokens).to(torch.int32)\n             )\n \n-            metadata.max_seq_len_k = seq_lens_cpu.max().item() + draft_token_num\n+            metadata.max_seq_len_k = (\n+                seq_lens_cpu.max().item() + self.speculative_num_draft_tokens\n+            )\n             metadata.cu_seqlens_k.copy_(\n                 torch.nn.functional.pad(\n                     torch.cumsum(\n@@ -972,14 +919,19 @@ class FlashAttentionMultiStepBackend:\n         self.topk = topk\n         self.speculative_num_steps = speculative_num_steps\n \n+        # TODO: Support Topk > 1 for FlashAttentionBackend Spec Decoding\n+        assert (\n+            self.topk == 1\n+        ), \"speculative_eagle_topk must be 1 for FlashAttentionMultiStepBackend\"\n+\n         self.attn_backends = []\n         for i in range(self.speculative_num_steps):\n             self.attn_backends.append(\n                 FlashAttentionBackend(\n                     model_runner,\n+                    speculative_step_id=i,\n                     topk=self.topk,\n                     speculative_num_steps=self.speculative_num_steps,\n-                    step_id=i,\n                 )\n             )",
  "apis": [
    "FlashAttentionBackend.__init__",
    "FlashAttentionBackend.init_forward_metadata",
    "FlashAttentionMultiStepBackend.__init__"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/attention/flashattention_backend.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/sgl-kernel/python/sgl_kernel/flash_attn.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit makes non-trivial changes to a core source code file (a backend for FlashAttention) by refactoring how metadata, caching, and cumulative sequence lengths are computed and reused. The changes aim to reduce redundant work and improve the efficiency of internal APIs that contribute to high-level attention computations. Although the commit message says \u201cRefactor and Optimize FA3 Code\u201d, the changes are in a performance\u2010critical section of the code and are not merely cosmetic or a simple refactor. They affect CPU-side operations and the performance of existing APIs rather than just bug fixes or new features.",
  "llm_api_reason": "The commit refactors the FlashAttention backend code by reordering import statements, modifying comments and docstrings, and updating parameters in constructors and metadata initialization. Notably, it removes the \u201cstep_id\u201d parameter in the FlashAttentionBackend constructor in favor of \u201cspeculative_step_id\u201d, and it adjusts how the forward metadata is computed (e.g. using \u201cspeculative_num_draft_tokens\u201d and \u201cspeculative_step_id\u201d instead of hardcoded values). Similar changes include an assertion change in the FlashAttentionMultiStepBackend constructor. Overall, the changes affect the initialization and metadata setup routines for the FlashAttention backend."
}