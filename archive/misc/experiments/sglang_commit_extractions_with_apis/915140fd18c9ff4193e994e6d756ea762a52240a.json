{
  "commit_hash": "915140fd18c9ff4193e994e6d756ea762a52240a",
  "pr_url": "https://github.com/sgl-project/sglang/pull/8552",
  "pr_date": "2025-08-04",
  "timeline_text": "Copy link Collaborator azhurkevich commented Jul 30, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation Bring best low latency NVFP4 kernels for Blackwell MoE. Currently enabling DSR1. Modifications Changing some weight preprocessing logic as well as exposing these kernels. Plus various piping to make it work. Accuracy Test Ran accuracy tests, find description and repro below. Benchmark & Profiling Added below with repros. Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 5 zhyncs, Alcanderian, minseokl, pavanimajety, and yuan-luo reacted with thumbs up emoji \ud83d\ude80 4 zhyncs, Alcanderian, pavanimajety, and djsaunde reacted with rocket emoji All reactions \ud83d\udc4d 5 reactions \ud83d\ude80 4 reactions Copy link Contributor gemini-code-assist bot commented Jul 30, 2025 Warning You have reached your daily quota limit. Please wait up to 24 hours and I will start processing your requests again! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . azhurkevich mentioned this pull request Jul 30, 2025 [Feature] Integrating FlashInfer FP4/FP8 Low-Latency MoE Kernels for DSR1 #8037 Closed 2 tasks fzyzcjy reviewed Jul 30, 2025 View reviewed changes python/sglang/srt/models/deepseek_v2.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . nekorobov reviewed Jul 30, 2025 View reviewed changes python/sglang/srt/models/deepseek_v2.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/quantization/modelopt_quant.py # Additional parameter needed for TRT-LLM layer.g1_scale_c = Parameter( (layer.w2_input_scale_quant * layer.g1_alphas).to(torch.float32), Copy link nekorobov Jul 30, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment FC1 is nvfp4 x nvfp4 -> nvfp4 , then the scaleC factor for FC1 is dequantA * dequantB * quantC . I am not sure what is g1_alphas here Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link nekorobov Jul 30, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment g1_alphas is used for scaleGated . ScaleGate should be dequantA * dequantB . Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link nekorobov Jul 30, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment scaleC for FC2 must be dequantA * dequantB as it takes nvfp4 as inputs and outputs bf16 . Just checking if the logic is as expected Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Jul 31, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Seems to work, just ran some evals quickly flashinfer trtllmgen moe: python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1-FP4 --trust-remote-code --tp-size 4 --quantization modelopt_fp4 --enable-flashinfer-trtllm-moe --disable-shared-experts-fusion &\necho $! > sglang_server.pid\necho \"Server PID: $(cat sglang_server.pid)\"\n\n\nEvals:\npython3 benchmark/gsm8k/bench_sglang.py \\\n  --num-questions 900 \\\n  --parallel 32 \\\n  --num-shots 8  \n  \n  \nAccuracy: 0.963\nInvalid: 0.000\nLatency: 512.660 s\nOutput throughput: 178.106 token/s  \n  \n  \nKill process:\nkill $(cat sglang_server.pid) baseline, flashinfer cutlass moe (disabled CUDA graph as it was crashing at high num question counts with seg fault): python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1-FP4 --trust-remote-code --tp-size 4 --quantization modelopt_fp4 --enable-flashinfer-cutlass-moe --disable-shared-experts-fusion  --disable-cuda-graph &\necho $! > sglang_server.pid\necho \"Server PID: $(cat sglang_server.pid)\"\n\n\nEvals:\npython3 benchmark/gsm8k/bench_sglang.py \\\n  --num-questions 900 \\\n  --parallel 32 \\\n  --num-shots 8\n\n\nAccuracy: 0.961\nInvalid: 0.000\nLatency: 772.978 s\nOutput throughput: 117.472 token/s\n\n\nKill process:\nkill $(cat sglang_server.pid) All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs assigned Alcanderian Jul 31, 2025 zhyncs added\n  the high priority label Jul 31, 2025 zhyncs assigned kushanam and zhyncs Aug 1, 2025 Copy link Member zhyncs commented Aug 1, 2025 @azhurkevich @kushanam May you help rebase this? Thanks! \ud83d\udc4d 1 azhurkevich reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 1, 2025 Yeah, I'll rebase it. Working on perf now \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . zhyncs added\n  the collaboration label Aug 1, 2025 zhyncs assigned azhurkevich and fzyzcjy Aug 1, 2025 zhyncs reviewed Aug 1, 2025 View reviewed changes python/sglang/srt/layers/moe/fused_moe_triton/layer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Commands used to launch and bench server. For repro steps. Default backend python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1-FP4 --trust-remote-code --tp-size 4 --quantization modelopt_fp4 CUTLASS fused_moe flashinfer backend: python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1-FP4 --trust-remote-code --tp-size 4 --quantization modelopt_fp4 --enable-flashinfer-cutlass-moe trtllmgen backend: python3 -m sglang.launch_server --model-path /dev/shm/DeepSeek-R1-FP4 --trust-remote-code --tp-size 4 --quantization modelopt_fp4 --enable-flashinfer-trtllm-moe --disable-shared-experts-fusion Universal command to run benchmarking: curl http://127.0.0.1:30000/flush_cache && \\\npython3 -m sglang.bench_serving --backend sglang-oai \\\n    --dataset-name random --random-input-len 1000 --random-output-len 1000 \\\n    --random-range-ratio 1 --num-prompts 5 --max-concurrency 1 \\\n    --warmup-requests 5 --output-file dsv3_con1.jsonl && \\\ncurl http://127.0.0.1:30000/flush_cache && \\\npython3 -m sglang.bench_serving --backend sglang-oai \\\n    --dataset-name random --random-input-len 1000 --random-output-len 1000 \\\n    --random-range-ratio 1 --num-prompts 20 --max-concurrency 4 \\\n    --warmup-requests 5 --output-file dsv3_con4.jsonl && \\\ncurl http://127.0.0.1:30000/flush_cache && \\\npython3 -m sglang.bench_serving --backend sglang-oai \\\n    --dataset-name random --random-input-len 1000 --random-output-len 1000 \\\n    --random-range-ratio 1 --num-prompts 80 --max-concurrency 16 \\\n    --warmup-requests 5 --output-file dsv3_con16.jsonl && \\\ncurl http://127.0.0.1:30000/flush_cache && \\\npython3 -m sglang.bench_serving --backend sglang-oai \\\n    --dataset-name random --random-input-len 1000 --random-output-len 1000 \\\n    --random-range-ratio 1 --num-prompts 160 --max-concurrency 32 \\\n    --warmup-requests 5 --output-file dsv3_con32.jsonl \ud83d\udc4d 2 pavanimajety and aleozlx reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 Default backend perf: ============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 1         \nSuccessful requests:                     5         \nBenchmark duration (s):                  83.35     \nTotal input tokens:                      5000      \nTotal generated tokens:                  5000      \nTotal generated tokens (retokenized):    4995      \nRequest throughput (req/s):              0.06      \nInput token throughput (tok/s):          59.99     \nOutput token throughput (tok/s):         59.99     \nTotal token throughput (tok/s):          119.98    \nConcurrency:                             1.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   16664.90  \nMedian E2E Latency (ms):                 16593.61  \n---------------Time to First Token----------------\nMean TTFT (ms):                          213.34    \nMedian TTFT (ms):                        142.98    \nP99 TTFT (ms):                           522.44    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           16.47     \nMedian ITL (ms):                         16.47     \nP95 ITL (ms):                            16.63     \nP99 ITL (ms):                            16.69     \nMax ITL (ms):                            49.54     \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 4         \nSuccessful requests:                     20        \nBenchmark duration (s):                  102.19    \nTotal input tokens:                      20000     \nTotal generated tokens:                  20000     \nTotal generated tokens (retokenized):    19938     \nRequest throughput (req/s):              0.20      \nInput token throughput (tok/s):          195.72    \nOutput token throughput (tok/s):         195.72    \nTotal token throughput (tok/s):          391.44    \nConcurrency:                             4.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   20431.53  \nMedian E2E Latency (ms):                 20579.65  \n---------------Time to First Token----------------\nMean TTFT (ms):                          594.20    \nMedian TTFT (ms):                        753.91    \nP99 TTFT (ms):                           824.97    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           19.91     \nMedian ITL (ms):                         19.86     \nP95 ITL (ms):                            20.04     \nP99 ITL (ms):                            20.11     \nMax ITL (ms):                            656.44    \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 16        \nSuccessful requests:                     80        \nBenchmark duration (s):                  153.64    \nTotal input tokens:                      80000     \nTotal generated tokens:                  80000     \nTotal generated tokens (retokenized):    79824     \nRequest throughput (req/s):              0.52      \nInput token throughput (tok/s):          520.69    \nOutput token throughput (tok/s):         520.69    \nTotal token throughput (tok/s):          1041.37   \nConcurrency:                             16.00     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   30720.48  \nMedian E2E Latency (ms):                 30798.60  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1798.36   \nMedian TTFT (ms):                        1758.86   \nP99 TTFT (ms):                           2208.70   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           29.01     \nMedian ITL (ms):                         28.78     \nP95 ITL (ms):                            29.07     \nP99 ITL (ms):                            29.20     \nMax ITL (ms):                            1422.57   \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 32        \nSuccessful requests:                     160       \nBenchmark duration (s):                  204.60    \nTotal input tokens:                      160000    \nTotal generated tokens:                  160000    \nTotal generated tokens (retokenized):    159645    \nRequest throughput (req/s):              0.78      \nInput token throughput (tok/s):          782.00    \nOutput token throughput (tok/s):         782.00    \nTotal token throughput (tok/s):          1564.00   \nConcurrency:                             31.99     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   40911.53  \nMedian E2E Latency (ms):                 40983.37  \n---------------Time to First Token----------------\nMean TTFT (ms):                          3249.13   \nMedian TTFT (ms):                        3410.76   \nP99 TTFT (ms):                           4278.45   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           37.76     \nMedian ITL (ms):                         37.38     \nP95 ITL (ms):                            37.79     \nP99 ITL (ms):                            37.91     \nMax ITL (ms):                            3203.13   \n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . CUTLASS fused_moe perf: ============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 1         \nSuccessful requests:                     5         \nBenchmark duration (s):                  64.10     \nTotal input tokens:                      5000      \nTotal generated tokens:                  5000      \nTotal generated tokens (retokenized):    4986      \nRequest throughput (req/s):              0.08      \nInput token throughput (tok/s):          78.00     \nOutput token throughput (tok/s):         78.00     \nTotal token throughput (tok/s):          155.99    \nConcurrency:                             1.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   12817.68  \nMedian E2E Latency (ms):                 12740.47  \n---------------Time to First Token----------------\nMean TTFT (ms):                          167.36    \nMedian TTFT (ms):                        90.98     \nP99 TTFT (ms):                           464.31    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           12.69     \nMedian ITL (ms):                         12.65     \nP95 ITL (ms):                            12.84     \nP99 ITL (ms):                            12.89     \nMax ITL (ms):                            38.32     \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 4         \nSuccessful requests:                     20        \nBenchmark duration (s):                  82.62     \nTotal input tokens:                      20000     \nTotal generated tokens:                  20000     \nTotal generated tokens (retokenized):    19969     \nRequest throughput (req/s):              0.24      \nInput token throughput (tok/s):          242.07    \nOutput token throughput (tok/s):         242.07    \nTotal token throughput (tok/s):          484.14    \nConcurrency:                             4.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   16520.44  \nMedian E2E Latency (ms):                 16581.57  \n---------------Time to First Token----------------\nMean TTFT (ms):                          496.05    \nMedian TTFT (ms):                        599.79    \nP99 TTFT (ms):                           636.77    \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           16.06     \nMedian ITL (ms):                         16.04     \nP95 ITL (ms):                            16.21     \nP99 ITL (ms):                            16.29     \nMax ITL (ms):                            507.20    \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 16        \nSuccessful requests:                     80        \nBenchmark duration (s):                  130.25    \nTotal input tokens:                      80000     \nTotal generated tokens:                  80000     \nTotal generated tokens (retokenized):    79785     \nRequest throughput (req/s):              0.61      \nInput token throughput (tok/s):          614.21    \nOutput token throughput (tok/s):         614.21    \nTotal token throughput (tok/s):          1228.41   \nConcurrency:                             16.00     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   26043.24  \nMedian E2E Latency (ms):                 26008.36  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1070.99   \nMedian TTFT (ms):                        1035.21   \nP99 TTFT (ms):                           1389.24   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           25.05     \nMedian ITL (ms):                         24.95     \nP95 ITL (ms):                            25.24     \nP99 ITL (ms):                            25.40     \nMax ITL (ms):                            827.10    \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 32        \nSuccessful requests:                     160       \nBenchmark duration (s):                  176.18    \nTotal input tokens:                      160000    \nTotal generated tokens:                  160000    \nTotal generated tokens (retokenized):    159616    \nRequest throughput (req/s):              0.91      \nInput token throughput (tok/s):          908.14    \nOutput token throughput (tok/s):         908.14    \nTotal token throughput (tok/s):          1816.28   \nConcurrency:                             31.99     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   35226.74  \nMedian E2E Latency (ms):                 35390.35  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1930.92   \nMedian TTFT (ms):                        2012.45   \nP99 TTFT (ms):                           2723.38   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           33.41     \nMedian ITL (ms):                         33.23     \nP95 ITL (ms):                            33.66     \nP99 ITL (ms):                            33.86     \nMax ITL (ms):                            1819.86   \n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 trtllmgen perf: ============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 1         \nSuccessful requests:                     5         \nBenchmark duration (s):                  44.20     \nTotal input tokens:                      5000      \nTotal generated tokens:                  5000      \nTotal generated tokens (retokenized):    4997      \nRequest throughput (req/s):              0.11      \nInput token throughput (tok/s):          113.12    \nOutput token throughput (tok/s):         113.12    \nTotal token throughput (tok/s):          226.24    \nConcurrency:                             1.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   8837.28   \nMedian E2E Latency (ms):                 8599.33   \n---------------Time to First Token----------------\nMean TTFT (ms):                          363.58    \nMedian TTFT (ms):                        125.85    \nP99 TTFT (ms):                           1063.32   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           8.49      \nMedian ITL (ms):                         8.47      \nP95 ITL (ms):                            8.66      \nP99 ITL (ms):                            8.70      \nMax ITL (ms):                            25.53     \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 4         \nSuccessful requests:                     20        \nBenchmark duration (s):                  56.80     \nTotal input tokens:                      20000     \nTotal generated tokens:                  20000     \nTotal generated tokens (retokenized):    19941     \nRequest throughput (req/s):              0.35      \nInput token throughput (tok/s):          352.12    \nOutput token throughput (tok/s):         352.12    \nTotal token throughput (tok/s):          704.25    \nConcurrency:                             4.00      \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   11355.31  \nMedian E2E Latency (ms):                 11631.33  \n---------------Time to First Token----------------\nMean TTFT (ms):                          615.56    \nMedian TTFT (ms):                        325.56    \nP99 TTFT (ms):                           1150.78   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           10.78     \nMedian ITL (ms):                         10.64     \nP95 ITL (ms):                            10.80     \nP99 ITL (ms):                            10.87     \nMax ITL (ms):                            925.45    \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 16        \nSuccessful requests:                     80        \nBenchmark duration (s):                  89.17     \nTotal input tokens:                      80000     \nTotal generated tokens:                  80000     \nTotal generated tokens (retokenized):    79758     \nRequest throughput (req/s):              0.90      \nInput token throughput (tok/s):          897.19    \nOutput token throughput (tok/s):         897.19    \nTotal token throughput (tok/s):          1794.37   \nConcurrency:                             15.99     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   17827.39  \nMedian E2E Latency (ms):                 17594.14  \n---------------Time to First Token----------------\nMean TTFT (ms):                          1671.07   \nMedian TTFT (ms):                        1459.20   \nP99 TTFT (ms):                           2193.36   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           16.21     \nMedian ITL (ms):                         16.13     \nP95 ITL (ms):                            16.33     \nP99 ITL (ms):                            16.45     \nMax ITL (ms):                            1014.18   \n==================================================\n\n============ Serving Benchmark Result ============\nBackend:                                 sglang-oai\nTraffic request rate:                    inf       \nMax request concurrency:                 32        \nSuccessful requests:                     160       \nBenchmark duration (s):                  116.25    \nTotal input tokens:                      160000    \nTotal generated tokens:                  160000    \nTotal generated tokens (retokenized):    159463    \nRequest throughput (req/s):              1.38      \nInput token throughput (tok/s):          1376.31   \nOutput token throughput (tok/s):         1376.31   \nTotal token throughput (tok/s):          2752.62   \nConcurrency:                             31.99     \n----------------End-to-End Latency----------------\nMean E2E Latency (ms):                   23241.97  \nMedian E2E Latency (ms):                 23089.07  \n---------------Time to First Token----------------\nMean TTFT (ms):                          2501.09   \nMedian TTFT (ms):                        2370.11   \nP99 TTFT (ms):                           3471.56   \n---------------Inter-Token Latency----------------\nMean ITL (ms):                           20.82     \nMedian ITL (ms):                         20.65     \nP95 ITL (ms):                            20.91     \nP99 ITL (ms):                            21.08     \nMax ITL (ms):                            1614.73   \n================================================== All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 trtllmgen vs. default backend All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 3, 2025 trtllmgen vs. CUTLASS fused_moe \ud83d\udc4d 3 yzh119, zhyncs, and pavanimajety reacted with thumbs up emoji All reactions \ud83d\udc4d 3 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan self-assigned this Aug 4, 2025 Copy link Collaborator ch-wan commented Aug 4, 2025 @azhurkevich Let me help with resolving the merge conflict. \ud83d\udc4d 1 zhyncs reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . azhurkevich force-pushed the low_latency_nvfp4_decode branch\n    from 04f3c1f to 0ff4840 Compare August 4, 2025 01:17 Copy link Collaborator Author azhurkevich commented Aug 4, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @ch-wan oof, sorry just noticed your comment. I squashed everything and doing final rebase. Do you want me to do it or you would like to do it? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Member zhyncs commented Aug 4, 2025 @azhurkevich go ahead All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ALEXANDER ZHURKEVICH added 2 commits August 3, 2025 18:51 Squash 3e37343 Rebased bc9fb6c azhurkevich force-pushed the low_latency_nvfp4_decode branch\n    from 0ff4840 to bc9fb6c Compare August 4, 2025 04:31 zhyncs marked this pull request as ready for review August 4, 2025 04:34 3 hidden items Load more\u2026 zhyncs requested review from xiezhq-hermann , ispobock , HaiShaw , ch-wan , BBuf and kushanam as code owners August 4, 2025 04:34 Copy link Member zhyncs commented Aug 4, 2025 pre-commit run --all-files @azhurkevich All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'main' into low_latency_nvfp4_decode 3b74195 ch-wan reviewed Aug 4, 2025 View reviewed changes python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/moe/fused_moe_triton/layer.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/server_args.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/models/deepseek_v2.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/moe/fused_moe_triton/layer.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/quantization/modelopt_quant.py Outdated Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . python/sglang/srt/layers/quantization/modelopt_quant.py Show resolved Hide resolved Uh oh! There was an error while loading. Please reload this page . ch-wan added 3 commits August 4, 2025 00:02 clean up 137ce54 clean up 666250e upd cb91e54 Hide details View details zhyncs merged commit 915140f into main Aug 4, 2025 60 of 67 checks passed Uh oh! There was an error while loading. Please reload this page . zhyncs deleted the low_latency_nvfp4_decode branch August 4, 2025 10:10 trevor-m mentioned this pull request Aug 4, 2025 [bugfix] Fix typo in modelopt quant: 'FusedMoE' object has no attribute 'local_num_experts' #8768 Merged pavanimajety reviewed Aug 4, 2025 View reviewed changes python/sglang/srt/server_args.py Comment on lines +486 to +489 self.disable_shared_experts_fusion = True logger.warning( \"FlashInfer TRTLLM MoE is enabled. --disable-shared-experts-fusion is automatically set.\" ) Copy link Collaborator pavanimajety Aug 4, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This is not required anymore? Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Member zhyncs Aug 4, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Hi @trevor-m can you help check this? Thanks! If so, may you help submit a pr for this @pavanimajety Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions kaixih mentioned this pull request Aug 4, 2025 [NVIDIA] Fix breakage of using trtllm-gen fp8 moe #8773 Merged wenscarl mentioned this pull request Aug 5, 2025 [NVIDIA]Fix local_num_experts for EP #8779 Merged 6 tasks Copy link Contributor yuan-luo commented Aug 5, 2025 What flashinfer_python version does it require? I tried 0.2.3 to 0.2.7.post1, it is lack of RoutingMethodType which is newly added in this PR. [2025-08-05 09:13:32 TP0] Scheduler hit an exception: Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 2534, in run_scheduler_process\n    scheduler = Scheduler(\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/scheduler.py\", line 313, in __init__\n    self.tp_worker = TpWorkerClass(\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/tp_worker_overlap_thread.py\", line 67, in __init__\n    self.worker = TpModelWorker(\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/managers/tp_worker.py\", line 84, in __init__\n    self.model_runner = ModelRunner(\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 242, in __init__\n    self.initialize(min_per_gpu_memory)\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 285, in initialize\n    self.load_model()\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/model_executor/model_runner.py\", line 640, in load_model\n    monkey_patch_isinstance_for_vllm_base_layer()\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/layers/quantization/__init__.py\", line 163, in monkey_patch_isinstance_for_vllm_base_layer\n    from sglang.srt.layers.moe.fused_moe_triton.layer import FusedMoE as PatchedFusedMoE\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/__init__.py\", line 10, in <module>\n    from sglang.srt.layers.moe.fused_moe_triton.layer import (\n  File \"/opt/conda/lib/python3.10/site-packages/sglang/srt/layers/moe/fused_moe_triton/layer.py\", line 44, in <module>\n    from flashinfer import (\nImportError: cannot import name 'RoutingMethodType' from 'flashinfer' (/opt/conda/lib/python3.10/site-packages/flashinfer/__init__.py)\n\n[2025-08-05 09:13:32] Received sigquit from a chil All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author azhurkevich commented Aug 5, 2025 @yuan-luo you at least gotta use flashinfer version v0.2.9rc1. These kernels didnt exist in flashinfer before that version came out All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor yuan-luo commented Aug 5, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @yuan-luo you at least gotta use flashinfer version v0.2.9rc1. These kernels didnt exist in flashinfer before that version came out Is it possible to make this feature optional if we are not using fp4? I guess it may impact lots of users, for example the local flashinfer repo mirror in my environment, the highest flashinfer version is v0.2.7.post1. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . htiennv pushed a commit\n        to htiennv/sglang\n      that referenced\n      this pull request Aug 5, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( sgl-pr\u2026 \u2026 21c305b \u2026oject#8552 )\n\nCo-authored-by: Cheng Wan <cwan@x.ai> ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( #8552 ) \u2026 39f91eb Co-authored-by: Cheng Wan <cwan@x.ai> ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( #8552 ) \u2026 4278118 Co-authored-by: Cheng Wan <cwan@x.ai> pavanimajety mentioned this pull request Aug 5, 2025 [NVIDIA] Fix num_experts in modelopt_quant #8811 Merged 6 tasks aleozlx mentioned this pull request Aug 13, 2025 Faster weight processing (trtllm-gen moe nvfp4) #9162 Merged 4 tasks narutolhy pushed a commit\n        to narutolhy/sglang\n      that referenced\n      this pull request Aug 17, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( sgl-pr\u2026 \u2026 39746b6 \u2026oject#8552 )\n\nCo-authored-by: Cheng Wan <cwan@x.ai> narutolhy pushed a commit\n        to narutolhy/sglang\n      that referenced\n      this pull request Aug 18, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( sgl-pr\u2026 \u2026 37789e3 \u2026oject#8552 )\n\nCo-authored-by: Cheng Wan <cwan@x.ai> fzyzcjy mentioned this pull request Aug 23, 2025 Reintroduce memory usage fix #9535 Merged 4 tasks MahmoudAshraf97 pushed a commit\n        to MahmoudAshraf97/sglang\n      that referenced\n      this pull request Sep 8, 2025 [NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer ( sgl-pr\u2026 \u2026 6099610 \u2026oject#8552 )\n\nCo-authored-by: Cheng Wan <cwan@x.ai> Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:00",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-R1"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3 -m sglang.bench_serving --backend sglang-oai --dataset-name random --random-input-len 1000 --random-output-len 1000 --random-range-ratio 1 --num-prompts 5 --max-concurrency 1 --warmup-requests 5 --output-file dsv3_con1.jsonl",
  "commit_subject": "[NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer (#8552)",
  "commit_message": "[NVIDIA] Add Low Latency NVFP4 decode kernels from Flashinfer (#8552)\n\nCo-authored-by: Cheng Wan <cwan@x.ai>",
  "commit_date": "2025-08-04T03:10:02-07:00",
  "files_changed": [
    "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "python/sglang/srt/layers/moe/utils.py",
    "python/sglang/srt/layers/quantization/modelopt_quant.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/models/deepseek_v2.py",
    "python/sglang/srt/models/glm4_moe.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 617,
    "num_files": 8,
    "num_hunks": 34,
    "num_non_test_edited_lines": 617,
    "num_non_test_files": 8,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex 66fbb36ea..ac5371871 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -14,13 +14,9 @@ from sglang.srt.layers.moe.ep_moe.kernels import (\n     silu_and_mul_masked_post_quant_fwd,\n     tma_align_input_scale,\n )\n-from sglang.srt.layers.moe.fused_moe_triton.layer import (\n-    FlashInferFusedMoE,\n-    FusedMoE,\n-    should_use_flashinfer_trtllm_moe,\n-)\n+from sglang.srt.layers.moe.fused_moe_triton.layer import FlashInferFusedMoE, FusedMoE\n from sglang.srt.layers.moe.topk import TopKOutput\n-from sglang.srt.layers.moe.utils import DeepEPMode\n+from sglang.srt.layers.moe.utils import DeepEPMode, should_use_flashinfer_trtllm_moe\n from sglang.srt.layers.quantization import deep_gemm_wrapper\n from sglang.srt.layers.quantization.base_config import QuantizationConfig\n from sglang.srt.layers.quantization.fp8 import (\n@@ -48,7 +44,6 @@ _is_npu = is_npu()\n _is_fp8_fnuz = is_fp8_fnuz()\n _use_aiter = get_bool_env_var(\"SGLANG_USE_AITER\") and _is_hip\n \n-\n if not (_is_npu or _is_hip):\n     from sgl_kernel import silu_and_mul\n \n@@ -741,6 +736,22 @@ class FlashInferEPMoE(EPMoE):\n def get_moe_impl_class():\n     if global_server_args_dict[\"moe_a2a_backend\"].is_deepep():\n         return DeepEPMoE\n+\n+    # NEW: Direct FP4 detection (bypasses EP requirements)\n+    # Check for FP4 quantization with TRTLLM flag, regardless of EP\n+    if global_server_args_dict.get(\"enable_flashinfer_trtllm_moe\", False):\n+        try:\n+            # Check the quantization argument directly\n+            quantization = global_server_args_dict.get(\"quantization\")\n+            if quantization == \"modelopt_fp4\":\n+                from sglang.srt.layers.moe.fused_moe_triton.layer import (\n+                    FlashInferFP4MoE,\n+                )\n+\n+                return FlashInferFP4MoE\n+        except:\n+            pass\n+\n     if global_server_args_dict[\"enable_flashinfer_cutlass_moe\"]:\n         return FusedMoE\n     if get_moe_expert_parallel_world_size() > 1:\ndiff --git a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\nindex d0a9ed132..c30535d7f 100644\n--- a/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\n+++ b/python/sglang/srt/layers/moe/fused_moe_triton/layer.py\n@@ -1,13 +1,14 @@\n # Adapted from https://github.com/vllm-project/vllm/blob/a6221a144af772fd1a68fe7e627935dc53e81738/vllm/model_executor/layers/fused_moe/layer.py\n \n-import importlib.util\n+import datetime\n+import glob\n import logging\n+import os\n+import sys\n from enum import Enum\n-from functools import lru_cache\n from typing import List, Optional, Tuple\n \n import torch\n-from packaging import version as pkg_version\n \n from sglang.srt.distributed import (\n     get_moe_expert_parallel_rank,\n@@ -22,6 +23,7 @@ from sglang.srt.distributed.device_communicators.pynccl_allocator import (\n )\n from sglang.srt.eplb.expert_location import get_global_expert_location_metadata\n from sglang.srt.layers.moe.topk import StandardTopKOutput\n+from sglang.srt.layers.moe.utils import should_use_flashinfer_trtllm_moe\n from sglang.srt.layers.quantization.base_config import (\n     QuantizationConfig,\n     QuantizeMethodBase,\n@@ -29,22 +31,58 @@ from sglang.srt.layers.quantization.base_config import (\n from sglang.srt.layers.quantization.unquant import UnquantizedFusedMoEMethod\n from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.model_loader.weight_utils import narrow_padded_param_and_loaded_weight\n-from sglang.srt.utils import cpu_has_amx_support, get_bool_env_var, is_cpu, is_hip\n+from sglang.srt.utils import (\n+    cpu_has_amx_support,\n+    get_bool_env_var,\n+    is_cpu,\n+    is_flashinfer_available,\n+    is_hip,\n+    next_power_of_2,\n+)\n+\n+if is_flashinfer_available():\n+    from flashinfer import (\n+        RoutingMethodType,\n+        fp4_quantize,\n+        reorder_rows_for_gated_act_gemm,\n+        shuffle_matrix_a,\n+        shuffle_matrix_sf_a,\n+    )\n \n _is_hip = is_hip()\n _is_cpu_amx_available = cpu_has_amx_support()\n _is_cpu = is_cpu()\n \n+\n+# Try to import FP4 TRTLLM function if flashinfer is available\n+trtllm_fp4_block_scale_moe = None\n+if should_use_flashinfer_trtllm_moe():\n+    try:\n+        from flashinfer.fused_moe import trtllm_fp4_block_scale_moe\n+    except ImportError:\n+        trtllm_fp4_block_scale_moe = None\n+\n logger = logging.getLogger(__name__)\n \n \n-@lru_cache(maxsize=1)\n-def should_use_flashinfer_trtllm_moe():\n-    return global_server_args_dict[\"enable_flashinfer_trtllm_moe\"] and (\n-        not importlib.util.find_spec(\"flashinfer\")\n-        or pkg_version.parse(__import__(\"flashinfer\").__version__)\n-        >= pkg_version.parse(\"0.2.9rc1\")\n-    )\n+def _is_fp4_quantization_enabled():\n+    \"\"\"Check if ModelOpt FP4 quantization is enabled.\"\"\"\n+    try:\n+        # Use the same simple check that works for class selection\n+        quantization = global_server_args_dict.get(\"quantization\")\n+        return quantization == \"modelopt_fp4\"\n+    except:\n+        return False\n+\n+\n+def _get_tile_tokens_dim(num_tokens, top_k, num_experts):\n+    # Guess tokens per expert assuming perfect expert distribution first.\n+    num_tokens_per_expert = (num_tokens * top_k) // num_experts\n+    # And pad the number to the next power of 2.\n+    tile_tokens_dim = next_power_of_2(num_tokens_per_expert)\n+    # Cap to 8-64 tokens per CTA tile as it's the range supported by the kernel.\n+    tile_tokens_dim = min(max(tile_tokens_dim, 8), 64)\n+    return tile_tokens_dim\n \n \n class FusedMoeWeightScaleSupported(Enum):\n@@ -157,10 +195,6 @@ class FusedMoE(torch.nn.Module):\n             )\n         else:\n             self.quant_method = quant_config.get_quant_method(self, prefix)\n-            if self.quant_method.__class__.__name__ == \"ModelOptNvFp4FusedMoEMethod\":\n-                self.quant_method.enable_flashinfer_cutlass_moe = (\n-                    self.enable_flashinfer_cutlass_moe\n-                )\n         assert self.quant_method is not None\n \n         self.quant_config = quant_config\n@@ -747,7 +781,130 @@ class FlashInferFusedMoE(FusedMoE):\n             routed_scaling_factor=self.routed_scaling_factor,\n         )\n \n-        if self.reduce_results and (self.tp_size > 1 or self.ep_size > 1):\n+        if self.reduce_results and (self.moe_tp_size > 1 or self.moe_ep_size > 1):\n             final_hidden_states = tensor_model_parallel_all_reduce(final_hidden_states)\n \n         return final_hidden_states\n+\n+\n+class FlashInferFP4MoE(FusedMoE):\n+    \"\"\"FP4 TRTLLM MoE implementation using FlashInfer.\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        # Extract DeepSeek-specific parameters\n+        renormalize = kwargs.pop(\"renormalize\", True)\n+        num_fused_shared_experts = kwargs.pop(\"num_fused_shared_experts\", 0)\n+        use_grouped_topk = kwargs.pop(\"use_grouped_topk\", False)\n+        num_expert_group = kwargs.pop(\"num_expert_group\", None)\n+        topk_group = kwargs.pop(\"topk_group\", None)\n+        correction_bias = kwargs.pop(\"correction_bias\", None)\n+\n+        # Extract additional TopK parameters that were previously extracted in forward\n+        routed_scaling_factor = kwargs.pop(\"routed_scaling_factor\", None)\n+\n+        super().__init__(*args, **kwargs)\n+\n+        # Store DeepSeek parameters\n+        self.renormalize = renormalize\n+        self.num_fused_shared_experts = num_fused_shared_experts\n+        self.use_grouped_topk = use_grouped_topk\n+        self.num_expert_group = num_expert_group\n+        self.topk_group = topk_group\n+        self.correction_bias = correction_bias\n+        self.routed_scaling_factor = routed_scaling_factor\n+\n+    # ---------------------------------------------------------------------\n+    # Helper: quantize hidden states to FP4 each forward pass\n+    # ---------------------------------------------------------------------\n+    def _quantize_hidden_states_fp4(self, hidden_states: torch.Tensor):\n+        \"\"\"\n+        Quantize hidden states using global scale factor from quantization method.\n+\n+        Global scale factor is set by ModelOptNvFp4FusedMoEMethod during weight loading.\n+        Only block scales are computed at runtime for efficiency.\n+\n+        Returns (packed_fp4_uint8, scale_float8_e4m3fn_runtime, global_scale_float32)\n+        \"\"\"\n+\n+        # flashinfer.fp4_quantize returns (packed_uint8, scale_fp8)\n+        # Only the block scales are computed at runtime\n+        hs_fp4_bytes, hs_sf_bytes = fp4_quantize(\n+            hidden_states,\n+            self.w13_input_scale_quant,\n+            16,  # sf_vec_size\n+            False,  # use_ue8m0\n+            False,  # is_sf_swizzled_layout\n+        )\n+\n+        hs_fp4 = hs_fp4_bytes.reshape(\n+            hidden_states.shape[0], hidden_states.shape[1] // 2\n+        )\n+        hs_sf = hs_sf_bytes.view(torch.float8_e4m3fn).reshape(-1)\n+\n+        return hs_fp4, hs_sf\n+\n+    def forward(self, hidden_states: torch.Tensor, topk_output):\n+        \"\"\"Forward pass using FP4 TRTLLM kernel.\n+\n+        Args:\n+            hidden_states: Input tensor\n+            topk_output: Should be tuple of (TopK_config, router_logits) for TRTLLM mode\n+        \"\"\"\n+\n+        # TRTLLM mode expects (TopK_config, router_logits) tuple\n+        if not isinstance(topk_output, tuple) or len(topk_output) != 2:\n+            raise ValueError(\n+                f\"FlashInferFP4MoE expects (TopK_config, router_logits) tuple, got {type(topk_output)}\"\n+            )\n+\n+        _, router_logits = topk_output\n+\n+        hs_fp4, hs_scale_linear = self._quantize_hidden_states_fp4(hidden_states)\n+\n+        router_logits = router_logits.to(torch.float32)\n+\n+        result = trtllm_fp4_block_scale_moe(\n+            routing_logits=router_logits,\n+            routing_bias=self.correction_bias.to(hidden_states.dtype),\n+            hidden_states=hs_fp4,\n+            hidden_states_scale=hs_scale_linear.view(torch.float8_e4m3fn).flatten(),\n+            gemm1_weights=self.gemm1_weights_fp4_shuffled.data,\n+            gemm1_weights_scale=self.gemm1_scales_fp4_shuffled.data.view(\n+                torch.float8_e4m3fn\n+            ),\n+            gemm2_weights=self.gemm2_weights_fp4_shuffled.data,\n+            gemm2_weights_scale=self.gemm2_scales_fp4_shuffled.data.view(\n+                torch.float8_e4m3fn\n+            ),\n+            output1_scale_scalar=self.g1_scale_c.data,\n+            output1_scale_gate_scalar=self.g1_alphas.data,\n+            output2_scale_scalar=self.g2_alphas.data,\n+            num_experts=self.num_experts,\n+            top_k=self.top_k,\n+            n_group=self.num_expert_group,\n+            topk_group=self.topk_group,\n+            intermediate_size=self.intermediate_size_per_partition,\n+            local_expert_offset=self.moe_ep_rank * self.num_local_experts,\n+            local_num_experts=self.num_local_experts,\n+            routed_scaling_factor=self.routed_scaling_factor,\n+            tile_tokens_dim=_get_tile_tokens_dim(\n+                hidden_states.shape[0], self.top_k, self.num_local_experts\n+            ),\n+            routing_method_type=RoutingMethodType.DeepSeekV3,\n+            do_finalize=True,\n+        )[0]\n+\n+        return result\n+\n+\n+def get_fused_moe_impl_class():\n+    \"\"\"Factory function to get the appropriate FusedMoE implementation class.\"\"\"\n+    if should_use_flashinfer_trtllm_moe() and _is_fp4_quantization_enabled():\n+        # Use FP4 variant when FP4 quantization is enabled\n+        return FlashInferFP4MoE\n+    elif should_use_flashinfer_trtllm_moe():\n+        # Use regular FlashInfer variant for non-FP4 FlashInfer cases\n+        return FlashInferFusedMoE\n+    else:\n+        # Default case\n+        return FusedMoE\ndiff --git a/python/sglang/srt/layers/moe/utils.py b/python/sglang/srt/layers/moe/utils.py\nindex 06b174995..f08b34e40 100644\n--- a/python/sglang/srt/layers/moe/utils.py\n+++ b/python/sglang/srt/layers/moe/utils.py\n@@ -1,4 +1,20 @@\n+import importlib.util\n from enum import Enum\n+from functools import lru_cache\n+\n+from packaging import version as pkg_version\n+\n+from sglang.srt.managers.schedule_batch import global_server_args_dict\n+\n+\n+@lru_cache(maxsize=1)\n+def should_use_flashinfer_trtllm_moe():\n+    result = global_server_args_dict[\"enable_flashinfer_trtllm_moe\"] and (\n+        not importlib.util.find_spec(\"flashinfer\")\n+        or pkg_version.parse(__import__(\"flashinfer\").__version__)\n+        >= pkg_version.parse(\"0.2.9rc1\")\n+    )\n+    return result\n \n \n class MoeA2ABackend(Enum):\ndiff --git a/python/sglang/srt/layers/quantization/modelopt_quant.py b/python/sglang/srt/layers/quantization/modelopt_quant.py\nindex bf7ce8727..7073f6be5 100755\n--- a/python/sglang/srt/layers/quantization/modelopt_quant.py\n+++ b/python/sglang/srt/layers/quantization/modelopt_quant.py\n@@ -1,13 +1,15 @@\n # Adapted from https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/quantization/modelopt.py\n from __future__ import annotations\n \n+import importlib.util\n import logging\n-from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union\n \n import torch\n from torch.nn.parameter import Parameter\n \n from sglang.srt.layers.moe.cutlass_moe_params import CutlassMoEParams, CutlassMoEType\n+from sglang.srt.layers.moe.utils import should_use_flashinfer_trtllm_moe\n from sglang.srt.layers.parameter import ModelWeightParameter, PerTensorScaleParameter\n from sglang.srt.layers.quantization.base_config import (\n     FusedMoEMethodBase,\n@@ -29,6 +31,7 @@ from sglang.srt.layers.quantization.utils import (\n     requantize_with_max_scale,\n )\n from sglang.srt.layers.radix_attention import RadixAttention\n+from sglang.srt.managers.schedule_batch import global_server_args_dict\n from sglang.srt.utils import is_cuda, next_power_of_2\n \n if TYPE_CHECKING:\n@@ -39,6 +42,11 @@ if is_cuda():\n \n try:\n     from flashinfer import mm_fp4 as fp4_gemm\n+    from flashinfer import (\n+        reorder_rows_for_gated_act_gemm,\n+        shuffle_matrix_a,\n+        shuffle_matrix_sf_a,\n+    )\n \n     enable_flashinfer_fp4_gemm = True\n except ImportError:\n@@ -47,6 +55,9 @@ except ImportError:\n     else:\n         fp4_gemm = None\n     enable_flashinfer_fp4_gemm = False\n+    reorder_rows_for_gated_act_gemm = None\n+    shuffle_matrix_a = None\n+    shuffle_matrix_sf_a = None\n \n try:\n     from flashinfer.fused_moe import cutlass_fused_moe as flashinfer_cutlass_fused_moe\n@@ -527,6 +538,7 @@ class ModelOptFp4Config(QuantizationConfig):\n     ) -> Optional[QuantizeMethodBase]:\n         from sglang.srt.layers.linear import LinearBase\n         from sglang.srt.layers.moe.fused_moe_triton import FusedMoE\n+        from sglang.srt.layers.moe.fused_moe_triton.layer import FlashInferFP4MoE\n \n         if isinstance(layer, LinearBase):\n             if is_layer_skipped(prefix, self.exclude_modules) or self.is_layer_excluded(\n@@ -536,6 +548,9 @@ class ModelOptFp4Config(QuantizationConfig):\n             return ModelOptFp4LinearMethod(self)\n         if self.kv_cache_quant_algo and isinstance(layer, RadixAttention):\n             return ModelOptFp8KVCacheMethod(self)\n+        elif isinstance(layer, FlashInferFP4MoE):\n+            # FlashInferFP4MoE needs the same quantization method but with compatible attribute handling\n+            return ModelOptNvFp4FusedMoEMethod(self)\n         elif isinstance(layer, FusedMoE):\n             return ModelOptNvFp4FusedMoEMethod(self)\n         return None\n@@ -726,7 +741,12 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n                 \" quantization. Please use Blackwell and\"\n                 \" above.\"\n             )\n-        self.enable_flashinfer_cutlass_moe = False\n+        self.enable_flashinfer_trtllm_moe = should_use_flashinfer_trtllm_moe()\n+\n+    @property\n+    def enable_flashinfer_cutlass_moe(self) -> bool:\n+        \"\"\"Access the global enable_flashinfer_cutlass_moe setting.\"\"\"\n+        return global_server_args_dict.get(\"enable_flashinfer_cutlass_moe\", False)\n \n     def create_weights(\n         self,\n@@ -743,16 +763,20 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n                 \" dynamic quantization is not supported.\"\n             )\n \n+        # TODO(ch-wan): check if this is needed\n         layer.num_experts = num_experts\n+        layer.num_local_experts = num_experts\n+        layer.intermediate_size_per_partition = intermediate_size_per_partition\n         layer.params_dtype = params_dtype\n         layer.quant_config = self.quant_config\n+\n         weight_dtype = torch.uint8\n         weight_scale_dtype = torch.float8_e4m3fn\n         weight_loader = extra_weight_attrs.get(\"weight_loader\")\n         # GEMM 1\n         w13_weight = ModelWeightParameter(\n             data=torch.empty(\n-                num_experts,\n+                layer.local_num_experts,\n                 2 * intermediate_size_per_partition,\n                 # 2 fp4 items are packed in the input dimension\n                 hidden_size // 2,\n@@ -767,7 +791,7 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n         # GEMM 2\n         w2_weight = ModelWeightParameter(\n             data=torch.empty(\n-                num_experts,\n+                layer.num_local_experts,\n                 hidden_size,\n                 # 2 fp4 items are packed in the input dimension\n                 intermediate_size_per_partition // 2,\n@@ -781,7 +805,7 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n \n         w13_weight_scale = ModelWeightParameter(\n             data=torch.empty(\n-                num_experts,\n+                layer.num_local_experts,\n                 2 * intermediate_size_per_partition,\n                 # 2 fp4 items are packed in the input dimension\n                 hidden_size // self.quant_config.group_size,\n@@ -795,7 +819,7 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n \n         w2_weight_scale = ModelWeightParameter(\n             data=torch.empty(\n-                num_experts,\n+                layer.num_local_experts,\n                 hidden_size,\n                 # 2 fp4 items are packed in the input dimension\n                 intermediate_size_per_partition // self.quant_config.group_size,\n@@ -814,13 +838,13 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n         )\n \n         w13_weight_scale_2 = PerTensorScaleParameter(\n-            data=torch.empty(num_experts, 2, dtype=torch.float32),\n+            data=torch.empty(layer.num_local_experts, 2, dtype=torch.float32),\n             weight_loader=weight_loader,\n         )\n         layer.register_parameter(\"w13_weight_scale_2\", w13_weight_scale_2)\n \n         w2_weight_scale_2 = PerTensorScaleParameter(\n-            data=torch.empty(num_experts, dtype=torch.float32),\n+            data=torch.empty(layer.num_local_experts, dtype=torch.float32),\n             weight_loader=weight_loader,\n         )\n         layer.register_parameter(\"w2_weight_scale_2\", w2_weight_scale_2)\n@@ -830,18 +854,18 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n         )\n \n         w13_input_scale = PerTensorScaleParameter(\n-            data=torch.empty(num_experts, 2, dtype=torch.float32),\n+            data=torch.empty(layer.num_local_experts, 2, dtype=torch.float32),\n             weight_loader=weight_loader,\n         )\n         layer.register_parameter(\"w13_input_scale\", w13_input_scale)\n \n         w2_input_scale = PerTensorScaleParameter(\n-            data=torch.empty(num_experts, dtype=torch.float32),\n+            data=torch.empty(layer.num_local_experts, dtype=torch.float32),\n             weight_loader=weight_loader,\n         )\n         layer.register_parameter(\"w2_input_scale\", w2_input_scale)\n \n-    def swizzle_blockscale(self, scale: torch.tensor):\n+    def swizzle_blockscale(self, scale: torch.Tensor):\n         assert scale.dtype == torch.float8_e4m3fn\n         # Pad and blockwise interleave weight_scale\n         scale_ndim = scale.ndim\n@@ -866,9 +890,125 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n             else swizzled_scale.reshape(B, M, K)\n         )\n \n+    def prepare_static_weights_for_kernel(\n+        self,\n+        # args_dequant,\n+        # args,\n+        gemm1_weights,\n+        gemm2_weights,\n+        gemm1_scales_linear_fp4_bytes,\n+        gemm2_scales_linear_fp4_bytes,\n+        hidden_size,\n+        intermediate_size,\n+        num_experts,\n+    ):\n+        from flashinfer import (\n+            RoutingMethodType,\n+            e2m1_and_ufp8sf_scale_to_float,\n+            fp4_quantize,\n+            next_positive_power_of_2,\n+            reorder_rows_for_gated_act_gemm,\n+            shuffle_matrix_a,\n+            shuffle_matrix_sf_a,\n+        )\n+\n+        \"\"\"Prepare quantized weights for kernel (done offline with weights).\"\"\"\n+        epilogue_tile_m = 128  # FIXME: this depends on the kernel internals\n+\n+        # Convert quantized weights to proper formats\n+        gemm1_weights_fp4 = gemm1_weights.view(torch.float8_e4m3fn).reshape(\n+            num_experts, 2 * intermediate_size, hidden_size // 2\n+        )  # packed fp4\n+        gemm1_scales_linear_fp4 = gemm1_scales_linear_fp4_bytes.view(\n+            torch.float8_e4m3fn\n+        ).reshape(\n+            num_experts, 2 * intermediate_size, hidden_size // 16\n+        )  # fp8 scaling factors\n+\n+        gemm2_weights_fp4 = gemm2_weights.view(torch.float8_e4m3fn).reshape(\n+            num_experts, hidden_size, intermediate_size // 2\n+        )  # packed fp4\n+        gemm2_scales_linear_fp4 = gemm2_scales_linear_fp4_bytes.view(\n+            torch.float8_e4m3fn\n+        ).reshape(\n+            num_experts, hidden_size, intermediate_size // 16\n+        )  # fp8 scaling factors\n+\n+        # Reorder rows of W1 and scales for fused gated activation\n+        gemm1_weights_fp4_interleaved = []\n+        gemm1_scales_fp4_interleaved = []\n+        for i in range(num_experts):\n+            gemm1_weights_fp4_interleaved.append(\n+                reorder_rows_for_gated_act_gemm(gemm1_weights_fp4[i].clone())\n+            )\n+            gemm1_scales_fp4_interleaved.append(\n+                reorder_rows_for_gated_act_gemm(gemm1_scales_linear_fp4[i].clone())\n+            )\n+\n+        # Stack weights and scales for all experts\n+        gemm1_weights_fp4_interleaved = torch.stack(\n+            gemm1_weights_fp4_interleaved\n+        ).reshape(num_experts, 2 * intermediate_size, hidden_size // 2)\n+        gemm1_scales_fp4_interleaved = torch.stack(\n+            gemm1_scales_fp4_interleaved\n+        ).reshape(num_experts, 2 * intermediate_size, hidden_size // 16)\n+\n+        # Shuffle weights and scaling factors for transposed mma output\n+        gemm1_weights_fp4_shuffled = []\n+        gemm1_scales_fp4_shuffled = []\n+        gemm2_weights_fp4_shuffled = []\n+        gemm2_scales_fp4_shuffled = []\n+        for i in range(num_experts):\n+            gemm1_weights_fp4_shuffled.append(\n+                shuffle_matrix_a(\n+                    gemm1_weights_fp4_interleaved[i].view(torch.uint8), epilogue_tile_m\n+                )\n+            )\n+            gemm1_scales_fp4_shuffled.append(\n+                shuffle_matrix_sf_a(\n+                    gemm1_scales_fp4_interleaved[i].view(torch.uint8), epilogue_tile_m\n+                )\n+            )\n+\n+            gemm2_weights_fp4_shuffled.append(\n+                shuffle_matrix_a(\n+                    gemm2_weights_fp4[i].view(torch.uint8), epilogue_tile_m\n+                )\n+            )\n+            gemm2_scales_fp4_shuffled.append(\n+                shuffle_matrix_sf_a(\n+                    gemm2_scales_linear_fp4[i].view(torch.uint8), epilogue_tile_m\n+                )\n+            )\n+\n+        # Stack weights for all experts\n+        gemm1_weights_fp4_shuffled = torch.stack(gemm1_weights_fp4_shuffled)\n+        gemm1_scales_fp4_shuffled = (\n+            torch.stack(gemm1_scales_fp4_shuffled)\n+            .view(torch.float8_e4m3fn)\n+            .reshape(num_experts, 2 * intermediate_size, hidden_size // 16)\n+        )\n+\n+        gemm2_weights_fp4_shuffled = torch.stack(gemm2_weights_fp4_shuffled)\n+        gemm2_scales_fp4_shuffled = (\n+            torch.stack(gemm2_scales_fp4_shuffled)\n+            .view(torch.float8_e4m3fn)\n+            .reshape(num_experts, hidden_size, intermediate_size // 16)\n+        )\n+        return (\n+            gemm1_weights_fp4_shuffled,\n+            gemm1_scales_fp4_shuffled,\n+            gemm2_weights_fp4_shuffled,\n+            gemm2_scales_fp4_shuffled,\n+        )\n+\n     def process_weights_after_loading(self, layer: torch.nn.Module) -> None:\n+        \"\"\"Process FP4 MoE weights after loading from serialized checkpoint.\n \n-        # GEMM 1\n+        Only supports pre-quantized checkpoints with FP8 weights and scales.\n+        \"\"\"\n+\n+        # GEMM 1 scale processing\n         if not torch.allclose(\n             layer.w13_weight_scale_2[:, 0], layer.w13_weight_scale_2[:, 1]\n         ):\n@@ -880,73 +1020,123 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n         w13_weight_scale_2 = layer.w13_weight_scale_2[:, 0]\n         layer.w13_weight_scale_2 = Parameter(w13_weight_scale_2, requires_grad=False)\n \n-        if self.enable_flashinfer_cutlass_moe:\n+        # Calculate input scales based on strategy\n+        if self.enable_flashinfer_cutlass_moe or self.enable_flashinfer_trtllm_moe:\n             w13_input_scale = layer.w13_input_scale.max().to(torch.float32)\n+            w2_input_scale = layer.w2_input_scale.max().to(torch.float32)\n         else:\n             w13_input_scale = layer.w13_input_scale.max(dim=1).values.to(torch.float32)\n+            w2_input_scale = layer.w2_input_scale\n+\n+        # Create shared parameters\n         layer.g1_alphas = Parameter(\n             (w13_input_scale * w13_weight_scale_2).to(torch.float32),\n             requires_grad=False,\n         )\n-\n-        assert (\n-            layer.w13_weight_scale.shape[2] % 16 == 0\n-        ), \"Expected weight_scale.dim(1) to be divisible by 16\"\n-        assert (\n-            layer.w13_weight_scale.dtype == torch.float8_e4m3fn\n-        ), \"Weight Blockscale must be represented as FP8-E4M3\"\n-        w13_blockscale_swizzled = self.swizzle_blockscale(layer.w13_weight_scale)\n-\n-        layer.w13_blockscale_swizzled = Parameter(\n-            w13_blockscale_swizzled, requires_grad=False\n+        layer.g2_alphas = Parameter(\n+            (w2_input_scale * layer.w2_weight_scale_2).to(torch.float32),\n+            requires_grad=False,\n         )\n-        del layer.w13_weight_scale\n-\n-        # This is for quantization, so we need to invert it.\n         layer.w13_input_scale_quant = Parameter(\n             (1 / w13_input_scale).to(torch.float32), requires_grad=False\n         )\n+        layer.w2_input_scale_quant = Parameter(\n+            (1 / w2_input_scale).to(torch.float32), requires_grad=False\n+        )\n \n-        layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)\n+        # Validate weight scales\n+        for name, weight_scale in [\n+            (\"w13\", layer.w13_weight_scale),\n+            (\"w2\", layer.w2_weight_scale),\n+        ]:\n+            assert (\n+                weight_scale.shape[2] % 16 == 0\n+            ), f\"Expected {name}_weight_scale.dim(2) to be divisible by 16\"\n+            assert (\n+                weight_scale.dtype == torch.float8_e4m3fn\n+            ), f\"{name} Weight Blockscale must be represented as FP8-E4M3\"\n+\n+        # Weight processing based on strategy\n+        if (\n+            self.enable_flashinfer_trtllm_moe\n+            and reorder_rows_for_gated_act_gemm is not None\n+            and shuffle_matrix_sf_a is not None\n+        ):\n+            # FlashInfer TRTLLM processing - handles both w13 and w2\n+            (\n+                gemm1_weights_fp4_shuffled,\n+                gemm1_scales_fp4_shuffled,\n+                gemm2_weights_fp4_shuffled,\n+                gemm2_scales_fp4_shuffled,\n+            ) = self.prepare_static_weights_for_kernel(\n+                layer.w13_weight,\n+                layer.w2_weight,\n+                layer.w13_weight_scale,\n+                layer.w2_weight_scale,\n+                layer.w2_weight.size(-2),  # hidden_size\n+                layer.w13_weight.size(-2) // 2,  # intermediate_size\n+                layer.w13_weight.size(0),  # num_experts\n+            )\n \n-        # GEMM 2\n-        if self.enable_flashinfer_cutlass_moe:\n-            w2_input_scale = layer.w2_input_scale.max().to(torch.float32)\n-        else:\n-            w2_input_scale = layer.w2_input_scale\n+            # Set flashinfer parameters\n+            layer.gemm1_weights_fp4_shuffled = Parameter(\n+                gemm1_weights_fp4_shuffled, requires_grad=False\n+            )\n+            layer.gemm2_weights_fp4_shuffled = Parameter(\n+                gemm2_weights_fp4_shuffled, requires_grad=False\n+            )\n+            layer.gemm1_scales_fp4_shuffled = Parameter(\n+                gemm1_scales_fp4_shuffled, requires_grad=False\n+            )\n+            layer.gemm2_scales_fp4_shuffled = Parameter(\n+                gemm2_scales_fp4_shuffled, requires_grad=False\n+            )\n \n-        layer.g2_alphas = Parameter(\n-            (w2_input_scale * layer.w2_weight_scale_2).to(torch.float32),\n-            requires_grad=False,\n-        )\n+            # Additional parameter needed for TRT-LLM\n+            layer.g1_scale_c = Parameter(\n+                (layer.w2_input_scale_quant * layer.g1_alphas).to(torch.float32),\n+                requires_grad=False,\n+            )\n \n-        # This is for quantization, so we need to invert it.\n-        layer.w2_input_scale_quant = Parameter(\n-            (1 / w2_input_scale).to(torch.float32), requires_grad=False\n-        )\n+            # Clean up weights that won't be used by TRT-LLM\n+            del (\n+                layer.w2_weight,\n+                layer.w2_weight_scale,\n+                layer.w13_weight,\n+                layer.w13_weight_scale,\n+            )\n \n-        assert (\n-            layer.w2_weight_scale.shape[2] % 16 == 0\n-        ), \"Expected weight_scale.dim(1) to be divisible by 16\"\n-        assert (\n-            layer.w2_weight_scale.dtype == torch.float8_e4m3fn\n-        ), \"Weight Blockscale must be represented as FP8-E4M3\"\n-        w2_blockscale_swizzled = self.swizzle_blockscale(layer.w2_weight_scale)\n+            print(\"Applied flashinfer weight processing for both w13 and w2\")\n \n-        layer.w2_blockscale_swizzled = Parameter(\n-            w2_blockscale_swizzled, requires_grad=False\n-        )\n-        del layer.w2_weight_scale\n-        layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)\n+        else:\n+            # CUTLASS processing - handle w13 and w2 separately\n+\n+            # Process w13 weights\n+            w13_blockscale_swizzled = self.swizzle_blockscale(layer.w13_weight_scale)\n+            layer.w13_blockscale_swizzled = Parameter(\n+                w13_blockscale_swizzled, requires_grad=False\n+            )\n+            layer.w13_weight = Parameter(layer.w13_weight.data, requires_grad=False)\n+\n+            # Process w2 weights\n+            w2_blockscale_swizzled = self.swizzle_blockscale(layer.w2_weight_scale)\n+            layer.w2_blockscale_swizzled = Parameter(\n+                w2_blockscale_swizzled, requires_grad=False\n+            )\n+            layer.w2_weight = Parameter(layer.w2_weight.data, requires_grad=False)\n+\n+            # Both flashinfer cutlass and regular cutlass use same processing for w2\n+            print(\"Applied weight processing for both w13 and w2\")\n \n-        device = layer.w13_weight.device\n-        layer.cutlass_moe_params = CutlassMoEParams(\n-            CutlassMoEType.BlockscaledFP4,\n-            device,\n-            num_experts=layer.num_experts,  # global num experts\n-            intermediate_size_per_partition=layer.w2_weight.shape[2] * 2,  # n\n-            hidden_size=layer.w13_weight.shape[2] * 2,\n-        )  # k\n+            # Set up CUTLASS MoE parameters\n+            device = layer.w13_weight.device\n+            layer.cutlass_moe_params = CutlassMoEParams(\n+                CutlassMoEType.BlockscaledFP4,\n+                device,\n+                num_experts=layer.num_experts,  # global num experts\n+                intermediate_size_per_partition=layer.w2_weight.shape[2] * 2,  # n\n+                hidden_size=layer.w13_weight.shape[2] * 2,\n+            )  # k\n \n     @property\n     def load_up_proj_weight_first(self) -> bool:\n@@ -971,13 +1161,20 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n     ) -> torch.Tensor:\n         assert activation == \"silu\", \"Only SiLU activation is supported.\"\n \n+        # Check if this is a FlashInferFP4MoE layer that should handle its own forward\n+        if hasattr(layer, \"gemm1_weights_fp4_shuffled\"):\n+            # This layer was processed with flashinfer TRTLLM - delegate to its own forward\n+            return layer.forward(x, topk_output)\n+\n         if self.enable_flashinfer_cutlass_moe:\n             assert (\n                 not apply_router_weight_on_input\n             ), \"apply_router_weight_on_input is not supported for Flashinfer\"\n             # TRTLLM Cutlass moe takes in activations in BF16/Half/nvfp4 precision\n             # and fp4 quantized weights loaded from the checkpoint\n-            topk_weights, topk_ids, _ = topk_output\n+\n+            topk_weights, topk_ids = topk_output.topk_weights, topk_output.topk_ids\n+\n             output = flashinfer_cutlass_fused_moe(\n                 x,\n                 topk_ids.to(torch.int),\n@@ -1005,7 +1202,7 @@ class ModelOptNvFp4FusedMoEMethod(FusedMoEMethodBase):\n \n         from sglang.srt.layers.moe.cutlass_moe import cutlass_moe_fp4\n \n-        topk_weights, topk_ids, _ = topk_output\n+        topk_weights, topk_ids = topk_output.topk_weights, topk_output.topk_ids\n         output = cutlass_moe_fp4(\n             a=x,\n             a1_gscale=layer.w13_input_scale_quant,\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex 03faea684..759bb6afa 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -51,7 +51,6 @@ from sglang.srt.disaggregation.decode_schedule_batch_mixin import (\n     ScheduleBatchDisaggregationDecodeMixin,\n )\n from sglang.srt.distributed.parallel_state import get_tensor_model_parallel_rank\n-from sglang.srt.layers.moe.utils import DeepEPMode, MoeA2ABackend\n from sglang.srt.mem_cache.allocator import (\n     BaseTokenToKVPoolAllocator,\n     SWATokenToKVPoolAllocator,\n@@ -109,6 +108,7 @@ GLOBAL_SERVER_ARGS_KEYS = [\n     \"enable_triton_kernel_moe\",\n     \"enable_multimodal\",\n     \"enable_symm_mem\",\n+    \"quantization\",\n ]\n \n # Put some global args for easy access\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex b5b13d9ac..009f926bf 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -60,12 +60,9 @@ from sglang.srt.layers.linear import (\n     RowParallelLinear,\n )\n from sglang.srt.layers.logits_processor import LogitsProcessor\n-from sglang.srt.layers.moe.ep_moe.layer import (\n-    DeepEPMoE,\n-    get_moe_impl_class,\n-    should_use_flashinfer_trtllm_moe,\n-)\n+from sglang.srt.layers.moe.ep_moe.layer import DeepEPMoE, get_moe_impl_class\n from sglang.srt.layers.moe.topk import TopK\n+from sglang.srt.layers.moe.utils import should_use_flashinfer_trtllm_moe\n from sglang.srt.layers.quantization import deep_gemm_wrapper\n from sglang.srt.layers.quantization.base_config import QuantizationConfig\n from sglang.srt.layers.quantization.fp8_kernel import (\n@@ -307,19 +304,15 @@ class DeepseekV2MoE(nn.Module):\n             config=config, prefix=add_prefix(\"gate\", prefix), is_nextn=is_nextn\n         )\n \n-        self.topk = (\n-            TopK(\n-                top_k=config.num_experts_per_tok + self.num_fused_shared_experts,\n-                renormalize=config.norm_topk_prob,\n-                use_grouped_topk=True,\n-                num_expert_group=config.n_group,\n-                num_fused_shared_experts=self.num_fused_shared_experts,\n-                topk_group=config.topk_group,\n-                correction_bias=self.gate.e_score_correction_bias,\n-                routed_scaling_factor=self.routed_scaling_factor,\n-            )\n-            if not should_use_flashinfer_trtllm_moe()\n-            else None\n+        self.topk = TopK(\n+            top_k=config.num_experts_per_tok + self.num_fused_shared_experts,\n+            renormalize=config.norm_topk_prob,\n+            use_grouped_topk=True,\n+            num_expert_group=config.n_group,\n+            num_fused_shared_experts=self.num_fused_shared_experts,\n+            topk_group=config.topk_group,\n+            correction_bias=self.gate.e_score_correction_bias,\n+            routed_scaling_factor=self.routed_scaling_factor,\n         )\n \n         self.experts = get_moe_impl_class()(\n@@ -476,10 +469,14 @@ class DeepseekV2MoE(nn.Module):\n             # router_logits: (num_tokens, n_experts)\n             router_logits = self.gate(hidden_states)\n             kwargs = {\"hidden_states\": hidden_states}\n-            if self.topk is not None:\n-                kwargs[\"topk_output\"] = self.topk(hidden_states, router_logits)\n+\n+            # FlashInferFP4MoE (TRTLLM path) expects (TopK, router_logits) tuple\n+            # Regular FusedMoE (CUTLASS path) expects StandardTopKOutput\n+            if should_use_flashinfer_trtllm_moe():\n+                kwargs[\"topk_output\"] = (self.topk, router_logits)\n             else:\n-                kwargs[\"router_logits\"] = router_logits\n+                kwargs[\"topk_output\"] = self.topk(hidden_states, router_logits)\n+\n             final_hidden_states = self.experts(**kwargs)\n             if not _is_cuda:\n                 final_hidden_states *= self.routed_scaling_factor\n@@ -505,10 +502,14 @@ class DeepseekV2MoE(nn.Module):\n         # router_logits: (num_tokens, n_experts)\n         router_logits = self.gate(hidden_states)\n         kwargs = {\"hidden_states\": hidden_states}\n-        if self.topk is not None:\n-            kwargs[\"topk_output\"] = self.topk(hidden_states, router_logits)\n+\n+        # FlashInferFP4MoE (TRTLLM path) expects (TopK, router_logits) tuple\n+        # Regular FusedMoE (CUTLASS path) expects StandardTopKOutput\n+        if should_use_flashinfer_trtllm_moe():\n+            kwargs[\"topk_output\"] = (self.topk, router_logits)\n         else:\n-            kwargs[\"router_logits\"] = router_logits\n+            kwargs[\"topk_output\"] = self.topk(hidden_states, router_logits)\n+\n         final_hidden_states = self.experts(**kwargs)\n         if not _is_cuda and not _use_aiter:\n             # fused in biased_grouped_topk so we can skip here\ndiff --git a/python/sglang/srt/models/glm4_moe.py b/python/sglang/srt/models/glm4_moe.py\nindex 76f954578..568f632f2 100644\n--- a/python/sglang/srt/models/glm4_moe.py\n+++ b/python/sglang/srt/models/glm4_moe.py\n@@ -50,11 +50,9 @@ from sglang.srt.layers.linear import (\n     RowParallelLinear,\n )\n from sglang.srt.layers.logits_processor import LogitsProcessor\n-from sglang.srt.layers.moe.ep_moe.layer import (\n-    get_moe_impl_class,\n-    should_use_flashinfer_trtllm_moe,\n-)\n+from sglang.srt.layers.moe.ep_moe.layer import get_moe_impl_class\n from sglang.srt.layers.moe.topk import TopK\n+from sglang.srt.layers.moe.utils import should_use_flashinfer_trtllm_moe\n from sglang.srt.layers.quantization.base_config import QuantizationConfig\n from sglang.srt.layers.quantization.fp8_kernel import (\n     is_fp8_fnuz,\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 6c63de973..fb3f80f87 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -481,6 +481,13 @@ class ServerArgs:\n                 self.tp_size,\n             ], \"The expert parallel size must be 1 or the same as the tensor parallel size\"\n \n+        if self.enable_flashinfer_trtllm_moe:\n+            if not self.disable_shared_experts_fusion:\n+                self.disable_shared_experts_fusion = True\n+                logger.warning(\n+                    \"FlashInfer TRTLLM MoE is enabled. --disable-shared-experts-fusion is automatically set.\"\n+                )\n+\n         # DeepEP MoE\n         if self.moe_a2a_backend == \"deepep\":\n             if self.deepep_mode == \"normal\":",
  "apis": [
    "sglang.srt.layers.moe.ep_moe.layer.get_moe_impl_class",
    "sglang.srt.layers.moe.fused_moe_triton.layer.FlashInferFP4MoE",
    "sglang.srt.layers.quantization.modelopt_quant.ModelOptNvFp4FusedMoEMethod"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/models/deepseek_v2.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit introduces changes to several core modules (e.g., the MoE layers, FlashInferFP4MoE, and quantization configurations) by adding support for low latency NVFP4 decode kernels from Flashinfer. The changes update internal APIs and shift computation paths for quantized models in a way that improves inference performance (low latency) on CPU or common hardware. The modifications are non-trivial, affect high-level API implementations (such as in DeepseekV2MoE and related quantization modules), and are intended to optimize performance rather than simply refactor or fix a bug. Thus, the commit satisfies the criteria for performance/optimization related changes.",
  "llm_api_reason": "This commit introduces new low\u2010latency support for NVFP4 decode kernels via Flashinfer. In the ep_moe layer, the factory function get_moe_impl_class is modified to return a new FP4 variant when FP4 quantization is enabled. In the fused_moe_triton layer a new class FlashInferFP4MoE is added with its own __init__, a helper method _quantize_hidden_states_fp4, and a forward method implementing the new FP4 decoding using the Flashinfer kernel. In addition, the ModelOptNvFp4FusedMoEMethod in the quantization module is updated to support the new FlashInfer-based FP4 method. These are the high\u2010level APIs affected by this commit."
}