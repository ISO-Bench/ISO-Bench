{
  "commit_hash": "23c764b18aeb37c42ddedd7468f1a5753df1f232",
  "pr_url": "https://github.com/sgl-project/sglang/pull/4767",
  "pr_date": "2025-04-01",
  "timeline_text": "Copy link Contributor liz-badada commented Mar 25, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation Support DeepEP low latency dispatch / combine, introduce a new command-line argument --deepep-mode to specify DeepEP mode ( auto , normal and low_latency ). Additionally, we believe DeepEP is particularly well-suited for PD disaggregation. Also in low-latency mode, the CUDA Graph feature functions seamlessly. deepep mode option # auto (default mode): use normal dispatch / combine for non decode and low_latency dispatch / combine for decode python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 8 --host 0.0.0.0 --port 30000 --enable-deepep-moe --deepep-mode auto \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--cuda-graph-max-bs 128 # normal: only use normal dispatch / combine for both prefill and decode (disable CUDA graph) python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 8 --dp 8 --host 0.0.0.0 --port 30000 --enable-dp-attention --enable-deepep-moe --deepep-mode normal \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--disable-cuda-graph # low_latency: only use low_latency dispatch / combine for both prefill and decode python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code \\\n--tp 8 --dp 8 --host 0.0.0.0 --port 30000 --enable-dp-attention --enable-deepep-moe --deepep-mode low_latency \\\n--max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output \\\n--cuda-graph-max-bs 128 Note: low_latency mode will limit max dispatch tokens to less than 256 (set as 128 currently), thus need to limit prefill running tokens, e.g., for dp=8, --max-running-requests 8 --chunked-prefill-size 1024 --max-prefill-tokens 128 . Suggest use this mode when PD disaggregation is ready. performance # bench cmd python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 --random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 --max-concurrency 128 single node (H20-3e) # EP MoE python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --host 0.0.0.0 --port 30000 --enable-ep-moe --max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output --cuda-graph-max-bs 128 # DeepEP MoE (auto) python3 -m sglang.launch_server --model-path deepseek-ai/DeepSeek-V3 --trust-remote-code --tp 8 --host 0.0.0.0 --port 30000 --enable-deepep-moe --deepep-mode auto --max-running-requests 128 --disable-radix-cache --mem-fraction-static 0.9 --stream-output --cuda-graph-max-bs 128 MoE Version Concurrency Input Output Num Requests Input Throughput(tok/s) Output Throughput (tok/s) Total Throughput (tok/s) DeepEP origin 127.97 1000 1000 512 581.94 581.94 1163.87 DeepEP auto 127.94 1000 1000 512 910.84 910.84 1821.68 EPMoE 127.94 1000 1000 512 862.52 862.52 1725.04 Modifications Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 7 zhyncs, ch-wan, saltyfish66, Edenzzzz, yuleil, cnwenf, and MARD1NO reacted with thumbs up emoji All reactions \ud83d\udc4d 7 reactions Support DeepEP Low Latency a43b7cb liz-badada changed the title Support DeepEP Low Latency [Feature] Support DeepEP Low Latency Mar 25, 2025 Copy link Contributor Author liz-badada commented Mar 25, 2025 #4734 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan mentioned this pull request Mar 25, 2025 [Roadmap] EP Enhancement #4734 Closed 18 tasks ch-wan self-assigned this Mar 25, 2025 refactor 55109b4 liz-badada marked this pull request as ready for review March 26, 2025 03:52 liz-badada requested review from merrymercy , Ying1123 , hnyls2002 , zhyncs , ispobock , ByronHsu , HaiShaw and zhaochenyang20 as code owners March 26, 2025 03:52 zhyncs added\n  the high priority label Mar 26, 2025 Copy link xle97 commented Mar 27, 2025 hi @liz-badada ,Why can\u2019t the  deepmoe forward be reused in low latency mode?\u201d All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . use deep_gemm m_grouped_gemm_fp8_fp8_bf16_nt_masked 98689e5 Copy link Collaborator ch-wan commented Mar 29, 2025 @liz-badada Is this PR ready to merge? I'm going to review it. Meanwhile, could you please resolve conflicts with the main branch? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . update 78e04de Copy link Collaborator zhaochenyang20 commented Mar 29, 2025 @liz-badada could you rebase with main? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Mar 30, 2025 @liz-badada Is this PR ready to merge? I'm going to review it. Meanwhile, could you please resolve conflicts with the main branch? Not yet, still lacks activation between up-proj and down_proj, will finish it. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Mar 30, 2025 @liz-badada could you rebase with main? sure All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Mar 30, 2025 hi @liz-badada ,Why can\u2019t the deepmoe forward be reused in low latency mode?\u201d Hi, deepepmoe forward still uses default group gemm as well as token permutation, while low latency has different layout, and it could simply utilize deepgemm masked gemm kernel, as described in deep_gemm : Use m_grouped_gemm_fp8_fp8_bf16_nt_masked for this purpose and consult the relevant documentation. An example usage is to use the output of low-latency kernels from DeepEP as input. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . add silu kernel from lightllm \u2026 7d43516 Co-authored-by: laixinn <xielx@shanghaitech.edu.cn> laixinn force-pushed the Support_DeepEP_Low_Latency branch\n    from 0b93880 to 7d43516 Compare March 31, 2025 08:54 Copy link Contributor liusy58 commented Mar 31, 2025 @liz-badada hi, I\u2019m working on this pull request and encountered the following error: AttributeError: 'DeepEPMoE' object has no attribute 'w13_weight_scale_inv'. Did you mean: 'w13_weight_scale'? It seems the attribute w13_weight_scale_inv is missing. Could you clarify how to resolve this? Any guidance would be greatly appreciated! All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Mar 31, 2025 @liz-badada hi, I\u2019m working on this pull request and encountered the following error: AttributeError: 'DeepEPMoE' object has no attribute 'w13_weight_scale_inv'. Did you mean: 'w13_weight_scale'? It seems the attribute w13_weight_scale_inv is missing. Could you clarify how to resolve this? Any guidance would be greatly appreciated! Thanks for trying, I will fix the self.w13_weight_fp8 init soon! \ud83d\udc4d 1 liusy58 reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . fix activation bugs in deepep low latency mode 2118193 Copy link Collaborator zhaochenyang20 commented Mar 31, 2025 @liz-badada fix the conflicts? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Apr 1, 2025 @liz-badada fix the conflicts? Still have some code to update, will merge main when ready. \ud83d\udc4d 1 zhaochenyang20 reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . liz-badada added 3 commits April 1, 2025 03:36 degbugging b479f66 fix_bug 953bfa1 Merge branch 'main' into low_latency_debug d667b83 liz-badada requested a review\n  from xiezhq-hermann as a code owner April 1, 2025 05:43 liz-badada added 2 commits April 1, 2025 06:54 support deepep normal only, low_latency only and auto mode 36b65cd remove_debug_info 0eb8754 Copy link Contributor Author liz-badada commented Apr 1, 2025 This PR is ready for review! \ud83d\ude80 1 ch-wan reacted with rocket emoji All reactions \ud83d\ude80 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . ch-wan added 3 commits April 1, 2025 09:01 add doc 6095279 todo for ep_size d17853d format 70c3133 ch-wan approved these changes Apr 1, 2025 View reviewed changes Hide details View details zhyncs merged commit 23c764b into sgl-project : main Apr 1, 2025 58 of 68 checks passed Uh oh! There was an error while loading. Please reload this page . fzyzcjy mentioned this pull request Apr 3, 2025 Add sanity check for max_running_requests #5016 Merged 6 tasks Copy link Contributor liusy58 commented Apr 4, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . hi, @liz-badada awesome work! ,I\u2019d like to use DeepSeek-V2-Lite-Chat with Deepep , but Deepep requires block quantization. Is there a way I can enable/support this? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Apr 6, 2025 hi, @liz-badada awesome work! ,I\u2019d like to use DeepSeek-V2-Lite-Chat with Deepep , but Deepep requires block quantization. Is there a way I can enable/support this? Hi, DeepEP support FP8 with block scaling and BF16 as well, but we simply set dispatch FP8 as default (especially for low latency mode), and integrate m_grouped_gemm_fp8_fp8_bf16_nt_masked of DeepGEMM, so for DeepSeek-V2-Lite, it can not run smoothly. All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor liusy58 commented Apr 7, 2025 @liz-badada Initially, I planned to use llm-compressor, but I encountered a bug ( #1320 ). Since DeepseekV3 consumes a significant amount of GPU memory, it's seriously affecting our development efficiency. Do you have any suggestions? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator ch-wan commented Apr 7, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Hi @liusy58 , we recently released a 5-layer DeepSeek to simplify the development process: https://huggingface.co/chwan/DeepSeek-V3-5layer . \ud83d\udc4d 2 liz-badada and liusy58 reacted with thumbs up emoji All reactions \ud83d\udc4d 2 reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor liusy58 commented Apr 8, 2025 @ch-wan Thank you~ All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . thyecust pushed a commit\n        to thyecust/sglang\n      that referenced\n      this pull request Apr 11, 2025 [Feature] Support DeepEP Low Latency ( sgl-project#4767 ) \u2026 d782244 Co-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu> jimoosciuc pushed a commit\n        to Furion-cn/sglang\n      that referenced\n      this pull request Apr 17, 2025 [Feature] Support DeepEP Low Latency ( sgl-project#4767 ) \u2026 9312847 Co-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu> Copy link Xiaofei-fei commented Apr 22, 2025 Hi\uff0chere are some problems I have occurred, could you help me analyze these problems? when I use low_latency mode,I failed with the info, RuntimeError: Failed: CUDA error /sgl-workspace/DeepEP/csrc/kernels/internode_ll.cu:341 'too many blocks in cooperative launch' when I use auto model ,I failed with the info, AssertionError: DeepEP MoE auto mode is not supported with DP Attention. 3.When I try to dropout the DP Attention with auto model again ,I failed with the info, AssertionError: multi-node data parallel is not supported unless dp attention! btw,other info in my experiment: 2 H20*8,(96G) node0: NCCL_IB_GID_INDEX=3 NCCL_DEBUG=INFO GLOO_SOCKET_IFNAME=eth0 TP_SOCKET_IFNAME=eth0 NVSHMEM_IB_ENABLE_IBGDA=0NVSHMEM_IBGDA_NIC_HANDLER=gpu python3 -m sglang.launch_server --model-path model/DeepSeek-V3 --trust-remote-code   --tp 16 --dp 16  --dist-init-addr 172.31.0.4:30000 --nnodes 2 --node-rank 0   --enable-dp-attention --enable-deepep-moe   --deepep-mode auto --disable-cuda-graph --host 0.0.0.0 --port 12123 node1: NCCL_IB_GID_INDEX=3 NCCL_DEBUG=INFO GLOO_SOCKET_IFNAME=eth0 TP_SOCKET_IFNAME=eth0 NVSHMEM_IB_ENABLE_IBGDA=0 NVSHMEM_IBGDA_NIC_HANDLER=gpu python3 -m sglang.launch_server --model-path model/DeepSeek-V3 --trust-remote-code   --tp 16 --dp 16  --dist-init-addr 172.31.0.4:30000 --nnodes 2 --node-rank 1   --enable-dp-attention --enable-deepep-moe   --deepep-mode auto --disable-cuda-graph All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor Author liz-badada commented Apr 22, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @Xiaofei-fei Refer https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py#L442-L473 2/3. Auto mode do not support DP Attention, remove --enable-dp-attention and --dp , Also for auto mode, --disable-cuda-graph could be removed, should add --cuda-graph-max-bs 128 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Apr 23, 2025 [SW-226289] rebase sglang to tag v0.4.5 ( sgl-project#12 ) \u2026 0eac714 * Fix ut mla-test-1-gpu-amd ( sgl-project#4813 )\n\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\n\n* Remove Unintended Capture Batch Sizes in AMD HIP Graph Runner ( sgl-project#4638 )\n\n* [k8s] Clarified the usage of shared memory. ( sgl-project#4341 )\n\n* gemma3: impl `get_attention_sliding_window_size` for attn init ( sgl-project#4823 )\n\n* add partial_json_parser and einops ( sgl-project#4827 )\n\n* fix the release doc dependency issue ( sgl-project#4828 )\n\n* Update doc for DeepSeek-V3-0324 ( sgl-project#4825 )\n\n* deps: lazy import optional dependencies `gguf` and `torchvision` ( sgl-project#4826 )\n\n* Update MMMU Benchmark instructions ( sgl-project#4694 )\n\n* Fix the nightly eval by lowering the threshold of `neuralmagic/gemma-2-2b-it-FP8` ( sgl-project#4830 )\n\n* Basic Cleanup ( sgl-project#4833 )\n\n* Support (1 <= dp < tp) in the dp attention in DeepEP ( sgl-project#4770 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\n\n* [Fix] Add compressed_tensors as deps ( sgl-project#4819 )\n\n* Fix error due to CustomAllreduce setup failure ( sgl-project#4815 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* use default for torch.ops ( sgl-project#4835 )\n\n* [CI] Remove unused imports with Ruff to pre-commit config, only to benchmarks/docs/examples folder ( sgl-project#3969 )\n\n* [Misc] Fix issues reported by torchfix ( sgl-project#4837 )\n\n* Include context length in /v1/models response. ( sgl-project#4809 )\n\n* [Fix] `self.worker` assignment in `TpModelWorker` and refactor references ( sgl-project#4788 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Fix the lora adapter when lora path is none ( sgl-project#4799 )\n\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\n\n* fix: fix typo of comments in w8a8_fp8.py ( sgl-project#4843 )\n\n* Remove retry in nightly tests ( sgl-project#4846 )\n\n* Fix CI of test_patch_torch ( sgl-project#4844 )\n\n* IPv6 support ( sgl-project#3949 )\n\nSigned-off-by: Brayden Zhong <b8zhong@uwaterloo.ca>\n\n* ci: add condition for daily docker build ( sgl-project#4487 )\n\n* [Fix] fix output_top_logprobs is not exist ( sgl-project#4597 )\n\n* fix: when use SGLANG_PORT this env,port is str ( sgl-project#4528 )\n\nSigned-off-by: rongfu.leng <lenronfu@gmail.com>\n\n* Support Page Size > 1 for FA3 ( sgl-project#4832 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\n\n* Fix Engine error when enabling DP attention ( sgl-project#4648 )\n\n* fix: Inappropriate lack of Optional type on OpenAI ChatCompletionRequest ( sgl-project#4681 )\n\n* Support controlling nsys start and end range programmatically ( sgl-project#4688 )\n\n* Remove empty tool function name ( sgl-project#4704 )\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\n\n* Fix missing arguments in SchedulePolicy and RadixCache initialization in tests. ( sgl-project#4712 )\n\n* get the python version from env ( sgl-project#4729 )\n\n* Fix torch.cuda.MemPool() internal assertion failure ( sgl-project#4687 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* Super tiny remove unused code ( sgl-project#4750 )\n\n* Support with_stack and record_shapes in profiler ( sgl-project#4740 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* test: reduce `mem_fraction_static` for gemma3 vision test ( sgl-project#4840 )\n\n* Fix CI tests ( sgl-project#4853 )\n\n* Fix fa3 cuda graph page_size > 1 precision and page_size=1 speed ( sgl-project#4855 )\n\n* Revert \"get the python version from env ( sgl-project#4729 )\" ( sgl-project#4863 )\n\n* [Feature] add multi-rank support for Lora ( sgl-project#4492 )\n\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\n\n* Clean up `import vllm` in quantization/__init__.py ( sgl-project#4834 )\n\n* Fix wrong variable name when stopping memory profile ( sgl-project#4772 )\n\n* [Feat] support deepgemm for cmake ( sgl-project#4864 )\n\n* Make torch compile configurable for biased_grouped_topk ( sgl-project#4749 )\n\n* update sgl-kernel test ci ( sgl-project#4866 )\n\n* fix sampling issue ( sgl-project#4871 )\n\n* bump sgl-kernel 0.0.5.post4 ( sgl-project#4768 )\n\n* fix sgl-kernel cu118 build ( sgl-project#4872 )\n\n* [Feature] Support FA3 backend for MLA ( sgl-project#4831 )\n\n* upgrade sgl-kernel 0.0.5.post4 ( sgl-project#4873 )\n\n* update torch compile doc ( sgl-project#4874 )\n\n* bump v0.4.4.post3 ( sgl-project#4878 )\n\n* Fix BadRequestError wrong arguments and remove openai dependency ( sgl-project#4882 )\n\n* Improve stack trace of retry errors ( sgl-project#4845 )\n\n* Tiny fix doc error ( sgl-project#4795 )\n\n* [Docs] Update DeepGEMM at README.md ( sgl-project#4886 )\n\n* Update CODEOWNERS ( sgl-project#4889 )\n\n* Delete test_deep_gemm.py ( sgl-project#4891 )\n\n* Add deepseek style fused moe group gate selection kernel ( sgl-project#4530 )\n\n* quick fix: add default for new kernel ( sgl-project#4898 )\n\n* remove setup for sgl-kernel ( sgl-project#4899 )\n\n* [Misc] Clean m.def and add Development Tips ( sgl-project#4890 )\n\n* fix allreduce test ( sgl-project#4909 )\n\n* Support page size > 1 + eagle ( sgl-project#4908 )\n\n* Fix retract for page size > 1 ( sgl-project#4914 )\n\n* [Feature] use pytest for sgl-kernel ( sgl-project#4896 )\n\n* fix bmm fp8 ( sgl-project#4926 )\n\n* Fix the timeout for unit-test-2-gpu in pr-test.yml ( sgl-project#4927 )\n\n* Fix 2-gpu CI test and suppress some warnings ( sgl-project#4930 )\n\n* [feat] add fa3 in sgl-kernel ( sgl-project#4902 )\n\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\n\n* Fix sglang frontend's incorrect dependency on torch ( sgl-project#4931 )\n\n* [Fix] avoid stream sync and torch compile in prefill for fa3 backend ( sgl-project#4932 )\n\n* cleanup sgl-kernel ( sgl-project#4933 )\n\n* [Fix] Improve Lora tests and reduce CI runtime ( sgl-project#4925 )\n\n* Fix DeepSeek bug causing 2.2% MMLU drop when TP!=DP ( sgl-project#4883 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* [Fix] Add torch compile for torch.clamp back ( sgl-project#4936 )\n\n* Fix oom error for large page size ( sgl-project#4913 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* [feat] interface for platforms abstraction ( sgl-project#4928 )\n\n* [Fix] revert clean m.def for cudagraph ( sgl-project#4944 )\n\n* refactor: multimodal data ( sgl-project#4754 )\n\n* bump sgl-kernel v0.0.6 ( sgl-project#4950 )\n\n* [Build] Fix cuda12.8 build error in nvfp4_scaled_mm_kernels.cu ( sgl-project#4953 )\n\n* use fa3 in sgl-kernel ( sgl-project#4954 )\n\n* Revert PR 4764 & 4813 related to R1 RoPE ( sgl-project#4959 )\n\n* [Feature] Support DeepEP Low Latency ( sgl-project#4767 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* update bench_serving ( sgl-project#4958 )\n\n* Prevent memory leak of retract_decode when page_size > 1 ( sgl-project#4977 )\n\n* [VLM RLHF] Take Image input for verl vlm rollout ( sgl-project#4915 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: GeLee <leege233@gmail.com>\n\n* Large page size aligned hierarchical caching ( sgl-project#4581 )\n\n* bug fix for hicache host eviction ( sgl-project#4989 )\n\n* sgl scaled_fp8_quant support output padding ( sgl-project#4861 )\n\n* Add Eagle Speculative Decoding to FA3 Backend ( sgl-project#4951 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\n\n* Update tokenizer_manager.py ( sgl-project#5008 )\n\n* [sgl-kernel] per token group quant support COLUMN MAJOR ( sgl-project#4817 )\n\n* update cutlass tag ( sgl-project#5011 )\n\n* Feature/revise docs ci ( sgl-project#5009 )\n\n* fix: fix illegal cuda memory access at fused_moe_kernel ( sgl-project#4727 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* [Build] Support build sgl-kernel with ccache ( sgl-project#5020 )\n\n* fix deepgemm as well ( sgl-project#5030 )\n\n* try to fix ci oserror ( sgl-project#5024 )\n\n* Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5005 )\n\n* Small refactor DeepEPMode to clean up code a bit ( sgl-project#4992 )\n\n* [Fix] fix fa3 build at cu118 ( sgl-project#5036 )\n\n* Revert \"Replace enable_flashinfer_mla argument with attention_backend\" ( sgl-project#5048 )\n\n* bump sgl-kernel v0.0.7 ( sgl-project#5046 )\n\n* update eagle-3 docs ( sgl-project#4796 )\n\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\n\n* Add LlavaLlamaForCausaLM in MultiModal Processors ( sgl-project#5039 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* Update the retry count ( sgl-project#5051 )\n\n* upgrade sgl-kernel v0.0.7 ( sgl-project#5049 )\n\n* [2/3] fix dsv3 awq issue  ( sgl-project#4625 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\n\n* Feature/revise docs ci ( sgl-project#5056 )\n\n* Add H20 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5057 )\n\n* [fix] remove `cuda_device_count_stateless` ( sgl-project#5060 )\n\n* Small refactor DeepEPDispatcher into subclasses ( sgl-project#4994 )\n\n* Support async DeepEP by splitting into two stages ( sgl-project#4995 )\n\n* Cleanup unused resources after DeepEP operation ( sgl-project#4996 )\n\n* Add DeepSeek V3/R1 shared experts fusion ( sgl-project#4918 )\n\n* [deepep] fix: shared experts are not initialized when shared experts fusion is enabled ( sgl-project#5072 )\n\n* fix dummy-load deepseekv2 ( sgl-project#4535 )\n\n* support sgl-kernel on blackwell ( sgl-project#5074 )\n\n* FA3 Spec Decoding to support top k = 1 and add cuda graph support ( sgl-project#5050 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Chunan Zeng <zcnrex@gmail.com>\n\n* [Revision] Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5052 )\n\n* upgrade transformers 4.51.0 ( sgl-project#5088 )\n\n* sgl-kernel transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5079 )\n\n* bump sgl-kernel 0.0.8 ( sgl-project#5089 )\n\n* python transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5080 )\n\n* bump v0.4.4.post4 ( sgl-project#5091 )\n\n* Fix: Reduce the number of document ci attempts to avoid long ci running ( sgl-project#5097 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Add Llama4 support ( sgl-project#5092 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Fix refactor error - fp8.py ( sgl-project#5106 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* bump v0.4.5 ( sgl-project#5117 )\n\n* Workaround for async copy issue in HPU eager mode ( sgl-project#1 )\n\nSigned-off-by: Rahul Vijayaraghavan <rvijayaraghavan@habana.ai>\nCo-authored-by: Rahul Vijayaraghavan <rvijayaraghavan@habana.ai>\n\n* [SW-223847]: Fix sgl_kernel module not available ( sgl-project#2 )\n\nCo-authored-by: vikram singh shekhawat <vshekhawat@habana.ai>\n\n* [Base] Enable torch compile ( sgl-project#4 )\n\n* [SW-226331] disable dynamic shape in torch compile mode\n\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\n\n---------\n\nSigned-off-by: Kebe <mail@kebe7jun.com>\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nSigned-off-by: rongfu.leng <lenronfu@gmail.com>\nSigned-off-by: Rahul Vijayaraghavan <rvijayaraghavan@habana.ai>\nSigned-off-by: Mohit Sinha <msinha@habana.ai>\nCo-authored-by: strgrb <zhangkaihong.zkh@antgroup.com>\nCo-authored-by: Zhang Kaihong <zhangkaihong.zkh@alibaba-inc.com>\nCo-authored-by: AinL <gmlwns5176@gmail.com>\nCo-authored-by: Ji\u0159\u00ed Suchomel <jiri.suchomel@statsperform.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Daniel Holanda <holand.daniel@gmail.com>\nCo-authored-by: tarinkk <129432511+tarinkk@users.noreply.github.com>\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: Junrong Lin <33685709+ocss884@users.noreply.github.com>\nCo-authored-by: Kebe <mail@kebe7jun.com>\nCo-authored-by: Brayden Zhong <b8zhong@uwaterloo.ca>\nCo-authored-by: Jon Durbin <jon@jondurbin.com>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: Qiaolin Yu <qy254@cornell.edu>\nCo-authored-by: Beichen Ma <mabeichen12@gmail.com>\nCo-authored-by: Jiaqi <57028284+ZhuJiaqi9905@users.noreply.github.com>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Vincent <vincentzhongy+githubvincent4@gmail.com>\nCo-authored-by: warjiang <1096409085@qq.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: rongfu.leng <lenronfu@gmail.com>\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: BroadbentJim <BroadbentJim@users.noreply.github.com>\nCo-authored-by: vikram singh shekhawat <vshekhawat@habana.ai>\nCo-authored-by: DavidChan <chengwei0519@163.com>\nCo-authored-by: chaobo jia <91889375+jcbjcbjc@users.noreply.github.com>\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\nCo-authored-by: Fr4nk1in <sh.fu@outlook.com>\nCo-authored-by: yinfan98 <1106310035@qq.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\nCo-authored-by: SEPLOS <seplos@aliyun.com>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: GeLee <leege233@gmail.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\nCo-authored-by: Kaiyu Yang <yangky@umich.edu>\nCo-authored-by: renxin <90580890+renxinx@users.noreply.github.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: Tommy Yang <tommyyang0524@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: Rahul Vijayaraghavan <rahul.vijayaraghavan@intel.com>\nCo-authored-by: Rahul Vijayaraghavan <rvijayaraghavan@habana.ai>\nCo-authored-by: Jay Thakur <jthakur@habana.ai>\nCo-authored-by: Anshuman Tripathy <atripathy@habana.ai> pi314ever pushed a commit\n        to pi314ever/sglang\n      that referenced\n      this pull request Apr 23, 2025 rebase sglang to tag v0.4.5.post1 ( sgl-project#13 ) \u2026 3ecb4e3 * Support with_stack and record_shapes in profiler ( sgl-project#4740 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* test: reduce `mem_fraction_static` for gemma3 vision test ( sgl-project#4840 )\n\n* Fix CI tests ( sgl-project#4853 )\n\n* Fix fa3 cuda graph page_size > 1 precision and page_size=1 speed ( sgl-project#4855 )\n\n* Revert \"get the python version from env ( sgl-project#4729 )\" ( sgl-project#4863 )\n\n* [Feature] add multi-rank support for Lora ( sgl-project#4492 )\n\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\n\n* Clean up `import vllm` in quantization/__init__.py ( sgl-project#4834 )\n\n* Fix wrong variable name when stopping memory profile ( sgl-project#4772 )\n\n* [Feat] support deepgemm for cmake ( sgl-project#4864 )\n\n* Make torch compile configurable for biased_grouped_topk ( sgl-project#4749 )\n\n* update sgl-kernel test ci ( sgl-project#4866 )\n\n* fix sampling issue ( sgl-project#4871 )\n\n* bump sgl-kernel 0.0.5.post4 ( sgl-project#4768 )\n\n* fix sgl-kernel cu118 build ( sgl-project#4872 )\n\n* [Feature] Support FA3 backend for MLA ( sgl-project#4831 )\n\n* upgrade sgl-kernel 0.0.5.post4 ( sgl-project#4873 )\n\n* update torch compile doc ( sgl-project#4874 )\n\n* bump v0.4.4.post3 ( sgl-project#4878 )\n\n* Fix BadRequestError wrong arguments and remove openai dependency ( sgl-project#4882 )\n\n* Improve stack trace of retry errors ( sgl-project#4845 )\n\n* Tiny fix doc error ( sgl-project#4795 )\n\n* [Docs] Update DeepGEMM at README.md ( sgl-project#4886 )\n\n* Update CODEOWNERS ( sgl-project#4889 )\n\n* Delete test_deep_gemm.py ( sgl-project#4891 )\n\n* Add deepseek style fused moe group gate selection kernel ( sgl-project#4530 )\n\n* quick fix: add default for new kernel ( sgl-project#4898 )\n\n* remove setup for sgl-kernel ( sgl-project#4899 )\n\n* [Misc] Clean m.def and add Development Tips ( sgl-project#4890 )\n\n* fix allreduce test ( sgl-project#4909 )\n\n* Support page size > 1 + eagle ( sgl-project#4908 )\n\n* Fix retract for page size > 1 ( sgl-project#4914 )\n\n* [Feature] use pytest for sgl-kernel ( sgl-project#4896 )\n\n* fix bmm fp8 ( sgl-project#4926 )\n\n* Fix the timeout for unit-test-2-gpu in pr-test.yml ( sgl-project#4927 )\n\n* Fix 2-gpu CI test and suppress some warnings ( sgl-project#4930 )\n\n* [feat] add fa3 in sgl-kernel ( sgl-project#4902 )\n\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\n\n* Fix sglang frontend's incorrect dependency on torch ( sgl-project#4931 )\n\n* [Fix] avoid stream sync and torch compile in prefill for fa3 backend ( sgl-project#4932 )\n\n* cleanup sgl-kernel ( sgl-project#4933 )\n\n* [Fix] Improve Lora tests and reduce CI runtime ( sgl-project#4925 )\n\n* Fix DeepSeek bug causing 2.2% MMLU drop when TP!=DP ( sgl-project#4883 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* [Fix] Add torch compile for torch.clamp back ( sgl-project#4936 )\n\n* Fix oom error for large page size ( sgl-project#4913 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* [feat] interface for platforms abstraction ( sgl-project#4928 )\n\n* [Fix] revert clean m.def for cudagraph ( sgl-project#4944 )\n\n* refactor: multimodal data ( sgl-project#4754 )\n\n* bump sgl-kernel v0.0.6 ( sgl-project#4950 )\n\n* [Build] Fix cuda12.8 build error in nvfp4_scaled_mm_kernels.cu ( sgl-project#4953 )\n\n* use fa3 in sgl-kernel ( sgl-project#4954 )\n\n* Revert PR 4764 & 4813 related to R1 RoPE ( sgl-project#4959 )\n\n* [Feature] Support DeepEP Low Latency ( sgl-project#4767 )\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* update bench_serving ( sgl-project#4958 )\n\n* Prevent memory leak of retract_decode when page_size > 1 ( sgl-project#4977 )\n\n* [VLM RLHF] Take Image input for verl vlm rollout ( sgl-project#4915 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nCo-authored-by: GeLee <leege233@gmail.com>\n\n* Large page size aligned hierarchical caching ( sgl-project#4581 )\n\n* bug fix for hicache host eviction ( sgl-project#4989 )\n\n* sgl scaled_fp8_quant support output padding ( sgl-project#4861 )\n\n* Add Eagle Speculative Decoding to FA3 Backend ( sgl-project#4951 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\n\n* Update tokenizer_manager.py ( sgl-project#5008 )\n\n* [sgl-kernel] per token group quant support COLUMN MAJOR ( sgl-project#4817 )\n\n* update cutlass tag ( sgl-project#5011 )\n\n* Feature/revise docs ci ( sgl-project#5009 )\n\n* fix: fix illegal cuda memory access at fused_moe_kernel ( sgl-project#4727 )\n\nCo-authored-by: yuethe <yuethe@tencent.com>\n\n* [Build] Support build sgl-kernel with ccache ( sgl-project#5020 )\n\n* fix deepgemm as well ( sgl-project#5030 )\n\n* try to fix ci oserror ( sgl-project#5024 )\n\n* Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5005 )\n\n* Small refactor DeepEPMode to clean up code a bit ( sgl-project#4992 )\n\n* [Fix] fix fa3 build at cu118 ( sgl-project#5036 )\n\n* Revert \"Replace enable_flashinfer_mla argument with attention_backend\" ( sgl-project#5048 )\n\n* bump sgl-kernel v0.0.7 ( sgl-project#5046 )\n\n* update eagle-3 docs ( sgl-project#4796 )\n\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\n\n* Add LlavaLlamaForCausaLM in MultiModal Processors ( sgl-project#5039 )\n\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\n\n* Update the retry count ( sgl-project#5051 )\n\n* upgrade sgl-kernel v0.0.7 ( sgl-project#5049 )\n\n* [2/3] fix dsv3 awq issue  ( sgl-project#4625 )\n\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\n\n* Feature/revise docs ci ( sgl-project#5056 )\n\n* Add H20 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5057 )\n\n* [fix] remove `cuda_device_count_stateless` ( sgl-project#5060 )\n\n* Small refactor DeepEPDispatcher into subclasses ( sgl-project#4994 )\n\n* Support async DeepEP by splitting into two stages ( sgl-project#4995 )\n\n* Cleanup unused resources after DeepEP operation ( sgl-project#4996 )\n\n* Add DeepSeek V3/R1 shared experts fusion ( sgl-project#4918 )\n\n* [deepep] fix: shared experts are not initialized when shared experts fusion is enabled ( sgl-project#5072 )\n\n* fix dummy-load deepseekv2 ( sgl-project#4535 )\n\n* support sgl-kernel on blackwell ( sgl-project#5074 )\n\n* FA3 Spec Decoding to support top k = 1 and add cuda graph support ( sgl-project#5050 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Chunan Zeng <zcnrex@gmail.com>\n\n* [Revision] Replace enable_flashinfer_mla argument with attention_backend ( sgl-project#5052 )\n\n* upgrade transformers 4.51.0 ( sgl-project#5088 )\n\n* sgl-kernel transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5079 )\n\n* bump sgl-kernel 0.0.8 ( sgl-project#5089 )\n\n* python transfer custom allreduce from trt kernel to vllm kernel ( sgl-project#5080 )\n\n* bump v0.4.4.post4 ( sgl-project#5091 )\n\n* Fix: Reduce the number of document ci attempts to avoid long ci running ( sgl-project#5097 )\n\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\n\n* Add Llama4 support ( sgl-project#5092 )\n\nCo-authored-by: Cheng Wan <cwan39@gatech.edu>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Fix refactor error - fp8.py ( sgl-project#5106 )\n\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\n\n* bump v0.4.5 ( sgl-project#5117 )\n\n* [ci] fix llama4 ci error ( sgl-project#5126 )\n\n* Refactor and Optimize FA3 Code ( sgl-project#5090 )\n\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\n\n* Add Llama4 user guide ( sgl-project#5133 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* [Misc] Use pytest.mark.skipif in sgl-kernel test ( sgl-project#5137 )\n\n* feat: disable grammar restrictions within reasoning sections ( sgl-project#4984 )\n\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\n\n* [modelopt] automatically inspect if model is ModelOpt quantized and set quantization method ( sgl-project#5145 )\n\n* [AMD] Fix missing per_token_group_quant_fp8 for ROCm ( sgl-project#5140 )\n\n* fix multimodal hash feature ( sgl-project#5083 )\n\n* Fix run time error in ROCm platform ( sgl-project#5147 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\n\n* [FA3 Feature] Support multi modal Llama-3.2-11B-Vision-Instruct ( sgl-project#5103 )\n\n* Add unit test on page_size > 1 and mla and  integration test for Flash Attention 3 ( sgl-project#4760 )\n\n* Use public model for FA3 speculative decode testing ( sgl-project#5152 )\n\n* Add dummy grok test to amd CI. ( sgl-project#5115 )\n\n* fix empty_cache error in pt_weights_iterator ( sgl-project#5151 )\n\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\n\n* Fix torch compile errors ( sgl-project#5158 )\n\n* Fix loading KV quantization scale; Enable modelopt kv cache ( sgl-project#4686 )\n\nCo-authored-by: qingquansong <ustcsqq@gmail.com>\n\n* [PD] Fix unclosed prefill connection warning of mini_lb ( sgl-project#5155 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\n\n* Add optimized native kernels in sgl-kernel ( sgl-project#5150 )\n\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\n\n* [PD] Simplify mini LB ( sgl-project#4911 )\n\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\n\n* Small improvement of native api docs ( sgl-project#5139 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* [feat&refactor] Enhance multimodal input support with refactor io_struct ( sgl-project#4938 )\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\n\n* Support 2x8xH100 for Llama 4 ( sgl-project#5159 )\n\n* FP4 weight loading and inference (2/2) ( sgl-project#3972 )\n\n* Fix multimodal hashing error ( sgl-project#5174 )\n\n* Tiny disable model that does not work ( sgl-project#5175 )\n\n* [Bugfix] Fix index out of bounds in local attention with large sequences ( sgl-project#5173 )\n\n* [Fix] DeepEP Compatibility with Low Latency ( sgl-project#5068 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* docs: remove the use of Downward API for LWS_WORKER_INDEX ( sgl-project#5110 )\n\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\n\n* feat: add DeepGEMM build warning ( sgl-project#5176 )\n\nCo-authored-by: grimoire <streetyao@live.com>\n\n* fix: use DeepEPDispatcher on CUDA ( sgl-project#5180 )\n\n* [DeepEP] fix: import buffer error ( sgl-project#5179 )\n\n* Let `bench_one_batch` support `enable_dp_attention` ( sgl-project#4058 )\n\n* [Misc] clean up vllm in sgl-kernel test ( sgl-project#5189 )\n\n* Fix ci test \"test_eval_fp8_accuracy\" failed ( sgl-project#5185 )\n\nCo-authored-by: wunhuang <wunhuang@amd.com>\n\n* Optimize topk operation in llama4 ( sgl-project#5128 )\n\n* Support Llama4 fp8 inference ( sgl-project#5194 )\n\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* [ci] fix ci test fused_moe op ( sgl-project#5102 )\n\n* model: support mllama4 ( sgl-project#5144 )\n\n* update grok test ( sgl-project#5171 )\n\n* sgl-kernel use cutlass latest version for fp8 blockwise gemm ( sgl-project#5207 )\n\n* Add H20 dtype fp8_w8a8 fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5196 )\n\n* fix: log warning when disable cuda graph ( sgl-project#5209 )\n\n* [metrics] Add in queue metrics ( sgl-project#4444 )\n\n* Fix DeepSeek error when using DeepEP mode ( sgl-project#5190 )\n\n* reduce moe_align_block_size_kernel small batch mode overhead ( sgl-project#5086 )\n\n* [PD] Support KV transfer with mooncake ( sgl-project#4880 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\n\n* [PD] Add get_contiguous_buf_infos interface for MLATokenToKVPool ( sgl-project#5204 )\n\n* Update deps for mllama4 ( sgl-project#5215 )\n\n* Fix deepseek-v3 with torch.compile in PyTorch 2.6. ( sgl-project#5213 )\n\n* ROCm sgl-kernel: compatible to later torch ( sgl-project#5167 )\n\n* [Misc] Clean sgl-kernel test  ( sgl-project#5216 )\n\n* Update Makefile / build script to avoid installing incompatible torch dependency ( sgl-project#5245 )\n\n* Fix torch.compile cacheing ( sgl-project#5259 )\n\nCo-authored-by: zhyncs <me@zhyncs.com>\n\n* ROCm/AITER CK_MoE: update 2-stage kernels & support both Activations ( sgl-project#5228 )\n\n* Optimize attention in llama4 ( sgl-project#5127 )\n\n* Optimize GPU memory usage in FlashAttentionBackend's strided indexing ( sgl-project#5262 )\n\nCo-authored-by: ch-wan <cwan39@gatech.edu>\n\n* Support `--enable-llama4-multimodal` ( sgl-project#5254 )\n\n* [fix] fix mrope positions not picked up ( sgl-project#5265 )\n\n* doc: nested loop code for offline engine ( sgl-project#5244 )\n\n* fix: examples for token_in_token_out_vlm  ( sgl-project#5193 )\n\n* Fix a 404 link in send_request.ipynb ( sgl-project#5280 )\n\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\n\n* fix: enable fp4 compilation on cu128 ( sgl-project#5286 )\n\n* feat: add cu128 identifier for sgl-kernel ( sgl-project#5287 )\n\n* chore: relax the torch version restriction for sgl-kernel compilation ( sgl-project#5288 )\n\n* chore: bump sgl-kernel v0.0.8.post1 ( sgl-project#5289 )\n\n* [PD] fix: skip warmup request in disaggregation mode to prevent crash on timeout ( sgl-project#5292 )\n\n* [Docs] Supported Model Docs - Major restructuring ( sgl-project#5290 )\n\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\n\n* fix: update update_wheel_index for cu128 ( sgl-project#5300 )\n\n* [Docs] Remove the older supported docs section ( sgl-project#5301 )\n\n* remove moe_align_block_size torch.zeros in small batch/expert mode ( sgl-project#5298 )\n\n* feat: add blackwell Dockerfile ( sgl-project#5302 )\n\n* feat: add blackwell workflow ( sgl-project#5303 )\n\n* fix: use fa3 unit test on hopper only ( sgl-project#5304 )\n\n* misc: update blackwell Dockerfile ( sgl-project#5306 )\n\n* fix: remove cublas_grouped_gemm ( sgl-project#5307 )\n\n* fix: update flash attn ( sgl-project#5308 )\n\n* fix: use deepgemm only on hopper ( sgl-project#5310 )\n\n* [VLM] Adopt fast image processor by default ( sgl-project#5065 )\n\n* Adjust ci test threshold ( sgl-project#5271 )\n\n* Blackwell Cutlass MLA kernel ( sgl-project#5142 )\n\n* misc: cleanup 3rdparty ( sgl-project#5311 )\n\n* update variable naming and comments for rocm ( sgl-project#5299 )\n\n* Fix w8a8_int8 model shared experts fusion load weights error ( sgl-project#5120 )\n\n* Add flash_attn_varlen_func to sgl-kernel ( sgl-project#5315 )\n\n* Fix fa3 window size setup ( sgl-project#5316 )\n\n* chore: bump sgl-kernel v0.0.8.post2 ( sgl-project#5317 )\n\n* feat: use fa3 mla by default on hopper ( sgl-project#5210 )\n\nCo-authored-by: yundai424 <yundai424@gmail.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* Fix: docs/backend/structured_outputs.ipynb ( sgl-project#4884 )\n\n* Delete python/sglang/srt/layers/moe/fused_moe_triton/configs/E=257,N=\u2026 ( sgl-project#5321 )\n\n* refine fused_moe tuning docs ( sgl-project#5294 )\n\n* Support server based rollout in Verlengine ( sgl-project#4848 )\n\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Chayenne <zhaochen20@outlook.com>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\n\n* [Feat] Add sparse attn to sgl-kernel ( sgl-project#5327 )\n\n* fix: solve cu118 issue for cutlass mla ( sgl-project#5331 )\n\n* chore: bump sgl-kernel v0.0.8.post3 ( sgl-project#5332 )\n\n* ci: update release node ( sgl-project#5333 )\n\n* fix: determine if flashinfer is installed ( sgl-project#5336 )\n\n* feat: adapt merge_state ( sgl-project#5337 )\n\n* misc: update sagemaker Dockerfile ( sgl-project#5341 )\n\n* Fix: Ensure tensors for dist.broadcast match NCCL backend device ( sgl-project#5322 )\n\n* docs: update adoption and sponsorship list with Oracle ( sgl-project#5343 )\n\n* chore: upgrade sgl-kernel 0.0.8.post3 ( sgl-project#5342 )\n\n* Fix typo: infight -> inflight ( sgl-project#5357 )\n\n* [PD] Add transfer backend abstraction ( sgl-project#5328 )\n\n* fix MLATokenToKVPoolHost get_size_per_token bug ( sgl-project#5161 )\n\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\n\n* fix sgl-project#5322 ( sgl-project#5359 )\n\n* feat: update experiment_runner ( sgl-project#5360 )\n\n* [DeepEP] Reduce routed scaling overhead ( sgl-project#5277 )\n\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Free metadata_buffer_index after transfer finished ( sgl-project#5364 )\n\n* Fix DeepSeek DP Attention + torch compile ( sgl-project#5367 )\n\nCo-authored-by: ispobock <ispobaoke@163.com>\n\n* Support for Qwen2.5-VL Model in bitsandbytes Format ( sgl-project#5003 )\n\n* Fix PD disaggregation bugs ( sgl-project#5326 )\n\n* [PD Bug] fix  MLA get_contiguous_buf_infos error ( sgl-project#5384 )\n\n* [perf] experimental enhance fp8 per-tensor quant ( sgl-project#5370 )\n\n* Apply deepseek cuda rope ( sgl-project#5385 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* apply fused moe gate in ds v3/r1 ( sgl-project#5371 )\n\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\n\n* fix: update test config ( sgl-project#5392 )\n\n* [Fix] Turn off DeepGEMM by default ( sgl-project#5263 )\n\n* minor clean up of sgl-kernel/CMakeLists.txt ( sgl-project#5393 )\n\n* Add A800 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5368 )\n\n* Add H20 dtype fp8_w8a8 shared experts fused MoE kernel tuning configs for DeepSeek V3/R1 ( sgl-project#5291 )\n\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\n\n* [fix/misc] remove duplicate row in deepseek v2 model ( sgl-project#5279 )\n\n* chore: upgrade DeepGEMM ( sgl-project#5395 )\n\n* fix: update pr-test-sgl-kernel ( sgl-project#5399 )\n\n* kernel: support slightly faster merge_state_v2 cuda kernel ( sgl-project#5381 )\n\n* chore: bump sgl-kernel 0.0.9 ( sgl-project#5400 )\n\n* chore: upgrade sgl-kernel 0.0.9 ( sgl-project#5401 )\n\n* Tiny fix DeepseekScalingRotaryEmbedding always use forward_native ( sgl-project#5406 )\n\n* Fix bench_serving with random-ids ( sgl-project#5214 )\n\n* [misc] fix ci flaky case ( sgl-project#5352 )\n\n* [FIX] Fix concatenation error in capture_bs when open --disable-cuda-graph-padding and without MTP ( sgl-project#5412 )\n\n* Support dynamic connection and TP 16 ( sgl-project#5351 )\n\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\n\n* Fix broadcast use cuda device lead to memory capacity unbalanced ( sgl-project#5416 )\n\n* [PD] Fix dynamic port support and MLA buffer for Mooncake ( sgl-project#5415 )\n\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\n\n* Distinguish bootstrap key only in decode server ( sgl-project#5422 )\n\n* [PD] Remove unused bootstrap param and fix port table type ( sgl-project#5423 )\n\n* [minor] cleanup cmakelists.txt ( sgl-project#5420 )\n\n* bugfix: fix merge_state_v2 cuda graph ( sgl-project#5419 )\n\n* chore: bump sgl-kernel v0.0.9.post1 ( sgl-project#5430 )\n\n* fix: solve release issue ( sgl-project#5434 )\n\n* BLackwell cutlass mla: Add check for bad page size/block num combinations ( sgl-project#5431 )\n\n* feat: update model_specific_adjustment ( sgl-project#5344 )\n\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\n\n* chore: upgrade sgl-kernel 0.0.9.post1 ( sgl-project#5436 )\n\n* Fix ignore_eos parameter when loading a chat template ( sgl-project#5264 )\n\n* add attention backend supporting matrix in the doc ( sgl-project#5211 )\n\nCo-authored-by: Stefan He <hebiaobuaa@gmail.com>\n\n* Support BNB quantization for llama/mllama ( sgl-project#5038 )\n\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com>\n\n* [Docs] Update start/install.md ( sgl-project#5398 )\n\n* [Minor] Move torch.compile patch to a better place ( sgl-project#5397 )\n\n* [Bug fix] need record start time in pd mode ( sgl-project#5425 )\n\n* Support MHA with chunked prefix cache for DeepSeek chunked prefill ( sgl-project#5113 )\n\n* chore: bump v0.4.5.post1 ( sgl-project#5445 )\n\n* Revert \"[SW-226289] rebase sglang to tag v0.4.5 ( sgl-project#12 )\"\n\nThis reverts commit 0eac714 .\n\n---------\n\nSigned-off-by: Xinyuan Tong <justinning0323@outlook.com>\nSigned-off-by: Shangming Cai <caishangming@linux.alibaba.com>\nSigned-off-by: Kay Yan <kay.yan@daocloud.io>\nSigned-off-by: windsonsea <haifeng.yao@daocloud.io>\nCo-authored-by: fzyzcjy <5236035+fzyzcjy@users.noreply.github.com>\nCo-authored-by: Lianmin Zheng <lianminzheng@gmail.com>\nCo-authored-by: Juwan Yoo <ryan@tmfi.us>\nCo-authored-by: Qingquan Song <ustcsqq@gmail.com>\nCo-authored-by: Yineng Zhang <me@zhyncs.com>\nCo-authored-by: chaobo jia <91889375+jcbjcbjc@users.noreply.github.com>\nCo-authored-by: rudy152 <czh1137892874@gmail.com>\nCo-authored-by: Fr4nk1in <sh.fu@outlook.com>\nCo-authored-by: yinfan98 <1106310035@qq.com>\nCo-authored-by: Baizhou Zhang <sobereddiezhang@gmail.com>\nCo-authored-by: Ke Bao <ISPObaoke@163.com>\nCo-authored-by: Yi Zhang <1109276519@qq.com>\nCo-authored-by: Adarsh Shirawalmath <114558126+adarshxs@users.noreply.github.com>\nCo-authored-by: Sleepcoo <Sleepcoo@gmail.com>\nCo-authored-by: SEPLOS <seplos@aliyun.com>\nCo-authored-by: ch-wan <cwan39@gatech.edu>\nCo-authored-by: Zhiqiang Xie <xiezhq@stanford.edu>\nCo-authored-by: JieXin Liang <Alcanderian@users.noreply.github.com>\nCo-authored-by: Mick <mickjagger19@icloud.com>\nCo-authored-by: Yuhong Guo <yuhong.gyh@antgroup.com>\nCo-authored-by: Jinyan Chen <93358689+liz-badada@users.noreply.github.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: XinyuanTong <115166877+JustinTong0323@users.noreply.github.com>\nCo-authored-by: GeLee <leege233@gmail.com>\nCo-authored-by: Xiaoyu Zhang <35585791+BBuf@users.noreply.github.com>\nCo-authored-by: hebiao064 <hebiaobuaa@gmail.com>\nCo-authored-by: zcnrex <zcnrex@gmail.com>\nCo-authored-by: Kaiyu Yang <yangky@umich.edu>\nCo-authored-by: renxin <90580890+renxinx@users.noreply.github.com>\nCo-authored-by: saltyfish66 <38240284+saltyfish66@users.noreply.github.com>\nCo-authored-by: yuethe <yuethe@tencent.com>\nCo-authored-by: simveit <69345428+simveit@users.noreply.github.com>\nCo-authored-by: Yifan Zhang <zhangyif21@mails.tsinghua.edu.cn>\nCo-authored-by: Ravi Theja <ravi03071991@gmail.com>\nCo-authored-by: Ravi Theja Desetty <ravitheja@Ravis-MacBook-Pro.local>\nCo-authored-by: AniZpZ <zhuangsen.zp@antgroup.com>\nCo-authored-by: \u665f\u6d77 <huangtingwei.htw@antgroup.com>\nCo-authored-by: Tommy Yang <tommyyang0524@gmail.com>\nCo-authored-by: Cheng Wan <54331508+ch-wan@users.noreply.github.com>\nCo-authored-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: mlmz <54172054+minleminzui@users.noreply.github.com>\nCo-authored-by: shuaills <shishuaiuoe@gmail.com>\nCo-authored-by: Chang Su <chang.s.su@oracle.com>\nCo-authored-by: fzyzcjy <ch271828n@outlook.com>\nCo-authored-by: HAI <hixiao@gmail.com>\nCo-authored-by: tianhaoyu <thy@mail.ecust.edu.cn>\nCo-authored-by: DarkSharpness <2040703891@qq.com>\nCo-authored-by: Yun Dai <yundai424@gmail.com>\nCo-authored-by: Hubert Lu <55214931+hubertlu-tw@users.noreply.github.com>\nCo-authored-by: huangtingwei <141888744+huangtingwei9988@users.noreply.github.com>\nCo-authored-by: kk <43161300+kkHuang-amd@users.noreply.github.com>\nCo-authored-by: wunhuang <wunhuang@amd.com>\nCo-authored-by: root <root@dell300x-pla-t10-17.pla.dcgpu>\nCo-authored-by: Yubo Wang <yubowang2019@gmail.com>\nCo-authored-by: saienduri <saimanas.enduri@amd.com>\nCo-authored-by: DangKai <dangkai4u@outlook.com>\nCo-authored-by: dangkai.dk <dangkai.dk@alibaba-inc.com>\nCo-authored-by: shangmingc <csmthu@gmail.com>\nCo-authored-by: Ma Mingfei <mingfei.ma@intel.com>\nCo-authored-by: Chunyuan WU <chunyuan.wu@intel.com>\nCo-authored-by: YanbingJiang <yanbing.jiang@intel.com>\nCo-authored-by: blzheng <beilei.zheng@intel.com>\nCo-authored-by: Byron Hsu <byronhsu1230@gmail.com>\nCo-authored-by: Liangsheng Yin <hnyls2002@gmail.com>\nCo-authored-by: zhaochenyang20 <zhaochen20@outlook.com>\nCo-authored-by: Trevor Morris <tmorris@nvidia.com>\nCo-authored-by: Kay Yan <kay.yan@daocloud.io>\nCo-authored-by: grimoire <streetyao@live.com>\nCo-authored-by: HandH1998 <1335248067@qq.com>\nCo-authored-by: Zhaoyang Hao <77828610+Muuuchen@users.noreply.github.com>\nCo-authored-by: Teng Ma <805522925@qq.com>\nCo-authored-by: Shangming Cai <caishangming@linux.alibaba.com>\nCo-authored-by: Xuchun Shang <xuchun.shang@linux.alibaba.com>\nCo-authored-by: Richard Zou <zou3519@users.noreply.github.com>\nCo-authored-by: Elfie Guo <164945471+elfiegg@users.noreply.github.com>\nCo-authored-by: Michael Yao <haifeng.yao@daocloud.io>\nCo-authored-by: Yusong Gao <yusong.gao@icloud.com>\nCo-authored-by: Zhaoyi Li <36555117+Lzy17@users.noreply.github.com>\nCo-authored-by: lambert0312 <lambert80.ios@gmail.com>\nCo-authored-by: tianlian yi <91449279+yitianlian@users.noreply.github.com>\nCo-authored-by: Jin Pan <jpan236@wisc.edu>\nCo-authored-by: Jinn <47354855+jhinpan@users.noreply.github.com>\nCo-authored-by: yulei <yuulei12@gmail.com>\nCo-authored-by: Yongtong Wu <914554688@qq.com>\nCo-authored-by: yhyang201 <47235274+yhyang201@users.noreply.github.com>\nCo-authored-by: ybyang <10629930+whybeyoung@users.noreply.github.com>\nCo-authored-by: Ximingwang-09 <72070413+Ximingwang-09@users.noreply.github.com>\nCo-authored-by: ximing.wxm <ximing.wxm@antgroup.com>\nCo-authored-by: Yangcheng Li <bluebluelitchi@hotmail.com>\nCo-authored-by: DefTruth <31974251+DefTruth@users.noreply.github.com>\nCo-authored-by: Yuan Luo <yuan.luo@hotmail.com>\nCo-authored-by: luoyuan.luo <luoyuan.luo@antgroup.com>\nCo-authored-by: ybyang <ybyang7@iflytek.com>\nCo-authored-by: mRSun15 <3150105645@zju.edu.cn>\nCo-authored-by: ryang <38470282+ryang-max@users.noreply.github.com>\nCo-authored-by: Yuhao Yang <yyh073@foxmail.com> Copy link ZhongYingMatrix commented Apr 24, 2025 2/3. Auto mode do not support DP Attention, remove --enable-dp-attention and --dp , Also for auto mode, --disable-cuda-graph could be removed, should add --cuda-graph-max-bs 128 Hi @liz-badada Thank you for your excellent work! May I ask why the Auto mode does not support DP Attention? Is it because different ranks might be in the prefill/decode phase simultaneously? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:59:06",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": true,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | SERVING | TEST",
  "analysis_extracted_at": null,
  "models": [
    "deepseek-ai/DeepSeek-V3"
  ],
  "lm_eval_commands": null,
  "perf_command": "python3 -m sglang.bench_serving --backend sglang --dataset-name random --num-prompt 512 --random-input 1000 --random-output 1000 --random-range-ratio 1 --host 127.0.0.1 --port 30000 --max-concurrency 128",
  "commit_subject": "[Feature] Support DeepEP Low Latency (#4767)",
  "commit_message": "[Feature] Support DeepEP Low Latency (#4767)\n\nCo-authored-by: sleepcoo <sleepcoo@gmail.com>\nCo-authored-by: laixinn <xielx@shanghaitech.edu.cn>\nCo-authored-by: ch-wan <cwan39@gatech.edu>",
  "commit_date": "2025-04-01T09:23:25-07:00",
  "files_changed": [
    "docs/backend/server_arguments.md",
    "python/sglang/srt/layers/moe/ep_moe/kernels.py",
    "python/sglang/srt/layers/moe/ep_moe/layer.py",
    "python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py",
    "python/sglang/srt/managers/schedule_batch.py",
    "python/sglang/srt/model_executor/model_runner.py",
    "python/sglang/srt/models/deepseek_v2.py",
    "python/sglang/srt/server_args.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 666,
    "num_files": 8,
    "num_hunks": 30,
    "num_non_test_edited_lines": 666,
    "num_non_test_files": 8,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/docs/backend/server_arguments.md b/docs/backend/server_arguments.md\nindex 3d2aae8f2..3c96a6816 100644\n--- a/docs/backend/server_arguments.md\n+++ b/docs/backend/server_arguments.md\n@@ -91,6 +91,7 @@ Please consult the documentation below to learn more about the parameters you ma\n * `enable_ep_moe`: Enables expert parallelism that distributes the experts onto multiple GPUs for MoE models.\n * `ep_size`: The size of EP. Please shard the model weights with `tp_size=ep_size`, for detailed benchmarking refer to [this PR](https://github.com/sgl-project/sglang/pull/2203). If not set, `ep_size` will be automatically set to `tp_size`.\n * `enable_deepep_moe`: Enables expert parallelism that distributes the experts onto multiple GPUs for DeepSeek-V3 model based on deepseek-ai/DeepEP.\n+* `deepep_mode`: Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or `auto`. Default is `auto`, which means `low_latency` for decode batch and `normal` for prefill batch.\n \n ## Memory and scheduling\n \ndiff --git a/python/sglang/srt/layers/moe/ep_moe/kernels.py b/python/sglang/srt/layers/moe/ep_moe/kernels.py\nindex 30c9eb6a7..3ea6b4b2f 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/kernels.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/kernels.py\n@@ -244,6 +244,148 @@ def silu_and_mul_triton_kernel(\n             tl.store(down_input_ptr + offset, silu_mul_output, mask=mask)\n \n \n+# copy from https://github.com/ModelTC/lightllm/blob/a000ab69098654df4731f5b12587dd4e7f0a4f41/lightllm/common/fused_moe/moe_silu_and_mul_mix_quant_ep.py\n+@triton.jit\n+def _silu_and_mul_post_quant_kernel(\n+    input_ptr,\n+    stride_input_0,\n+    stride_input_1,\n+    stride_input_2,\n+    output_ptr,\n+    stride_output_0,\n+    stride_output_1,\n+    stride_output_2,\n+    output_scale_ptr,\n+    stride_output_scale_0,\n+    stride_output_scale_1,\n+    stride_output_scale_2,\n+    masked_m_ptr,\n+    size_n,\n+    fp8_max,\n+    fp8_min,\n+    BLOCK_N: tl.constexpr,\n+    NUM_STAGE: tl.constexpr,\n+):\n+    expert_id = tl.program_id(2)\n+    token_id = tl.program_id(1)\n+    hidden_dim_block_index = tl.program_id(0)\n+\n+    block_num_per_expert = tl.num_programs(1)\n+\n+    token_num_cur_expert = tl.load(masked_m_ptr + expert_id)\n+\n+    stride_input_0 = tl.cast(stride_input_0, dtype=tl.int64)\n+    stride_output_0 = tl.cast(stride_output_0, dtype=tl.int64)\n+    stride_input_1 = tl.cast(stride_input_1, dtype=tl.int64)\n+    stride_output_1 = tl.cast(stride_output_1, dtype=tl.int64)\n+\n+    offs_in_d = hidden_dim_block_index * BLOCK_N + tl.arange(0, BLOCK_N)\n+    input_ptr_offs = input_ptr + expert_id * stride_input_0 + offs_in_d\n+    output_ptr_offs = output_ptr + expert_id * stride_output_0 + offs_in_d\n+    output_scale_offs = (\n+        output_scale_ptr\n+        + expert_id * stride_output_scale_0\n+        + hidden_dim_block_index * stride_output_scale_2\n+    )\n+\n+    for token_index in tl.range(\n+        token_id, token_num_cur_expert, block_num_per_expert, num_stages=NUM_STAGE\n+    ):\n+        gate = tl.load(\n+            input_ptr_offs + token_index * stride_input_1,\n+            mask=offs_in_d < size_n,\n+            other=0.0,\n+        ).to(tl.float32)\n+        up = tl.load(\n+            input_ptr_offs + token_index * stride_input_1 + size_n,\n+            mask=offs_in_d < size_n,\n+            other=0.0,\n+        )\n+        gate = gate / (1 + tl.exp(-gate))\n+        gate = gate.to(input_ptr.dtype.element_ty)\n+        gate_up = up * gate\n+        _absmax = tl.maximum(tl.max(tl.abs(gate_up)), 1e-10)\n+        output_s = _absmax / fp8_max\n+        output_q = tl.clamp(gate_up / output_s, fp8_min, fp8_max).to(\n+            output_ptr.dtype.element_ty\n+        )\n+        tl.store(\n+            output_ptr_offs + token_index * stride_output_1,\n+            output_q,\n+            mask=offs_in_d < size_n,\n+        )\n+        tl.store(\n+            output_scale_offs + token_index * stride_output_scale_1,\n+            output_s,\n+        )\n+\n+\n+def silu_and_mul_masked_post_quant_fwd(\n+    input: torch.Tensor,\n+    output: torch.Tensor,\n+    output_scale: torch.Tensor,\n+    quant_group_size: int,\n+    masked_m: torch.Tensor,\n+):\n+    \"\"\"\n+    input shape [expert_num, token_num_padded, hidden_dim]\n+    output shape [expert_num, token_num_padded, hidden_dim // 2], dtype fp8\n+    output_scale [expert_num token_num_paddded, hidden_dim // 2 // 128] dtype float32\n+    quant_group_size  int,\n+    masked_m shape [expert_num],\n+    \"\"\"\n+\n+    assert input.is_contiguous()\n+    assert output.dtype == torch.float8_e4m3fn\n+    assert output.is_contiguous()\n+    assert len(input.shape) == 3\n+    assert input.shape[0] == masked_m.shape[0]\n+    assert input.shape[-1] % 2 == 0\n+\n+    size_n = input.shape[-1] // 2\n+    assert size_n % quant_group_size == 0\n+\n+    expert_num = len(masked_m)\n+\n+    if expert_num < 4:\n+        BLOCK_NUM_PER_EXPERT = 64\n+    else:\n+        BLOCK_NUM_PER_EXPERT = 32\n+\n+    BLOCK_N = quant_group_size\n+    num_warps = 1\n+    NUM_STAGES = 6\n+    hidden_dim_split_block_num = triton.cdiv(size_n, BLOCK_N)\n+    assert BLOCK_N % quant_group_size == 0\n+\n+    grid = (\n+        hidden_dim_split_block_num,\n+        BLOCK_NUM_PER_EXPERT,\n+        expert_num,\n+    )\n+\n+    finfo = torch.finfo(torch.float8_e4m3fn)\n+    fp8_max = finfo.max\n+    fp8_min = -fp8_max\n+\n+    _silu_and_mul_post_quant_kernel[grid](\n+        input,\n+        *input.stride(),\n+        output,\n+        *output.stride(),\n+        output_scale,\n+        *output_scale.stride(),\n+        masked_m,\n+        size_n,\n+        fp8_max,\n+        fp8_min,\n+        BLOCK_N=BLOCK_N,\n+        NUM_STAGE=NUM_STAGES,\n+        num_warps=num_warps,\n+    )\n+    return\n+\n+\n @triton.jit\n def tanh(x):\n     return 2 * tl.sigmoid(2 * x) - 1\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/layer.py b/python/sglang/srt/layers/moe/ep_moe/layer.py\nindex f0595bfb1..814dc469e 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/layer.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/layer.py\n@@ -3,12 +3,16 @@ from typing import Callable, List, Optional, Tuple\n \n import torch\n \n-# TODO: use deep_gemm masked kernel after low latency dispatch\n-# import deep_gemm\n-# from deep_gemm import (\n-#     get_col_major_tma_aligned_tensor,\n-#     m_grouped_gemm_fp8_fp8_bf16_nt_masked,\n-# )\n+try:\n+    from deep_gemm import (\n+        get_col_major_tma_aligned_tensor,\n+        m_grouped_gemm_fp8_fp8_bf16_nt_masked,\n+    )\n+\n+    use_deep_gemm = True\n+except ImportError:\n+    use_deep_gemm = False\n+\n from torch.nn import Module\n \n from sglang.srt.custom_op import CustomOp\n@@ -22,6 +26,7 @@ from sglang.srt.layers.moe.ep_moe.kernels import (\n     post_reorder_triton_kernel,\n     pre_reorder_triton_kernel,\n     run_moe_ep_preproess,\n+    silu_and_mul_masked_post_quant_fwd,\n     silu_and_mul_triton_kernel,\n )\n from sglang.srt.layers.moe.fused_moe_triton import FusedMoeWeightScaleSupported\n@@ -809,6 +814,7 @@ class DeepEPMoE(EPMoE):\n         correction_bias: Optional[torch.Tensor] = None,\n         custom_routing_function: Optional[Callable] = None,\n         activation: str = \"silu\",\n+        deepep_mode: str = \"auto\",\n     ):\n         super().__init__(\n             num_experts,\n@@ -827,21 +833,41 @@ class DeepEPMoE(EPMoE):\n             custom_routing_function,\n             activation,\n         )\n+        self.deepep_mode = deepep_mode\n+        if self.deepep_mode in [\"low_latency\", \"auto\"]:\n+            assert use_deep_gemm, f\"DeepEP {self.deepep_mode} mode requires deep_gemm\"\n+        self.w13_weight_fp8 = (\n+            self.w13_weight,\n+            (\n+                self.w13_weight_scale_inv\n+                if self.use_block_quant\n+                else self.w13_weight_scale\n+            ),\n+        )\n+        self.w2_weight_fp8 = (\n+            self.w2_weight,\n+            self.w2_weight_scale_inv if self.use_block_quant else self.w2_weight_scale,\n+        )\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         reorder_topk_ids: torch.Tensor,\n         seg_indptr: torch.Tensor,\n+        masked_m: torch.Tensor,\n+        expected_m: int,\n         forward_mode: ForwardMode,\n     ):\n-        # Todo: use m_grouped_gemm_fp8_fp8_bf16_nt_masked after low_latency dispatch (decode)\n-        if True:  # not forward_mode.is_decode():\n+        if self.deepep_mode == \"normal\" or (\n+            self.deepep_mode == \"auto\" and not forward_mode.is_decode()\n+        ):\n             return self.forward_normal(hidden_states, reorder_topk_ids, seg_indptr)\n+        elif self.deepep_mode == \"low_latency\" or (\n+            self.deepep_mode == \"auto\" and forward_mode.is_decode()\n+        ):\n+            return self.forward_deepgemm_masked(hidden_states, masked_m, expected_m)\n         else:\n-            return self.forward_deepgemm_masked(\n-                hidden_states, reorder_topk_ids, seg_indptr\n-            )\n+            raise ValueError(f\"Invalid deepep_mode: {self.deepep_mode}\")\n \n     def forward_normal(\n         self,\n@@ -958,89 +984,66 @@ class DeepEPMoE(EPMoE):\n \n     def forward_deepgemm_masked(\n         self,\n-        hidden_states: torch.Tensor,\n-        reorder_topk_ids: torch.Tensor,\n-        seg_indptr: torch.Tensor,\n+        hidden_states_fp8: Tuple[torch.Tensor, torch.Tensor],\n+        masked_m: torch.Tensor,\n+        expected_m: int,\n     ):\n         assert self.quant_method is not None\n         assert self.activation == \"silu\"\n-\n-        if self.activation_scheme == \"dynamic\" and not self.use_block_quant:\n-            max_value = (\n-                torch.max(hidden_states)\n-                .repeat(self.num_experts_per_partition)\n-                .to(torch.float32)\n-            )\n-            self.w13_input_scale = max_value / torch.finfo(self.fp8_dtype).max\n+        assert (\n+            hidden_states_fp8[0].size(0) % 4 == 0\n+        ), f\"TMA alignment error: {hidden_states_fp8[0].size(0)}\"\n \n         # GroupGemm-0\n+        num_groups, m, k = hidden_states_fp8[0].size()\n+        n = self.w13_weight.size(1)\n+        expected_m = min(expected_m, m)\n         gateup_output = torch.empty(\n-            hidden_states.shape[0],\n-            self.w13_weight.shape[1],\n-            device=hidden_states.device,\n-            dtype=hidden_states.dtype,\n+            (num_groups, m, n), device=hidden_states_fp8[0].device, dtype=torch.bfloat16\n+        )\n+        m_grouped_gemm_fp8_fp8_bf16_nt_masked(\n+            hidden_states_fp8, self.w13_weight_fp8, gateup_output, masked_m, expected_m\n         )\n-        if hidden_states.shape[0] > 0:\n-            # Transpose earlier so that the testing will not trigger transposing kernels\n-            hidden_states = (\n-                hidden_states[0],\n-                get_col_major_tma_aligned_tensor(hidden_states[1]),\n-            )\n-            \"\"\"\n-            gateup_output = deep_gemm.m_grouped_gemm_fp8_fp8_bf16_nt_masked(\n-                hidden_states, self.w13_weight, out, masked_m, expected_m\n-            )\n-            \"\"\"\n \n         # Act\n         down_input = torch.empty(\n-            gateup_output.shape[0],\n-            gateup_output.shape[1] // 2,\n-            device=gateup_output.device,\n-            dtype=(\n-                self.fp8_dtype\n-                if (self.use_fp8_w8a8 and not self.use_block_quant)\n-                else hidden_states.dtype\n+            (\n+                gateup_output.shape[0],\n+                gateup_output.shape[1],\n+                gateup_output.shape[2] // 2,\n             ),\n+            device=gateup_output.device,\n+            dtype=self.fp8_dtype,\n         )\n-        if self.w2_input_scale is None and not self.use_block_quant:\n-            self.w2_input_scale = torch.ones(\n-                self.num_experts_per_partition,\n-                dtype=torch.float32,\n-                device=hidden_states.device,\n-            )\n-\n-        if self.activation == \"silu\":\n-            silu_and_mul_triton_kernel[(gateup_output.shape[0],)](\n-                gateup_output,\n-                down_input,\n+        scale_block_size = 128\n+        down_input_scale = torch.empty(\n+            (\n+                gateup_output.shape[0],\n                 gateup_output.shape[1],\n-                reorder_topk_ids,\n-                self.w2_input_scale,\n-                0,\n-                self.num_experts_per_partition - 1,\n-                BLOCK_SIZE=512,\n-            )\n-        else:\n-            raise ValueError(f\"Unsupported activation: {self.activation=}\")\n+                gateup_output.shape[2] // 2 // scale_block_size,\n+            ),\n+            device=gateup_output.device,\n+            dtype=torch.float32,\n+        )\n+        silu_and_mul_masked_post_quant_fwd(\n+            gateup_output,\n+            down_input,\n+            down_input_scale,\n+            scale_block_size,\n+            masked_m,\n+        )\n \n         # GroupGemm-1\n+        n = self.w2_weight.size(1)\n+        down_input_fp8 = (\n+            down_input,\n+            get_col_major_tma_aligned_tensor(down_input_scale),\n+        )\n         down_output = torch.empty(\n-            down_input.shape[0],\n-            self.w2_weight.shape[1],\n-            device=hidden_states.device,\n-            dtype=hidden_states.dtype,\n+            (num_groups, m, n), device=down_input.device, dtype=torch.bfloat16\n+        )\n+        m_grouped_gemm_fp8_fp8_bf16_nt_masked(\n+            down_input_fp8, self.w2_weight_fp8, down_output, masked_m, expected_m\n         )\n-        if down_input.shape[0] > 0:\n-            # Transpose earlier so that the testing will not trigger transposing kernels\n-            down_input = (\n-                down_input[0],\n-                get_col_major_tma_aligned_tensor(down_input[1]),\n-            )\n-            \"\"\"\n-            down_output = deep_gemm.m_grouped_gemm_fp8_fp8_bf16_nt_masked(\n-                down_input, self.w2_weight, out, masked_m, expected_m\n-            )\n-            \"\"\"\n \n         return down_output\ndiff --git a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\nindex 6b67f6cea..f4e673535 100644\n--- a/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n+++ b/python/sglang/srt/layers/moe/ep_moe/token_dispatcher.py\n@@ -76,8 +76,7 @@ def get_buffer_low_latency(\n         assert num_experts % group.size() == 0\n         _buffer_low_latency = Buffer(\n             group,\n-            0,\n-            num_rdma_bytes,\n+            num_rdma_bytes=num_rdma_bytes,\n             low_latency_mode=True,\n             num_qps_per_rank=num_experts // group.size(),\n         )\n@@ -95,62 +94,63 @@ class DeepEPDispatcher:\n         group: torch.distributed.ProcessGroup,\n         router_topk: int,\n         permute_fusion: bool = False,\n-        capacity_factor: float = None,\n         num_experts: int = None,\n         num_local_experts: int = None,\n         hidden_size: int = None,\n         params_dtype: torch.dtype = None,\n+        deepep_mode: str = \"auto\",\n         async_finish: bool = False,\n+        return_recv_hook: bool = False,\n     ):\n+        if not use_deepep:\n+            raise ImportError(\n+                \"DeepEP is not installed. Please install DeepEP package from \"\n+                \"https://github.com/deepseek-ai/deepep.\"\n+            )\n+\n         self.group = group\n         self.router_topk = router_topk\n-        self.capacity_factor = capacity_factor\n         self.permute_fusion = permute_fusion\n         self.num_experts = num_experts\n         self.num_local_experts = num_local_experts\n         self.hidden_size = hidden_size\n-        self.recv_expert_count = None\n         self.params_dtype = params_dtype\n         self.params_bytes = 2\n-        # Metadata\n-        self.token_indices = None\n-        self.token_probs = None\n-        # Handle used for combine operation\n-        self.handle = None\n-        self.async_finish = async_finish\n \n-        # `num_max_dispatch_tokens_per_rank` (the actual batch size in the decoding engine) should be less than 256\n-        # https://github.com/deepseek-ai/DeepEP?tab=readme-ov-file#example-use-in-inference-decoding\n-        self.num_max_dispatch_tokens_per_rank = 128\n+        self.deepep_mode = deepep_mode\n+        self.handle = None\n \n-        if not use_deepep:\n-            raise ImportError(\n-                \"DeepEP is not installed. Please install DeepEP package from \"\n-                \"https://github.com/deepseek-ai/deepep.\"\n+        if self.deepep_mode in [\"normal\", \"auto\"]:  # for normal / auto mode\n+            self.buffer_normal = get_buffer_normal(\n+                self.group, self.hidden_size * self.params_bytes\n             )\n-        self.buffer_normal = get_buffer_normal(\n-            self.group, self.hidden_size * self.params_bytes\n-        )\n-        self.buffer_low_latency = None\n-        # Todo: enable low latency dispatch\n-        \"\"\"\n-        self.buffer_low_latency = get_buffer_low_latency(\n-            self.group,\n-            self.num_max_dispatch_tokens_per_rank,\n-            self.hidden_size * self.params_bytes,\n-            self.num_experts,\n-        )\n-        \"\"\"\n+            self.async_finish = async_finish\n+            self.src2dst = None\n+        if self.deepep_mode in [\"low_latency\", \"auto\"]:  # for low_latency / auto mode\n+            \"\"\"\n+            num_max_dispatch_tokens_per_rank: the actual batch size in the decoding engine should be less than 256\n+            https://github.com/deepseek-ai/DeepEP?tab=readme-ov-file#example-use-in-inference-decoding\n+            \"\"\"\n+            # TODO(ch-wan): allow users to set this value\n+            self.num_max_dispatch_tokens_per_rank = 128\n+            self.buffer_low_latency = get_buffer_low_latency(\n+                self.group,\n+                self.num_max_dispatch_tokens_per_rank,\n+                self.hidden_size,\n+                self.num_experts,\n+            )\n+            self.return_recv_hook = return_recv_hook\n \n     def deepep_permute(\n         self,\n-        hidden_states,\n-        fp8_dtype=None,\n-        use_fp8_w8a8=False,\n-        use_block_quant=False,\n+        hidden_states: torch.Tensor,\n+        topk_idx: torch.Tensor,\n+        fp8_dtype: Optional[torch.dtype] = None,\n+        use_fp8_w8a8: bool = False,\n+        use_block_quant: bool = False,\n     ):\n-        reorder_topk_ids, src2dst, seg_indptr = deepep_run_moe_deep_preprocess(\n-            self.topk_idx, self.num_experts\n+        reorder_topk_ids, self.src2dst, seg_indptr = deepep_run_moe_deep_preprocess(\n+            topk_idx, self.num_experts\n         )\n         num_total_tokens = reorder_topk_ids.numel()\n         gateup_input = torch.empty(\n@@ -166,14 +166,13 @@ class DeepEPDispatcher:\n         deepep_permute_triton_kernel[(hidden_states.shape[0],)](\n             hidden_states,\n             gateup_input,\n-            src2dst,\n-            self.topk_idx,\n+            self.src2dst,\n+            topk_idx,\n             None,\n             self.router_topk,\n             hidden_states.shape[1],\n             BLOCK_SIZE=512,\n         )\n-        self.src2dst = src2dst\n         return reorder_topk_ids, seg_indptr, gateup_input\n \n     def dispatch(\n@@ -182,54 +181,64 @@ class DeepEPDispatcher:\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n         num_experts: int,\n-        forward_mode: ForwardMode,\n         num_max_dispatch_tokens_per_rank: int = 128,\n-    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        forward_mode: ForwardMode = None,\n+    ) -> Tuple:\n         topk_idx = topk_idx.to(torch.int64)\n-        # Todo: enable low latency dispatch\n-        if True:  # not forward_mode.is_decode():\n+        reorder_topk_ids = torch.empty(\n+            (0,), device=hidden_states.device, dtype=torch.int64\n+        )\n+        seg_indptr = torch.zeros(\n+            (num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n+        )\n+        masked_m = torch.empty(\n+            (self.num_local_experts,), device=hidden_states.device, dtype=torch.int64\n+        )\n+        expected_m = 0\n+\n+        if self.deepep_mode == \"normal\" or (\n+            self.deepep_mode == \"auto\" and not forward_mode.is_decode()\n+        ):\n             (\n                 hidden_states,\n                 topk_idx,\n                 topk_weights,\n-                num_recv_tokens_per_expert_list,\n-                handle,\n                 event,\n             ) = self.dispatch_normal(hidden_states, topk_idx, topk_weights, num_experts)\n-            self.tokens_per_expert = torch.tensor(\n-                num_recv_tokens_per_expert_list,\n-                device=hidden_states.device,\n-                dtype=torch.int64,\n-            )\n-        else:\n-            hidden_states, recv_expert_count, handle, event, hook = (\n-                self.dispatch_low_latency(\n-                    hidden_states,\n-                    topk_idx,\n-                    num_max_dispatch_tokens_per_rank,\n-                    num_experts,\n+            event.current_stream_wait() if self.async_finish else ()\n+            if hidden_states.shape[0] > 0:\n+                reorder_topk_ids, seg_indptr, hidden_states = self.deepep_permute(\n+                    hidden_states, topk_idx, fp8_dtype=hidden_states.dtype\n                 )\n+        elif self.deepep_mode == \"low_latency\" or (\n+            self.deepep_mode == \"auto\" and forward_mode.is_decode()\n+        ):\n+            expected_m = (\n+                hidden_states.shape[0]\n+                * self.buffer_low_latency.group_size\n+                * topk_idx.shape[1]\n+                + num_experts\n+            ) // num_experts\n+            hidden_states, masked_m, event, hook = self.dispatch_low_latency(\n+                hidden_states,\n+                topk_idx,\n+                num_max_dispatch_tokens_per_rank,\n+                num_experts,\n+                use_fp8=True,\n             )\n-            self.recv_expert_count = recv_expert_count\n-\n-        if self.async_finish:\n-            event.current_stream_wait()\n-\n-        self.handle = handle\n-        self.topk_idx = topk_idx\n-        self.topk_weights = topk_weights\n-        if hidden_states.shape[0] > 0:\n-            reorder_topk_ids, seg_indptr, hidden_states = self.deepep_permute(\n-                hidden_states, fp8_dtype=hidden_states.dtype\n-            )\n+            hook() if self.return_recv_hook else event.current_stream_wait()\n         else:\n-            reorder_topk_ids = torch.empty(\n-                (0,), device=hidden_states.device, dtype=torch.int64\n-            )\n-            seg_indptr = torch.zeros(\n-                (num_experts + 1,), device=hidden_states.device, dtype=torch.int64\n-            )\n-        return hidden_states, reorder_topk_ids, seg_indptr\n+            raise ValueError(f\"Invalid deepep_mode: {self.deepep_mode}\")\n+\n+        return (\n+            hidden_states,\n+            topk_idx,\n+            topk_weights,\n+            reorder_topk_ids,\n+            seg_indptr,\n+            masked_m,\n+            expected_m,\n+        )\n \n     def dispatch_normal(\n         self,\n@@ -254,12 +263,15 @@ class DeepEPDispatcher:\n             allocate_on_comm_stream=previous_event is not None,\n         )\n \n+        # FIXME: `handle` should be transmitted with tokens from dispatch to combine.\n+        # However, doing this would incur an unknown synchronization error, but keeping\n+        # `handle` as a member variable works.\n         (\n             recv_x,\n             recv_topk_idx,\n             recv_topk_weights,\n-            num_recv_tokens_per_expert_list,\n-            handle,\n+            _,  # num_recv_tokens_per_expert_list\n+            self.handle,\n             event,\n         ) = self.buffer_normal.dispatch(\n             x,\n@@ -278,8 +290,6 @@ class DeepEPDispatcher:\n             recv_x,\n             recv_topk_idx,\n             recv_topk_weights,\n-            num_recv_tokens_per_expert_list,\n-            handle,\n             event,\n         )\n \n@@ -289,18 +299,19 @@ class DeepEPDispatcher:\n         topk_idx: torch.Tensor,\n         num_max_dispatch_tokens_per_rank: int,\n         num_experts: int,\n+        use_fp8: bool = False,\n     ):\n         \"\"\"\n-        # For H20, there will be an CUDA error: DeepEP/csrc/kernels/internode_ll.cu:337 'too many blocks in cooperative launch'\n-        # Please please make sure to change DeepEP code in internode_ll.cu dispatch / combine first and then reinstall!\n+        # For H20, there will be an CUDA error: DeepEP/csrc/kernels/internode_ll.cu:337 'too many blocks in cooperative launch'.\n+        # Please make sure to change DeepEP code in internode_ll.cu dispatch / combine as below first and then reinstall.\n         # More details refer: https://github.com/deepseek-ai/DeepEP/issues/15#issuecomment-2709715782\n-        +\n+\n         diff --git a/csrc/kernels/internode_ll.cu b/csrc/kernels/internode_ll.cu\n-        index f60e933..cddaabf 100644\n+        index 76ae2e2..8ecd08f 100644\n         --- a/csrc/kernels/internode_ll.cu\n         +++ b/csrc/kernels/internode_ll.cu\n-        @@ -307,14 +307,14 @@ void dispatch(void* packed_recv_x, float* packed_recv_x_scales,\n-                    int num_topk, int num_experts, int rank, int num_ranks,\n+        @@ -310,8 +310,8 @@ void dispatch(void* packed_recv_x, float* packed_recv_x_scales,\n+                    int num_topk, int num_experts, int rank, int num_ranks, bool use_fp8,\n                     void* workspace, cudaStream_t stream, int phases) {\n             constexpr int kNumMaxTopK = 9;\n         -    constexpr int kNumWarpsPerGroup = 10;\n@@ -308,16 +319,9 @@ class DeepEPDispatcher:\n         +    constexpr int kNumWarpsPerGroup = 8;\n         +    constexpr int kNumWarpGroups = 4;\n             EP_STATIC_ASSERT(kNumMaxTopK + 1 <= kNumWarpGroups * kNumWarpsPerGroup, \"Too many top-k selections\");\n-        +\n+\n             const auto num_warps = kNumWarpGroups * kNumWarpsPerGroup;\n-            const auto num_sms = cell_div(num_experts, kNumWarpGroups);\n-            EP_HOST_ASSERT(num_topk <= kNumMaxTopK);\n-        -    EP_HOST_ASSERT(cell_div(static_cast<int>(hidden * 2 / sizeof(int4)), 32 * (num_warps - 1)) <= 2);\n-        +    // EP_HOST_ASSERT(cell_div(static_cast<int>(hidden * 2 / sizeof(int4)), 32 * (num_warps - 1)) <= 2);\n-        +\n-            // Workspace checks\n-            auto atomic_counter_per_expert = reinterpret_cast<int*>(workspace);\n-        @@ -505,8 +505,8 @@ void combine(void* combined_x,\n+        @@ -501,8 +501,8 @@ void combine(void* combined_x,\n                     int num_combined_tokens, int hidden, int num_max_dispatch_tokens_per_rank,\n                     int num_topk, int num_experts, int rank, int num_ranks,\n                     void* workspace, cudaStream_t stream, int phases) {\n@@ -326,28 +330,33 @@ class DeepEPDispatcher:\n         +    constexpr int kNumWarpsPerGroup = 8;\n         +    constexpr int kNumWarpGroups = 4;\n             constexpr int kNumMaxTopk = 9;\n-        +\n+\n             const auto num_warps = kNumWarpGroups * kNumWarpsPerGroup;\n         \"\"\"\n \n-        recv_hidden_states, recv_expert_count, handle, event, hook = (\n+        packed_recv_hidden, packed_recv_count, self.handle, event, hook = (\n             self.buffer_low_latency.low_latency_dispatch(\n                 hidden_states,\n                 topk_idx,\n                 num_max_dispatch_tokens_per_rank,\n                 num_experts,\n-                async_finish=self.async_finish,\n-                return_recv_hook=False,  # True for double-batch overlapping, need call hook()\n+                use_fp8=use_fp8,\n+                async_finish=not self.return_recv_hook,\n+                return_recv_hook=self.return_recv_hook,\n             )\n         )\n-        # hook()\n-        return recv_hidden_states, recv_expert_count, handle, event, hook\n+        return packed_recv_hidden, packed_recv_count, event, hook\n \n     def combine(\n-        self, hidden_states: torch.Tensor, forward_mode: ForwardMode\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n-        # Todo: enable low latency combine\n-        if True:  # not forward_mode.is_decode():\n+        self,\n+        hidden_states: torch.Tensor,\n+        topk_idx: torch.Tensor,\n+        topk_weights: torch.Tensor,\n+        forward_mode: ForwardMode,\n+    ) -> torch.Tensor:\n+        if self.deepep_mode == \"normal\" or (\n+            self.deepep_mode == \"auto\" and not forward_mode.is_decode()\n+        ):\n             if hidden_states.shape[0] > 0:\n                 num_tokens = self.src2dst.shape[0] // self.router_topk\n                 output = torch.empty(\n@@ -359,8 +368,8 @@ class DeepEPDispatcher:\n                     hidden_states,\n                     output,\n                     self.src2dst,\n-                    self.topk_idx,\n-                    self.topk_weights,\n+                    topk_idx,\n+                    topk_weights,\n                     self.router_topk,\n                     hidden_states.shape[1],\n                     BLOCK_SIZE=512,\n@@ -371,24 +380,30 @@ class DeepEPDispatcher:\n                     device=hidden_states.device,\n                     dtype=hidden_states.dtype,\n                 )\n-            hidden_states, event = self.combine_normal(output, self.handle)\n-        else:\n+            hidden_states, event = self.combine_normal(\n+                output,\n+            )\n+            event.current_stream_wait() if self.async_finish else ()\n+        elif self.deepep_mode == \"low_latency\" or (\n+            self.deepep_mode == \"auto\" and forward_mode.is_decode()\n+        ):\n             hidden_states, event, hook = self.combine_low_latency(\n-                hidden_states, self.topk_idx, self.topk_weights, self.handle\n+                hidden_states,\n+                topk_idx,\n+                topk_weights,\n             )\n+            hook() if self.return_recv_hook else event.current_stream_wait()\n+        else:\n+            raise ValueError(f\"Invalid deepep_mode: {self.deepep_mode}\")\n \n-        if self.async_finish:\n-            event.current_stream_wait()\n-\n-        self.handle = None\n         return hidden_states\n \n-    def combine_normal(self, x: torch.Tensor, handle: Tuple):\n+    def combine_normal(self, x: torch.Tensor):\n         previous_event = Buffer.capture() if self.async_finish else None\n \n         combined_x, _, event = self.buffer_normal.combine(\n             x,\n-            handle,\n+            self.handle,\n             async_finish=self.async_finish,\n             previous_event=previous_event,\n             allocate_on_comm_stream=previous_event is not None,\n@@ -400,17 +415,15 @@ class DeepEPDispatcher:\n         hidden_states: torch.Tensor,\n         topk_idx: torch.Tensor,\n         topk_weights: torch.Tensor,\n-        handle: Tuple,\n     ):\n-        combined_hidden_states, event_overlap, hook = (\n+        combined_hidden_states, event, hook = (\n             self.buffer_low_latency.low_latency_combine(\n                 hidden_states,\n                 topk_idx,\n                 topk_weights,\n-                handle,\n-                async_finish=self.async_finish,\n-                return_recv_hook=False,  # True for double-batch overlapping, need call hook()\n+                self.handle,\n+                async_finish=not self.return_recv_hook,\n+                return_recv_hook=self.return_recv_hook,\n             )\n         )\n-        # hook()\n-        return combined_hidden_states, event_overlap, hook\n+        return combined_hidden_states, event, hook\ndiff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py\nindex ab8b81602..991ec0551 100644\n--- a/python/sglang/srt/managers/schedule_batch.py\n+++ b/python/sglang/srt/managers/schedule_batch.py\n@@ -72,6 +72,7 @@ global_server_args_dict = {\n     \"enable_dp_attention\": ServerArgs.enable_dp_attention,\n     \"enable_ep_moe\": ServerArgs.enable_ep_moe,\n     \"enable_deepep_moe\": ServerArgs.enable_deepep_moe,\n+    \"deepep_mode\": ServerArgs.deepep_mode,\n     \"device\": ServerArgs.device,\n     \"speculative_accept_threshold_single\": ServerArgs.speculative_accept_threshold_single,\n     \"speculative_accept_threshold_acc\": ServerArgs.speculative_accept_threshold_acc,\ndiff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py\nindex f5405c9af..f42ea02d5 100644\n--- a/python/sglang/srt/model_executor/model_runner.py\n+++ b/python/sglang/srt/model_executor/model_runner.py\n@@ -147,6 +147,7 @@ class ModelRunner:\n                 \"enable_dp_attention\": server_args.enable_dp_attention,\n                 \"enable_ep_moe\": server_args.enable_ep_moe,\n                 \"enable_deepep_moe\": server_args.enable_deepep_moe,\n+                \"deepep_mode\": server_args.deepep_mode,\n                 \"device\": server_args.device,\n                 \"speculative_accept_threshold_single\": server_args.speculative_accept_threshold_single,\n                 \"speculative_accept_threshold_acc\": server_args.speculative_accept_threshold_acc,\n@@ -272,7 +273,7 @@ class ModelRunner:\n                 server_args.disable_radix_cache = True\n \n         if server_args.enable_deepep_moe:\n-            logger.info(\"DeepEP is turned on.\")\n+            logger.info(f\"DeepEP is turned on. DeepEP mode: {server_args.deepep_mode}\")\n \n     def init_torch_distributed(self):\n         logger.info(\"Init torch distributed begin.\")\ndiff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py\nindex 37760407b..6aaa3744a 100644\n--- a/python/sglang/srt/models/deepseek_v2.py\n+++ b/python/sglang/srt/models/deepseek_v2.py\n@@ -188,19 +188,35 @@ class DeepseekV2MoE(nn.Module):\n             if global_server_args_dict[\"enable_deepep_moe\"]\n             else (EPMoE if global_server_args_dict[\"enable_ep_moe\"] else FusedMoE)\n         )\n-        self.experts = MoEImpl(\n-            num_experts=config.n_routed_experts,\n-            top_k=config.num_experts_per_tok,\n-            hidden_size=config.hidden_size,\n-            intermediate_size=config.moe_intermediate_size,\n-            renormalize=config.norm_topk_prob,\n-            quant_config=quant_config,\n-            use_grouped_topk=True,\n-            num_expert_group=config.n_group,\n-            topk_group=config.topk_group,\n-            correction_bias=self.gate.e_score_correction_bias,\n-            prefix=add_prefix(\"experts\", prefix),\n-        )\n+        if not global_server_args_dict[\"enable_deepep_moe\"]:\n+            self.experts = MoEImpl(\n+                num_experts=config.n_routed_experts,\n+                top_k=config.num_experts_per_tok,\n+                hidden_size=config.hidden_size,\n+                intermediate_size=config.moe_intermediate_size,\n+                renormalize=config.norm_topk_prob,\n+                quant_config=quant_config,\n+                use_grouped_topk=True,\n+                num_expert_group=config.n_group,\n+                topk_group=config.topk_group,\n+                correction_bias=self.gate.e_score_correction_bias,\n+                prefix=add_prefix(\"experts\", prefix),\n+            )\n+        else:\n+            self.experts = MoEImpl(\n+                num_experts=config.n_routed_experts,\n+                top_k=config.num_experts_per_tok,\n+                hidden_size=config.hidden_size,\n+                intermediate_size=config.moe_intermediate_size,\n+                renormalize=config.norm_topk_prob,\n+                quant_config=quant_config,\n+                use_grouped_topk=True,\n+                num_expert_group=config.n_group,\n+                topk_group=config.topk_group,\n+                correction_bias=self.gate.e_score_correction_bias,\n+                prefix=add_prefix(\"experts\", prefix),\n+                deepep_mode=global_server_args_dict[\"deepep_mode\"],\n+            )\n \n         if config.n_shared_experts is not None:\n             intermediate_size = config.moe_intermediate_size * config.n_shared_experts\n@@ -227,6 +243,8 @@ class DeepseekV2MoE(nn.Module):\n                 )\n \n         if global_server_args_dict[\"enable_deepep_moe\"]:\n+            # TODO: we will support tp < ep in the future\n+            self.ep_size = get_tensor_model_parallel_world_size()\n             self.num_experts = config.n_routed_experts\n             self.top_k = config.num_experts_per_tok\n             self.renormalize = config.norm_topk_prob\n@@ -246,7 +264,9 @@ class DeepseekV2MoE(nn.Module):\n                 num_local_experts=config.n_routed_experts // self.tp_size,\n                 hidden_size=config.hidden_size,\n                 params_dtype=config.torch_dtype,\n+                deepep_mode=global_server_args_dict[\"deepep_mode\"],\n                 async_finish=True,  # TODO\n+                return_recv_hook=True,\n             )\n \n     def forward(\n@@ -301,28 +321,39 @@ class DeepseekV2MoE(nn.Module):\n                 num_expert_group=self.num_expert_group,\n                 correction_bias=self.correction_bias,\n             )\n-        if self.tp_size > 1:\n-            recv_hidden_states, reorder_topk_ids, seg_indptr = (\n-                self.deepep_dispatcher.dispatch(\n-                    hidden_states,\n-                    topk_idx,\n-                    topk_weights,\n-                    self.num_experts,\n-                    forward_mode,\n-                )\n+        if self.ep_size > 1:\n+            (\n+                hidden_states,\n+                topk_idx,\n+                topk_weights,\n+                reorder_topk_ids,\n+                seg_indptr,\n+                masked_m,\n+                expected_m,\n+            ) = self.deepep_dispatcher.dispatch(\n+                hidden_states,\n+                topk_idx,\n+                topk_weights,\n+                self.num_experts,\n+                forward_mode=forward_mode,\n             )\n         final_hidden_states = (\n             self.experts(\n-                hidden_states=recv_hidden_states,\n+                hidden_states=hidden_states,\n                 reorder_topk_ids=reorder_topk_ids,\n                 seg_indptr=seg_indptr,\n+                masked_m=masked_m,\n+                expected_m=expected_m,\n                 forward_mode=forward_mode,\n             )\n             * self.routed_scaling_factor\n         )\n-        if self.tp_size > 1:\n+        if self.ep_size > 1:\n             final_hidden_states = self.deepep_dispatcher.combine(\n-                final_hidden_states, forward_mode\n+                final_hidden_states,\n+                topk_idx,\n+                topk_weights,\n+                forward_mode,\n             )\n         if shared_output is not None:\n             final_hidden_states = final_hidden_states + shared_output\ndiff --git a/python/sglang/srt/server_args.py b/python/sglang/srt/server_args.py\nindex 6f4725487..1a19bbea2 100644\n--- a/python/sglang/srt/server_args.py\n+++ b/python/sglang/srt/server_args.py\n@@ -161,6 +161,7 @@ class ServerArgs:\n     enable_dp_attention: bool = False\n     enable_ep_moe: bool = False\n     enable_deepep_moe: bool = False\n+    deepep_mode: Optional[str] = \"auto\"\n     enable_torch_compile: bool = False\n     torch_compile_max_bs: int = 32\n     cuda_graph_max_bs: Optional[int] = None\n@@ -285,6 +286,13 @@ class ServerArgs:\n         if self.grammar_backend is None:\n             self.grammar_backend = \"xgrammar\"\n \n+        # Expert parallelism\n+        if self.enable_ep_moe:\n+            self.ep_size = self.tp_size\n+            logger.info(\n+                f\"EP MoE is enabled. The expert parallel size is adjusted to be the same as the tensor parallel size[{self.tp_size}].\"\n+            )\n+\n         # Data parallelism attention\n         if self.enable_dp_attention:\n             self.schedule_conservativeness = self.schedule_conservativeness * 0.3\n@@ -300,6 +308,10 @@ class ServerArgs:\n         self.enable_sp_layernorm = False\n         # DeepEP MoE\n         if self.enable_deepep_moe:\n+            if self.deepep_mode == \"auto\":\n+                assert (\n+                    not self.enable_dp_attention\n+                ), \"DeepEP MoE `auto` mode is not supported with DP Attention.\"\n             self.ep_size = self.tp_size\n             self.enable_sp_layernorm = (\n                 self.dp_size < self.tp_size if self.enable_dp_attention else True\n@@ -1082,6 +1094,12 @@ class ServerArgs:\n             action=\"store_true\",\n             help=\"Enabling DeepEP MoE implementation for EP MoE.\",\n         )\n+        parser.add_argument(\n+            \"--deepep-mode\",\n+            type=str,\n+            choices=[\"normal\", \"low_latency\", \"auto\"],\n+            help=\"Select the mode when enable DeepEP MoE, could be `normal`, `low_latency` or `auto`. Default is `auto`, which means `low_latency` for decode batch and `normal` for prefill batch.\",\n+        )\n \n         # Server warmups\n         parser.add_argument(",
  "apis": [
    "DeepseekV2MoE",
    "DeepseekV2ForCausalLM",
    "DeepEPMoE",
    "ServerArgs"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/models/deepseek_v2.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/fused_moe_triton/layer.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/ep_moe/layer.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "This commit introduces a new \u201cdeepep_mode\u201d option along with multiple changes in the MoE dispatch and kernel code that directly impact the performance of key inference paths. The modifications include adding a low\u2010latency mode and reallocating compute paths (e.g., new Triton kernels, altered dispatch and combine logic, and adjustments in deep_gemm integration) intended to lower latency during decode or prefill phases. The changes are applied to production (non-test) files and modify performance-critical APIs rather than being simple refactoring or bug fixes. Despite the commit message being tagged as \u201c[Feature]\u201d, the underlying intent and modifications are squarely focused on optimizing performance.",
  "llm_api_reason": "This commit introduces support for DeepEP low latency mode. In the documentation it adds a new server argument \u201cdeepep_mode\u201d (normal, low_latency, auto) with its description. In the code changes many parts of the DeepEP MoE pipeline are updated. For instance, the DeepEPMoE layer in the EP MoE modules has been modified to select between \u201cnormal\u201d and \u201clow_latency\u201d paths, and parameters (like deepep_mode) are passed from the server arguments through schedule_batch and model_runner to the DeepseekV2MoE and downstream MoE implementation. Additionally, a new Triton kernel (_silu_and_mul_post_quant_kernel) and its Python wrapper (silu_and_mul_masked_post_quant_fwd) are added to support fp8 post-quantization in low latency mode. Overall, these modifications affect the high-level APIs responsible for model inference in DeepseekV2 and its MoE layers as well as the server configuration."
}