{
  "commit_hash": "a99801e0750f41553fedd02e36f58d835c4d4bd6",
  "pr_url": "https://github.com/sgl-project/sglang/pull/8133",
  "pr_date": "2025-07-23",
  "timeline_text": "Copy link Contributor YiXR commented Jul 18, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Motivation In large-scale distributed inference, especially with PD Disaggregation, fast and efficient management of KV cache token indices is crucial. Fragmentation of available indices can disrupt peer-to-peer (p2p) data transfers by making it harder to allocate large contiguous regions, resulting in inefficient communication patterns. This fragmentation directly increases end-to-end latency, especially the time to first token (TTFT). This PR introduces release cache mechanisms to optimize TokenToKVPoolAllocator page management. By deferring the actual release of freed pages and supporting batch reclamation, this design significantly reduces allocation contention and fragmentation during high-concurrency operations, thereby boosting p2p transfer efficiency and reducing TTFT. Modifications Introduced release_pages Freed token indices are now buffered in a temporary cache rather than immediately added to the free pool. On allocation, merging and sorting When an allocation request cannot be satisfied by the current free pool alone, the allocator will merge the buffered indices from release cache with the free pool, sort them, and then retry the allocation. This sorting and merging process helps to coalesce adjacent free regions, mitigating fragmentation. Test on 2 * A10 * 8\uff08RDMA\uff09\uff0cqwen 0.6b\uff0c1P1D Case 1\uff08200 warmup\uff1b400 nr\uff1b16 concurrency; 1024 input len; 512 output len\uff09 without optimization:  TTFT 81ms with optimization: TTFT 69ms\uff08improve 14.8%\uff09 Case 2\uff08200 warmup\uff1b400 nr\uff1b16 concurrency; 2048 input len; 1024 output len\uff09 without optimization:  TTFT 144ms with optimization: TTFT 108ms\uff08improve 25%\uff09 A simple comparison of the number of kv_blocks that need to be transferred by send_kvcache() (1024 in; 512 out) Detailed data 1024 input len; 512 output len without optimization Backend:                                 sglang Traffic request rate:                    inf Max request concurrency:                 16 Successful requests:                     400 Benchmark duration (s):                  188.20 Total input tokens:                      409600 Total generated tokens:                  204800 Total generated tokens (retokenized):    204796 Request throughput (req/s):              2.13 Input token throughput (tok/s):          2176.35 Output token throughput (tok/s):         1088.18 Total token throughput (tok/s):          3264.53 Concurrency:                             15.98 ----------------End-to-End Latency---------------- Mean E2E Latency (ms):                   7517.90 Median E2E Latency (ms):                 7501.12 ---------------Time to First Token---------------- Mean TTFT (ms):                          81.29 Median TTFT (ms):                        74.55 P99 TTFT (ms):                           300.72 ---------------Inter-Token Latency---------------- Mean ITL (ms):                           14.55 Median ITL (ms):                         14.47 P95 ITL (ms):                            14.88 P99 ITL (ms):                            15.69 Max ITL (ms):                            33.38 1024 input len; 512 output len with optimization Backend:                                 sglang Traffic request rate:                    inf Max request concurrency:                 16 Successful requests:                     400 Benchmark duration (s):                  185.05 Total input tokens:                      409600 Total generated tokens:                  204800 Total generated tokens (retokenized):    204792 Request throughput (req/s):              2.16 Input token throughput (tok/s):          2213.43 Output token throughput (tok/s):         1106.71 Total token throughput (tok/s):          3320.14 Concurrency:                             15.98 ----------------End-to-End Latency---------------- Mean E2E Latency (ms):                   7393.70 Median E2E Latency (ms):                 7386.50 ---------------Time to First Token---------------- Mean TTFT (ms):                          69.35 Median TTFT (ms):                        59.20 P99 TTFT (ms):                           363.33 ---------------Inter-Token Latency---------------- Mean ITL (ms):                           14.33 Median ITL (ms):                         14.27 P95 ITL (ms):                            14.67 P99 ITL (ms):                            15.65 Max ITL (ms):                            35.02 2048 input len; 1024 output len without optimization Backend:                                 sglang Traffic request rate:                    inf Max request concurrency:                 16 Successful requests:                     400 Benchmark duration (s):                  395.88 Total input tokens:                      819200 Total generated tokens:                  409600 Total generated tokens (retokenized):    409584 Request throughput (req/s):              1.01 Input token throughput (tok/s):          2069.32 Output token throughput (tok/s):         1034.66 Total token throughput (tok/s):          3103.98 Concurrency:                             15.97 ----------------End-to-End Latency---------------- Mean E2E Latency (ms):                   15803.31 Median E2E Latency (ms):                 15800.76 ---------------Time to First Token---------------- Mean TTFT (ms):                          144.02 Median TTFT (ms):                        118.62 P99 TTFT (ms):                           975.52 ---------------Inter-Token Latency---------------- Mean ITL (ms):                           15.31 Median ITL (ms):                         15.17 P95 ITL (ms):                            16.62 P99 ITL (ms):                            16.91 Max ITL (ms):                            44.18 2048 input len; 1024 output len with optimization Backend:                                 sglang Traffic request rate:                    inf Max request concurrency:                 16 Successful requests:                     400 Benchmark duration (s):                  392.21 Total input tokens:                      819200 Total generated tokens:                  409600 Total generated tokens (retokenized):    409586 Request throughput (req/s):              1.02 Input token throughput (tok/s):          2088.66 Output token throughput (tok/s):         1044.33 Total token throughput (tok/s):          3132.99 Concurrency:                             15.98 ----------------End-to-End Latency---------------- Mean E2E Latency (ms):                   15669.23 Median E2E Latency (ms):                 15658.25 ---------------Time to First Token---------------- Mean TTFT (ms):                          108.04 Median TTFT (ms):                        89.39 P99 TTFT (ms):                           733.34 ---------------Inter-Token Latency---------------- Mean ITL (ms):                           15.21 Median ITL (ms):                         15.08 P95 ITL (ms):                            16.57 P99 ITL (ms):                            16.89 Max ITL (ms):                            39.90 Checklist Format your code according to the Code Formatting with Pre-Commit . Add unit tests as outlined in the Running Unit Tests . Update documentation / docstrings / example tutorials as needed, according to Writing Documentation . Provide throughput / latency benchmark results and accuracy evaluation results as needed, according to Benchmark and Profiling and Accuracy Results . For reviewers: If you haven't made any contributions to this PR and are only assisting with merging the main branch, please remove yourself as a co-author when merging the PR. Please feel free to join our Slack channel at https://slack.sglang.ai to discuss your PR. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 5 Swipe4057, XucSh, Edenzzzz, whybeyoung, and xu-yfei reacted with thumbs up emoji All reactions \ud83d\udc4d 5 reactions YiXR requested review from merrymercy , Ying1123 , hnyls2002 and xiezhq-hermann as code owners July 18, 2025 04:58 gemini-code-assist bot reviewed Jul 18, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Summary of Changes Hello @YiXR , I'm Gemini Code Assist 1 ! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed! This pull request introduces a refined memory management strategy within the TokenToKVPoolAllocator and related allocator classes. The core change involves deferring the immediate release of freed KV cache pages and implementing an on-demand consolidation mechanism that merges and sorts these deferred pages when more memory is needed. This approach aims to reduce memory fragmentation and allocation contention, thereby improving peer-to-peer data transfer efficiency and overall inference latency in large-scale distributed setups. Highlights Deferred Page Release : Freed token indices are no longer immediately returned to the main free pool ( free_pages ). Instead, they are buffered in a new release_pages temporary cache, reducing immediate contention. On-Demand Free Page Consolidation : When an allocation request cannot be met by the currently available free_pages , the system now merges the buffered release_pages into free_pages , sorts the combined set to coalesce adjacent regions, and then retries the allocation. This strategy mitigates fragmentation. Performance Improvements : Benchmarks demonstrate significant reductions in Time To First Token (TTFT), with improvements of 14.8% and 25% in tested scenarios, indicating enhanced memory utilization and reduced fragmentation in distributed inference. Using Gemini Code Assist The full guide for Gemini Code Assist can be found on our documentation page , here are some quick tips. Invoking Gemini You can request assistance from Gemini at any point in your pull request via creating an issue comment (i.e. comment on the pull request page) using either /gemini <command> or @gemini-code-assist <command> . Below is a summary of the supported commands. Feature Command Description Code Review /gemini review Performs a code review for the current pull request in its current state. Pull Request Summary /gemini summary Provides a summary of the current pull request in its current state. Comment @gemini-code-assist Responds in comments when explicitly tagged, both in issue comments and review comments. Help /gemini help Displays a list of available commands. Customization To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a .gemini/ folder in the base of the repository. Detailed instructions can be found here . Limitations & Feedback Gemini Code Assist is currently in preview and may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with \ud83d\udc4d and \ud83d\udc4e on @gemini-code-assist comments to provide feedback. You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the Gemini Code Assist IDE Extension . Footnotes Review the Privacy Notices , Generative AI Prohibited Use Policy , Terms of Service , and learn how to configure Gemini Code Assist in GitHub here . Gemini can make mistakes, so double check it and use code with caution . \u21a9 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions gemini-code-assist bot reviewed Jul 18, 2025 View reviewed changes Copy link Contributor gemini-code-assist bot left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Code Review This pull request introduces optimizations to the TokenToKVPoolAllocator, enhancing memory management and reducing fragmentation, leading to performance improvements. The primary area for improvement is to address code duplication in allocation logic across multiple classes. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/mem_cache/allocator.py Outdated Comment on lines 432 to 435 if len(self.release_pages) > 0: self.free_pages = torch.cat((self.free_pages, self.release_pages)) self.free_pages, _ = torch.sort(self.free_pages) self.release_pages = torch.empty((0,), dtype=torch.int64, device=self.device) Copy link Contributor gemini-code-assist bot Jul 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This allocation logic is duplicated in TokenToKVPoolAllocator , PagedTokenToKVPoolAllocator , and AscendPagedTokenToKVPoolAllocator .[^1] Consider refactoring into a helper method within BaseTokenToKVPoolAllocator to improve maintainability and reduce code duplication. Suggested change if len ( self . release_pages ) > 0 : self . free_pages = torch . cat (( self . free_pages , self . release_pages )) self . free_pages , _ = torch . sort ( self . free_pages ) self . release_pages = torch . empty (( 0 ,), dtype = torch . int64 , device = self . device ) if len ( self . release_pages ) > 0 : self . free_pages = torch . cat (( self . free_pages , self . release_pages )) self . free_pages , _ = torch . sort ( self . free_pages ) self . release_pages = torch . empty (( 0 ,), dtype = self . release_pages . dtype , device = self . device ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 Swipe4057 reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction python/sglang/srt/mem_cache/allocator.py Outdated Comment on lines 679 to 682 if len(self.release_pages) > 0: self.free_pages = torch.cat((self.free_pages, self.release_pages)) self.free_pages, _ = torch.sort(self.free_pages) self.release_pages = torch.empty((0,), dtype=torch.int32, device=self.device) Copy link Contributor gemini-code-assist bot Jul 18, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment The release_pages tensor is initialized with dtype=torch.int32 here, while in TokenToKVPoolAllocator and PagedTokenToKVPoolAllocator it's initialized with dtype=torch.int64 . This inconsistency could lead to unexpected behavior or errors. Ensure consistent dtype usage across all allocators. Suggested change if len ( self . release_pages ) > 0 : self . free_pages = torch . cat (( self . free_pages , self . release_pages )) self . free_pages , _ = torch . sort ( self . free_pages ) self . release_pages = torch . empty (( 0 ,), dtype = torch . int32 , device = self . device ) self . free_pages = torch . cat (( self . free_pages , self . release_pages )) self . free_pages , _ = torch . sort ( self . free_pages ) self . release_pages = torch . empty (( 0 ,), dtype = torch . int64 , device = self . device ) Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \ud83d\udc4d 1 Swipe4057 reacted with thumbs up emoji All reactions \ud83d\udc4d 1 reaction YiXR pushed a commit\n        to YiXR/sglang\n      that referenced\n      this pull request Jul 18, 2025 optimize TokenToKVPoolAllocator by sorting free pages ( sgl-project#8133 ) \u2026 58c1b58 Signed-off-by: Xingrui Yi <yixingrui@linux.alibaba.com> YiXR force-pushed the main branch\n    from 42b7691 to 58c1b58 Compare July 18, 2025 05:05 YiXR pushed a commit\n        to YiXR/sglang\n      that referenced\n      this pull request Jul 18, 2025 optimize TokenToKVPoolAllocator by sorting free pages ( sgl-project#8133 ) \u2026 af75a4d Signed-off-by: Xingrui Yi <yixingrui@linux.alibaba.com> YiXR force-pushed the main branch\n    from 58c1b58 to af75a4d Compare July 18, 2025 05:15 xiezhq-hermann self-assigned this Jul 18, 2025 YiXR pushed a commit\n        to YiXR/sglang\n      that referenced\n      this pull request Jul 18, 2025 optimize TokenToKVPoolAllocator by sorting free pages ( sgl-project#8133 ) \u2026 57351db Signed-off-by: Xingrui Yi <yixingrui@linux.alibaba.com> YiXR force-pushed the main branch\n      2 times, most recently\n    from 56ee8c5 to ee6cbd8 Compare July 22, 2025 01:58 YiXR pushed a commit\n        to YiXR/sglang\n      that referenced\n      this pull request Jul 22, 2025 optimize TokenToKVPoolAllocator by sorting free pages ( sgl-project#8133 ) \u2026 ee6cbd8 Signed-off-by: Xingrui Yi <yixingrui@linux.alibaba.com> Copy link Collaborator xiezhq-hermann commented Jul 22, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . Nice work and overall it looks reasonable to me : ) @ByronHsu @ShangmingCai can you also help take a look on this? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . xiezhq-hermann assigned ByronHsu and ShangmingCai Jul 22, 2025 ShangmingCai approved these changes Jul 22, 2025 View reviewed changes Copy link Collaborator ShangmingCai left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment LGTM, but need to fix CI. Maybe merge main later, I will retrigger the CI. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions optimize TokenToKVPoolAllocator by sorting free pages ( sgl-project#8133 ) \u2026 9b14579 Signed-off-by: Xingrui Yi <yixingrui@linux.alibaba.com> YiXR force-pushed the main branch\n    from ee6cbd8 to 9b14579 Compare July 23, 2025 03:20 Copy link Contributor Author YiXR commented Jul 23, 2025 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . I've just fixed a problem when using alloc_extend() and alloc_decode(), please help review this, thx. @xiezhq-hermann @ShangmingCai \ud83c\udf89 1 ShangmingCai reacted with hooray emoji All reactions \ud83c\udf89 1 reaction Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Merge branch 'main' into main 2cdf77a Copy link Collaborator whybeyoung commented Jul 23, 2025 LGTM All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Hide details View details zhyncs merged commit a99801e into sgl-project : main Jul 23, 2025 101 of 108 checks passed Uh oh! There was an error while loading. Please reload this page . lihaoyang-amd pushed a commit\n        to lihaoyang-amd/sglang\n      that referenced\n      this pull request Jul 24, 2025 [Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by s\u2026 \u2026 d724503 \u2026orting free pages ( sgl-project#8133 )\n\nSigned-off-by: Xingrui Yi <yixingrui@linux.alibaba.com>\nCo-authored-by: Xingrui Yi <yixingrui@linux.alibaba.com> merrymercy reviewed Aug 4, 2025 View reviewed changes Copy link Contributor merrymercy left a comment There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment @YiXR I left some comments. Please address them. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/mem_cache/allocator.py @@ -446,6 +460,17 @@ def alloc_extend( (last_loc + 1) % self.page_size == prefix_lens % self.page_size ) estimated_num_new_pages = ( Copy link Contributor merrymercy Aug 4, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment This code has been duplicated too many times. Please write a common subfunction for it. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author YiXR Aug 5, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Thanks for your comment, I'll fix it soon. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions python/sglang/srt/mem_cache/allocator.py - (prefix_lens + self.page_size - 1) // self.page_size ) .sum() .item() Copy link Contributor merrymercy Aug 4, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment Is it possible to reduce the sync by estimating with extend_num_tokens Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions Copy link Contributor Author YiXR Aug 5, 2025 There was a problem hiding this comment. Choose a reason for hiding this comment The reason will be displayed to describe this comment to others. Learn more . Choose a reason Spam Abuse Off Topic Outdated Duplicate Resolved Hide comment I've just changed this logic by using extend_num_tokens #8794 Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . All reactions YiXR mentioned this pull request Aug 5, 2025 [Optimization] Update estimated_num_new_pages logic in TokenToKVPoolAllocator #8794 Merged 6 tasks ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 [Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by s\u2026 \u2026 63b1f38 \u2026orting free pages ( #8133 )\n\nSigned-off-by: Xingrui Yi <yixingrui@linux.alibaba.com>\nCo-authored-by: Xingrui Yi <yixingrui@linux.alibaba.com> ShangmingCai pushed a commit\n      that referenced\n      this pull request Aug 5, 2025 [Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by s\u2026 \u2026 44e64b8 \u2026orting free pages ( #8133 )\n\nSigned-off-by: Xingrui Yi <yixingrui@linux.alibaba.com>\nCo-authored-by: Xingrui Yi <yixingrui@linux.alibaba.com> fungaren mentioned this pull request Sep 9, 2025 [Bug] ValueError: token_to_kv_pool_allocator memory leak detected! #6888 Closed 5 tasks Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 18:56:08",
  "has_lm_eval": false,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": null,
  "commit_subject": "[Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by sorting free pages (#8133)",
  "commit_message": "[Performance][PD Disaggregation] optimize TokenToKVPoolAllocator by sorting free pages (#8133)\n\nSigned-off-by: Xingrui Yi <yixingrui@linux.alibaba.com>\nCo-authored-by: Xingrui Yi <yixingrui@linux.alibaba.com>",
  "commit_date": "2025-07-23T13:28:12-07:00",
  "files_changed": [
    "python/sglang/srt/mem_cache/allocator.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2025,
    "num_edited_lines": 74,
    "num_files": 1,
    "num_hunks": 13,
    "num_non_test_edited_lines": 74,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py\nindex 7dd488e9c..58afbf312 100644\n--- a/python/sglang/srt/mem_cache/allocator.py\n+++ b/python/sglang/srt/mem_cache/allocator.py\n@@ -51,6 +51,7 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         self._kvcache = kvcache\n \n         self.free_pages = None\n+        self.release_pages = None\n         self.is_not_in_free_group = True\n         self.free_group = []\n \n@@ -58,16 +59,16 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         return \"\"\n \n     def available_size(self):\n-        return len(self.free_pages) * self.page_size\n+        return (len(self.free_pages) + len(self.release_pages)) * self.page_size\n \n     def get_kvcache(self):\n         return self._kvcache\n \n-    def restore_state(self, free_pages):\n-        self.free_pages = free_pages\n+    def restore_state(self, state):\n+        self.free_pages, self.release_pages = state\n \n     def backup_state(self):\n-        return self.free_pages\n+        return (self.free_pages, self.release_pages)\n \n     def free_group_begin(self):\n         self.is_not_in_free_group = False\n@@ -78,6 +79,14 @@ class BaseTokenToKVPoolAllocator(abc.ABC):\n         if self.free_group:\n             self.free(torch.cat(self.free_group))\n \n+    def merge_and_sort_free(self):\n+        if len(self.release_pages) > 0:\n+            self.free_pages = torch.cat((self.free_pages, self.release_pages))\n+            self.free_pages, _ = torch.sort(self.free_pages)\n+            self.release_pages = torch.empty(\n+                (0,), dtype=self.release_pages.dtype, device=self.device\n+            )\n+\n     def get_cpu_copy(self, *args, **kwargs):\n         # FIXME: reuse the get_cpu_copy after paged allocator is implemented\n         raise NotImplementedError()\n@@ -119,12 +128,15 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         )\n         self.is_not_in_free_group = True\n         self.free_group = []\n+        self.release_pages = torch.empty((0,), dtype=torch.int64, device=self.device)\n \n     def available_size(self):\n         # To avoid minor \"len(free_pages) * 1\" overhead\n-        return len(self.free_pages)\n+        return len(self.free_pages) + len(self.release_pages)\n \n     def alloc(self, need_size: int):\n+        if need_size > len(self.free_pages):\n+            self.merge_and_sort_free()\n         if need_size > len(self.free_pages):\n             return None\n \n@@ -137,7 +149,7 @@ class TokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n             return\n \n         if self.is_not_in_free_group:\n-            self.free_pages = torch.cat((self.free_pages, free_index))\n+            self.release_pages = torch.cat((self.release_pages, free_index))\n         else:\n             self.free_group.append(free_index)\n \n@@ -421,6 +433,8 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n             ), \"The allocation size should be page-aligned\"\n \n         num_pages = need_size // self.page_size\n+        if num_pages > len(self.free_pages):\n+            self.merge_and_sort_free()\n         if num_pages > len(self.free_pages):\n             return None\n \n@@ -446,6 +460,17 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n                 (last_loc + 1) % self.page_size == prefix_lens % self.page_size\n             )\n \n+        estimated_num_new_pages = (\n+            (\n+                (seq_lens + self.page_size - 1) // self.page_size\n+                - (prefix_lens + self.page_size - 1) // self.page_size\n+            )\n+            .sum()\n+            .item()\n+        )\n+        if estimated_num_new_pages > len(self.free_pages):\n+            self.merge_and_sort_free()\n+\n         bs = len(prefix_lens)\n         out_indices = torch.empty(\n             (extend_num_tokens,), dtype=torch.int64, device=self.device\n@@ -483,6 +508,17 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n                 (last_loc + 2) % self.page_size == seq_lens % self.page_size\n             )\n \n+        estimated_num_new_pages = (\n+            (\n+                (seq_lens + self.page_size - 1) // self.page_size\n+                - (seq_lens - 1 + self.page_size - 1) // self.page_size\n+            )\n+            .sum()\n+            .item()\n+        )\n+        if estimated_num_new_pages > len(self.free_pages):\n+            self.merge_and_sort_free()\n+\n         bs = len(seq_lens)\n         out_indices = torch.empty((bs,), dtype=torch.int64, device=self.device)\n         alloc_decode_kernel[(bs,)](\n@@ -511,7 +547,7 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n \n         if self.is_not_in_free_group:\n             free_page_indices = torch.unique(free_index // self.page_size)\n-            self.free_pages = torch.cat((free_page_indices, self.free_pages))\n+            self.release_pages = torch.cat((free_page_indices, self.release_pages))\n         else:\n             self.free_group.append(free_index)\n \n@@ -525,6 +561,7 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):\n         )\n         self.is_not_in_free_group = True\n         self.free_group = []\n+        self.release_pages = torch.empty((0,), dtype=torch.int64, device=self.device)\n \n     def get_cpu_copy(self, indices):\n         return self._kvcache.get_cpu_copy(indices)\n@@ -633,6 +670,17 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n                 (last_loc + 1) % self.page_size == prefix_lens % self.page_size\n             )\n \n+        estimated_num_new_pages = (\n+            (\n+                (seq_lens + self.page_size - 1) // self.page_size\n+                - (prefix_lens + self.page_size - 1) // self.page_size\n+            )\n+            .sum()\n+            .item()\n+        )\n+        if estimated_num_new_pages > len(self.free_pages):\n+            self.merge_and_sort_free()\n+\n         bs = len(prefix_lens)\n         out_indices = torch.empty(\n             (extend_num_tokens,), dtype=torch.int32, device=self.device\n@@ -668,6 +716,17 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n                 (last_loc + 2) % self.page_size == seq_lens % self.page_size\n             )\n \n+        estimated_num_new_pages = (\n+            (\n+                (seq_lens + self.page_size - 1) // self.page_size\n+                - (seq_lens - 1 + self.page_size - 1) // self.page_size\n+            )\n+            .sum()\n+            .item()\n+        )\n+        if estimated_num_new_pages > len(self.free_pages):\n+            self.merge_and_sort_free()\n+\n         bs = len(seq_lens)\n         out_indices = torch.empty((bs,), dtype=torch.int32, device=self.device)\n \n@@ -692,3 +751,4 @@ class AscendPagedTokenToKVPoolAllocator(PagedTokenToKVPoolAllocator):\n     def clear(self):\n         super().clear()\n         self.free_pages = self.free_pages.to(torch.int32)\n+        self.release_pages = self.release_pages.to(torch.int32)",
  "apis": [
    "BaseTokenToKVPoolAllocator.available_size",
    "TokenToKVPoolAllocator.alloc",
    "PagedTokenToKVPoolAllocator.alloc_extend",
    "PagedTokenToKVPoolAllocator.alloc_decode",
    "AscendPagedTokenToKVPoolAllocator.alloc_extend"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/srt/mem_cache/allocator.py",
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/benchmark/lora/launch_server.py",
    "/path/to/repos/sglang/python/sglang/launch_server.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_server.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The commit modifies a non-test source file by adding a new field (\"release_pages\") and associated logic to merge and sort free pages. This change aims to optimize memory allocation by improving the management of free pages, which can have a direct impact on runtime performance, particularly in allocation routines. The modifications are work on internal APIs affecting performance and are CPU-related rather than mere bug fixes, refactoring, or new feature additions. Although the commit message uses the word \u201coptimize\u201d, in this context it distinctly refers to performance improvement via sorting strategies rather than a trivial naming change. Overall, the changes target performance optimization.",
  "llm_api_reason": "This commit optimizes the memory allocation for the KV cache by modifying how the free pages are managed. It adds a new attribute (release_pages) and a merge_and_sort_free method in BaseTokenToKVPoolAllocator, and adjusts the available_size, backup_state, and restore_state methods accordingly. Moreover, the alloc, alloc_extend, and alloc_decode methods in TokenToKVPoolAllocator, PagedTokenToKVPoolAllocator, and AscendPagedTokenToKVPoolAllocator are updated to merge released pages when there aren\u2019t enough free pages. These changes affect the high-level Python APIs used for memory (KV cache) management in the runtime system."
}