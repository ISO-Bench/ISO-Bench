{
  "commit_hash": "e822e5900b98d89d19e0a293d9ad384f4df2945a",
  "pr_url": "https://github.com/sgl-project/sglang/pull/364",
  "pr_date": "2024-04-17",
  "timeline_text": "Copy link Collaborator ispobock commented Apr 14, 2024 Use the first token to find child, avoiding traversing all children. Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . \u2764\ufe0f 1 Ying1123 reacted with heart emoji \ud83d\ude80 1 Qubitium reacted with rocket emoji All reactions \u2764\ufe0f 1 reaction \ud83d\ude80 1 reaction optimize radix tree match 549cdec Copy link Contributor merrymercy commented Apr 16, 2024 @ispobock This seems to be an interesting optimization. Did you check the correctness of many workloads? Did you notice any performance improvements? All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author ispobock commented Apr 17, 2024 \u2022 edited Loading Uh oh! There was an error while loading. Please reload this page . @merrymercy I tested on mmlu dataset with different parallel, the accuracy is the same with the main branch. The total latency has slight improvement. with optimization parallel total latency avg accuracy N 64 258.204 0.528 Y 64 257.729 0.528 N 128 254.929 0.528 Y 128 254.867 0.528 I also test on sharegpt dataset with 1000 prompts and found the throughput has slight improvement. without optimization: Total time: 175.29 s\nThroughput: 5.70 requests/s\nAverage latency: 65.24 s\nAverage latency per token: 0.18 s\nAverage latency per output token: 1.10 s with optimization: Total time: 173.21 s\nThroughput: 5.77 requests/s\nAverage latency: 64.15 s\nAverage latency per token: 0.17 s\nAverage latency per output token: 1.08 s Test env: model: llama2_13b_chat gpu: 1 A100 80G Reproduce: python -m sglang.launch_server --model-path /workdir/llama2_13b_chat/ --port 30000\n\npython benchmark/mmlu/bench_sglang.py\n\npython benchmark/latency_throughput/bench_throughput.py --backend srt --tokenizer /workdir/llama2_13b_chat/ --dataset /workdir/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate 128 --port 30000 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Collaborator Author ispobock commented Apr 17, 2024 For 5000 prompts of sharegpt dataset benchmark, the throughput is improved 11.2% with this optimization. without optimization: Total time: 973.67 s\nThroughput: 5.14 requests/s\nAverage latency: 439.68 s\nAverage latency per token: 1.17 s\nAverage latency per output token: 10.23 s with optimization: Total time: 874.84 s\nThroughput: 5.72 requests/s\nAverage latency: 382.35 s\nAverage latency per token: 1.01 s\nAverage latency per output token: 8.91 s All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Copy link Contributor merrymercy commented Apr 17, 2024 Awesome! cc @hnyls2002 @Ying1123 All reactions Sorry, something went wrong. Uh oh! There was an error while loading. Please reload this page . Ying1123 merged commit e822e59 into sgl-project : main Apr 17, 2024 zhyncs mentioned this pull request Apr 18, 2024 Torch engine prefix caching InternLM/lmdeploy#1393 Closed merrymercy mentioned this pull request May 16, 2024 [RFC] Automatic Prefix Caching vllm-project/vllm#2614 Closed timethink pushed a commit\n        to timethink/sglang\n      that referenced\n      this pull request Mar 9, 2025 Optimize radix tree matching ( sgl-project#364 ) a2497e9 Sign up for free to join this conversation on GitHub .\n    Already have an account? Sign in to comment",
  "timeline_extracted_at": "2025-09-11 19:01:12",
  "has_lm_eval": true,
  "has_performance": true,
  "has_serving": false,
  "has_general_test": true,
  "test_details": "LM_EVAL | PERF | TEST",
  "analysis_extracted_at": null,
  "models": [
    "N/A"
  ],
  "lm_eval_commands": null,
  "perf_command": "python benchmark/latency_throughput/bench_throughput.py --backend srt --tokenizer /workdir/llama2_13b_chat/ --dataset /workdir/ShareGPT_V3_unfiltered_cleaned_split.json --num-prompts 1000 --request-rate 128 --port 30000",
  "commit_subject": "Optimize radix tree matching (#364)",
  "commit_message": "Optimize radix tree matching (#364)",
  "commit_date": "2024-04-17T09:47:37-07:00",
  "files_changed": [
    "python/sglang/srt/managers/router/radix_cache.py"
  ],
  "functions_changed": [],
  "stats": {
    "commit_year": 2024,
    "num_edited_lines": 63,
    "num_files": 1,
    "num_hunks": 5,
    "num_non_test_edited_lines": 63,
    "num_non_test_files": 1,
    "num_test_files": 0,
    "only_non_test_files": 1,
    "only_test_files": 0
  },
  "diff_text": "diff --git a/python/sglang/srt/managers/router/radix_cache.py b/python/sglang/srt/managers/router/radix_cache.py\nindex 6ee670309..7bb8a4b2a 100644\n--- a/python/sglang/srt/managers/router/radix_cache.py\n+++ b/python/sglang/srt/managers/router/radix_cache.py\n@@ -11,6 +11,7 @@ class TreeNode:\n     def __init__(self):\n         self.children = defaultdict(TreeNode)\n         self.parent = None\n+        self.key = None\n         self.value = None\n         self.ref_counter = 0\n         self.last_access_time = time.time()\n@@ -37,6 +38,7 @@ class RadixCache:\n \n     def reset(self):\n         self.root_node = TreeNode()\n+        self.root_node.key = []\n         self.root_node.value = []\n         self.root_node.ref_counter = 1\n         self.evictable_size_ = 0\n@@ -115,40 +117,45 @@ class RadixCache:\n     ##### Internal Helper Functions #####\n     def _match_prefix_helper(self, node, key, value, last_node):\n         node.last_access_time = time.time()\n-\n-        for c_key, child in node.children.items():\n-            prefix_len = match(c_key, key)\n-            if prefix_len != 0:\n-                if prefix_len < len(c_key):\n-                    new_node = self._split_node(c_key, child, prefix_len)\n-                    value.append(new_node.value)\n-                    last_node[0] = new_node\n-                else:\n-                    value.append(child.value)\n-                    last_node[0] = child\n-                    self._match_prefix_helper(child, key[prefix_len:], value, last_node)\n-                break\n+        if len(key) == 0:\n+            return\n+\n+        if key[0] in node.children.keys():\n+            child = node.children[key[0]]\n+            prefix_len = match(child.key, key)\n+            if prefix_len < len(child.key):\n+                new_node = self._split_node(child.key, child, prefix_len)\n+                value.append(new_node.value)\n+                last_node[0] = new_node\n+            else:\n+                value.append(child.value)\n+                last_node[0] = child\n+                self._match_prefix_helper(child, key[prefix_len:], value, last_node)\n \n     def _split_node(self, key, child, split_len):\n         # new_node -> child\n         new_node = TreeNode()\n-        new_node.children = {key[split_len:]: child}\n+        new_node.children = {key[split_len:][0]: child}\n         new_node.parent = child.parent\n         new_node.ref_counter = child.ref_counter\n+        new_node.key = child.key[:split_len]\n         new_node.value = child.value[:split_len]\n         child.parent = new_node\n+        child.key = child.key[split_len:]\n         child.value = child.value[split_len:]\n-        new_node.parent.children[key[:split_len]] = new_node\n-        del new_node.parent.children[key]\n+        new_node.parent.children[key[:split_len][0]] = new_node\n         return new_node\n \n     def _insert_helper(self, node, key, value):\n         node.last_access_time = time.time()\n+        if len(key) == 0:\n+            return 0\n \n-        for c_key, child in node.children.items():\n-            prefix_len = match(c_key, key)\n+        if key[0] in node.children.keys():\n+            child = node.children[key[0]]\n+            prefix_len = match(child.key, key)\n \n-            if prefix_len == len(c_key):\n+            if prefix_len == len(child.key):\n                 if prefix_len == len(key):\n                     return prefix_len\n                 else:\n@@ -156,23 +163,23 @@ class RadixCache:\n                     value = value[prefix_len:]\n                     return prefix_len + self._insert_helper(child, key, value)\n \n-            if prefix_len:\n-                new_node = self._split_node(c_key, child, prefix_len)\n-                return prefix_len + self._insert_helper(\n-                    new_node, key[prefix_len:], value[prefix_len:]\n-                )\n+            new_node = self._split_node(child.key, child, prefix_len)\n+            return prefix_len + self._insert_helper(\n+                new_node, key[prefix_len:], value[prefix_len:]\n+            )\n \n         if len(key):\n             new_node = TreeNode()\n             new_node.parent = node\n+            new_node.key = key\n             new_node.value = value\n-            node.children[key] = new_node\n+            node.children[key[0]] = new_node\n             self.evictable_size_ += len(value)\n         return 0\n \n     def _print_helper(self, node, indent):\n-        for key, child in node.children.items():\n-            print(\" \" * indent, len(key), key[:10], f\"r={child.ref_counter}\")\n+        for _, child in node.children.items():\n+            print(\" \" * indent, len(child.key), child.key[:10], f\"r={child.ref_counter}\")\n             self._print_helper(child, indent=indent + 2)\n \n     def _delete_leaf(self, node):\n@@ -180,7 +187,7 @@ class RadixCache:\n             if v == node:\n                 break\n         del node.parent.children[k]\n-        self.evictable_size_ -= len(k)\n+        self.evictable_size_ -= len(node.key)\n \n     def _total_size_helper(self, node):\n         x = len(node.value)",
  "apis": [
    "sglang.srt.managers.router.Router"
  ],
  "affected_paths": [
    "/path/to/repos/sglang/python/sglang/api.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/router.py",
    "/path/to/repos/sglang/python/sglang/srt/layers/moe/router.py",
    "/path/to/repos/sglang/sgl-router/py_src/sglang_router/launch_router.py"
  ],
  "repo_path": "/path/to/repos/sglang",
  "llm_reason": "The changes in the RadixCache implementation modify the core lookup and insertion mechanisms of a radix tree for caching. Instead of iterating over all children in a loop, the implementation now uses direct dictionary key lookups (e.g., key[0] in node.children), which should speed-up common operations. The changes affect the non-test source file handling tree node splitting and key matching, which are central to the high-level API's performance. The commit message \"Optimize radix tree matching\" and the modifications support performance improvements for CPU operations without being tied to GPU/TPU specifics. Therefore, it qualifies as a performance, optimization-related commit.",
  "llm_api_reason": "This commit refactors and optimizes the internal radix tree matching logic used in the router\u2019s cache. Although the changes occur in internal helper functions of the RadixCache class, they ultimately affect the performance of the router manager, which is exposed via the public Router API. The optimization in tree splitting and prefix matching will improve routing efficiency through sglang.srt.managers.router.Router."
}