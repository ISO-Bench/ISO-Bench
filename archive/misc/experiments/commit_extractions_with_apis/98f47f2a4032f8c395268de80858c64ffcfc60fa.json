{
  "commit_hash": "98f47f2a4032f8c395268de80858c64ffcfc60fa",
  "parent_hash": "8c1e77fb585c4f42783a3d88c1efc7c9e15fd89f",
  "message": "[V1] Optimize the CPU overheads in FlashAttention custom op (#10733)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2024-11-28 09:01:02 -0800",
  "files_changed": [
    {
      "file_path": "vllm/v1/attention/backends/flash_attn.py",
      "old_content": "\"\"\"Attention layer with FlashAttention.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Type\n\nimport torch\n\nfrom vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                              AttentionMetadata, AttentionType)\nfrom vllm.forward_context import get_forward_context\nfrom vllm.utils import direct_register_custom_op\nfrom vllm.vllm_flash_attn import flash_attn_varlen_func\n\n\nclass FlashAttentionBackend(AttentionBackend):\n\n    @staticmethod\n    def get_supported_head_sizes() -> List[int]:\n        return [32, 64, 96, 128, 160, 192, 224, 256]\n\n    @staticmethod\n    def get_name() -> str:\n        return \"FLASH_ATTN_VLLM_V1\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"FlashAttentionImpl\"]:\n        return FlashAttentionImpl\n\n    @staticmethod\n    def get_metadata_cls() -> Type[\"AttentionMetadata\"]:\n        return FlashAttentionMetadata\n\n    @staticmethod\n    def get_kv_cache_shape(\n        num_blocks: int,\n        block_size: int,\n        num_kv_heads: int,\n        head_size: int,\n    ) -> Tuple[int, ...]:\n        if block_size % 16 != 0:\n            raise ValueError(\"Block size must be a multiple of 16.\")\n        return (2, num_blocks, block_size, num_kv_heads, head_size)\n\n\n@dataclass\nclass FlashAttentionMetadata:\n    # NOTE(sang): Definition of context_len, query_len, and seq_len.\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ---------------------|\n    #                                   |-- query_len ---|\n\n    num_actual_tokens: int  # Number of tokens excluding padding.\n    max_query_len: int\n    query_start_loc: torch.Tensor\n    max_seq_len: int\n    seq_start_loc: torch.Tensor\n    block_table: torch.Tensor\n    slot_mapping: torch.Tensor\n\n\nclass FlashAttentionImpl(AttentionImpl):\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n        logits_soft_cap: Optional[float] = None,\n    ) -> None:\n        if blocksparse_params is not None:\n            raise ValueError(\n                \"FlashAttention does not support block-sparse attention.\")\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        if alibi_slopes is not None:\n            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n        self.alibi_slopes = alibi_slopes\n        if sliding_window is None:\n            self.sliding_window = (-1, -1)\n        else:\n            self.sliding_window = (sliding_window - 1, 0)\n        self.kv_cache_dtype = kv_cache_dtype\n        if logits_soft_cap is None:\n            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.\n            logits_soft_cap = 0\n        self.logits_soft_cap = logits_soft_cap\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()\n        if head_size not in support_head_sizes:\n            raise ValueError(\n                f\"Head size {head_size} is not supported by FlashAttention. \"\n                f\"Supported head sizes are: {support_head_sizes}.\")\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: FlashAttentionMetadata,\n        k_scale: float = 1.0,\n        v_scale: float = 1.0,\n        attn_type: AttentionType = AttentionType.DECODER,\n    ) -> torch.Tensor:\n        \"\"\"Forward pass with FlashAttention.\n\n        Args:\n            query: shape = [num_tokens, num_heads * head_size]\n            key: shape = [num_tokens, num_kv_heads * head_size]\n            value: shape = [num_tokens, num_kv_heads * head_size]\n            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]\n            attn_metadata: Metadata for attention.\n        Returns:\n            shape = [num_tokens, num_heads * head_size]\n        \"\"\"\n        if attn_type != AttentionType.DECODER:\n            raise NotImplementedError(\"Encoder self-attention and \"\n                                      \"encoder/decoder cross-attention \"\n                                      \"are not implemented for \"\n                                      \"FlashAttentionImpl\")\n\n        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.\n        assert k_scale == 1.0 and v_scale == 1.0, (\n            \"key/v_scale is not supported in FlashAttention.\")\n\n        output = torch.empty_like(query)\n        torch.ops.vllm.unified_v1_flash_attention(\n            output,\n            query,\n            key,\n            value,\n            self.num_heads,\n            self.head_size,\n            self.num_kv_heads,\n            kv_cache,\n            self.kv_cache_dtype,\n            k_scale,\n            v_scale,\n            self.scale,\n            self.sliding_window,\n            self.alibi_slopes,\n            self.logits_soft_cap,\n        )\n        return output\n\n\ndef unified_v1_flash_attention(\n    output: torch.Tensor,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> None:\n    context = get_forward_context()\n    current_metadata = context.dynamic_forward_context\n    if current_metadata is None:\n        # Profiling run.\n        return\n\n    assert current_metadata is not None\n    assert isinstance(current_metadata, FlashAttentionMetadata)\n    attn_metadata: FlashAttentionMetadata = current_metadata\n    num_actual_tokens = attn_metadata.num_actual_tokens\n\n    # Reshape the query, key, and value tensors.\n    query = query.view(-1, num_heads, head_size)\n    key = key.view(-1, num_kv_heads, head_size)\n    value = value.view(-1, num_kv_heads, head_size)\n\n    # Reshape the input keys and values and store them in the cache.\n    key_cache = kv_cache[0]\n    value_cache = kv_cache[1]\n    torch.ops._C_cache_ops.reshape_and_cache_flash(\n        key[:num_actual_tokens],\n        value[:num_actual_tokens],\n        key_cache,\n        value_cache,\n        attn_metadata.slot_mapping,\n        kv_cache_dtype,\n        k_scale,\n        v_scale,\n    )\n\n    attn_output = flash_attn_varlen_func(\n        q=query[:num_actual_tokens],\n        k=key_cache,\n        v=value_cache,\n        cu_seqlens_q=attn_metadata.query_start_loc,\n        max_seqlen_q=attn_metadata.max_query_len,\n        cu_seqlens_k=attn_metadata.seq_start_loc,\n        max_seqlen_k=attn_metadata.max_seq_len,\n        softmax_scale=softmax_scale,\n        causal=True,\n        alibi_slopes=alibi_slopes,\n        window_size=window_size,\n        block_table=attn_metadata.block_table,\n        softcap=logits_soft_cap,\n    )\n    attn_output = attn_output.view(num_actual_tokens, -1)\n    # TODO(woosuk): Optimize this.\n    output[:num_actual_tokens].copy_(attn_output)\n\n\ndef unified_v1_flash_attention_fake(\n    output: torch.Tensor,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> None:\n    return\n\n\ndirect_register_custom_op(\n    op_name=\"unified_v1_flash_attention\",\n    op_func=unified_v1_flash_attention,\n    mutates_args=[\"kv_cache\", \"output\"],\n    fake_impl=unified_v1_flash_attention_fake,\n)\n",
      "diff": "diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py\nindex 5f8535eaa..e618edf7d 100644\n--- a/vllm/v1/attention/backends/flash_attn.py\n+++ b/vllm/v1/attention/backends/flash_attn.py\n@@ -135,6 +135,13 @@ class FlashAttentionImpl(AttentionImpl):\n         assert k_scale == 1.0 and v_scale == 1.0, (\n             \"key/v_scale is not supported in FlashAttention.\")\n \n+        # Reshape the query, key, and value tensors.\n+        # NOTE(woosuk): We do this outside the custom op to minimize the CPU\n+        # overheads from the non-CUDA-graph regions.\n+        query = query.view(-1, self.num_heads, self.head_size)\n+        key = key.view(-1, self.num_kv_heads, self.head_size)\n+        value = value.view(-1, self.num_kv_heads, self.head_size)\n+\n         output = torch.empty_like(query)\n         torch.ops.vllm.unified_v1_flash_attention(\n             output,\n@@ -153,7 +160,7 @@ class FlashAttentionImpl(AttentionImpl):\n             self.alibi_slopes,\n             self.logits_soft_cap,\n         )\n-        return output\n+        return output.view(-1, self.num_heads * self.head_size)\n \n \n def unified_v1_flash_attention(\n@@ -184,11 +191,6 @@ def unified_v1_flash_attention(\n     attn_metadata: FlashAttentionMetadata = current_metadata\n     num_actual_tokens = attn_metadata.num_actual_tokens\n \n-    # Reshape the query, key, and value tensors.\n-    query = query.view(-1, num_heads, head_size)\n-    key = key.view(-1, num_kv_heads, head_size)\n-    value = value.view(-1, num_kv_heads, head_size)\n-\n     # Reshape the input keys and values and store them in the cache.\n     key_cache = kv_cache[0]\n     value_cache = kv_cache[1]\n@@ -218,8 +220,7 @@ def unified_v1_flash_attention(\n         block_table=attn_metadata.block_table,\n         softcap=logits_soft_cap,\n     )\n-    attn_output = attn_output.view(num_actual_tokens, -1)\n-    # TODO(woosuk): Optimize this.\n+    # TODO(woosuk): Remove this unnecessary copy.\n     output[:num_actual_tokens].copy_(attn_output)",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 9
    }
  ],
  "affected_apis": [
    "vllm.v1.attention.backends.flash_attn.FlashAttentionImpl",
    "vllm.v1.attention.backends.flash_attn.unified_v1_flash_attention"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "NO (cpu)",
    "is_benchmark_actually_there": "",
    "sample_clues": "attention, attn, backends"
  }
}