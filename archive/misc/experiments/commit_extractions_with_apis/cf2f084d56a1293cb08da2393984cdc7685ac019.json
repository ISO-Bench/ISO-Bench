{
  "commit_hash": "cf2f084d56a1293cb08da2393984cdc7685ac019",
  "parent_hash": "f721096d48a7e3b98dffcb9b400bf58989cef64d",
  "message": "Dynamic scheduler delay to improve ITL performance  (#3279)\n\nCo-authored-by: Jan van Lunteren <jvl@zurich.ibm.com>",
  "author": "Thomas Parnell <tpa@zurich.ibm.com>",
  "date": "2024-03-22 12:28:14 -0700",
  "files_changed": [
    {
      "file_path": "tests/core/test_scheduler.py",
      "old_content": "from typing import List\nimport pytest  # noqa\n\nfrom vllm.config import CacheConfig, SchedulerConfig\nfrom vllm.core.scheduler import Scheduler\nfrom vllm.sequence import SequenceGroup, Logprob\n\nfrom .utils import create_dummy_prompt\n\n\ndef test_scheduler_add_seq_group():\n    block_size = 4\n    scheduler_config = SchedulerConfig(100, 64, 1)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq group to scheduler.\n    num_seq_group = 4\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        scheduler.add_seq_group(seq_group)\n        assert scheduler.get_num_unfinished_seq_groups() == i + 1\n\n\ndef test_scheduler_abort_seq_group():\n    block_size = 4\n    scheduler_config = SchedulerConfig(100, 64, 1)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add multiple seq groups to scheduler.\n    num_seq_group = 4\n    request_ids = set()\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        scheduler.add_seq_group(seq_group)\n        request_ids.add(str(i))\n\n    # Abort all added seq groups.\n    assert scheduler.get_num_unfinished_seq_groups() == num_seq_group\n    scheduler.abort_seq_group(request_ids)\n    assert scheduler.get_num_unfinished_seq_groups() == 0\n\n\ndef test_scheduler_schedule_simple():\n    block_size = 4\n    num_seq_group = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, num_seq_group, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    running: List[SequenceGroup] = []\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Schedule seq groups prompts.\n    num_tokens = block_size * num_seq_group\n    seq_group_meta, out = scheduler.schedule()\n    assert set(out.scheduled_seq_groups) == set(running)\n    assert out.num_batched_tokens == num_tokens\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = scheduler.schedule()\n    assert set(out.scheduled_seq_groups) == set(running)\n    assert out.num_batched_tokens == num_seq_group\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n\n\ndef test_scheduler_schedule_preempt_abort():\n    block_size = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, 2, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 2\n    cache_config.num_gpu_blocks = 2\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    seq_a, seq_group_a = create_dummy_prompt(\"1\", block_size)\n    seq_b, seq_group_b = create_dummy_prompt(\"2\", block_size)\n    scheduler.add_seq_group(seq_group_a)\n    scheduler.add_seq_group(seq_group_b)\n\n    # Schedule seq groups prompts.\n    seq_group_meta, out = scheduler.schedule()\n    assert out.scheduled_seq_groups == [seq_group_a, seq_group_b]\n    assert out.num_batched_tokens == block_size * 2  # seq_a and seq_b\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 2\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n\n    # Append \"generated\" tokens, allowing the sequence to mark prompt tokens as\n    # processed.\n    token_id = 0\n    seq_a.append_token_id(token_id, {token_id: Logprob(0.0)})\n    seq_b.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Schedule seq groups generation and preempt seq group b.\n    seq_group_meta, out = scheduler.schedule()\n    assert out.scheduled_seq_groups == [seq_group_a]\n    assert out.num_batched_tokens == 1\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n\n    # Abort seq group a. Re-schedule seq group b prompt with recomputation.\n    scheduler.abort_seq_group(\"1\")\n    seq_group_meta, out = scheduler.schedule()\n    assert out.scheduled_seq_groups == [seq_group_b]\n    assert out.num_batched_tokens == 5  # 4 prompt + 1 generation.\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 1\n\n\ndef test_scheduler_max_seqs():\n    block_size = 4\n    num_seq_group = 4\n    max_seq_group = 2\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(64, max_seq_group, max_model_len)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    all_seq_groups: List[SequenceGroup] = []\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size)\n        all_seq_groups.append(seq_group)\n\n    # Append 1 seq group\n    scheduler.add_seq_group(all_seq_groups[0])\n\n    # Schedule seq groups prompts.\n    _, out = scheduler.schedule()\n    assert set(out.scheduled_seq_groups) == set([all_seq_groups[0]])\n\n    # Schedule seq groups generation.\n    _, out = scheduler.schedule()\n    assert set(out.scheduled_seq_groups) == set([all_seq_groups[0]])\n\n    # Append 2 more seq group\n    scheduler.add_seq_group(all_seq_groups[1])\n    scheduler.add_seq_group(all_seq_groups[2])\n\n    # Schedule seq groups prompts.\n    # Only 1 seq group should be scheduled since max_seq_group is 2\n    # and one is prompting.\n    _, out = scheduler.schedule()\n    assert set(out.scheduled_seq_groups) == set([all_seq_groups[1]])\n",
      "diff": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 397101fa8..4a690e24e 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -1,5 +1,6 @@\n from typing import List\n import pytest  # noqa\n+import time\n \n from vllm.config import CacheConfig, SchedulerConfig\n from vllm.core.scheduler import Scheduler\n@@ -168,3 +169,36 @@ def test_scheduler_max_seqs():\n     # and one is prompting.\n     _, out = scheduler.schedule()\n     assert set(out.scheduled_seq_groups) == set([all_seq_groups[1]])\n+\n+\n+def test_scheduler_delay_factor():\n+\n+    block_size = 4\n+    scheduler_config = SchedulerConfig(100, 64, 16, delay_factor=0.5)\n+    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n+    cache_config.num_cpu_blocks = 8\n+    cache_config.num_gpu_blocks = 8\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+\n+    # schedule first prompt\n+    _, seq_group = create_dummy_prompt(\"0\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for a second before scheduling next prompt\n+    time.sleep(1)\n+    _, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size)\n+    scheduler.add_seq_group(seq_group)\n+\n+    # second prompt should *not* be scheduled\n+    seq_group_meta, out = scheduler.schedule()\n+    assert not out.prompt_run\n+    assert seq_group_meta[0].request_id == '0'\n+\n+    # wait for more than 0.5 second and try again\n+    time.sleep(0.6)\n+    seq_group_meta, out = scheduler.schedule()\n+    assert out.prompt_run\n+    assert seq_group_meta[0].request_id == '1'",
      "change_type": "modified",
      "lines_added": 35,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/config.py",
      "old_content": "from typing import TYPE_CHECKING, Optional, Union, ClassVar\nfrom dataclasses import dataclass\nimport os\nfrom packaging.version import Version\n\nimport json\nimport torch\nfrom transformers import PretrainedConfig\n\nfrom vllm.logger import init_logger\nfrom vllm.transformers_utils.config import get_config\nfrom vllm.utils import get_cpu_memory, is_hip, is_neuron, get_nvcc_cuda_version\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\nlogger = init_logger(__name__)\n\n_GB = 1 << 30\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, and \"slow\" will always use the slow tokenizer.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        download_dir: Directory to download and load the weights, default to the\n            default cache directory of huggingface.\n        load_format: The format of the model weights to load:\n            \"auto\" will try to load the weights in the safetensors format and\n                fall back to the pytorch bin format if safetensors format is\n                not available.\n            \"pt\" will load the weights in the pytorch bin format.\n            \"safetensors\" will load the weights in the safetensors format.\n            \"npcache\" will load the weights in pytorch format and store\n                a numpy cache to speed up the loading.\n            \"dummy\" will initialize the weights with random values, which is\n                mainly for profiling.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        code_revision: The specific revision to use for the model code on\n            Hugging Face Hub. It can be a branch name, a tag name, or a\n            commit id. If unspecified, will use the default version.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n        max_context_len_to_capture: Maximum context len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str,\n        tokenizer: str,\n        tokenizer_mode: str,\n        trust_remote_code: bool,\n        download_dir: Optional[str],\n        load_format: str,\n        dtype: Union[str, torch.dtype],\n        seed: int,\n        revision: Optional[str] = None,\n        code_revision: Optional[str] = None,\n        tokenizer_revision: Optional[str] = None,\n        max_model_len: Optional[int] = None,\n        quantization: Optional[str] = None,\n        enforce_eager: bool = False,\n        max_context_len_to_capture: Optional[int] = None,\n        max_logprobs: int = 5,\n    ) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.download_dir = download_dir\n        self.load_format = load_format\n        self.seed = seed\n        self.revision = revision\n        self.code_revision = code_revision\n        self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n        self.enforce_eager = enforce_eager\n        self.max_context_len_to_capture = max_context_len_to_capture\n        self.max_logprobs = max_logprobs\n\n        if os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\":\n            # download model from ModelScope hub,\n            # lazy import so that modelscope is not required for normal use.\n            from modelscope.hub.snapshot_download import snapshot_download  # pylint: disable=C\n\n            if not os.path.exists(model):\n                model_path = snapshot_download(model_id=model,\n                                               cache_dir=download_dir,\n                                               revision=revision)\n            else:\n                model_path = model\n            self.model = model_path\n            self.download_dir = model_path\n            self.tokenizer = model_path\n\n        self.hf_config = get_config(self.model, trust_remote_code, revision,\n                                    code_revision)\n        self.dtype = _get_and_verify_dtype(self.hf_config, dtype)\n        self.max_model_len = _get_and_verify_max_len(self.hf_config,\n                                                     max_model_len)\n        self._verify_load_format()\n        self._verify_tokenizer_mode()\n        self._verify_quantization()\n        self._verify_cuda_graph()\n\n    def _verify_load_format(self) -> None:\n        load_format = self.load_format.lower()\n        supported_load_format = [\n            \"auto\", \"pt\", \"safetensors\", \"npcache\", \"dummy\"\n        ]\n        rocm_not_supported_load_format = []\n        if load_format not in supported_load_format:\n            raise ValueError(\n                f\"Unknown load format: {self.load_format}. Must be one of \"\n                \"'auto', 'pt', 'safetensors', 'npcache', or 'dummy'.\")\n        if is_hip() and load_format in rocm_not_supported_load_format:\n            rocm_supported_load_format = [\n                f for f in supported_load_format\n                if (f not in rocm_not_supported_load_format)\n            ]\n            raise ValueError(\n                f\"load format '{load_format}' is not supported in ROCm. \"\n                f\"Supported load format are \"\n                f\"{rocm_supported_load_format}\")\n\n        # TODO: Remove this check once HF updates the pt weights of Mixtral.\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if \"MixtralForCausalLM\" in architectures and load_format == \"pt\":\n            raise ValueError(\n                \"Currently, the 'pt' format is not supported for Mixtral. \"\n                \"Please use the 'safetensors' format instead. \")\n        self.load_format = load_format\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto' or 'slow'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = [\"awq\", \"gptq\", \"squeezellm\", \"marlin\"]\n        rocm_not_supported_quantization = [\"awq\", \"marlin\"]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        hf_quant_config = getattr(self.hf_config, \"quantization_config\", None)\n        if hf_quant_config is not None:\n            hf_quant_method = str(hf_quant_config[\"quant_method\"]).lower()\n\n            # If the GPTQ model is serialized in marlin format, use marlin.\n            if (hf_quant_method == \"gptq\"\n                    and \"is_marlin_format\" in hf_quant_config\n                    and hf_quant_config[\"is_marlin_format\"]):\n                logger.info(\"The model is serialized in Marlin format. \"\n                            \"Using Marlin kernel.\")\n                hf_quant_method = \"marlin\"\n                if self.quantization == \"gptq\":\n                    self.quantization = hf_quant_method\n\n            if self.quantization is None:\n                self.quantization = hf_quant_method\n            elif self.quantization != hf_quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({hf_quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            if is_hip(\n            ) and self.quantization in rocm_not_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in ROCm.\")\n            if self.quantization != \"marlin\":\n                logger.warning(\n                    f\"{self.quantization} quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\")\n\n    def _verify_cuda_graph(self) -> None:\n        if self.max_context_len_to_capture is None:\n            self.max_context_len_to_capture = self.max_model_len\n        self.max_context_len_to_capture = min(self.max_context_len_to_capture,\n                                              self.max_model_len)\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = self.hf_config.num_attention_heads\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if total_num_hidden_layers % pipeline_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of hidden layers ({total_num_hidden_layers}) \"\n                \"must be divisible by pipeline parallel size \"\n                f\"({pipeline_parallel_size}).\")\n\n    def get_sliding_window(self) -> Optional[int]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_config, \"use_sliding_window\")\n                and not self.hf_config.use_sliding_window):\n            return None\n        return getattr(self.hf_config, \"sliding_window\", None)\n\n    def get_vocab_size(self) -> int:\n        return self.hf_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_config.hidden_size\n\n    def get_head_size(self) -> int:\n        if hasattr(self.hf_config, \"head_dim\"):\n            return self.hf_config.head_dim\n        # FIXME(woosuk): This may not be true for all models.\n        return self.hf_config.hidden_size // self.hf_config.num_attention_heads\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        total_num_hidden_layers = self.hf_config.num_hidden_layers\n        return total_num_hidden_layers // parallel_config.pipeline_parallel_size\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n        cache_dtype: Data type for kv cache storage.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: int,\n        cache_dtype: str,\n        sliding_window: Optional[int] = None,\n        enable_prefix_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.swap_space_bytes = swap_space * _GB\n        self.cache_dtype = cache_dtype\n        self.sliding_window = sliding_window\n        self.enable_prefix_caching = enable_prefix_caching\n        self._verify_args()\n        self._verify_cache_dtype()\n\n        # Will be set after profiling.\n        self.num_gpu_blocks = None\n        self.num_cpu_blocks = None\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype == \"fp8_e5m2\":\n            if is_hip():\n                raise NotImplementedError(\n                    \"FP8_E5M2 KV Cache on AMD GPU has not been supported yet.\")\n            nvcc_cuda_version = get_nvcc_cuda_version()\n            if nvcc_cuda_version and nvcc_cuda_version < Version(\"11.8\"):\n                raise ValueError(\n                    \"FP8 is not supported when cuda version is lower than 11.8.\"\n                )\n            logger.info(\n                \"Using fp8_e5m2 data type to store kv cache. It reduces \"\n                \"the GPU memory footprint and boosts the performance. \"\n                \"But it may cause slight accuracy drop. \"\n                \"Currently we only support fp8 without scaling factors and \"\n                \"make e5m2 as a default format.\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / _GB:.2f} GiB out of \"\n               f\"the {total_cpu_memory / _GB:.2f} GiB total CPU memory is \"\n               \"allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. \" + msg)\n\n\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"Configuration for the tokenizer pool.\n    \n    Args:\n        pool_size: Number of tokenizer workers in the pool.\n        pool_type: Type of the pool.\n        extra_config: Additional config for the pool.\n            The way the config will be used depends on the\n            pool type.\n    \"\"\"\n    pool_size: int\n    pool_type: str\n    extra_config: dict\n\n    def __post_init__(self):\n        if self.pool_type not in (\"ray\", ):\n            raise ValueError(f\"Unknown pool type: {self.pool_type}\")\n        if not isinstance(self.extra_config, dict):\n            raise ValueError(\"extra_config must be a dictionary.\")\n\n    @classmethod\n    def create_config(\n        cls, tokenizer_pool_size: int, tokenizer_pool_type: str,\n        tokenizer_pool_extra_config: Optional[Union[str, dict]]\n    ) -> Optional[\"TokenizerPoolConfig\"]:\n        \"\"\"Create a TokenizerPoolConfig from the given parameters.\n        \n        If tokenizer_pool_size is 0, return None.\n        \n        Args:\n            tokenizer_pool_size: Number of tokenizer workers in the pool.\n            tokenizer_pool_type: Type of the pool.\n            tokenizer_pool_extra_config: Additional config for the pool.\n                The way the config will be used depends on the\n                pool type. This can be a JSON string (will be parsed).\n        \"\"\"\n        if tokenizer_pool_size:\n            if isinstance(tokenizer_pool_extra_config, str):\n                tokenizer_pool_extra_config_parsed = json.loads(\n                    tokenizer_pool_extra_config)\n            else:\n                tokenizer_pool_extra_config_parsed = (\n                    tokenizer_pool_extra_config or {})\n            tokenizer_pool_config = cls(tokenizer_pool_size,\n                                        tokenizer_pool_type,\n                                        tokenizer_pool_extra_config_parsed)\n        else:\n            tokenizer_pool_config = None\n        return tokenizer_pool_config\n\n\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\n\n    Args:\n        pipeline_parallel_size: Number of pipeline parallel groups.\n        tensor_parallel_size: Number of tensor parallel groups.\n        worker_use_ray: Whether to use Ray for model workers. Will be set to\n            True if either pipeline_parallel_size or tensor_parallel_size is\n            greater than 1.\n        max_parallel_loading_workers: Maximum number of multiple batches\n            when load model sequentially. To avoid RAM OOM when using tensor\n            parallel and large models.\n        disable_custom_all_reduce: Disable the custom all-reduce kernel and\n            fall back to NCCL.\n        tokenizer_pool_config: Config for the tokenizer pool.\n            If None, will use synchronous tokenization.\n        ray_workers_use_nsight: Whether to profile Ray workers with nsight, see\n            https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_parallel_size: int,\n        tensor_parallel_size: int,\n        worker_use_ray: bool,\n        max_parallel_loading_workers: Optional[int] = None,\n        disable_custom_all_reduce: bool = False,\n        tokenizer_pool_config: Optional[TokenizerPoolConfig] = None,\n        ray_workers_use_nsight: bool = False,\n        placement_group: Optional[\"PlacementGroup\"] = None,\n    ) -> None:\n        self.pipeline_parallel_size = pipeline_parallel_size\n        self.tensor_parallel_size = tensor_parallel_size\n        self.worker_use_ray = worker_use_ray\n        self.max_parallel_loading_workers = max_parallel_loading_workers\n        self.disable_custom_all_reduce = disable_custom_all_reduce\n        self.tokenizer_pool_config = tokenizer_pool_config\n        self.ray_workers_use_nsight = ray_workers_use_nsight\n        self.placement_group = placement_group\n\n        self.world_size = pipeline_parallel_size * self.tensor_parallel_size\n        if self.world_size > 1:\n            self.worker_use_ray = True\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is not supported yet.\")\n        if not self.disable_custom_all_reduce and self.world_size > 1:\n            if is_hip():\n                self.disable_custom_all_reduce = True\n                logger.info(\n                    \"Disabled the custom all-reduce kernel because it is not \"\n                    \"supported on AMD GPUs.\")\n            elif self.pipeline_parallel_size > 1:\n                self.disable_custom_all_reduce = True\n                logger.info(\n                    \"Disabled the custom all-reduce kernel because it is not \"\n                    \"supported with pipeline parallelism.\")\n        if self.ray_workers_use_nsight and not self.worker_use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\n\n    Args:\n        max_num_batched_tokens: Maximum number of tokens to be processed in\n            a single iteration.\n        max_num_seqs: Maximum number of sequences to be processed in a single\n            iteration.\n        max_model_len: Maximum length of a sequence (including prompt\n            and generated text).\n    \"\"\"\n\n    def __init__(\n        self,\n        max_num_batched_tokens: Optional[int],\n        max_num_seqs: int,\n        max_model_len: int,\n    ) -> None:\n        if max_num_batched_tokens is not None:\n            self.max_num_batched_tokens = max_num_batched_tokens\n        else:\n            # If max_model_len is too short, use 2048 as the default value for\n            # higher throughput.\n            self.max_num_batched_tokens = max(max_model_len, 2048)\n        self.max_num_seqs = max_num_seqs\n        self.max_model_len = max_model_len\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.max_num_batched_tokens < self.max_model_len:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n\nclass DeviceConfig:\n\n    def __init__(self, device: str = \"auto\") -> None:\n        if device == \"auto\":\n            # Automated device type detection\n            if is_neuron():\n                self.device_type = \"neuron\"\n            else:\n                # We don't call torch.cuda.is_available() here to\n                # avoid initializing CUDA before workers are forked\n                self.device_type = \"cuda\"\n        else:\n            # Device type is assigned explicitly\n            self.device_type = device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\"]:\n            self.device = torch.device(\"cpu\")\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\n@dataclass\nclass LoRAConfig:\n    max_lora_rank: int\n    max_loras: int\n    max_cpu_loras: Optional[int] = None\n    lora_dtype: Optional[torch.dtype] = None\n    lora_extra_vocab_size: int = 256\n    # This is a constant.\n    lora_vocab_padding_size: ClassVar[int] = 256\n\n    def __post_init__(self):\n        # Keep this in sync with csrc/punica/bgmv/bgmv_config.h\n        possible_max_ranks = (8, 16, 32, 64)\n        possible_lora_extra_vocab_size = (0, 256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n        if model_config.quantization is not None:\n            raise ValueError(\n                \"LoRA is not supported with quantized models yet.\")\n\n    def verify_with_scheduler_config(self, scheduler_config: SchedulerConfig):\n        if scheduler_config.max_num_batched_tokens > 65528:\n            raise ValueError(\n                \"Due to limitations of the custom LoRA CUDA kernel, \"\n                \"max_num_batched_tokens must be <= 65528 when \"\n                \"LoRA is enabled.\")\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE = [\"float\", \"float32\"]\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            if config_dtype == torch.float32:\n                # Following the common practice, we use float16 for float32\n                # models.\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    if is_hip() and torch_dtype == torch.float32:\n        rocm_supported_dtypes = [\n            k for k, v in _STR_DTYPE_TO_TORCH_DTYPE.items()\n            if (k not in _ROCM_NOT_SUPPORTED_DTYPE)\n        ]\n        raise ValueError(f\"dtype '{dtype}' is not supported in ROCm. \"\n                         f\"Supported dtypes are {rocm_supported_dtypes}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(f\"Casting {config_dtype} to {torch_dtype}.\")\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    for key in possible_keys:\n        max_len_key = getattr(hf_config, key, None)\n        if max_len_key is not None:\n            derived_max_model_len = min(derived_max_model_len, max_len_key)\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            f\"{possible_keys}. Assuming the model's maximum length is \"\n            f\"{default_max_len}.\")\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        assert \"factor\" in rope_scaling\n        scaling_factor = rope_scaling[\"factor\"]\n        if rope_scaling[\"type\"] == \"yarn\":\n            derived_max_model_len = rope_scaling[\n                \"original_max_position_embeddings\"]\n        derived_max_model_len *= scaling_factor\n\n    if max_model_len is None:\n        max_model_len = derived_max_model_len\n    elif max_model_len > derived_max_model_len:\n        raise ValueError(\n            f\"User-specified max_model_len ({max_model_len}) is greater than \"\n            f\"the derived max_model_len ({max_len_key}={derived_max_model_len}\"\n            \" in model's config.json). This may lead to incorrect model \"\n            \"outputs or CUDA errors. Make sure the value is correct and \"\n            \"within the model context size.\")\n    return int(max_model_len)\n",
      "diff": "diff --git a/vllm/config.py b/vllm/config.py\nindex 6dfb51586..2003563e4 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -517,6 +517,8 @@ class SchedulerConfig:\n             iteration.\n         max_model_len: Maximum length of a sequence (including prompt\n             and generated text).\n+        delay_factor: Apply a delay (of delay factor multiplied by previous\n+            prompt latency) before scheduling next prompt.\n     \"\"\"\n \n     def __init__(\n@@ -524,6 +526,7 @@ class SchedulerConfig:\n         max_num_batched_tokens: Optional[int],\n         max_num_seqs: int,\n         max_model_len: int,\n+        delay_factor: float = 0.0,\n     ) -> None:\n         if max_num_batched_tokens is not None:\n             self.max_num_batched_tokens = max_num_batched_tokens\n@@ -533,6 +536,7 @@ class SchedulerConfig:\n             self.max_num_batched_tokens = max(max_model_len, 2048)\n         self.max_num_seqs = max_num_seqs\n         self.max_model_len = max_model_len\n+        self.delay_factor = delay_factor\n         self._verify_args()\n \n     def _verify_args(self) -> None:",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/core/scheduler.py",
      "old_content": "from collections import deque\nimport enum\nimport time\nfrom typing import Deque, Dict, Iterable, List, Optional, Tuple, Union, Set\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.block_manager import AllocStatus, BlockSpaceManager\nfrom vllm.core.policy import PolicyFactory\nfrom vllm.lora.request import LoRARequest\nfrom vllm.logger import init_logger\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceStatus)\n\nlogger = init_logger(__name__)\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\nclass SchedulerOutputs:\n\n    def __init__(\n        self,\n        scheduled_seq_groups: Iterable[SequenceGroup],\n        prompt_run: bool,\n        num_batched_tokens: int,\n        blocks_to_swap_in: Dict[int, int],\n        blocks_to_swap_out: Dict[int, int],\n        blocks_to_copy: Dict[int, List[int]],\n        ignored_seq_groups: List[SequenceGroup],\n    ) -> None:\n        self.scheduled_seq_groups = scheduled_seq_groups\n        self.prompt_run = prompt_run\n        self.num_batched_tokens = num_batched_tokens\n        self.blocks_to_swap_in = blocks_to_swap_in\n        self.blocks_to_swap_out = blocks_to_swap_out\n        self.blocks_to_copy = blocks_to_copy\n        # Swap in and swap out should never happen at the same time.\n        assert not (blocks_to_swap_in and blocks_to_swap_out)\n        self.ignored_seq_groups = ignored_seq_groups\n\n        self.num_loras = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self) -> bool:\n        self.scheduled_seq_groups = sorted(self.scheduled_seq_groups,\n                                           key=lambda g:\n                                           (g.lora_int_id, g.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {g.lora_request for g in self.scheduled_seq_groups}\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        self.prompt_limit = min(self.scheduler_config.max_model_len,\n                                self.scheduler_config.max_num_batched_tokens)\n\n        # Instantiate the scheduling policy.\n        self.policy = PolicyFactory.get_policy(policy_name=\"fcfs\")\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManager(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=self.cache_config.num_gpu_blocks,\n            num_cpu_blocks=self.cache_config.num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        self.swapped: Deque[SequenceGroup] = deque()\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\n        if isinstance(request_id, str):\n            request_id = (request_id, )\n        request_ids = set(request_id)\n        for state_queue in [self.waiting, self.running, self.swapped]:\n            aborted_groups: List[SequenceGroup] = []\n            for seq_group in state_queue:\n                if not request_ids:\n                    # Using 'break' here may add two extra iterations,\n                    # but is acceptable to reduce complexity .\n                    break\n                if seq_group.request_id in request_ids:\n                    # Appending aborted group into pending list.\n                    aborted_groups.append(seq_group)\n                    request_ids.remove(seq_group.request_id)\n            for aborted_group in aborted_groups:\n                # Remove the sequence group from the state queue.\n                state_queue.remove(aborted_group)\n                for seq in aborted_group.get_seqs():\n                    if seq.is_finished():\n                        continue\n                    seq.status = SequenceStatus.FINISHED_ABORTED\n                    self.free_seq(seq)\n\n    def has_unfinished_seqs(self) -> bool:\n        return self.waiting or self.running or self.swapped\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def _schedule(self) -> SchedulerOutputs:\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_in: Dict[int, int] = {}\n        blocks_to_swap_out: Dict[int, int] = {}\n        blocks_to_copy: Dict[int, List[int]] = {}\n\n        # Fix the current time.\n        now = time.time()\n\n        # Join waiting sequences if possible.\n        if not self.swapped:\n            ignored_seq_groups: List[SequenceGroup] = []\n            scheduled: List[SequenceGroup] = []\n            # The total number of sequences on the fly, including the\n            # requests in the generation phase.\n            num_curr_seqs = sum(seq_group.get_max_num_running_seqs()\n                                for seq_group in self.running)\n            curr_loras = set(\n                seq_group.lora_int_id\n                for seq_group in self.running) if self.lora_enabled else None\n\n            # Optimization: We do not sort the waiting queue since the preempted\n            # sequence groups are added to the front and the new sequence groups\n            # are added to the back.\n            leftover_waiting_sequences = deque()\n            num_batched_tokens = 0\n            while self.waiting:\n                seq_group = self.waiting[0]\n                waiting_seqs = seq_group.get_seqs(\n                    status=SequenceStatus.WAITING)\n                assert len(waiting_seqs) == 1, (\n                    \"Waiting sequence group should have only one prompt \"\n                    \"sequence.\")\n                num_prompt_tokens = waiting_seqs[0].get_len()\n                if num_prompt_tokens > self.prompt_limit:\n                    logger.warning(\n                        f\"Input prompt ({num_prompt_tokens} tokens) is too long\"\n                        f\" and exceeds limit of {self.prompt_limit}\")\n                    for seq in waiting_seqs:\n                        seq.status = SequenceStatus.FINISHED_IGNORED\n                    ignored_seq_groups.append(seq_group)\n                    self.waiting.popleft()\n                    continue\n\n                # If the sequence group cannot be allocated, stop.\n                can_allocate = self.block_manager.can_allocate(seq_group)\n                if can_allocate == AllocStatus.LATER:\n                    break\n                elif can_allocate == AllocStatus.NEVER:\n                    logger.warning(\n                        f\"Input prompt ({num_prompt_tokens} tokens) is too long\"\n                        f\" and exceeds the capacity of block_manager\")\n                    for seq in waiting_seqs:\n                        seq.status = SequenceStatus.FINISHED_IGNORED\n                    ignored_seq_groups.append(seq_group)\n                    self.waiting.popleft()\n                    continue\n\n                lora_int_id = 0\n                if self.lora_enabled:\n                    lora_int_id = seq_group.lora_int_id\n                    if (lora_int_id > 0 and lora_int_id not in curr_loras\n                            and len(curr_loras) >= self.lora_config.max_loras):\n                        # We don't have a space for another LoRA, so\n                        # we ignore this request for now.\n                        leftover_waiting_sequences.appendleft(seq_group)\n                        self.waiting.popleft()\n                        continue\n\n                # If the number of batched tokens exceeds the limit, stop.\n                num_batched_tokens += num_prompt_tokens\n                if (num_batched_tokens >\n                        self.scheduler_config.max_num_batched_tokens):\n                    break\n\n                # The total number of sequences in the RUNNING state should not\n                # exceed the maximum number of sequences.\n                num_new_seqs = seq_group.get_max_num_running_seqs()\n                if (num_curr_seqs + num_new_seqs >\n                        self.scheduler_config.max_num_seqs):\n                    break\n\n                if lora_int_id > 0:\n                    curr_loras.add(lora_int_id)\n                self.waiting.popleft()\n                self._allocate(seq_group)\n                self.running.append(seq_group)\n                num_curr_seqs += num_new_seqs\n                scheduled.append(seq_group)\n\n            self.waiting.extendleft(leftover_waiting_sequences)\n\n            if scheduled or ignored_seq_groups:\n                scheduler_outputs = SchedulerOutputs(\n                    scheduled_seq_groups=scheduled,\n                    prompt_run=True,\n                    num_batched_tokens=num_batched_tokens,\n                    blocks_to_swap_in=blocks_to_swap_in,\n                    blocks_to_swap_out=blocks_to_swap_out,\n                    blocks_to_copy=blocks_to_copy,\n                    ignored_seq_groups=ignored_seq_groups,\n                )\n                return scheduler_outputs\n\n        # NOTE(woosuk): Preemption happens only when there is no available slot\n        # to keep all the sequence groups in the RUNNING state.\n        # In this case, the policy is responsible for deciding which sequence\n        # groups to preempt.\n        self.running = self.policy.sort_by_priority(now, self.running)\n\n        # Reserve new token slots for the running sequence groups.\n        running: Deque[SequenceGroup] = deque()\n        preempted: List[SequenceGroup] = []\n        while self.running:\n            seq_group = self.running.popleft()\n            while not self.block_manager.can_append_slot(seq_group):\n                if self.running:\n                    # Preempt the lowest-priority sequence groups.\n                    victim_seq_group = self.running.pop()\n                    self._preempt(victim_seq_group, blocks_to_swap_out)\n                    preempted.append(victim_seq_group)\n                else:\n                    # No other sequence groups can be preempted.\n                    # Preempt the current sequence group.\n                    self._preempt(seq_group, blocks_to_swap_out)\n                    preempted.append(seq_group)\n                    break\n            else:\n                # Append new slots to the sequence group.\n                self._append_slot(seq_group, blocks_to_copy)\n                running.append(seq_group)\n        self.running = running\n\n        # Swap in the sequence groups in the SWAPPED state if possible.\n        self.swapped = self.policy.sort_by_priority(now, self.swapped)\n        if not preempted:\n            num_curr_seqs = sum(seq_group.get_max_num_running_seqs()\n                                for seq_group in self.running)\n            curr_loras = set(\n                seq_group.lora_int_id\n                for seq_group in self.running) if self.lora_enabled else None\n\n            leftover_swapped = deque()\n\n            while self.swapped:\n                seq_group = self.swapped[0]\n                lora_int_id = 0\n                if self.lora_enabled:\n                    lora_int_id = seq_group.lora_int_id\n                    if (lora_int_id > 0 and lora_int_id not in curr_loras\n                            and len(curr_loras) >= self.lora_config.max_loras):\n                        # We don't have a space for another LoRA, so\n                        # we ignore this request for now.\n                        leftover_swapped.appendleft(seq_group)\n                        self.swapped.popleft()\n                        continue\n\n                # If the sequence group cannot be swapped in, stop.\n                if not self.block_manager.can_swap_in(seq_group):\n                    break\n\n                # The total number of sequences in the RUNNING state should not\n                # exceed the maximum number of sequences.\n                num_new_seqs = seq_group.get_max_num_running_seqs()\n                if (num_curr_seqs + num_new_seqs >\n                        self.scheduler_config.max_num_seqs):\n                    break\n\n                if lora_int_id > 0:\n                    curr_loras.add(lora_int_id)\n                self.swapped.popleft()\n                self._swap_in(seq_group, blocks_to_swap_in)\n                self._append_slot(seq_group, blocks_to_copy)\n                num_curr_seqs += num_new_seqs\n                self.running.append(seq_group)\n\n            self.swapped.extendleft(leftover_swapped)\n\n        # Each sequence in the generation phase only takes one token slot.\n        # Therefore, the number of batched tokens is equal to the number of\n        # sequences in the RUNNING state.\n        num_batched_tokens = sum(\n            seq_group.num_seqs(status=SequenceStatus.RUNNING)\n            for seq_group in self.running)\n\n        scheduler_outputs = SchedulerOutputs(\n            scheduled_seq_groups=self.running,\n            prompt_run=False,\n            num_batched_tokens=num_batched_tokens,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_swap_out=blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=[],\n        )\n        return scheduler_outputs\n\n    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:\n        # Schedule sequence groups.\n        # This function call changes the internal states of the scheduler\n        # such as self.running, self.swapped, and self.waiting.\n        scheduler_outputs = self._schedule()\n        now = time.time()\n\n        # Create input data structures.\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        for seq_group in scheduler_outputs.scheduled_seq_groups:\n            seq_group.maybe_set_first_scheduled_time(now)\n\n            seq_data: Dict[int, SequenceData] = {}\n            block_tables: Dict[int, List[int]] = {}\n\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n                seq_id = seq.seq_id\n                seq_data[seq_id] = seq.data\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                self.block_manager.access_all_blocks_in_seq(seq, now)\n\n            seq_group_metadata = SequenceGroupMetadata(\n                request_id=seq_group.request_id,\n                is_prompt=scheduler_outputs.prompt_run,\n                seq_data=seq_data,\n                sampling_params=seq_group.sampling_params,\n                block_tables=block_tables,\n                lora_request=seq_group.lora_request,\n                computed_block_nums=self.block_manager.\n                get_common_computed_block_ids(seq_group),\n                state=seq_group.state,\n            )\n            seq_group_metadata_list.append(seq_group_metadata)\n        return seq_group_metadata_list, scheduler_outputs\n\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        self.block_manager.fork(parent_seq, child_seq)\n\n    def free_seq(self, seq: Sequence) -> None:\n        self.block_manager.free(seq)\n\n    def free_finished_seq_groups(self) -> None:\n        self.running = deque(seq_group for seq_group in self.running\n                             if not seq_group.is_finished())\n\n    def _allocate(self, seq_group: SequenceGroup) -> None:\n        self.block_manager.allocate(seq_group)\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            seq.status = SequenceStatus.RUNNING\n\n    def _append_slot(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_copy: Dict[int, List[int]],\n    ) -> None:\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            ret = self.block_manager.append_slot(seq)\n            if ret is not None:\n                src_block, dst_block = ret\n                if src_block in blocks_to_copy:\n                    blocks_to_copy[src_block].append(dst_block)\n                else:\n                    blocks_to_copy[src_block] = [dst_block]\n\n    def _preempt(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n        preemption_mode: Optional[PreemptionMode] = None,\n    ) -> None:\n        # If preemption mode is not specified, we determine the mode as follows:\n        # We use recomputation by default since it incurs lower overhead than\n        # swapping. However, when the sequence group has multiple sequences\n        # (e.g., beam search), recomputation is not currently supported. In\n        # such a case, we use swapping instead.\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\n        # As swapped sequences are prioritized over waiting sequences,\n        # sequence groups with multiple sequences are implicitly prioritized\n        # over sequence groups with a single sequence.\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\n        # sequences. This may require a more sophisticated CUDA kernel.\n        if preemption_mode is None:\n            if seq_group.get_max_num_running_seqs() == 1:\n                preemption_mode = PreemptionMode.RECOMPUTE\n            else:\n                preemption_mode = PreemptionMode.SWAP\n        if preemption_mode == PreemptionMode.RECOMPUTE:\n            self._preempt_by_recompute(seq_group)\n        elif preemption_mode == PreemptionMode.SWAP:\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\n        else:\n            raise AssertionError(\"Invalid preemption mode.\")\n\n    def _preempt_by_recompute(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        assert len(seqs) == 1\n        for seq in seqs:\n            seq.status = SequenceStatus.WAITING\n            self.block_manager.free(seq)\n        # NOTE: For FCFS, we insert the preempted sequence group to the front\n        # of the waiting queue.\n        self.waiting.appendleft(seq_group)\n\n    def _preempt_by_swap(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n    ) -> None:\n        self._swap_out(seq_group, blocks_to_swap_out)\n        self.swapped.append(seq_group)\n\n    def _swap_in(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_in: Dict[int, int],\n    ) -> None:\n        mapping = self.block_manager.swap_in(seq_group)\n        blocks_to_swap_in.update(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            seq.status = SequenceStatus.RUNNING\n\n    def _swap_out(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: Dict[int, int],\n    ) -> None:\n        if not self.block_manager.can_swap_out(seq_group):\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\n            # entire engine.\n            raise RuntimeError(\n                \"Aborted due to the lack of CPU swap space. Please increase \"\n                \"the swap space to avoid this error.\")\n        mapping = self.block_manager.swap_out(seq_group)\n        blocks_to_swap_out.update(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            seq.status = SequenceStatus.SWAPPED\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        self.block_manager.mark_blocks_as_computed(seq_group)\n",
      "diff": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex be55e8520..4bd0ef360 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -103,6 +103,13 @@ class Scheduler:\n         # Sequence groups in the SWAPPED state.\n         self.swapped: Deque[SequenceGroup] = deque()\n \n+        # Time at previous scheduling step\n+        self.prev_time = 0.0\n+        # Did we schedule a prompt at previous step?\n+        self.prev_prompt = False\n+        # Latency of the last prompt step\n+        self.last_prompt_latency = 0.0\n+\n     @property\n     def lora_enabled(self) -> bool:\n         return bool(self.lora_config)\n@@ -179,7 +186,7 @@ class Scheduler:\n             # are added to the back.\n             leftover_waiting_sequences = deque()\n             num_batched_tokens = 0\n-            while self.waiting:\n+            while self._passed_delay(now) and self.waiting:\n                 seq_group = self.waiting[0]\n                 waiting_seqs = seq_group.get_seqs(\n                     status=SequenceStatus.WAITING)\n@@ -246,6 +253,7 @@ class Scheduler:\n             self.waiting.extendleft(leftover_waiting_sequences)\n \n             if scheduled or ignored_seq_groups:\n+                self.prev_prompt = True\n                 scheduler_outputs = SchedulerOutputs(\n                     scheduled_seq_groups=scheduled,\n                     prompt_run=True,\n@@ -491,3 +499,19 @@ class Scheduler:\n \n     def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n         self.block_manager.mark_blocks_as_computed(seq_group)\n+\n+    def _passed_delay(self, now: float) -> bool:\n+        if self.prev_prompt:\n+            self.last_prompt_latency = now - self.prev_time\n+        self.prev_time, self.prev_prompt = now, False\n+        # Delay scheduling prompts to let waiting queue fill up\n+        if self.scheduler_config.delay_factor > 0 and self.waiting:\n+            earliest_arrival_time = min(\n+                [e.metrics.arrival_time for e in self.waiting])\n+            passed_delay = (\n+                (now - earliest_arrival_time) >\n+                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n+                or not self.running)\n+        else:\n+            passed_delay = True\n+        return passed_delay",
      "change_type": "modified",
      "lines_added": 26,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/engine/arg_utils.py",
      "old_content": "import argparse\nimport dataclasses\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple\n\nfrom vllm.config import (CacheConfig, DeviceConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig, LoRAConfig,\n                         TokenizerPoolConfig)\n\n\n@dataclass\nclass EngineArgs:\n    \"\"\"Arguments for vLLM engine.\"\"\"\n    model: str\n    tokenizer: Optional[str] = None\n    tokenizer_mode: str = 'auto'\n    trust_remote_code: bool = False\n    download_dir: Optional[str] = None\n    load_format: str = 'auto'\n    dtype: str = 'auto'\n    kv_cache_dtype: str = 'auto'\n    seed: int = 0\n    max_model_len: Optional[int] = None\n    worker_use_ray: bool = False\n    pipeline_parallel_size: int = 1\n    tensor_parallel_size: int = 1\n    max_parallel_loading_workers: Optional[int] = None\n    block_size: int = 16\n    enable_prefix_caching: bool = False\n    swap_space: int = 4  # GiB\n    gpu_memory_utilization: float = 0.90\n    max_num_batched_tokens: Optional[int] = None\n    max_num_seqs: int = 256\n    max_logprobs: int = 5  # OpenAI default value\n    disable_log_stats: bool = False\n    revision: Optional[str] = None\n    code_revision: Optional[str] = None\n    tokenizer_revision: Optional[str] = None\n    quantization: Optional[str] = None\n    enforce_eager: bool = False\n    max_context_len_to_capture: int = 8192\n    disable_custom_all_reduce: bool = False\n    tokenizer_pool_size: int = 0\n    tokenizer_pool_type: str = \"ray\"\n    tokenizer_pool_extra_config: Optional[dict] = None\n    enable_lora: bool = False\n    max_loras: int = 1\n    max_lora_rank: int = 16\n    lora_extra_vocab_size: int = 256\n    lora_dtype = 'auto'\n    max_cpu_loras: Optional[int] = None\n    device: str = 'auto'\n    ray_workers_use_nsight: bool = False\n\n    def __post_init__(self):\n        if self.tokenizer is None:\n            self.tokenizer = self.model\n\n    @staticmethod\n    def add_cli_args(\n            parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        \"\"\"Shared CLI arguments for vLLM engine.\"\"\"\n\n        # NOTE: If you update any of the arguments below, please also\n        # make sure to update docs/source/models/engine_args.rst\n\n        # Model arguments\n        parser.add_argument(\n            '--model',\n            type=str,\n            default='facebook/opt-125m',\n            help='name or path of the huggingface model to use')\n        parser.add_argument(\n            '--tokenizer',\n            type=str,\n            default=EngineArgs.tokenizer,\n            help='name or path of the huggingface tokenizer to use')\n        parser.add_argument(\n            '--revision',\n            type=str,\n            default=None,\n            help='the specific model version to use. It can be a branch '\n            'name, a tag name, or a commit id. If unspecified, will use '\n            'the default version.')\n        parser.add_argument(\n            '--code-revision',\n            type=str,\n            default=None,\n            help='the specific revision to use for the model code on '\n            'Hugging Face Hub. It can be a branch name, a tag name, or a '\n            'commit id. If unspecified, will use the default version.')\n        parser.add_argument(\n            '--tokenizer-revision',\n            type=str,\n            default=None,\n            help='the specific tokenizer version to use. It can be a branch '\n            'name, a tag name, or a commit id. If unspecified, will use '\n            'the default version.')\n        parser.add_argument('--tokenizer-mode',\n                            type=str,\n                            default=EngineArgs.tokenizer_mode,\n                            choices=['auto', 'slow'],\n                            help='tokenizer mode. \"auto\" will use the fast '\n                            'tokenizer if available, and \"slow\" will '\n                            'always use the slow tokenizer.')\n        parser.add_argument('--trust-remote-code',\n                            action='store_true',\n                            help='trust remote code from huggingface')\n        parser.add_argument('--download-dir',\n                            type=str,\n                            default=EngineArgs.download_dir,\n                            help='directory to download and load the weights, '\n                            'default to the default cache dir of '\n                            'huggingface')\n        parser.add_argument(\n            '--load-format',\n            type=str,\n            default=EngineArgs.load_format,\n            choices=['auto', 'pt', 'safetensors', 'npcache', 'dummy'],\n            help='The format of the model weights to load. '\n            '\"auto\" will try to load the weights in the safetensors format '\n            'and fall back to the pytorch bin format if safetensors format '\n            'is not available. '\n            '\"pt\" will load the weights in the pytorch bin format. '\n            '\"safetensors\" will load the weights in the safetensors format. '\n            '\"npcache\" will load the weights in pytorch format and store '\n            'a numpy cache to speed up the loading. '\n            '\"dummy\" will initialize the weights with random values, '\n            'which is mainly for profiling.')\n        parser.add_argument(\n            '--dtype',\n            type=str,\n            default=EngineArgs.dtype,\n            choices=[\n                'auto', 'half', 'float16', 'bfloat16', 'float', 'float32'\n            ],\n            help='data type for model weights and activations. '\n            'The \"auto\" option will use FP16 precision '\n            'for FP32 and FP16 models, and BF16 precision '\n            'for BF16 models.')\n        parser.add_argument(\n            '--kv-cache-dtype',\n            type=str,\n            choices=['auto', 'fp8_e5m2'],\n            default=EngineArgs.kv_cache_dtype,\n            help='Data type for kv cache storage. If \"auto\", will use model '\n            'data type. Note FP8 is not supported when cuda version is '\n            'lower than 11.8.')\n        parser.add_argument('--max-model-len',\n                            type=int,\n                            default=EngineArgs.max_model_len,\n                            help='model context length. If unspecified, '\n                            'will be automatically derived from the model.')\n        # Parallel arguments\n        parser.add_argument('--worker-use-ray',\n                            action='store_true',\n                            help='use Ray for distributed serving, will be '\n                            'automatically set when using more than 1 GPU')\n        parser.add_argument('--pipeline-parallel-size',\n                            '-pp',\n                            type=int,\n                            default=EngineArgs.pipeline_parallel_size,\n                            help='number of pipeline stages')\n        parser.add_argument('--tensor-parallel-size',\n                            '-tp',\n                            type=int,\n                            default=EngineArgs.tensor_parallel_size,\n                            help='number of tensor parallel replicas')\n        parser.add_argument(\n            '--max-parallel-loading-workers',\n            type=int,\n            default=EngineArgs.max_parallel_loading_workers,\n            help='load model sequentially in multiple batches, '\n            'to avoid RAM OOM when using tensor '\n            'parallel and large models')\n        parser.add_argument(\n            '--ray-workers-use-nsight',\n            action='store_true',\n            help='If specified, use nsight to profile ray workers')\n        # KV cache arguments\n        parser.add_argument('--block-size',\n                            type=int,\n                            default=EngineArgs.block_size,\n                            choices=[8, 16, 32, 128],\n                            help='token block size')\n\n        parser.add_argument('--enable-prefix-caching',\n                            action='store_true',\n                            help='Enables automatic prefix caching')\n\n        parser.add_argument('--seed',\n                            type=int,\n                            default=EngineArgs.seed,\n                            help='random seed')\n        parser.add_argument('--swap-space',\n                            type=int,\n                            default=EngineArgs.swap_space,\n                            help='CPU swap space size (GiB) per GPU')\n        parser.add_argument(\n            '--gpu-memory-utilization',\n            type=float,\n            default=EngineArgs.gpu_memory_utilization,\n            help='the fraction of GPU memory to be used for '\n            'the model executor, which can range from 0 to 1.'\n            'If unspecified, will use the default value of 0.9.')\n        parser.add_argument('--max-num-batched-tokens',\n                            type=int,\n                            default=EngineArgs.max_num_batched_tokens,\n                            help='maximum number of batched tokens per '\n                            'iteration')\n        parser.add_argument('--max-num-seqs',\n                            type=int,\n                            default=EngineArgs.max_num_seqs,\n                            help='maximum number of sequences per iteration')\n        parser.add_argument(\n            '--max-logprobs',\n            type=int,\n            default=EngineArgs.max_logprobs,\n            help=('max number of log probs to return logprobs is specified in'\n                  ' SamplingParams'))\n        parser.add_argument('--disable-log-stats',\n                            action='store_true',\n                            help='disable logging statistics')\n        # Quantization settings.\n        parser.add_argument('--quantization',\n                            '-q',\n                            type=str,\n                            choices=['awq', 'gptq', 'squeezellm', None],\n                            default=EngineArgs.quantization,\n                            help='Method used to quantize the weights. If '\n                            'None, we first check the `quantization_config` '\n                            'attribute in the model config file. If that is '\n                            'None, we assume the model weights are not '\n                            'quantized and use `dtype` to determine the data '\n                            'type of the weights.')\n        parser.add_argument('--enforce-eager',\n                            action='store_true',\n                            help='Always use eager-mode PyTorch. If False, '\n                            'will use eager mode and CUDA graph in hybrid '\n                            'for maximal performance and flexibility.')\n        parser.add_argument('--max-context-len-to-capture',\n                            type=int,\n                            default=EngineArgs.max_context_len_to_capture,\n                            help='maximum context length covered by CUDA '\n                            'graphs. When a sequence has context length '\n                            'larger than this, we fall back to eager mode.')\n        parser.add_argument('--disable-custom-all-reduce',\n                            action='store_true',\n                            default=EngineArgs.disable_custom_all_reduce,\n                            help='See ParallelConfig')\n        parser.add_argument('--tokenizer-pool-size',\n                            type=int,\n                            default=EngineArgs.tokenizer_pool_size,\n                            help='Size of tokenizer pool to use for '\n                            'asynchronous tokenization. If 0, will '\n                            'use synchronous tokenization.')\n        parser.add_argument('--tokenizer-pool-type',\n                            type=str,\n                            default=EngineArgs.tokenizer_pool_type,\n                            help='Type of tokenizer pool to use for '\n                            'asynchronous tokenization. Ignored '\n                            'if tokenizer_pool_size is 0.')\n        parser.add_argument('--tokenizer-pool-extra-config',\n                            type=str,\n                            default=EngineArgs.tokenizer_pool_extra_config,\n                            help='Extra config for tokenizer pool. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary. Ignored if '\n                            'tokenizer_pool_size is 0.')\n        # LoRA related configs\n        parser.add_argument('--enable-lora',\n                            action='store_true',\n                            help='If True, enable handling of LoRA adapters.')\n        parser.add_argument('--max-loras',\n                            type=int,\n                            default=EngineArgs.max_loras,\n                            help='Max number of LoRAs in a single batch.')\n        parser.add_argument('--max-lora-rank',\n                            type=int,\n                            default=EngineArgs.max_lora_rank,\n                            help='Max LoRA rank.')\n        parser.add_argument(\n            '--lora-extra-vocab-size',\n            type=int,\n            default=EngineArgs.lora_extra_vocab_size,\n            help=('Maximum size of extra vocabulary that can be '\n                  'present in a LoRA adapter (added to the base '\n                  'model vocabulary).'))\n        parser.add_argument(\n            '--lora-dtype',\n            type=str,\n            default=EngineArgs.lora_dtype,\n            choices=['auto', 'float16', 'bfloat16', 'float32'],\n            help=('Data type for LoRA. If auto, will default to '\n                  'base model dtype.'))\n        parser.add_argument(\n            '--max-cpu-loras',\n            type=int,\n            default=EngineArgs.max_cpu_loras,\n            help=('Maximum number of LoRAs to store in CPU memory. '\n                  'Must be >= than max_num_seqs. '\n                  'Defaults to max_num_seqs.'))\n        parser.add_argument(\"--device\",\n                            type=str,\n                            default=EngineArgs.device,\n                            choices=[\"auto\", \"cuda\", \"neuron\"],\n                            help='Device type for vLLM execution.')\n        return parser\n\n    @classmethod\n    def from_cli_args(cls, args: argparse.Namespace) -> 'EngineArgs':\n        # Get the list of attributes of this dataclass.\n        attrs = [attr.name for attr in dataclasses.fields(cls)]\n        # Set the attributes from the parsed arguments.\n        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})\n        return engine_args\n\n    def create_engine_configs(\n        self,\n    ) -> Tuple[ModelConfig, CacheConfig, ParallelConfig, SchedulerConfig,\n               DeviceConfig, Optional[LoRAConfig]]:\n        device_config = DeviceConfig(self.device)\n        model_config = ModelConfig(\n            self.model, self.tokenizer, self.tokenizer_mode,\n            self.trust_remote_code, self.download_dir, self.load_format,\n            self.dtype, self.seed, self.revision, self.code_revision,\n            self.tokenizer_revision, self.max_model_len, self.quantization,\n            self.enforce_eager, self.max_context_len_to_capture,\n            self.max_logprobs)\n        cache_config = CacheConfig(self.block_size,\n                                   self.gpu_memory_utilization,\n                                   self.swap_space, self.kv_cache_dtype,\n                                   model_config.get_sliding_window())\n        parallel_config = ParallelConfig(\n            self.pipeline_parallel_size, self.tensor_parallel_size,\n            self.worker_use_ray, self.max_parallel_loading_workers,\n            self.disable_custom_all_reduce,\n            TokenizerPoolConfig.create_config(\n                self.tokenizer_pool_size,\n                self.tokenizer_pool_type,\n                self.tokenizer_pool_extra_config,\n            ), self.ray_workers_use_nsight)\n        scheduler_config = SchedulerConfig(self.max_num_batched_tokens,\n                                           self.max_num_seqs,\n                                           model_config.max_model_len)\n        lora_config = LoRAConfig(\n            max_lora_rank=self.max_lora_rank,\n            max_loras=self.max_loras,\n            lora_extra_vocab_size=self.lora_extra_vocab_size,\n            lora_dtype=self.lora_dtype,\n            max_cpu_loras=self.max_cpu_loras if self.max_cpu_loras\n            and self.max_cpu_loras > 0 else None) if self.enable_lora else None\n        return (model_config, cache_config, parallel_config, scheduler_config,\n                device_config, lora_config)\n\n\n@dataclass\nclass AsyncEngineArgs(EngineArgs):\n    \"\"\"Arguments for asynchronous vLLM engine.\"\"\"\n    engine_use_ray: bool = False\n    disable_log_requests: bool = False\n    max_log_len: Optional[int] = None\n\n    @staticmethod\n    def add_cli_args(\n            parser: argparse.ArgumentParser) -> argparse.ArgumentParser:\n        parser = EngineArgs.add_cli_args(parser)\n        parser.add_argument('--engine-use-ray',\n                            action='store_true',\n                            help='use Ray to start the LLM engine in a '\n                            'separate process as the server process.')\n        parser.add_argument('--disable-log-requests',\n                            action='store_true',\n                            help='disable logging requests')\n        parser.add_argument('--max-log-len',\n                            type=int,\n                            default=None,\n                            help='max number of prompt characters or prompt '\n                            'ID numbers being printed in log. '\n                            'Default: unlimited.')\n        return parser\n",
      "diff": "diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 94c80f428..2070686ea 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -51,6 +51,7 @@ class EngineArgs:\n     max_cpu_loras: Optional[int] = None\n     device: str = 'auto'\n     ray_workers_use_nsight: bool = False\n+    scheduler_delay_factor: float = 0.0\n \n     def __post_init__(self):\n         if self.tokenizer is None:\n@@ -305,6 +306,12 @@ class EngineArgs:\n                             default=EngineArgs.device,\n                             choices=[\"auto\", \"cuda\", \"neuron\"],\n                             help='Device type for vLLM execution.')\n+        parser.add_argument(\n+            '--scheduler-delay-factor',\n+            type=float,\n+            default=EngineArgs.scheduler_delay_factor,\n+            help='Apply a delay (of delay factor multiplied by previous'\n+            'prompt latency) before scheduling next prompt.')\n         return parser\n \n     @classmethod\n@@ -342,7 +349,8 @@ class EngineArgs:\n             ), self.ray_workers_use_nsight)\n         scheduler_config = SchedulerConfig(self.max_num_batched_tokens,\n                                            self.max_num_seqs,\n-                                           model_config.max_model_len)\n+                                           model_config.max_model_len,\n+                                           self.scheduler_delay_factor)\n         lora_config = LoRAConfig(\n             max_lora_rank=self.max_lora_rank,\n             max_loras=self.max_loras,",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 2
    }
  ],
  "affected_apis": [
    "vllm.config.SchedulerConfig.__init__",
    "vllm.core.scheduler.Scheduler.schedule",
    "vllm.engine.arg_utils.EngineArgs.__init__"
  ],
  "summary": {
    "total_files": 4,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 4
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_scheduler)",
    "is_benchmark_actually_there": "",
    "sample_clues": "arg, arg_utils, config"
  }
}