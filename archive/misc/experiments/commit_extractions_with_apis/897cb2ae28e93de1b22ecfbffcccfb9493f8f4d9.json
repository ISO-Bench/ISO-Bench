{
  "commit_hash": "897cb2ae28e93de1b22ecfbffcccfb9493f8f4d9",
  "parent_hash": "1f01a18d39b7fc873b79024b5799597cb6fc88bc",
  "message": "Optimize data movement (#20)",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2023-04-02 00:30:17 -0700",
  "files_changed": [
    {
      "file_path": "cacheflow/models/activation.py",
      "old_content": "",
      "diff": "diff --git a/cacheflow/models/activation.py b/cacheflow/models/activation.py\nnew file mode 100644\nindex 000000000..c3267ebcb\n--- /dev/null\n+++ b/cacheflow/models/activation.py\n@@ -0,0 +1,20 @@\n+import torch\n+import torch.nn as nn\n+\n+from cacheflow import activation_ops\n+\n+\n+class SiluAndMul(nn.Module):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def forward(\n+        self,\n+        x: torch.Tensor,        # (num_tokens, 2 * d)\n+    ) -> torch.Tensor:          # (num_tokens, d)\n+        num_tokens = x.shape[0]\n+        d = x.shape[1] // 2\n+        out = torch.empty(num_tokens, d, dtype=x.dtype, device=x.device)\n+        activation_ops.silu_and_mul(out, x)\n+        return out",
      "change_type": "added",
      "lines_added": 21,
      "lines_removed": 1
    },
    {
      "file_path": "cacheflow/models/attention.py",
      "old_content": "from typing import List, Optional\n\nfrom flash_attn.flash_attention import FlashAttention\nimport torch\nimport torch.nn as nn\n\nfrom cacheflow import attention_ops\nfrom cacheflow import cache_ops\nfrom cacheflow import pos_encoding_ops\nfrom cacheflow.models import InputMetadata\n\n\nclass GPTCacheFlowAttention(nn.Module):\n\n    def __init__(self, scale: float) -> None:\n        super().__init__()\n        self.scale = float(scale)\n\n        self.flash_attn = FlashAttention(softmax_scale=self.scale)\n\n    def multi_query_kv_attention(\n        self,\n        output: torch.Tensor,       # [num_prompt_tokens, num_heads, head_size]\n        query: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n        key: torch.Tensor,          # [num_prompt_tokens, num_heads, head_size]\n        value: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n        prompt_lens: List[int],\n    ) -> None:\n        if query.dtype == torch.float:\n            raise ValueError('The float data type is not supported by '\n                             'FlashAttention. Use the half data type instead.')\n        head_size = query.shape[2]\n        if head_size > 128:\n            raise ValueError('FlashAttention does not support head_size > 128.')\n\n        device = query.device\n        prefix_sum = [0]\n        for prompt_len in prompt_lens:\n            prefix_sum.append(prefix_sum[-1] + prompt_len)\n        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n        max_prompt_len = max(prompt_lens)\n\n        # FIXME(woosuk): Unnecessary copy. Optimize this.\n        qkv = torch.stack([query, key, value], dim=1)\n        out = self.flash_attn(\n            qkv,\n            cu_seqlens=prefix_sum,\n            max_s=max_prompt_len,\n            causal=True,\n        )[0]\n        # FIXME(woosuk): Unnecessary copy. Optimize this.\n        output.copy_(out, non_blocking=True)\n\n    def single_query_cached_kv_attention(\n        self,\n        output: torch.Tensor,           # [num_generation_tokens, num_heads, head_size]\n        query: torch.Tensor,            # [num_generation_tokens, num_heads, head_size]\n        key_cache: torch.Tensor,        # [num_blocks, num_heads, head_size/x, block_size, x]\n        value_cache: torch.Tensor,      # [num_blocks, num_heads, head_size, block_size]\n        input_metadata: InputMetadata,\n    ) -> None:\n        head_size = value_cache.shape[2]\n        supported_head_sizes = [32, 64, 80, 96, 128, 160, 192, 256]\n        if head_size not in supported_head_sizes:\n            raise ValueError(f'head_size ({head_size}) is not supported by '\n                             'the single_query_cached_kv_attention kernel. '\n                             'Use one of the following head sizes: '\n                             f'{supported_head_sizes}.')\n\n        block_size = value_cache.shape[3]\n        attention_ops.single_query_cached_kv_attention(\n            output,\n            query,\n            key_cache,\n            value_cache,\n            self.scale,\n            input_metadata.block_tables,\n            input_metadata.context_lens,\n            block_size,\n            input_metadata.max_context_len,\n        )\n\n    def forward(\n        self,\n        query: torch.Tensor,                    # [num_tokens, num_heads * head_size]\n        key: torch.Tensor,                      # [num_tokens, num_heads * head_size]\n        value: torch.Tensor,                    # [num_tokens, num_heads * head_size]\n        key_cache: torch.Tensor,                # [num_blocks, num_heads, head_size/x, block_size, x]\n        value_cache: torch.Tensor,              # [num_blocks, num_heads, head_size, block_size]\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:                          # [num_tokens, num_heads * head_size]\n        # Pre-allocate the output tensor.\n        output = torch.empty_like(query)\n\n        # Prune out paddings if any.\n        query = query[:input_metadata.num_valid_tokens]\n        key = key[:input_metadata.num_valid_tokens]\n        value = value[:input_metadata.num_valid_tokens]\n\n        # Reshape the input tensors.\n        num_heads = value_cache.shape[1]\n        head_size = value_cache.shape[2]\n        query = query.view(-1, num_heads, head_size)\n        key = key.view(-1, num_heads, head_size)\n        value = value.view(-1, num_heads, head_size)\n        output = output.view(-1, num_heads, head_size)\n\n        # Compute the attention op for prompts.\n        num_prompt_tokens = input_metadata.num_prompt_tokens\n        if num_prompt_tokens > 0:\n            self.multi_query_kv_attention(\n                output[:num_prompt_tokens],\n                query[:num_prompt_tokens],\n                key[:num_prompt_tokens],\n                value[:num_prompt_tokens],\n                input_metadata.prompt_lens,\n            )\n\n        # Wait until the cache op is done.\n        if cache_event is not None:\n            cache_event.wait()\n\n        # Reshape the keys and values and store them in the cache.\n        cache_ops.reshape_and_cache(\n            key, value, key_cache, value_cache, input_metadata.slot_mapping)\n\n        if input_metadata.num_generation_tokens > 0:\n            # Compute the attention op for generation tokens.\n            self.single_query_cached_kv_attention(\n                output[num_prompt_tokens:],\n                query[num_prompt_tokens:],\n                key_cache,\n                value_cache,\n                input_metadata)\n\n        # Reshape the output tensor.\n        # NOTE(woosuk): The output tensor may include paddings.\n        return output.view(-1, num_heads * head_size)\n\n\nclass OPTCacheFlowAttention(GPTCacheFlowAttention):\n    \"\"\"OPT uses the same attention mechanism as GPT.\"\"\"\n\n    def __init__(self, scale: float) -> None:\n        super().__init__(scale)\n\n\nclass LlamaCacheFlowAttention(GPTCacheFlowAttention):\n    \"\"\"Llama uses GPT-NeoX style rotary embedding.\"\"\"\n\n    def __init__(\n        self,\n        scale: float,\n        head_size: int,\n        max_position: int = 8192,\n        base: int = 10000,\n    ) -> None:\n        super().__init__(scale)\n\n        # Create the cos and sin cache.\n        inv_freq = 1.0 / (base ** (torch.arange(0, head_size, 2) / head_size))\n        t = torch.arange(max_position).float()\n        freqs = torch.einsum('i,j -> ij', t, inv_freq.float())\n        cos = freqs.cos()\n        sin = freqs.sin()\n        cache = torch.cat((cos, sin), dim=-1)\n\n        # FIXME(woosuk): This assumes that we configure the default dtype when\n        # initializing the model. Make it more robust.\n        torch_dtype = torch.get_default_dtype()\n        cache = cache.to(torch_dtype)\n        # Embedding size: [max_position, head_size]\n        self.register_buffer('cos_sin_cache', cache, persistent=False)\n\n    def forward(\n        self,\n        positions: torch.LongTensor,            # [num_tokens]\n        query: torch.Tensor,                    # [num_tokens, num_heads * head_size]\n        key: torch.Tensor,                      # [num_tokens, num_heads * head_size]\n        value: torch.Tensor,                    # [num_tokens, num_heads * head_size]\n        key_cache: torch.Tensor,                # [num_blocks, num_heads, head_size/x, block_size, x]\n        value_cache: torch.Tensor,              # [num_blocks, num_heads, head_size, block_size]\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:                          # [num_tokens, num_heads * head_size]\n        # Apply rotary embedding to the query and key before passing them\n        # to the attention op.\n        out_query = torch.empty_like(query)\n        out_key = torch.empty_like(key)\n        pos_encoding_ops.rotary_embedding_neox(\n            out_query,\n            out_key,\n            positions,\n            query,\n            key,\n            self.cos_sin_cache,\n        )\n        return super().forward(\n            out_query,\n            out_key,\n            value,\n            key_cache,\n            value_cache,\n            input_metadata,\n            cache_event,\n        )\n",
      "diff": "diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nindex 8b132e442..6fa197e7c 100644\n--- a/cacheflow/models/attention.py\n+++ b/cacheflow/models/attention.py\n@@ -1,6 +1,6 @@\n-from typing import List, Optional\n+from typing import Optional\n \n-from flash_attn.flash_attention import FlashAttention\n+from flash_attn.flash_attn_interface import _flash_attn_forward\n import torch\n import torch.nn as nn\n \n@@ -16,40 +16,38 @@ class GPTCacheFlowAttention(nn.Module):\n         super().__init__()\n         self.scale = float(scale)\n \n-        self.flash_attn = FlashAttention(softmax_scale=self.scale)\n-\n     def multi_query_kv_attention(\n         self,\n-        output: torch.Tensor,       # [num_prompt_tokens, num_heads, head_size]\n-        query: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n-        key: torch.Tensor,          # [num_prompt_tokens, num_heads, head_size]\n-        value: torch.Tensor,        # [num_prompt_tokens, num_heads, head_size]\n-        prompt_lens: List[int],\n+        output: torch.Tensor,                   # [num_prompt_tokens, num_heads, head_size]\n+        query: torch.Tensor,                    # [num_prompt_tokens, num_heads, head_size]\n+        key: torch.Tensor,                      # [num_prompt_tokens, num_heads, head_size]\n+        value: torch.Tensor,                    # [num_prompt_tokens, num_heads, head_size]\n+        cumulative_prompt_lens: torch.Tensor,   # [num_prompts + 1]\n+        max_prompt_len: int,\n     ) -> None:\n         if query.dtype == torch.float:\n             raise ValueError('The float data type is not supported by '\n                              'FlashAttention. Use the half data type instead.')\n-        head_size = query.shape[2]\n+        head_size = query.shape[-1]\n         if head_size > 128:\n             raise ValueError('FlashAttention does not support head_size > 128.')\n \n-        device = query.device\n-        prefix_sum = [0]\n-        for prompt_len in prompt_lens:\n-            prefix_sum.append(prefix_sum[-1] + prompt_len)\n-        prefix_sum = torch.tensor(prefix_sum, dtype=torch.int, device=device)\n-        max_prompt_len = max(prompt_lens)\n-\n-        # FIXME(woosuk): Unnecessary copy. Optimize this.\n-        qkv = torch.stack([query, key, value], dim=1)\n-        out = self.flash_attn(\n-            qkv,\n-            cu_seqlens=prefix_sum,\n-            max_s=max_prompt_len,\n+        # Directly call FlashAttention's internal function to avoid allocating\n+        # a new tensor for the output.\n+        _flash_attn_forward(\n+            query,\n+            key,\n+            value,\n+            output,\n+            cumulative_prompt_lens,\n+            cumulative_prompt_lens,\n+            max_prompt_len,\n+            max_prompt_len,\n+            dropout_p=0.0,\n+            softmax_scale=self.scale,\n             causal=True,\n-        )[0]\n-        # FIXME(woosuk): Unnecessary copy. Optimize this.\n-        output.copy_(out, non_blocking=True)\n+            return_softmax=False,\n+        )\n \n     def single_query_cached_kv_attention(\n         self,\n@@ -90,21 +88,18 @@ class GPTCacheFlowAttention(nn.Module):\n         input_metadata: InputMetadata,\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:                          # [num_tokens, num_heads * head_size]\n-        # Pre-allocate the output tensor.\n-        output = torch.empty_like(query)\n-\n-        # Prune out paddings if any.\n-        query = query[:input_metadata.num_valid_tokens]\n-        key = key[:input_metadata.num_valid_tokens]\n-        value = value[:input_metadata.num_valid_tokens]\n+        # NOTE: The query, key, and value tensors must be sliced from a qkv\n+        # tensor of shape [num_tokens, 3 * num_heads * head_size].\n \n-        # Reshape the input tensors.\n+        # Reshape the query, key, and value tensors.\n         num_heads = value_cache.shape[1]\n         head_size = value_cache.shape[2]\n         query = query.view(-1, num_heads, head_size)\n         key = key.view(-1, num_heads, head_size)\n         value = value.view(-1, num_heads, head_size)\n-        output = output.view(-1, num_heads, head_size)\n+\n+        # Pre-allocate the output tensor.\n+        output = torch.empty_like(query)\n \n         # Compute the attention op for prompts.\n         num_prompt_tokens = input_metadata.num_prompt_tokens\n@@ -114,7 +109,8 @@ class GPTCacheFlowAttention(nn.Module):\n                 query[:num_prompt_tokens],\n                 key[:num_prompt_tokens],\n                 value[:num_prompt_tokens],\n-                input_metadata.prompt_lens,\n+                input_metadata.cumulative_prompt_lens,\n+                input_metadata.max_prompt_len,\n             )\n \n         # Wait until the cache op is done.\n@@ -122,14 +118,22 @@ class GPTCacheFlowAttention(nn.Module):\n             cache_event.wait()\n \n         # Reshape the keys and values and store them in the cache.\n-        cache_ops.reshape_and_cache(\n-            key, value, key_cache, value_cache, input_metadata.slot_mapping)\n+        num_valid_tokens = input_metadata.num_valid_tokens\n+        if num_valid_tokens > 0:\n+            # The stride is 3 because the key and value are sliced from qkv.\n+            cache_ops.reshape_and_cache(\n+                key[:num_valid_tokens],\n+                value[:num_valid_tokens],\n+                key_cache,\n+                value_cache,\n+                input_metadata.slot_mapping,\n+            )\n \n         if input_metadata.num_generation_tokens > 0:\n             # Compute the attention op for generation tokens.\n             self.single_query_cached_kv_attention(\n-                output[num_prompt_tokens:],\n-                query[num_prompt_tokens:],\n+                output[num_prompt_tokens:num_valid_tokens],\n+                query[num_prompt_tokens:num_valid_tokens],\n                 key_cache,\n                 value_cache,\n                 input_metadata)\n@@ -186,19 +190,15 @@ class LlamaCacheFlowAttention(GPTCacheFlowAttention):\n     ) -> torch.Tensor:                          # [num_tokens, num_heads * head_size]\n         # Apply rotary embedding to the query and key before passing them\n         # to the attention op.\n-        out_query = torch.empty_like(query)\n-        out_key = torch.empty_like(key)\n         pos_encoding_ops.rotary_embedding_neox(\n-            out_query,\n-            out_key,\n             positions,\n             query,\n             key,\n             self.cos_sin_cache,\n         )\n         return super().forward(\n-            out_query,\n-            out_key,\n+            query,\n+            key,\n             value,\n             key_cache,\n             value_cache,",
      "change_type": "modified",
      "lines_added": 47,
      "lines_removed": 47
    },
    {
      "file_path": "cacheflow/models/input_metadata.py",
      "old_content": "from typing import List, Dict, Tuple\n\nimport torch\n\nfrom cacheflow.sampling_params import SamplingParams\n\n\nclass InputMetadata:\n\n    def __init__(\n        self,\n        seq_groups: List[Tuple[List[int], SamplingParams]],\n        seq_logprobs: Dict[int, float],                         # Seq id -> cumulative logprobs.\n        prompt_lens: List[int],\n        slot_mapping: torch.Tensor,\n        context_lens: torch.Tensor,\n        max_context_len: int,\n        block_tables: torch.Tensor,\n    ) -> None:\n        self.seq_groups = seq_groups\n        self.seq_logprobs = seq_logprobs\n        self.prompt_lens = prompt_lens\n        self.slot_mapping = slot_mapping\n        self.context_lens = context_lens\n        self.max_context_len = max_context_len\n        self.block_tables = block_tables\n\n        self.num_prompts = len(prompt_lens)\n        self.num_prompt_tokens = sum(prompt_lens)\n        self.num_generation_tokens = context_lens.shape[0]\n        self.num_valid_tokens = slot_mapping.shape[0]\n        if block_tables.numel() > 0:\n            self.max_num_blocks_per_seq = block_tables.shape[1]\n        else:\n            self.max_num_blocks_per_seq = 0\n        assert block_tables.shape[0] == self.num_generation_tokens\n        assert context_lens.shape[0] == self.num_generation_tokens\n\n    def __repr__(self) -> str:\n        return (f'InputMetadata('\n                f'num_prompts={self.num_prompts}, '\n                f'num_prompt_tokens={self.num_prompt_tokens}, '\n                f'num_generation_tokens={self.num_generation_tokens}, '\n                f'num_valid_tokens={self.num_valid_tokens}, '\n                f'max_num_blocks_per_seq={self.max_num_blocks_per_seq}, '\n                f'max_context_len={self.max_context_len}), '\n                f'prompt_lens={self.prompt_lens}, '\n                f'slot_mapping={self.slot_mapping}, '\n                f'context_lens={self.context_lens}, '\n                f'block_tables={self.block_tables})')\n",
      "diff": "diff --git a/cacheflow/models/input_metadata.py b/cacheflow/models/input_metadata.py\nindex 8a341fbac..c61bfff20 100644\n--- a/cacheflow/models/input_metadata.py\n+++ b/cacheflow/models/input_metadata.py\n@@ -12,6 +12,7 @@ class InputMetadata:\n         seq_groups: List[Tuple[List[int], SamplingParams]],\n         seq_logprobs: Dict[int, float],                         # Seq id -> cumulative logprobs.\n         prompt_lens: List[int],\n+        cumulative_prompt_lens: torch.Tensor,\n         slot_mapping: torch.Tensor,\n         context_lens: torch.Tensor,\n         max_context_len: int,\n@@ -20,6 +21,7 @@ class InputMetadata:\n         self.seq_groups = seq_groups\n         self.seq_logprobs = seq_logprobs\n         self.prompt_lens = prompt_lens\n+        self.cumulative_prompt_lens = cumulative_prompt_lens\n         self.slot_mapping = slot_mapping\n         self.context_lens = context_lens\n         self.max_context_len = max_context_len\n@@ -27,6 +29,7 @@ class InputMetadata:\n \n         self.num_prompts = len(prompt_lens)\n         self.num_prompt_tokens = sum(prompt_lens)\n+        self.max_prompt_len = max(prompt_lens) if prompt_lens else 0\n         self.num_generation_tokens = context_lens.shape[0]\n         self.num_valid_tokens = slot_mapping.shape[0]\n         if block_tables.numel() > 0:\n@@ -40,11 +43,13 @@ class InputMetadata:\n         return (f'InputMetadata('\n                 f'num_prompts={self.num_prompts}, '\n                 f'num_prompt_tokens={self.num_prompt_tokens}, '\n+                f'max_prompt_len={self.max_prompt_len}, '\n                 f'num_generation_tokens={self.num_generation_tokens}, '\n                 f'num_valid_tokens={self.num_valid_tokens}, '\n                 f'max_num_blocks_per_seq={self.max_num_blocks_per_seq}, '\n                 f'max_context_len={self.max_context_len}), '\n                 f'prompt_lens={self.prompt_lens}, '\n+                f'cumulative_prompt_lens={self.cumulative_prompt_lens}, '\n                 f'slot_mapping={self.slot_mapping}, '\n                 f'context_lens={self.context_lens}, '\n                 f'block_tables={self.block_tables})')",
      "change_type": "modified",
      "lines_added": 6,
      "lines_removed": 1
    },
    {
      "file_path": "cacheflow/models/llama.py",
      "old_content": "\"\"\"1D LLaMA model compatible with HuggingFace weights.\"\"\"\nimport os\nimport glob\nimport filelock\nfrom tqdm import tqdm\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import LlamaConfig\n\nfrom cacheflow.models import InputMetadata\nfrom cacheflow.models.attention import LlamaCacheFlowAttention\nfrom cacheflow.models.layernorm import RMSNorm\nfrom cacheflow.models.sample import Sampler\nfrom cacheflow.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom cacheflow.parallel_utils.tensor_parallel import (VocabParallelEmbedding,\n                                                      ColumnParallelLinear,\n                                                      RowParallelLinear)\nfrom cacheflow.sequence import SequenceOutputs\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass LlamaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_up_proj = ColumnParallelLinear(hidden_size, 2 * intermediate_size,\n                                                 bias=False, gather_output=False,\n                                                 perform_initialization=False)\n        self.down_proj = RowParallelLinear(intermediate_size, hidden_size,\n                                           bias=False, input_is_parallel=True,\n                                           perform_initialization=False)\n        assert hidden_act == 'silu'\n        self.act_fn = nn.SiLU()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        gate_up = gate_up.reshape(gate_up.shape[:-1] + (2, -1))\n        gate, up = torch.split(gate_up, 1, dim=-2)\n        gate = gate.squeeze(dim=-2).contiguous()\n        up = up.squeeze(dim=-2).contiguous()\n        x = self.act_fn(gate) * up\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass LlamaAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = self.total_num_heads // tensor_model_parallel_world_size\n        self.head_dim = hidden_size // self.total_num_heads\n        self.scaling = self.head_dim ** -0.5\n\n        self.qkv_proj = ColumnParallelLinear(\n            hidden_size,\n            3 * self.total_num_heads * self.head_dim,\n            bias=False,\n            gather_output=False,\n            perform_initialization=False,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            input_is_parallel=True,\n            perform_initialization=False,\n        )\n        self.attn = LlamaCacheFlowAttention(self.scaling, self.head_dim)\n\n    def forward(\n        self,\n        positions: torch.LongTensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        qkv = qkv.reshape(qkv.shape[:-1] + (3, -1))\n        q, k, v = torch.split(qkv, 1, dim=-2)\n        q = q.squeeze(dim=-2).contiguous()\n        k = k.squeeze(dim=-2).contiguous()\n        v = v.squeeze(dim=-2).contiguous()\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(\n            positions, q, k, v, k_cache, v_cache, input_metadata, cache_event)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass LlamaDecoderLayer(nn.Module):\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n        )\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.LongTensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n            cache_event=cache_event,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nclass LlamaModel(nn.Module):\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(config.vocab_size, config.hidden_size,\n                                                   perform_initialization=False)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        for i in range(len(self.layers)):\n            if cache_events is None:\n                cache_event = None\n            else:\n                cache_event = cache_events[i]\n            layer = self.layers[i]\n            hidden_states = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                cache_event,\n            )\n        hidden_states = self.norm(hidden_states)\n        return hidden_states\n\n\nclass LlamaForCausalLM(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = LlamaModel(config)\n        self.lm_head = ColumnParallelLinear(config.hidden_size,\n                                            config.vocab_size,\n                                            bias=False,\n                                            gather_output=False,\n                                            perform_initialization=False)\n        self.sampler = Sampler()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> Dict[int, SequenceOutputs]:\n        hidden_states = self.model(\n            input_ids, positions, kv_caches, input_metadata, cache_events)\n        next_tokens = self.sampler(\n            self.lm_head.weight, hidden_states, input_metadata)\n        return next_tokens\n\n    _column_parallel_weights = [\"embed_tokens.weight\", \"lm_head.weight\",\n                                \"qkv_proj.weight\", \"gate_proj.weight\",\n                                \"up_proj.weight\"]\n    _row_parallel_weights = [\"o_proj.weight\", \"down_proj.weight\"]\n\n    def load_weights(self, weights_path: str):\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\n        state_dict = self.state_dict()\n        for name, param in state_dict.items():\n            if \"qkv_proj\" in name or \"gate_up_proj\" in name:\n                if \"qkv_proj\" in name:\n                    original_name = \"qkv_proj\"\n                    weight_names = [\"q_proj\", \"k_proj\", \"v_proj\"]\n                    shard_size = param.shape[0] // 3\n                else:\n                    original_name = \"gate_up_proj\"\n                    weight_names = [\"gate_proj\", \"up_proj\"]\n                    shard_size = param.shape[0] // 2\n                weights_to_concat = []\n                for weight_name in weight_names:\n                    weight = np.load(os.path.join(\n                        weights_path, name.replace(original_name, weight_name)))\n                    weights_to_concat.append(weight[\n                        shard_size * tensor_model_parallel_rank\n                        :shard_size * (tensor_model_parallel_rank + 1)])\n                loaded_weight = torch.from_numpy(\n                    np.concatenate(weights_to_concat, axis=0))\n            else:\n                loaded_weight = torch.from_numpy(\n                    np.load(os.path.join(weights_path, name)))\n                for p in self._column_parallel_weights:\n                    if p in name:\n                        shard_size = param.shape[0]\n                        loaded_weight = loaded_weight[\n                            shard_size * tensor_model_parallel_rank\n                            :shard_size * (tensor_model_parallel_rank + 1)]\n                        break\n                for p in self._row_parallel_weights:\n                    if p in name:\n                        shard_size = param.shape[1]\n                        loaded_weight = loaded_weight[\n                            :,\n                            shard_size * tensor_model_parallel_rank\n                            :shard_size * (tensor_model_parallel_rank + 1)]\n                        break\n\n            assert param.shape == loaded_weight.shape\n            param.data.copy_(loaded_weight)\n\n    @staticmethod\n    def get_weights(model_name: str, path: str):\n        if not os.path.isfile(os.path.join(model_name, \"config.json\")):\n            raise ValueError(\"LLaMA model's model_name has to be a path\"\n                             \"to the huggingface model's directory.\")\n        path = os.path.join(model_name, f\"np\")\n        path = os.path.abspath(os.path.expanduser(path))\n        os.makedirs(path, exist_ok=True)\n        lock_path = os.path.join(path, \"file_lock\")\n        lock = filelock.FileLock(lock_path)\n\n        with lock:\n            test_weight_path = os.path.join(path, \"model.embed_tokens.weight\")\n            if os.path.exists(test_weight_path):\n                return path\n\n            bin_files = glob.glob(os.path.join(model_name, \"*.bin\"))\n\n            for bin_file in tqdm(bin_files, desc=\"Convert format\"):\n                state = torch.load(bin_file, map_location=\"cpu\")\n                for name, param in tqdm(state.items(), leave=False):\n                    param_path = os.path.join(path, name)\n                    with open(param_path, \"wb\") as f:\n                        np.save(f, param.cpu().detach().numpy())\n\n            return path\n",
      "diff": "diff --git a/cacheflow/models/llama.py b/cacheflow/models/llama.py\nindex 86f350bd1..2a3c8b007 100644\n--- a/cacheflow/models/llama.py\n+++ b/cacheflow/models/llama.py\n@@ -11,6 +11,7 @@ from torch import nn\n from transformers import LlamaConfig\n \n from cacheflow.models import InputMetadata\n+from cacheflow.models.activation import SiluAndMul\n from cacheflow.models.attention import LlamaCacheFlowAttention\n from cacheflow.models.layernorm import RMSNorm\n from cacheflow.models.sample import Sampler\n@@ -39,16 +40,14 @@ class LlamaMLP(nn.Module):\n         self.down_proj = RowParallelLinear(intermediate_size, hidden_size,\n                                            bias=False, input_is_parallel=True,\n                                            perform_initialization=False)\n-        assert hidden_act == 'silu'\n-        self.act_fn = nn.SiLU()\n+        if hidden_act != 'silu':\n+            raise ValueError(f'Unsupported activation: {hidden_act}. '\n+                             'Only silu is supported for now.')\n+        self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n         gate_up, _ = self.gate_up_proj(x)\n-        gate_up = gate_up.reshape(gate_up.shape[:-1] + (2, -1))\n-        gate, up = torch.split(gate_up, 1, dim=-2)\n-        gate = gate.squeeze(dim=-2).contiguous()\n-        up = up.squeeze(dim=-2).contiguous()\n-        x = self.act_fn(gate) * up\n+        x = self.act_fn(gate_up)\n         x, _ = self.down_proj(x)\n         return x\n \n@@ -94,11 +93,7 @@ class LlamaAttention(nn.Module):\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        qkv = qkv.reshape(qkv.shape[:-1] + (3, -1))\n-        q, k, v = torch.split(qkv, 1, dim=-2)\n-        q = q.squeeze(dim=-2).contiguous()\n-        k = k.squeeze(dim=-2).contiguous()\n-        v = v.squeeze(dim=-2).contiguous()\n+        q, k, v = qkv.chunk(chunks=3, dim=-1)\n         k_cache, v_cache = kv_cache\n         attn_output = self.attn(\n             positions, q, k, v, k_cache, v_cache, input_metadata, cache_event)",
      "change_type": "modified",
      "lines_added": 8,
      "lines_removed": 13
    },
    {
      "file_path": "cacheflow/models/opt.py",
      "old_content": "\"\"\"1D OPT model compatible with HuggingFace weights.\"\"\"\nimport os\nimport glob\nimport filelock\nfrom tqdm import tqdm\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom transformers import OPTConfig\nfrom huggingface_hub import snapshot_download\n\nfrom cacheflow.models import InputMetadata\nfrom cacheflow.models.attention import OPTCacheFlowAttention\nfrom cacheflow.models.sample import Sampler\nfrom cacheflow.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom cacheflow.parallel_utils.tensor_parallel import (VocabParallelEmbedding,\n                                                      ColumnParallelLinear,\n                                                      RowParallelLinear)\nfrom cacheflow.sequence import SequenceOutputs\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass OPTLearnedPositionalEmbedding(nn.Embedding):\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models don't have this hack\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n\n    def forward(self, positions: torch.LongTensor):\n        return super().forward(positions + self.offset)\n\n\nclass OPTAttention(nn.Module):\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        bias: bool = True,\n    ) -> None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size()\n        total_num_heads = num_heads\n        assert num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = total_num_heads // tensor_model_parallel_world_size\n        self.head_dim = embed_dim // total_num_heads\n        self.scaling = self.head_dim ** -0.5\n\n        self.qkv_proj = ColumnParallelLinear(embed_dim, 3 * embed_dim, bias=bias,\n                                             gather_output=False,\n                                             perform_initialization=False)\n        self.out_proj = RowParallelLinear(embed_dim, embed_dim, bias=bias,\n                                          input_is_parallel=True,\n                                          perform_initialization=False)\n        self.attn = OPTCacheFlowAttention(scale=self.scaling)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        qkv = qkv.reshape(qkv.shape[:-1] + (3, -1))\n        q, k, v = torch.split(qkv, 1, dim=-2)\n        q = q.squeeze(dim=-2).contiguous()\n        k = k.squeeze(dim=-2).contiguous()\n        v = v.squeeze(dim=-2).contiguous()\n        key_cache, value_cache = kv_cache\n        attn_output = self.attn(\n            q, k, v, key_cache, value_cache, input_metadata, cache_event)\n        output, _ = self.out_proj(attn_output)\n        return output\n\nclass OPTDecoderLayer(nn.Module):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.self_attn = OPTAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.num_attention_heads,\n            bias=config.enable_bias,\n        )\n        self.do_layer_norm_before = config.do_layer_norm_before\n        assert config.activation_function == 'relu'\n        self.activation_fn = nn.ReLU()\n\n        self.self_attn_layer_norm = nn.LayerNorm(\n            self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n        self.fc1 = ColumnParallelLinear(self.embed_dim, config.ffn_dim,\n                                        bias=config.enable_bias,\n                                        gather_output=False,\n                                        perform_initialization=False)\n        self.fc2 = RowParallelLinear(config.ffn_dim, self.embed_dim,\n                                     bias=config.enable_bias,\n                                     input_is_parallel=True,\n                                     perform_initialization=False)\n        self.final_layer_norm = nn.LayerNorm(\n            self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        cache_event: Optional[torch.cuda.Event],\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states = self.self_attn(\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n            cache_event=cache_event)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Fully Connected\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states, _ = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states, _ = self.fc2(hidden_states)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        return hidden_states\n\n\nclass OPTDecoder(nn.Module):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(config.vocab_size,\n                                                   config.word_embed_proj_dim,\n                                                   perform_initialization=False)\n        # Positional embeddings are replicated (not sharded).\n        self.embed_positions = OPTLearnedPositionalEmbedding(\n            config.max_position_embeddings, config.hidden_size)\n\n        # Project out & in will be replicated if they exist.\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_out = nn.Linear(config.hidden_size, config.word_embed_proj_dim, bias=False)\n        else:\n            self.project_out = None\n\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_in = nn.Linear(config.word_embed_proj_dim, config.hidden_size, bias=False)\n        else:\n            self.project_in = None\n\n        # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n        # with checkpoints that have been fine-tuned before transformers v4.20.1\n        # see https://github.com/facebookresearch/metaseq/pull/164\n        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n            self.final_layer_norm = nn.LayerNorm(\n                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine\n            )\n        else:\n            self.final_layer_norm = None\n\n        self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> torch.Tensor:\n        inputs_embeds = self.embed_tokens(input_ids)\n        pos_embeds = self.embed_positions(positions)\n        if self.project_in is not None:\n            inputs_embeds = self.project_in(inputs_embeds)\n        hidden_states = inputs_embeds + pos_embeds\n\n        for i in range(len(self.layers)):\n            if cache_events is None:\n                cache_event = None\n            else:\n                cache_event = cache_events[i]\n            layer = self.layers[i]\n            hidden_states = layer(\n                hidden_states, kv_caches[i], input_metadata, cache_event)\n\n        if self.final_layer_norm is not None:\n            hidden_states = self.final_layer_norm(hidden_states)\n        if self.project_out is not None:\n            hidden_states = self.project_out(hidden_states)\n        return hidden_states\n\n\nclass OPTModel(nn.Module):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__()\n        self.decoder = OPTDecoder(config)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> torch.Tensor:\n        return self.decoder(\n            input_ids, positions, kv_caches, input_metadata, cache_events)\n\n\nclass OPTForCausalLM(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.model = OPTModel(config)\n        # TODO(zhuohan): create a new weight after implementing pipeline\n        #                parallelism\n        self.lm_head_weight = self.model.decoder.embed_tokens.weight\n        self.sampler = Sampler()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        cache_events: Optional[List[torch.cuda.Event]],\n    ) -> Dict[int, SequenceOutputs]:\n        hidden_states = self.model(\n            input_ids, positions, kv_caches, input_metadata, cache_events)\n        next_tokens = self.sampler(\n            self.lm_head_weight, hidden_states, input_metadata)\n        return next_tokens\n\n    _column_parallel_weights = [\"embed_tokens.weight\", \"fc1.weight\", \"fc1.bias\"]\n    _row_parallel_weights = [\"out_proj.weight\", \"fc2.weight\"]\n\n    def load_weights(self, weights_path: str):\n        tensor_model_parallel_rank = get_tensor_model_parallel_rank()\n        state_dict = self.state_dict()\n        for name, param in state_dict.items():\n            if \"lm_head_weight\" in name:\n                continue\n            if \"qkv_proj\" in name:\n                shard_size = param.shape[0] // 3\n                weights_to_concat = []\n                for weight_name in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n                    weight = np.load(os.path.join(\n                        weights_path, name.replace(\"qkv_proj\", weight_name)))\n                    weights_to_concat.append(weight[\n                        shard_size * tensor_model_parallel_rank\n                        :shard_size * (tensor_model_parallel_rank + 1)])\n                loaded_weight = torch.from_numpy(\n                    np.concatenate(weights_to_concat, axis=0))\n            else:\n                loaded_weight = torch.from_numpy(\n                    np.load(os.path.join(weights_path, name)))\n                for p in self._column_parallel_weights:\n                    if p in name:\n                        shard_size = param.shape[0]\n                        loaded_weight = loaded_weight[\n                            shard_size * tensor_model_parallel_rank\n                            :shard_size * (tensor_model_parallel_rank + 1)]\n                        break\n                for p in self._row_parallel_weights:\n                    if p in name:\n                        shard_size = param.shape[1]\n                        loaded_weight = loaded_weight[\n                            :,\n                            shard_size * tensor_model_parallel_rank\n                            :shard_size * (tensor_model_parallel_rank + 1)]\n                        break\n\n            assert param.shape == loaded_weight.shape\n            param.data.copy_(loaded_weight)\n\n    @staticmethod\n    def get_weights(model_name: str, path: str):\n        path = os.path.join(path, f\"{model_name}-np\")\n        path = os.path.abspath(os.path.expanduser(path))\n        os.makedirs(path, exist_ok=True)\n        lock_path = os.path.join(path, \"file_lock\")\n        lock = filelock.FileLock(lock_path)\n\n        with lock:\n            test_weight_path = os.path.join(\n                path, \"model.decoder.embed_positions.weight\")\n            if os.path.exists(test_weight_path):\n                return path\n\n            folder = snapshot_download(model_name, allow_patterns=\"*.bin\",\n                                       cache_dir=os.path.join(path, \"cache\"))\n            bin_files = glob.glob(os.path.join(folder, \"*.bin\"))\n\n            for bin_file in tqdm(bin_files, desc=\"Convert format\"):\n                state = torch.load(bin_file, map_location=\"cpu\")\n                for name, param in tqdm(state.items(), leave=False):\n                    if name.startswith(\"decoder.\"):\n                        name = \"model.\" + name\n                    param_path = os.path.join(path, name)\n                    with open(param_path, \"wb\") as f:\n                        np.save(f, param.cpu().detach().numpy())\n\n            return path\n",
      "diff": "diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py\nindex eed20ea41..9ecd9e70f 100644\n--- a/cacheflow/models/opt.py\n+++ b/cacheflow/models/opt.py\n@@ -69,17 +69,14 @@ class OPTAttention(nn.Module):\n         cache_event: Optional[torch.cuda.Event],\n     ) -> torch.Tensor:\n         qkv, _ = self.qkv_proj(hidden_states)\n-        qkv = qkv.reshape(qkv.shape[:-1] + (3, -1))\n-        q, k, v = torch.split(qkv, 1, dim=-2)\n-        q = q.squeeze(dim=-2).contiguous()\n-        k = k.squeeze(dim=-2).contiguous()\n-        v = v.squeeze(dim=-2).contiguous()\n+        q, k, v = qkv.chunk(chunks=3, dim=-1)\n         key_cache, value_cache = kv_cache\n         attn_output = self.attn(\n             q, k, v, key_cache, value_cache, input_metadata, cache_event)\n         output, _ = self.out_proj(attn_output)\n         return output\n \n+\n class OPTDecoderLayer(nn.Module):\n \n     def __init__(self, config: OPTConfig):",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 6
    },
    {
      "file_path": "cacheflow/worker/worker.py",
      "old_content": "from typing import Dict, List, Tuple\n\nimport torch\n\nfrom cacheflow.models import get_model\nfrom cacheflow.models import InputMetadata\nfrom cacheflow.sampling_params import SamplingParams\nfrom cacheflow.sequence import SequenceGroupInputs\nfrom cacheflow.sequence import SequenceOutputs\nfrom cacheflow.worker.cache_engine import CacheEngine\nfrom cacheflow.parallel_utils.parallel_state import (\n    initialize_model_parallel, get_tensor_model_parallel_world_size)\nfrom cacheflow.utils import set_random_seed\n\n\nclass Worker:\n\n    def __init__(\n        self,\n        model_name: str,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        dtype: str,\n        seed: int,\n        distributed_init_method: str,\n        rank: int,\n        world_size: int,\n        model_path: str,\n        tensor_parallel_size: int = 1,\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.init_distributed_environment(distributed_init_method,\n                                          rank,\n                                          world_size,\n                                          tensor_parallel_size,\n                                          pipeline_parallel_size)\n        self.worker_id = rank\n        self.block_size = block_size\n        set_random_seed(seed)\n\n        # Initialize the model.\n        self.model, self.dtype = get_model(model_name, dtype=dtype, path=model_path)\n        self.model = self.model.cuda()\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        self.num_layers = self.model.config.num_hidden_layers\n        assert self.model.config.num_attention_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = self.model.config.num_attention_heads // tensor_model_parallel_world_size\n        self.head_size = self.model.config.hidden_size // (self.num_heads * tensor_model_parallel_world_size)\n\n        # We reset the seed after initializing the model to ensure that\n        # the random state is not affected by the model initialization.\n        set_random_seed(seed)\n\n        self.cache_engine = CacheEngine(\n            worker_id=self.worker_id,\n            num_layers=self.num_layers,\n            num_heads=self.num_heads,\n            head_size=self.head_size,\n            block_size=block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            dtype=self.dtype,\n        )\n        self.cache_events = self.cache_engine.events\n        self.gpu_cache = self.cache_engine.gpu_cache\n\n\n    def init_distributed_environment(self,\n                                     distributed_init_method: str,\n                                     rank: int,\n                                     world_size: int,\n                                     tensor_parallel_size: int = 1,\n                                     pipeline_parallel_size: int = 1) -> None:\n        \"\"\"Initialize the distributed environment.\"\"\"\n        torch.distributed.init_process_group(\n            backend='nccl',\n            init_method=distributed_init_method,\n            world_size=world_size,\n            rank=rank,\n        )\n        # A small all_reduce for warmup.\n        torch.distributed.all_reduce(torch.zeros(1).cuda())\n        initialize_model_parallel(tensor_parallel_size,\n                                  pipeline_parallel_size)\n\n\n    def prepare_inputs(\n        self,\n        input_seq_groups: List[SequenceGroupInputs],\n    ) -> Tuple[torch.LongTensor, torch.LongTensor, InputMetadata]:\n        seq_groups: List[Tuple[List[int], SamplingParams]] = []\n        seq_logprobs: Dict[int, float] = {}\n        sampling_params: Dict[int, SamplingParams] = {}\n        input_tokens: List[int] = []\n        input_positions: List[int] = []\n        slot_mapping: List[int] = []\n\n        # Add prompt tokens.\n        prompt_lens: List[int] = []\n        for input_seq_group in input_seq_groups:\n            if not input_seq_group.is_prompt:\n                continue\n\n            seq_ids = list(input_seq_group.input_tokens.keys())\n            sampling_params = input_seq_group.sampling_params\n            seq_groups.append((seq_ids, sampling_params))\n            seq_logprobs.update(input_seq_group.seq_logprobs)\n\n            # Use any sequence in the group.\n            seq_id = seq_ids[0]\n\n            prompt_tokens = input_seq_group.input_tokens[seq_id]\n            prompt_len = len(prompt_tokens)\n            prompt_lens.append(prompt_len)\n\n            input_tokens.extend(prompt_tokens)\n            # NOTE(woosuk): Here we assume that the first token in the prompt\n            # is always the first token in the sequence.\n            input_positions.extend(range(len(prompt_tokens)))\n\n            # Compute the slot mapping.\n            block_table = input_seq_group.block_tables[seq_id]\n            for i in range(prompt_len):\n                block_number = block_table[i // self.block_size]\n                block_offset = i % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append(slot)\n\n        # Add generation tokens.\n        max_context_len = 0\n        max_num_blocks_per_seq = 0\n        context_lens: List[int] = []\n        generation_block_tables: List[List[int]] = []\n        for input_seq_group in input_seq_groups:\n            if input_seq_group.is_prompt:\n                continue\n\n            seq_ids = list(input_seq_group.input_tokens.keys())\n            sampling_params = input_seq_group.sampling_params\n            seq_groups.append((seq_ids, sampling_params))\n            seq_logprobs.update(input_seq_group.seq_logprobs)\n\n            for seq_id in seq_ids:\n                assert len(input_seq_group.input_tokens[seq_id]) == 1\n                generation_token = input_seq_group.input_tokens[seq_id][0]\n                input_tokens.append(generation_token)\n\n                position = input_seq_group.context_len - 1\n                input_positions.append(position)\n\n                block_table = input_seq_group.block_tables[seq_id]\n                generation_block_tables.append(block_table)\n\n                max_context_len = max(\n                    max_context_len, input_seq_group.context_len)\n                max_num_blocks_per_seq = max(\n                    max_num_blocks_per_seq, len(block_table))\n                context_lens.append(input_seq_group.context_len)\n\n                block_number = block_table[position // self.block_size]\n                block_offset = position % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append(slot)\n\n        # Optimization: Pad the input length to be a multiple of 8.\n        # This is required for utilizing the Tensor Cores in NVIDIA GPUs.\n        input_tokens = _pad_to_alignment(input_tokens, multiple_of=8)\n        input_positions = _pad_to_alignment(input_positions, multiple_of=8)\n\n        # Convert to tensors.\n        tokens_tensor = torch.tensor(\n            input_tokens, dtype=torch.long, device='cuda')\n        positions_tensor = torch.tensor(\n            input_positions, dtype=torch.long, device='cuda')\n        slot_mapping_tensor = torch.tensor(\n            slot_mapping, dtype=torch.int, device='cuda')\n        context_lens_tensor = torch.tensor(\n            context_lens, dtype=torch.int, device='cuda')\n        padded_block_tables = [\n            _pad_to_max(block_table, max_num_blocks_per_seq)\n            for block_table in generation_block_tables]\n        block_tables_tensor = torch.tensor(\n            padded_block_tables, dtype=torch.int, device='cuda')\n\n        input_metadata = InputMetadata(\n            seq_groups=seq_groups,\n            seq_logprobs=seq_logprobs,\n            prompt_lens=prompt_lens,\n            slot_mapping=slot_mapping_tensor,\n            context_lens=context_lens_tensor,\n            max_context_len=max_context_len,\n            block_tables=block_tables_tensor,\n        )\n        return tokens_tensor, positions_tensor, input_metadata\n\n    @torch.inference_mode()\n    def execute_stage(\n        self,\n        input_seq_groups: List[SequenceGroupInputs],\n        blocks_to_swap_in: Dict[int, int],\n        blocks_to_swap_out: Dict[int, int],\n        blocks_to_copy: Dict[int, List[int]],\n    ) -> Dict[int, SequenceOutputs]:\n        # Issue cache operations.\n        command_issued = False\n        if blocks_to_swap_in:\n            self.cache_engine.swap_in(blocks_to_swap_in)\n            command_issued = True\n        if blocks_to_swap_out:\n            self.cache_engine.swap_out(blocks_to_swap_out)\n            command_issued = True\n        if blocks_to_copy:\n            self.cache_engine.copy(blocks_to_copy)\n            command_issued = True\n\n        if command_issued:\n            cache_events = self.cache_events\n        else:\n            cache_events = None\n\n        # If there is no input, we don't need to execute the model.\n        if not input_seq_groups:\n            if cache_events is not None:\n                for event in cache_events:\n                    event.wait()\n            return {}\n\n        # Prepare input tensors.\n        input_tokens, input_positions, input_metadata = self.prepare_inputs(\n            input_seq_groups)\n\n        # Execute the model.\n        output = self.model(\n            input_ids=input_tokens,\n            positions=input_positions,\n            kv_caches=self.gpu_cache,\n            input_metadata=input_metadata,\n            cache_events=cache_events,\n        )\n        return output\n\n\ndef _pad_to_alignment(x: List[int], multiple_of: int) -> List[int]:\n    return x + [0] * ((-len(x)) % multiple_of)\n\n\ndef _pad_to_max(x: List[int], max_len: int) -> List[int]:\n    return x + [0] * (max_len - len(x))\n",
      "diff": "diff --git a/cacheflow/worker/worker.py b/cacheflow/worker/worker.py\nindex f309cf12f..db0d46aab 100644\n--- a/cacheflow/worker/worker.py\n+++ b/cacheflow/worker/worker.py\n@@ -128,6 +128,11 @@ class Worker:\n                 slot = block_number * self.block_size + block_offset\n                 slot_mapping.append(slot)\n \n+        cumulative_prompt_lens: List[int] = [0]\n+        for prompt_len in prompt_lens:\n+            cumulative_prompt_lens.append(\n+                cumulative_prompt_lens[-1] + prompt_len)\n+\n         # Add generation tokens.\n         max_context_len = 0\n         max_num_blocks_per_seq = 0\n@@ -183,11 +188,14 @@ class Worker:\n             for block_table in generation_block_tables]\n         block_tables_tensor = torch.tensor(\n             padded_block_tables, dtype=torch.int, device='cuda')\n+        cumulative_prompt_lens_tensor = torch.tensor(\n+            cumulative_prompt_lens, dtype=torch.int, device='cuda')\n \n         input_metadata = InputMetadata(\n             seq_groups=seq_groups,\n             seq_logprobs=seq_logprobs,\n             prompt_lens=prompt_lens,\n+            cumulative_prompt_lens=cumulative_prompt_lens_tensor,\n             slot_mapping=slot_mapping_tensor,\n             context_lens=context_lens_tensor,\n             max_context_len=max_context_len,",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 1
    },
    {
      "file_path": "csrc/activation.cpp",
      "old_content": "",
      "diff": "diff --git a/csrc/activation.cpp b/csrc/activation.cpp\nnew file mode 100644\nindex 000000000..f95afdc00\n--- /dev/null\n+++ b/csrc/activation.cpp\n@@ -0,0 +1,12 @@\n+#include <torch/extension.h>\n+\n+void silu_and_mul(\n+  torch::Tensor& out,\n+  torch::Tensor& input);\n+\n+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n+  m.def(\n+    \"silu_and_mul\",\n+    &silu_and_mul,\n+    \"Activation function used in SwiGLU.\");\n+}",
      "change_type": "added",
      "lines_added": 13,
      "lines_removed": 1
    },
    {
      "file_path": "csrc/activation_kernels.cu",
      "old_content": "",
      "diff": "diff --git a/csrc/activation_kernels.cu b/csrc/activation_kernels.cu\nnew file mode 100644\nindex 000000000..12ee6c548\n--- /dev/null\n+++ b/csrc/activation_kernels.cu\n@@ -0,0 +1,46 @@\n+#include <torch/extension.h>\n+#include <ATen/cuda/CUDAContext.h>\n+\n+namespace cacheflow {\n+\n+template<typename T>\n+__device__ __forceinline__ T silu(const T& x) {\n+  // x * sigmoid(x)\n+  return (T) (((float) x) / (1.0f + expf((float) -x)));\n+}\n+\n+template<typename scalar_t>\n+__global__ void silu_and_mul_kernel(\n+  scalar_t* __restrict__ out,               // [num_tokens, d]\n+  const scalar_t* __restrict__ input,       // [num_tokens, 2, d]\n+  const int d) {\n+  const int token_idx = blockIdx.x;\n+  for (int idx = threadIdx.x; idx < d; idx += blockDim.x) {\n+    const scalar_t x = __ldg(&input[token_idx * 2 * d + idx]);\n+    const scalar_t y = __ldg(&input[token_idx * 2 * d + d + idx]);\n+    out[token_idx * d + idx] = silu(x) * y;\n+  }\n+}\n+\n+} // namespace cacheflow\n+\n+void silu_and_mul(\n+  torch::Tensor& out,      // [num_tokens, d]\n+  torch::Tensor& input)    // [num_tokens, 2 * d]\n+{\n+  int num_tokens = input.size(0);\n+  int d = input.size(1) / 2;\n+\n+  dim3 grid(num_tokens);\n+  dim3 block(std::min(d, 1024));\n+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n+    input.scalar_type(),\n+    \"silu_and_mul_kernel\",\n+    [&] {\n+      cacheflow::silu_and_mul_kernel<scalar_t><<<grid, block, 0, stream>>>(\n+        out.data_ptr<scalar_t>(),\n+        input.data_ptr<scalar_t>(),\n+        d);\n+    });\n+}",
      "change_type": "added",
      "lines_added": 47,
      "lines_removed": 1
    },
    {
      "file_path": "csrc/attention_kernels.cu",
      "old_content": "#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n\n#include \"attention_utils.h\"\n#include \"cuda_primitives.h\"\n#include \"reduction_utils.h\"\n\n#include <algorithm>\n\n#define WARP_SIZE 32\n\nnamespace cacheflow {\n\n// Grid: (num_heads, num_seqs).\ntemplate<\n  typename scalar_t,\n  int HEAD_SIZE,\n  int BLOCK_SIZE,\n  int NUM_THREADS>\n__global__ void single_query_cached_kv_attention_kernel(\n  scalar_t* __restrict__ out,             // [num_seqs, num_heads, head_size]\n  const scalar_t* __restrict__ q,         // [num_seqs, num_heads, head_size]\n  const scalar_t* __restrict__ k_cache,   // [num_blocks, num_heads, head_size/x, block_size, x]\n  const scalar_t* __restrict__ v_cache,   // [num_blocks, num_heads, head_size, block_size]\n  const float scale,\n  const int* __restrict__ block_tables,   // [num_seqs, max_num_blocks_per_seq]\n  const int* __restrict__ context_lens,   // [num_seqs]\n  const int max_num_blocks_per_seq) {\n  constexpr int THREAD_GROUP_SIZE = WARP_SIZE / BLOCK_SIZE;\n  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n  const int thread_idx = threadIdx.x;\n  const int warp_idx = thread_idx / WARP_SIZE;\n  const int lane = thread_idx % WARP_SIZE;\n\n  const int head_idx = blockIdx.x;\n  const int num_heads = gridDim.x;\n  const int seq_idx = blockIdx.y;\n\n  // A vector type to store a part of a key or a query.\n  // The vector size is configured in such a way that the threads in a thread group\n  // fetch or comput 16 bytes at a time.\n  // For example, if the size of a thread group is 4 and the data type is half,\n  // then the vector size is 16 / (4 * sizeof(half)) == 2.\n  constexpr int VEC_SIZE = 16 / (THREAD_GROUP_SIZE * sizeof(scalar_t));\n  using K_vec = typename Vec<scalar_t, VEC_SIZE>::Type;\n  using Q_vec = typename Vec<scalar_t, VEC_SIZE>::Type;\n\n  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;\n  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;\n\n  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;\n  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;\n\n  // Load the query to registers.\n  // Each thread in a thread group has a different part of the query.\n  // For example, if the the thread group size is 4, then the first thread in the group\n  // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...\n  // th vectors of the query, and so on.\n  const scalar_t* q_ptr = q + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;\n  Q_vec q_vecs[NUM_VECS_PER_THREAD];\n#pragma unroll\n  for (int i = 0; i < NUM_VECS_PER_THREAD; i++) {\n    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;\n    q_vecs[i] = *reinterpret_cast<const Q_vec*>(q_ptr + vec_idx * VEC_SIZE);\n  }\n\n  // Memory planning.\n  extern __shared__ char shared_mem[];\n  // NOTE(woosuk): We use FP32 logits and accumulation.\n  float *logits = reinterpret_cast<float*>(shared_mem);\n  // Workspace for reduction.\n  __shared__ float red_smem[2 * NUM_WARPS];\n\n  // x == THREAD_GROUP_SIZE * VEC_SIZE\n  // Each thread group fetches x elements from the key at a time.\n  constexpr int x = 16 / sizeof(scalar_t);\n  float qk_max = -FLT_MAX;\n\n  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;\n  const int context_len = context_lens[seq_idx];\n  const int num_blocks = (context_len + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n  // Iterate over the key blocks.\n  // Each warp fetches a block of keys for each iteration.\n  // Each thread group in a warp fetches a key from the block, and computes\n  // dot product with the query.\n  for (int block_idx = warp_idx; block_idx < num_blocks; block_idx += NUM_WARPS) {\n    const int physical_block_number = block_table[block_idx];\n    const int physical_block_offset = thread_group_idx % BLOCK_SIZE;\n    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;\n\n    // Load a key to registers.\n    // Each thread in a thread group has a different part of the key.\n    // For example, if the the thread group size is 4, then the first thread in the group\n    // has 0, 4, 8, ... th vectors of the key, and the second thread has 1, 5, 9, ... th\n    // vectors of the key, and so on.\n    K_vec k_vecs[NUM_VECS_PER_THREAD];\n#pragma unroll\n    for (int i = 0; i < NUM_VECS_PER_THREAD; i++) {\n      const scalar_t* k_ptr = k_cache + physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE\n                                      + head_idx * HEAD_SIZE * BLOCK_SIZE\n                                      + physical_block_offset * x;\n      const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;\n      const int offset1 = (vec_idx * VEC_SIZE) / x;\n      const int offset2 = (vec_idx * VEC_SIZE) % x;\n      k_vecs[i] = *reinterpret_cast<const K_vec*>(k_ptr + offset1 * BLOCK_SIZE * x + offset2);\n    }\n\n    // Compute dot product.\n    // This includes a reduction across the threads in the same thread group.\n    const float qk = scale * Qk_dot<scalar_t, THREAD_GROUP_SIZE>::dot(q_vecs, k_vecs);\n    const bool mask = token_idx >= context_len;\n  \n    if (thread_group_offset == 0) {\n      // Store the partial reductions to shared memory.\n      // NOTE(woosuk): It is required to zero out the masked logits.\n      logits[token_idx] = mask ? 0.f : qk;\n      // Update the max value.\n      qk_max = mask ? qk_max : fmaxf(qk_max, qk);\n    }\n  }\n\n  // Perform reduction across the threads in the same warp to get the\n  // max qk value for each \"warp\" (not across the thread block yet).\n  // The 0-th thread of each thread group already has its max qk value.\n#pragma unroll\n  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {\n    qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask));\n  }\n  if (lane == 0) {\n    red_smem[warp_idx] = qk_max;\n  }\n  __syncthreads();\n\n  // TODO(woosuk): Refactor this part.\n  // Get the max qk value for the sequence.\n  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;\n#pragma unroll\n  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {\n      qk_max = fmaxf(qk_max, __shfl_xor_sync(uint32_t(-1), qk_max, mask));\n  }\n  // Broadcast the max qk value to all threads.\n  qk_max = __shfl_sync(uint32_t(-1), qk_max, 0);\n\n  // Get the sum of the exp values.\n  float exp_sum = 0.f;\n  for (int i = thread_idx; i < context_len; i += NUM_THREADS) {\n    float val = __expf(logits[i] - qk_max);\n    logits[i] = val;\n    exp_sum += val;\n  }\n  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum);\n\n  // Compute softmax.\n  const float inv_sum = __fdividef(1.f, exp_sum + 1e-6f);\n  for (int i = thread_idx; i < context_len; i += NUM_THREADS) {\n    logits[i] *= inv_sum;\n  }\n  __syncthreads();\n\n  // Each thread will fetch 16 bytes from the value cache at a time.\n  constexpr int V_VEC_SIZE = 16 / sizeof(scalar_t);\n  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;\n  using L_vec = typename FloatVec<V_vec>::Type;\n\n  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;\n  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;\n  constexpr int NUM_ROWS_PER_THREAD = (HEAD_SIZE + NUM_ROWS_PER_ITER - 1) / NUM_ROWS_PER_ITER;\n\n  float accs[NUM_ROWS_PER_THREAD];\n#pragma unroll\n  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n    accs[i] = 0.f;\n  }\n\n  for (int block_idx = warp_idx; block_idx < num_blocks; block_idx += NUM_WARPS) {\n    const int physical_block_number = block_table[block_idx];\n    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;\n    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;\n    L_vec logits_vec = *reinterpret_cast<L_vec*>(logits + token_idx);\n\n    const scalar_t* v_ptr = v_cache + physical_block_number * num_heads * HEAD_SIZE * BLOCK_SIZE\n                                    + head_idx * HEAD_SIZE * BLOCK_SIZE;\n#pragma unroll\n    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n      if (row_idx < HEAD_SIZE) {\n        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;\n        V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);\n        accs[i] += dot(logits_vec, cast_to_float(v_vec));\n      }\n    }\n  }\n\n  // Perform reduction within each warp.\n#pragma unroll\n  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n    float acc = accs[i];\n#pragma unroll\n    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {\n      acc += __shfl_xor_sync(uint32_t(-1), acc, mask);\n    }\n    accs[i] = acc;\n  }\n\n  // NOTE(woosuk): A barrier is required because the shared memory space for logits\n  // is reused for the output.\n  __syncthreads();\n\n  // Perform reduction across warps.\n  float* out_smem = reinterpret_cast<float*>(shared_mem);\n#pragma unroll\n  for (int i = NUM_WARPS; i > 1; i /= 2) {\n    int mid = i / 2;\n    // Upper warps write to shared memory.\n    if (warp_idx >= mid && warp_idx < i) {\n      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];\n#pragma unroll\n      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n          dst[row_idx] = accs[i];\n        }\n      }\n    }\n    __syncthreads();\n\n    // Lower warps update the output.\n    if (warp_idx < mid) {\n      const float* src = &out_smem[warp_idx * HEAD_SIZE];\n#pragma unroll\n      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n          accs[i] += src[row_idx];\n        }\n      }\n    }\n    __syncthreads();\n  }\n\n  // Write the final output.\n  if (warp_idx == 0) {\n    scalar_t* out_ptr = out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;\n#pragma unroll\n    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {\n      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;\n      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {\n        convert_from_float(*(out_ptr + row_idx), accs[i]);\n      }\n    }\n  }\n}\n\n} // namespace cacheflow\n\n#define LAUNCH_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS)                        \\\n  cacheflow::single_query_cached_kv_attention_kernel<T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS>   \\\n  <<<grid, block, shared_mem_size, stream>>>(                                                 \\\n    out_ptr,                                                                                  \\\n    query_ptr,                                                                                \\\n    key_cache_ptr,                                                                            \\\n    value_cache_ptr,                                                                          \\\n    scale,                                                                                    \\\n    block_tables_ptr,                                                                         \\\n    context_lens_ptr,                                                                         \\\n    max_num_blocks_per_seq);\n\n// TODO(woosuk): Tune NUM_THREADS.\ntemplate<\n  typename T,\n  int BLOCK_SIZE,\n  int NUM_THREADS = 128>\nvoid single_query_cached_kv_attention_launcher(\n  torch::Tensor& out,\n  torch::Tensor& query,\n  torch::Tensor& key_cache,\n  torch::Tensor& value_cache,\n  float scale,\n  torch::Tensor& block_tables,\n  torch::Tensor& context_lens,\n  int max_context_len) {\n  int num_seqs = query.size(0);\n  int num_heads = query.size(1);\n  int head_size = query.size(2);\n  int max_num_blocks_per_seq = block_tables.size(1);\n\n  T* out_ptr = reinterpret_cast<T*>(out.data_ptr());\n  T* query_ptr = reinterpret_cast<T*>(query.data_ptr());\n  T* key_cache_ptr = reinterpret_cast<T*>(key_cache.data_ptr());\n  T* value_cache_ptr = reinterpret_cast<T*>(value_cache.data_ptr());\n  int* block_tables_ptr = block_tables.data_ptr<int>();\n  int* context_lens_ptr = context_lens.data_ptr<int>();\n\n  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n  int padded_max_context_len = ((max_context_len + BLOCK_SIZE - 1) / BLOCK_SIZE) * BLOCK_SIZE;\n  int logits_size = padded_max_context_len * sizeof(float);\n  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);\n  int shared_mem_size = std::max(logits_size, outputs_size);\n\n  dim3 grid(num_heads, num_seqs);\n  dim3 block(NUM_THREADS);\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  switch (head_size) {\n    case 32:\n      LAUNCH_ATTENTION_KERNEL(T, 32, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 64:\n      LAUNCH_ATTENTION_KERNEL(T, 64, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 80:\n      LAUNCH_ATTENTION_KERNEL(T, 80, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 96:\n      LAUNCH_ATTENTION_KERNEL(T, 96, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 128:\n      LAUNCH_ATTENTION_KERNEL(T, 128, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 160:\n      LAUNCH_ATTENTION_KERNEL(T, 160, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 192:\n      LAUNCH_ATTENTION_KERNEL(T, 192, BLOCK_SIZE, NUM_THREADS);\n      break;\n    case 256:\n      LAUNCH_ATTENTION_KERNEL(T, 256, BLOCK_SIZE, NUM_THREADS);\n      break;\n    default:\n      assert(false);\n      break;\n  }\n}\n\nvoid single_query_cached_kv_attention(\n  torch::Tensor& out,\n  torch::Tensor& query,\n  torch::Tensor& key_cache,\n  torch::Tensor& value_cache,\n  float scale,\n  torch::Tensor& block_tables,\n  torch::Tensor& context_lens,\n  int block_size,\n  int max_context_len) {\n  // TODO(woosuk): Support BF16.\n  if (query.element_size() == 2) {\n    // Half.\n    if (block_size == 8) {\n      single_query_cached_kv_attention_launcher<uint16_t, 8>(\n        out,\n        query,\n        key_cache,\n        value_cache,\n        scale,\n        block_tables,\n        context_lens,\n        max_context_len);\n    } else if (block_size == 16) {\n      single_query_cached_kv_attention_launcher<uint16_t, 16>(\n        out,\n        query,\n        key_cache,\n        value_cache,\n        scale,\n        block_tables,\n        context_lens,\n        max_context_len);\n    } else {\n      assert(false);\n    }\n  } else if (query.element_size() == 4) {\n    // Float.\n    if (block_size == 8) {\n      single_query_cached_kv_attention_launcher<float, 8>(\n        out,\n        query,\n        key_cache,\n        value_cache,\n        scale,\n        block_tables,\n        context_lens,\n        max_context_len);\n    } else if (block_size == 16) {\n      single_query_cached_kv_attention_launcher<float, 16>(\n        out,\n        query,\n        key_cache,\n        value_cache,\n        scale,\n        block_tables,\n        context_lens,\n        max_context_len);\n    } else {\n      assert(false);\n    }\n  } else {\n    assert(false);\n  }\n}\n\n#undef WARP_SIZE\n",
      "diff": "diff --git a/csrc/attention_kernels.cu b/csrc/attention_kernels.cu\nindex 5b24120ea..60c0d0c6c 100644\n--- a/csrc/attention_kernels.cu\n+++ b/csrc/attention_kernels.cu\n@@ -25,7 +25,8 @@ __global__ void single_query_cached_kv_attention_kernel(\n   const float scale,\n   const int* __restrict__ block_tables,   // [num_seqs, max_num_blocks_per_seq]\n   const int* __restrict__ context_lens,   // [num_seqs]\n-  const int max_num_blocks_per_seq) {\n+  const int max_num_blocks_per_seq,\n+  const int q_stride) {\n   constexpr int THREAD_GROUP_SIZE = WARP_SIZE / BLOCK_SIZE;\n   constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;\n   const int thread_idx = threadIdx.x;\n@@ -56,7 +57,8 @@ __global__ void single_query_cached_kv_attention_kernel(\n   // For example, if the the thread group size is 4, then the first thread in the group\n   // has 0, 4, 8, ... th vectors of the query, and the second thread has 1, 5, 9, ...\n   // th vectors of the query, and so on.\n-  const scalar_t* q_ptr = q + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;\n+  // NOTE(woosuk): Because q is split from a qkv tensor, it may not be contiguous.\n+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;\n   Q_vec q_vecs[NUM_VECS_PER_THREAD];\n #pragma unroll\n   for (int i = 0; i < NUM_VECS_PER_THREAD; i++) {\n@@ -264,7 +266,8 @@ __global__ void single_query_cached_kv_attention_kernel(\n     scale,                                                                                    \\\n     block_tables_ptr,                                                                         \\\n     context_lens_ptr,                                                                         \\\n-    max_num_blocks_per_seq);\n+    max_num_blocks_per_seq,                                                                   \\\n+    query_stride);\n \n // TODO(woosuk): Tune NUM_THREADS.\n template<\n@@ -284,6 +287,7 @@ void single_query_cached_kv_attention_launcher(\n   int num_heads = query.size(1);\n   int head_size = query.size(2);\n   int max_num_blocks_per_seq = block_tables.size(1);\n+  int query_stride = query.stride(0);\n \n   T* out_ptr = reinterpret_cast<T*>(out.data_ptr());\n   T* query_ptr = reinterpret_cast<T*>(query.data_ptr());\n@@ -333,13 +337,13 @@ void single_query_cached_kv_attention_launcher(\n }\n \n void single_query_cached_kv_attention(\n-  torch::Tensor& out,\n-  torch::Tensor& query,\n-  torch::Tensor& key_cache,\n-  torch::Tensor& value_cache,\n+  torch::Tensor& out,             // [num_seqs, num_heads, head_size]\n+  torch::Tensor& query,           // [num_seqs, num_heads, head_size]\n+  torch::Tensor& key_cache,       // [num_blocks, num_heads, head_size/x, block_size, x]\n+  torch::Tensor& value_cache,     // [num_blocks, num_heads, head_size, block_size]\n   float scale,\n-  torch::Tensor& block_tables,\n-  torch::Tensor& context_lens,\n+  torch::Tensor& block_tables,    // [num_seqs, max_num_blocks_per_seq]\n+  torch::Tensor& context_lens,    // [num_seqs]\n   int block_size,\n   int max_context_len) {\n   // TODO(woosuk): Support BF16.",
      "change_type": "modified",
      "lines_added": 14,
      "lines_removed": 10
    },
    {
      "file_path": "csrc/cache_kernels.cu",
      "old_content": "#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n\n#include <algorithm>\n#include <cassert>\n#include <map>\n#include <vector>\n\nvoid swap_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, int64_t>& block_mapping) {\n  torch::Device src_device = src.device();\n  torch::Device dst_device = dst.device();\n  cudaMemcpyKind memcpy_type;\n  if (src_device.is_cuda() && dst_device.is_cuda()) {\n    assert(src_device.index() == dst_device.index());\n    memcpy_type = cudaMemcpyDeviceToDevice;\n  } else if (src_device.is_cuda() && dst_device.is_cpu()) {\n    memcpy_type = cudaMemcpyDeviceToHost;\n  } else if (src_device.is_cpu() && dst_device.is_cuda()) {\n    memcpy_type = cudaMemcpyHostToDevice;\n  } else {\n    assert(false);\n  }\n\n  void *src_ptr = src.data_ptr();\n  void *dst_ptr = dst.data_ptr();\n\n  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  for (const auto& pair : block_mapping) {\n    int64_t src_block_number = pair.first;\n    int64_t dst_block_number = pair.second;\n    int64_t src_offset = src_block_number * block_size_in_bytes;\n    int64_t dst_offset = dst_block_number * block_size_in_bytes;\n    cudaMemcpyAsync(\n      dst_ptr + dst_offset,\n      src_ptr + src_offset,\n      block_size_in_bytes,\n      memcpy_type,\n      stream);\n  }\n}\n\nvoid copy_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n  torch::Device src_device = src.device();\n  torch::Device dst_device = dst.device();\n  assert(src_device.is_cuda() && dst_device.is_cuda());\n  cudaMemcpyKind memcpy_type = cudaMemcpyDeviceToDevice;\n\n  void *src_ptr = src.data_ptr();\n  void *dst_ptr = dst.data_ptr();\n\n  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  for (const auto& pair : block_mapping) {\n    int64_t src_block_number = pair.first;\n    for (int64_t dst_block_number : pair.second) {\n      int64_t src_offset = src_block_number * block_size_in_bytes;\n      int64_t dst_offset = dst_block_number * block_size_in_bytes;\n      cudaMemcpyAsync(\n        dst_ptr + dst_offset,\n        src_ptr + src_offset,\n        block_size_in_bytes,\n        memcpy_type,\n        stream);\n    }\n  }\n}\n\nnamespace cacheflow {\n\ntemplate<typename scalar_t>\n__global__ void reshape_and_cache_kernel(\n  const scalar_t* __restrict__ key,     // [num_tokens, num_heads, head_size]\n  const scalar_t* __restrict__ value,   // [num_tokens, num_heads, head_size]\n  scalar_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x, block_size, x]\n  scalar_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size, block_size]\n  const int* __restrict__ slot_mapping, // [num_tokens]\n  const int num_heads,\n  const int head_size,\n  const int block_size,\n  const int x) {\n  const int token_idx = blockIdx.x;\n  const int slot_idx = slot_mapping[token_idx];\n  const int block_idx = slot_idx / block_size;\n  const int block_offset = slot_idx % block_size;\n\n  const int n = num_heads * head_size;\n  for (int i = threadIdx.x; i < n; i += blockDim.x) {\n    const int src_idx = token_idx * n + i;\n\n    const int head_idx = i / head_size;\n    const int head_offset = i % head_size;\n    const int x_idx = head_offset / x;\n    const int x_offset = head_offset % x;\n\n    const int tgt_key_idx = block_idx * num_heads * (head_size / x) * block_size * x\n                            + head_idx * (head_size / x) * block_size * x\n                            + x_idx * block_size * x\n                            + block_offset * x\n                            + x_offset;\n    const int tgt_value_idx = block_idx * num_heads * head_size * block_size\n                              + head_idx * head_size * block_size\n                              + head_offset * block_size\n                              + block_offset;\n    key_cache[tgt_key_idx] = __ldg(&key[src_idx]);\n    value_cache[tgt_value_idx] = __ldg(&value[src_idx]);\n  }\n}\n\n} // namespace cacheflow\n\nvoid reshape_and_cache(\n  torch::Tensor& key,\n  torch::Tensor& value,\n  torch::Tensor& key_cache,\n  torch::Tensor& value_cache,\n  torch::Tensor& slot_mapping) {\n  int num_tokens = key.size(0);\n  int num_heads = key.size(1);\n  int head_size = key.size(2);\n  int block_size = key_cache.size(3);\n  int x = key_cache.size(4);\n\n  dim3 grid(num_tokens);\n  dim3 block(std::min(num_heads * head_size, 512));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n    key.scalar_type(),\n    \"reshape_and_cache_kernel\",\n    [&] {\n      cacheflow::reshape_and_cache_kernel<scalar_t><<<grid, block, 0, stream>>>(\n        key.data_ptr<scalar_t>(),\n        value.data_ptr<scalar_t>(),\n        key_cache.data_ptr<scalar_t>(),\n        value_cache.data_ptr<scalar_t>(),\n        slot_mapping.data_ptr<int>(),\n        num_heads,\n        head_size,\n        block_size,\n        x);\n    });\n}\n",
      "diff": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex d7a0faa81..8b5537c47 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -81,6 +81,8 @@ __global__ void reshape_and_cache_kernel(\n   scalar_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x, block_size, x]\n   scalar_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size, block_size]\n   const int* __restrict__ slot_mapping, // [num_tokens]\n+  const int key_stride,\n+  const int value_stride,\n   const int num_heads,\n   const int head_size,\n   const int block_size,\n@@ -92,7 +94,8 @@ __global__ void reshape_and_cache_kernel(\n \n   const int n = num_heads * head_size;\n   for (int i = threadIdx.x; i < n; i += blockDim.x) {\n-    const int src_idx = token_idx * n + i;\n+    const int src_key_idx = token_idx * key_stride + i;\n+    const int src_value_idx = token_idx * value_stride + i;\n \n     const int head_idx = i / head_size;\n     const int head_offset = i % head_size;\n@@ -108,25 +111,29 @@ __global__ void reshape_and_cache_kernel(\n                               + head_idx * head_size * block_size\n                               + head_offset * block_size\n                               + block_offset;\n-    key_cache[tgt_key_idx] = __ldg(&key[src_idx]);\n-    value_cache[tgt_value_idx] = __ldg(&value[src_idx]);\n+    key_cache[tgt_key_idx] = __ldg(&key[src_key_idx]);\n+    value_cache[tgt_value_idx] = __ldg(&value[src_value_idx]);\n   }\n }\n \n } // namespace cacheflow\n \n void reshape_and_cache(\n-  torch::Tensor& key,\n-  torch::Tensor& value,\n-  torch::Tensor& key_cache,\n-  torch::Tensor& value_cache,\n-  torch::Tensor& slot_mapping) {\n+  torch::Tensor& key,           // [num_tokens, num_heads, head_size]\n+  torch::Tensor& value,         // [num_tokens, num_heads, head_size]\n+  torch::Tensor& key_cache,     // [num_blocks, num_heads, head_size/x, block_size, x]\n+  torch::Tensor& value_cache,   // [num_blocks, num_heads, head_size, block_size]\n+  torch::Tensor& slot_mapping)  // [num_tokens]\n+{\n   int num_tokens = key.size(0);\n   int num_heads = key.size(1);\n   int head_size = key.size(2);\n   int block_size = key_cache.size(3);\n   int x = key_cache.size(4);\n \n+  int key_stride = key.stride(0);\n+  int value_stride = value.stride(0);\n+\n   dim3 grid(num_tokens);\n   dim3 block(std::min(num_heads * head_size, 512));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n@@ -140,6 +147,8 @@ void reshape_and_cache(\n         key_cache.data_ptr<scalar_t>(),\n         value_cache.data_ptr<scalar_t>(),\n         slot_mapping.data_ptr<int>(),\n+        key_stride,\n+        value_stride,\n         num_heads,\n         head_size,\n         block_size,",
      "change_type": "modified",
      "lines_added": 18,
      "lines_removed": 9
    },
    {
      "file_path": "csrc/pos_encoding.cpp",
      "old_content": "#include <torch/extension.h>\n\nvoid rotary_embedding_neox(\n  torch::Tensor& out_query,\n  torch::Tensor& out_key,\n  torch::Tensor& positions,\n  torch::Tensor& query,\n  torch::Tensor& key,\n  torch::Tensor& cos_sin_cache);\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\n    \"rotary_embedding_neox\",\n    &rotary_embedding_neox,\n    \"Apply GPT-NeoX style rotary embedding to query and key\");\n}\n",
      "diff": "diff --git a/csrc/pos_encoding.cpp b/csrc/pos_encoding.cpp\nindex a10bec85a..5966751a4 100644\n--- a/csrc/pos_encoding.cpp\n+++ b/csrc/pos_encoding.cpp\n@@ -1,8 +1,6 @@\n #include <torch/extension.h>\n \n void rotary_embedding_neox(\n-  torch::Tensor& out_query,\n-  torch::Tensor& out_key,\n   torch::Tensor& positions,\n   torch::Tensor& query,\n   torch::Tensor& key,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 3
    },
    {
      "file_path": "csrc/pos_encoding_kernels.cu",
      "old_content": "#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n\nnamespace cacheflow {\n\ntemplate<typename scalar_t>\n__global__ void rotary_embedding_neox_kernel(\n  scalar_t* __restrict__ out_query,             // [num_tokens, num_heads, head_size]\n  scalar_t* __restrict__ out_key,               // [num_tokens, num_heads, head_size]\n  const int64_t* __restrict__ positions,        // [num_tokens]\n  const scalar_t* __restrict__ query,           // [num_tokens, num_heads, head_size]\n  const scalar_t* __restrict__ key,             // [num_tokens, num_heads, head_size]\n  const scalar_t* __restrict__ cos_sin_cache,   // [max_position, 2, head_size // 2]\n  const int num_heads,\n  const int head_size) {\n  // Each thread block is responsible for one token.\n  const int token_idx = blockIdx.x;\n  int64_t pos = positions[token_idx];\n  const scalar_t* cache_ptr = cos_sin_cache + pos * head_size;\n\n  const int embed_dim = head_size / 2;\n  const int n = num_heads * head_size;\n  for (int i = threadIdx.x; i < n; i += blockDim.x) {\n    const int idx = token_idx * n + i;\n\n    const int head_idx = i / head_size;\n    const int head_offset = i % head_size;\n    const int token_head = token_idx * n + head_idx * head_size;\n\n    const bool is_first_half = head_offset < embed_dim;\n    const int rot_offset = head_offset % embed_dim;\n    const int x_index = rot_offset;\n    const int y_index = embed_dim + rot_offset;\n\n    const scalar_t cos = __ldg(cache_ptr + x_index);\n    const scalar_t sin = __ldg(cache_ptr + y_index);\n\n    const scalar_t q_x = __ldg(query + token_head + x_index);\n    const scalar_t q_y = __ldg(query + token_head + y_index);\n    const scalar_t q_cos = is_first_half ? q_x : q_y;\n    const scalar_t q_sin = is_first_half ? -q_y : q_x;\n    out_query[idx] = q_cos * cos + q_sin * sin;\n\n    const scalar_t k_x = __ldg(key + token_head + x_index);\n    const scalar_t k_y = __ldg(key + token_head + y_index);\n    const scalar_t k_cos = is_first_half ? k_x : k_y;\n    const scalar_t k_sin = is_first_half ? -k_y : k_x;\n    out_key[idx] = k_cos * cos + k_sin * sin;\n  }\n}\n\n} // namespace cacheflow\n\nvoid rotary_embedding_neox(\n  torch::Tensor& out_query,         // [num_tokens, num_heads * head_size]\n  torch::Tensor& out_key,           // [num_tokens, num_heads * head_size]\n  torch::Tensor& positions,         // [num_tokens]\n  torch::Tensor& query,             // [num_tokens, num_heads * head_size]\n  torch::Tensor& key,               // [num_tokens, num_heads * head_size]\n  torch::Tensor& cos_sin_cache)     // [max_position, head_size]\n{\n  int num_tokens = query.size(0);\n  int head_size = cos_sin_cache.size(1);\n  int num_heads = query.size(1) / head_size;\n\n  dim3 grid(num_tokens);\n  dim3 block(std::min(num_heads * head_size, 512));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n    query.scalar_type(),\n    \"rotary_embedding_neox\",\n    [&] {\n      cacheflow::rotary_embedding_neox_kernel<scalar_t><<<grid, block, 0, stream>>>(\n        out_query.data_ptr<scalar_t>(),\n        out_key.data_ptr<scalar_t>(),\n        positions.data_ptr<int64_t>(),\n        query.data_ptr<scalar_t>(),\n        key.data_ptr<scalar_t>(),\n        cos_sin_cache.data_ptr<scalar_t>(),\n        num_heads,\n        head_size);\n    });\n}\n",
      "diff": "diff --git a/csrc/pos_encoding_kernels.cu b/csrc/pos_encoding_kernels.cu\nindex 50cf209fb..525f0fef4 100644\n--- a/csrc/pos_encoding_kernels.cu\n+++ b/csrc/pos_encoding_kernels.cu\n@@ -5,12 +5,11 @@ namespace cacheflow {\n \n template<typename scalar_t>\n __global__ void rotary_embedding_neox_kernel(\n-  scalar_t* __restrict__ out_query,             // [num_tokens, num_heads, head_size]\n-  scalar_t* __restrict__ out_key,               // [num_tokens, num_heads, head_size]\n   const int64_t* __restrict__ positions,        // [num_tokens]\n-  const scalar_t* __restrict__ query,           // [num_tokens, num_heads, head_size]\n-  const scalar_t* __restrict__ key,             // [num_tokens, num_heads, head_size]\n+  scalar_t* __restrict__ query,                 // [num_tokens, num_heads, head_size]\n+  scalar_t* __restrict__ key,                   // [num_tokens, num_heads, head_size]\n   const scalar_t* __restrict__ cos_sin_cache,   // [max_position, 2, head_size // 2]\n+  const int stride,\n   const int num_heads,\n   const int head_size) {\n   // Each thread block is responsible for one token.\n@@ -19,41 +18,36 @@ __global__ void rotary_embedding_neox_kernel(\n   const scalar_t* cache_ptr = cos_sin_cache + pos * head_size;\n \n   const int embed_dim = head_size / 2;\n-  const int n = num_heads * head_size;\n+  const int n = num_heads * embed_dim;\n   for (int i = threadIdx.x; i < n; i += blockDim.x) {\n-    const int idx = token_idx * n + i;\n+    const int head_idx = i / embed_dim;\n+    const int token_head = token_idx * stride + head_idx * head_size;\n \n-    const int head_idx = i / head_size;\n-    const int head_offset = i % head_size;\n-    const int token_head = token_idx * n + head_idx * head_size;\n-\n-    const bool is_first_half = head_offset < embed_dim;\n-    const int rot_offset = head_offset % embed_dim;\n+    const int rot_offset = i % embed_dim;\n     const int x_index = rot_offset;\n     const int y_index = embed_dim + rot_offset;\n \n+    const int out_x = token_idx * stride + head_idx * head_size + x_index;\n+    const int out_y = token_idx * stride + head_idx * head_size + y_index;\n+\n     const scalar_t cos = __ldg(cache_ptr + x_index);\n     const scalar_t sin = __ldg(cache_ptr + y_index);\n \n-    const scalar_t q_x = __ldg(query + token_head + x_index);\n-    const scalar_t q_y = __ldg(query + token_head + y_index);\n-    const scalar_t q_cos = is_first_half ? q_x : q_y;\n-    const scalar_t q_sin = is_first_half ? -q_y : q_x;\n-    out_query[idx] = q_cos * cos + q_sin * sin;\n+    const scalar_t q_x = query[token_head + x_index];\n+    const scalar_t q_y = query[token_head + y_index];\n+    query[out_x] = q_x * cos - q_y * sin;\n+    query[out_y] = q_y * cos + q_x * sin;\n \n-    const scalar_t k_x = __ldg(key + token_head + x_index);\n-    const scalar_t k_y = __ldg(key + token_head + y_index);\n-    const scalar_t k_cos = is_first_half ? k_x : k_y;\n-    const scalar_t k_sin = is_first_half ? -k_y : k_x;\n-    out_key[idx] = k_cos * cos + k_sin * sin;\n+    const scalar_t k_x = key[token_head + x_index];\n+    const scalar_t k_y = key[token_head + y_index];\n+    key[out_x] = k_x * cos - k_y * sin;\n+    key[out_y] = k_y * cos + k_x * sin;\n   }\n }\n \n } // namespace cacheflow\n \n void rotary_embedding_neox(\n-  torch::Tensor& out_query,         // [num_tokens, num_heads * head_size]\n-  torch::Tensor& out_key,           // [num_tokens, num_heads * head_size]\n   torch::Tensor& positions,         // [num_tokens]\n   torch::Tensor& query,             // [num_tokens, num_heads * head_size]\n   torch::Tensor& key,               // [num_tokens, num_heads * head_size]\n@@ -62,21 +56,22 @@ void rotary_embedding_neox(\n   int num_tokens = query.size(0);\n   int head_size = cos_sin_cache.size(1);\n   int num_heads = query.size(1) / head_size;\n+  int stride = query.stride(0);\n+  TORCH_CHECK(stride == key.stride(0));\n \n   dim3 grid(num_tokens);\n-  dim3 block(std::min(num_heads * head_size, 512));\n+  dim3 block(std::min(num_heads * head_size / 2, 512));\n   const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n   AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n     query.scalar_type(),\n     \"rotary_embedding_neox\",\n     [&] {\n       cacheflow::rotary_embedding_neox_kernel<scalar_t><<<grid, block, 0, stream>>>(\n-        out_query.data_ptr<scalar_t>(),\n-        out_key.data_ptr<scalar_t>(),\n         positions.data_ptr<int64_t>(),\n         query.data_ptr<scalar_t>(),\n         key.data_ptr<scalar_t>(),\n         cos_sin_cache.data_ptr<scalar_t>(),\n+        stride,\n         num_heads,\n         head_size);\n     });",
      "change_type": "modified",
      "lines_added": 23,
      "lines_removed": 28
    },
    {
      "file_path": "setup.py",
      "old_content": "import setuptools\nfrom torch.utils import cpp_extension\n\nCXX_FLAGS = ['-g']\nNVCC_FLAGS = ['-O2']\n\n\next_modules = []\n\n# Cache operations.\ncache_extension = cpp_extension.CUDAExtension(\n    name='cacheflow.cache_ops',\n    sources=['csrc/cache.cpp', 'csrc/cache_kernels.cu'],\n    extra_compile_args={'cxx': CXX_FLAGS, 'nvcc': NVCC_FLAGS},\n)\next_modules.append(cache_extension)\n\n# Attention kernels.\nattention_extension = cpp_extension.CUDAExtension(\n    name='cacheflow.attention_ops',\n    sources=['csrc/attention.cpp', 'csrc/attention_kernels.cu'],\n    extra_compile_args={'cxx': CXX_FLAGS, 'nvcc': NVCC_FLAGS},\n)\next_modules.append(attention_extension)\n\n# Positional encodings.\npositional_encoding_extension = cpp_extension.CUDAExtension(\n    name='cacheflow.pos_encoding_ops',\n    sources=['csrc/pos_encoding.cpp', 'csrc/pos_encoding_kernels.cu'],\n    extra_compile_args={'cxx': CXX_FLAGS, 'nvcc': NVCC_FLAGS},\n)\next_modules.append(positional_encoding_extension)\n\n# Layer normalization kernels.\nlayernorm_extension = cpp_extension.CUDAExtension(\n    name='cacheflow.layernorm_ops',\n    sources=['csrc/layernorm.cpp', 'csrc/layernorm_kernels.cu'],\n    extra_compile_args={'cxx': CXX_FLAGS, 'nvcc': NVCC_FLAGS},\n)\next_modules.append(layernorm_extension)\n\nsetuptools.setup(\n    name='cacheflow',\n    ext_modules=ext_modules,\n    cmdclass={'build_ext': cpp_extension.BuildExtension},\n)\n",
      "diff": "diff --git a/setup.py b/setup.py\nindex df7551989..e96c73033 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -39,6 +39,13 @@ layernorm_extension = cpp_extension.CUDAExtension(\n )\n ext_modules.append(layernorm_extension)\n \n+activation_extension = cpp_extension.CUDAExtension(\n+    name='cacheflow.activation_ops',\n+    sources=['csrc/activation.cpp', 'csrc/activation_kernels.cu'],\n+    extra_compile_args={'cxx': CXX_FLAGS, 'nvcc': NVCC_FLAGS},\n+)\n+ext_modules.append(activation_extension)\n+\n setuptools.setup(\n     name='cacheflow',\n     ext_modules=ext_modules,",
      "change_type": "modified",
      "lines_added": 8,
      "lines_removed": 1
    },
    {
      "file_path": "tests/kernels/activation.py",
      "old_content": "",
      "diff": "diff --git a/tests/kernels/activation.py b/tests/kernels/activation.py\nnew file mode 100644\nindex 000000000..3d9a9a644\n--- /dev/null\n+++ b/tests/kernels/activation.py\n@@ -0,0 +1,30 @@\n+import torch\n+import torch.nn.functional as F\n+\n+from cacheflow import activation_ops\n+\n+\n+def ref_silu_and_mul(x: torch.Tensor) -> torch.Tensor:\n+    x1, x2 = x.chunk(chunks=2, dim=1)\n+    return F.silu(x1) * x2\n+\n+\n+@torch.inference_mode()\n+def test_silu_and_mul(\n+    num_tokens: int,\n+    d: int,\n+    dtype: torch.dtype,\n+) -> None:\n+    x = torch.randn(num_tokens, 2 * d, dtype=dtype, device='cuda')\n+    out = torch.empty(num_tokens, d, dtype=dtype, device='cuda')\n+    activation_ops.silu_and_mul(out, x)\n+    ref_out = ref_silu_and_mul(x)\n+    assert torch.allclose(out, ref_out, atol=1e-5, rtol=1e-5)\n+\n+\n+if __name__ == '__main__':\n+    for dtype in [torch.half, torch.float]:\n+        for num_tokens in [7, 83, 2048]:\n+            for d in [512, 4096, 13824]:\n+                print(f'Testing dtype={dtype}, num_tokens={num_tokens}, d={d}')\n+                test_silu_and_mul(num_tokens, d, dtype)",
      "change_type": "added",
      "lines_added": 31,
      "lines_removed": 1
    },
    {
      "file_path": "tests/kernels/attention.py",
      "old_content": "import random\nfrom typing import List, Optional\n\nfrom flash_attn.flash_attention import FlashAttention\nimport torch\n\nfrom cacheflow import attention_ops\n\nMAX_SEQ_LEN = 4096\n\n\ndef ref_masked_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    scale: float,\n    attn_mask: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    query = query * scale\n    attn = torch.einsum('qhd,khd->hqk', query, key)\n    if attn_mask is not None:\n        attn = attn + attn_mask\n    attn = torch.softmax(attn, dim=-1)\n    out = torch.einsum('hqk,khd->qhd', attn, value)\n    return out\n\n\ndef ref_single_query_cached_kv_attention(\n    output: torch.Tensor,\n    query: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    block_tables: torch.Tensor,\n    context_lens: torch.Tensor,\n) -> None:\n    num_heads = value_cache.shape[1]\n    head_size = value_cache.shape[2]\n    block_size = value_cache.shape[3]\n\n    num_input_tokens = query.shape[0]\n    for i in range(num_input_tokens):\n        q = query[i].unsqueeze(0)\n        block_table = block_tables[i]\n        context_len = int(context_lens[i])\n\n        keys = []\n        values = []\n        for j in range(context_len):\n            block_number = int(block_table[j // block_size])\n            block_offset = j % block_size\n\n            k = key_cache[block_number, :, :, block_offset, :]\n            k = k.reshape(num_heads, head_size)\n            keys.append(k)\n\n            v = value_cache[block_number, :, :, block_offset]\n            values.append(v)\n        keys = torch.stack(keys, dim=0)\n        values = torch.stack(values, dim=0)\n\n        scale = 1.0 / (head_size ** 0.5)\n        out = ref_masked_attention(q, keys, values, scale)\n        out = out.view(num_heads, head_size)\n        output[i].copy_(out, non_blocking=True)\n\n\ndef ref_multi_query_kv_attention(\n    cu_seq_lens: List[int],\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    dtype: torch.dtype,\n) -> torch.Tensor:\n    head_size = query.shape[-1]\n    scale = 1.0 / (head_size ** 0.5)\n\n    num_seqs = len(cu_seq_lens) - 1\n    ref_outputs = []\n    for i in range(num_seqs):\n        start_idx = cu_seq_lens[i]\n        end_idx = cu_seq_lens[i + 1]\n        seq_len = end_idx - start_idx\n\n        # Create attention mask\n        attn_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * -1e5\n        attn_mask = attn_mask.to(dtype=dtype, device='cuda')\n\n        ref_output = ref_masked_attention(\n            query[start_idx:end_idx],\n            key[start_idx:end_idx],\n            value[start_idx:end_idx],\n            scale,\n            attn_mask=attn_mask,\n        )\n        ref_outputs.append(ref_output)\n    ref_output = torch.cat(ref_outputs, dim=0)\n    return ref_output\n\n\ndef test_single_query_cached_kv_attention(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    query = torch.randn(\n        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_block_shape = (num_heads, head_size // x, block_size, x)\n    key_cache = torch.randn(\n        size=(num_blocks, *key_block_shape), dtype=dtype, device='cuda')\n    value_block_shape = (num_heads, head_size, block_size)\n    value_cache = torch.randn(\n        size=(num_blocks, *value_block_shape), dtype=dtype, device='cuda')\n\n    context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_tokens)] \n    max_context_len = max(context_lens)\n    context_lens = torch.tensor(context_lens, dtype=torch.int, device='cuda')\n\n    max_num_blocks_per_seq = (max_context_len + block_size - 1) // block_size\n    block_tables = []\n    for _ in range(num_tokens):\n        block_table = [\n            random.randint(0, num_blocks - 1)\n            for _ in range(max_num_blocks_per_seq)\n        ]\n        block_tables.append(block_table)\n    block_tables = torch.tensor(block_tables, dtype=torch.int, device='cuda')\n\n    scale = float(1.0 / (head_size ** 0.5))\n    output = torch.empty_like(query)\n    attention_ops.single_query_cached_kv_attention(\n        output,\n        query,\n        key_cache,\n        value_cache,\n        scale,\n        block_tables,\n        context_lens,\n        block_size,\n        max_context_len,\n    )\n\n    ref_output = torch.empty_like(query)\n    ref_single_query_cached_kv_attention(\n        ref_output,\n        query,\n        key_cache,\n        value_cache,\n        block_tables,\n        context_lens,\n    )\n    # NOTE(woosuk): Due to the difference in the data types the two\n    # implementations use for attention softmax logits and accumulation,\n    # there is a small difference in the final outputs.\n    # We should use a relaxed tolerance for the test.\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n\n\ndef test_multi_query_kv_attention(\n    num_seqs: int,\n    num_heads: int,\n    head_size: int,\n    dtype: torch.dtype,\n) -> None:\n    seq_lens = random.sample(range(1, MAX_SEQ_LEN), num_seqs)\n    max_seq_len = max(seq_lens)\n    num_tokens = sum(seq_lens)\n\n    cu_seq_lens = [0]\n    for seq_len in seq_lens:\n        cu_seq_lens.append(cu_seq_lens[-1] + seq_len)\n    cu_seq_lens = torch.tensor(cu_seq_lens, dtype=torch.int, device='cuda')\n\n    scale = float(1.0 / (head_size ** 0.5))\n    query = torch.randn(\n        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n    key = torch.rand_like(query)\n    value = torch.rand_like(query)\n\n    qkv = torch.stack([query, key, value], dim=1)\n    flash_attn = FlashAttention(softmax_scale=scale)\n    output = flash_attn(\n        qkv,\n        cu_seqlens=cu_seq_lens,\n        max_s=max_seq_len,\n        causal=True,\n    )[0]\n\n    cu_seq_lens = cu_seq_lens.cpu().tolist()\n    ref_output = ref_multi_query_kv_attention(\n        cu_seq_lens,\n        query,\n        key,\n        value,\n        dtype,\n    )\n    assert torch.allclose(output, ref_output, atol=1e-3, rtol=1e-5)\n\n\n@torch.inference_mode()\ndef test_attention(seed: int) -> None:\n    # NOTE(woosuk): Even when the seed is fixed, there is a chance that\n    # the test fails due to the precision issue. Re-run the test if it fails.\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    for dtype in [torch.half, torch.float]:\n        for block_size in [8, 16]:\n            for head_size in [32, 64, 80, 96, 128, 160, 192, 256]:\n                print(f'Testing single_query_cached_kv_attention with '\n                      f'dtype={dtype}, block_size={block_size}, '\n                      f'head_size={head_size}')\n                test_single_query_cached_kv_attention(\n                    num_tokens=37,\n                    num_heads=3,\n                    head_size=head_size,\n                    block_size=block_size,\n                    num_blocks=1024,\n                    dtype=dtype,\n                )\n\n    # NOTE(woosuk): FlashAttention does not support FP32.\n    for dtype in [torch.half]:\n        # NOTE(woosuk): FlashAttention does not support head_size > 128.\n        for head_size in [64, 80, 96, 128]:\n            print(f'Testing multi_query_kv_attention with dtype={dtype}, '\n                  f'head_size={head_size}')\n            test_multi_query_kv_attention(\n                num_seqs=11,\n                num_heads=3,\n                head_size=head_size,\n                dtype=dtype,\n            )\n\n\nif __name__ == '__main__':\n    test_attention(seed=0)\n",
      "diff": "diff --git a/tests/kernels/attention.py b/tests/kernels/attention.py\nindex b6766e1ed..409da9efa 100644\n--- a/tests/kernels/attention.py\n+++ b/tests/kernels/attention.py\n@@ -1,7 +1,7 @@\n import random\n from typing import List, Optional\n \n-from flash_attn.flash_attention import FlashAttention\n+from flash_attn.flash_attn_interface import _flash_attn_forward\n import torch\n \n from cacheflow import attention_ops\n@@ -105,8 +105,9 @@ def test_single_query_cached_kv_attention(\n     num_blocks: int,\n     dtype: torch.dtype,\n ) -> None:\n-    query = torch.randn(\n-        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n+    qkv = torch.randn(\n+        num_tokens, 3, num_heads, head_size, dtype=dtype, device='cuda')\n+    query, _, _ = qkv.unbind(dim=1)\n     x = 16 // torch.tensor([], dtype=dtype).element_size()\n     key_block_shape = (num_heads, head_size // x, block_size, x)\n     key_cache = torch.randn(\n@@ -115,6 +116,11 @@ def test_single_query_cached_kv_attention(\n     value_cache = torch.randn(\n         size=(num_blocks, *value_block_shape), dtype=dtype, device='cuda')\n \n+    # Adjust the range of the values to reduce precision errors.\n+    query = query / (head_size ** 0.5)\n+    key_cache = key_cache / (head_size ** 0.5)\n+    value_cache = value_cache / (head_size ** 0.5)\n+\n     context_lens = [random.randint(1, MAX_SEQ_LEN) for _ in range(num_tokens)] \n     max_context_len = max(context_lens)\n     context_lens = torch.tensor(context_lens, dtype=torch.int, device='cuda')\n@@ -130,7 +136,8 @@ def test_single_query_cached_kv_attention(\n     block_tables = torch.tensor(block_tables, dtype=torch.int, device='cuda')\n \n     scale = float(1.0 / (head_size ** 0.5))\n-    output = torch.empty_like(query)\n+    output = torch.empty(\n+        num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n     attention_ops.single_query_cached_kv_attention(\n         output,\n         query,\n@@ -175,19 +182,28 @@ def test_multi_query_kv_attention(\n     cu_seq_lens = torch.tensor(cu_seq_lens, dtype=torch.int, device='cuda')\n \n     scale = float(1.0 / (head_size ** 0.5))\n-    query = torch.randn(\n+    qkv = torch.randn(\n+        num_tokens, 3, num_heads, head_size, dtype=dtype, device='cuda')\n+    # Adjust the range of the values to reduce precision errors.\n+    qkv = qkv / (head_size ** 0.5)\n+\n+    query, key, value = qkv.unbind(dim=1)\n+    output = torch.empty(\n         num_tokens, num_heads, head_size, dtype=dtype, device='cuda')\n-    key = torch.rand_like(query)\n-    value = torch.rand_like(query)\n-\n-    qkv = torch.stack([query, key, value], dim=1)\n-    flash_attn = FlashAttention(softmax_scale=scale)\n-    output = flash_attn(\n-        qkv,\n-        cu_seqlens=cu_seq_lens,\n-        max_s=max_seq_len,\n+    _flash_attn_forward(\n+        query,\n+        key,\n+        value,\n+        output,\n+        cu_seq_lens,\n+        cu_seq_lens,\n+        max_seq_len,\n+        max_seq_len,\n+        dropout_p=0.0,\n+        softmax_scale=scale,\n         causal=True,\n-    )[0]\n+        return_softmax=False,\n+    )\n \n     cu_seq_lens = cu_seq_lens.cpu().tolist()\n     ref_output = ref_multi_query_kv_attention(",
      "change_type": "modified",
      "lines_added": 32,
      "lines_removed": 16
    },
    {
      "file_path": "tests/kernels/cache.py",
      "old_content": "import random\n\nimport torch\n\nfrom cacheflow import cache_ops\n\n\ndef test_reshape_and_cache(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    num_slots = block_size * num_blocks\n    slot_mapping = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device='cuda')\n\n    kv_shape = (num_tokens, num_heads, head_size)\n    key = torch.randn(size=kv_shape, dtype=dtype, device='cuda')\n    value = torch.randn(size=kv_shape, dtype=dtype, device='cuda')\n    \n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device='cuda')\n    cloned_key_cache = key_cache.clone()\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_cache = torch.randn(\n        size=value_cache_shape, dtype=dtype, device='cuda')\n    cloned_value_cache = value_cache.clone()\n\n    cache_ops.reshape_and_cache(key, value, key_cache, value_cache, slot_mapping)\n\n    for i in range(num_tokens):\n        reshaped_key = key.reshape(num_tokens, num_heads, head_size // x, x)\n        block_idx = slot_mapping[i] // block_size\n        block_offset = slot_mapping[i] % block_size\n        cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\n        cloned_value_cache[block_idx, :, :, block_offset] = value[i]\n\n    assert torch.allclose(key_cache, cloned_key_cache)\n    assert torch.allclose(value_cache, cloned_value_cache)\n\n\n@torch.inference_mode()\ndef test_cache() -> None:\n    test_reshape_and_cache(\n        num_tokens=3, num_heads=2, head_size=16, block_size=8, num_blocks=2,\n        dtype=torch.half)\n\n\nif __name__ == '__main__':\n    test_cache()\n",
      "diff": "diff --git a/tests/kernels/cache.py b/tests/kernels/cache.py\nindex 9eebe4374..d6b1c3d2d 100644\n--- a/tests/kernels/cache.py\n+++ b/tests/kernels/cache.py\n@@ -17,10 +17,10 @@ def test_reshape_and_cache(\n     slot_mapping = random.sample(range(num_slots), num_tokens)\n     slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device='cuda')\n \n-    kv_shape = (num_tokens, num_heads, head_size)\n-    key = torch.randn(size=kv_shape, dtype=dtype, device='cuda')\n-    value = torch.randn(size=kv_shape, dtype=dtype, device='cuda')\n-    \n+    qkv = torch.randn(\n+        num_tokens, 3, num_heads, head_size, dtype=dtype, device='cuda')\n+    _, key, value = qkv.unbind(dim=1)\n+\n     x = 16 // torch.tensor([], dtype=dtype).element_size()\n     key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n     key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device='cuda')\n@@ -35,7 +35,7 @@ def test_reshape_and_cache(\n \n     for i in range(num_tokens):\n         reshaped_key = key.reshape(num_tokens, num_heads, head_size // x, x)\n-        block_idx = slot_mapping[i] // block_size\n+        block_idx = torch.div(slot_mapping[i], block_size, rounding_mode='floor')\n         block_offset = slot_mapping[i] % block_size\n         cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\n         cloned_value_cache[block_idx, :, :, block_offset] = value[i]",
      "change_type": "modified",
      "lines_added": 6,
      "lines_removed": 6
    },
    {
      "file_path": "tests/kernels/pos_encoding.py",
      "old_content": "from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom cacheflow import pos_encoding_ops\n\n\ndef rotate_half(x: torch.Tensor) -> torch.Tensor:\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    cos: torch.Tensor,\n    sin: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass RefRotaryEmbeddingNeox(nn.Module):\n    \"\"\"Reference implementation of the GPT-NeoX style rotary embedding.\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        max_position_embeddings: int = 2048,\n        base: int = 10000,\n    ) -> None:\n        super().__init__()\n        self.max_position_embeddings = max_position_embeddings\n\n        # Create cos and sin embeddings.\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2) / dim))\n        t = torch.arange(max_position_embeddings).float()\n        freqs = torch.einsum(\"i,j->ij\", t, inv_freq.float())\n        emb = torch.cat((freqs, freqs), dim=-1)\n        cos = emb.cos().to(dtype=inv_freq.dtype)\n        sin = emb.sin().to(dtype=inv_freq.dtype)\n        self.register_buffer(\"cos_cached\", cos, persistent=False)\n        self.register_buffer(\"sin_cached\", sin, persistent=False)\n\n    def forward(\n        self,\n        positions: torch.LongTensor,    # [num_tokens]\n        query: torch.Tensor,            # [num_tokens, num_heads, head_size]\n        key: torch.Tensor,              # [num_tokens, num_heads, head_size]\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        cos = F.embedding(positions, self.cos_cached)\n        sin = F.embedding(positions, self.sin_cached)\n        query = query.transpose(0, 1)\n        key = key.transpose(0, 1)\n        query, key = apply_rotary_pos_emb(query, key, cos, sin)\n        query = query.transpose(0, 1).contiguous()\n        key = key.transpose(0, 1).contiguous()\n        # Output query/key shape: [num_tokens, num_tokens, head_size]\n        return query, key\n\n\n@torch.inference_mode()\ndef test_rotary_embedding_neox(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    max_position: int,\n    dtype: torch.dtype,\n    base: int = 10000,\n) -> None:\n    positions = torch.randint(0, max_position, (num_tokens,), device='cuda')\n    query = torch.randn(num_tokens, num_heads * head_size, dtype=dtype, device='cuda')\n    key = torch.randn(num_tokens, num_heads * head_size, dtype=dtype, device='cuda')\n\n    # Create the rotary embedding.\n    inv_freq = 1.0 / (base ** (torch.arange(0, head_size, 2) / head_size))\n    t = torch.arange(max_position).float()\n    freqs = torch.einsum('i,j -> ij', t, inv_freq.float())\n    cos = freqs.cos()\n    sin = freqs.sin()\n    cos_sin_cache = torch.cat((cos, sin), dim=-1)\n    cos_sin_cache = cos_sin_cache.to(dtype=dtype, device='cuda')\n\n    # Run the kernel.\n    out_query = torch.empty_like(query)\n    out_key = torch.empty_like(key)\n    pos_encoding_ops.rotary_embedding_neox(\n        out_query,\n        out_key,\n        positions,\n        query,\n        key,\n        cos_sin_cache,\n    )\n\n    # Run the reference implementation.\n    ref_rotary_embedding = RefRotaryEmbeddingNeox(\n        dim=head_size,\n        max_position_embeddings=max_position,\n        base=base,\n    ).to(dtype=dtype, device='cuda')\n    ref_query, ref_key = ref_rotary_embedding(\n        positions,\n        query.view(num_tokens, num_heads, head_size),\n        key.view(num_tokens, num_heads, head_size),\n    )\n    ref_query = ref_query.view(num_tokens, num_heads * head_size)\n    ref_key = ref_key.view(num_tokens, num_heads * head_size)\n\n    # Compare the results.\n    assert torch.allclose(out_query, ref_query, atol=1e-3, rtol=1e-5)\n    assert torch.allclose(out_key, ref_key, atol=1e-3, rtol=1e-5)\n\n\nif __name__ == '__main__':\n    for dtype in [torch.half, torch.float]:\n        for head_size in [32, 64, 80, 96, 128, 160, 192, 256]:\n            print(f'Running tests for head_size={head_size} and dtype={dtype}')\n            test_rotary_embedding_neox(\n                num_tokens=2145,\n                num_heads=5,\n                head_size=head_size,\n                max_position=8192,\n                dtype=dtype,\n            )\n",
      "diff": "diff --git a/tests/kernels/pos_encoding.py b/tests/kernels/pos_encoding.py\nindex 2dbce545e..502eedfbd 100644\n--- a/tests/kernels/pos_encoding.py\n+++ b/tests/kernels/pos_encoding.py\n@@ -85,15 +85,13 @@ def test_rotary_embedding_neox(\n     cos_sin_cache = torch.cat((cos, sin), dim=-1)\n     cos_sin_cache = cos_sin_cache.to(dtype=dtype, device='cuda')\n \n-    # Run the kernel.\n-    out_query = torch.empty_like(query)\n-    out_key = torch.empty_like(key)\n+    # Run the kernel. The kernel is in-place, so we need to clone the inputs.\n+    out_query = query.clone()\n+    out_key = key.clone()\n     pos_encoding_ops.rotary_embedding_neox(\n+        positions,\n         out_query,\n         out_key,\n-        positions,\n-        query,\n-        key,\n         cos_sin_cache,\n     )",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 7
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 17,
    "files_added": 4,
    "files_deleted": 0,
    "files_modified": 13
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (nah bruda too much BS)",
    "is_benchmark_actually_there": "",
    "sample_clues": "activation, activation_kernels, attention"
  }
}