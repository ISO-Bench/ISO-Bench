{
  "commit_hash": "379da6dcb5f5d062d0452b2fc23291e5113dcf04",
  "parent_hash": "ebce310b7433e050086f52ca48571807df467f50",
  "message": "[Kernel] [FP8] Improve FP8 linear layer performance (#4691)\n\nThis PR improves the FP8 performance of linear layers, which had been lacking before (#4118 (comment) and #4118 (comment)).\n\nWe noticed that CUBLASLt can find a better algorithm if the first dimension of the matrix is greater than 16. So this PR enlarges matrices appropriately during quantization. This improves FP8 performance and removes the performance regression vs. FP16, in many cases exceeding FP16 performance.\n\nHere are benchmarks on llama3 70b (ITL numbers for 1000 input and 50 output tokens at fixed qps and at TP 4), all FP8 measurements are for dynamic quantization:\n\nqps = 1: 24 ms (FP8, this PR), 32 ms (FP8, previous main), 26 ms (FP16)\nqps = 2: 26 ms (FP8, this PR), 34ms (FP8, previous main), 28 ms (FP16) \nqps = 4: 33 ms (FP8, this PR), 44 ms (FP8, previous main), 36 ms (FP16)\nqps = 6: 46 ms (FP8, this PR), 56 ms (FP8, previous main), 54 ms (FP16)\nqps = 8: 85 ms (FP8, this PR), 85 ms (FP8, previous main), 138 ms (FP16)",
  "author": "Philipp Moritz <pcmoritz@gmail.com>",
  "date": "2024-05-09 16:38:07 -0700",
  "files_changed": [
    {
      "file_path": "vllm/_custom_ops.py",
      "old_content": "from typing import Dict, Optional, Tuple\n\nimport torch\n\ntry:\n    from vllm._C import cache_ops as vllm_cache_ops\n    from vllm._C import ops as vllm_ops\nexcept ImportError:\n    pass\n\n\n# activation ops\ndef silu_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:\n    vllm_ops.silu_and_mul(out, x)\n\n\ndef gelu_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:\n    vllm_ops.gelu_and_mul(out, x)\n\n\ndef gelu_tanh_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:\n    vllm_ops.gelu_tanh_and_mul(out, x)\n\n\ndef gelu_fast(out: torch.Tensor, x: torch.Tensor) -> None:\n    vllm_ops.gelu_fast(out, x)\n\n\ndef gelu_new(out: torch.Tensor, x: torch.Tensor) -> None:\n    vllm_ops.gelu_new(out, x)\n\n\n# page attention ops\ndef paged_attention_v1(\n    out: torch.Tensor,\n    query: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    num_kv_heads: int,\n    scale: float,\n    block_tables: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_size: int,\n    max_seq_len: int,\n    alibi_slopes: Optional[torch.Tensor],\n    kv_cache_dtype: str,\n    kv_scale: float,\n) -> None:\n    vllm_ops.paged_attention_v1(out, query, key_cache, value_cache,\n                                num_kv_heads, scale, block_tables, seq_lens,\n                                block_size, max_seq_len, alibi_slopes,\n                                kv_cache_dtype, kv_scale)\n\n\ndef paged_attention_v2(\n    out: torch.Tensor,\n    exp_sum: torch.Tensor,\n    max_logits: torch.Tensor,\n    tmp_out: torch.Tensor,\n    query: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    num_kv_heads: int,\n    scale: float,\n    block_tables: torch.Tensor,\n    seq_lens: torch.Tensor,\n    block_size: int,\n    max_seq_len: int,\n    alibi_slopes: Optional[torch.Tensor],\n    kv_cache_dtype: str,\n    kv_scale: float,\n) -> None:\n    vllm_ops.paged_attention_v2(out, exp_sum, max_logits, tmp_out, query,\n                                key_cache, value_cache, num_kv_heads, scale,\n                                block_tables, seq_lens, block_size,\n                                max_seq_len, alibi_slopes, kv_cache_dtype,\n                                kv_scale)\n\n\n# pos encoding ops\ndef rotary_embedding(\n    positions: torch.Tensor,\n    query: torch.Tensor,\n    key: torch.Tensor,\n    head_size: int,\n    cos_sin_cache: torch.Tensor,\n    is_neox: bool,\n) -> None:\n    vllm_ops.rotary_embedding(positions, query, key, head_size, cos_sin_cache,\n                              is_neox)\n\n\ndef batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,\n                             key: torch.Tensor, head_size: int,\n                             cos_sin_cache: torch.Tensor, is_neox: bool,\n                             rot_dim: int,\n                             cos_sin_cache_offsets: torch.Tensor) -> None:\n    vllm_ops.batched_rotary_embedding(positions, query, key, head_size,\n                                      cos_sin_cache, is_neox, rot_dim,\n                                      cos_sin_cache_offsets)\n\n\n# layer norm ops\ndef rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,\n             epsilon: float) -> None:\n    vllm_ops.rms_norm(out, input, weight, epsilon)\n\n\ndef fused_add_rms_norm(input: torch.Tensor, residual: torch.Tensor,\n                       weight: torch.Tensor, epsilon: float) -> None:\n    vllm_ops.fused_add_rms_norm(input, residual, weight, epsilon)\n\n\n# quantization ops\n# awq\ndef awq_dequantize(qweight: torch.Tensor, scales: torch.Tensor,\n                   zeros: torch.Tensor, split_k_iters: int, thx: int,\n                   thy: int) -> torch.Tensor:\n    return vllm_ops.awq_dequantize(qweight, scales, zeros, split_k_iters, thx,\n                                   thy)\n\n\ndef awq_gemm(input: torch.Tensor, qweight: torch.Tensor, qzeros: torch.Tensor,\n             scales: torch.Tensor, split_k_iters: int) -> torch.Tensor:\n    return vllm_ops.awq_gemm(input, qweight, qzeros, scales, split_k_iters)\n\n\n# gptq\ndef gptq_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n              b_gptq_qzeros: torch.Tensor, b_gptq_scales: torch.Tensor,\n              b_g_idx: torch.Tensor, use_exllama: bool,\n              bit: int) -> torch.Tensor:\n    return vllm_ops.gptq_gemm(a, b_q_weight, b_gptq_qzeros, b_gptq_scales,\n                              b_g_idx, use_exllama, bit)\n\n\ndef gptq_shuffle(q_weight: torch.Tensor, q_perm: torch.Tensor,\n                 bit: int) -> None:\n    vllm_ops.gptq_shuffle(q_weight, q_perm, bit)\n\n\n# squeezellm\ndef squeezellm_gemm(vec: torch.Tensor, mat: torch.Tensor, mul: torch.Tensor,\n                    lookup_table: torch.Tensor) -> None:\n    vllm_ops.squeezellm_gemm(vec, mat, mul, lookup_table)\n\n\n# marlin\ndef marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n                b_scales: torch.Tensor, workspace: torch.Tensor, size_m: int,\n                size_n: int, size_k: int) -> torch.Tensor:\n    return vllm_ops.marlin_gemm(a, b_q_weight, b_scales, workspace, size_m,\n                                size_n, size_k)\n\n\n# aqlm\ndef aqlm_gemm(input: torch.Tensor, codes: torch.Tensor,\n              codebooks: torch.Tensor, scales: torch.Tensor,\n              codebook_partition_sizes: torch.Tensor,\n              bias: Optional[torch.Tensor]) -> torch.Tensor:\n    return vllm_ops.aqlm_gemm(input, codes, codebooks, scales,\n                              codebook_partition_sizes, bias)\n\n\ndef aqlm_dequant(codes: torch.Tensor, codebooks: torch.Tensor,\n                 codebook_partition_sizes: torch.Tensor) -> torch.Tensor:\n    return vllm_ops.aqlm_dequant(codes, codebooks, codebook_partition_sizes)\n\n\n# gptq_marlin\ndef gptq_marlin_repack(b_q_weight: torch.Tensor, perm: torch.Tensor,\n                       size_k: int, size_n: int,\n                       num_bits: int) -> torch.Tensor:\n    return vllm_ops.gptq_marlin_repack(b_q_weight, perm, size_k, size_n,\n                                       num_bits)\n\n\ndef gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n                     b_scales: torch.Tensor, g_idx: torch.Tensor,\n                     perm: torch.Tensor, workspace: torch.Tensor,\n                     num_bits: int, size_m: int, size_n: int, size_k: int,\n                     is_k_full: bool) -> torch.Tensor:\n    return vllm_ops.gptq_marlin_gemm(a, b_q_weight, b_scales, g_idx, perm,\n                                     workspace, num_bits, size_m, size_n,\n                                     size_k, is_k_full)\n\n\n# fp8\ndef scaled_fp8_quant(\n    input: torch.Tensor,\n    scale: Optional[torch.Tensor] = None,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n    if scale is None:\n        scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n        vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)\n    else:\n        vllm_ops.static_scaled_fp8_quant(output, input, scale)\n    return output, scale\n\n\n# moe\ndef moe_align_block_size(topk_ids: torch.Tensor, num_experts: int,\n                         block_size: int, sorted_token_ids: torch.Tensor,\n                         experts_ids: torch.Tensor,\n                         num_tokens_post_pad: torch.Tensor) -> None:\n    vllm_ops.moe_align_block_size(topk_ids, num_experts, block_size,\n                                  sorted_token_ids, experts_ids,\n                                  num_tokens_post_pad)\n\n\ndef reshape_and_cache(\n    key: torch.Tensor,\n    value: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    slot_mapping: torch.Tensor,\n    kv_cache_dtype: str,\n    kv_scale: float,\n) -> None:\n    vllm_cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\n                                     slot_mapping, kv_cache_dtype, kv_scale)\n\n\ndef reshape_and_cache_flash(\n    key: torch.Tensor,\n    value: torch.Tensor,\n    key_cache: torch.Tensor,\n    value_cache: torch.Tensor,\n    slot_mapping: torch.Tensor,\n    kv_cache_dtype: str,\n) -> None:\n    vllm_cache_ops.reshape_and_cache_flash(key, value, key_cache, value_cache,\n                                           slot_mapping, kv_cache_dtype)\n\n\ndef copy_blocks(key_caches: torch.Tensor, value_caches: torch.Tensor,\n                block_mapping: torch.Tensor) -> None:\n    vllm_cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n\n\ndef swap_blocks(src: torch.Tensor, dst: torch.Tensor,\n                block_mapping: Dict[int, int]) -> None:\n    vllm_cache_ops.swap_blocks(src, dst, block_mapping)\n\n\ndef convert_fp8(output: torch.Tensor, input: torch.Tensor) -> None:\n    vllm_cache_ops.convert_fp8(output, input)\n\n\n#TODO: cuda_utils, custom_ar\n",
      "diff": "diff --git a/vllm/_custom_ops.py b/vllm/_custom_ops.py\nindex 5b5643748..829c47003 100644\n--- a/vllm/_custom_ops.py\n+++ b/vllm/_custom_ops.py\n@@ -189,8 +189,34 @@ def gptq_marlin_gemm(a: torch.Tensor, b_q_weight: torch.Tensor,\n def scaled_fp8_quant(\n     input: torch.Tensor,\n     scale: Optional[torch.Tensor] = None,\n+    batch_dim_padding: Optional[int] = None,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n+    \"\"\"\n+    Quantize input tensor to FP8 and return quantized tensor and scale.\n+\n+    This function supports both static and dynamic quantization: If you\n+    provide the scale, it will use static scaling and if you omit it,\n+    the scale will be determined dynamically. The function also allows\n+    optional padding of the output tensor for downstream kernels that\n+    will benefit from padding.\n+\n+    Args:\n+        input: The input tensor to be quantized to FP8\n+        scale: Optional scaling factor for the FP8 quantization\n+        batch_dim_padding: If specified, pad the first dimension\n+            of the output to at least this value.\n+\n+    Returns:\n+        Tuple[torch.Tensor, torch.Tensor]: The output tensor in FP8 and\n+            scaling factor.\n+    \"\"\"\n+    if batch_dim_padding:\n+        shape = (max(batch_dim_padding, input.shape[0]), *input.shape[1:])\n+        output = torch.empty(shape,\n+                             device=input.device,\n+                             dtype=torch.float8_e4m3fn)\n+    else:\n+        output = torch.empty_like(input, dtype=torch.float8_e4m3fn)\n     if scale is None:\n         scale = torch.zeros(1, device=input.device, dtype=torch.float32)\n         vllm_ops.dynamic_scaled_fp8_quant(output, input, scale)",
      "change_type": "modified",
      "lines_added": 28,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/layers/quantization/fp8.py",
      "old_content": "from typing import Any, Dict, List, Optional, Tuple, Union\n\nimport torch\nfrom torch.nn import Module\nfrom torch.nn.parameter import Parameter\n\nfrom vllm import _custom_ops as ops\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.linear import LinearBase, LinearMethodBase\nfrom vllm.model_executor.layers.quantization.base_config import (\n    QuantizationConfig)\nfrom vllm.model_executor.utils import set_weight_attrs\n\nACTIVATION_SCHEMES = [\"static\", \"dynamic\"]\n\nlogger = init_logger(__name__)\n\n\nclass Fp8Config(QuantizationConfig):\n    \"\"\"Config class for FP8.\"\"\"\n\n    def __init__(\n        self,\n        is_checkpoint_fp8_serialized: bool = False,\n        activation_scheme: str = \"dynamic\",\n    ) -> None:\n        self.is_checkpoint_fp8_serialized = is_checkpoint_fp8_serialized\n        if is_checkpoint_fp8_serialized:\n            logger.warning(\"Detected fp8 checkpoint. Please note that the \"\n                           \"format is experimental and subject to change.\")\n        if activation_scheme not in ACTIVATION_SCHEMES:\n            raise ValueError(\n                f\"Unsupported activation scheme {activation_scheme}\")\n        self.activation_scheme = activation_scheme\n\n    @classmethod\n    def get_name(cls) -> str:\n        return \"fp8\"\n\n    @classmethod\n    def get_supported_act_dtypes(cls) -> List[torch.dtype]:\n        return [torch.bfloat16, torch.half]\n\n    @classmethod\n    def get_min_capability(cls) -> int:\n        return 89\n\n    @classmethod\n    def get_config_filenames(cls) -> List[str]:\n        return []\n\n    @classmethod\n    def from_config(cls, config: Dict[str, Any]) -> \"Fp8Config\":\n        quant_method = cls.get_from_keys(config, [\"quant_method\"])\n        is_checkpoint_fp8_serialized = (\"fp8\" in quant_method)\n        activation_scheme = cls.get_from_keys(config, [\"activation_scheme\"])\n        return cls(is_checkpoint_fp8_serialized=is_checkpoint_fp8_serialized,\n                   activation_scheme=activation_scheme)\n\n    def get_quant_method(\n            self, layer: torch.nn.Module) -> Optional[\"Fp8LinearMethod\"]:\n        if isinstance(layer, LinearBase):\n            return Fp8LinearMethod(self)\n        return None\n\n    def get_scaled_act_names(self) -> List[str]:\n        return []\n\n\nclass Fp8LinearMethod(LinearMethodBase):\n    \"\"\"Linear method for FP8.\n    Supports loading FP8 checkpoints with static weight scale and\n    dynamic/static activation scale.\n\n    Also supports loading quantized FP16/BF16 model checkpoints with dynamic\n    activation scaling. The weight scaling factor will be initialized after\n    the model weights are loaded.\n\n    Limitations:\n    1. Only support per-tensor quantization due to torch._scaled_mm support.\n    2. Only support float8_e4m3fn data type due to the limitation of\n       torch._scaled_mm (https://github.com/pytorch/pytorch/blob/2e48b39603411a41c5025efbe52f89560b827825/aten/src/ATen/native/cuda/Blas.cpp#L854-L856)\n       \n    Args:\n        quant_config: The quantization config.\n    \"\"\"\n\n    def __init__(self, quant_config: Fp8Config):\n        self.quant_config = quant_config\n\n    def _create_scale_param(\n        self,\n        scale_name: str,\n        layer: torch.nn.Module,\n        output_partition_sizes: List[int],\n        **extra_weight_attrs,\n    ) -> None:\n        scale = Parameter(torch.empty(len(output_partition_sizes),\n                                      dtype=torch.float32),\n                          requires_grad=False)\n        layer.register_parameter(scale_name, scale)\n        set_weight_attrs(\n            scale, {\n                **extra_weight_attrs,\n                \"fp8_scales_shard_indexer\":\n                self.scales_shard_indexer,\n            })\n\n    def create_weights(\n        self,\n        layer: torch.nn.Module,\n        input_size_per_partition: int,\n        output_partition_sizes: List[int],\n        input_size: int,\n        output_size: int,\n        params_dtype: torch.dtype,\n        **extra_weight_attrs,\n    ):\n        del input_size, output_size\n        output_size_per_partition = sum(output_partition_sizes)\n\n        layer.process_after_load = True\n        layer.logical_widths = output_partition_sizes\n\n        # WEIGHT\n        weight_dtype = (torch.float8_e4m3fn\n                        if self.quant_config.is_checkpoint_fp8_serialized else\n                        params_dtype)\n        weight = Parameter(torch.empty(output_size_per_partition,\n                                       input_size_per_partition,\n                                       dtype=weight_dtype),\n                           requires_grad=False)\n        layer.register_parameter(\"weight\", weight)\n        set_weight_attrs(weight, {\n            **extra_weight_attrs,\n            \"input_dim\": 1,\n            \"output_dim\": 0,\n        })\n\n        # If checkpoint is serialized fp8, load them.\n        # Otherwise, wait until process_weights_after_loading.\n        if self.quant_config.is_checkpoint_fp8_serialized:\n            # WEIGHT SCALE\n            self._create_scale_param(\n                scale_name=\"weight_scale\",\n                layer=layer,\n                output_partition_sizes=output_partition_sizes,\n                **extra_weight_attrs)\n\n            # ACTIVATION SCALE\n            if self.quant_config.activation_scheme == \"static\":\n                self._create_scale_param(\n                    scale_name=\"act_scale\",\n                    layer=layer,\n                    output_partition_sizes=output_partition_sizes,\n                    **extra_weight_attrs)\n\n    def scales_shard_indexer(\n            self, param: torch.Tensor, loaded_weight: torch.Tensor,\n            shard_id: Union[str, int]) -> Tuple[torch.Tensor, torch.Tensor]:\n        qkv_idxs = {\"q\": 0, \"k\": 1, \"v\": 2}\n\n        if isinstance(shard_id, int):\n            pass\n        elif isinstance(shard_id, str):\n            if shard_id not in qkv_idxs:\n                raise ValueError(f\"Unknown shard_id: {shard_id}\")\n            shard_id = qkv_idxs[shard_id]\n        else:\n            ValueError(f\"Shard id must be int or str but got {type(shard_id)}\")\n\n        return param[shard_id], loaded_weight\n\n    def process_weights_after_loading(self, layer: Module) -> None:\n        if (not hasattr(layer, \"process_after_load\")\n                or not layer.process_after_load):\n            return\n\n        # If checkpoint is fp/bf16 (not serialized fp8), quantize the weights.\n        if not self.quant_config.is_checkpoint_fp8_serialized:\n            qweight, weight_scale = ops.scaled_fp8_quant(layer.weight,\n                                                         scale=None)\n            layer.weight = Parameter(qweight.t(), requires_grad=False)\n            layer.weight_scale = Parameter(weight_scale, requires_grad=False)\n            layer.logical_widths = None\n            layer.act_scale = None\n            return\n\n        # If checkpoint is fp8, requantize the separately quantized logical\n        # weights into a single fp8 weight with a single weight scale.\n        else:\n            # WEIGHT_SCALE / WEIGHT\n            #   Loop over logical weights, requantizing with single scale.\n            max_w_scale = layer.weight_scale.max()\n            start = 0\n            for idx, logical_width in enumerate(layer.logical_widths):\n                end = start + logical_width\n                weight_dq = per_tensor_dequantize(layer.weight[start:end, :],\n                                                  layer.weight_scale[idx])\n\n                layer.weight[start:end, :] = per_tensor_quantize(\n                    weight_dq, layer.weight_scale.max())\n                start = end\n            layer.weight_scale = Parameter(max_w_scale, requires_grad=False)\n\n            # WEIGHT\n            #   Transpose weight for passing to torch._scaled_mm\n            weight = layer.weight\n            layer.weight = Parameter(weight.t(), requires_grad=False)\n\n            # ACT_SCALE\n            #   Dynamic: set to None (required input to ops.scaled_fp8_quant).\n            #   Static:  set to max of the act_scales (since they are equal).\n            if self.quant_config.activation_scheme == \"dynamic\":\n                layer.act_scale = None\n            elif self.quant_config.activation_scheme == \"static\":\n                if not all_close_1d(layer.act_scale):\n                    raise ValueError(\n                        \"All the act_scales for the logical weights of a layer \"\n                        f\"must be equal. But got {layer.act_scale}\")\n                layer.act_scale = Parameter(layer.act_scale.max(),\n                                            requires_grad=False)\n            else:\n                raise ValueError(\n                    f\"Unknown scheme {self.quant_config.activation_scheme}\")\n\n    def apply(self,\n              layer: torch.nn.Module,\n              x: torch.Tensor,\n              bias: Optional[torch.Tensor] = None) -> torch.Tensor:\n        # ops.scaled_fp8_quant supports both dynamic and static quant.\n        #   If dynamic, layer.act_scale is None and x_scale computed from x.\n        #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n\n        # Fused GEMM_DQ\n        output, _ = torch._scaled_mm(\n            qinput,\n            layer.weight,\n            out_dtype=x.dtype,\n            scale_a=x_scale,\n            scale_b=layer.weight_scale,\n            bias=bias,\n        )\n\n        return output\n\n\ndef all_close_1d(x: torch.Tensor) -> bool:\n    assert len(x.shape) == 1\n    return all(torch.allclose(x[0], x[i]) for i in range(x.shape[0]))\n\n\ndef per_tensor_quantize(tensor: torch.Tensor,\n                        inv_scale: float) -> torch.Tensor:\n    finfo = torch.finfo(torch.float8_e4m3fn)\n    qweight = (tensor / inv_scale).clamp(min=finfo.min, max=finfo.max)\n    return qweight.to(torch.float8_e4m3fn)\n\n\ndef per_tensor_dequantize(tensor: torch.Tensor,\n                          inv_scale: float) -> torch.Tensor:\n    fake_qweight = tensor.to(torch.float16)\n    dq_weight = fake_qweight * inv_scale\n    return dq_weight\n",
      "diff": "diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py\nindex b57e1dde8..ff996741c 100644\n--- a/vllm/model_executor/layers/quantization/fp8.py\n+++ b/vllm/model_executor/layers/quantization/fp8.py\n@@ -231,9 +231,14 @@ class Fp8LinearMethod(LinearMethodBase):\n         # ops.scaled_fp8_quant supports both dynamic and static quant.\n         #   If dynamic, layer.act_scale is None and x_scale computed from x.\n         #   If static,  layer.act_scale is scalar and x_scale set to act_scale.\n-        qinput, x_scale = ops.scaled_fp8_quant(x, layer.act_scale)\n-\n-        # Fused GEMM_DQ\n+        qinput, x_scale = ops.scaled_fp8_quant(x,\n+                                               layer.act_scale,\n+                                               batch_dim_padding=17)\n+\n+        # Fused GEMM_DQ -- note we padded the input above because\n+        # torch._scaled_mm is more performant for matrices with\n+        # batch dimension > 16. Note that this could change\n+        # in the future.\n         output, _ = torch._scaled_mm(\n             qinput,\n             layer.weight,\n@@ -243,7 +248,7 @@ class Fp8LinearMethod(LinearMethodBase):\n             bias=bias,\n         )\n \n-        return output\n+        return torch.narrow(output, 0, 0, x.shape[0])\n \n \n def all_close_1d(x: torch.Tensor) -> bool:",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 5
    }
  ],
  "affected_apis": [
    "vllm._custom_ops.scaled_fp8_quant",
    "vllm.model_executor.layers.quantization.fp8.Fp8LinearMethod"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 2
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_fp8_quant, test_fp8, test_enabled_custom_ops)",
    "is_benchmark_actually_there": "",
    "sample_clues": "_custom_ops, apply, custom"
  }
}