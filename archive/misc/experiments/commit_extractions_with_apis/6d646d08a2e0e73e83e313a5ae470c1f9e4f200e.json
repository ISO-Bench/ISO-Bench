{
  "commit_hash": "6d646d08a2e0e73e83e313a5ae470c1f9e4f200e",
  "parent_hash": "95a178f86120f42d183b3af5ee1ce58ee05c8889",
  "message": "[Core] Optimize Async + Multi-step (#8050)",
  "author": "Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>",
  "date": "2024-09-03 18:50:29 +0000",
  "files_changed": [
    {
      "file_path": "tests/multi_step/test_correctness_async_llm.py",
      "old_content": "# Test the AsyncLLMEngine with multi-step-decoding\n\nfrom typing import List, Optional\n\nimport pytest\n\nfrom ..models.utils import check_logprobs_close\nfrom ..utils import (completions_with_server_args, get_client_text_generations,\n                     get_client_text_logprob_generations)\n\nMODELS = [\n    \"JackFram/llama-160m\",\n]\nNUM_SCHEDULER_STEPS = [8]  # Multi-step decoding steps\nNUM_PROMPTS = [10]\n\nDEFAULT_SERVER_ARGS: List[str] = [\n    \"--disable-log-requests\",\n    \"--use-v2-block-manager\",\n    \"--worker-use-ray\",\n    \"--gpu-memory-utilization\",\n    \"0.85\",\n    \"--swap-space\",\n    \"16\",\n]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize((\"tp_size, pp_size\"), [\n    (1, 1),\n    (2, 2),\n])\n@pytest.mark.parametrize(\"eager_mode\", [False, True])\n@pytest.mark.parametrize(\"num_scheduler_steps\", NUM_SCHEDULER_STEPS)\n@pytest.mark.parametrize(\"num_prompts\", NUM_PROMPTS)\n@pytest.mark.parametrize(\"num_logprobs\", [None, 5])\n@pytest.mark.parametrize(\"is_async\", [False, True])\n@pytest.mark.asyncio\nasync def test_multi_step(\n    example_prompts,\n    model: str,\n    tp_size: int,\n    pp_size: int,\n    eager_mode: int,\n    num_scheduler_steps: int,\n    num_prompts: int,\n    is_async: bool,\n    num_logprobs: Optional[int],\n) -> None:\n    \"\"\"Test vLLM engine with multi-step scheduling in an OpenAI-protocol\n    client/server environment.\n\n    Set up an engine with single-step scheduling as a ground-truth reference.\n\n    Send a completions API request to both engines with the same prompts.\n\n    Validate:\n    * Generated tokens match\n    * Generated logprobs are all very close\n\n    Args:\n      example_prompts: test fixture providing example prompts\n      model: model under test (same for single- and multi-step engines)\n      tp_size: degree of tensor-parallelism\n      pp_size: degree of pipeline-parallelism\n      eager_mode\n      num_scheduler_steps: for multi-step scheduling, GPU-side steps per\n                           GPU -> CPU output transfer\n      num_prompts: number of example prompts under test\n      num_logprobs: corresponds to the `logprobs` argument to the OpenAI\n                    completions endpoint; `None` -> no logprobs\n    \"\"\"\n\n    prompts = example_prompts\n    if len(prompts) < num_prompts:\n        prompts = prompts * ((num_prompts // len(prompts)) + 1)\n    prompts = prompts[:num_prompts]\n    assert len(prompts) == num_prompts\n\n    server_args = DEFAULT_SERVER_ARGS + [\"--enforce-eager\"]\n    ms_server_args = DEFAULT_SERVER_ARGS + \\\n        [\"--num-scheduler-steps\", f\"{num_scheduler_steps}\"]\n\n    if not is_async:\n        ms_server_args += [\"--disable-async-output-proc\"]\n\n    if eager_mode:\n        ms_server_args.append(\"--enforce-eager\")\n\n    distributed_args = [\n        \"--tensor-parallel-size\",\n        str(tp_size),\n        \"--pipeline-parallel-size\",\n        str(pp_size),\n    ]\n\n    # Spin up client/server & issue completion API requests.\n    # Default `max_wait_seconds` is 240 but was empirically\n    # was raised 3x to 720 *just for this test* due to\n    # observed timeouts in GHA CI\n    ref_completions = await completions_with_server_args(\n        prompts,\n        model,\n        server_args + distributed_args,\n        num_logprobs,\n        max_wait_seconds=3 * 240)\n    test_completions = await completions_with_server_args(\n        prompts,\n        model,\n        ms_server_args + distributed_args,\n        num_logprobs,\n        max_wait_seconds=3 * 240)\n\n    # Assert multi-step scheduling produces identical tokens\n    # to single-step scheduling.\n    ref_generations = get_client_text_generations(ref_completions)\n    test_generations = get_client_text_generations(test_completions)\n    assert ref_generations == test_generations\n\n    # Assert multi-step scheduling produces nearly-identical logprobs\n    # to single-step scheduling.\n    ref_text_logprobs = get_client_text_logprob_generations(ref_completions)\n    test_text_logprobs = get_client_text_logprob_generations(test_completions)\n    check_logprobs_close(\n        outputs_0_lst=ref_text_logprobs,\n        outputs_1_lst=test_text_logprobs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n",
      "diff": "diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex d054ca341..0cbe8371e 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -103,13 +103,13 @@ async def test_multi_step(\n         model,\n         server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n     test_completions = await completions_with_server_args(\n         prompts,\n         model,\n         ms_server_args + distributed_args,\n         num_logprobs,\n-        max_wait_seconds=3 * 240)\n+        max_wait_seconds=5 * 240)\n \n     # Assert multi-step scheduling produces identical tokens\n     # to single-step scheduling.",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/engine/async_llm_engine.py",
      "old_content": "import asyncio\nimport time\nfrom functools import partial\nfrom typing import (Any, AsyncGenerator, Callable, Dict, Iterable, List,\n                    Mapping, Optional, Set, Tuple, Type, Union)\n\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, EngineConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import (DecoderPromptComponents, LLMEngine,\n                                    PromptComponents, SchedulerOutputState)\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster, ray\nfrom vllm.inputs import (EncoderDecoderLLMInputs, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.outputs import EmbeddingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import print_warning_once\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or EmbeddingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, EmbeddingRequestOutput,\n                              Exception]) -> None:\n        if not self._finished:\n            self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if self._is_raisable(exception) else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        try:\n            while True:\n                result = await self._queue.get()\n                if self._is_raisable(result):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n    @staticmethod\n    def _is_raisable(value: Any):\n        return isinstance(value, BaseException) or \\\n                (isinstance(value, type) and \\\n                 issubclass(value, BaseException))\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n    def process_request_output(self,\n                               request_output: Union[RequestOutput,\n                                                     EmbeddingRequestOutput],\n                               *,\n                               verbose: bool = False) -> None:\n        \"\"\"Process a request output from the engine.\"\"\"\n        request_id = request_output.request_id\n        finished = request_output.finished\n\n        if finished:\n            stream = self._request_streams.pop(request_id, None)\n        else:\n            stream = self._request_streams.get(request_id)\n        # Guard against a KeyError which can occur if the request was aborted\n        # while the output was generated\n        if stream is not None:\n            stream.put(request_output)\n            if finished:\n                stream.finish()\n\n        if verbose and finished:\n            logger.info(\"Finished request %s.\", request_id)\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n    def add_request(self,\n                    request_id: str,\n                    *,\n                    verbose: bool = False,\n                    **engine_add_request_kwargs) -> AsyncStream:\n        \"\"\"Add a request to be sent to the engine on the next background\n        loop iteration.\"\"\"\n        if request_id in self._request_streams:\n            raise KeyError(f\"Request {request_id} already exists.\")\n\n        abort_request = partial(self.abort_request, verbose=verbose)\n        stream = AsyncStream(request_id, abort_request)\n        self._new_requests.put_nowait((stream, {\n            \"request_id\": request_id,\n            **engine_add_request_kwargs\n        }))\n\n        self.new_requests_event.set()\n\n        if verbose:\n            logger.info(\"Added request %s.\", request_id)\n\n        return stream\n\n    def abort_request(self,\n                      request_id: str,\n                      *,\n                      exception: Optional[Union[BaseException,\n                                                Type[BaseException]]] = None,\n                      verbose: bool = False) -> None:\n        \"\"\"Abort a request during next background loop iteration.\"\"\"\n        if verbose:\n            logger.info(\"Aborted request %s.\", request_id)\n\n        self._aborted_requests.put_nowait(request_id)\n\n        stream = self._request_streams.pop(request_id, None)\n        if stream is not None:\n            stream.finish(exception=exception)\n\n    def get_new_and_aborted_requests(self) -> Tuple[List[Dict], Set[str]]:\n        \"\"\"Get the new requests and finished requests to be\n        sent to the engine.\"\"\"\n        new_requests: List[Dict] = []\n        finished_requests: Set[str] = set()\n\n        while not self._aborted_requests.empty():\n            request_id = self._aborted_requests.get_nowait()\n            finished_requests.add(request_id)\n\n        while not self._new_requests.empty():\n            stream, new_request = self._new_requests.get_nowait()\n            request_id = stream.request_id\n            if request_id in finished_requests:\n                # The request has already been aborted.\n                stream.finish(asyncio.CancelledError)\n                finished_requests.discard(request_id)\n            else:\n                self._request_streams[request_id] = stream\n                new_requests.append(new_request)\n\n        return new_requests, finished_requests\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        # these are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        # Detect async + multi-step\n        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n                                    and allow_async_output_proc)\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        if not self._has_remaining_steps(seq_group_metadata_list):\n\n            # Clear outputs on scheduler iteration start\n            ctx.request_outputs.clear()\n\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            # Detect async + multi-step\n            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n                                        and allow_async_output_proc)\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n\n            # For async + multi-step, init the queue\n            if use_async_and_multi_step:\n                assert len(ctx.output_queue) == 0\n                assert seq_group_metadata_list is not None\n                ctx.output_queue.append(\n                    (None, seq_group_metadata_list, scheduler_outputs))\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                async_callback = self.async_callback_multi_step[\n                    virtual_engine] if use_async_and_multi_step \\\n                    else self.async_callback[virtual_engine]\n\n                execute_model_req.async_callback = async_callback\n                execute_model_req.use_async_and_multi_step = \\\n                    use_async_and_multi_step\n\n            # Execute the model.\n            output = await self.model_executor.execute_model_async(\n                execute_model_req)\n            # we need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, output)\n        else:\n            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n                assert not self.scheduler_config.is_multi_step\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n            output = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # Clear the cache if we have finished all the steps\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[\n                    virtual_engine] = SchedulerOutputState()\n\n            if use_async_and_multi_step:\n                # For async + multi-step, clear the queue\n                ctx.output_queue.clear()\n            else:\n                ctx.output_queue.append(\n                    (output, seq_group_metadata_list, scheduler_outputs))\n\n                if output and allow_async_output_proc:\n                    assert len(\n                        output\n                    ) == 1, \"Multi step decoding does not work with async output processing.\"  # noqa: E501\n                    self._advance_to_next_step(\n                        output[0], seq_group_metadata_list,\n                        scheduler_outputs.scheduled_seq_groups)\n\n            if not allow_async_output_proc:\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=False)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, output)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n\n        else:\n            # Multi-step case\n            if use_async_and_multi_step:\n                return []\n            else:\n                ctx.request_outputs = []\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                assert not self.scheduler_config.is_multi_step\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n            assert len(ctx.output_queue) == 0\n\n        return ctx.request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        \"\"\"Async version of :meth:`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group(\n            missing_msg=\"prompts must be None if skip_tokenizer_init is True\")\n\n        return await tokenizer.encode_async(request_id=request_id,\n                                            prompt=prompt,\n                                            lora_request=lora_request)\n\n    async def _extract_prompt_components_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        \"\"\"Async version of :meth:`_extract_prompt_components`.\"\"\"\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = await self._tokenize_prompt_async(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        \"\"\"Async version of :meth:`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_task = self._extract_prompt_components_async(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                encoder_comps = await encoder_task\n                decoder_comps = None, None, None\n            else:\n                decoder_task = self._extract_prompt_components_async(\n                    decoder_input,\n                    request_id=request_id,\n                )\n\n                encoder_comps, decoder_comps = await asyncio.gather(\n                    encoder_task, decoder_task)\n        else:\n            encoder_comps = await self._extract_prompt_components_async(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        \"\"\"Async version of :meth:`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._extract_prompt_components_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def process_model_inputs_async(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n        \"\"\"Async version of :meth:`process_model_inputs`.\"\"\"\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = await self._process_encoder_decoder_prompt_async(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = await self._process_decoder_only_prompt_async(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    async def add_request_async(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = await self.process_model_inputs_async(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.engine = self._init_engine(*args, **kwargs)\n\n        if self.engine_use_ray:\n            print_warning_once(\n                \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n                \"be removed in a future update. \"\n                \"See https://github.com/vllm-project/vllm/issues/7045.\")\n\n            if envs.VLLM_ALLOW_ENGINE_USE_RAY:\n                print_warning_once(\n                    \"VLLM_ALLOW_ENGINE_USE_RAY is set, force engine use Ray\")\n            else:\n                raise ValueError(\"`--engine-use-ray` is deprecated. \"\n                                 \"Set `VLLM_ALLOW_ENGINE_USE_RAY=1` to \"\n                                 \"force use it\")\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: EngineConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            elif distributed_executor_backend == \"mp\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.multiproc_xpu_executor import (\n                    MultiprocessingXPUExecutorAsync)\n                executor_class = MultiprocessingXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: AsyncEngineArgs,\n        start_engine_loop: bool = True,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"AsyncLLMEngine\":\n        \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config()\n\n        if engine_args.engine_use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n\n        executor_class = cls._get_executor_cls(engine_config)\n\n        # Create the async LLM engine.\n        engine = cls(\n            executor_class.uses_ray,\n            engine_args.engine_use_ray,\n            **engine_config.to_dict(),\n            executor_class=executor_class,\n            log_requests=not engine_args.disable_log_requests,\n            log_stats=not engine_args.disable_log_stats,\n            start_engine_loop=start_engine_loop,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n        return engine\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    @property\n    def limit_concurrency(self) -> Optional[int]:\n        \"\"\"Maximum number of concurrently running requests.\"\"\"\n        return None\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        if self.engine_use_ray:\n            return await self.engine.get_tokenizer.remote(  # type: ignore\n                lora_request)\n\n        return await (self.engine.get_tokenizer_group().\n                      get_lora_tokenizer_async(lora_request))\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = kwargs[\"cache_config\"]\n            parallel_config = kwargs[\"parallel_config\"]\n            if (parallel_config.tensor_parallel_size == 1\n                    and parallel_config.pipeline_parallel_size == 1):\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            try:\n                if self.engine_use_ray:\n                    await self.engine.add_request.remote(  # type: ignore\n                        **new_request)\n                else:\n                    await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await self.engine.step.remote()  # type: ignore\n        else:\n            request_outputs = await self.engine.step_async(virtual_engine)\n\n        # Put the outputs into the corresponding streams.\n        finished = True\n        for request_output in request_outputs:\n            self._request_tracker.process_request_output(\n                request_output, verbose=self.log_requests)\n            finished = finished and request_output.finished\n\n        return not finished\n\n    async def _engine_abort(self, request_ids: Iterable[str]):\n        if self.engine_use_ray:\n            await self.engine.abort_request.remote(request_ids)  # type: ignore\n        else:\n            self.engine.abort_request(request_ids)\n\n    async def run_engine_loop(self):\n        if self.engine_use_ray:\n            pipeline_parallel_size = 1  # type: ignore\n        else:\n            pipeline_parallel_size = \\\n                self.engine.parallel_config.pipeline_parallel_size\n        has_requests_in_progress = [False] * pipeline_parallel_size\n        while True:\n            if not any(has_requests_in_progress):\n                logger.debug(\"Waiting for new requests...\")\n                # Stop the execute model loop in parallel workers until there\n                # are more requests to process. This avoids waiting\n                # indefinitely in torch.distributed ops which may otherwise\n                # timeout, and unblocks the RPC thread in the workers so that\n                # they can process any other queued control plane messages,\n                # such as add/remove lora adapters.\n                if self.engine_use_ray:\n                    await (self.engine.stop_remote_worker_execution_loop.\n                           remote()  # type: ignore\n                           )\n                else:\n                    await self.engine.stop_remote_worker_execution_loop_async()\n                await self._request_tracker.wait_for_new_requests()\n                logger.debug(\"Got new requests!\")\n                requests_in_progress = [\n                    asyncio.create_task(self.engine_step(ve))\n                    for ve in range(pipeline_parallel_size)\n                ]\n                has_requests_in_progress = [True] * pipeline_parallel_size\n\n            # Abort if iteration takes too long due to unrecoverable errors\n            # (eg. NCCL timeouts).\n            try:\n                async with asyncio_timeout(ENGINE_ITERATION_TIMEOUT_S):\n                    done, _ = await asyncio.wait(\n                        requests_in_progress,\n                        return_when=asyncio.FIRST_COMPLETED)\n                    for _ in range(pipeline_parallel_size):\n                        await asyncio.sleep(0)\n                for task in done:\n                    result = task.result()\n                    virtual_engine = requests_in_progress.index(task)\n                    if self.engine_use_ray:\n                        has_unfinished_requests = (\n                            await (self.engine.\n                                   has_unfinished_requests_for_virtual_engine.\n                                   remote(  # type: ignore\n                                       virtual_engine)))\n                    else:\n                        has_unfinished_requests = (\n                            self.engine.\n                            has_unfinished_requests_for_virtual_engine(\n                                virtual_engine))\n                    if result or has_unfinished_requests:\n                        requests_in_progress[virtual_engine] = (\n                            asyncio.create_task(\n                                self.engine_step(virtual_engine)))\n                        has_requests_in_progress[virtual_engine] = True\n                    else:\n                        has_requests_in_progress[virtual_engine] = False\n            except asyncio.TimeoutError as exc:\n                logger.error(\n                    \"Engine iteration timed out. This should never happen!\")\n                self.set_errored(exc)\n                raise\n            await asyncio.sleep(0)\n\n    # This method does not need to be async, but kept that way\n    # for backwards compatibility.\n    async def add_request(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None\n    ) -> AsyncGenerator[Union[RequestOutput, EmbeddingRequestOutput], None]:\n        if not self.is_running:\n            if self.start_engine_loop:\n                self.start_background_loop()\n            else:\n                raise AsyncEngineDeadError(\n                    \"Background loop is not running. If it was running, \"\n                    \"inspect the output to find the stacktrace of the \"\n                    \"error that caused the background loop to stop \"\n                    \"(AsyncEngineDeadError).\")\n\n        stream = self._request_tracker.add_request(\n            request_id,\n            verbose=self.log_requests,\n            inputs=inputs,\n            params=params,\n            arrival_time=arrival_time or time.time(),\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request)\n\n        return stream.generator()\n\n    async def generate(\n        self,\n        inputs: PromptInputs,\n        sampling_params: SamplingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None\n    ) -> AsyncGenerator[RequestOutput, None]:\n        \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n            prompt_adapter_request: Prompt Adapter request to use \n                                            for generation, if any.\n\n        Yields:\n            The output `RequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"prompt\": \"What is LLM?\",\n            >>>     \"stream\": False, # assume the non-streaming case\n            >>>     \"temperature\": 0.0,\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.generate(\n            >>>    example_input[\"prompt\"],\n            >>>    SamplingParams(temperature=example_input[\"temperature\"]),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in await self.add_request(\n                request_id,\n                inputs,\n                sampling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n        ):\n            yield LLMEngine.validate_output(output, RequestOutput)\n\n    async def encode(\n        self,\n        inputs: PromptInputs,\n        pooling_params: PoolingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n    ) -> AsyncGenerator[EmbeddingRequestOutput, None]:\n        \"\"\"Generate outputs for a request from an embedding model.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            pooling_params: The pooling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n\n        Yields:\n            The output `EmbeddingRequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"input\": \"What is LLM?\",\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.encode(\n            >>>    example_input[\"input\"],\n            >>>    PoolingParams(),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in await self.add_request(\n                request_id,\n                inputs,\n                pooling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n        ):\n            yield LLMEngine.validate_output(output, EmbeddingRequestOutput)\n\n    async def abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        if not self.is_running:\n            raise AsyncEngineDeadError(\n                \"Background loop is not running. If it was running, \"\n                \"inspect the output to find the stacktrace of the \"\n                \"error that caused the background loop to stop \"\n                \"(AsyncEngineDeadError).\")\n\n        return self._abort(request_id)\n\n    def _abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        self._request_tracker.abort_request(request_id,\n                                            exception=asyncio.CancelledError,\n                                            verbose=self.log_requests)\n\n    async def get_model_config(self) -> ModelConfig:\n        \"\"\"Get the model configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_model_config.remote()  # type: ignore\n        else:\n            return self.engine.get_model_config()\n\n    async def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Get the parallel configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_parallel_config.remote(  # type: ignore\n            )\n        else:\n            return self.engine.get_parallel_config()\n\n    async def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Get the decoding configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_decoding_config.remote(  # type: ignore\n            )\n        else:\n            return self.engine.get_decoding_config()\n\n    async def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Get the scheduling configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_scheduler_config.remote(  # type: ignore\n            )\n        else:\n            return self.engine.get_scheduler_config()\n\n    async def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Get the lora configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_lora_config.remote(  # type: ignore\n            )\n        else:\n            return self.engine.get_lora_config()\n\n    async def do_log_stats(\n            self,\n            scheduler_outputs: Optional[SchedulerOutputs] = None,\n            model_output: Optional[List[SamplerOutput]] = None) -> None:\n        if self.engine_use_ray:\n            await self.engine.do_log_stats.remote(  # type: ignore\n                scheduler_outputs, model_output)\n        else:\n            self.engine.do_log_stats()\n\n    async def check_health(self) -> None:\n        \"\"\"Raises an error if engine is unhealthy.\"\"\"\n        t = time.perf_counter()\n        logger.debug(\"Starting health check...\")\n        if self.is_stopped:\n            raise AsyncEngineDeadError(\"Background loop is stopped.\")\n\n        if self.engine_use_ray:\n            try:\n                await self.engine.check_health.remote()  # type: ignore\n            except ray.exceptions.RayActorError as e:\n                raise RuntimeError(\"Engine is dead.\") from e\n        else:\n            await self.engine.check_health_async()\n        logger.debug(\"Health check took %fs\", time.perf_counter() - t)\n\n    async def is_tracing_enabled(self) -> bool:\n        if self.engine_use_ray:\n            return await self.engine.is_tracing_enabled.remote(  # type: ignore\n            )\n        else:\n            return self.engine.is_tracing_enabled()\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        if self.engine_use_ray:\n            ray.get(\n                self.engine.add_logger.remote(  # type: ignore\n                    logger_name=logger_name, logger=logger))\n        else:\n            self.engine.add_logger(logger_name=logger_name, logger=logger)\n\n    def remove_logger(self, logger_name: str) -> None:\n        if self.engine_use_ray:\n            ray.get(\n                self.engine.remove_logger.remote(  # type: ignore\n                    logger_name=logger_name))\n        else:\n            self.engine.remove_logger(logger_name=logger_name)\n\n    async def start_profile(self) -> None:\n        self.engine.model_executor._run_workers(\"start_profile\")\n\n    async def stop_profile(self) -> None:\n        self.engine.model_executor._run_workers(\"stop_profile\")\n",
      "diff": "diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 159281dab..7fe8053ff 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -280,40 +280,27 @@ class _AsyncLLMEngine(LLMEngine):\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n \n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -351,26 +338,20 @@ class _AsyncLLMEngine(LLMEngine):\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             # Execute the model.\n             output = await self.model_executor.execute_model_async(\n                 execute_model_req)\n+\n             # we need to do this here so that last step's sampled_token_ids can\n             # be passed to the next iteration for PP.\n             if self.scheduler_config.is_multi_step:\n                 self._update_cached_scheduler_output(virtual_engine, output)\n         else:\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             output = []\n \n         # Finish the current step for all the sequence groups.\n@@ -384,24 +365,22 @@ class _AsyncLLMEngine(LLMEngine):\n                 self.cached_scheduler_outputs[\n                     virtual_engine] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(\n-                        output\n-                    ) == 1, \"Multi step decoding does not work with async output processing.\"  # noqa: E501\n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+            if output and allow_async_output_proc:\n+                assert len(\n+                    output\n+                ) == 1, \"Async postprocessor expects only a single output set\"\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -411,17 +390,12 @@ class _AsyncLLMEngine(LLMEngine):\n \n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n         return ctx.request_outputs\n@@ -640,6 +614,17 @@ class AsyncLLMEngine:\n         self.log_requests = log_requests\n         self.engine = self._init_engine(*args, **kwargs)\n \n+        # This ensures quick processing of request outputs\n+        # so the append to asyncio queues is not delayed,\n+        # especially for multi-step.\n+        #\n+        # TODO: Currently, disabled for engine_use_ray, ask\n+        # Cody/Will/Woosuk about this case.\n+        self.use_process_request_outputs_callback = not self.engine_use_ray\n+        if self.use_process_request_outputs_callback:\n+            self.engine.process_request_outputs_callback = \\\n+                self.process_request_outputs\n+\n         if self.engine_use_ray:\n             print_warning_once(\n                 \"DEPRECATED. `--engine-use-ray` is deprecated and will \"\n@@ -883,13 +868,27 @@ class AsyncLLMEngine:\n             request_outputs = await self.engine.step_async(virtual_engine)\n \n         # Put the outputs into the corresponding streams.\n-        finished = True\n+        # If used as a callback, then already invoked inside\n+        # LLMEngine's _process_model_outputs\n+        if not self.use_process_request_outputs_callback:\n+            all_finished = self.process_request_outputs(request_outputs)\n+        else:\n+            # For callback case, we only need to detect when all\n+            # requests are finished\n+            all_finished = all(request_output.finished\n+                               for request_output in request_outputs)\n+\n+        return not all_finished\n+\n+    def process_request_outputs(self, request_outputs) -> bool:\n+        # Put the outputs into the corresponding streams.\n+        all_finished = True\n         for request_output in request_outputs:\n             self._request_tracker.process_request_output(\n                 request_output, verbose=self.log_requests)\n-            finished = finished and request_output.finished\n+            all_finished = all_finished and request_output.finished\n \n-        return not finished\n+        return all_finished\n \n     async def _engine_abort(self, request_ids: Iterable[str]):\n         if self.engine_use_ray:",
      "change_type": "modified",
      "lines_added": 55,
      "lines_removed": 56
    },
    {
      "file_path": "vllm/engine/llm_engine.py",
      "old_content": "import functools\nimport time\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field\nfrom typing import (TYPE_CHECKING, Any, ClassVar, Deque, Dict, Iterable, List,\n                    Mapping, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple, Type, Union\n\nimport torch\nfrom typing_extensions import TypeVar, assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\n                         EngineConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, EncoderDecoderLLMInputs,\n                         InputRegistry, LLMInputs, PromptInputs,\n                         SingletonPromptInputs)\nfrom vllm.inputs.parse import is_explicit_encoder_decoder_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.multimodal import MultiModalDataDict\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           Sequence, SequenceGroup, SequenceGroupMetadata,\n                           SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter, Device\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_G = TypeVar(\"_G\", bound=BaseTokenizerGroup, default=BaseTokenizerGroup)\n_O = TypeVar(\"_O\", RequestOutput, EmbeddingRequestOutput)\n\nPromptComponents = Tuple[Optional[str], List[int],\n                         Optional[MultiModalDataDict]]\nDecoderPromptComponents = Tuple[Optional[str], Optional[List[int]],\n                                Optional[MultiModalDataDict]]\n\n\n@dataclass\nclass SchedulerOutputState:\n    \"\"\"Caches the scheduler outputs for a virtual engine. Used for Multi-Step\"\"\"\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    scheduler_outputs: Optional[SchedulerOutputs] = None\n    allow_async_output_proc: bool = False\n    last_output: Optional[SamplerOutput] = None\n\n\n@dataclass\nclass SchedulerContext:\n    output_queue: Deque[Tuple[Optional[List[SamplerOutput]],\n                              List[SequenceGroupMetadata],\n                              SchedulerOutputs]] = field(\n                                  default_factory=lambda: deque())\n\n    request_outputs: List[Union[RequestOutput,\n                                EmbeddingRequestOutput]] = field(\n                                    default_factory=lambda: [])\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving \n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return output\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        observability_config: Optional[ObservabilityConfig],\n        prompt_adapter_config: Optional[PromptAdapterConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        # To improve performance, only final requests outputs may be required.\n        # If this set to true, then no intermediate outputs will be returned.\n        step_return_finished_only: bool = False,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"rope_scaling=%r, rope_theta=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n            \"num_scheduler_steps=%d, enable_prefix_caching=%s, \"\n            \"use_async_output_proc=%s)\",\n            VLLM_VERSION,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.rope_scaling,\n            model_config.rope_theta,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.pipeline_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            observability_config,\n            model_config.seed,\n            model_config.served_model_name,\n            scheduler_config.use_v2_block_manager,\n            scheduler_config.num_scheduler_steps,\n            cache_config.enable_prefix_caching,\n            model_config.use_async_output_proc,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.prompt_adapter_config = prompt_adapter_config\n        self.observability_config = observability_config or ObservabilityConfig(\n        )\n        self.log_stats = log_stats\n        self.step_return_finished_only = step_return_finished_only\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=self.observability_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(\n                scheduler_config, cache_config, lora_config,\n                parallel_config.pipeline_parallel_size,\n                functools.partial(self._process_model_outputs,\n                                  virtual_engine=v_id,\n                                  is_async=True)\n                if model_config.use_async_output_proc else None)\n            for v_id in range(parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(model_name=model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n        self.cached_scheduler_outputs = [\n            SchedulerOutputState()\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.scheduler_contexts = [\n            SchedulerContext()\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.async_callback = [\n            functools.partial(self._process_model_outputs,\n                              virtual_engine=v_id,\n                              is_async=True)\n            for v_id in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.async_callback_multi_step = [\n            functools.partial(self._process_model_outputs,\n                              virtual_engine=v_id,\n                              is_async=False)\n            for v_id in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: EngineConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            elif distributed_executor_backend == \"mp\":\n                # FIXME(kunshang):\n                # spawn needs calling `if __name__ == '__main__':``\n                # fork is not supported for xpu start new process.\n                logger.error(\n                    \"Both start methods (spawn and fork) have issue \"\n                    \"on XPU if you use mp backend, Please try ray instead.\")\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config()\n        executor_class = cls._get_executor_cls(engine_config)\n        # Create the LLM engine.\n        engine = cls(\n            **engine_config.to_dict(),\n            executor_class=executor_class,\n            log_stats=not engine_args.disable_log_stats,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n\n        return engine\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    MISSING_TOKENIZER_GROUP_MSG = (\"Unable to get tokenizer because \"\n                                   \"skip_tokenizer_init is True\")\n\n    def get_tokenizer_group(\n        self,\n        group_type: Type[_G] = BaseTokenizerGroup,\n        *,\n        missing_msg: str = MISSING_TOKENIZER_GROUP_MSG,\n    ) -> _G:\n        tokenizer_group = self.tokenizer\n\n        if tokenizer_group is None:\n            raise ValueError(missing_msg)\n        if not isinstance(tokenizer_group, group_type):\n            raise TypeError(\"Invalid type of tokenizer group. \"\n                            f\"Expected type: {group_type}, but \"\n                            f\"found type: {type(tokenizer_group)}\")\n\n        return tokenizer_group\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _get_bos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def _get_eos_token_id(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def _get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.is_encoder_decoder_model():\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning(\"Using None for decoder start token id because \"\n                           \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning(\"Falling back on <BOS> for decoder start token id \"\n                           \"because decoder start token id is not available.\")\n            dec_start_token_id = self._get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: Union[LLMInputs, EncoderDecoderLLMInputs],\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n    ) -> None:\n        self._validate_model_inputs(processed_inputs)\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self._get_eos_token_id(lora_request)\n\n        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = None\n        if 'encoder_prompt_token_ids' in processed_inputs:\n            encoder_seq = Sequence(seq_id,\n                                   processed_inputs,\n                                   block_size,\n                                   eos_token_id,\n                                   lora_request,\n                                   prompt_adapter_request,\n                                   from_decoder_prompt=False)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    _LLMInputComponentsType = Tuple[str, List[int]]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[List[int]],\n    ) -> List[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on\n\n        https://github.com/huggingface/transformers/blob/\n        4037a2b5b1278736e566aec12e169100275545ea/\n        src/transformers/generation/utils.py\n\n        specifically GenerationMixin._prepare_decoder_input_ids_for_generation()\n\n        Arguments:\n\n        * decoder_input_ids: input token ids to preprocess\n\n        Returns:\n\n        * Processed token list\n        \"\"\"\n\n        decoder_start_token_id = self._get_decoder_start_token_id()\n        assert decoder_start_token_id is not None\n\n        if decoder_input_ids is None:\n            # no decoder prompt input ->\n            # use decoder_start_token_id as decoder_input_ids\n            decoder_input_ids = self._get_default_enc_dec_decoder_prompt()\n\n        if (len(decoder_input_ids) == 0\n                or decoder_input_ids[0] != decoder_start_token_id):\n            decoder_input_ids = [decoder_start_token_id] + decoder_input_ids\n\n        return decoder_input_ids\n\n    def _tokenize_prompt(\n        self,\n        prompt: str,\n        request_id: str,\n        lora_request: Optional[LoRARequest],\n    ) -> List[int]:\n        '''\n        Wrapper around application of the model's tokenizer.\n\n        Arguments:\n\n        * prompt\n        * request_id\n        * lora_request\n\n        Returns:\n\n        * prompt token ids\n        '''\n\n        tokenizer = self.get_tokenizer_group(\n            missing_msg=\"prompts must be None if skip_tokenizer_init is True\")\n\n        return tokenizer.encode(request_id=request_id,\n                                prompt=prompt,\n                                lora_request=lora_request)\n\n    def _extract_prompt_components(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> PromptComponents:\n        '''\n        Extract the components of any single encoder or decoder input prompt.\n\n        Arguments:\n\n        * request_id\n        * inputs: single encoder or decoder input prompt\n        * lora_request: this is only valid for decoder prompts\n\n        Returns:\n\n        * prompt\n        * prompt_token_ids\n        * multi_modal_data\n        '''\n\n        if isinstance(inputs, str):\n            prompt = inputs\n            prompt_token_ids = self._tokenize_prompt(\n                prompt,\n                request_id=request_id,\n                lora_request=lora_request,\n            )\n            multi_modal_data = None\n        elif isinstance(inputs, dict):\n            if \"prompt_token_ids\" in inputs:\n                prompt = None\n                prompt_token_ids = inputs[\"prompt_token_ids\"]\n            else:\n                # NOTE: This extra assignment is required to pass mypy\n                prompt = parsed_prompt = inputs[\"prompt\"]\n                prompt_token_ids = self._tokenize_prompt(\n                    parsed_prompt,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                )\n\n            multi_modal_data = inputs.get(\"multi_modal_data\")\n        else:\n            assert_never(inputs)\n\n        return prompt, prompt_token_ids, multi_modal_data\n\n    def _apply_prompt_adapter(\n        self,\n        prompt_token_ids: List[int],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> List[int]:\n        if prompt_adapter_request:\n            prompt_token_ids = (\n                [0] * prompt_adapter_request.prompt_adapter_num_virtual_tokens\n                + prompt_token_ids)\n\n        return prompt_token_ids\n\n    def _get_default_enc_dec_decoder_prompt(self) -> List[int]:\n        '''\n        Specifically for encoder/decoder models:\n        generate a default decoder prompt for when\n        the user specifies only the encoder prompt.\n\n        Encoder/decoder models utilize the decoder\n        prompt in different ways; as new models are\n        added, it is intended that this function\n        will be extended to produce differing\n        default decoder prompts, depending on the\n        model variety.\n\n        Absent a special case, the default behavior\n        of this method is to mirror the behavior of\n        the HuggingFace (HF) GenerationMixin for a None\n        decoder prompt, which is to employ a logit processor\n        setting to force the first decoded token to be <BOS>.\n        Here, this behavior is approximated by having the\n        \"default\" decoder prompt be <BOS>.\n\n        However, it is possible that in the future\n        other models may have different or more \n        complex logic for the default decoder prompt.\n        This motivates having a special helper method\n        for default decoder prompts.\n\n        Returns:\n\n        * prompt_token_ids\n        '''\n\n        bos_token_id = self._get_bos_token_id()\n        assert bos_token_id is not None\n        return [bos_token_id]\n\n    def _build_enc_dec_llm_inputs(\n        self,\n        encoder_comps: PromptComponents,\n        decoder_comps: DecoderPromptComponents,\n    ) -> EncoderDecoderLLMInputs:\n        encoder_prompt, encoder_prompt_ids, encoder_mm_data = encoder_comps\n        decoder_prompt, decoder_prompt_ids, decoder_mm_data = decoder_comps\n\n        if encoder_mm_data is not None or decoder_mm_data is not None:\n            raise ValueError(\"Multi-modal encoder-decoder models are \"\n                             \"not supported yet\")\n\n        decoder_prompt_ids = (\n            self._prepare_decoder_input_ids_for_generation(decoder_prompt_ids))\n\n        return EncoderDecoderLLMInputs(\n            prompt_token_ids=decoder_prompt_ids,\n            prompt=decoder_prompt,\n            encoder_prompt_token_ids=encoder_prompt_ids,\n            encoder_prompt=encoder_prompt,\n        )\n\n    def _process_encoder_decoder_prompt(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n    ) -> EncoderDecoderLLMInputs:\n        '''\n        For encoder/decoder models only:\n        Process an input prompt into an\n        :class:`EncoderDecoderLLMInputs` instance.\n\n        There are two types of input prompts:\n        singleton prompts which carry only the\n        encoder prompt, and explicit encoder/decoder\n        prompts which carry both the encoder and the\n        decoder prompts as member variables.\n\n        This function handles the following scenarios:\n        * Singleton encoder prompt: extract encoder prompt\n          token ids & infer default decoder prompt token ids\n        * Explicit encoder/decoder prompt: extract encoder\n          and decoder prompt token ids\n\n        Note that for Explicit encoder/decoder prompts,\n        each sub-prompt (encoder or decoder prompt) can\n        have any possible singleton type; thus this\n        method relies on helper functions to obtain\n        token ids for the sub-prompts.\n        \n        Arguments:\n\n        * inputs: an input prompt\n        * request_id\n\n        Returns:\n\n        * :class:`EncoderDecoderLLMInputs` instance\n        '''\n\n        encoder_comps: PromptComponents\n        decoder_comps: DecoderPromptComponents\n\n        if is_explicit_encoder_decoder_prompt(inputs):\n            encoder_comps = self._extract_prompt_components(\n                inputs[\"encoder_prompt\"],\n                request_id=request_id,\n            )\n\n            if (decoder_input := inputs[\"decoder_prompt\"]) is None:\n                decoder_comps = None, None, None\n            else:\n                decoder_comps = self._extract_prompt_components(\n                    decoder_input,\n                    request_id=request_id,\n                )\n        else:\n            encoder_comps = self._extract_prompt_components(\n                inputs,\n                request_id=request_id,\n            )\n\n            decoder_comps = None, None, None\n\n        return self._build_enc_dec_llm_inputs(encoder_comps, decoder_comps)\n\n    def _build_decoder_only_llm_inputs(\n        self,\n        prompt_comps: PromptComponents,\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> LLMInputs:\n        prompt, prompt_token_ids, multi_modal_data = prompt_comps\n\n        prompt_token_ids = self._apply_prompt_adapter(\n            prompt_token_ids, prompt_adapter_request=prompt_adapter_request)\n\n        return LLMInputs(prompt_token_ids=prompt_token_ids,\n                         prompt=prompt,\n                         multi_modal_data=multi_modal_data)\n\n    def _process_decoder_only_prompt(\n        self,\n        inputs: SingletonPromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> LLMInputs:\n        '''\n        For decoder-only models:\n        Process an input prompt into an :class:`LLMInputs` instance.\n\n        Arguments:\n\n        * inputs: input prompt\n        * request_id\n        * lora_request\n        * prompt_adapter_request\n\n        Returns:\n\n        * :class:`LLMInputs` instance\n        '''\n\n        prompt_comps = self._extract_prompt_components(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    def process_model_inputs(\n        self,\n        inputs: PromptInputs,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> Union[LLMInputs, EncoderDecoderLLMInputs]:\n\n        if self.is_encoder_decoder_model():\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            model_inputs = self._process_encoder_decoder_prompt(\n                inputs,\n                request_id=request_id,\n            )\n        else:\n            if is_explicit_encoder_decoder_prompt(inputs):\n                raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                                 \"to decoder-only models\")\n\n            # Decoder-only operation\n            model_inputs = self._process_decoder_only_prompt(\n                inputs,\n                request_id=request_id,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return self.input_processor(model_inputs)\n\n    def add_request(\n        self,\n        request_id: str,\n        inputs: PromptInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            inputs: The inputs to the LLM. See\n                :class:`~vllm.inputs.PromptInputs`\n                for more details about the format of each input.\n            params: Parameters for sampling or pooling.\n                :class:`~vllm.SamplingParams` for text generation.\n                :class:`~vllm.PoolingParams` for pooling.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            trace_headers: OpenTelemetry trace headers.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `best_of` number of :class:`~vllm.Sequence` objects.\n            - Create a :class:`~vllm.SequenceGroup` object\n              from the list of :class:`~vllm.Sequence`.\n            - Add the :class:`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        processed_inputs = self.process_model_inputs(\n            inputs,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n        )\n\n    def _create_sequence_group_with_sampling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        sampling_params: SamplingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        encoder_seq: Optional[Sequence] = None,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with SamplingParams.\"\"\"\n        max_logprobs = self.get_model_config().max_logprobs\n        if (sampling_params.logprobs\n                and sampling_params.logprobs > max_logprobs) or (\n                    sampling_params.prompt_logprobs\n                    and sampling_params.prompt_logprobs > max_logprobs):\n            raise ValueError(f\"Cannot request more than \"\n                             f\"{max_logprobs} logprobs.\")\n\n        # Defensive copy of SamplingParams, which are used by the sampler,\n        # this doesn't deep-copy LogitsProcessor objects\n        sampling_params = sampling_params.clone()\n\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, seq.eos_token_id)\n\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq)\n\n        return seq_group\n\n    def _create_sequence_group_with_pooling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        pooling_params: PoolingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        encoder_seq: Optional[Sequence] = None,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with PoolingParams.\"\"\"\n        # Defensive copy of PoolingParams, which are used by the pooler\n        pooling_params = pooling_params.clone()\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            pooling_params=pooling_params,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq)\n        return seq_group\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n\n        Details:\n            - Refer to the\n              :meth:`~vllm.core.scheduler.Scheduler.abort_seq_group`\n              from class :class:`~vllm.core.scheduler.Scheduler`.\n\n        Example:\n            >>> # initialize engine and add a request with request_id\n            >>> request_id = str(0)\n            >>> # abort the request\n            >>> engine.abort_request(request_id)\n        \"\"\"\n        for scheduler in self.scheduler:\n            scheduler.abort_seq_group(request_id)\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Gets the parallel configuration.\"\"\"\n        return self.parallel_config\n\n    def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Gets the decoding configuration.\"\"\"\n        return self.decoding_config\n\n    def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Gets the scheduler configuration.\"\"\"\n        return self.scheduler_config\n\n    def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Gets the LoRA configuration.\"\"\"\n        return self.lora_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return sum(scheduler.get_num_unfinished_seq_groups()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return any(scheduler.has_unfinished_seqs()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests_for_virtual_engine(\n            self, virtual_engine: int) -> bool:\n        \"\"\"\n        Returns True if there are unfinished requests for the virtual engine.\n        \"\"\"\n        return self.scheduler[virtual_engine].has_unfinished_seqs()\n\n    def _process_sequence_group_outputs(\n        self,\n        seq_group: SequenceGroup,\n        outputs: List[EmbeddingSequenceGroupOutput],\n    ) -> None:\n        seq_group.embeddings = outputs[0].embeddings\n\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.FINISHED_STOPPED\n\n        return\n\n    def _process_model_outputs(self,\n                               virtual_engine: int,\n                               is_async: bool,\n                               sampler_output: Optional[SamplerOutput] = None,\n                               is_last_output: bool = False) -> None:\n        \"\"\"Apply the model output to the sequences in the scheduled seq groups.\n\n        virtual_engine: The engine id to operate on\n        \n        is_async: Indicates whether this postprocessor runs in \n            parallel with the GPU forward pass and is processing \n            tokens from the previous step. If this is true, then\n            no tokens need to be appended since it is already done\n            externally (before the next schedule() call)\n        \n        sampler_output: Used with multi-step execution to provide \n            sampler_output of each step\n        is_last_output: Used with multi-step execution to indicate\n            the last step (of each multi-step group)\n            \n        Returns RequestOutputs that can be returned to the client.\n        \"\"\"\n        now = time.time()\n\n        is_multi_step = sampler_output is not None\n\n        ctx: SchedulerContext = self.scheduler_contexts[virtual_engine]\n\n        if len(ctx.output_queue) == 0:\n            return None\n\n        if is_multi_step:\n            # Async + multi-step case\n            (outputs, seq_group_metadata_list,\n             scheduler_outputs) = ctx.output_queue[0]\n            assert outputs is None\n            outputs = [sampler_output]\n        else:\n            # Async standard case\n            (outputs, seq_group_metadata_list,\n             scheduler_outputs) = ctx.output_queue.popleft()\n\n        assert outputs is not None\n\n        # Sanity check\n        assert len(seq_group_metadata_list) == len(\n            scheduler_outputs.scheduled_seq_groups)\n\n        # Organize outputs by [step][sequence group] instead of\n        # [sequence group][step].\n        if len(outputs) > 1:\n            outputs_by_sequence_group = create_output_by_sequence_group(\n                outputs, num_seq_groups=len(seq_group_metadata_list))\n        else:\n            outputs_by_sequence_group = outputs\n\n        finished_before: List[int] = []\n        for i, seq_group_meta in enumerate(seq_group_metadata_list):\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                finished_before.append(i)\n                continue\n\n            if len(outputs) > 1:\n                output = outputs_by_sequence_group[i]\n            else:\n                output = [outputs_by_sequence_group[0][i]]\n\n            if not is_async:\n                seq_group.update_num_computed_tokens(\n                    scheduled_seq_group.token_chunk_size)\n\n            if outputs:\n                for o in outputs:\n                    if (isinstance(o, SamplerOutput)\n                            and seq_group.metrics is not None):\n                        if seq_group.metrics.model_forward_time is not None:\n                            seq_group.metrics.model_forward_time += (\n                                o.model_forward_time)\n                        else:\n                            seq_group.metrics.model_forward_time = (\n                                o.model_forward_time)\n                        if seq_group.metrics.model_execute_time is not None:\n                            seq_group.metrics.model_execute_time += (\n                                o.model_execute_time)\n                        else:\n                            seq_group.metrics.model_execute_time = (\n                                o.model_execute_time)\n\n            if self.model_config.embedding_mode:\n                self._process_sequence_group_outputs(seq_group, output)\n                continue\n\n            self.output_processor.process_prompt_logprob(seq_group, output)\n            if seq_group_meta.do_sample:\n                self.output_processor.process_outputs(seq_group, output,\n                                                      is_async)\n\n        # For async + multi-step, free finished seqs and create outputs\n        # only on the final step.\n        if is_multi_step and not is_last_output:\n            return\n\n        for scheduler in self.scheduler:\n            scheduler.free_finished_seq_groups()\n\n        # Create the outputs.\n        for i, _ in enumerate(seq_group_metadata_list):\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            if not is_multi_step and i in finished_before:\n                continue  # Avoids double processing\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            if (seq_group.is_finished()\n                    if self.step_return_finished_only else True):\n                request_output = RequestOutputFactory.create(seq_group)\n                ctx.request_outputs.append(request_output)\n\n        for seq_group in scheduler_outputs.ignored_seq_groups:\n            request_output = RequestOutputFactory.create(seq_group)\n            ctx.request_outputs.append(request_output)\n\n        # For async + multi-step, do stats only on the last output.\n        # Otherwise, do stats if the execution is async\n        do_stats = is_multi_step or is_async\n\n        if do_stats:\n            # Log stats.\n            self.do_log_stats(scheduler_outputs, outputs, finished_before)\n\n            # Tracing\n            self.do_tracing(scheduler_outputs)\n\n        return None\n\n    def _advance_to_next_step(\n            self, output: List[SamplerOutput],\n            seq_group_metadata_list: List[SequenceGroupMetadata],\n            scheduled_seq_groups: List[ScheduledSequenceGroup]) -> None:\n        \"\"\"Given model output from a single run, append the tokens to the\n        sequences. This is normally done inside output processor, but it is\n        required if the worker is to perform async forward pass to next step.\n        \"\"\"\n        for seq_group_metadata, sequence_group_outputs, scheduled_seq_group in \\\n            zip(seq_group_metadata_list, output, scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                continue\n\n            seq_group.update_num_computed_tokens(\n                seq_group_metadata.token_chunk_size)\n\n            if seq_group_metadata.do_sample:\n                assert len(sequence_group_outputs.samples) == 1, (\n                    \"Async output processor expects a single sample\"\n                    \" (i.e sampling_params.n == 1 and no \"\n                    \"sampling_params.best_of > 1)\")\n                sample = sequence_group_outputs.samples[0]\n\n                assert len(seq_group.seqs) == 1\n                seq = seq_group.seqs[0]\n                seq.append_token_id(sample.output_token, sample.logprobs)\n\n    def step(self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        .. figure:: https://i.imgur.com/sv2HssD.png\n            :alt: Overview of the step function\n            :align: center\n\n            Overview of the step function.\n\n        Details:\n            - Step 1: Schedules the sequences to be executed in the next\n              iteration and the token blocks to be swapped in/out/copy.\n\n                - Depending on the scheduling policy,\n                  sequences may be `preempted/reordered`.\n                - A Sequence Group (SG) refer to a group of sequences\n                  that are generated from the same prompt.\n\n            - Step 2: Calls the distributed executor to execute the model.\n            - Step 3: Processes the model output. This mainly includes:\n\n                - Decodes the relevant outputs.\n                - Updates the scheduled sequence groups with model outputs\n                  based on its `sampling parameters` (`use_beam_search` or not).\n                - Frees the finished sequence groups.\n\n            - Finally, it creates and returns the newly generated results.\n\n        Example:\n            >>> # Please see the example/ folder for more detailed examples.\n            >>>\n            >>> # initialize engine and request arguments\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> example_inputs = [(0, \"What is LLM?\",\n            >>>    SamplingParams(temperature=0.0))]\n            >>>\n            >>> # Start the engine with an event loop\n            >>> while True:\n            >>>     if example_inputs:\n            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)\n            >>>         engine.add_request(str(req_id),prompt,sampling_params)\n            >>>\n            >>>     # continue the request processing\n            >>>     request_outputs = engine.step()\n            >>>     for request_output in request_outputs:\n            >>>         if request_output.finished:\n            >>>             # return or show the request output\n            >>>\n            >>>     if not (engine.has_unfinished_requests() or example_inputs):\n            >>>         break\n        \"\"\"\n        if self.parallel_config.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is only supported through AsyncLLMEngine \"\n                \"as performance will be severely degraded otherwise.\")\n\n        # For llm_engine, there is no pipeline parallel support, so the engine\n        # used is always 0.\n        virtual_engine = 0\n\n        # These are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        # Detect async + multi-step\n        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n                                    and allow_async_output_proc)\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # Skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        if not self._has_remaining_steps(seq_group_metadata_list):\n\n            # Clear outputs on scheduler iteration start\n            ctx.request_outputs.clear()\n\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            # Detect async + multi-step\n            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n                                        and allow_async_output_proc)\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n\n            # For async + multi-step, init the queue\n            if use_async_and_multi_step:\n                assert len(ctx.output_queue) == 0\n                assert seq_group_metadata_list is not None\n                ctx.output_queue.append(\n                    (None, seq_group_metadata_list, scheduler_outputs))\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                async_callback = self.async_callback_multi_step[\n                    virtual_engine] if use_async_and_multi_step \\\n                    else self.async_callback[virtual_engine]\n\n                execute_model_req.async_callback = async_callback\n                execute_model_req.use_async_and_multi_step = \\\n                    use_async_and_multi_step\n\n            output = self.model_executor.execute_model(\n                execute_model_req=execute_model_req)\n\n            # We need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, output)\n        else:\n            # Nothing scheduled => If there is pending async postprocessor,\n            # then finish it here.\n            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n                assert not self.scheduler_config.is_multi_step\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n            # No outputs in this case\n            output = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # clear the cache if we have finished all the steps.\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[0] = SchedulerOutputState()\n\n            if use_async_and_multi_step:\n                # For async + multi-step, clear the queue\n                ctx.output_queue.clear()\n            else:\n                # Add results to the output_queue\n                # (for async or non-async postprocessing)\n                ctx.output_queue.append(\n                    (output, seq_group_metadata_list, scheduler_outputs))\n\n                if output and allow_async_output_proc:\n                    assert len(output) == 1, (\n                        \"Multi step decoding does not work \"\n                        \"with async output processing.\")\n\n                    self._advance_to_next_step(\n                        output[0], seq_group_metadata_list,\n                        scheduler_outputs.scheduled_seq_groups)\n\n            # Check if need to run the usual non-async path\n            if not allow_async_output_proc:\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=False)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, output)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n        else:\n            # Multi-step case\n            if use_async_and_multi_step:\n                return []\n            else:\n                ctx.request_outputs = []\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                assert not self.scheduler_config.is_multi_step\n                self._process_model_outputs(virtual_engine=virtual_engine,\n                                            is_async=True)\n            assert len(ctx.output_queue) == 0\n\n            # Stop the execute model loop in parallel workers until there are\n            # more requests to process. This avoids waiting indefinitely in\n            # torch.distributed ops which may otherwise timeout, and unblocks\n            # the RPC thread in the workers so that they can process any other\n            # queued control plane messages, such as add/remove lora adapters.\n            self.model_executor.stop_remote_worker_execution_loop()\n\n        return ctx.request_outputs\n\n    def _has_remaining_steps(\n        self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]\n    ) -> bool:\n        if (not self.scheduler_config.is_multi_step\n                or not seq_group_metadata_list):\n            return False\n\n        # TODO(will) this is a sanity check for nowto make sure that all the\n        # seqs are on the same steps. Eventually we will want to do some sort of\n        # dynamic scheduling when doing multi-step decoding.\n        ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps\n        if any([\n                seq_group.state.remaining_steps != ref_remaining_steps\n                for seq_group in seq_group_metadata_list[1:]\n        ]):\n            raise AssertionError((\"All running sequence groups should \"\n                                  \"have the same remaining steps.\"))\n\n        return ref_remaining_steps > 0\n\n    def _cache_scheduler_outputs_for_multi_step(\n            self, virtual_engine: int,\n            seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n            scheduler_outputs: SchedulerOutputs,\n            allow_async_output_proc: bool) -> None:\n        co = self.cached_scheduler_outputs[virtual_engine]\n\n        co.seq_group_metadata_list = seq_group_metadata_list\n        co.scheduler_outputs = scheduler_outputs\n        co.allow_async_output_proc = allow_async_output_proc\n        co.last_output = None\n\n    def _update_cached_scheduler_output(\n            self, virtual_engine: int,\n            output: List[Optional[SamplerOutput]]) -> None:\n        if (self.parallel_config.pipeline_parallel_size > 1 and len(output) > 0\n                and output[0] is not None):\n            last_output = output[-1]\n            assert last_output is not None\n            assert last_output.sampled_token_ids_cpu is not None\n            assert last_output.sampled_token_ids is None\n            assert last_output.sampled_token_probs is None\n            self.cached_scheduler_outputs[\n                virtual_engine].last_output = last_output\n\n    def _get_last_sampled_token_ids(\n            self, virtual_engine: int) -> Optional[torch.Tensor]:\n        cached_last_output = self.cached_scheduler_outputs[\n            virtual_engine].last_output\n        if (self.scheduler_config.is_multi_step\n                and self.parallel_config.pipeline_parallel_size > 1\n                and cached_last_output is not None\n                and cached_last_output.sampled_token_ids_cpu is not None):\n            return cached_last_output.sampled_token_ids_cpu\n        return None\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        if logger_name in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} already exists.\")\n        self.stat_loggers[logger_name] = logger\n\n    def remove_logger(self, logger_name: str) -> None:\n        if logger_name not in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} does not exist.\")\n        del self.stat_loggers[logger_name]\n\n    def do_log_stats(self,\n                     scheduler_outputs: Optional[SchedulerOutputs] = None,\n                     model_output: Optional[List[SamplerOutput]] = None,\n                     finished_before: Optional[List[int]] = None) -> None:\n        \"\"\"Forced log when no requests active.\"\"\"\n        if self.log_stats:\n            stats = self._get_stats(scheduler_outputs, model_output,\n                                    finished_before)\n            for logger in self.stat_loggers.values():\n                logger.log(stats)\n\n    def _get_stats(self,\n                   scheduler_outputs: Optional[SchedulerOutputs],\n                   model_output: Optional[List[SamplerOutput]] = None,\n                   finished_before: Optional[List[int]] = None) -> Stats:\n        \"\"\"Get Stats to be Logged to Prometheus.\n\n        Args:\n            scheduler_outputs: Optional, used to populate metrics related to\n                the scheduled batch,\n            model_output: Optional, used to emit speculative decoding metrics\n                which are created by the workers.\n        \"\"\"\n        now = time.time()\n\n        # System State\n        #   Scheduler State\n        num_running_sys = sum(\n            len(scheduler.running) for scheduler in self.scheduler)\n        num_swapped_sys = sum(\n            len(scheduler.swapped) for scheduler in self.scheduler)\n        num_waiting_sys = sum(\n            len(scheduler.waiting) for scheduler in self.scheduler)\n\n        # KV Cache Usage in %\n        num_total_gpu = self.cache_config.num_gpu_blocks\n        gpu_cache_usage_sys = 0.\n        if num_total_gpu is not None:\n            num_free_gpu = sum(\n                scheduler.block_manager.get_num_free_gpu_blocks()\n                for scheduler in self.scheduler)\n            gpu_cache_usage_sys = 1.0 - (num_free_gpu / num_total_gpu)\n\n        num_total_cpu = self.cache_config.num_cpu_blocks\n        cpu_cache_usage_sys = 0.\n        if num_total_cpu is not None and num_total_cpu > 0:\n            num_free_cpu = sum(\n                scheduler.block_manager.get_num_free_cpu_blocks()\n                for scheduler in self.scheduler)\n            cpu_cache_usage_sys = 1.0 - (num_free_cpu / num_total_cpu)\n\n        # Prefix Cache Hit Rate. Note that we always use\n        # the cache hit rate of the first virtual engine.\n        cpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.CPU)\n        gpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.GPU)\n\n        # Iteration stats\n        num_prompt_tokens_iter = 0\n        num_generation_tokens_iter = 0\n        time_to_first_tokens_iter: List[float] = []\n        time_per_output_tokens_iter: List[float] = []\n        num_preemption_iter = (0 if scheduler_outputs is None else\n                               scheduler_outputs.preempted)\n\n        # Request stats\n        #   Latency\n        time_e2e_requests: List[float] = []\n        #   Metadata\n        num_prompt_tokens_requests: List[int] = []\n        num_generation_tokens_requests: List[int] = []\n        best_of_requests: List[int] = []\n        n_requests: List[int] = []\n        finished_reason_requests: List[str] = []\n\n        # NOTE: This loop assumes prefill seq_groups are before\n        # decode seq_groups in scheduled_seq_groups.\n        if scheduler_outputs is not None:\n            # For async postprocessor, already finished sequences need to be\n            # not counted (to avoid double counting)\n            actual_num_batched_tokens = scheduler_outputs.num_batched_tokens  # type: ignore\n\n            num_generation_tokens_from_prefill_groups = 0.\n            # NOTE: if scheduler_outputs.num_prefill_groups > 0 and\n            # the len of scheduler_outputs.scheduled_seq_groups is !=\n            # scheduler_outputs.num_prefill_groups, this means that\n            # chunked prefills have been detected.\n\n            for idx, scheduled_seq_group in enumerate(\n                    scheduler_outputs.scheduled_seq_groups):\n                # Skip double logging when using async output proc\n                if finished_before and idx in finished_before:\n                    actual_num_batched_tokens -= 1\n                    continue\n\n                group_was_prefill = idx < scheduler_outputs.num_prefill_groups\n                seq_group = scheduled_seq_group.seq_group\n\n                # NOTE: a seq_group that completed all of its prefill tokens\n                # in the last iteration will have seq_group.is_prefill() = False\n                # with group_was_prefill = True\n                if group_was_prefill:\n                    # Number of prompt tokens.\n                    num_prompt_tokens_iter += (\n                        scheduled_seq_group.token_chunk_size)\n\n                    # If the seq_group just finished the prefill state\n                    # get TTFT.\n                    if not seq_group.is_prefill():\n                        latency = seq_group.get_last_latency(now)\n                        time_to_first_tokens_iter.append(latency)\n\n                        # One generation token per finished prefill.\n                        num_generation_tokens_from_prefill_groups += (\n                            seq_group.num_seqs())\n                else:\n                    # TPOTs.\n                    latency = seq_group.get_last_latency(now)\n                    time_per_output_tokens_iter.append(latency)\n\n                # Because of chunked prefill, we can have a single sequence\n                # group that does multiple prompt_runs. To prevent logging\n                # the same metadata more than once per request, we standardize\n                # on logging request level information for finished requests,\n                # which can only happen once.\n                if seq_group.is_finished():\n                    # Latency timings\n                    time_e2e_requests.append(now -\n                                             seq_group.metrics.arrival_time)\n                    # Metadata\n                    num_prompt_tokens_requests.append(\n                        len(seq_group.prompt_token_ids))\n                    num_generation_tokens_requests.extend([\n                        seq.get_output_len()\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n                    if seq_group.sampling_params is not None:\n                        best_of_requests.append(\n                            seq_group.sampling_params.best_of)\n                        n_requests.append(seq_group.sampling_params.n)\n                    finished_reason_requests.extend([\n                        SequenceStatus.get_finished_reason(seq.status)\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n\n            # Number of generation tokens.\n            #   num_batched_tokens equals the number of prompt_tokens plus the\n            #   number of decode_tokens in a single iteration. So,\n            #   num_generation_tokens = num_batched_tokens - num_prompt_tokens\n            #   + num_generation_tokens_from_prefill_groups (since we generate\n            #   one token on prefills on iters where the prefill finishes).\n            num_generation_tokens_iter = (\n                actual_num_batched_tokens - num_prompt_tokens_iter +\n                num_generation_tokens_from_prefill_groups)\n\n        # Spec decode, if enabled, emits specialized metrics from the worker in\n        # sampler output.\n        if model_output and (model_output[0].spec_decode_worker_metrics\n                             is not None):\n            spec_decode_metrics = model_output[0].spec_decode_worker_metrics\n        else:\n            spec_decode_metrics = None\n\n        return Stats(\n            now=now,\n            # System stats\n            #   Scheduler State\n            num_running_sys=num_running_sys,\n            num_swapped_sys=num_swapped_sys,\n            num_waiting_sys=num_waiting_sys,\n            #   KV Cache Usage in %\n            gpu_cache_usage_sys=gpu_cache_usage_sys,\n            cpu_cache_usage_sys=cpu_cache_usage_sys,\n            #   Prefix Cache Hit Rate\n            cpu_prefix_cache_hit_rate=cpu_prefix_cache_hit_rate,\n            gpu_prefix_cache_hit_rate=gpu_prefix_cache_hit_rate,\n\n            # Iteration stats\n            num_prompt_tokens_iter=num_prompt_tokens_iter,\n            num_generation_tokens_iter=num_generation_tokens_iter,\n            time_to_first_tokens_iter=time_to_first_tokens_iter,\n            time_per_output_tokens_iter=time_per_output_tokens_iter,\n            spec_decode_metrics=spec_decode_metrics,\n            num_preemption_iter=num_preemption_iter,\n\n            # Request stats\n            #   Latency\n            time_e2e_requests=time_e2e_requests,\n            #   Metadata\n            num_prompt_tokens_requests=num_prompt_tokens_requests,\n            num_generation_tokens_requests=num_generation_tokens_requests,\n            best_of_requests=best_of_requests,\n            n_requests=n_requests,\n            finished_reason_requests=finished_reason_requests,\n        )\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_executor.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_executor.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_executor.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.model_executor.pin_lora(lora_id)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        return self.model_executor.add_prompt_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_executor.remove_prompt_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> List[int]:\n        return self.model_executor.list_prompt_adapters()\n\n    def check_health(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n    def is_tracing_enabled(self) -> bool:\n        return self.tracer is not None\n\n    def do_tracing(self, scheduler_outputs: SchedulerOutputs) -> None:\n        if self.tracer is None:\n            return\n\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            seq_group = scheduled_seq_group.seq_group\n            if seq_group.is_finished():\n                self.create_trace_span(seq_group)\n\n    def create_trace_span(self, seq_group: SequenceGroup) -> None:\n        if self.tracer is None or seq_group.sampling_params is None:\n            return\n        arrival_time_nano_seconds = int(seq_group.metrics.arrival_time * 1e9)\n\n        trace_context = extract_trace_context(seq_group.trace_headers)\n\n        with self.tracer.start_as_current_span(\n                \"llm_request\",\n                kind=SpanKind.SERVER,\n                context=trace_context,\n                start_time=arrival_time_nano_seconds) as seq_span:\n            metrics = seq_group.metrics\n            ttft = metrics.first_token_time - metrics.arrival_time\n            e2e_time = metrics.finished_time - metrics.arrival_time\n            # attribute names are based on\n            # https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md\n            seq_span.set_attribute(SpanAttributes.LLM_RESPONSE_MODEL,\n                                   self.model_config.model)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_ID,\n                                   seq_group.request_id)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TEMPERATURE,\n                                   seq_group.sampling_params.temperature)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TOP_P,\n                                   seq_group.sampling_params.top_p)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_MAX_TOKENS,\n                                   seq_group.sampling_params.max_tokens)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_BEST_OF,\n                                   seq_group.sampling_params.best_of)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_N,\n                                   seq_group.sampling_params.n)\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_NUM_SEQUENCES,\n                                   seq_group.num_seqs())\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_PROMPT_TOKENS,\n                                   len(seq_group.prompt_token_ids))\n            seq_span.set_attribute(\n                SpanAttributes.LLM_USAGE_COMPLETION_TOKENS,\n                sum([\n                    seq.get_output_len()\n                    for seq in seq_group.get_finished_seqs()\n                ]))\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_TIME_IN_QUEUE,\n                                   metrics.time_in_queue)\n            seq_span.set_attribute(\n                SpanAttributes.LLM_LATENCY_TIME_TO_FIRST_TOKEN, ttft)\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_E2E, e2e_time)\n            if metrics.scheduler_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_SCHEDULER,\n                    metrics.scheduler_time)\n            if metrics.model_forward_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_FORWARD,\n                    metrics.model_forward_time / 1000.0)\n            if metrics.model_execute_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_EXECUTE,\n                    metrics.model_execute_time)\n\n    def is_encoder_decoder_model(self):\n        return self.model_config.is_encoder_decoder_model\n\n    def is_embedding_model(self):\n        return self.model_config.is_embedding_model\n\n    def _validate_model_inputs(self, inputs: Union[LLMInputs,\n                                                   EncoderDecoderLLMInputs]):\n        if self.is_encoder_decoder_model():\n            prompt_ids = inputs.get(\"encoder_prompt_token_ids\")\n        else:\n            prompt_ids = inputs.get(\"prompt_token_ids\")\n\n        if prompt_ids is None or len(prompt_ids) == 0:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        if self.model_config.is_multimodal_model:\n            max_prompt_len = self.model_config.max_model_len\n\n            if len(prompt_ids) > max_prompt_len:\n                raise ValueError(\n                    f\"The prompt (total length {len(prompt_ids)}) is too long \"\n                    f\"to fit into the model (context length {max_prompt_len}). \"\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n",
      "diff": "diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 1eab83f3b..8c5ca81fb 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -93,13 +93,14 @@ class SchedulerOutputState:\n @dataclass\n class SchedulerContext:\n     output_queue: Deque[Tuple[Optional[List[SamplerOutput]],\n-                              List[SequenceGroupMetadata],\n-                              SchedulerOutputs]] = field(\n-                                  default_factory=lambda: deque())\n-\n+                              List[SequenceGroupMetadata], SchedulerOutputs,\n+                              bool,\n+                              bool]] = field(default_factory=lambda: deque())\n     request_outputs: List[Union[RequestOutput,\n                                 EmbeddingRequestOutput]] = field(\n                                     default_factory=lambda: [])\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n \n class LLMEngine:\n@@ -357,6 +358,26 @@ class LLMEngine:\n             # different process.\n             self.tokenizer.ping()\n \n+        self.cached_scheduler_outputs = [\n+            SchedulerOutputState()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.scheduler_contexts = [\n+            SchedulerContext()\n+            for _ in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        self.async_callbacks = [\n+            functools.partial(self._process_model_outputs,\n+                              ctx=self.scheduler_contexts[v_id])\n+            for v_id in range(self.parallel_config.pipeline_parallel_size)\n+        ]\n+\n+        # Currently used by AsyncLLMEngine to ensure quick append\n+        # of request outputs to asyncio queues\n+        self.process_request_outputs_callback = None\n+\n         # Create the scheduler.\n         # NOTE: the cache_config here have been updated with the numbers of\n         # GPU and CPU blocks, which are profiled in the distributed executor.\n@@ -364,9 +385,7 @@ class LLMEngine:\n             Scheduler(\n                 scheduler_config, cache_config, lora_config,\n                 parallel_config.pipeline_parallel_size,\n-                functools.partial(self._process_model_outputs,\n-                                  virtual_engine=v_id,\n-                                  is_async=True)\n+                self.async_callbacks[v_id]\n                 if model_config.use_async_output_proc else None)\n             for v_id in range(parallel_config.pipeline_parallel_size)\n         ]\n@@ -417,30 +436,6 @@ class LLMEngine:\n                 ),\n             ))\n \n-        self.cached_scheduler_outputs = [\n-            SchedulerOutputState()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.scheduler_contexts = [\n-            SchedulerContext()\n-            for _ in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=True)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n-        self.async_callback_multi_step = [\n-            functools.partial(self._process_model_outputs,\n-                              virtual_engine=v_id,\n-                              is_async=False)\n-            for v_id in range(self.parallel_config.pipeline_parallel_size)\n-        ]\n-\n     def _initialize_kv_caches(self) -> None:\n         \"\"\"Initialize the KV cache in the worker(s).\n \n@@ -1249,11 +1244,7 @@ class LLMEngine:\n \n         return\n \n-    def _process_model_outputs(self,\n-                               virtual_engine: int,\n-                               is_async: bool,\n-                               sampler_output: Optional[SamplerOutput] = None,\n-                               is_last_output: bool = False) -> None:\n+    def _process_model_outputs(self, ctx: SchedulerContext) -> None:\n         \"\"\"Apply the model output to the sequences in the scheduled seq groups.\n \n         virtual_engine: The engine id to operate on\n@@ -1273,24 +1264,12 @@ class LLMEngine:\n         \"\"\"\n         now = time.time()\n \n-        is_multi_step = sampler_output is not None\n-\n-        ctx: SchedulerContext = self.scheduler_contexts[virtual_engine]\n-\n         if len(ctx.output_queue) == 0:\n             return None\n \n-        if is_multi_step:\n-            # Async + multi-step case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue[0]\n-            assert outputs is None\n-            outputs = [sampler_output]\n-        else:\n-            # Async standard case\n-            (outputs, seq_group_metadata_list,\n-             scheduler_outputs) = ctx.output_queue.popleft()\n-\n+        # Get pending async postprocessor\n+        (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n+         is_last_step) = ctx.output_queue.popleft()\n         assert outputs is not None\n \n         # Sanity check\n@@ -1306,6 +1285,7 @@ class LLMEngine:\n             outputs_by_sequence_group = outputs\n \n         finished_before: List[int] = []\n+        finished_now: List[int] = []\n         for i, seq_group_meta in enumerate(seq_group_metadata_list):\n             scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n@@ -1343,26 +1323,44 @@ class LLMEngine:\n \n             if self.model_config.embedding_mode:\n                 self._process_sequence_group_outputs(seq_group, output)\n-                continue\n+            else:\n+                self.output_processor.process_prompt_logprob(seq_group, output)\n+                if seq_group_meta.do_sample:\n+                    self.output_processor.process_outputs(\n+                        seq_group, output, is_async)\n \n-            self.output_processor.process_prompt_logprob(seq_group, output)\n-            if seq_group_meta.do_sample:\n-                self.output_processor.process_outputs(seq_group, output,\n-                                                      is_async)\n+            if seq_group.is_finished():\n+                finished_now.append(i)\n \n-        # For async + multi-step, free finished seqs and create outputs\n-        # only on the final step.\n-        if is_multi_step and not is_last_output:\n-            return\n+        # Generate outputs for the requests that finished this iteration\n+        for i in finished_now:\n+            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n \n-        for scheduler in self.scheduler:\n-            scheduler.free_finished_seq_groups()\n+            seq_group = scheduled_seq_group.seq_group\n+            seq_group.maybe_set_first_token_time(now)\n+            request_output = RequestOutputFactory.create(seq_group)\n+            ctx.request_outputs.append(request_output)\n \n-        # Create the outputs.\n-        for i, _ in enumerate(seq_group_metadata_list):\n-            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n+        # Free currently finished requests\n+        if finished_now:\n+            for scheduler in self.scheduler:\n+                scheduler.free_finished_seq_groups()\n+\n+        # For multi-step, do not create outputs each iteration\n+        if not is_last_step:\n+            # Immediately process request outputs here (if callback is given)\n+            if (finished_now\n+                    and self.process_request_outputs_callback is not None):\n+                self.process_request_outputs_callback(ctx.request_outputs)\n+            return\n+\n+        # Create the outputs\n+        # Note: scheduled_seq_groups and seq_group_metadata_list\n+        # must match with the indices\n+        for i, scheduled_seq_group in enumerate(\n+                scheduler_outputs.scheduled_seq_groups):\n \n-            if not is_multi_step and i in finished_before:\n+            if i in finished_before or i in finished_now:\n                 continue  # Avoids double processing\n \n             seq_group = scheduled_seq_group.seq_group\n@@ -1376,11 +1374,15 @@ class LLMEngine:\n             request_output = RequestOutputFactory.create(seq_group)\n             ctx.request_outputs.append(request_output)\n \n-        # For async + multi-step, do stats only on the last output.\n-        # Otherwise, do stats if the execution is async\n-        do_stats = is_multi_step or is_async\n+        # Immediately process request outputs here (if callback is given)\n+        if (ctx.request_outputs\n+                and self.process_request_outputs_callback is not None):\n+            self.process_request_outputs_callback(ctx.request_outputs)\n \n-        if do_stats:\n+        # For async case, we need to record the stats here.\n+        # For non-async case, the stats are done in the\n+        # LLMEngine/AsyncLLMEngine directly\n+        if is_async:\n             # Log stats.\n             self.do_log_stats(scheduler_outputs, outputs, finished_before)\n \n@@ -1485,40 +1487,26 @@ class LLMEngine:\n         scheduler_outputs = cached_outputs.scheduler_outputs\n         allow_async_output_proc = cached_outputs.allow_async_output_proc\n \n-        # Detect async + multi-step\n-        use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                    and allow_async_output_proc)\n-\n         ctx = self.scheduler_contexts[virtual_engine]\n \n+        # Clear outputs for each new scheduler iteration\n+        ctx.request_outputs.clear()\n+\n         # Skip the scheduler if there are any remaining steps in the seq groups.\n         # This ensures that the scheduler is only called again when the current\n         # batch has completed.\n         if not self._has_remaining_steps(seq_group_metadata_list):\n-\n-            # Clear outputs on scheduler iteration start\n-            ctx.request_outputs.clear()\n-\n             # Schedule iteration\n             (seq_group_metadata_list, scheduler_outputs,\n              allow_async_output_proc\n              ) = self.scheduler[virtual_engine].schedule()\n \n-            # Detect async + multi-step\n-            use_async_and_multi_step = (self.scheduler_config.is_multi_step\n-                                        and allow_async_output_proc)\n+            ctx.seq_group_metadata_list = seq_group_metadata_list\n+            ctx.scheduler_outputs = scheduler_outputs\n \n             # Maybe switch from async mode to sync mode\n             if not allow_async_output_proc and len(ctx.output_queue) > 0:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n-\n-            # For async + multi-step, init the queue\n-            if use_async_and_multi_step:\n-                assert len(ctx.output_queue) == 0\n-                assert seq_group_metadata_list is not None\n-                ctx.output_queue.append(\n-                    (None, seq_group_metadata_list, scheduler_outputs))\n+                self._process_model_outputs(ctx=ctx)\n \n             if (self.scheduler_config.is_multi_step\n                     and scheduler_outputs.num_lookahead_slots > 0):\n@@ -1555,13 +1543,8 @@ class LLMEngine:\n                 last_sampled_token_ids=last_sampled_token_ids)\n \n             if allow_async_output_proc:\n-                async_callback = self.async_callback_multi_step[\n-                    virtual_engine] if use_async_and_multi_step \\\n-                    else self.async_callback[virtual_engine]\n-\n-                execute_model_req.async_callback = async_callback\n-                execute_model_req.use_async_and_multi_step = \\\n-                    use_async_and_multi_step\n+                execute_model_req.async_callback = self.async_callbacks[\n+                    virtual_engine]\n \n             output = self.model_executor.execute_model(\n                 execute_model_req=execute_model_req)\n@@ -1573,10 +1556,8 @@ class LLMEngine:\n         else:\n             # Nothing scheduled => If there is pending async postprocessor,\n             # then finish it here.\n-            if not use_async_and_multi_step and len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+            if len(ctx.output_queue) > 0:\n+                self._process_model_outputs(ctx=ctx)\n             # No outputs in this case\n             output = []\n \n@@ -1590,28 +1571,24 @@ class LLMEngine:\n             if self.scheduler_config.is_multi_step:\n                 self.cached_scheduler_outputs[0] = SchedulerOutputState()\n \n-            if use_async_and_multi_step:\n-                # For async + multi-step, clear the queue\n-                ctx.output_queue.clear()\n-            else:\n-                # Add results to the output_queue\n-                # (for async or non-async postprocessing)\n-                ctx.output_queue.append(\n-                    (output, seq_group_metadata_list, scheduler_outputs))\n+            # Add results to the output_queue\n+            is_async = allow_async_output_proc\n+            is_last_step = True\n+            ctx.output_queue.append(\n+                (output, seq_group_metadata_list, scheduler_outputs, is_async,\n+                 is_last_step))\n \n-                if output and allow_async_output_proc:\n-                    assert len(output) == 1, (\n-                        \"Multi step decoding does not work \"\n-                        \"with async output processing.\")\n+            if output and allow_async_output_proc:\n+                assert len(output) == 1, (\n+                    \"Async postprocessor expects only a single output set\")\n \n-                    self._advance_to_next_step(\n-                        output[0], seq_group_metadata_list,\n-                        scheduler_outputs.scheduled_seq_groups)\n+                self._advance_to_next_step(\n+                    output[0], seq_group_metadata_list,\n+                    scheduler_outputs.scheduled_seq_groups)\n \n             # Check if need to run the usual non-async path\n             if not allow_async_output_proc:\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=False)\n+                self._process_model_outputs(ctx=ctx)\n \n                 # Log stats.\n                 self.do_log_stats(scheduler_outputs, output)\n@@ -1620,17 +1597,12 @@ class LLMEngine:\n                 self.do_tracing(scheduler_outputs)\n         else:\n             # Multi-step case\n-            if use_async_and_multi_step:\n-                return []\n-            else:\n-                ctx.request_outputs = []\n+            return ctx.request_outputs\n \n         if not self.has_unfinished_requests():\n             # Drain async postprocessor (if exists)\n             if len(ctx.output_queue) > 0:\n-                assert not self.scheduler_config.is_multi_step\n-                self._process_model_outputs(virtual_engine=virtual_engine,\n-                                            is_async=True)\n+                self._process_model_outputs(ctx=ctx)\n             assert len(ctx.output_queue) == 0\n \n             # Stop the execute model loop in parallel workers until there are",
      "change_type": "modified",
      "lines_added": 98,
      "lines_removed": 126
    },
    {
      "file_path": "vllm/engine/output_processor/multi_step.py",
      "old_content": "import functools\nfrom typing import Callable, List\n\nfrom vllm.core.scheduler import Scheduler\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.single_step import (\n    single_step_process_prompt_logprob)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.logger import init_logger\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (Sequence, SequenceGroup, SequenceGroupOutput,\n                           SequenceOutput, SequenceStatus)\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.utils import Counter\n\nlogger = init_logger(__name__)\n\n\nclass MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n    \"\"\"SequenceGroupOutputProcessor which handles logic related to\n    detokenization and stopping conditions. It specializes to \"multi-step\n    decoding\", where vLLM's worker may generate multiple tokens per invocation.\n    This is currently mutually exclusive with advanced sampling techniques like\n    beam search, which motivates the separation of this logic from the single\n    step output processor.\n\n    This class is responsible for things such as correctly appending all new\n    token ids to their sequence, detokenizing new token ids, truncating new\n    output tokens after an eos token, and correctly handling the case where the\n    number of new output tokens per sequence differs in a single batch.\n    \"\"\"\n\n    def __init__(\n        self,\n        detokenizer: Detokenizer,\n        scheduler: List[Scheduler],\n        seq_counter: Counter,\n        get_tokenizer_for_seq: Callable[[Sequence], AnyTokenizer],\n        stop_checker: StopChecker,\n    ):\n        self.detokenizer = detokenizer\n        self.scheduler = scheduler\n        self.seq_counter = seq_counter\n        self.get_tokenizer_for_seq = get_tokenizer_for_seq\n        self.stop_checker = stop_checker\n\n    def process_prompt_logprob(self, seq_group: SequenceGroup,\n                               outputs: List[SequenceGroupOutput]) -> None:\n        \"\"\"Process prompt logprobs associated with each step of a multi-step-\n        scheduled computation.\n\n        Args:\n          seq_group: the outputs are associated with this :class:`SequenceGroup`\n          outputs: the :class:`SequenceGroupOutput`s for all scheduler steps\n        \"\"\"\n        for output in outputs:\n            # Concatenate single-step prompt logprob processing results.\n            single_step_process_prompt_logprob(self, seq_group, output)\n\n    @staticmethod\n    @functools.lru_cache()\n    def _log_prompt_logprob_unsupported_warning_once():\n        logger.warning(\n            \"Prompt logprob is not supported by multi step workers. \"\n            \"(e.g., speculative decode uses multi step workers).\")\n\n    def process_outputs(self,\n                        sequence_group: SequenceGroup,\n                        outputs: List[SequenceGroupOutput],\n                        is_async: bool = False) -> None:\n        \"\"\"Append new tokens in the outputs to sequences in the sequence group.\n\n        This only supports sequence groups of size 1. It supports greater than\n        one new token per sequence.\n\n        This applies logic like stop condition checking and detokenization.\n        It also handles cases where there are tokens emitted after \n        the EOS token.\n\n        is_async - Indicates whether this postprocessor runs in \n            parallel with the GPU forward pass and is processing \n            tokens from the previous step. If this is true, then\n            no tokens need to be appended since it is already done\n            externally (before the next schedule() call)\n        \"\"\"\n        # TODO: Add support for async if necessary\n        assert not is_async\n\n        # Sequences can be in RUNNING or FINISHED_ABORTED state\n        # once scheduled, as a sequence is moved to FINSIHED_ABORTED\n        # if a client disconnects from the api server.\n        seqs = sequence_group.get_seqs(status=SequenceStatus.RUNNING)\n        if seqs is None:\n            seqs = sequence_group.get_seqs(\n                status=SequenceStatus.FINISHED_ABORTED)\n\n        assert seqs, \"Expected RUNNING or FINISHED_ABORTED sequences\"\n        assert len(seqs) == 1, (\n            \"Beam search not supported in multi-step decoding.\")\n        seq = seqs[0]\n\n        # Since there's only one sequence per sequence group, we can take the\n        # first sample.\n        samples = [output.samples[0] for output in outputs]\n\n        # -1 means the output token is not valid (eg. due to spec decode\n        # rejecting tokens).\n        valid_samples = [\n            sample for sample in samples if sample.output_token != -1\n        ]\n        assert valid_samples\n\n        self._process_seq_outputs(seq, valid_samples,\n                                  sequence_group.sampling_params)\n\n    def _process_seq_outputs(self, seq: Sequence,\n                             valid_samples: List[SequenceOutput],\n                             sampling_params: SamplingParams) -> None:\n        output_token_ids = [sample.output_token for sample in valid_samples]\n        output_logprobs = [sample.logprobs for sample in valid_samples]\n\n        # Truncate to max_tokens if necessary.\n        remaining_tokens = sampling_params.max_tokens - (seq.get_output_len() +\n                                                         len(output_token_ids))\n        if remaining_tokens < 0:\n            valid_samples = valid_samples[:remaining_tokens]\n            output_token_ids = output_token_ids[:remaining_tokens]\n\n        # Truncate any tokens after EOS. This is required as spec decode\n        # generates a fixed number of tokens without evaluating stopping\n        # conditions within the block. This can cause an eos token to be\n        # unintentionally ignored.\n        if not sampling_params.ignore_eos:\n            eos_token_id = self.get_tokenizer_for_seq(seq).eos_token_id\n            # Avoiding .index calls as exception throwing in the happy path\n            # is expensive.\n            for i in range(len(output_token_ids)):\n                if output_token_ids[i] == eos_token_id:\n                    output_token_ids = output_token_ids[:i + 1]\n                    valid_samples = valid_samples[:i + 1]\n                    break\n\n        # Incrementally append tokens to the sequence, as if we had only one new\n        # token.\n        for output_token_id, output_logprob in zip(output_token_ids,\n                                                   output_logprobs):\n            seq.append_token_id(\n                token_id=output_token_id,\n                logprobs=output_logprob,\n            )\n\n            new_char_count = 0\n            if sampling_params.detokenize:\n                new_char_count = self.detokenizer.decode_sequence_inplace(\n                    seq, sampling_params)\n\n            # TODO(sang): Support lora.\n            self.stop_checker.maybe_stop_sequence(\n                seq,\n                new_char_count=new_char_count,\n                sampling_params=sampling_params,\n            )\n            if seq.is_finished():\n                break\n",
      "diff": "diff --git a/vllm/engine/output_processor/multi_step.py b/vllm/engine/output_processor/multi_step.py\nindex e182cee8b..c73db765f 100644\n--- a/vllm/engine/output_processor/multi_step.py\n+++ b/vllm/engine/output_processor/multi_step.py\n@@ -85,9 +85,6 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             no tokens need to be appended since it is already done\n             externally (before the next schedule() call)\n         \"\"\"\n-        # TODO: Add support for async if necessary\n-        assert not is_async\n-\n         # Sequences can be in RUNNING or FINISHED_ABORTED state\n         # once scheduled, as a sequence is moved to FINSIHED_ABORTED\n         # if a client disconnects from the api server.\n@@ -101,19 +98,41 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n             \"Beam search not supported in multi-step decoding.\")\n         seq = seqs[0]\n \n-        # Since there's only one sequence per sequence group, we can take the\n-        # first sample.\n-        samples = [output.samples[0] for output in outputs]\n-\n-        # -1 means the output token is not valid (eg. due to spec decode\n-        # rejecting tokens).\n-        valid_samples = [\n-            sample for sample in samples if sample.output_token != -1\n-        ]\n-        assert valid_samples\n-\n-        self._process_seq_outputs(seq, valid_samples,\n-                                  sequence_group.sampling_params)\n+        if is_async:\n+            # Async case: We process tokens one by one. Here, we know the token\n+            # was already appended, so we only need to do the rest of the\n+            # postprocessor: Detokenization + stopping logic\n+            self._process_decode_and_stop(seq, sequence_group.sampling_params)\n+        else:\n+            # Standard multi-step case\n+\n+            # Since there's only one sequence per sequence group,\n+            # we can take the first sample.\n+            samples = [output.samples[0] for output in outputs]\n+\n+            # -1 means the output token is not valid (eg. due to spec decode\n+            # rejecting tokens).\n+            valid_samples = [\n+                sample for sample in samples if sample.output_token != -1\n+            ]\n+            assert valid_samples\n+\n+            self._process_seq_outputs(seq, valid_samples,\n+                                      sequence_group.sampling_params)\n+\n+    def _process_decode_and_stop(self, seq: Sequence,\n+                                 sampling_params: SamplingParams) -> None:\n+        new_char_count = 0\n+        if sampling_params.detokenize:\n+            new_char_count = self.detokenizer.decode_sequence_inplace(\n+                seq, sampling_params)\n+\n+        # TODO(sang): Support lora.\n+        self.stop_checker.maybe_stop_sequence(\n+            seq,\n+            new_char_count=new_char_count,\n+            sampling_params=sampling_params,\n+        )\n \n     def _process_seq_outputs(self, seq: Sequence,\n                              valid_samples: List[SequenceOutput],\n@@ -151,16 +170,7 @@ class MultiStepOutputProcessor(SequenceGroupOutputProcessor):\n                 logprobs=output_logprob,\n             )\n \n-            new_char_count = 0\n-            if sampling_params.detokenize:\n-                new_char_count = self.detokenizer.decode_sequence_inplace(\n-                    seq, sampling_params)\n+            self._process_decode_and_stop(seq, sampling_params)\n \n-            # TODO(sang): Support lora.\n-            self.stop_checker.maybe_stop_sequence(\n-                seq,\n-                new_char_count=new_char_count,\n-                sampling_params=sampling_params,\n-            )\n             if seq.is_finished():\n                 break",
      "change_type": "modified",
      "lines_added": 37,
      "lines_removed": 27
    },
    {
      "file_path": "vllm/sequence.py",
      "old_content": "\"\"\"Sequence and its related classes.\"\"\"\nimport copy\nimport enum\nfrom abc import ABC, abstractmethod\nfrom array import array\nfrom collections import defaultdict\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Callable, Dict, List, Mapping,\n                    Optional, Set, Tuple, Union, cast)\n\nimport msgspec\nimport torch\n\nfrom vllm.inputs.parse import is_valid_encoder_decoder_llm_inputs\nfrom vllm.lora.request import LoRARequest\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n\nif TYPE_CHECKING:\n    from vllm.inputs import LLMInputs\n    from vllm.multimodal.base import MultiModalDataDict\n\nVLLM_TOKEN_ID_ARRAY_TYPE = \"l\"\n\n\n# We use dataclass for now because it is used for\n# openai server output, and msgspec is not serializable.\n# TODO(sang): Fix it.\n@dataclass\nclass Logprob:\n    \"\"\"Infos for supporting OpenAI compatible logprobs and token ranks.\n\n    Attributes:\n        logprob: The logprob of chosen token\n        rank: The vocab rank of chosen token (>=1)\n        decoded_token: The decoded chosen token index\n    \"\"\"\n    logprob: float\n    rank: Optional[int] = None\n    decoded_token: Optional[str] = None\n\n\n# {token_id -> logprob} per each sequence group. None if the corresponding\n# sequence group doesn't require prompt logprob.\nPromptLogprobs = List[Optional[Dict[int, Logprob]]]\n# {token_id -> logprob} for each sequence group.\nSampleLogprobs = List[Dict[int, Logprob]]\n\n\nclass SequenceStatus(enum.IntEnum):\n    \"\"\"Status of a sequence.\"\"\"\n    WAITING = 0\n    RUNNING = 1\n    SWAPPED = 2\n    # Note: anything after SWAPPED (2) will be considered\n    # as a finished status.\n    FINISHED_STOPPED = 3\n    FINISHED_LENGTH_CAPPED = 4\n    FINISHED_ABORTED = 5\n    FINISHED_IGNORED = 6\n\n    @staticmethod\n    def is_finished(status: \"SequenceStatus\") -> bool:\n        return status > SequenceStatus.SWAPPED\n\n    @staticmethod\n    def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n        if status == SequenceStatus.FINISHED_STOPPED:\n            finish_reason = \"stop\"\n        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n            finish_reason = \"length\"\n        elif status == SequenceStatus.FINISHED_ABORTED:\n            finish_reason = \"abort\"\n        elif status == SequenceStatus.FINISHED_IGNORED:\n            # The ignored sequences are the sequences whose prompt lengths\n            # are longer than the model's length cap. Therefore, the stop\n            # reason should also be \"length\" as in OpenAI API.\n            finish_reason = \"length\"\n        else:\n            finish_reason = None\n        return finish_reason\n\n\nclass SequenceStage(enum.Enum):\n    PREFILL = enum.auto()\n    DECODE = enum.auto()\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Metrics associated with a request.\n\n    Attributes:\n        arrival_time: The time when the request arrived.\n        first_scheduled_time: The time when the request was first scheduled.\n        first_token_time: The time when the first token was generated.\n        time_in_queue: The time the request spent in the queue.\n        finished_time: The time when the request was finished.\n        scheduler_time: The time spent in the scheduler when this request was\n                        being considered by the scheduler.\n        model_forward_time: The time spent in the model forward pass when this\n                            request was in the batch.\n        model_execute_time: The time spent in the model execute function. This\n                            will include model forward, block/sync across\n                            workers, cpu-gpu sync time and sampling time.\n    \"\"\"\n    arrival_time: float\n    last_token_time: float\n    first_scheduled_time: Optional[float]\n    first_token_time: Optional[float]\n    time_in_queue: Optional[float]\n    finished_time: Optional[float] = None\n    scheduler_time: Optional[float] = None\n    model_forward_time: Optional[float] = None\n    model_execute_time: Optional[float] = None\n\n\nclass SequenceDataDelta(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Delta SequenceData to send to workers per step.\"\"\"\n    # A new token to be appended to existing SequenceData.\n    new_output_token_ids: List[int]\n    # Overwriting existing `cumulative_logprob`\n    new_cumulative_logprob: float\n    # Overwriting existing `num_computed_tokens`.\n    new_num_computed_tokens: int\n    # Overwriting existing `stage`.\n    new_stage: SequenceStage\n\n\nclass SequenceData(msgspec.Struct,\n                   omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Data associated with a sequence.\n\n    Args:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output. Set to an empty list if\n            None.\n\n    Attributes:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output.\n        cumulative_logprob: The cumulative log probability of the output.\n    \"\"\"\n    # NOTE: we cannot use Union[List, array] because msgspec cannot support\n    # union of 2 list types.\n    _prompt_token_ids: array\n    _output_token_ids: array = msgspec.field(\n        default_factory=lambda: array(VLLM_TOKEN_ID_ARRAY_TYPE, []))\n\n    ### The below fields should not be passed as an argument ###\n    _cumulative_logprob: float = 0.0\n    _prompt_token_ids_tuple: Tuple[int,\n                                   ...] = msgspec.field(default_factory=tuple)\n    # The number of tokens that are computed (that run against the model).\n    _num_computed_tokens: int = 0\n    _stage: SequenceStage = SequenceStage.PREFILL\n    _cached_all_token_ids: List[int] = msgspec.field(default_factory=list)\n\n    # It is used to get delta input. It is reset when `get_delta_and_reset`\n    # is called.\n    _new_appended_tokens: List[int] = msgspec.field(default_factory=list)\n\n    def __post_init__(self) -> None:\n        assert self._prompt_token_ids.typecode == \"l\"\n        assert self._output_token_ids.typecode == \"l\"\n        self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(\n            self._prompt_token_ids)\n        self._update_cached_all_tokens()\n\n    def _update_cached_all_tokens(self):\n        assert isinstance(self._prompt_token_ids, array)\n        assert isinstance(self._output_token_ids, array)\n        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n                                                     self._output_token_ids)\n\n    @property\n    def cumulative_logprob(self) -> float:\n        return self._cumulative_logprob\n\n    @property\n    def prompt_token_ids(self) -> Tuple[int, ...]:\n        return self._prompt_token_ids_tuple\n\n    @prompt_token_ids.setter\n    def prompt_token_ids(self, new_prompt_token_ids) -> None:\n        raise NotImplementedError\n\n    @property\n    def prompt_token_ids_array(self) -> array:\n        \"\"\"Return the prompt token ids in array type.\n\n        Note that the array is in \"I\" type, and it is not compatible\n        with torch.long (2 bytes vs 4 bytes). So beware of the usage.\n        \"\"\"\n        return self._prompt_token_ids\n\n    @property\n    def output_token_ids(self) -> Tuple[int, ...]:\n        return tuple(self._output_token_ids)\n\n    @output_token_ids.setter\n    def output_token_ids(self, new_output_token_ids: List[int]) -> None:\n        self._output_token_ids = array(VLLM_TOKEN_ID_ARRAY_TYPE,\n                                       new_output_token_ids)\n        self._update_cached_all_tokens()\n\n    @property\n    def output_token_ids_array(self) -> array:\n        \"\"\"Return the prompt token ids in array type.\n\n        Note that the array is in \"I\" type, and it is not compatible\n        with torch.long (2 bytes vs 4 bytes). So beware of the usage.\n        \"\"\"\n        assert isinstance(self._output_token_ids, array)\n        return self._output_token_ids\n\n    def append_token_id(self, token_id: int, logprob: float) -> None:\n        self._output_token_ids.append(token_id)\n        self._new_appended_tokens.append(token_id)\n        self._cached_all_token_ids.append(token_id)\n        self._cumulative_logprob += logprob\n\n    def get_len(self) -> int:\n        return len(self._output_token_ids) + len(self._prompt_token_ids)\n\n    def get_prompt_len(self) -> int:\n        return len(self._prompt_token_ids)\n\n    def get_output_len(self) -> int:\n        return len(self._output_token_ids)\n\n    def get_token_ids(self) -> List[int]:\n        return self._cached_all_token_ids\n\n    def get_prefix_token_ids(\n            self, num_tokens: int\n    ) -> Tuple[Tuple[int, ...], Optional[Tuple[int, ...]]]:\n        \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n        prompt_length = self.get_prompt_len()\n        if num_tokens > prompt_length:\n            return (self._prompt_token_ids_tuple,\n                    tuple(self._output_token_ids[:num_tokens - prompt_length]))\n        else:\n            return (self._prompt_token_ids_tuple[:num_tokens], None)\n\n    def get_num_computed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are already computed.\"\"\"\n        return self._num_computed_tokens\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        self._num_computed_tokens += num_new_computed_tokens\n        assert self._num_computed_tokens <= self.get_len(), (\n            self._num_computed_tokens, self.get_len())\n        # If all tokens are computed, it means it is in decoding phase.\n        if self.get_num_uncomputed_tokens() == 0:\n            self._stage = SequenceStage.DECODE\n\n    def reset_state_for_recompute(self) -> None:\n        \"\"\"Reset the number of computed tokens from this sequence. It is\n        supposed to be called when a sequence needs to be started from\n        the beginning again (e.g., sequence is preempted).\n        \"\"\"\n        self._num_computed_tokens = 0\n        self._stage = SequenceStage.PREFILL\n        self._new_appended_tokens = []\n\n    def get_num_uncomputed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are not computed.\"\"\"\n        # we use `get_len()` which includes prompt_len + output_len instead\n        # of prompt_len here. This is because during recompute we need to\n        # prefill for both prompt and output.\n        return self.get_len() - self.get_num_computed_tokens()\n\n    def get_last_token_id(self) -> int:\n        if not self._output_token_ids:\n            return self._prompt_token_ids[-1]\n        return self._output_token_ids[-1]\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.prompt_token_ids\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.output_token_ids\n\n    def get_delta_and_reset(self) -> SequenceDataDelta:\n        delta = SequenceDataDelta(self._new_appended_tokens,\n                                  self._cumulative_logprob,\n                                  self.get_num_computed_tokens(), self.stage)\n        # Reset delta state.\n        self._new_appended_tokens = []\n        return delta\n\n    def apply_delta(self, delta: SequenceDataDelta):\n        self._num_computed_tokens = delta.new_num_computed_tokens\n        self._cumulative_logprob = delta.new_cumulative_logprob\n        self._stage = delta.new_stage\n        self._output_token_ids.extend(delta.new_output_token_ids)\n        self._cached_all_token_ids.extend(delta.new_output_token_ids)\n\n    @property\n    def stage(self) -> SequenceStage:\n        return self._stage\n\n    def __repr__(self) -> str:\n        return (f\"SequenceData(\"\n                f\"prompt_token_ids={self._prompt_token_ids}, \"\n                f\"output_token_ids={self.output_token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob}, \"\n                f\"get_num_computed_tokens={self.get_num_computed_tokens()}\")\n\n\nclass Sequence:\n    \"\"\"Stores the data, status, and block information of a sequence.\n\n    The sequence is constructed from the LLMInputs instance passed\n    in through the `inputs` constructor argument.\n\n    For encoder/decoder models, LLMInputs encapsulates both a\n    decoder and encoder prompt, creating an ambiguity about which\n    prompt to construct the sequence from. The `from_decoder_prompt`\n    constructor argument signals whether to construct the Sequence\n    from the LLMInputs decoder prompt, or encoder prompt.\n\n    Args:\n        seq_id: The ID of the sequence.\n        inputs: The inputs of the sequence.\n        block_size: The block size of the sequence. Should be the same as the\n            block size used by the block manager and cache engine.\n        eos_token_id: The end-of-sequence (EOS) token id recognized by this LLM.\n        lora_request: LoRA request.\n        prompt_adapter_request: Prompt Adapter request.\n        from_decoder_prompt: Construct Sequence from LLMInputs decoder prompt\n                             (True) or encoder prompt (False.) Must be True\n                             for decoder-only model.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        seq_id: int,\n        inputs: \"LLMInputs\",\n        block_size: int,\n        eos_token_id: Optional[int] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        from_decoder_prompt: bool = True,\n    ) -> None:\n        self.seq_id = seq_id\n        self.inputs = inputs\n        self.block_size = block_size\n        self.eos_token_id = eos_token_id\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n        self.from_decoder_prompt = from_decoder_prompt\n        self._prompt: Optional[str] = None\n        self._prompt_token_ids: Optional[List[int]] = None\n\n        # For decoder-only models, a Sequence is constructed\n        # from an LLMInputs instance (the `inputs` arg.)\n        #\n        # For encoder/decoder models the same `inputs`\n        # instance could be utilized to construct either an\n        # encoder sequence or a decoder sequence, because\n        # `LLMInputs` has both decoder- and encoder-oriented\n        # member variables (i.e. it encapsulates both an encoder\n        # and a decoder prompt.) The decision of which type of sequence\n        # to generate is determined by the `from_decoder_prompt` argument.\n        #\n        # When constructing a encoder sequence\n        # (`from_decoder_prompt` False) it matters that\n        # the `LLMInputs` instance stored in `inputs` is valid\n        # in the sense that its encoder-related member variables are\n        # populated; below, an exception is raised if this is\n        # not the case.\n        #\n        # When constructing a decoder sequence (`from_decoder_prompt` True)\n        # it does not matter whether `inputs` has its encoder-related\n        # member variables populated.\n        if not (from_decoder_prompt\n                or is_valid_encoder_decoder_llm_inputs(inputs)):\n            raise ValueError(\"Cannot extract encoder input prompt from \"\n                             f\"invalid input {inputs}; did you forget the \"\n                             \"encoder input prompt fields?\")\n\n        self.data = SequenceData(\n            array(VLLM_TOKEN_ID_ARRAY_TYPE, self.prompt_token_ids))\n        self.output_logprobs: SampleLogprobs = []\n        self.output_text = \"\"\n\n        self.status = SequenceStatus.WAITING\n        self.stop_reason: Union[int, str, None] = None\n\n        # Used for incremental detokenization\n        self.prefix_offset = 0\n        self.read_offset = 0\n        # Input + output tokens\n        self.tokens: Optional[List[str]] = None\n\n    @property\n    def n_blocks(self) -> int:\n        return (self.get_len() + self.block_size - 1) // self.block_size\n\n    @property\n    def prompt(self) -> Optional[str]:\n        if self._prompt is not None:\n            # Reuse precomputed prompt string\n            return self._prompt\n\n        # Select decoder or encoder input prompt str,\n        # as appropriate\n        prompt_key: str = (\"prompt\"\n                           if self.from_decoder_prompt else \"encoder_prompt\")\n\n        # Cache prompt\n        self._prompt = cast(Optional[str], self.inputs.get(prompt_key))\n        return self._prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        if self._prompt_token_ids is not None:\n            # Reuse precomputed prompt token ids\n            return self._prompt_token_ids\n\n        # Select decoder or encoder input prompt\n        # token ids, as appropriate\n        prompt_token_ids_key: str = (\"prompt_token_ids\"\n                                     if self.from_decoder_prompt else\n                                     \"encoder_prompt_token_ids\")\n\n        # Cache computed prompt token ids\n        self._prompt_token_ids = cast(List[int],\n                                      self.inputs.get(prompt_token_ids_key))\n        return self._prompt_token_ids\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        return self.inputs.get(\"multi_modal_data\") or {}\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    def get_output_text_to_return(self, buffer_length: int):\n        # We return the full output text if the sequence is finished.\n        truncate = buffer_length and not self.is_finished()\n        return self.output_text[:-buffer_length] if truncate else (\n            self.output_text)\n\n    def hash_of_block(self, logical_idx: int) -> int:\n        # TODO This can produce incorrect hash when block size > prompt size\n\n        # Compute the number of tokens in the sequence\n        # TODO: The current hashing function is O(L^2). We should optimize\n        # this in the future.\n        num_tokens = self.num_hashed_tokens_of_block(logical_idx)\n        hashed_tokens = self.data.get_prefix_token_ids(num_tokens)\n        return hash((hashed_tokens, self.lora_int_id))\n\n    def num_hashed_tokens_of_block(self, logical_idx: int):\n        return logical_idx * self.block_size + self.block_size\n\n    def reset_state_for_recompute(self):\n        \"\"\"Reset the sequence states for recomputation.\"\"\"\n        self.data.reset_state_for_recompute()\n\n    def append_token_id(self, token_id: int, logprobs: Dict[int,\n                                                            Logprob]) -> None:\n        assert token_id in logprobs\n        self.output_logprobs.append(logprobs)\n        self.data.append_token_id(token_id, logprobs[token_id].logprob)\n\n    def get_len(self) -> int:\n        return self.data.get_len()\n\n    def get_prompt_len(self) -> int:\n        return self.data.get_prompt_len()\n\n    def get_output_len(self) -> int:\n        return self.data.get_output_len()\n\n    def get_token_ids(self) -> List[int]:\n        return self.data.get_token_ids()\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_prompt_token_ids()\n\n    def get_last_token_id(self) -> int:\n        return self.data.get_last_token_id()\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_output_token_ids()\n\n    def get_cumulative_logprob(self) -> float:\n        return self.data.cumulative_logprob\n\n    def get_beam_search_score(self,\n                              length_penalty: float = 1.0,\n                              seq_len: Optional[int] = None,\n                              eos_token_id: Optional[int] = None) -> float:\n        \"\"\"Calculate the beam search score with length penalty.\n\n        Adapted from\n\n        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938\n        \"\"\"\n        if seq_len is None:\n            seq_len = self.get_len()\n            # NOTE: HF implementation does not count the EOS token\n            # towards the length, we align with that here for testing.\n            if (eos_token_id is not None\n                    and self.get_last_token_id() == eos_token_id):\n                seq_len -= 1\n        return self.get_cumulative_logprob() / (seq_len**length_penalty)\n\n    def is_finished(self) -> bool:\n        return SequenceStatus.is_finished(self.status)\n\n    def fork(self, new_seq_id: int) -> \"Sequence\":\n        new_seq = copy.deepcopy(self)\n        new_seq.seq_id = new_seq_id\n        return new_seq\n\n    def get_num_new_tokens(self) -> int:\n        \"\"\"Get the number of new tokens to be computed.\n\n        Returns:\n            The new number of tokens to be computed. I.e., 1 for decode, or\n            the remaining prompt size for prefill.\n        \"\"\"\n        if self.data.stage == SequenceStage.DECODE:\n            return 1\n        return self.data.get_num_uncomputed_tokens()\n\n    def is_prefill(self) -> bool:\n        return self.data.stage == SequenceStage.PREFILL\n\n    def __repr__(self) -> str:\n        return (f\"Sequence(seq_id={self.seq_id}, \"\n                f\"status={self.status.name}, \"\n                f\"num_blocks={self.n_blocks}, \")\n\n\nclass SequenceGroupState(msgspec.Struct,\n                         omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Mutable state tied to a specific sequence group\"\"\"\n\n    # for multi-step decoding\n    num_steps: int = 1\n    current_step: int = 0\n\n    @property\n    def remaining_steps(self) -> int:\n        return self.num_steps - self.current_step\n\n\nclass SequenceGroup:\n    \"\"\"A group of sequences that are generated from the same prompt.\n\n    Args:\n        request_id: The ID of the request.\n        seqs: The list of sequences.\n        sampling_params: The sampling parameters used to generate the outputs.\n        arrival_time: The arrival time of the request.\n        lora_request: LoRA request.\n        embeddings: The embeddings vectors of the prompt of the sequence group\n            for an embedding model.\n        pooling_params: The pooling parameters used to generate the pooling\n            for an embedding model.\n        encoder_seq: Optional, the single encoder sequence. Should be None\n                     unless you are working with an encoder/decoder model.\n        trace_headers: OpenTelemetry trace headers.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        seqs: List[Sequence],\n        arrival_time: float,\n        sampling_params: Optional[SamplingParams] = None,\n        lora_request: Optional[LoRARequest] = None,\n        embeddings: Optional[List[float]] = None,\n        pooling_params: Optional[PoolingParams] = None,\n        encoder_seq: Optional[Sequence] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.seqs = seqs\n        self.is_single_seq = len(seqs) == 1\n        self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n\n        self.sampling_params = sampling_params\n        self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                      last_token_time=arrival_time,\n                                      first_scheduled_time=None,\n                                      first_token_time=None,\n                                      time_in_queue=None)\n        self.lora_request = lora_request\n        self.prompt_logprobs: Optional[PromptLogprobs] = None\n        self.state = SequenceGroupState()\n        self.embeddings = embeddings\n        self.pooling_params = pooling_params\n        self.prompt_adapter_request = prompt_adapter_request\n        self.encoder_seq = encoder_seq\n        self.trace_headers = trace_headers\n\n    @property\n    def prompt(self) -> Optional[str]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return self.seqs[0].prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return self.seqs[0].prompt_token_ids\n\n    @property\n    def encoder_prompt(self) -> Optional[str]:\n        # There are either 0 or 1 encoder sequences\n        # If one is present, its prompt is distinct\n        # from the decoder's.\n        return (self.encoder_seq.prompt\n                if self.encoder_seq is not None else None)\n\n    @property\n    def encoder_prompt_token_ids(self) -> Optional[List[int]]:\n        # There are either 0 or 1 encoder sequences\n        # If one is present, its prompt token ids are\n        # distinct from the decoder's.\n        return (self.encoder_seq.prompt_token_ids\n                if self.encoder_seq is not None else None)\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        # All sequences in the group should have the same multi-modal data.\n        # We use the multi-modal data of an arbitrary sequence.\n        return self.seqs[0].multi_modal_data\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens\\\n                         if self.prompt_adapter_request else 0\n\n    def init_multi_step(self, num_scheduler_steps: int) -> None:\n        self.state.num_steps = num_scheduler_steps\n        self.state.current_step = 0\n\n    def get_last_latency(self, now: float) -> Optional[float]:\n        \"\"\"Sets the last token time for Request level timings.\"\"\"\n        # If still in prefill phase, raise Error.\n        if self.is_prefill():\n            raise ValueError(\n                \"seq_group.get_last_latency() should not be called \"\n                \"if the seq_group is in prefill phase.\")\n\n        # Otherwise return token latency.\n        latency = now - self.metrics.last_token_time\n        self.metrics.last_token_time = now\n        return latency\n\n    def maybe_set_first_token_time(self, time: float) -> None:\n        \"\"\"Sets the first token time for Request level timings.\"\"\"\n        # Note: in a case where a sequence_group is swapped and\n        #   recomputed, the time between iterations is counted\n        #   in TPOT, rather than recalculating TTFT (since from the )\n        #   POV of the user, there is simply a long generation delay.\n        if (self.metrics.first_token_time is None\n                and self.seqs[0].get_output_len() == 1):\n            self.metrics.first_token_time = time\n\n    def maybe_set_first_scheduled_time(self, time: float) -> None:\n        \"\"\"Sets the first scheduled time and time in queue for Request\n        level timings.\"\"\"\n        if self.metrics.first_scheduled_time is None:\n            self.metrics.first_scheduled_time = time\n            self.metrics.time_in_queue = time - self.metrics.arrival_time\n\n    def set_finished_time(self, time: Optional[float]) -> None:\n        \"\"\"Sets the finished time for Request level timings.\"\"\"\n        self.metrics.finished_time = time\n\n    def get_max_num_running_seqs(self) -> int:\n        \"\"\"The maximum number of sequences running in parallel in the remaining\n        lifetime of the request.\"\"\"\n        if self.sampling_params and self.sampling_params.use_beam_search:\n            # For beam search, maximally there will always be `best_of` beam\n            # candidates running in the future.\n            best_of = self.sampling_params.best_of\n            assert isinstance(best_of, int)\n            return best_of\n        else:\n            if self.sampling_params:\n                best_of = self.sampling_params.best_of\n                assert isinstance(best_of, int)\n                if best_of > self.num_seqs():\n                    # At prompt stage, the sequence group is not yet filled up\n                    # and only have one sequence running. However, in the\n                    # generation stage, we will have `best_of` sequences\n                    # running.\n                    return best_of\n            # At sampling stages, return the number of actual sequences\n            # that are not finished yet.\n            return self.num_unfinished_seqs()\n\n    def get_seqs(\n        self,\n        status: Optional[SequenceStatus] = None,\n    ) -> List[Sequence]:\n        if status is None:\n            return self.seqs\n\n        if self.is_single_seq:\n            return self.seqs if self.seqs[0].status == status else []\n\n        return [seq for seq in self.seqs if seq.status == status]\n\n    def is_encoder_decoder(self) -> bool:\n        return self.encoder_seq is not None\n\n    def get_encoder_seq(self) -> Optional[Sequence]:\n        return self.encoder_seq\n\n    def get_unfinished_seqs(self) -> List[Sequence]:\n        if self.is_single_seq:\n            return self.seqs if not self.seqs[0].is_finished() else []\n\n        return [seq for seq in self.seqs if not seq.is_finished()]\n\n    def get_finished_seqs(self) -> List[Sequence]:\n        if self.is_single_seq:\n            return self.seqs if self.seqs[0].is_finished() else []\n\n        return [seq for seq in self.seqs if seq.is_finished()]\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        for seq in self.seqs:\n            if not seq.is_finished():\n                seq.data.update_num_computed_tokens(num_new_computed_tokens)\n\n    def get_num_uncomputed_tokens(self) -> int:\n        num_uncomputed_tokens = 0\n        for seq in self.seqs:\n            if not seq.is_finished():\n                num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n        return num_uncomputed_tokens\n\n    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:\n        # Optimization. We don't need to call get_seqs if we don't need to\n        # filter by states.\n        if status is None:\n            return len(self.seqs)\n\n        if self.is_single_seq:\n            return 1 if self.seqs[0].status == status else 0\n\n        return len(self.get_seqs(status))\n\n    def num_unfinished_seqs(self) -> int:\n        if self.is_single_seq:\n            return 1 if not self.seqs[0].is_finished() else 0\n\n        return len(self.get_unfinished_seqs())\n\n    def num_finished_seqs(self) -> int:\n        if self.is_single_seq:\n            return 1 if self.seqs[0].is_finished() else 0\n\n        return len(self.get_finished_seqs())\n\n    def find(self, seq_id: int) -> Sequence:\n        if seq_id not in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        return self.seqs_dict[seq_id]\n\n    def add(self, seq: Sequence) -> None:\n        if seq.seq_id in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n        self.seqs_dict[seq.seq_id] = seq\n        self.seqs.append(seq)\n        self.is_single_seq = len(self.seqs) == 1\n\n    def remove(self, seq_id: int) -> None:\n        seq = self.seqs_dict.pop(seq_id, None)\n        if seq is None:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        self.seqs.remove(seq)\n        self.is_single_seq = len(self.seqs) == 1\n\n    def is_finished(self) -> bool:\n        if self.is_single_seq:\n            return self.seqs[0].is_finished()\n\n        return all(seq.is_finished() for seq in self.seqs)\n\n    def is_prefill(self) -> bool:\n        # Every sequence should be in the same stage.\n        return self.seqs[0].is_prefill()\n\n    def __repr__(self) -> str:\n        return (f\"SequenceGroup(request_id={self.request_id}, \"\n                f\"sampling_params={self.sampling_params}, \"\n                f\"num_seqs={len(self.seqs)})\")\n\n\nclass SequenceGroupMetadataDelta(\n        msgspec.Struct,\n        tag=True,  # type: ignore[call-arg]\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Delta of SequenceGroupMetadata.\n\n    After sending the first SequenceGroupMetadata, vLLM scheduler\n    only sends delta to reduce the data payload size.\n    \"\"\"\n    seq_data_delta: Dict[int, SequenceDataDelta]\n    request_id: str\n    block_tables: Dict[int, List[int]]\n    is_prompt: bool\n    do_sample: bool = True\n    token_chunk_size: Optional[int] = None\n    computed_block_nums: Optional[List[int]] = None\n    state: Optional[SequenceGroupState] = msgspec.field(\n        default_factory=lambda: SequenceGroupState())\n\n\nclass SequenceGroupMetadata(\n        msgspec.Struct,\n        tag=True,  # type: ignore[call-arg]\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Metadata for a sequence group. Used to create `AttentionMetadata`.\n\n    Args:\n        request_id: The ID of the request.\n        is_prompt: Whether the request is at prompt stage.\n        seq_data: The sequence data. (Seq id -> sequence data)\n        sampling_params: The sampling parameters used to generate the outputs.\n        block_tables: The block tables. (Seq id -> list of physical block\n            numbers)\n        do_sample: True if sampling is required. Sampling is not required when\n            e.g., prefill is chunked, and the current iteration only computes\n            query tokens for prefill, we don't need sampling.\n        token_chunk_size: The number of tokens to be processed (per sequence).\n            None if chunking is not required.\n        lora_request: LoRA request.\n        computed_block_nums: The block numbers that are already computed,\n            used in prefix caching.\n        state: Internal state tied to this sequence group.\n        multi_modal_data: Multi modal data.\n        encoder_seq_data: Optional sequence data for encoder prompt\n                          (SequenceGroup.encoder_seq). Should be None \n                          unless you are working with an encoder/decoder\n                          model.\n        cross_block_table: Optional cross-attention block table associated\n                           with the encoder prompt\n                           (SequenceGroup.encoder_seq). Should be None\n                           unless you are working with an encoder/decoder\n                           model.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    request_id: str\n    is_prompt: bool\n    seq_data: Dict[int, SequenceData]\n    sampling_params: Optional[SamplingParams]\n    block_tables: Dict[int, List[int]]\n    do_sample: bool = True\n    pooling_params: Optional[PoolingParams] = None\n    lora_request: Optional[LoRARequest] = None\n    computed_block_nums: Optional[List[int]] = None\n    state: Optional[SequenceGroupState] = msgspec.field(\n        default_factory=lambda: SequenceGroupState())\n    # \"MultiModalDataDict\" types. We have to use Any due to msgspec\n    # doesn't allow to have union of 2 different dicts.\n    multi_modal_data: Optional[Any] = None\n    encoder_seq_data: Optional[SequenceData] = None\n    cross_block_table: Optional[List[int]] = None\n    prompt_adapter_request: Optional[PromptAdapterRequest] = None\n    token_chunk_size: Optional[int] = None\n\n    ### Stateful fields that are lazily defined. ###\n    # The number of speculative tokens adopted in this request.\n    # None means specuative decoding is not used.\n    # Zero means speculative decoding is disabled for some reasons.\n    # TODO: We should maintain this states out of the sequence group.\n    num_speculative_tokens: Optional[int] = None\n\n    def __post_init__(self):\n        if self.seq_data is not None and self.token_chunk_size is None:\n            if self.is_prompt:\n                self.token_chunk_size = next(iter(\n                    self.seq_data.values())).get_len()\n            else:\n                self.token_chunk_size = 1\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens \\\n                        if self.prompt_adapter_request else 0\n\n    def apply_delta(self,\n                    sequence_group_metadata_delta: SequenceGroupMetadataDelta):\n        for id, delta in sequence_group_metadata_delta.seq_data_delta.items():\n            self.seq_data[id].apply_delta(delta)\n        assert self.request_id == sequence_group_metadata_delta.request_id\n        self.block_tables = sequence_group_metadata_delta.block_tables\n        self.token_chunk_size = sequence_group_metadata_delta.token_chunk_size\n        self.do_sample = sequence_group_metadata_delta.do_sample\n        self.is_prompt = sequence_group_metadata_delta.is_prompt\n\n    def finish_step(self) -> None:\n        assert self.state is not None\n        assert self.state.current_step < self.state.num_steps\n        self.state.current_step += 1\n\n\nclass SequenceOutput(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        array_like=True):  # type: ignore[call-arg]\n    \"\"\"The model output associated with a sequence.\n\n    Args:\n        parent_seq_id: The ID of the parent sequence (for forking in beam\n            search).\n        output_token: The output token ID.\n        logprobs: The logprobs of the output token.\n            (Token id -> logP(x_i+1 | x_0, ..., x_i))\n    \"\"\"\n    parent_seq_id: int\n    output_token: int\n    logprobs: Dict[int, Logprob]\n\n    def __repr__(self) -> str:\n        return (f\"SequenceOutput(parent_seq_id={self.parent_seq_id}, \"\n                f\"output_token={self.output_token}, \"\n                f\"logprobs={self.logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, SequenceOutput):\n            raise NotImplementedError()\n        equal = (self.parent_seq_id == other.parent_seq_id\n                 and self.output_token == other.output_token)\n        log_probs_equal = other.logprobs == self.logprobs\n        return equal and log_probs_equal\n\n\nclass SequenceGroupOutput(ABC):\n    \"\"\"The base class for model outputs associated with a sequence group.\"\"\"\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        pass\n\n    @abstractmethod\n    def __eq__(self, other: object) -> bool:\n        pass\n\n\nclass CompletionSequenceGroupOutput(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        array_like=True):  # type: ignore[call-arg]\n    __metaclass__ = SequenceGroupOutput\n    \"\"\"The model output associated with a completion sequence group.\"\"\"\n    samples: List[SequenceOutput]\n    # Prompt logprob for each prompt query token.\n    prompt_logprobs: Optional[PromptLogprobs]\n\n    def __repr__(self) -> str:\n        return (f\"CompletionSequenceGroupOutput(samples={self.samples}, \"\n                f\"prompt_logprobs={self.prompt_logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, CompletionSequenceGroupOutput):\n            raise NotImplementedError()\n        return (self.samples == other.samples\n                and self.prompt_logprobs == other.prompt_logprobs)\n\n\nclass EmbeddingSequenceGroupOutput(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        array_like=True,  # type: ignore[call-arg]\n):\n    \"\"\"The model output associated with an embedding sequence group.\"\"\"\n    __metaclass__ = SequenceGroupOutput\n    embeddings: List[int]\n\n    def __repr__(self) -> str:\n        return (f\"EmbeddingSequenceGroupOutput(\"\n                f\"embeddings_shape={len(self.embeddings)})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, EmbeddingSequenceGroupOutput):\n            raise NotImplementedError()\n        return self.embeddings == other.embeddings\n\n\nclass IntermediateTensors(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        array_like=True):  # type: ignore[call-arg]\n    \"\"\"For all pipeline stages except the last, we need to return the hidden\n    states and residuals to be sent to the next stage. This data structure\n    contains the hidden states and residuals for a request.\n    \"\"\"\n\n    tensors: Dict[str, torch.Tensor]\n\n    def __getitem__(self, key: Union[str, slice]):\n        if isinstance(key, str):\n            return self.tensors[key]\n        elif isinstance(key, slice):\n            return self.__class__({k: v[key] for k, v in self.tensors.items()})\n\n    def __setitem__(self, key: str, value):\n        self.tensors[key] = value\n\n    def __len__(self):\n        return len(self.tensors)\n\n    def __eq__(self, other: object):\n        return isinstance(other, self.__class__) and self\n\n    def __repr__(self) -> str:\n        return f\"IntermediateTensors(tensors={self.tensors})\"\n\n\nclass PoolerOutput(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        array_like=True):  # type: ignore[call-arg]\n    \"\"\"The output from a pooling operation in the embedding model.\"\"\"\n    outputs: List[EmbeddingSequenceGroupOutput]\n\n    spec_decode_worker_metrics: Optional[SpecDecodeWorkerMetrics] = None\n\n    def __getitem__(self, idx: int):\n        return self.outputs[idx]\n\n    def __setitem__(self, idx: int, value):\n        self.outputs[idx] = value\n\n    def __len__(self):\n        return len(self.outputs)\n\n    def __eq__(self, other: object):\n        return isinstance(other,\n                          self.__class__) and self.outputs == other.outputs\n\n\ndef get_all_seq_ids(\n        seq_group_metadata_list: List[SequenceGroupMetadata]) -> List[int]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    return [seq_id for sg in seq_group_metadata_list for seq_id in sg.seq_data]\n\n\ndef get_all_seq_ids_and_request_ids(\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n) -> Tuple[List[int], Dict[str, Set[int]]]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    seq_ids: List[int] = []\n    request_id_seq_ids_mapping: Dict[str, Set[int]] = defaultdict(set)\n    for sg in seq_group_metadata_list:\n        for seq_id in sg.seq_data:\n            seq_ids.append(seq_id)\n            request_id_seq_ids_mapping[sg.request_id].add(seq_id)\n    return seq_ids, request_id_seq_ids_mapping\n\n\nclass HiddenStates(msgspec.Struct, array_like=True,\n                   omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"Hidden states corresponding to in-progress sequences.\n    Used in speculative decoding to pass hidden states from\n    the target model to the proposer model.\n\n    seq_ids are the sequence ids of each entry of the batch\n    dimension of the hidden_states tensor\"\"\"\n    # Scorer hidden states. For prefill step, it is used for hidden states of\n    # all tokens, whereas for decode step, it use used for last accepted tokens.\n    hidden_states: torch.Tensor\n    # The sequence group metadata list. Only needed for decode step.\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    # Scorer hidden states of the 2nd last token proposed by the proposer (\n    # irrespective of whether it was accepted or not). Only used for cases when\n    # last proposed token is accepted (i.e., in case of bonus tokens). For the\n    # case of no bonus tokens, these are ignored.\n    second_last_token_hidden_states: Optional[torch.Tensor] = None\n\n    _seq_ids: List[int] = msgspec.field(default_factory=list)\n\n    def __post_init__(self):\n        if self.seq_group_metadata_list is not None:\n            assert len(self.seq_group_metadata_list) == len(self.hidden_states)\n            self._seq_ids = get_all_seq_ids(self.seq_group_metadata_list)\n\n    @property\n    def seq_ids(self) -> List[int]:\n        return self._seq_ids\n\n    def update(self,\n               hidden_states: torch.Tensor,\n               seq_group_metadata_list: List[SequenceGroupMetadata],\n               second_last_token_hidden_states: Optional[torch.Tensor] = None):\n        \"\"\"Update hidden states from target model invocation. Only used for\n        decode steps\"\"\"\n        assert len(seq_group_metadata_list) == len(hidden_states)\n        self._seq_ids.extend(get_all_seq_ids(seq_group_metadata_list))\n        self.hidden_states = torch.cat([self.hidden_states, hidden_states])\n\n        if self.second_last_token_hidden_states is not None:\n            # Adding dummy hidden_states to this to maintain same shape\n            self.second_last_token_hidden_states = torch.cat([\n                self.second_last_token_hidden_states,\n                torch.zeros_like(hidden_states)\n                if second_last_token_hidden_states is None else\n                second_last_token_hidden_states\n            ])\n\n    def prune(self,\n              seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        \"\"\"Prune to provided list of sequence ids. Only used for decode steps.\n        \"\"\"\n        # Currently this prunes all seq_ids not present in\n        # seq_group_metadata_list which might cause problems where a sequence\n        # may be \"paused\" then \"resumed\" later. This should only prune sequences\n        # which are confirmed to be aborted.\n        seq_ids = get_all_seq_ids(seq_group_metadata_list)\n        if seq_ids != self._seq_ids:\n            # Batch contents changed - prune removed sequences.\n            index = [self._seq_ids.index(seq_id) for seq_id in seq_ids]\n            self.hidden_states = self.hidden_states[index]\n            if self.second_last_token_hidden_states is not None:\n                self.second_last_token_hidden_states = self\\\n                    .second_last_token_hidden_states[index]\n            self._seq_ids = seq_ids\n\n    def expand_with_bonus_tokens(\n            self, seq_with_bonus_token_in_last_step: set) -> None:\n        \"\"\"Expand hidden states for sequences with bonus tokens. This is in\n        alignment with `MultiStepWorker._expand_execute_model_request`.\"\"\"\n        if self.second_last_token_hidden_states is None \\\n            or not seq_with_bonus_token_in_last_step:\n            return\n\n        index = []\n        for seq_id in self._seq_ids:\n            i = self._seq_ids.index(seq_id)\n            if seq_id in seq_with_bonus_token_in_last_step:\n                index.append(i + len(self._seq_ids))\n            index.append(i)\n\n        self.hidden_states = torch.cat(\n            [self.hidden_states, self.second_last_token_hidden_states])[index]\n\n\nclass ExecuteModelRequest(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True):  # type: ignore[call-arg]\n    \"\"\"The model execution request, containing CPU metadata only. The LLM\n    engine should create an instance of this class for each request batch.\"\"\"\n    # The sequence group metadata list.\n    seq_group_metadata_list: List[Union[SequenceGroupMetadata,\n                                        SequenceGroupMetadataDelta]]\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int,\n                                  int]] = msgspec.field(default_factory=list)\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int,\n                                   int]] = msgspec.field(default_factory=list)\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]] = msgspec.field(default_factory=list)\n    # Virtual engine ID for pipeline parallel.\n    virtual_engine: int = 0\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int = 0\n    # The number of requests in the running queue.\n    running_queue_size: int = 0\n    # Optional hidden states from prior step.\n    previous_hidden_states: Optional[HiddenStates] = None\n    # The number of forward steps to run.\n    num_steps: int = 1\n    # Finished request ids since last step.\n    finished_requests_ids: List[str] = msgspec.field(default_factory=list)\n    # The last sampled token ids for multi step decoding.\n    last_sampled_token_ids: Optional[torch.Tensor] = None\n    # Async callback\n    async_callback: Optional[Callable] = None\n    use_async_and_multi_step: bool = False\n\n    @property\n    def is_first_multi_step(self) -> bool:\n        # TODO(will) make this be able to handle batches with variable number of\n        # steps\n        assert len(self.seq_group_metadata_list) > 0\n        first_seq_group = self.seq_group_metadata_list[0]\n        assert first_seq_group.state is not None\n        return first_seq_group.state.current_step == 0\n\n    @property\n    def is_last_step(self) -> bool:\n        # TODO(will) make this be able to handle batches with variable number of\n        # steps\n        assert len(self.seq_group_metadata_list) > 0\n        first_seq_group = self.seq_group_metadata_list[0]\n        assert first_seq_group.state is not None\n        return first_seq_group.state.remaining_steps == 1\n\n    @property\n    def current_step(self) -> int:\n        # TODO(will) make this be able to handle batches with variable number of\n        # steps\n        assert len(self.seq_group_metadata_list) > 0\n        state = self.seq_group_metadata_list[0].state\n        assert state is not None\n        return state.current_step\n\n    def clone(\n        self, seq_group_metadata_list: List[Union[SequenceGroupMetadata,\n                                                  SequenceGroupMetadataDelta]]\n    ) -> \"ExecuteModelRequest\":\n        \"\"\"Clone the request with a new sequence group metadata list.\"\"\"\n        return ExecuteModelRequest(\n            seq_group_metadata_list=seq_group_metadata_list,\n            blocks_to_swap_in=self.blocks_to_swap_in.copy(),\n            blocks_to_swap_out=self.blocks_to_swap_out.copy(),\n            blocks_to_copy=self.blocks_to_copy.copy(),\n            virtual_engine=self.virtual_engine,\n            num_lookahead_slots=self.num_lookahead_slots,\n            running_queue_size=self.running_queue_size,\n            previous_hidden_states=self.previous_hidden_states,\n            num_steps=self.num_steps,\n            finished_requests_ids=self.finished_requests_ids,\n            last_sampled_token_ids=self.last_sampled_token_ids.clone()\n            if self.last_sampled_token_ids is not None else None,\n            async_callback=self.async_callback,\n            use_async_and_multi_step=self.use_async_and_multi_step)\n",
      "diff": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 87b3d21fa..a5ebf152c 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1225,7 +1225,6 @@ class ExecuteModelRequest(\n     last_sampled_token_ids: Optional[torch.Tensor] = None\n     # Async callback\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n \n     @property\n     def is_first_multi_step(self) -> bool:\n@@ -1272,5 +1271,4 @@ class ExecuteModelRequest(\n             finished_requests_ids=self.finished_requests_ids,\n             last_sampled_token_ids=self.last_sampled_token_ids.clone()\n             if self.last_sampled_token_ids is not None else None,\n-            async_callback=self.async_callback,\n-            use_async_and_multi_step=self.use_async_and_multi_step)\n+            async_callback=self.async_callback)",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import dataclasses\nimport gc\nimport inspect\nimport itertools\nimport time\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set,\n                    Tuple, Type, TypeVar, Union)\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.attention.backends.abstract import AttentionState\nfrom vllm.attention.backends.utils import CommonAttentionState\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.distributed import get_pp_group\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata, SamplingMetadataCache\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.model_executor.models.interfaces import (supports_lora,\n                                                   supports_multimodal)\nfrom vllm.model_executor.models.utils import set_cpu_offload_max_bytes\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,\n                             MultiModalInputs, MultiModalRegistry)\nfrom vllm.prompt_adapter.layers import PromptAdapterMapping\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.prompt_adapter.worker_manager import (\n    LRUCacheWorkerPromptAdapterManager)\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import IntermediateTensors, SequenceGroupMetadata\nfrom vllm.utils import (CudaMemoryProfiler, PyObjectCache, async_tensor_h2d,\n                        flatten_2d_lists, is_hip, is_pin_memory_available,\n                        supports_dynamo)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# all the token sizes that **can** be captured by cudagraph.\n# they can be arbitrarily large.\n# currently it includes: 1, 2, 4, 8, 16, 24, 32, 40, ..., 8192.\n# the actual sizes to capture will be determined by the model,\n# depending on the model's max_num_seqs.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 1025)\n]\n_NUM_WARMUP_ITERS = 2\n\nTModelInputForGPU = TypeVar('TModelInputForGPU', bound=\"ModelInputForGPU\")\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPU(ModelRunnerInputBase):\n    \"\"\"\n    This base class contains metadata needed for the base model forward pass\n    but not metadata for possible additional steps, e.g., sampling. Model\n    runners that run additional steps should subclass this method to add\n    additional fields.\n    \"\"\"\n    input_tokens: Optional[torch.Tensor] = None\n    input_positions: Optional[torch.Tensor] = None\n    seq_lens: Optional[List[int]] = None\n    query_lens: Optional[List[int]] = None\n    lora_mapping: Optional[\"LoRAMapping\"] = None\n    lora_requests: Optional[Set[LoRARequest]] = None\n    attn_metadata: Optional[\"AttentionMetadata\"] = None\n    prompt_adapter_mapping: Optional[PromptAdapterMapping] = None\n    prompt_adapter_requests: Optional[Set[PromptAdapterRequest]] = None\n    multi_modal_kwargs: Optional[BatchedTensorInputs] = None\n    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None\n    finished_requests_ids: Optional[List[str]] = None\n    virtual_engine: int = 0\n    async_callback: Optional[Callable] = None\n    use_async_and_multi_step: bool = False\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[TModelInputForGPU],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> TModelInputForGPU:\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):\n    \"\"\"\n    Used by the ModelRunner.\n    \"\"\"\n    sampling_metadata: Optional[\"SamplingMetadata\"] = None\n    # Used for speculative decoding. We do not broadcast it because it is only\n    # used by the driver worker.\n    is_prompt: Optional[bool] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForGPUWithSamplingMetadata\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n    \"\"\"Build ModelInputForGPU from SequenceGroupMetadata.\"\"\"\n\n    # Note: ideally we would be using a dataclass(kw_only=True)\n    # here, so that this can be subclassed easily,\n    # but kw_only is not supported in python<3.10.\n    class InterDataForSeqGroup:\n        \"\"\"Intermediate data for the current sequence group.\"\"\"\n\n        def simple_reinit(self):\n            self.input_tokens[0].clear()  # type: ignore\n            self.input_positions[0].clear()  # type: ignore\n            self.seq_lens[0] = 0  # type: ignore\n            self.orig_seq_lens[0] = 0  # type: ignore\n            self.query_lens[0] = 0  # type: ignore\n            self.context_lens[0] = 0  # type: ignore\n            self.curr_sliding_window_blocks[0] = 0  # type: ignore\n            self.lora_index_mapping.clear()  # type: ignore\n            self.lora_prompt_mapping.clear()  # type: ignore\n            self.lora_requests.clear()  # type: ignore\n            self.prompt_adapter_index_mapping.clear()  # type: ignore\n            self.prompt_adapter_prompt_mapping.clear()  # type: ignore\n\n        def __init__(\n            self,\n            *,\n            # From sequence group metadata.\n            request_id: str,\n            seq_ids: List[int],\n            is_prompt: bool,\n            block_tables: Optional[Dict[int, List[int]]],\n            computed_block_nums: List[int],\n            n_seqs: int = 0,\n\n            # Input tokens and positions.\n            input_tokens: Optional[List[List[int]]] = None,\n            input_positions: Optional[List[List[int]]] = None,\n\n            # The sequence length (may be capped to the sliding window).\n            seq_lens: Optional[List[int]] = None,\n            # The original sequence length (before applying sliding window).\n            # This is used to compute slot mapping.\n            orig_seq_lens: Optional[List[int]] = None,\n            # The query length.\n            query_lens: Optional[List[int]] = None,\n            # The number of tokens that are already computed.\n            context_lens: Optional[List[int]] = None,\n            # The current sliding window block.\n            curr_sliding_window_blocks: Optional[List[int]] = None,\n\n            # LoRA inputs.\n            lora_index_mapping: Optional[List[List[int]]] = None,\n            lora_prompt_mapping: Optional[List[List[int]]] = None,\n            lora_requests: Optional[Set[LoRARequest]] = None,\n\n            # Prompt adapter inputs.\n            prompt_adapter_index_mapping: Optional[List[int]] = None,\n            prompt_adapter_prompt_mapping: Optional[List[int]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n\n            # Multi-modal inputs.\n            multi_modal_inputs: Optional[MultiModalInputs] = None,\n\n            # Whether the prefix cache is hit (prefill only).\n            prefix_cache_hit: bool = False,\n            reinit: bool = False,\n            reinit_use_defaults: bool = False,\n        ):\n            if reinit:\n                assert len(self.seq_ids) == len(seq_ids)  # type: ignore\n                for i, seq_id in enumerate(seq_ids):\n                    self.seq_ids[i] = seq_id  # type: ignore\n            else:\n                self.seq_ids = seq_ids\n\n            self.request_id = request_id\n            self.is_prompt = is_prompt\n            self.block_tables = block_tables\n            self.computed_block_nums = computed_block_nums\n            self.n_seqs = n_seqs\n\n            if reinit:\n                if len(self.seq_ids) == 1 and reinit_use_defaults:\n                    self.simple_reinit()\n                else:\n                    if input_tokens:\n                        self.input_tokens = input_tokens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_tokens[seq_id].clear()\n\n                    if input_positions:\n                        self.input_positions = input_positions\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_positions[seq_id].clear()\n\n                    if seq_lens:\n                        self.seq_lens = seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.seq_lens[seq_id] = 0\n\n                    if orig_seq_lens:\n                        self.orig_seq_lens = orig_seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.orig_seq_lens[seq_id] = 0\n\n                    if query_lens:\n                        self.query_lens = query_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.query_lens[seq_id] = 0\n\n                    if context_lens:\n                        self.context_lens = context_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.context_lens[seq_id] = 0\n\n                    if curr_sliding_window_blocks:\n                        self.curr_sliding_window_blocks = \\\n                            curr_sliding_window_blocks\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.curr_sliding_window_blocks[seq_id] = 0\n\n                    if lora_index_mapping:\n                        self.lora_index_mapping = lora_index_mapping\n                    else:\n                        self.lora_index_mapping.clear()\n\n                    if lora_prompt_mapping:\n                        self.lora_prompt_mapping = lora_prompt_mapping\n                    else:\n                        self.lora_prompt_mapping.clear()\n\n                    if lora_requests:\n                        self.lora_requests = lora_requests\n                    else:\n                        self.lora_requests.clear()\n\n                    if prompt_adapter_index_mapping:\n                        self.prompt_adapter_index_mapping = \\\n                            prompt_adapter_index_mapping\n                    else:\n                        self.prompt_adapter_index_mapping.clear()\n\n                    if prompt_adapter_prompt_mapping:\n                        self.prompt_adapter_prompt_mapping = \\\n                            prompt_adapter_prompt_mapping\n                    else:\n                        self.prompt_adapter_prompt_mapping.clear()\n\n            else:\n                self.input_tokens = input_tokens or []\n                self.input_positions = input_positions or []\n                self.seq_lens = seq_lens or []\n                self.orig_seq_lens = orig_seq_lens or []\n                self.query_lens = query_lens or []\n                self.context_lens = context_lens or []\n                self.curr_sliding_window_blocks = \\\n                    curr_sliding_window_blocks or []\n\n                self.lora_index_mapping = lora_index_mapping or []\n                self.lora_prompt_mapping = lora_prompt_mapping or []\n                self.lora_requests = lora_requests or set()\n\n                self.prompt_adapter_index_mapping = (\n                    prompt_adapter_index_mapping or [])\n                self.prompt_adapter_prompt_mapping = (\n                    prompt_adapter_prompt_mapping or [])\n\n            self.prompt_adapter_request = prompt_adapter_request\n            self.multi_modal_inputs = multi_modal_inputs\n            self.prefix_cache_hit = prefix_cache_hit\n\n            self.n_seqs = len(self.seq_ids)\n\n            if not reinit:\n                self.__post_init__()\n\n        def __post_init__(self):\n            self.n_seqs = len(self.seq_ids)\n\n            self.input_tokens = [[] for _ in range(self.n_seqs)]\n            self.input_positions = [[] for _ in range(self.n_seqs)]\n            self.seq_lens = [0] * self.n_seqs\n            self.orig_seq_lens = [0] * self.n_seqs\n            self.query_lens = [0] * self.n_seqs\n            self.context_lens = [0] * self.n_seqs\n            self.curr_sliding_window_blocks = [0] * self.n_seqs\n\n            self.lora_index_mapping = []\n            self.lora_prompt_mapping = []\n\n    def gen_inter_data_builder(self, num_seqs: int):\n        return lambda: ModelInputForGPUBuilder.InterDataForSeqGroup(\n            request_id=\"\",\n            seq_ids=[0] * num_seqs,\n            is_prompt=True,\n            block_tables=None,\n            computed_block_nums=[])\n\n    def init_cached_inter_data(self, *args, **kwargs):\n        assert len(args) == 0\n        assert \"seq_ids\" in kwargs\n        seq_ids = kwargs[\"seq_ids\"]\n        num_seqs = len(seq_ids)\n\n        # The inter-data cache is per model_runner\n        inter_data_cache = self.runner.inter_data_cache\n        if num_seqs not in inter_data_cache:\n            inter_data_cache[num_seqs] = PyObjectCache(\n                self.gen_inter_data_builder(num_seqs))\n\n        obj = inter_data_cache[num_seqs].get_object()\n        obj.__init__(*args, **kwargs)\n        return obj\n\n    def reset_cached_inter_data(self):\n        for cache in self.runner.inter_data_cache.values():\n            cache.reset()\n\n    def __init__(self,\n                 runner: \"GPUModelRunnerBase\",\n                 finished_requests_ids: Optional[List[str]] = None):\n        super().__init__()\n        # Compute functions for each sequence in a sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_compute_fns = [\n            self._compute_lens,\n            self._compute_for_prefix_cache_hit,\n            self._compute_for_sliding_window,\n            self._compute_lora_input,\n        ]\n        # Compute functions for each sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_group_compute_fns = [\n            self._compute_prompt_adapter_input,\n            self._compute_multi_modal_input,\n        ]\n\n        self.runner = runner\n        self.model_input_cls = self.runner._model_input_cls\n        self.attn_backend = self.runner.attn_backend\n        self.scheduler_config = self.runner.scheduler_config\n        self.sliding_window = self.runner.sliding_window\n        self.block_size = self.runner.block_size\n        self.enable_lora = self.runner.lora_config is not None\n        self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                      is not None)\n        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n        self.finished_requests_ids = finished_requests_ids\n        self.decode_only = True\n\n        # Intermediate data (data in CPU before going to GPU) for\n        # the current sequence group.\n        self.inter_data_list: List[\n            ModelInputForGPUBuilder.InterDataForSeqGroup] = []\n\n        # Attention metadata inputs.\n        self.attn_metadata_builder = self.attn_backend.make_metadata_builder(\n            weakref.proxy(self))\n\n        # Engine/Model configurations.\n        self.chunked_prefill_enabled = (\n            self.scheduler_config is not None\n            and self.scheduler_config.chunked_prefill_enabled)\n        if self.sliding_window is not None:\n            self.sliding_window_blocks = (\n                self.sliding_window + self.block_size - 1) // self.block_size\n            self.block_aligned_sliding_window = \\\n                self.sliding_window_blocks * self.block_size\n\n    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,\n                      seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Compute context length, sequence length and tokens\n        for the given sequence data.\n        \"\"\"\n        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]\n        token_chunk_size = seq_group_metadata.token_chunk_size\n\n        # Compute context length (the number of tokens that are\n        # already computed) and sequence length (total number of tokens).\n        seq_len = seq_data.get_len()\n        if inter_data.is_prompt:\n            context_len = seq_data.get_num_computed_tokens()\n        else:\n            # get_num_computed_tokens is incorrect for spec decoding.\n            # So, we should have a special logic here.\n            # TODO(sang): Fix it.\n            context_len = seq_len - 1\n        seq_len = min(seq_len, context_len + token_chunk_size)\n\n        # Compute tokens.\n        if inter_data.is_prompt:\n            tokens = seq_data.get_token_ids()\n            if context_len != 0 or seq_len < len(tokens):\n                tokens = tokens[context_len:seq_len]\n        else:\n            # Optimization. get_token_ids requires the entire copy of\n            # tokens.\n            tokens = seq_data.get_last_token_id()\n\n        inter_data.seq_lens[seq_idx] = seq_len\n        inter_data.orig_seq_lens[seq_idx] = seq_len\n        inter_data.context_lens[seq_idx] = context_len\n\n        if isinstance(tokens, list):\n            inter_data.input_tokens[seq_idx].extend(tokens)\n        else:\n            inter_data.input_tokens[seq_idx].append(tokens)\n\n        if (seq_len - context_len) == 1:\n            inter_data.input_positions[seq_idx].append(seq_len - 1)\n        else:\n            inter_data.input_positions[seq_idx].extend(\n                range(context_len, seq_len))\n\n        inter_data.query_lens[\n            seq_idx] = seq_len - context_len if inter_data.is_prompt else 1\n\n    def _compute_for_prefix_cache_hit(\n            self, inter_data: InterDataForSeqGroup, seq_idx: int,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Check if hit prefix cache (i.e., some blocks are already computed).\n        If hit, update input tokens and positions to only compute the\n        remaining blocks.\n        \"\"\"\n        computed_block_nums = inter_data.computed_block_nums\n\n        # Note that prefix caching does not support sliding window.\n        prefix_cache_hit = (computed_block_nums is not None\n                            and len(computed_block_nums) > 0\n                            and self.sliding_window is None\n                            and inter_data.is_prompt)\n        inter_data.prefix_cache_hit = prefix_cache_hit\n\n        if not prefix_cache_hit:\n            return\n\n        assert computed_block_nums is not None\n        # The cache hit prompt tokens in this sequence. Note that\n        # this may be larger than the sequence length if chunked\n        # prefill is enabled.\n        prefix_cache_len = len(computed_block_nums) * self.block_size\n        # The number of so far computed prompt tokens in this sequence.\n        context_len = inter_data.context_lens[seq_idx]\n        # The total number of prompt tokens in this sequence.\n        # When chunked prefill is enabled, this is the token number of\n        # computed chunks + current chunk.\n        seq_len = inter_data.seq_lens[seq_idx]\n        if prefix_cache_len <= context_len:\n            # We already passed the cache hit region,\n            # so do normal computation.\n            pass\n        elif context_len < prefix_cache_len < seq_len:\n            # Partial hit. Compute the missing part.\n            uncomputed_start = prefix_cache_len - context_len\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][uncomputed_start:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][uncomputed_start:]\n            context_len = prefix_cache_len\n\n            inter_data.context_lens[seq_idx] = context_len\n            inter_data.query_lens[\n                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n        elif seq_len <= prefix_cache_len:\n            # Full hit. Only compute the last token to avoid\n            # erroneous behavior. FIXME: Ideally we should directly\n            # mark all tokens as computed in the scheduler and do not\n            # schedule this sequence, so this case should not happen.\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][-1:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][-1:]\n            inter_data.query_lens[seq_idx] = 1\n            inter_data.context_lens[seq_idx] = inter_data.seq_lens[seq_idx] - 1\n\n    def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                    seq_idx: int,\n                                    seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Update seq_len and curr_sliding_window_block for the given\n        sequence data (only required by decoding) if sliding window is enabled.\n        \"\"\"\n        curr_sliding_window_block = 0\n        sliding_seq_len = inter_data.seq_lens[seq_idx]\n        if not inter_data.is_prompt and self.sliding_window is not None:\n            # TODO(sang): This is a hack to make sliding window work with\n            # paged attn. We can remove it if we make paged attn kernel\n            # to properly handle slinding window attn.\n            curr_sliding_window_block = self.sliding_window_blocks\n            if self.scheduler_config.use_v2_block_manager:\n                # number of elements in last block\n                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n                sliding_seq_len = min(\n                    inter_data.seq_lens[seq_idx],\n                    self.block_aligned_sliding_window + suff_len)\n                if suff_len > 0:\n                    curr_sliding_window_block += 1\n            else:\n                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n                                      self.sliding_window)\n\n        inter_data.curr_sliding_window_blocks[\n            seq_idx] = curr_sliding_window_block\n        inter_data.seq_lens[seq_idx] = sliding_seq_len\n\n    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,\n                            seq_idx: int,\n                            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If LoRA is enabled, compute LoRA index and prompt mapping.\"\"\"\n        if not self.enable_lora:\n            return\n\n        lora_id = seq_group_metadata.lora_int_id\n        if lora_id > 0:\n            inter_data.lora_requests.add(seq_group_metadata.lora_request)\n        query_len = inter_data.query_lens[seq_idx]\n        inter_data.lora_index_mapping.append([lora_id] * query_len)\n        inter_data.lora_prompt_mapping.append(\n            [lora_id] *\n            (query_len if seq_group_metadata.sampling_params\n             and seq_group_metadata.sampling_params.prompt_logprobs is not None\n             else 1))\n\n    def _compute_prompt_adapter_input(\n            self, inter_data: InterDataForSeqGroup,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If prompt adapter is enabled, compute index and prompt mapping.\n        \"\"\"\n        # Note that when is_prompt=True, we expect only one sequence\n        # in the group.\n        if not self.enable_prompt_adapter:\n            return\n\n        prompt_adapter_id = seq_group_metadata.prompt_adapter_id\n        if prompt_adapter_id <= 0 or not inter_data.is_prompt:\n            return\n\n        # We expect only one sequence in the group when is_prompt=True.\n        assert inter_data.n_seqs == 1\n        query_len = inter_data.query_lens[0]\n        inter_data.prompt_adapter_request = (\n            seq_group_metadata.prompt_adapter_request)\n\n        num_tokens = seq_group_metadata.prompt_adapter_num_virtual_tokens\n        inter_data.prompt_adapter_index_mapping = [\n            prompt_adapter_id\n        ] * num_tokens + [0] * (query_len - num_tokens)\n        inter_data.prompt_adapter_prompt_mapping = [prompt_adapter_id] * (\n            query_len if seq_group_metadata.sampling_params\n            and seq_group_metadata.sampling_params.prompt_logprobs else 1)\n\n    def _compute_multi_modal_input(self, inter_data: InterDataForSeqGroup,\n                                   seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If multi-modal data is given, add it to the input.\"\"\"\n        mm_data = seq_group_metadata.multi_modal_data\n        if not mm_data:\n            return\n\n        mm_kwargs = self.multi_modal_input_mapper(mm_data)\n        inter_data.multi_modal_inputs = mm_kwargs\n\n    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Add a sequence group to the builder.\"\"\"\n        seq_ids = seq_group_metadata.seq_data.keys()\n        n_seqs = len(seq_ids)\n        is_prompt = seq_group_metadata.is_prompt\n\n        if is_prompt:\n            assert n_seqs == 1\n            self.decode_only = False\n\n        inter_data = self.init_cached_inter_data(\n            request_id=seq_group_metadata.request_id,\n            seq_ids=seq_ids,\n            is_prompt=is_prompt,\n            block_tables=seq_group_metadata.block_tables,\n            computed_block_nums=seq_group_metadata.computed_block_nums,\n            reinit=True,\n            reinit_use_defaults=True)\n\n        self.inter_data_list.append(inter_data)\n\n        for seq_idx in range(n_seqs):\n            for per_seq_fn in self.per_seq_compute_fns:\n                per_seq_fn(inter_data, seq_idx, seq_group_metadata)\n        for per_seq_group_fn in self.per_seq_group_compute_fns:\n            per_seq_group_fn(inter_data, seq_group_metadata)\n\n    def _use_captured_graph(self, batch_size: int,\n                            max_decode_seq_len: int) -> bool:\n        return (self.decode_only and not self.runner.model_config.enforce_eager\n                and batch_size <= self.runner.max_batchsize_to_capture\n                and max_decode_seq_len <= self.runner.max_seq_len_to_capture)\n\n    def build(self) -> ModelInputForGPU:\n        \"\"\"Finalize the builder intermediate data and\n        create on-device tensors.\n        \"\"\"\n        # Combine and flatten intermediate data.\n        input_tokens = []\n        for inter_data in self.inter_data_list:\n            for cur_input_tokens in inter_data.input_tokens:\n                input_tokens.extend(cur_input_tokens)\n\n        if not input_tokens:\n            # This may happen when all prefill requests hit\n            # prefix caching and there is no decode request.\n            return self.model_input_cls()\n\n        input_positions = []\n        for inter_data in self.inter_data_list:\n            for cur_input_positions in inter_data.input_positions:\n                input_positions.extend(cur_input_positions)\n\n        seq_lens = []\n        max_decode_seq_len = 0\n        for inter_data in self.inter_data_list:\n            seq_lens.extend(inter_data.seq_lens)\n            if not inter_data.is_prompt:\n                max_decode_seq_len = max(max_decode_seq_len,\n                                         max(inter_data.seq_lens))\n        query_lens = []\n        for inter_data in self.inter_data_list:\n            query_lens.extend(inter_data.query_lens)\n\n        # Mapping from request IDs to sequence IDs. Used for Jamba models\n        # that manages the cache by itself.\n        request_ids_to_seq_ids = {\n            data.request_id: data.seq_ids\n            for data in self.inter_data_list\n        }\n\n        batch_size = len(input_tokens)\n        use_captured_graph = self._use_captured_graph(batch_size,\n                                                      max_decode_seq_len)\n\n        # If cuda graph can be used, pad tensors accordingly.\n        # See `capture_model` API for more details.\n        # vLLM uses cuda graph only for decoding requests.\n        cuda_graph_pad_size = -1\n        if use_captured_graph:\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            cuda_graph_pad_size = graph_batch_size - batch_size\n            batch_size = graph_batch_size\n\n        # Tokens and positions.\n        if cuda_graph_pad_size:\n            input_tokens.extend(itertools.repeat(0, cuda_graph_pad_size))\n            input_positions.extend(itertools.repeat(0, cuda_graph_pad_size))\n        assert self.runner.device is not None\n        input_tokens_tensor = async_tensor_h2d(input_tokens, torch.long,\n                                               self.runner.device,\n                                               self.runner.pin_memory)\n        input_positions_tensor = async_tensor_h2d(input_positions, torch.long,\n                                                  self.runner.device,\n                                                  self.runner.pin_memory)\n\n        # Sequence and query lengths.\n        if cuda_graph_pad_size:\n            seq_lens.extend(itertools.repeat(1, cuda_graph_pad_size))\n\n        # Attention metadata.\n        attn_metadata = self.attn_metadata_builder.build(\n            seq_lens, query_lens, cuda_graph_pad_size, batch_size)\n\n        # LoRA data.\n        lora_requests = set()\n        lora_mapping = None\n        if self.enable_lora:\n            lora_requests = set(r for data in self.inter_data_list\n                                for r in data.lora_requests)\n            lora_index_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_index_mapping)\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                lora_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            lora_prompt_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_prompt_mapping)\n                for inter_data in self.inter_data_list\n            ])\n\n            lora_mapping = LoRAMapping(\n                **dict(index_mapping=lora_index_mapping,\n                       prompt_mapping=lora_prompt_mapping,\n                       is_prefill=not self.decode_only))\n\n        # Prompt adapter data.\n        prompt_adapter_requests: Set[PromptAdapterRequest] = set()\n        prompt_adapter_mapping = None\n        if self.enable_prompt_adapter:\n            prompt_adapter_requests = set(\n                data.prompt_adapter_request for data in self.inter_data_list\n                if data.prompt_adapter_request is not None)\n            prompt_adapter_index_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_index_mapping\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                prompt_adapter_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            prompt_adapter_prompt_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_prompt_mapping\n                for inter_data in self.inter_data_list\n            ])\n            prompt_adapter_mapping = PromptAdapterMapping(\n                prompt_adapter_index_mapping,\n                prompt_adapter_prompt_mapping,\n            )\n\n        # Multi-modal data.\n        multi_modal_inputs_list = [\n            data.multi_modal_inputs for data in self.inter_data_list\n            if data.multi_modal_inputs is not None\n        ]\n        multi_modal_kwargs = MultiModalInputs.batch(multi_modal_inputs_list)\n\n        return self.model_input_cls(\n            input_tokens=input_tokens_tensor,\n            input_positions=input_positions_tensor,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_mapping=lora_mapping,\n            lora_requests=lora_requests,\n            multi_modal_kwargs=multi_modal_kwargs,\n            request_ids_to_seq_ids=request_ids_to_seq_ids,\n            finished_requests_ids=self.finished_requests_ids,\n            prompt_adapter_mapping=prompt_adapter_mapping,\n            prompt_adapter_requests=prompt_adapter_requests)\n\n\nclass GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n    \"\"\"\n    Helper class for shared methods between GPU model runners.\n    \"\"\"\n    _model_input_cls: Type[TModelInputForGPU]\n    _builder_cls: Type[ModelInputForGPUBuilder]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        return_hidden_states: bool = False,\n        observability_config: Optional[ObservabilityConfig] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        self.prompt_adapter_config = prompt_adapter_config\n        self.return_hidden_states = return_hidden_states\n        self.observability_config = observability_config\n\n        self.device = self.device_config.device\n        self.pin_memory = is_pin_memory_available()\n\n        self.kv_cache_dtype = kv_cache_dtype\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_seq_len_to_capture = self.model_config.max_seq_len_to_capture\n        self.max_batchsize_to_capture = _get_max_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n\n        self.graph_runners: List[Dict[int, CUDAGraphRunner]] = [\n            {} for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n\n        self.has_seqlen_agnostic = model_config.contains_seqlen_agnostic_layers(\n            parallel_config)\n\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = np.zeros(\n            (self.max_batchsize_to_capture, self.get_max_block_per_batch()),\n            dtype=np.int32)\n        num_attn_heads = self.model_config.get_num_attention_heads(\n            self.parallel_config)\n        self.attn_backend = get_attn_backend(\n            num_attn_heads,\n            self.model_config.get_head_size(),\n            self.model_config.get_num_kv_heads(self.parallel_config),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.kv_cache_dtype,\n            self.block_size,\n        ) if num_attn_heads else None\n        if self.attn_backend:\n            self.attn_state = self.attn_backend.get_state_cls()(\n                weakref.proxy(self))\n        else:\n            self.attn_state = CommonAttentionState(weakref.proxy(self))\n\n        # Multi-modal data support\n        self.input_registry = input_registry\n        self.mm_registry = mm_registry\n        self.multi_modal_input_mapper = mm_registry \\\n            .create_input_mapper(model_config)\n        self.mm_registry.init_mm_limits_per_prompt(self.model_config)\n\n        # Lazy initialization\n        self.model: nn.Module  # Set after load_model\n        # Set after load_model.\n        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None\n        self.prompt_adapter_manager: LRUCacheWorkerPromptAdapterManager = None\n\n        set_cpu_offload_max_bytes(\n            int(self.cache_config.cpu_offload_gb * 1024**3))\n\n        # Used to cache python objects\n        self.inter_data_cache: Dict[int, PyObjectCache] = {}\n        self.sampling_metadata_cache: SamplingMetadataCache = \\\n            SamplingMetadataCache()\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with CudaMemoryProfiler() as m:\n            self.model = get_model(model_config=self.model_config,\n                                   device_config=self.device_config,\n                                   load_config=self.load_config,\n                                   lora_config=self.lora_config,\n                                   parallel_config=self.parallel_config,\n                                   scheduler_config=self.scheduler_config,\n                                   cache_config=self.cache_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert supports_lora(self.model), \"Model does not support LoRA\"\n            assert not supports_multimodal(\n                self.model\n            ), \"To be tested: Multi-modal model with LoRA settings.\"\n\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens,\n                self.vocab_size,\n                self.lora_config,\n                self.device,\n                self.model.embedding_modules,\n                self.model.embedding_padding_modules,\n                max_position_embeddings=self.model.config.\n                max_position_embeddings,\n            )\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.prompt_adapter_config:\n            self.prompt_adapter_manager = LRUCacheWorkerPromptAdapterManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens, self.device,\n                self.prompt_adapter_config)\n            self.model = (\n                self.prompt_adapter_manager.create_prompt_adapter_manager(\n                    self.model))\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently only ROCm accepts kv-cache scaling factors\n            # via quantization_param_path and this will be deprecated\n            # in the future.\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    warnings.warn(\n                        \"Loading kv cache scaling factor from JSON is \"\n                        \"deprecated and will be removed. Please include \"\n                        \"kv cache scaling factors in the model checkpoint.\",\n                        FutureWarning,\n                        stacklevel=2)\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                    logger.info(\"Loaded KV cache scaling factors from %s\",\n                                self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n\n        if envs.VLLM_TEST_DYNAMO_GRAPH_CAPTURE and supports_dynamo():\n            self.model = torch.compile(self.model,\n                                       fullgraph=True,\n                                       backend=\"eager\")\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import ShardedStateLoader\n        ShardedStateLoader.save_model(\n            self.model,\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def save_tensorized_model(\n        self,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import TensorizerLoader\n        TensorizerLoader.save_model(\n            self.model,\n            tensorizer_config=tensorizer_config,\n        )\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_model_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> TModelInputForGPU:\n        \"\"\"Helper method to prepare the model input based on a given sequence\n        group. Prepares metadata needed for the base model forward pass but not\n        metadata for possible additional steps, e.g., sampling.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)\n        for seq_group_metadata in seq_group_metadata_list:\n            builder.add_seq_group(seq_group_metadata)\n\n        builder.reset_cached_inter_data()\n\n        return builder.build()  # type: ignore\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests: List[LoRARequest] = []\n        dummy_lora_requests_per_seq: List[LoRARequest] = []\n        if self.lora_config:\n            assert self.lora_manager is not None\n            with self.lora_manager.dummy_lora_cache():\n                for idx in range(self.lora_config.max_loras):\n                    lora_id = idx + 1\n                    dummy_lora_request = LoRARequest(\n                        lora_name=f\"warmup_{lora_id}\",\n                        lora_int_id=lora_id,\n                        lora_path=\"/not/a/real/path\",\n                    )\n                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                     rank=LORA_WARMUP_RANK)\n                    dummy_lora_requests.append(dummy_lora_request)\n                dummy_lora_requests_per_seq = [\n                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n                    for idx in range(max_num_seqs)\n                ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for multi-modal encoding, which\n        # needs to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n\n        max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\n            self.model_config)\n        if max_mm_tokens > 0:\n            max_num_seqs_orig = max_num_seqs\n            max_num_seqs = min(max_num_seqs,\n                               max_num_batched_tokens // max_mm_tokens)\n            if max_num_seqs < 1:\n                expr = (f\"min({max_num_seqs_orig}, \"\n                        f\"{max_num_batched_tokens} // {max_mm_tokens})\")\n                logger.warning(\n                    \"Computed max_num_seqs (%s) to be less than 1. \"\n                    \"Setting it to the minimum value of 1.\", expr)\n                max_num_seqs = 1\n\n        batch_size = 0\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            batch_size += seq_len\n\n            seq_data, dummy_multi_modal_data = self.input_registry \\\n                .dummy_data_for_profiling(self.model_config,\n                                          seq_len,\n                                          self.mm_registry)\n\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=dummy_multi_modal_data,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        finished_requests_ids = [seq.request_id for seq in seqs]\n        model_input = self.prepare_model_input(\n            seqs, finished_requests_ids=finished_requests_ids)\n        intermediate_tensors = None\n        if not get_pp_group().is_first_rank:\n            intermediate_tensors = self.model.make_empty_intermediate_tensors(\n                batch_size=batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n        self.execute_model(model_input, kv_caches, intermediate_tensors)\n        torch.cuda.synchronize()\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_adapters()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_adapter(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_adapter(lora_id)\n\n    def pin_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.pin_adapter(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_adapters()\n\n    def remove_all_prompt_adapters(self):\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.remove_all_adapters()\n\n    def set_active_prompt_adapters(\n            self, prompt_adapter_requests: Set[PromptAdapterRequest],\n            prompt_adapter_mapping: PromptAdapterMapping) -> None:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.set_active_adapters(\n            prompt_adapter_requests, prompt_adapter_mapping)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.add_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.remove_adapter(prompt_adapter_id)\n\n    def pin_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.pin_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> Set[int]:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.list_adapters()\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[List[torch.Tensor]]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = self.max_batchsize_to_capture\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n\n        # Prepare dummy previous_hidden_states only if needed by the model.\n        # This is used by draft models such as EAGLE.\n        previous_hidden_states = None\n        if \"previous_hidden_states\" in inspect.signature(\n                self.model.forward).parameters:\n            previous_hidden_states = torch.empty(\n                [max_batch_size,\n                 self.model_config.get_hidden_size()],\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        intermediate_inputs = None\n        if not get_pp_group().is_first_rank:\n            intermediate_inputs = self.model.make_empty_intermediate_tensors(\n                batch_size=max_batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        # Prepare buffer for outputs. These will be reused for all batch sizes.\n        # It will be filled after the first graph capture.\n        hidden_or_intermediate_states: List[Optional[torch.Tensor]] = [\n            None\n        ] * self.parallel_config.pipeline_parallel_size\n\n        graph_batch_size = self.max_batchsize_to_capture\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        with self.attn_state.graph_capture(\n                max_batch_size), graph_capture() as graph_capture_context:\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for virtual_engine in range(\n                    self.parallel_config.pipeline_parallel_size):\n                for batch_size in reversed(batch_size_capture_list):\n                    attn_metadata = (\n                        self.attn_state.graph_capture_get_metadata_for_batch(\n                            batch_size))\n\n                    if self.lora_config:\n                        lora_mapping = LoRAMapping(\n                            **dict(index_mapping=[0] * batch_size,\n                                   prompt_mapping=[0] * batch_size,\n                                   is_prefill=False))\n                        self.set_active_loras(set(), lora_mapping)\n\n                    if self.prompt_adapter_config:\n                        prompt_adapter_mapping = PromptAdapterMapping(\n                            [-1] * batch_size,\n                            [-1] * batch_size,\n                        )\n                        self.set_active_prompt_adapters(\n                            set(), prompt_adapter_mapping)\n\n                    graph_runner = CUDAGraphRunner(\n                        self.model, self.attn_backend.get_name(),\n                        self.attn_state.graph_clone(batch_size))\n\n                    capture_inputs = {\n                        \"input_ids\":\n                        input_tokens[:batch_size],\n                        \"positions\":\n                        input_positions[:batch_size],\n                        \"hidden_or_intermediate_states\":\n                        hidden_or_intermediate_states[\n                            virtual_engine]  # type: ignore\n                        [:batch_size]\n                        if hidden_or_intermediate_states[virtual_engine]\n                        is not None else None,\n                        \"intermediate_inputs\":\n                        intermediate_inputs[:batch_size]\n                        if intermediate_inputs is not None else None,\n                        \"kv_caches\":\n                        kv_caches[virtual_engine],\n                        \"attn_metadata\":\n                        attn_metadata,\n                        \"memory_pool\":\n                        self.graph_memory_pool,\n                        \"stream\":\n                        graph_capture_context.stream\n                    }\n                    if previous_hidden_states is not None:\n                        capture_inputs[\n                            \"previous_hidden_states\"] = previous_hidden_states[:\n                                                                               batch_size]\n\n                    if self.has_seqlen_agnostic:\n                        # Only used by Mamba-based models CUDA graph atm (Jamba)\n                        capture_inputs.update({\n                            \"seqlen_agnostic_capture_inputs\":\n                            self.model.get_seqlen_agnostic_capture_inputs(\n                                batch_size)\n                        })\n                    graph_runner.capture(**capture_inputs)\n                    self.graph_memory_pool = graph_runner.graph.pool()\n                    self.graph_runners[virtual_engine][batch_size] = (\n                        graph_runner)\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n    \"\"\"\n    GPU model runner with sampling step.\n    \"\"\"\n    _model_input_cls: Type[ModelInputForGPUWithSamplingMetadata] = (\n        ModelInputForGPUWithSamplingMetadata)\n    _builder_cls: Type[ModelInputForGPUBuilder] = ModelInputForGPUBuilder\n\n    def make_model_input_from_broadcasted_tensor_dict(\n        self,\n        tensor_dict: Dict[str, Any],\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        model_input = \\\n            ModelInputForGPUWithSamplingMetadata.from_broadcasted_tensor_dict(\n                tensor_dict,\n                attn_backend=self.attn_backend,\n            )\n        return model_input\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None,\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        model_input = self._prepare_model_input_tensors(\n            seq_group_metadata_list, finished_requests_ids)\n        if get_pp_group().is_last_rank:\n            # Sampling metadata is only required for the final pp group\n            generators = self.get_generators(finished_requests_ids)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, model_input.seq_lens,\n                model_input.query_lens, self.device, self.pin_memory,\n                generators, self.sampling_metadata_cache)\n        else:\n            sampling_metadata = None\n        is_prompt = (seq_group_metadata_list[0].is_prompt\n                     if seq_group_metadata_list else None)\n        return dataclasses.replace(model_input,\n                                   sampling_metadata=sampling_metadata,\n                                   is_prompt=is_prompt,\n                                   virtual_engine=virtual_engine)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        model_input: ModelInputForGPUWithSamplingMetadata,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:\n        if num_steps > 1:\n            raise ValueError(\"num_steps > 1 is not supported in ModelRunner\")\n\n        if self.lora_config:\n            assert model_input.lora_requests is not None\n            assert model_input.lora_mapping is not None\n            self.set_active_loras(model_input.lora_requests,\n                                  model_input.lora_mapping)\n\n        if self.prompt_adapter_config:\n            assert model_input.prompt_adapter_requests is not None\n            assert model_input.prompt_adapter_mapping is not None\n            self.set_active_prompt_adapters(\n                model_input.prompt_adapter_requests,\n                model_input.prompt_adapter_mapping)\n\n        self.attn_state.begin_forward(model_input)\n\n        # Currently cuda graph is only supported by the decode phase.\n        assert model_input.attn_metadata is not None\n        prefill_meta = model_input.attn_metadata.prefill_metadata\n        decode_meta = model_input.attn_metadata.decode_metadata\n        # TODO(andoorve): We can remove this once all\n        # virtual engines share the same kv cache.\n        virtual_engine = model_input.virtual_engine\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            assert model_input.input_tokens is not None\n            graph_batch_size = model_input.input_tokens.shape[0]\n            model_executable = self.graph_runners[virtual_engine][\n                graph_batch_size]\n        else:\n            model_executable = self.model\n\n        multi_modal_kwargs = model_input.multi_modal_kwargs or {}\n        seqlen_agnostic_kwargs = {\n            \"finished_requests_ids\": model_input.finished_requests_ids,\n            \"request_ids_to_seq_ids\": model_input.request_ids_to_seq_ids,\n        } if self.has_seqlen_agnostic else {}\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_start = torch.cuda.Event(enable_timing=True)\n            model_forward_end = torch.cuda.Event(enable_timing=True)\n            model_forward_start.record()\n\n        hidden_or_intermediate_states = model_executable(\n            input_ids=model_input.input_tokens,\n            positions=model_input.input_positions,\n            kv_caches=kv_caches,\n            attn_metadata=model_input.attn_metadata,\n            intermediate_tensors=intermediate_tensors,\n            **MultiModalInputs.as_kwargs(multi_modal_kwargs,\n                                         device=self.device),\n            **seqlen_agnostic_kwargs)\n\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_end.record()\n\n        # Compute the logits in the last pipeline stage.\n        if not get_pp_group().is_last_rank:\n            if (self.is_driver_worker\n                    and hidden_or_intermediate_states is not None\n                    and isinstance(hidden_or_intermediate_states,\n                                   IntermediateTensors)\n                    and self.observability_config is not None\n                    and self.observability_config.collect_model_forward_time):\n                model_forward_end.synchronize()\n                model_forward_time = model_forward_start.elapsed_time(\n                    model_forward_end)\n                orig_model_forward_time = 0.0\n                if intermediate_tensors is not None:\n                    orig_model_forward_time = intermediate_tensors.tensors.get(\n                        \"model_forward_time\", torch.tensor(0.0)).item()\n                hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                    torch.tensor(model_forward_time + orig_model_forward_time))\n            return hidden_or_intermediate_states\n\n        logits = self.model.compute_logits(hidden_or_intermediate_states,\n                                           model_input.sampling_metadata)\n\n        if not self.is_driver_worker:\n            return []\n\n        if model_input.async_callback is not None:\n            model_input.async_callback()\n\n        # Sample the next token.\n        output: SamplerOutput = self.model.sample(\n            logits=logits,\n            sampling_metadata=model_input.sampling_metadata,\n        )\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time\n                and output is not None):\n            model_forward_end.synchronize()\n            model_forward_time = model_forward_start.elapsed_time(\n                model_forward_end)\n            orig_model_forward_time = 0.0\n            if intermediate_tensors is not None:\n                orig_model_forward_time = intermediate_tensors.tensors.get(\n                    \"model_forward_time\", torch.tensor(0.0)).item()\n            # If there are multiple workers, we are still tracking the latency\n            # from the start time of the driver worker to the end time of the\n            # driver worker. The model forward time will then end up covering\n            # the communication time as well.\n            output.model_forward_time = (orig_model_forward_time +\n                                         model_forward_time)\n\n        if self.return_hidden_states:\n            # we only need to pass hidden states of most recent token\n            assert model_input.sampling_metadata is not None\n            indices = model_input.sampling_metadata.selected_token_indices\n            if model_input.is_prompt:\n                hidden_states = hidden_or_intermediate_states.index_select(\n                    0, indices)\n                output.prefill_hidden_states = hidden_or_intermediate_states\n            elif decode_meta.use_cuda_graph:\n                hidden_states = hidden_or_intermediate_states[:len(indices)]\n            else:\n                hidden_states = hidden_or_intermediate_states\n\n            output.hidden_states = hidden_states\n\n        return [output]\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module, backend_name: str,\n                 attn_state: AttentionState):\n        self.model = model\n        self.backend_name = backend_name\n        self.attn_state = attn_state\n\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        hidden_or_intermediate_states: Optional[Union[IntermediateTensors,\n                                                      torch.Tensor]],\n        intermediate_inputs: Optional[IntermediateTensors],\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool: Optional[Tuple[int, int]],\n        stream: torch.cuda.Stream,\n        **kwargs,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        assert self._graph is None\n        # Run the model a few times without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        # Note one iteration is not enough for torch.jit.script\n        for _ in range(_NUM_WARMUP_ITERS):\n            self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            output_hidden_or_intermediate_states = self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n            if hidden_or_intermediate_states is not None:\n                if get_pp_group().is_last_rank:\n                    hidden_or_intermediate_states.copy_(\n                        output_hidden_or_intermediate_states)\n                else:\n                    for key in hidden_or_intermediate_states.tensors:\n                        hidden_or_intermediate_states[key].copy_(\n                            output_hidden_or_intermediate_states[key])\n            else:\n                hidden_or_intermediate_states = (\n                    output_hidden_or_intermediate_states)\n\n            del output_hidden_or_intermediate_states\n            # make sure `output_hidden_states` is deleted\n            # in the graph's memory pool\n            gc.collect()\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\": input_ids,\n            \"positions\": positions,\n            \"kv_caches\": kv_caches,\n            **self.attn_state.get_graph_input_buffers(attn_metadata),\n            **kwargs,\n        }\n        if intermediate_inputs is not None:\n            self.input_buffers.update(intermediate_inputs.tensors)\n        if get_pp_group().is_last_rank:\n            self.output_buffers = {\n                \"hidden_states\": hidden_or_intermediate_states\n            }\n        else:\n            self.output_buffers = hidden_or_intermediate_states\n        return hidden_or_intermediate_states\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(attn_metadata.slot_mapping,\n                                                 non_blocking=True)\n        self.attn_state.prepare_graph_input_buffers(self.input_buffers,\n                                                    attn_metadata)\n        if \"seqlen_agnostic_capture_inputs\" in self.input_buffers:\n            self.model.copy_inputs_before_cuda_graphs(self.input_buffers,\n                                                      **kwargs)\n\n        if \"previous_hidden_states\" in self.input_buffers:\n            self.input_buffers[\"previous_hidden_states\"].copy_(\n                kwargs[\"previous_hidden_states\"], non_blocking=True)\n\n        if intermediate_tensors is not None:\n            for key in intermediate_tensors.tensors:\n                if key != \"model_execute_time\" and key != \"model_forward_time\":\n                    self.input_buffers[key].copy_(intermediate_tensors[key],\n                                                  non_blocking=True)\n        # Run the graph.\n        self.graph.replay()\n        # Return the output tensor.\n        if get_pp_group().is_last_rank:\n            return self.output_buffers[\"hidden_states\"]\n\n        return self.output_buffers\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n\n\ndef _get_max_graph_batch_size(max_num_seqs: int) -> int:\n    \"\"\"\n    max_num_seqs: Maximum number of sequences in a batch.\n    _BATCH_SIZES_TO_CAPTURE: all the sizes that we want to capture.\n\n    pad the max_num_seqs if necessary by calling _get_graph_batch_size,\n    which will deal with some edge cases like 1, 2, 4.\n\n    if the padded size is in _BATCH_SIZES_TO_CAPTURE, return the padded size.\n    if not, it means the padded size is larger than the largest size in\n    _BATCH_SIZES_TO_CAPTURE, return the largest size in _BATCH_SIZES_TO_CAPTURE.\n    \"\"\"\n    padded_size = _get_graph_batch_size(max_num_seqs)\n    if padded_size in _BATCH_SIZES_TO_CAPTURE:\n        return padded_size\n    assert padded_size > _BATCH_SIZES_TO_CAPTURE[-1]\n    return _BATCH_SIZES_TO_CAPTURE[-1]\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 8a3c99a45..74f7d4e08 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -21,6 +21,7 @@ from vllm.attention.backends.utils import CommonAttentionState\n from vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                          ModelConfig, ObservabilityConfig, ParallelConfig,\n                          PromptAdapterConfig, SchedulerConfig)\n+from vllm.core.scheduler import SchedulerOutputs\n from vllm.distributed import get_pp_group\n from vllm.distributed.parallel_state import graph_capture\n from vllm.inputs import INPUT_REGISTRY, InputRegistry\n@@ -96,7 +97,8 @@ class ModelInputForGPU(ModelRunnerInputBase):\n     finished_requests_ids: Optional[List[str]] = None\n     virtual_engine: int = 0\n     async_callback: Optional[Callable] = None\n-    use_async_and_multi_step: bool = False\n+    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n+    scheduler_outputs: Optional[SchedulerOutputs] = None\n \n     def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n         tensor_dict = {",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/worker/multi_step_model_runner.py",
      "old_content": "import dataclasses\nimport functools\nfrom dataclasses import dataclass, field\nfrom typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple,\n                    Union)\n\ntry:\n    from vllm.attention.backends.flash_attn import FlashAttentionMetadata\nexcept ModuleNotFoundError:\n    # vllm_flash_attn is not installed, use the identical ROCm FA metadata\n    from vllm.attention.backends.rocm_flash_attn import (\n        ROCmFlashAttentionMetadata as FlashAttentionMetadata)\n\nimport torch\n\nfrom vllm import _custom_ops as ops\nfrom vllm.distributed import get_pp_group\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                SamplerOutput,\n                                                SamplingMetadata, get_logprobs,\n                                                get_pythonized_sample_results)\nfrom vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                           Logprob, SequenceGroupMetadata, SequenceOutput)\nfrom vllm.worker.model_runner import (GPUModelRunnerBase,\n                                      ModelInputForGPUWithSamplingMetadata)\nfrom vllm.worker.model_runner_base import (\n    BroadcastableModelInput, _init_attn_metadata_from_tensor_dict,\n    _init_frozen_model_input_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict)\n\nfrom ..model_executor.model_loader.tensorizer import TensorizerConfig\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\n\n@dataclass\nclass ModelOutput:\n    \"\"\"The output of a single model forward pass.\n\n    The sampler_output_ready_event is set when the tensors in\n    sampler_output are ready (the model+sampler forward pass has\n    completed). We use the event to synchronize the GPU->CPU transfer,\n    which we want to only run when the data has been written to the\n    GPU tensors. Until the event is ready, the tensors in sampler_output\n    will have garbage data.\n\n    There are two scenarios:\n    1. The output tensors are ready and we can pythonize them immediately.\n    2. The output tensors are not ready and we need to wait for the event to be\n    ready.\n    \"\"\"\n    sampler_output: SamplerOutput\n    sampler_output_ready_event: torch.cuda.Event\n    sampled_token_ids: Optional[torch.Tensor] = None\n    pythonized: bool = False\n    # On-device tensor containing the logprobs of each token.\n    logprobs: Optional[\"torch.Tensor\"] = None\n\n    def pythonize(self, input_metadata: \"StatefulModelInput\",\n                  copy_stream: torch.cuda.Stream,\n                  pinned_sampled_token_buffer: torch.Tensor) -> None:\n        \"\"\"Pythonize the output. Blocking.\"\"\"\n        if not self.pythonized:\n            self._pythonize_sampler_output(input_metadata, copy_stream,\n                                           pinned_sampled_token_buffer, True)\n            self.pythonized = True\n\n    def maybe_pythonize(self, input_metadata: \"StatefulModelInput\",\n                        copy_stream: torch.cuda.Stream,\n                        pinned_sampled_token_buffer: torch.Tensor) -> None:\n        \"\"\"Pythonize the output if ready, else return None. Non-blocking.\"\"\"\n        if not self.pythonized:\n            self.pythonized = self._pythonize_sampler_output(\n                input_metadata, copy_stream, pinned_sampled_token_buffer,\n                False)\n\n    def _pythonize_sampler_output(self, input_metadata: \"StatefulModelInput\",\n                                  copy_stream: torch.cuda.Stream,\n                                  pinned_sampled_token_buffer: torch.Tensor,\n                                  blocking: bool) -> bool:\n        \"\"\"\n        If blocking is set, will block until the forward pass for the output is\n        ready and pythonize the output. Upon completing Pythonization, erases\n        self.logprobs (note that a non-blocking call that is performed when\n        the sampler output is not yet ready, will not erase self.logprobs.)\n        \"\"\"\n        assert self.sampled_token_ids is not None\n        if not blocking and not self.sampler_output_ready_event.query():\n            return False\n\n        if blocking:\n            self.sampler_output_ready_event.synchronize()\n        with torch.cuda.stream(copy_stream):\n            _pythonize_sampler_output(input_metadata, self.sampler_output,\n                                      pinned_sampled_token_buffer,\n                                      self.sampled_token_ids, self.logprobs)\n\n        # Erase the logprobs GPU-side tensor.\n        # Note that although _pythonize_sampler_output() runs in its\n        # own CUDA stream, nonetheless _pythonize_sampler_output()\n        # cannot return until Pythonization is complete; therefore\n        # we know that by the time the CPU reaches this point,\n        # `self.logprobs` is no longer needed.\n        self.logprobs = None\n        return True\n\n\n@dataclass(frozen=False)\nclass StatefulModelInput(BroadcastableModelInput):\n    # actual frozen model input dataclass passed to _base_model_runner\n    frozen_model_input: Optional[ModelInputForGPUWithSamplingMetadata] = None\n\n    # list of model outputs for each step, may not be all pythonized\n    cached_outputs: List[ModelOutput] = field(default_factory=list)\n\n    # used to pass sampled token ids from the last step to the current step for\n    # TP workers. Used to append to end of outputs and used by advance_step\n    last_sampled_token_ids: Optional[torch.Tensor] = None\n    current_step: int = 0\n    is_multi_step: bool = True\n    is_last_step: bool = False\n    is_first_multi_step: bool = False\n    # ping-pong data structures for multi-step to wait on the previous step\n    step_cuda_events: List[torch.cuda.Event] = field(\n        default_factory=lambda: [torch.cuda.Event(blocking=True)] * 2)\n    num_seqs: int = -1\n    num_queries: int = -1\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        assert self.frozen_model_input is not None\n        tensor_dict = self.frozen_model_input.as_broadcastable_tensor_dict()\n        new_tensor_dict = {\n            'last_sampled_token_ids': self.last_sampled_token_ids,\n            'current_step': self.current_step,\n            'is_multi_step': self.is_multi_step,\n            'is_last_step': self.is_last_step,\n            'is_first_multi_step': self.is_first_multi_step,\n            'num_seqs': self.num_seqs,\n            'num_queries': self.num_queries,\n        }\n        tensor_dict.update(new_tensor_dict)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"StatefulModelInput\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        tensor_dict = _init_frozen_model_input_from_tensor_dict(\n            ModelInputForGPUWithSamplingMetadata, tensor_dict)\n\n        return cls(**tensor_dict)\n\n    def record_step_event(self, current_stream: torch.cuda.Stream):\n        # record the event for the current step so that the next step can sync\n        # on it. We modulo by 2 to keep the events in a circular buffer and\n        # support any attn backends that may be supported in the future. ie\n        # Flashinfer would want two DecodeWrappers to overlap the CPU and GPU.\n        self.step_cuda_events[self.current_step & 1] = \\\n            torch.cuda.Event(blocking=True)\n        self.step_cuda_events[self.current_step & 1].record(current_stream)\n\n    def wait_previous_step(self):\n        # These cuda events are an explicit synchronization to ensure that\n        # advance_step() (for other attn backends that may be supported in the\n        # future) do not clobber any data structures that is also used by any\n        # enqueued forwards steps. For distributed case, only a single event is\n        # needed, but for single GPU case, since we can let the CPU run much\n        # further ahead, two events allow us to overlap the advance_step with\n        # the previous forward (ie using two DecodeWrappers for flashinfer\n        # backend)\n        self.step_cuda_events[(self.current_step + 1) & 1].wait()\n\n    def add_sampler_output(self,\n                           sampler_output: SamplerOutput,\n                           sampled_token_ids: Optional[torch.Tensor] = None):\n        self.cached_outputs.append(\n            ModelOutput(sampler_output=sampler_output,\n                        sampler_output_ready_event=None,\n                        sampled_token_ids=sampled_token_ids,\n                        pythonized=False))\n\n\n# MutableModelInputForGPUWithMultiStepMetadata is not subclass of\n# ModelInputForGPU but it wraps the actual input dataclass and adds multi-step\n# metadata\n# mypy: disable-error-code=type-var\nclass MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n    # mypy: enable-error-code=type-var\n\n    def __init__(self, base_model_runner: GPUModelRunnerBase, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # uses the base model runner to execute the model and wraps it with\n        # multi-step logic\n        self._base_model_runner: GPUModelRunnerBase = base_model_runner\n\n        self.is_multi_step = self.scheduler_config.is_multi_step\n        # used to copy tensors from GPU to CPU asynchronously\n        self._copy_stream = torch.cuda.Stream()\n        self.pinned_sampled_token_ids: Optional[torch.Tensor] = None\n\n    def make_model_input_from_broadcasted_tensor_dict(\n            self, tensor_dict: Dict[str, Any]) -> StatefulModelInput:\n        model_input = (StatefulModelInput.from_broadcasted_tensor_dict(\n            tensor_dict,\n            attn_backend=self.attn_backend,\n        ))\n        return model_input\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> StatefulModelInput:\n        frozen_model_input = self._base_model_runner.prepare_model_input(\n            seq_group_metadata_list, virtual_engine, finished_requests_ids)\n\n        model_input = StatefulModelInput(\n            frozen_model_input=frozen_model_input,\n            num_seqs=len(frozen_model_input.seq_lens),\n            num_queries=len(frozen_model_input.query_lens),\n        )\n        return model_input\n\n    def _async_process_outputs(self, model_input: StatefulModelInput,\n                               output_proc_callback: Callable):\n        # Proceed with pythonization and output_proc in order.\n        # Stop on the first one that fails to pythonize\n        cont = True\n        for model_output in model_input.cached_outputs:\n            if not model_output.pythonized:\n                model_output.maybe_pythonize(model_input, self._copy_stream,\n                                             self.pinned_sampled_token_ids)\n                if model_output.pythonized:\n                    output_proc_callback(\n                        sampler_output=model_output.sampler_output)\n                else:\n                    cont = False\n\n            if not cont:\n                break\n\n    def _final_process_outputs(self, model_input: StatefulModelInput,\n                               output_proc_callback: Optional[Callable]):\n        assert model_input.frozen_model_input is not None\n\n        outputs = []\n        for output_id in range(len(model_input.cached_outputs)):\n            is_last_output = output_id == len(model_input.cached_outputs) - 1\n\n            output = model_input.cached_outputs[output_id]\n            if not output.pythonized:\n                output.pythonize(model_input, self._copy_stream,\n                                 self.pinned_sampled_token_ids)\n\n                if model_input.frozen_model_input.use_async_and_multi_step:\n                    assert output_proc_callback is not None\n                    output_proc_callback(sampler_output=output.sampler_output,\n                                         is_last_output=is_last_output)\n\n            outputs.append(output.sampler_output)\n\n        return outputs\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        model_input: StatefulModelInput,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:\n        \"\"\" \n        Execute the model for a single step and update multi-step\n        metadata\n        \"\"\"\n        assert num_steps == 1, \"MultiStepModelRunner only supports num_steps=1\"\n        frozen_model_input = model_input.frozen_model_input\n        assert frozen_model_input is not None\n\n        # path for warm up runs\n        if not model_input.is_multi_step:\n            return self._base_model_runner.execute_model(\n                frozen_model_input, kv_caches, intermediate_tensors, num_steps)\n\n        # make sure we skip the sampler on the lask rank and only pythonize\n        # if CPU is ahead.\n        if self.is_driver_worker and get_pp_group().is_last_rank:\n            if self.pinned_sampled_token_ids is None:\n                self.pinned_sampled_token_ids = torch.zeros(\n                    (self.scheduler_config.max_num_seqs, 1),\n                    dtype=torch.long,\n                    device=\"cpu\",\n                    pin_memory=True)\n\n            self._base_model_runner.model.sampler.include_gpu_probs_tensor = (\n                True)\n            if frozen_model_input.sampling_metadata:\n                frozen_model_input.sampling_metadata.skip_sampler_cpu_output = (\n                    True)\n\n        # some pre-execute model logic for multi-step:\n        #   - if it's the first step, we need to reset the sampling tensors\n        #   - if it's not the first step, we need to advance the step using the\n        #   appended sampler output from last iteration\n        #   - also maybe pythonize if CPU is ahead of GPU\n\n        current_stream = torch.cuda.current_stream()\n        if not model_input.is_first_multi_step:\n            # Explicitly block on the previous step's forward to make sure we\n            # don't clobber any GPU tensors still in use.\n            # This is not needed for flashattn backend, but for other attn\n            # backends such as flashinfer that performs extra CPU operations on\n            # input metadata we may need to synchronize any CPU operations that\n            # might clobber enqueued forwards. (prevents CPU from running too\n            # far ahead if needed)\n            model_input.wait_previous_step()\n            model_input = self._advance_step(\n                model_input, model_input.cached_outputs[-1].sampler_output)\n\n        output_proc_callback = None\n        if frozen_model_input.use_async_and_multi_step:\n            output_proc_callback = frozen_model_input.async_callback\n            assert output_proc_callback is not None\n            async_callback = functools.partial(\n                self._async_process_outputs,\n                model_input=model_input,\n                output_proc_callback=output_proc_callback)\n\n            frozen_model_input = dataclasses.replace(  # type: ignore\n                model_input.frozen_model_input,\n                async_callback=async_callback)\n            assert frozen_model_input is not None\n\n        # Execute the model\n        output = self._base_model_runner.execute_model(frozen_model_input,\n                                                       kv_caches,\n                                                       intermediate_tensors,\n                                                       num_steps=1)\n\n        # record the event for the current step so that the next step can sync\n        model_input.record_step_event(current_stream)\n\n        if get_pp_group().is_last_rank and self.is_driver_worker:\n            assert len(\n                output\n            ) == 1, \"MultiStepModelRunner requires single-step base_models\"\n\n            # event for the pythonization so that we only pythonize if the\n            # tensors are ready. May be able to be combined with the step event\n            output_ready_event = torch.cuda.Event()\n            output_ready_event.record(current_stream)\n            if self.parallel_config.pipeline_parallel_size > 1:\n                output[0].sampled_token_ids_cpu = output[\n                    0].sampled_token_ids.cpu()\n            model_input.cached_outputs.append(\n                ModelOutput(output[0], output_ready_event,\n                            output[0].sampled_token_ids, False,\n                            output[0].logprobs))\n\n            # These GPU tensors are not required by multi-step;\n            # erase them to ensure they are not pythonized or\n            # transferred to CPU\n            output[0].sampled_token_ids = None\n            output[0].sampled_token_probs = None\n            output[0].logprobs = None\n\n            # Pythonize the output if CPU is ahead and the previous step is\n            # ready.\n            if not frozen_model_input.use_async_and_multi_step:\n                for model_output in model_input.cached_outputs:\n                    model_output.maybe_pythonize(model_input,\n                                                 self._copy_stream,\n                                                 self.pinned_sampled_token_ids)\n\n        model_input.current_step += 1\n\n        if not get_pp_group().is_last_rank:\n            # Should be IntermediateTensors\n            assert isinstance(output, IntermediateTensors)\n            return output\n        if not self.is_driver_worker:\n            return []\n\n        # Pythonize the output and block if needed since it is the last step\n        if model_input.is_last_step:\n            outputs = self._final_process_outputs(model_input,\n                                                  output_proc_callback)\n            return outputs\n\n        # should be [SamplerOutput]\n        return output\n\n    def _update_sampling_metadata(self, sampling_metadata, num_seqs,\n                                  num_queries):\n\n        assert sampling_metadata.num_prompts == 0\n        assert len(sampling_metadata.seq_groups) == num_queries\n        assert sampling_metadata.selected_token_indices.shape == (\n            num_queries, )\n        # assert sampling_metadata.categorized_sample_indices == TODO: Add if needed # noqa: E501\n\n        # Verify that all sequences are decodes\n        for i in range(num_queries):\n            seq_group = sampling_metadata.seq_groups[i]\n\n            assert seq_group.is_prompt is False  # No prompt\n            assert seq_group.prompt_logprob_indices == []  # No prompt\n            assert seq_group.sample_indices == [i]  # Simple\n            assert seq_group.seq_len is None  # Decode\n            assert seq_group.query_len is None  # Decode\n\n    def _advance_step(self, model_input: StatefulModelInput,\n                      out: SamplerOutput) -> StatefulModelInput:\n        frozen_model_input = model_input.frozen_model_input\n        assert frozen_model_input is not None\n        assert frozen_model_input.attn_metadata is not None\n\n        num_seqs = model_input.num_seqs\n        num_queries = model_input.num_queries\n        assert num_seqs > 0\n        assert num_queries > 0\n        assert num_seqs >= num_queries\n\n        attn_metadata = frozen_model_input.attn_metadata\n        assert isinstance(attn_metadata, FlashAttentionMetadata)\n        attn_metadata.advance_step(num_seqs, num_queries)\n\n        # Update GPU tensors\n        ops.advance_step(\n            num_seqs=num_seqs,\n            num_queries=num_queries,\n            block_size=self.block_size,\n            input_tokens=frozen_model_input.input_tokens,\n            sampled_token_ids=model_input.cached_outputs[-1].sampled_token_ids,\n            input_positions=frozen_model_input.input_positions,\n            seq_lens=attn_metadata.seq_lens_tensor,\n            slot_mapping=attn_metadata.slot_mapping,\n            block_tables=attn_metadata.block_tables)\n\n        if frozen_model_input.seq_lens is not None:\n            for i in range(num_queries):\n                frozen_model_input.seq_lens[i] = attn_metadata.seq_lens[i]\n\n        return model_input\n\n    def load_model(self) -> None:\n        return self._base_model_runner.load_model()\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        return self._base_model_runner.save_sharded_state(\n            path, pattern, max_size)\n\n    def save_tensorized_model(self,\n                              tensorizer_config: TensorizerConfig) -> None:\n        return self._base_model_runner.save_tensorized_model(tensorizer_config)\n\n    def profile_run(self) -> None:\n        return self._base_model_runner.profile_run()\n\n    def remove_all_loras(self):\n        return self._base_model_runner.remove_all_loras()\n\n    def capture_model(self, kv_caches: List[List]) -> None:\n        return self._base_model_runner.capture_model(kv_caches)\n\n    @property\n    def vocab_size(self) -> int:\n        return self._base_model_runner.vocab_size\n\n\nDeferredLogprobsReturnType = Tuple[Optional[List[Optional[PromptLogprobs]]],\n                                   Optional[List[SampleLogprobs]]]\n\n\ndef deferred_pythonize_logprobs(\n    output: SamplerOutput,\n    sampling_metadata: SamplingMetadata,\n    logprobs_tensor: Optional[torch.Tensor],\n) -> DeferredLogprobsReturnType:\n    \"\"\"Perform deferred logprob Pythonization.\n\n    1. Pythonize GPU-side sampler result tensors into CPU-side sampler result.\n    2. Pythonize GPU-side logprobs tensor into CPU-side logprobs lists,\n       utilizing  the Pythonized sampler result computed in step 1.\n    \n    These deferred computations are not required for single-step scheduling\n    or the `profile_run()` phase of multi-step scheduling.\n\n    Args:\n        output: sampler output (under deferred Pythonization)\n        sampling_metadata\n        \n    Returns:\n        prompt_logprobs (CPU), sample_logprobs (CPU)\n    \"\"\"\n\n    # - Deferred pythonization of sample result\n    sampler_result = get_pythonized_sample_results(\n        output.deferred_sample_results_args)\n\n    # - Erase the GPU-side deferred sample_result\n    #   computation args to ensure it is never\n    #   pythonized or transferred to CPU\n    output.deferred_sample_results_args = None\n\n    # - Deferred pythonization of logprobs\n    (\n        prompt_logprobs,\n        sample_logprobs,\n    ) = get_logprobs(logprobs_tensor, sampling_metadata, sampler_result)\n    assert len(prompt_logprobs) == len(sampling_metadata.seq_groups)\n    assert len(sample_logprobs) == len(sampling_metadata.seq_groups)\n\n    return prompt_logprobs, sample_logprobs\n\n\ndef _pythonize_sampler_output(\n    model_input: StatefulModelInput,\n    output: SamplerOutput,\n    pinned_sampled_token_buffer: torch.Tensor,\n    sampled_token_ids: torch.Tensor,\n    logprobs_tensor: Optional[torch.Tensor],\n) -> None:\n    \"\"\" This function is only called when the output tensors are ready. \n    See :class:`ModelOutput`. \n    \n    Modifies `output.outputs` and `pinned_sampled_token_buffer` in-place, \n    adding a Pythonized output data structure\n    (:class:`CompletionSequenceGroupOutput`) for each :class:`SequenceGroup`.\n\n    Args:\n      model_input\n      output: sampler output\n      pinned_sampled_token_token_buffer: CPU-side pinned memory\n                                         (receives copy of\n                                         GPU-side token buffer.)\n      sampled_token_ids: GPU-side token buffer\n      logprobs_tensor: GPU-side tensor containing \n                       logprobs computed during sampling\n    \"\"\"\n\n    assert model_input.frozen_model_input is not None\n\n    frozen_model_input = model_input.frozen_model_input\n    assert frozen_model_input.sampling_metadata is not None\n    # samples generation should have been skipped\n    assert not output.outputs\n\n    pinned_buffer = pinned_sampled_token_buffer[:model_input.num_queries]\n\n    # CPU GPU sync\n    pinned_buffer = pinned_buffer.copy_(sampled_token_ids, non_blocking=False)\n\n    # this will not block as the tensors are already on CPU\n    samples_list = pinned_buffer.tolist()\n\n    sampling_metadata = frozen_model_input.sampling_metadata\n\n    skip_sampler_cpu_output = (\n        frozen_model_input.sampling_metadata.skip_sampler_cpu_output)\n\n    # We are guaranteed output tensors are ready, so it is safe to\n    # pythonize the sampler output & obtain CPU-side logprobs.\n    #\n    # However this computation may be skipped entirely\n    # if no pythonization was deferred.\n    seq_groups = sampling_metadata.seq_groups\n    logprobs_are_requested = any([\n        sg.sampling_params.logprobs is not None\n        or sg.sampling_params.prompt_logprobs is not None for sg in seq_groups\n    ])\n    do_pythonize_logprobs = (skip_sampler_cpu_output\n                             and logprobs_are_requested)\n    (\n        prompt_logprobs,\n        sample_logprobs,\n    ) = (deferred_pythonize_logprobs(output, sampling_metadata,\n                                     logprobs_tensor)\n         if do_pythonize_logprobs else (None, None))\n\n    for sgdx, (seq_group,\n               sample_result) in enumerate(zip(seq_groups, samples_list)):\n\n        if do_pythonize_logprobs:\n            assert prompt_logprobs is not None\n            assert sample_logprobs is not None\n\n            (\n                group_prompt_logprobs,\n                group_sample_logprobs,\n            ) = (  # Utilize deferred pythonization results\n                prompt_logprobs[sgdx],\n                sample_logprobs[sgdx],\n            )\n        elif logprobs_are_requested:\n            (\n                group_prompt_logprobs,\n                group_sample_logprobs,\n            ) = (\n                # profile_run: use already-computed logprobs\n                output.outputs[sgdx].prompt_logprobs,\n                [sample.logprobs for sample in output.outputs[sgdx].samples])\n\n        seq_ids = seq_group.seq_ids\n        next_token_ids = sample_result\n        parent_ids = [0]\n        seq_outputs: List[SequenceOutput] = []\n        if seq_group.sampling_params.logits_processors:\n            assert len(seq_group.sampling_params.logits_processors) == 0, (\n                \"Logits Processors are not supported in multi-step decoding\")\n        for tdx, (parent_id,\n                  next_token_id) in enumerate(zip(parent_ids, next_token_ids)):\n            seq_outputs.append(\n                SequenceOutput(seq_ids[parent_id], next_token_id,\n                               (group_sample_logprobs[tdx]\n                                if logprobs_are_requested else {\n                                    next_token_id:\n                                    Logprob(logprob=float('inf'),\n                                            rank=None,\n                                            decoded_token=None)\n                                })))\n        output.outputs.append(\n            CompletionSequenceGroupOutput(\n                seq_outputs,\n                (group_prompt_logprobs if logprobs_are_requested else None)))\n    assert len(output.outputs) > 0\n",
      "diff": "diff --git a/vllm/worker/multi_step_model_runner.py b/vllm/worker/multi_step_model_runner.py\nindex be0c75bc0..b52f2a07e 100644\n--- a/vllm/worker/multi_step_model_runner.py\n+++ b/vllm/worker/multi_step_model_runner.py\n@@ -22,6 +22,7 @@ from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,\n                                                 get_pythonized_sample_results)\n from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,\n                            Logprob, SequenceGroupMetadata, SequenceOutput)\n+from vllm.utils import PyObjectCache\n from vllm.worker.model_runner import (GPUModelRunnerBase,\n                                       ModelInputForGPUWithSamplingMetadata)\n from vllm.worker.model_runner_base import (\n@@ -37,6 +38,29 @@ if TYPE_CHECKING:\n logger = init_logger(__name__)\n \n \n+def seq_output_builder():\n+    return SequenceOutput(\n+        0, 0,\n+        {0: Logprob(logprob=float('inf'), rank=None, decoded_token=None)})\n+\n+\n+def completion_seq_group_output_builder():\n+    return CompletionSequenceGroupOutput([], None)\n+\n+\n+# Used by pythonization to reduce python object allocations\n+class PythonizationCache:\n+\n+    def __init__(self):\n+        self.cached_seq_output = PyObjectCache(seq_output_builder)\n+        self.cached_completion_seq_group_output = PyObjectCache(\n+            completion_seq_group_output_builder)\n+\n+    def reset(self):\n+        self.cached_seq_output.reset()\n+        self.cached_completion_seq_group_output.reset()\n+\n+\n @dataclass\n class ModelOutput:\n     \"\"\"The output of a single model forward pass.\n@@ -59,6 +83,7 @@ class ModelOutput:\n     pythonized: bool = False\n     # On-device tensor containing the logprobs of each token.\n     logprobs: Optional[\"torch.Tensor\"] = None\n+    pythonization_cache: Optional[PythonizationCache] = None\n \n     def pythonize(self, input_metadata: \"StatefulModelInput\",\n                   copy_stream: torch.cuda.Stream,\n@@ -97,7 +122,8 @@ class ModelOutput:\n         with torch.cuda.stream(copy_stream):\n             _pythonize_sampler_output(input_metadata, self.sampler_output,\n                                       pinned_sampled_token_buffer,\n-                                      self.sampled_token_ids, self.logprobs)\n+                                      self.sampled_token_ids, self.logprobs,\n+                                      self.pythonization_cache)\n \n         # Erase the logprobs GPU-side tensor.\n         # Note that although _pythonize_sampler_output() runs in its\n@@ -209,6 +235,8 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         self._copy_stream = torch.cuda.Stream()\n         self.pinned_sampled_token_ids: Optional[torch.Tensor] = None\n \n+        self.pythonization_cache = PythonizationCache()\n+\n     def make_model_input_from_broadcasted_tensor_dict(\n             self, tensor_dict: Dict[str, Any]) -> StatefulModelInput:\n         model_input = (StatefulModelInput.from_broadcasted_tensor_dict(\n@@ -237,14 +265,22 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Callable):\n         # Proceed with pythonization and output_proc in order.\n         # Stop on the first one that fails to pythonize\n+        output_proc_callback()\n+\n         cont = True\n         for model_output in model_input.cached_outputs:\n             if not model_output.pythonized:\n                 model_output.maybe_pythonize(model_input, self._copy_stream,\n                                              self.pinned_sampled_token_ids)\n                 if model_output.pythonized:\n-                    output_proc_callback(\n-                        sampler_output=model_output.sampler_output)\n+                    ctx = output_proc_callback.keywords[\"ctx\"]\n+                    is_async = False\n+                    is_last_step = False\n+                    ctx.output_queue.append(\n+                        ([model_output.sampler_output\n+                          ], ctx.seq_group_metadata_list,\n+                         ctx.scheduler_outputs, is_async, is_last_step))\n+                    output_proc_callback()\n                 else:\n                     cont = False\n \n@@ -255,21 +291,46 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                                output_proc_callback: Optional[Callable]):\n         assert model_input.frozen_model_input is not None\n \n+        has_async_callback = output_proc_callback is not None\n+\n         outputs = []\n         for output_id in range(len(model_input.cached_outputs)):\n-            is_last_output = output_id == len(model_input.cached_outputs) - 1\n-\n             output = model_input.cached_outputs[output_id]\n-            if not output.pythonized:\n+            is_last_step = output_id == len(model_input.cached_outputs) - 1\n+\n+            # For non-async case:\n+            #   -- We simply add the outputs\n+            # For async case:\n+            #   -- Invoke callback, pythonize, add to callback queue and repeat\n+            #   -- For last output, just add to callback queue\n+            if has_async_callback:\n+                assert output_proc_callback is not None\n+\n+                # Invoke callback before pythonize (to overlap with GPU)\n+                output_proc_callback()\n+\n+                # Pythonize\n+                if not output.pythonized:\n+                    output.pythonize(model_input, self._copy_stream,\n+                                     self.pinned_sampled_token_ids)\n+\n+                    # For non last step, add to callback queue to chain\n+                    # callbacks=>pythonize pairs (for GPU overlap)\n+                    if not is_last_step:\n+                        ctx = output_proc_callback.keywords[  # type: ignore\n+                            \"ctx\"]  # type: ignore\n+                        is_async = False\n+                        is_last_step = False\n+                        ctx.output_queue.append(\n+                            ([output.sampler_output\n+                              ], ctx.seq_group_metadata_list,\n+                             ctx.scheduler_outputs, is_async, is_last_step))\n+                    else:\n+                        outputs.append(output.sampler_output)\n+            else:\n                 output.pythonize(model_input, self._copy_stream,\n                                  self.pinned_sampled_token_ids)\n-\n-                if model_input.frozen_model_input.use_async_and_multi_step:\n-                    assert output_proc_callback is not None\n-                    output_proc_callback(sampler_output=output.sampler_output,\n-                                         is_last_output=is_last_output)\n-\n-            outputs.append(output.sampler_output)\n+                outputs.append(output.sampler_output)\n \n         return outputs\n \n@@ -330,7 +391,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n                 model_input, model_input.cached_outputs[-1].sampler_output)\n \n         output_proc_callback = None\n-        if frozen_model_input.use_async_and_multi_step:\n+        if frozen_model_input.async_callback is not None:\n             output_proc_callback = frozen_model_input.async_callback\n             assert output_proc_callback is not None\n             async_callback = functools.partial(\n@@ -367,7 +428,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n             model_input.cached_outputs.append(\n                 ModelOutput(output[0], output_ready_event,\n                             output[0].sampled_token_ids, False,\n-                            output[0].logprobs))\n+                            output[0].logprobs, self.pythonization_cache))\n \n             # These GPU tensors are not required by multi-step;\n             # erase them to ensure they are not pythonized or\n@@ -378,7 +439,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n \n             # Pythonize the output if CPU is ahead and the previous step is\n             # ready.\n-            if not frozen_model_input.use_async_and_multi_step:\n+            if frozen_model_input.async_callback is None:\n                 for model_output in model_input.cached_outputs:\n                     model_output.maybe_pythonize(model_input,\n                                                  self._copy_stream,\n@@ -397,6 +458,7 @@ class MultiStepModelRunner(GPUModelRunnerBase[StatefulModelInput]):\n         if model_input.is_last_step:\n             outputs = self._final_process_outputs(model_input,\n                                                   output_proc_callback)\n+            self.pythonization_cache.reset()\n             return outputs\n \n         # should be [SamplerOutput]\n@@ -537,6 +599,7 @@ def _pythonize_sampler_output(\n     pinned_sampled_token_buffer: torch.Tensor,\n     sampled_token_ids: torch.Tensor,\n     logprobs_tensor: Optional[torch.Tensor],\n+    cache: Optional[PythonizationCache],\n ) -> None:\n     \"\"\" This function is only called when the output tensors are ready. \n     See :class:`ModelOutput`. \n@@ -597,6 +660,9 @@ def _pythonize_sampler_output(\n \n     for sgdx, (seq_group,\n                sample_result) in enumerate(zip(seq_groups, samples_list)):\n+        if seq_group.sampling_params.logits_processors:\n+            assert len(seq_group.sampling_params.logits_processors) == 0, (\n+                \"Logits Processors are not supported in multi-step decoding\")\n \n         if do_pythonize_logprobs:\n             assert prompt_logprobs is not None\n@@ -621,23 +687,56 @@ def _pythonize_sampler_output(\n         seq_ids = seq_group.seq_ids\n         next_token_ids = sample_result\n         parent_ids = [0]\n-        seq_outputs: List[SequenceOutput] = []\n-        if seq_group.sampling_params.logits_processors:\n-            assert len(seq_group.sampling_params.logits_processors) == 0, (\n-                \"Logits Processors are not supported in multi-step decoding\")\n+\n+        if cache is not None:\n+            completion_seq_group_output: CompletionSequenceGroupOutput = \\\n+                cache.cached_completion_seq_group_output.get_object()\n+            completion_seq_group_output.samples.clear()\n+            seq_outputs: List[\n+                SequenceOutput] = completion_seq_group_output.samples\n+        else:\n+            seq_outputs = []\n+\n         for tdx, (parent_id,\n                   next_token_id) in enumerate(zip(parent_ids, next_token_ids)):\n-            seq_outputs.append(\n-                SequenceOutput(seq_ids[parent_id], next_token_id,\n-                               (group_sample_logprobs[tdx]\n-                                if logprobs_are_requested else {\n-                                    next_token_id:\n-                                    Logprob(logprob=float('inf'),\n-                                            rank=None,\n-                                            decoded_token=None)\n-                                })))\n-        output.outputs.append(\n-            CompletionSequenceGroupOutput(\n-                seq_outputs,\n-                (group_prompt_logprobs if logprobs_are_requested else None)))\n+            if cache is not None:\n+                seq_output: SequenceOutput = cache.cached_seq_output.get_object(\n+                )\n+                seq_output.parent_seq_id = seq_ids[parent_id]\n+                seq_output.output_token = next_token_id\n+\n+                if logprobs_are_requested:\n+                    seq_output.logprobs = group_sample_logprobs[tdx]\n+                else:\n+                    logprobs = next(iter(seq_output.logprobs.values()))\n+                    seq_output.logprobs.clear()\n+\n+                    logprobs.logprob = float('inf')\n+                    logprobs.rank = None\n+                    logprobs.decoded_token = None\n+\n+                    seq_output.logprobs[next_token_id] = logprobs\n+\n+                seq_outputs.append(seq_output)\n+\n+            else:\n+                seq_outputs.append(\n+                    SequenceOutput(seq_ids[parent_id], next_token_id,\n+                                   (group_sample_logprobs[tdx]\n+                                    if logprobs_are_requested else {\n+                                        next_token_id:\n+                                        Logprob(logprob=float('inf'),\n+                                                rank=None,\n+                                                decoded_token=None)\n+                                    })))\n+        if cache is not None:\n+            completion_seq_group_output.prompt_logprobs = \\\n+                group_prompt_logprobs if logprobs_are_requested else None\n+            output.outputs.append(completion_seq_group_output)\n+        else:\n+            output.outputs.append(\n+                CompletionSequenceGroupOutput(\n+                    seq_outputs, (group_prompt_logprobs\n+                                  if logprobs_are_requested else None)))\n+\n     assert len(output.outputs) > 0",
      "change_type": "modified",
      "lines_added": 133,
      "lines_removed": 34
    },
    {
      "file_path": "vllm/worker/multi_step_worker.py",
      "old_content": "import dataclasses\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\n\nfrom vllm.distributed import broadcast_tensor_dict, get_pp_group\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.sequence import ExecuteModelRequest\nfrom vllm.worker.model_runner_base import BroadcastableModelInput\nfrom vllm.worker.multi_step_model_runner import (MultiStepModelRunner,\n                                                 StatefulModelInput)\nfrom vllm.worker.worker import Worker, WorkerInput\n\n\n@dataclass\nclass MultiStepState:\n    worker_input: WorkerInput\n    model_input: StatefulModelInput\n\n\nclass MultiStepWorker(Worker):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        base_model_runner = self.model_runner\n        # for multi-step model, wrap the model runner with MultiStepModelRunner\n        self.model_runner = MultiStepModelRunner(\n            base_model_runner,\n            base_model_runner.model_config,\n            base_model_runner.parallel_config,\n            base_model_runner.scheduler_config,\n            base_model_runner.device_config,\n            base_model_runner.cache_config,\n            load_config=base_model_runner.load_config,\n            lora_config=self.lora_config,\n            kv_cache_dtype=self.cache_config.cache_dtype,\n            is_driver_worker=base_model_runner.is_driver_worker,\n            prompt_adapter_config=base_model_runner.prompt_adapter_config,\n            observability_config=base_model_runner.observability_config,\n        )\n\n        pipeline_parallel_size = self.parallel_config.pipeline_parallel_size\n        self.multi_step_states: List[\n            Optional[MultiStepState]] = [None] * pipeline_parallel_size\n        self.temp_output = None\n\n    def _get_driver_input_and_broadcast(\n        self, execute_model_req: ExecuteModelRequest\n    ) -> Tuple[BroadcastableModelInput, WorkerInput, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Get the driver input and broadcast it to other workers.\n        \"\"\"\n        assert self.is_driver_worker\n        virtual_engine = execute_model_req.virtual_engine\n        is_first_multi_step = execute_model_req.is_first_multi_step\n        if is_first_multi_step:\n            # on first step we prepare the worker input and model input normally\n            worker_input: WorkerInput = self.prepare_worker_input(\n                execute_model_req=execute_model_req)\n            model_input: StatefulModelInput = (\n                self.model_runner.prepare_model_input(\n                    execute_model_req.seq_group_metadata_list,\n                    execute_model_req.virtual_engine,\n                    execute_model_req.finished_requests_ids))\n\n            if execute_model_req.async_callback:\n                model_input.frozen_model_input = dataclasses.replace(  # type: ignore\n                    model_input.frozen_model_input,\n                    async_callback=execute_model_req.async_callback,\n                    use_async_and_multi_step=execute_model_req.\n                    use_async_and_multi_step)\n        else:\n            # on subsequent steps we reuse the worker input and model input\n            multi_step_state = self.multi_step_states[virtual_engine]\n            worker_input = multi_step_state.worker_input\n            model_input = multi_step_state.model_input\n            frozen_model_input = model_input.frozen_model_input\n            assert frozen_model_input is not None\n            assert frozen_model_input.attn_metadata is not None\n            # clear the cached decode metadata so that it can be recomputed on\n            # the workers\n            frozen_model_input.attn_metadata._cached_decode_metadata = None\n\n        model_input.is_first_multi_step = is_first_multi_step\n        model_input.is_last_step = execute_model_req.is_last_step\n\n        if not is_first_multi_step:\n            # we broadcast the last sampled token ids to all TP workers so they\n            # can update their model input metadata in-place.\n            self._prepare_last_sampled_token_ids_for_tp_workers(\n                execute_model_req=execute_model_req, model_input=model_input)\n\n        if self.do_metadata_broadcast:\n            broadcast_data = worker_input.as_broadcastable_tensor_dict()\n            broadcast_data.update(model_input.as_broadcastable_tensor_dict())\n            broadcast_tensor_dict(broadcast_data, src=0)\n\n        # Retuning empty dict here to keep this compatible with\n        # `LocalOrDistributedWorkerBase._get_driver_input_and_broadcast`\n        return model_input, worker_input, {}\n\n    def _prepare_last_sampled_token_ids_for_tp_workers(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        model_input: StatefulModelInput,\n    ) -> None:\n        \"\"\" \n        Prepare the last sampled token ids for TP workers. If it's the last \n        PP rank, then the last sampled token ids are already in the model_input.\n        If it is NOT the last PP rank, then we need to get the last sampled\n        token that is cached in the execute_model_req.\n        \"\"\"\n        if get_pp_group().is_last_rank:\n            assert model_input.cached_outputs[\n                -1].sampler_output.sampled_token_ids is None\n            assert model_input.cached_outputs[-1].sampled_token_ids is not None\n            model_input.last_sampled_token_ids = model_input.cached_outputs[\n                -1].sampled_token_ids\n            # free sampled token ids from the previous step if it has been\n            # pythonized. Cannot free the last sampled token ids because\n            # we need it for GPU advance_step.\n            for output in model_input.cached_outputs[:-1]:\n                if output.pythonized:\n                    output.sampled_token_ids = None\n        else:\n            # otherwise we need to get the cached sampled token ids from the\n            # execute_model_req\n            assert execute_model_req.last_sampled_token_ids is not None\n            model_input.last_sampled_token_ids = (\n                execute_model_req.last_sampled_token_ids.cuda())\n            model_input.add_sampler_output(\n                SamplerOutput(outputs=[], sampled_token_ids=None),\n                model_input.last_sampled_token_ids)\n\n            # free sampled token ids from the previous step.\n            # TODO(will) we could reuse the sampled token ids tensor from\n            # the previous step instead.\n            for output in model_input.cached_outputs[:-1]:\n                output.sampled_token_ids = None\n            assert model_input.cached_outputs[-1].sampled_token_ids is not None\n\n    def prepare_input(\n        self,\n        execute_model_req: Optional[ExecuteModelRequest] = None,\n    ) -> Optional[Tuple[StatefulModelInput, WorkerInput, Dict[str,\n                                                              torch.Tensor]]]:\n        \"\"\"\n        Depending on the current state of the request and multi step worker,\n        this method may skip the normal _prepare_model_input and\n        _prepare_worker_input methods and instead used cached values.\n        \"\"\"\n        if self.is_driver_worker:\n            if execute_model_req is None:\n                if self.do_metadata_broadcast:\n                    # This signals that there's no more requests to process for\n                    # now. All workers are running infinite loop with\n                    # broadcast_tensor_dict, and it stops the loop when the\n                    # driver broadcasts an empty input. Send an empty input to\n                    # notify all other workers to stop their execution loop.\n                    broadcast_tensor_dict({}, src=0)\n                return None\n\n            virtual_engine = execute_model_req.virtual_engine\n            (model_input, worker_input,\n             kwargs) = self._get_driver_input_and_broadcast(execute_model_req)\n            assert isinstance(model_input, StatefulModelInput)\n            if execute_model_req.is_first_multi_step:\n                # cache the worker input and model input for the next steps\n                self.multi_step_states[virtual_engine] = MultiStepState(\n                    worker_input=worker_input, model_input=model_input)\n        # if TP workers\n        else:\n            broadcast_data = self._get_worker_input_from_broadcast()\n            # if the driver has sent an empty input, we should stop the worker\n            # loop\n            if broadcast_data is None:\n                return None\n            model_input, worker_input, kwargs = broadcast_data\n            assert isinstance(model_input, StatefulModelInput)\n            virtual_engine = worker_input.virtual_engine\n            if model_input.is_first_multi_step:\n                pass\n                # TODO(will) Can cache the worker input and model input for the\n                # next steps. See below for details\n            else:\n                # TODO(will) possible to also cache and reuse the cached worker\n                # input and model input. The idea is essentially the delta\n                # optimization for model_inputs. Where the TP workers can cache\n                # the model input states and we only broadcast the delta need\n                # for the next step (sampled_token_ids from the previous step)\n\n                assert isinstance(model_input, StatefulModelInput)\n                # we need to update the last sampled token ids in the model\n                # input for the workers so that they can run inplace\n                # advance_step\n                model_input.add_sampler_output(\n                    SamplerOutput(outputs=[], sampled_token_ids=None),\n                    model_input.last_sampled_token_ids)\n\n        assert model_input is not None\n        assert worker_input is not None\n        return model_input, worker_input, kwargs\n",
      "diff": "diff --git a/vllm/worker/multi_step_worker.py b/vllm/worker/multi_step_worker.py\nindex 517b0ab78..562285f82 100644\n--- a/vllm/worker/multi_step_worker.py\n+++ b/vllm/worker/multi_step_worker.py\n@@ -67,9 +67,7 @@ class MultiStepWorker(Worker):\n             if execute_model_req.async_callback:\n                 model_input.frozen_model_input = dataclasses.replace(  # type: ignore\n                     model_input.frozen_model_input,\n-                    async_callback=execute_model_req.async_callback,\n-                    use_async_and_multi_step=execute_model_req.\n-                    use_async_and_multi_step)\n+                    async_callback=execute_model_req.async_callback)\n         else:\n             # on subsequent steps we reuse the worker input and model input\n             multi_step_state = self.multi_step_states[virtual_engine]",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 4
    }
  ],
  "affected_apis": [
    "LLMEngine._process_model_outputs",
    "LLMEngine.__init__",
    "AsyncLLMEngine.process_request_outputs"
  ],
  "summary": {
    "total_files": 8,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 8
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_correctness_async_llm)",
    "is_benchmark_actually_there": "",
    "sample_clues": "async_llm_engine, asyncllmengine, execute"
  }
}