{
  "commit_hash": "83450458339b07765b0e72a822e5fe93eeaf5258",
  "parent_hash": "5b8a1fde84224e24ec121e0dc149d775330d911b",
  "message": "[Performance][Spec Decode] Optimize ngram lookup performance (#9333)",
  "author": "Lily Liu <lilyliupku@gmail.com>",
  "date": "2024-10-16 13:37:45 -0600",
  "files_changed": [
    {
      "file_path": "vllm/spec_decode/ngram_worker.py",
      "old_content": "import weakref\nfrom typing import List, Optional, Set, Tuple\n\nimport torch\n\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.sequence import ExecuteModelRequest\nfrom vllm.spec_decode.interfaces import SpeculativeProposals\nfrom vllm.spec_decode.proposer_worker_base import NonLLMProposerWorkerBase\nfrom vllm.spec_decode.top1_proposer import Top1Proposer\n\n\nclass NGramWorker(NonLLMProposerWorkerBase):\n    \"\"\"NGramWorker provides a light drafter without need for model.\n\n    Current NGramWorker only implements prompt lookup decoding,\n    and in future we may also do RAG type drafter and other scenarios\n    which don't rely on LLM model to give proposals.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Get local_rank/vocab_size from kwargs attribute\n        self.local_rank = kwargs[\"local_rank\"]\n        self.vocab_size = kwargs[\"model_config\"].get_vocab_size()\n\n        # Lazy initialization list.\n        self._proposer: Top1Proposer\n\n    def set_ngram_window_size(self, ngram_prompt_lookup_min: int,\n                              ngram_prompt_lookup_max: int):\n        # Search valid candidate window between\n        # ngram_prompt_lookup_min/ngram_prompt_lookup_max\n        self.ngram_prompt_lookup_max = ngram_prompt_lookup_max\n        self.ngram_prompt_lookup_min = ngram_prompt_lookup_min\n\n    def init_device(self):\n        self.device = torch.device(f\"cuda:{self.local_rank}\")\n        self.load_model = lambda *args, **kwargs: None\n\n        # Current NGramWorker only supports Top1Proposer\n        self._proposer = Top1Proposer(\n            weakref.proxy(self),  # type: ignore[arg-type]\n            device=self.device,\n            vocab_size=self.vocab_size,\n        )\n\n    def sampler_output(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        sample_len: int,\n        # Unused parameter. NGramWorker does not use the KV Cache and\n        # therefore does not need this parameter.\n        seq_ids_with_bonus_token_in_last_step: Set[int],\n    ) -> Tuple[Optional[List[Optional[SamplerOutput]]], bool]:\n        \"\"\"NGram match algo to pick proposal candidate. Returns the list of\n        sampler output, one per SequenceGroupMetadata.\n\n        For ngram worker, we already done needed transposed internal, so the\n        indicator pass to sampler_output_to_torch shall be False.\n        \"\"\"\n        self._raise_if_unsupported(execute_model_req)\n\n        has_spec_out = False\n        token_id_list: List[Optional[torch.Tensor]] = []\n        token_prob_list: List[Optional[torch.Tensor]] = []\n        for idx, seq_group_metadata in enumerate(\n                execute_model_req.seq_group_metadata_list):\n            seq_data = next(iter(seq_group_metadata.seq_data.values()))\n\n            input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                        dtype=torch.long,\n                                        device=self.device)\n            input_length = seq_data.get_len()\n\n            for ngram_size in range(\n                    min(self.ngram_prompt_lookup_max, input_length - 1),\n                    self.ngram_prompt_lookup_min - 1,\n                    -1,\n            ):\n                ngram_tensor = input_ids[-ngram_size:]\n                if ngram_size == 1:\n                    # Do not match itself and do not use unfold and all\n                    matches = (input_ids[:-1] == ngram_tensor)\n                else:\n                    windows = input_ids.unfold(dimension=0,\n                                               size=ngram_size,\n                                               step=1)\n                    # Do not match itself\n                    matches = (windows[:-1] == ngram_tensor).all(dim=-1)\n\n                # first_match includes \"values\" (bool), indicating whether\n                # the match is found, and \"indices\", indicating the index\n                # of the first match.\n                # Note that \"first_match.values.item()\" triggers GPU-CPU\n                # sync so it is a bit inefficient, but we have not found\n                # a better way to do this.\n                first_match = matches.max(dim=-1)\n                if first_match.values.item():\n                    proposal_start_idx = first_match.indices.add_(ngram_size)\n                    spec_indices = (\n                        proposal_start_idx).repeat(sample_len) + torch.arange(\n                            sample_len, device=self.device)\n                    spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n                    res = input_ids.gather(dim=-1, index=spec_indices)\n                    token_id_list.append(res)\n                    token_prob_list.append(\n                        torch.nn.functional.one_hot(\n                            res,\n                            num_classes=self.vocab_size).to(torch.float32))\n                    has_spec_out = True\n                    break\n            else:\n                token_id_list.append(None)\n                token_prob_list.append(None)\n\n        if not has_spec_out:\n            return None, False\n\n        outputs: List[Optional[SamplerOutput]] = []\n        for idx in range(len(execute_model_req.seq_group_metadata_list)):\n            if token_id_list[idx] is None:\n                outputs.append(None)\n            else:\n                outputs.append(\n                    SamplerOutput(\n                        outputs=None,\n                        sampled_token_probs=token_prob_list[idx],\n                        logprobs=torch.zeros((sample_len, self.vocab_size),\n                                             dtype=torch.float32,\n                                             device=self.device),\n                        sampled_token_ids=token_id_list[idx],\n                    ))\n\n        return outputs, False\n\n    def get_spec_proposals(\n        self,\n        execute_model_req: ExecuteModelRequest,\n        # Unused parameter. NGramWorker does not use the KV Cache and\n        # therefore does not need this parameter.\n        seq_ids_with_bonus_token_in_last_step: Set[int],\n    ) -> SpeculativeProposals:\n        \"\"\"Produce speculations given an input batch of sequences. The number of\n        speculative tokens per sequence is determined by max_proposal_len.\n        \"\"\"\n        return self._proposer.get_spec_proposals(\n            execute_model_req, seq_ids_with_bonus_token_in_last_step)\n\n    def _raise_if_unsupported(\n        self,\n        execute_model_req: ExecuteModelRequest,\n    ) -> None:\n        \"\"\"NGramWorker does not yet implement support for cache swap\n        operations or beam search.\n        \"\"\"\n        if any([\n                execute_model_req.blocks_to_swap_in,\n                execute_model_req.blocks_to_swap_out,\n                execute_model_req.blocks_to_copy\n        ]):\n            raise NotImplementedError(\n                \"NGramWorker does not support cache operations\")\n\n        if any(\n                len(seq_group_metadata.seq_data.keys()) != 1\n                for seq_group_metadata in\n                execute_model_req.seq_group_metadata_list):\n            raise NotImplementedError(\n                \"NGramWorker does not support beam search.\")\n",
      "diff": "diff --git a/vllm/spec_decode/ngram_worker.py b/vllm/spec_decode/ngram_worker.py\nindex 36e5e1774..a777e5c3f 100644\n--- a/vllm/spec_decode/ngram_worker.py\n+++ b/vllm/spec_decode/ngram_worker.py\n@@ -67,9 +67,16 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 execute_model_req.seq_group_metadata_list):\n             seq_data = next(iter(seq_group_metadata.seq_data.values()))\n \n+            seq_len = seq_data.get_len()\n+            # When seq_len is less than 3072 (3K), we use CPU to perform\n+            # the ngram match. Otherwise, we use the device specified in\n+            # the model config (normally GPU). 3072 is a rough threshold\n+            # based on profiling on H100, and it can be adjusted based\n+            # on the actual performance on different hardware.\n+            cur_device = \"cpu\" if seq_len < 3072 else self.device\n             input_ids = torch.as_tensor(seq_data.get_token_ids(),\n                                         dtype=torch.long,\n-                                        device=self.device)\n+                                        device=cur_device)\n             input_length = seq_data.get_len()\n \n             for ngram_size in range(\n@@ -91,17 +98,15 @@ class NGramWorker(NonLLMProposerWorkerBase):\n                 # first_match includes \"values\" (bool), indicating whether\n                 # the match is found, and \"indices\", indicating the index\n                 # of the first match.\n-                # Note that \"first_match.values.item()\" triggers GPU-CPU\n-                # sync so it is a bit inefficient, but we have not found\n-                # a better way to do this.\n                 first_match = matches.max(dim=-1)\n                 if first_match.values.item():\n                     proposal_start_idx = first_match.indices.add_(ngram_size)\n                     spec_indices = (\n                         proposal_start_idx).repeat(sample_len) + torch.arange(\n-                            sample_len, device=self.device)\n+                            sample_len, device=cur_device)\n                     spec_indices.clamp_(max=input_ids.shape[-1] - 1)\n-                    res = input_ids.gather(dim=-1, index=spec_indices)\n+                    res = input_ids.gather(dim=-1,\n+                                           index=spec_indices).to(self.device)\n                     token_id_list.append(res)\n                     token_prob_list.append(\n                         torch.nn.functional.one_hot(",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 7
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_ngram, test_spec_decode)",
    "is_benchmark_actually_there": "",
    "sample_clues": "ngram_worker, none"
  }
}