{
  "commit_hash": "b2e0ad3b598ed0e022cdbd678a20821d411873c2",
  "parent_hash": "4a18fd14ba4a349291c798a16bf62fa8a9af0b6b",
  "message": "[Perf] Reduce peak memory usage of llama (#10339)\n\nSigned-off-by: andoorve <37849411+andoorve@users.noreply.github.com>",
  "author": "Murali Andoorveedu <37849411+andoorve@users.noreply.github.com>",
  "date": "2024-11-15 00:38:20 +0000",
  "files_changed": [
    {
      "file_path": "vllm/model_executor/models/llama.py",
      "old_content": "# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only LLaMA model compatible with HuggingFace weights.\"\"\"\nfrom typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom transformers import LlamaConfig\n\nfrom vllm.attention import Attention, AttentionMetadata\nfrom vllm.compilation.decorators import support_torch_compile\nfrom vllm.config import CacheConfig, VllmConfig\nfrom vllm.distributed import (get_pp_group, get_tensor_model_parallel_rank,\n                              get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.logits_processor import LogitsProcessor\nfrom vllm.model_executor.layers.pooler import Pooler, PoolingType\nfrom vllm.model_executor.layers.quantization import QuantizationConfig\nfrom vllm.model_executor.layers.quantization.compressed_tensors.utils import (\n    get_compressed_tensors_cache_scale)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import SamplerOutput, get_sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    DEFAULT_VOCAB_PADDING_SIZE, ParallelLMHead, VocabParallelEmbedding)\nfrom vllm.model_executor.model_loader.weight_utils import (\n    default_weight_loader, kv_cache_scales_loader, maybe_remap_kv_scale_name)\nfrom vllm.model_executor.pooling_metadata import PoolingMetadata\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.platforms import current_platform\nfrom vllm.sequence import IntermediateTensors, PoolerOutput\n\nfrom .interfaces import SupportsLoRA, SupportsPP\nfrom .utils import (AutoWeightsLoader, PPMissingLayer, is_pp_missing_parameter,\n                    make_empty_intermediate_tensors_factory, make_layers,\n                    maybe_prefix)\n\n\nclass LlamaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        quant_config: Optional[QuantizationConfig] = None,\n        bias: bool = False,\n        prefix: str = \"\",\n    ) -> None:\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            input_size=hidden_size,\n            output_sizes=[intermediate_size] * 2,\n            bias=bias,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.gate_up_proj\",\n        )\n        self.down_proj = RowParallelLinear(\n            input_size=intermediate_size,\n            output_size=hidden_size,\n            bias=bias,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.down_proj\",\n        )\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass LlamaAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        hidden_size: int,\n        num_heads: int,\n        num_kv_heads: int,\n        rope_theta: float = 10000,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        max_position_embeddings: int = 8192,\n        quant_config: Optional[QuantizationConfig] = None,\n        bias: bool = False,\n        cache_config: Optional[CacheConfig] = None,\n        prefix: str = \"\",\n    ) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        # MistralConfig has an optional head_dim introduced by Mistral-Nemo\n        self.head_dim = getattr(config, \"head_dim\",\n                                self.hidden_size // self.total_num_heads)\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size=hidden_size,\n            head_size=self.head_dim,\n            total_num_heads=self.total_num_heads,\n            total_num_kv_heads=self.total_num_kv_heads,\n            bias=bias,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.qkv_proj\",\n        )\n\n        self.o_proj = RowParallelLinear(\n            input_size=self.total_num_heads * self.head_dim,\n            output_size=hidden_size,\n            bias=bias,\n            quant_config=quant_config,\n            prefix=f\"{prefix}.o_proj\",\n        )\n\n        is_neox_style = True\n        if quant_config is not None and quant_config.get_name() == \"gguf\":\n            is_neox_style = False\n\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n            rope_scaling=rope_scaling,\n            is_neox_style=is_neox_style,\n        )\n        self.attn = Attention(\n            self.num_heads,\n            self.head_dim,\n            self.scaling,\n            num_kv_heads=self.num_kv_heads,\n            cache_config=cache_config,\n            quant_config=quant_config,\n        )\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass LlamaDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        cache_config: Optional[CacheConfig] = None,\n        quant_config: Optional[QuantizationConfig] = None,\n        prefix: str = \"\",\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        if rope_scaling is not None and getattr(\n                config, \"original_max_position_embeddings\", None):\n            rope_scaling[\"original_max_position_embeddings\"] = (\n                config.original_max_position_embeddings)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        # Support abacusai/Smaug-72B-v0.1 with attention_bias\n        # Support internlm/internlm-7b with bias\n        attention_bias = getattr(config, \"attention_bias\", False) or getattr(\n            config, \"bias\", False)\n        self.self_attn = LlamaAttention(\n            config=config,\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=getattr(config, \"num_key_value_heads\",\n                                 config.num_attention_heads),\n            rope_theta=rope_theta,\n            rope_scaling=rope_scaling,\n            max_position_embeddings=max_position_embeddings,\n            quant_config=quant_config,\n            bias=attention_bias,\n            cache_config=cache_config,\n            prefix=f\"{prefix}.self_attn\",\n        )\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            quant_config=quant_config,\n            bias=getattr(config, \"mlp_bias\", False),\n            prefix=f\"{prefix}.mlp\",\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: AttentionMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(positions=positions,\n                                       hidden_states=hidden_states,\n                                       kv_cache=kv_cache,\n                                       attn_metadata=attn_metadata)\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\n@support_torch_compile\nclass LlamaModel(nn.Module):\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__()\n\n        config = vllm_config.model_config.hf_config\n        cache_config = vllm_config.cache_config\n        quant_config = vllm_config.quant_config\n        lora_config = vllm_config.lora_config\n\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        lora_vocab = (lora_config.lora_extra_vocab_size *\n                      (lora_config.max_loras or 1)) if lora_config else 0\n        self.vocab_size = config.vocab_size + lora_vocab\n        self.org_vocab_size = config.vocab_size\n        if get_pp_group().is_first_rank or (config.tie_word_embeddings\n                                            and get_pp_group().is_last_rank):\n            self.embed_tokens = VocabParallelEmbedding(\n                self.vocab_size,\n                config.hidden_size,\n                org_num_embeddings=config.vocab_size,\n                quant_config=quant_config,\n            )\n        else:\n            self.embed_tokens = PPMissingLayer()\n        self.start_layer, self.end_layer, self.layers = make_layers(\n            config.num_hidden_layers,\n            lambda prefix: LlamaDecoderLayer(config=config,\n                                             cache_config=cache_config,\n                                             quant_config=quant_config,\n                                             prefix=prefix),\n            prefix=f\"{prefix}.layers\",\n        )\n        if get_pp_group().is_last_rank:\n            self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        else:\n            self.norm = PPMissingLayer()\n\n        self.make_empty_intermediate_tensors = (\n            make_empty_intermediate_tensors_factory(\n                [\"hidden_states\", \"residual\"], config.hidden_size))\n\n    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n        return self.embed_tokens(input_ids)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor],\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        inputs_embeds: Optional[torch.Tensor] = None,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        if get_pp_group().is_first_rank:\n            if inputs_embeds is not None:\n                hidden_states = inputs_embeds\n            else:\n                hidden_states = self.get_input_embeddings(input_ids)\n            residual = None\n        else:\n            assert intermediate_tensors is not None\n            hidden_states = intermediate_tensors[\"hidden_states\"]\n            residual = intermediate_tensors[\"residual\"]\n\n        for i in range(self.start_layer, self.end_layer):\n            layer = self.layers[i]\n            hidden_states, residual = layer(positions, hidden_states,\n                                            kv_caches[i - self.start_layer],\n                                            attn_metadata, residual)\n\n        if not get_pp_group().is_last_rank:\n            return IntermediateTensors({\n                \"hidden_states\": hidden_states,\n                \"residual\": residual\n            })\n\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\".qkv_proj\", \".q_proj\", \"q\"),\n            (\".qkv_proj\", \".k_proj\", \"k\"),\n            (\".qkv_proj\", \".v_proj\", \"v\"),\n            (\".gate_up_proj\", \".gate_proj\", 0),\n            (\".gate_up_proj\", \".up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in weights:\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            if (\"rotary_emb.cos_cached\" in name\n                    or \"rotary_emb.sin_cached\" in name):\n                # Models trained using ColossalAI may include these tensors in\n                # the checkpoint. Skip them.\n                continue\n            if scale_name := get_compressed_tensors_cache_scale(name):\n                # Loading kv cache scales for compressed-tensors quantization\n                param = params_dict[scale_name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                loaded_weight = loaded_weight[0]\n                weight_loader(param, loaded_weight)\n                continue\n            for param_name, weight_name, shard_id in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n\n                if is_pp_missing_parameter(name, self):\n                    continue\n\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                # Remapping the name of FP8 kv-scale.\n                name = maybe_remap_kv_scale_name(name, params_dict)\n                if name is None:\n                    continue\n\n                if is_pp_missing_parameter(name, self):\n                    continue\n\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n\n    # If this function is called, it should always initialize KV cache scale\n    # factors (or else raise an exception). Thus, handled exceptions should\n    # make sure to leave KV cache scale factors in a known good (dummy) state\n    def load_kv_cache_scales(self, quantization_param_path: str) -> None:\n        tp_size = get_tensor_model_parallel_world_size()\n        tp_rank = get_tensor_model_parallel_rank()\n        for layer_idx, scaling_factor in kv_cache_scales_loader(\n                quantization_param_path, tp_rank, tp_size,\n                self.config.num_hidden_layers,\n                self.config.__class__.model_type):\n            if not isinstance(self.layers[layer_idx], nn.Identity):\n                layer_self_attn = self.layers[layer_idx].self_attn\n\n            if current_platform.is_rocm():\n                # The scaling factor convention we are assuming is\n                # quantized_value * scaling_factor ~= true_value\n                # which is consistent with the practice of setting\n                # scaling_factor = tensor_amax / FPtype_max\n                scaling_factor *= 2\n            if hasattr(layer_self_attn, \"kv_scale\"):\n                layer_self_attn.attn._kv_scale = scaling_factor\n            else:\n                raise RuntimeError(\"Self attention has no KV cache scaling \"\n                                   \"factor attribute!\")\n\n\nclass LlamaForCausalLM(nn.Module, SupportsLoRA, SupportsPP):\n    packed_modules_mapping = {\n        \"qkv_proj\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n        \"gate_up_proj\": [\"gate_proj\", \"up_proj\"]\n    }\n\n    # LoRA specific attributes\n    supported_lora_modules = [\n        \"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\", \"embed_tokens\",\n        \"lm_head\"\n    ]\n    embedding_modules = {\n        \"embed_tokens\": \"input_embeddings\",\n        \"lm_head\": \"output_embeddings\"\n    }\n    embedding_padding_modules = [\"lm_head\"]\n\n    # BitandBytes specific attributes\n    default_bitsandbytes_target_modules = [\n        \".gate_proj.\",\n        \".down_proj.\",\n        \".up_proj.\",\n        \".q_proj.\",\n        \".k_proj.\",\n        \".v_proj.\",\n        \".o_proj.\",\n    ]\n    bitsandbytes_stacked_params_mapping = {\n        # shard_name, weight_name, index\n        \"q_proj\": (\"qkv_proj\", 0),\n        \"k_proj\": (\"qkv_proj\", 1),\n        \"v_proj\": (\"qkv_proj\", 2),\n        \"gate_proj\": (\"gate_up_proj\", 0),\n        \"up_proj\": (\"gate_up_proj\", 1),\n    }\n\n    # Mistral/Llama models can also be loaded with --load-format mistral\n    # from consolidated.safetensors checkpoints\n    mistral_mapping = {\n        \"layers\": \"model.layers\",\n        \"attention\": \"self_attn\",\n        \"wq\": \"q_proj\",\n        \"wk\": \"k_proj\",\n        \"wv\": \"v_proj\",\n        \"wo\": \"o_proj\",\n        \"attention_norm\": \"input_layernorm\",\n        \"feed_forward\": \"mlp\",\n        \"w1\": \"gate_proj\",\n        \"w2\": \"down_proj\",\n        \"w3\": \"up_proj\",\n        \"ffn_norm\": \"post_attention_layernorm\",\n        \"tok_embeddings\": \"model.embed_tokens\",\n        \"output\": \"lm_head\",\n        \"norm\": \"model.norm\"\n    }\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__()\n        config = vllm_config.model_config.hf_config\n        quant_config = vllm_config.quant_config\n        lora_config = vllm_config.lora_config\n        pooler_config = vllm_config.model_config.pooler_config\n        self.config = config\n        self.lora_config = lora_config\n\n        self.model = LlamaModel(vllm_config=vllm_config,\n                                prefix=maybe_prefix(prefix, \"model\"))\n        if get_pp_group().is_last_rank:\n            self.unpadded_vocab_size = config.vocab_size\n            if lora_config:\n                self.unpadded_vocab_size += lora_config.lora_extra_vocab_size\n            self.lm_head = ParallelLMHead(\n                self.unpadded_vocab_size,\n                config.hidden_size,\n                org_num_embeddings=config.vocab_size,\n                padding_size=(\n                    DEFAULT_VOCAB_PADDING_SIZE\n                    # We need bigger padding if using lora for kernel\n                    # compatibility\n                    if not lora_config else\n                    lora_config.lora_vocab_padding_size),\n                quant_config=quant_config,\n                prefix=maybe_prefix(prefix, \"lm_head\"),\n            )\n            if config.tie_word_embeddings:\n                self.lm_head = self.lm_head.tie_weights(\n                    self.model.embed_tokens)\n\n            logit_scale = getattr(config, \"logit_scale\", 1.0)\n            self.logits_processor = LogitsProcessor(self.unpadded_vocab_size,\n                                                    config.vocab_size,\n                                                    logit_scale)\n            self.sampler = get_sampler()\n        else:\n            self.lm_head = PPMissingLayer()\n        self.make_empty_intermediate_tensors = (\n            self.model.make_empty_intermediate_tensors)\n        self._pooler = Pooler.from_config_with_defaults(\n            pooler_config,\n            pooling_type=PoolingType.STEP,\n            normalize=False,\n            softmax=False)\n\n    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:\n        return self.model.get_input_embeddings(input_ids)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        model_output = self.model(input_ids, positions, kv_caches,\n                                  attn_metadata, intermediate_tensors,\n                                  inputs_embeds)\n        return model_output\n\n    def compute_logits(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> Optional[torch.Tensor]:\n        logits = self.logits_processor(self.lm_head, hidden_states,\n                                       sampling_metadata)\n        return logits\n\n    def pooler(\n        self,\n        hidden_states: torch.Tensor,\n        pooling_metadata: PoolingMetadata,\n    ) -> Optional[PoolerOutput]:\n        logits = self.compute_logits(hidden_states, None)\n        return self._pooler(logits, pooling_metadata)\n\n    def sample(self, logits: torch.Tensor,\n               sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:\n        next_tokens = self.sampler(logits, sampling_metadata)\n        return next_tokens\n\n    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n        loader = AutoWeightsLoader(\n            self,\n            skip_prefixes=([\"lm_head.\"]\n                           if self.config.tie_word_embeddings else None),\n        )\n        loader.load_weights(\n            self.maybe_remap_mistral(name, loaded_weight)\n            for name, loaded_weight in weights)\n\n    def load_kv_cache_scales(self, quantization_param_path: str) -> None:\n        self.model.load_kv_cache_scales(quantization_param_path)\n\n    # This function is used to remap the mistral format as\n    # used by Mistral and Llama <=2\n    def maybe_remap_mistral(\n        self,\n        name: str,\n        loaded_weight: torch.Tensor,\n    ) -> Tuple[str, torch.Tensor]:\n\n        def permute(w: torch.Tensor, n_heads: int):\n            attn_in = self.config.head_dim * n_heads\n            attn_out = self.config.hidden_size\n\n            return w.view(n_heads, attn_in // n_heads // 2, 2,\n                          attn_out).transpose(1, 2).reshape(attn_in, attn_out)\n\n        mapping = self.mistral_mapping\n        modules = name.split(\".\")\n\n        # rotary embeds should be sliced\n        if \"wk\" in modules:\n            loaded_weight = permute(loaded_weight,\n                                    self.config.num_key_value_heads)\n        elif \"wq\" in modules:\n            loaded_weight = permute(loaded_weight,\n                                    self.config.num_attention_heads)\n\n        for item in modules:\n            if item in mapping and mapping[item] not in name:\n                name = name.replace(item, mapping[item])\n\n        return name, loaded_weight\n\n\nclass LlamaEmbeddingModel(nn.Module, SupportsLoRA, SupportsPP):\n    \"\"\"\n    A model that uses Llama with additional embedding functionalities.\n\n    This class encapsulates the LlamaModel and provides an interface for\n    embedding operations and customized pooling functions.\n\n    Attributes:\n        model: An instance of LlamaModel used for forward operations.\n        _pooler: An instance of Pooler used for pooling operations.\n    \"\"\"\n    packed_modules_mapping = {\n        \"qkv_proj\": [\"q_proj\", \"k_proj\", \"v_proj\"],\n        \"gate_up_proj\": [\"gate_proj\", \"up_proj\"]\n    }\n\n    # LoRA specific attributes\n    supported_lora_modules = [\n        \"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\", \"embed_tokens\"\n    ]\n    embedding_modules = {\n        \"embed_tokens\": \"input_embeddings\",\n    }\n    embedding_padding_modules = []\n\n    def __init__(self, *, vllm_config: VllmConfig, prefix: str = \"\"):\n        super().__init__()\n\n        pooler_config = vllm_config.model_config.pooler_config\n\n        self.model = LlamaModel(vllm_config=vllm_config,\n                                prefix=maybe_prefix(prefix, \"model\"))\n        self._pooler = Pooler.from_config_with_defaults(\n            pooler_config,\n            pooling_type=PoolingType.LAST,\n            normalize=True,\n            softmax=False)\n        self.make_empty_intermediate_tensors = (\n            self.model.make_empty_intermediate_tensors)\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor],\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        return self.model(input_ids, positions, kv_caches, attn_metadata,\n                          intermediate_tensors, inputs_embeds)\n\n    def pooler(\n        self,\n        hidden_states: torch.Tensor,\n        pooling_metadata: PoolingMetadata,\n    ) -> Optional[PoolerOutput]:\n        return self._pooler(hidden_states, pooling_metadata)\n\n    def load_weights(self, weights: Iterable[Tuple[str, torch.Tensor]]):\n        self.model.load_weights(weights)\n\n    def load_kv_cache_scales(self, quantization_param_path: str) -> None:\n        self.model.load_kv_cache_scales(quantization_param_path)\n\n    # LRUCacheWorkerLoRAManager instantiation requires model config.\n    @property\n    def config(self):\n        return self.model.config\n",
      "diff": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex 8aed0fead..e53631ef1 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -90,8 +90,8 @@ class LlamaMLP(nn.Module):\n         self.act_fn = SiluAndMul()\n \n     def forward(self, x):\n-        gate_up, _ = self.gate_up_proj(x)\n-        x = self.act_fn(gate_up)\n+        x, _ = self.gate_up_proj(x)\n+        x = self.act_fn(x)\n         x, _ = self.down_proj(x)\n         return x",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    }
  ],
  "affected_apis": [
    "LlamaMLP.forward"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_toy_llama)",
    "is_benchmark_actually_there": "",
    "sample_clues": "forward, llama, llamaforcausallm"
  }
}