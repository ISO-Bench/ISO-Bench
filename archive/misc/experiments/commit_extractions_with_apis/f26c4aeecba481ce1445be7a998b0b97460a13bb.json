{
  "commit_hash": "f26c4aeecba481ce1445be7a998b0b97460a13bb",
  "parent_hash": "8936316d587ca0afb5ef058584c407d404c0ffb0",
  "message": "[Misc] Optimize ray worker initialization time (#11275)\n\nSigned-off-by: Rui Qiao <ruisearch42@gmail.com>\nCo-authored-by: Cody Yu <hao.yu.cody@gmail.com>",
  "author": "Rui Qiao <161574667+ruisearch42@users.noreply.github.com>",
  "date": "2024-12-18 23:38:02 -0800",
  "files_changed": [
    {
      "file_path": "vllm/executor/ray_gpu_executor.py",
      "old_content": "import asyncio\nimport os\nfrom collections import defaultdict\nfrom itertools import islice, repeat\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n\nimport msgspec\n\nimport vllm.envs as envs\nfrom vllm.executor.distributed_gpu_executor import (  # yapf: disable\n    DistributedGPUExecutor, DistributedGPUExecutorAsync)\nfrom vllm.executor.msgspec_utils import encode_hook\nfrom vllm.executor.ray_utils import RayWorkerWrapper, ray\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.sequence import ExecuteModelRequest\nfrom vllm.utils import (_run_task_with_lock, get_distributed_init_method,\n                        get_ip, get_open_port, make_async)\n\nif ray is not None:\n    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\nlogger = init_logger(__name__)\n\n\nclass RayGPUExecutor(DistributedGPUExecutor):\n\n    uses_ray: bool = True\n\n    def _init_executor(self) -> None:\n        self.forward_dag: Optional[ray.dag.CompiledDAG] = None\n        # If the env var is set, it uses the Ray's compiled DAG API\n        # which optimizes the control plane overhead.\n        # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n        # Currently, this requires USE_RAY_SPMD_WORKER=True.\n        self.use_ray_compiled_dag = envs.VLLM_USE_RAY_COMPILED_DAG\n        # If the env var is set, then we do not distinguish between the\n        # \"driver worker\" vs other workers. Also, the rank 0 worker will\n        # be executed in a remote Ray worker. Currently this requires\n        # USE_RAY_COMPILED_DAG=True.\n        self.use_ray_spmd_worker = envs.VLLM_USE_RAY_SPMD_WORKER\n        if self.use_ray_compiled_dag:\n            assert self.use_ray_spmd_worker, (\n                \"VLLM_USE_RAY_COMPILED_DAG=1 requires \"\n                \"VLLM_USE_RAY_SPMD_WORKER=1\")\n        if self.use_ray_spmd_worker:\n            # TODO: Support SPMD worker for non-DAG Ray executor.\n            assert self.use_ray_compiled_dag, (\n                \"VLLM_USE_RAY_SPMD_WORKER=1 requires \"\n                \"VLLM_USE_RAY_COMPILED_DAG=1\")\n\n        assert self.uses_ray\n        placement_group = self.parallel_config.placement_group\n\n        # Disable Ray usage stats collection.\n        ray_usage = os.environ.get(\"RAY_USAGE_STATS_ENABLED\", \"0\")\n        if ray_usage != \"1\":\n            os.environ[\"RAY_USAGE_STATS_ENABLED\"] = \"0\"\n\n        # Create the parallel GPU workers.\n        self._init_workers_ray(placement_group)\n\n        self.input_encoder = msgspec.msgpack.Encoder(enc_hook=encode_hook)\n        self.output_decoder = msgspec.msgpack.Decoder(\n            Optional[List[SamplerOutput]])\n\n    def shutdown(self) -> None:\n        if hasattr(self, \"forward_dag\") and self.forward_dag is not None:\n            self.forward_dag.teardown()\n            import ray\n            for worker in self.workers:\n                ray.kill(worker)\n            self.forward_dag = None\n\n    def _configure_ray_workers_use_nsight(self,\n                                          ray_remote_kwargs) -> Dict[str, Any]:\n        # If nsight profiling is enabled, we need to set the profiling\n        # configuration for the ray workers as runtime env.\n        runtime_env = ray_remote_kwargs.setdefault(\"runtime_env\", {})\n        runtime_env.update({\n            \"nsight\": {\n                \"t\": \"cuda,cudnn,cublas\",\n                \"o\": \"'worker_process_%p'\",\n                \"cuda-graph-trace\": \"node\",\n            }\n        })\n\n        return ray_remote_kwargs\n\n    # child class could overwrite this to return actual env vars.\n    def _get_env_vars_to_be_updated(self):\n        return self._env_vars_for_all_workers\n\n    def _init_workers_ray(self, placement_group: \"PlacementGroup\",\n                          **ray_remote_kwargs):\n        if (self.parallel_config.tensor_parallel_size == 1\n                and self.parallel_config.pipeline_parallel_size == 1):\n            # For single GPU case, we use a ray worker with constrained memory.\n            num_gpus = self.cache_config.gpu_memory_utilization\n        else:\n            # Otherwise, the ray workers are allocated with a full GPU.\n            num_gpus = 1\n\n        # The driver dummy worker does not actually use any resources.\n        # It holds the resource for the driver worker.\n        self.driver_dummy_worker: Optional[RayWorkerWrapper] = None\n        # The remaining workers are the actual ray actors.\n        self.workers: List[RayWorkerWrapper] = []\n\n        # Used in ray compiled DAG: indexed first by PP rank,\n        # and then TP rank. In other words, the inner list is\n        # the TP group of workers for a PP rank.\n        self.pp_tp_workers: List[List[RayWorkerWrapper]] = []\n\n        if self.parallel_config.ray_workers_use_nsight:\n            ray_remote_kwargs = self._configure_ray_workers_use_nsight(\n                ray_remote_kwargs)\n\n        logger.info(\"use_ray_spmd_worker: %s\", self.use_ray_spmd_worker)\n\n        # Create the workers.\n        driver_ip = get_ip()\n        for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n            if not bundle.get(\"GPU\", 0):\n                continue\n            scheduling_strategy = PlacementGroupSchedulingStrategy(\n                placement_group=placement_group,\n                placement_group_capture_child_tasks=True,\n                placement_group_bundle_index=bundle_id,\n            )\n\n            worker = ray.remote(\n                num_cpus=0,\n                num_gpus=num_gpus,\n                scheduling_strategy=scheduling_strategy,\n                **ray_remote_kwargs,\n            )(RayWorkerWrapper).remote(vllm_config=self.vllm_config)\n\n            if self.use_ray_spmd_worker:\n                self.workers.append(worker)\n            else:\n                worker_ip = ray.get(worker.get_node_ip.remote())\n                if worker_ip == driver_ip and self.driver_dummy_worker is None:\n                    # If the worker is on the same node as the driver, we use it\n                    # as the resource holder for the driver process.\n                    self.driver_dummy_worker = worker\n                    self.driver_worker = RayWorkerWrapper(\n                        vllm_config=self.vllm_config)\n                else:\n                    # Else, added to the list of workers.\n                    self.workers.append(worker)\n\n        logger.debug(\"workers: %s\", self.workers)\n        logger.debug(\"driver_dummy_worker: %s\", self.driver_dummy_worker)\n        if not self.use_ray_spmd_worker and self.driver_dummy_worker is None:\n            raise ValueError(\n                \"Ray does not allocate any GPUs on the driver node. Consider \"\n                \"adjusting the Ray placement group or running the driver on a \"\n                \"GPU node.\")\n\n        worker_ips = [\n            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n            for worker in self.workers\n        ]\n        ip_counts: Dict[str, int] = {}\n        for ip in worker_ips:\n            ip_counts[ip] = ip_counts.get(ip, 0) + 1\n\n        def sort_by_driver_then_worker_ip(worker):\n            \"\"\"\n            Sort the workers based on 3 properties:\n            1. If the worker is on the same node as the driver (vllm engine),\n                it should be placed first.\n            2. Then, if the worker is on a node with fewer workers, it should\n                be placed first.\n            3. Finally, if the work is on a node with smaller IP address, it\n                should be placed first.\n            \"\"\"\n            ip = ray.get(worker.get_node_ip.remote())\n            return (ip != driver_ip, ip_counts[ip], ip)\n\n        # After sorting, the workers on the same node will be\n        # close to each other, and the workers on the driver\n        # node will be placed first.\n        self.workers = sorted(self.workers, key=sort_by_driver_then_worker_ip)\n\n        # Get the set of GPU IDs used on each node.\n        worker_node_and_gpu_ids = []\n        for worker in [self.driver_dummy_worker] + self.workers:\n            if worker is None:\n                # driver_dummy_worker can be None when using ray spmd worker.\n                continue\n            worker_node_and_gpu_ids.append(\n                ray.get(worker.get_node_and_gpu_ids.remote()) \\\n            ) # type: ignore\n\n        node_workers = defaultdict(list)  # node id -> list of worker ranks\n        node_gpus = defaultdict(list)  # node id -> list of gpu ids\n\n        for i, (node_id, gpu_ids) in enumerate(worker_node_and_gpu_ids):\n            node_workers[node_id].append(i)\n            # `gpu_ids` can be a list of strings or integers.\n            # convert them to integers for consistency.\n            # NOTE: gpu_ids can be larger than 9 (e.g. 16 GPUs),\n            # string sorting is not sufficient.\n            # see https://github.com/vllm-project/vllm/issues/5590\n            gpu_ids = [int(x) for x in gpu_ids]\n            node_gpus[node_id].extend(gpu_ids)\n        for node_id, gpu_ids in node_gpus.items():\n            node_gpus[node_id] = sorted(gpu_ids)\n\n        all_ips = set(worker_ips + [driver_ip])\n        n_ips = len(all_ips)\n        n_nodes = len(node_workers)\n\n        if n_nodes != n_ips:\n            raise RuntimeError(\n                f\"Every node should have a unique IP address. Got {n_nodes}\"\n                f\" nodes with node ids {list(node_workers.keys())} and \"\n                f\"{n_ips} unique IP addresses {all_ips}. Please check your\"\n                \" network configuration. If you set `VLLM_HOST_IP`\"\n                \" environment variable, make sure it is unique for\"\n                \" each node.\")\n\n        # Set environment variables for the driver and workers.\n        all_args_to_update_environment_variables = [({\n            \"CUDA_VISIBLE_DEVICES\":\n            \",\".join(map(str, node_gpus[node_id])),\n            \"VLLM_TRACE_FUNCTION\":\n            str(envs.VLLM_TRACE_FUNCTION),\n            **({\n                \"VLLM_ATTENTION_BACKEND\": envs.VLLM_ATTENTION_BACKEND\n            } if envs.VLLM_ATTENTION_BACKEND is not None else {})\n        }, ) for (node_id, _) in worker_node_and_gpu_ids]\n\n        self._env_vars_for_all_workers = (\n            all_args_to_update_environment_variables)\n\n        self._run_workers(\"update_environment_variables\",\n                          all_args=self._get_env_vars_to_be_updated())\n\n        if len(node_gpus) == 1:\n            # in single node case, we don't need to get the IP address.\n            # the loopback address is sufficient\n            # NOTE: a node may have several IP addresses, one for each\n            # network interface. `get_ip()` might return any of them,\n            # while they might not work for communication inside the node\n            # if the network setup is complicated. Using the loopback address\n            # solves this issue, as it always works for communication inside\n            # the node.\n            driver_ip = \"127.0.0.1\"\n        distributed_init_method = get_distributed_init_method(\n            driver_ip, get_open_port())\n\n        # Initialize the actual workers inside worker wrapper.\n        init_worker_all_kwargs = [\n            self._get_worker_kwargs(\n                local_rank=node_workers[node_id].index(rank),\n                rank=rank,\n                distributed_init_method=distributed_init_method,\n            ) for rank, (node_id, _) in enumerate(worker_node_and_gpu_ids)\n        ]\n        self._run_workers(\"init_worker\", all_kwargs=init_worker_all_kwargs)\n\n        self._run_workers(\"init_device\")\n        self._run_workers(\"load_model\",\n                          max_concurrent_workers=self.parallel_config.\n                          max_parallel_loading_workers)\n\n        if self.use_ray_spmd_worker:\n            for pp_rank in range(self.parallel_config.pipeline_parallel_size):\n                self.pp_tp_workers.append([])\n                for tp_rank in range(\n                        self.parallel_config.tensor_parallel_size):\n                    # PP=2, TP=4\n                    # pp_tp_workers = [[0, 1, 2, 3], [4, 5, 6, 7]]\n                    rank = (pp_rank * self.parallel_config.tensor_parallel_size\n                            ) + tp_rank\n                    assert len(self.pp_tp_workers[pp_rank]) == tp_rank\n                    assert pp_rank < len(self.pp_tp_workers)\n                    self.pp_tp_workers[pp_rank].append(self.workers[rank])\n\n        # This is the list of workers that are rank 0 of each TP group EXCEPT\n        # global rank 0. These are the workers that will broadcast to the\n        # rest of the workers.\n        self.tp_driver_workers: List[RayWorkerWrapper] = []\n        # This is the list of workers that are not drivers and not the first\n        # worker in a TP group. These are the workers that will be\n        # broadcasted to.\n        self.non_driver_workers: List[RayWorkerWrapper] = []\n\n        # Enforce rank order for correct rank to return final output.\n        for index, worker in enumerate(self.workers):\n            # The driver worker is rank 0 and not in self.workers.\n            rank = index + 1\n            if rank % self.parallel_config.tensor_parallel_size == 0:\n                self.tp_driver_workers.append(worker)\n            else:\n                self.non_driver_workers.append(worker)\n\n    def _driver_execute_model(\n        self, execute_model_req: Optional[ExecuteModelRequest]\n    ) -> Optional[List[SamplerOutput]]:\n        \"\"\"Run execute_model in the driver worker.\n\n        Passing None will cause the driver to stop the model execution\n        loop running in each of the remote workers.\n        \"\"\"\n        assert not self.use_ray_spmd_worker, (\n            \"driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1\")\n        return self.driver_worker.execute_method(\"execute_model\",\n                                                 execute_model_req)\n\n    def execute_model(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        if not self.use_ray_spmd_worker:\n            return super().execute_model(execute_model_req)\n\n        if self.forward_dag is None:\n            self.forward_dag = self._compiled_ray_dag(enable_asyncio=False)\n\n        serialized_data = self.input_encoder.encode(execute_model_req)\n        outputs = ray.get(self.forward_dag.execute(serialized_data))\n        output = self.output_decoder.decode(outputs[0])\n        return output\n\n    def _run_workers(\n        self,\n        method: str,\n        *args,\n        async_run_tensor_parallel_workers_only: bool = False,\n        all_args: Optional[List[Tuple[Any, ...]]] = None,\n        all_kwargs: Optional[List[Dict[str, Any]]] = None,\n        max_concurrent_workers: Optional[int] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers. Can be used in the following\n        ways:\n\n        Args:\n        - async_run_tensor_parallel_workers_only: If True the method will be\n          run only in the remote TP workers, not the driver worker.\n          It will also be run asynchronously and return a list of futures\n          rather than blocking on the results.\n        - args/kwargs: All workers share the same args/kwargs\n        - all_args/all_kwargs: args/kwargs for each worker are specified\n          individually\n        \"\"\"\n        if self.use_ray_spmd_worker:\n            assert not async_run_tensor_parallel_workers_only, (\n                \"async_run_tensor_parallel_workers_only is not supported for \"\n                \"spmd mode.\")\n\n        if max_concurrent_workers:\n            raise NotImplementedError(\n                \"max_concurrent_workers is not supported yet.\")\n\n        count = len(self.workers) if not \\\n            async_run_tensor_parallel_workers_only \\\n            else len(self.non_driver_workers)\n        # If using SPMD worker, all workers are the same, so we should execute\n        # the args on all workers. Otherwise, we skip the first worker's args\n        # because those args will go to the driver worker.\n        first_worker_args_index: int = 0 if self.use_ray_spmd_worker else 1\n        all_worker_args = repeat(args, count) if all_args is None \\\n            else islice(all_args, first_worker_args_index, None)\n        all_worker_kwargs = repeat(kwargs, count) if all_kwargs is None \\\n            else islice(all_kwargs, first_worker_args_index, None)\n\n        # Start the ray workers first.\n        ray_workers = self.workers\n        if async_run_tensor_parallel_workers_only:\n            ray_workers = self.non_driver_workers\n        ray_worker_outputs = [\n            worker.execute_method.remote(method, *worker_args, **worker_kwargs)\n            for (worker, worker_args, worker_kwargs\n                 ) in zip(ray_workers, all_worker_args, all_worker_kwargs)\n        ]\n\n        if async_run_tensor_parallel_workers_only:\n            # Just return futures\n            return ray_worker_outputs\n\n        driver_worker_output = []\n        # In SPMD mode, the driver worker is the same as any other worker,\n        # so we only explicitly execute on the driver worker if using a\n        # non-SPMD worker class.\n        if not self.use_ray_spmd_worker:\n            driver_args = args if all_args is None else all_args[0]\n            driver_kwargs = kwargs if all_kwargs is None else all_kwargs[0]\n\n            # Start the driver worker after all the ray workers.\n            driver_worker_output = [\n                self.driver_worker.execute_method(method, *driver_args,\n                                                  **driver_kwargs)\n            ]\n\n        # Get the results of the ray workers.\n        if self.workers:\n            ray_worker_outputs = ray.get(ray_worker_outputs)\n\n        return driver_worker_output + ray_worker_outputs\n\n    def _wait_for_tasks_completion(self, parallel_worker_tasks: Any) -> None:\n        \"\"\"Wait for futures returned from _run_workers() with\n        async_run_remote_workers_only to complete.\"\"\"\n        ray.get(parallel_worker_tasks)\n\n    def _check_ray_adag_installation(self):\n        import pkg_resources\n        from packaging import version\n\n        required_version = version.parse(\"2.40\")\n        current_version = version.parse(\n            pkg_resources.get_distribution(\"ray\").version)\n        if current_version < required_version:\n            raise ValueError(f\"Ray version {required_version} is \"\n                             f\"required, but found {current_version}\")\n\n        import importlib.util\n        adag_spec = importlib.util.find_spec(\n            \"ray.experimental.compiled_dag_ref\")\n        if adag_spec is None:\n            raise ValueError(\"Ray accelerated DAG is not installed. \"\n                             \"Run `pip install ray[adag]` to install it.\")\n\n        cupy_spec = importlib.util.find_spec(\"cupy\")\n        if cupy_spec is None and envs.VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL:\n            raise ValueError(\n                \"cupy is not installed but required since \"\n                \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL is set.\"\n                \"Run `pip install ray[adag]` and check cupy installation.\")\n\n    def _compiled_ray_dag(self, enable_asyncio: bool):\n        assert self.parallel_config.use_ray\n        self._check_ray_adag_installation()\n        from ray.dag import InputNode, MultiOutputNode\n        from ray.experimental.channel.torch_tensor_type import TorchTensorType\n\n        logger.info(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL = %s\",\n                    envs.VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL)\n        logger.info(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = %s\",\n                    envs.VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM)\n        with InputNode() as input_data:\n            # Example DAG: PP=2, TP=4\n            # (ExecuteModelReq, None) -> 0 -> (ExecuteModelReq, IntermediateOutput) -> 4 -> SamplerOutput   # noqa: E501\n            #                         -> 1 -> (ExecuteModelReq, IntermediateOutput) -> 5 -> SamplerOutput   # noqa: E501\n            #                         -> 2 -> (ExecuteModelReq, IntermediateOutput) -> 6 -> SamplerOutput   # noqa: E501\n            #                         -> 3 -> (ExecuteModelReq, IntermediateOutput) -> 7 -> SamplerOutput   # noqa: E501\n\n            # All workers in the first TP group will take in the\n            # ExecuteModelRequest as input.\n            outputs = [input_data for _ in self.pp_tp_workers[0]]\n            for pp_rank, tp_group in enumerate(self.pp_tp_workers):\n                # Each PP worker takes in the output of the previous PP worker,\n                # and the TP group executes in SPMD fashion.\n                outputs = [\n                    worker.execute_model_spmd.\n                    bind(  # type: ignore[attr-defined]\n                        outputs[i]) for i, worker in enumerate(tp_group)\n                ]\n\n                last_pp_rank = len(self.pp_tp_workers) - 1\n                if pp_rank < last_pp_rank:\n                    # Specify how intermediate tensors should be passed\n                    # between pp stages, no need to specify for the last\n                    # pp stage.\n                    transport = \"nccl\" \\\n                        if envs.VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL \\\n                        else \"auto\"\n                    outputs = [\n                        output.with_type_hint(\n                            TorchTensorType(transport=transport))\n                        for output in outputs\n                    ]\n\n            forward_dag = MultiOutputNode(outputs)\n\n        return forward_dag.experimental_compile(\n            enable_asyncio=enable_asyncio,\n            _overlap_gpu_communication=envs.\n            VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM)\n\n    def __del__(self):\n        self.shutdown()\n\n\nclass RayGPUExecutorAsync(RayGPUExecutor, DistributedGPUExecutorAsync):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pp_locks: Optional[List[asyncio.Lock]] = None\n        self.use_ray_spmd_worker = envs.VLLM_USE_RAY_SPMD_WORKER\n        if not self.use_ray_compiled_dag:\n            self.driver_exec_method = make_async(\n                self.driver_worker.execute_method)\n\n    async def execute_model_async(\n            self,\n            execute_model_req: ExecuteModelRequest) -> List[SamplerOutput]:\n        if not self.use_ray_spmd_worker:\n            return await super().execute_model_async(execute_model_req)\n\n        if self.forward_dag is None:\n            self.forward_dag = self._compiled_ray_dag(enable_asyncio=True)\n\n        serialized_data = self.input_encoder.encode(execute_model_req)\n        dag_future = await self.forward_dag.execute_async(serialized_data)\n        output = await dag_future[0]\n        return self.output_decoder.decode(output)\n\n    async def _driver_execute_model_async(\n        self,\n        execute_model_req: Optional[ExecuteModelRequest] = None\n    ) -> List[SamplerOutput]:\n        assert not self.use_ray_spmd_worker, (\n            \"driver_worker does not exist for VLLM_USE_RAY_SPMD_WORKER=1\")\n        if not self.tp_driver_workers:\n            return await self.driver_exec_method(\"execute_model\",\n                                                 execute_model_req)\n        if self.pp_locks is None:\n            # This locks each pipeline parallel stage so multiple virtual\n            # engines can't execute on the same stage at the same time\n            # We create the locks here to avoid creating them in the constructor\n            # which uses a different asyncio loop.\n            self.pp_locks = [\n                asyncio.Lock()\n                for _ in range(self.parallel_config.pipeline_parallel_size)\n            ]\n\n        tasks = [\n            asyncio.create_task(\n                _run_task_with_lock(self.driver_exec_method, self.pp_locks[0],\n                                    \"execute_model\", execute_model_req))\n        ]\n        for pp_rank, driver_worker in enumerate(self.tp_driver_workers,\n                                                start=1):\n            tasks.append(\n                asyncio.create_task(\n                    _run_task_with_lock(driver_worker.execute_method.remote,\n                                        self.pp_locks[pp_rank],\n                                        \"execute_model\", execute_model_req)))\n\n        results = await asyncio.gather(*tasks)\n\n        # Only the last PP stage has the final results.\n        return results[-1]\n\n    async def _start_worker_execution_loop(self):\n        assert not self.use_ray_spmd_worker, (\n            \"worker loop is disabled for VLLM_USE_RAY_SPMD_WORKER=1\")\n        coros = [\n            worker.execute_method.remote(\"start_worker_execution_loop\")\n            for worker in self.non_driver_workers\n        ]\n        return await asyncio.gather(*coros)\n\n    def __del__(self):\n        self.shutdown()\n",
      "diff": "diff --git a/vllm/executor/ray_gpu_executor.py b/vllm/executor/ray_gpu_executor.py\nindex 4bf5cbbd1..e2c549cbd 100644\n--- a/vllm/executor/ray_gpu_executor.py\n+++ b/vllm/executor/ray_gpu_executor.py\n@@ -123,6 +123,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n \n         # Create the workers.\n         driver_ip = get_ip()\n+        workers = []\n         for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n             if not bundle.get(\"GPU\", 0):\n                 continue\n@@ -138,20 +139,30 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 scheduling_strategy=scheduling_strategy,\n                 **ray_remote_kwargs,\n             )(RayWorkerWrapper).remote(vllm_config=self.vllm_config)\n+            workers.append(worker)\n \n-            if self.use_ray_spmd_worker:\n-                self.workers.append(worker)\n-            else:\n-                worker_ip = ray.get(worker.get_node_ip.remote())\n-                if worker_ip == driver_ip and self.driver_dummy_worker is None:\n+        worker_ip_refs = [\n+            worker.get_node_ip.remote()  # type: ignore[attr-defined]\n+            for worker in workers\n+        ]\n+        worker_ips = ray.get(worker_ip_refs)\n+\n+        if not self.use_ray_spmd_worker:\n+            for i in range(len(workers)):\n+                worker = workers[i]\n+                worker_ip = worker_ips[i]\n+                if self.driver_dummy_worker is None and worker_ip == driver_ip:\n                     # If the worker is on the same node as the driver, we use it\n                     # as the resource holder for the driver process.\n                     self.driver_dummy_worker = worker\n                     self.driver_worker = RayWorkerWrapper(\n                         vllm_config=self.vllm_config)\n-                else:\n-                    # Else, added to the list of workers.\n-                    self.workers.append(worker)\n+                    workers.pop(i)\n+                    worker_ips.pop(i)\n+                    self.workers = workers\n+                    break\n+        else:\n+            self.workers = workers\n \n         logger.debug(\"workers: %s\", self.workers)\n         logger.debug(\"driver_dummy_worker: %s\", self.driver_dummy_worker)\n@@ -161,14 +172,12 @@ class RayGPUExecutor(DistributedGPUExecutor):\n                 \"adjusting the Ray placement group or running the driver on a \"\n                 \"GPU node.\")\n \n-        worker_ips = [\n-            ray.get(worker.get_node_ip.remote())  # type: ignore[attr-defined]\n-            for worker in self.workers\n-        ]\n         ip_counts: Dict[str, int] = {}\n         for ip in worker_ips:\n             ip_counts[ip] = ip_counts.get(ip, 0) + 1\n \n+        worker_to_ip = dict(zip(self.workers, worker_ips))\n+\n         def sort_by_driver_then_worker_ip(worker):\n             \"\"\"\n             Sort the workers based on 3 properties:\n@@ -179,7 +188,7 @@ class RayGPUExecutor(DistributedGPUExecutor):\n             3. Finally, if the work is on a node with smaller IP address, it\n                 should be placed first.\n             \"\"\"\n-            ip = ray.get(worker.get_node_ip.remote())\n+            ip = worker_to_ip[worker]\n             return (ip != driver_ip, ip_counts[ip], ip)\n \n         # After sorting, the workers on the same node will be",
      "change_type": "modified",
      "lines_added": 23,
      "lines_removed": 14
    }
  ],
  "affected_apis": [
    "RayGPUExecutor"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "NO (ray)",
    "is_benchmark_actually_there": "",
    "sample_clues": "none, ray_gpu_executor"
  }
}