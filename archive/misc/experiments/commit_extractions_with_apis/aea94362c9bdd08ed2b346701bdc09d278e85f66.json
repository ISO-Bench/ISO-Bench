{
  "commit_hash": "aea94362c9bdd08ed2b346701bdc09d278e85f66",
  "parent_hash": "7206ce4ce112ed117796a59045c968a6d353f691",
  "message": "[Frontend][V1] Online serving performance improvements (#12287)",
  "author": "Nick Hill <nickhill@us.ibm.com>",
  "date": "2025-01-22 22:22:12 +0000",
  "files_changed": [
    {
      "file_path": "vllm/entrypoints/openai/api_server.py",
      "old_content": "import asyncio\nimport atexit\nimport importlib\nimport inspect\nimport multiprocessing\nimport os\nimport re\nimport signal\nimport socket\nimport sys\nimport tempfile\nimport uuid\nfrom argparse import Namespace\nfrom contextlib import asynccontextmanager\nfrom functools import partial\nfrom http import HTTPStatus\nfrom typing import AsyncIterator, Dict, Optional, Set, Tuple, Union\n\nimport uvloop\nfrom fastapi import APIRouter, FastAPI, HTTPException, Request\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse, Response, StreamingResponse\nfrom starlette.datastructures import State\nfrom starlette.routing import Mount\nfrom typing_extensions import assert_never\n\nimport vllm.envs as envs\nfrom vllm.config import ModelConfig\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine  # type: ignore\nfrom vllm.engine.multiprocessing.client import MQLLMEngineClient\nfrom vllm.engine.multiprocessing.engine import run_mp_engine\nfrom vllm.engine.protocol import EngineClient\nfrom vllm.entrypoints.chat_utils import load_chat_template\nfrom vllm.entrypoints.launcher import serve_http\nfrom vllm.entrypoints.logger import RequestLogger\nfrom vllm.entrypoints.openai.cli_args import (make_arg_parser,\n                                              validate_parsed_serve_args)\n# yapf conflicts with isort for this block\n# yapf: disable\nfrom vllm.entrypoints.openai.protocol import (ChatCompletionRequest,\n                                              ChatCompletionResponse,\n                                              CompletionRequest,\n                                              CompletionResponse,\n                                              DetokenizeRequest,\n                                              DetokenizeResponse,\n                                              EmbeddingChatRequest,\n                                              EmbeddingCompletionRequest,\n                                              EmbeddingRequest,\n                                              EmbeddingResponse,\n                                              EmbeddingResponseData,\n                                              ErrorResponse,\n                                              LoadLoraAdapterRequest,\n                                              PoolingChatRequest,\n                                              PoolingCompletionRequest,\n                                              PoolingRequest, PoolingResponse,\n                                              ScoreRequest, ScoreResponse,\n                                              TokenizeRequest,\n                                              TokenizeResponse,\n                                              UnloadLoraAdapterRequest)\n# yapf: enable\nfrom vllm.entrypoints.openai.serving_chat import OpenAIServingChat\nfrom vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion\nfrom vllm.entrypoints.openai.serving_embedding import OpenAIServingEmbedding\nfrom vllm.entrypoints.openai.serving_engine import OpenAIServing\nfrom vllm.entrypoints.openai.serving_models import (BaseModelPath,\n                                                    OpenAIServingModels)\nfrom vllm.entrypoints.openai.serving_pooling import OpenAIServingPooling\nfrom vllm.entrypoints.openai.serving_score import OpenAIServingScores\nfrom vllm.entrypoints.openai.serving_tokenization import (\n    OpenAIServingTokenization)\nfrom vllm.entrypoints.openai.tool_parsers import ToolParserManager\nfrom vllm.entrypoints.utils import with_cancellation\nfrom vllm.logger import init_logger\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import (FlexibleArgumentParser, get_open_zmq_ipc_path,\n                        is_valid_ipv6_address, set_ulimit)\nfrom vllm.version import __version__ as VLLM_VERSION\n\nTIMEOUT_KEEP_ALIVE = 5  # seconds\n\nprometheus_multiproc_dir: tempfile.TemporaryDirectory\n\n# Cannot use __name__ (https://github.com/vllm-project/vllm/pull/4765)\nlogger = init_logger('vllm.entrypoints.openai.api_server')\n\n_running_tasks: Set[asyncio.Task] = set()\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    try:\n        if app.state.log_stats:\n            engine_client: EngineClient = app.state.engine_client\n\n            async def _force_log():\n                while True:\n                    await asyncio.sleep(10.)\n                    await engine_client.do_log_stats()\n\n            task = asyncio.create_task(_force_log())\n            _running_tasks.add(task)\n            task.add_done_callback(_running_tasks.remove)\n        else:\n            task = None\n        try:\n            yield\n        finally:\n            if task is not None:\n                task.cancel()\n    finally:\n        # Ensure app state including engine ref is gc'd\n        del app.state\n\n\n@asynccontextmanager\nasync def build_async_engine_client(\n        args: Namespace) -> AsyncIterator[EngineClient]:\n\n    # Context manager to handle engine_client lifecycle\n    # Ensures everything is shutdown and cleaned up on error/exit\n    engine_args = AsyncEngineArgs.from_cli_args(args)\n\n    async with build_async_engine_client_from_engine_args(\n            engine_args, args.disable_frontend_multiprocessing) as engine:\n        yield engine\n\n\n@asynccontextmanager\nasync def build_async_engine_client_from_engine_args(\n    engine_args: AsyncEngineArgs,\n    disable_frontend_multiprocessing: bool = False,\n) -> AsyncIterator[EngineClient]:\n    \"\"\"\n    Create EngineClient, either:\n        - in-process using the AsyncLLMEngine Directly\n        - multiprocess using AsyncLLMEngine RPC\n\n    Returns the Client or None if the creation failed.\n    \"\"\"\n\n    # AsyncLLMEngine.\n    if (MQLLMEngineClient.is_unsupported_config(engine_args)\n            or envs.VLLM_USE_V1 or disable_frontend_multiprocessing):\n\n        engine_client: Optional[EngineClient] = None\n        try:\n            engine_client = AsyncLLMEngine.from_engine_args(\n                engine_args=engine_args,\n                usage_context=UsageContext.OPENAI_API_SERVER)\n            yield engine_client\n        finally:\n            if engine_client and hasattr(engine_client, \"shutdown\"):\n                engine_client.shutdown()\n\n    # MQLLMEngine.\n    else:\n        if \"PROMETHEUS_MULTIPROC_DIR\" not in os.environ:\n            # Make TemporaryDirectory for prometheus multiprocessing\n            # Note: global TemporaryDirectory will be automatically\n            #   cleaned up upon exit.\n            global prometheus_multiproc_dir\n            prometheus_multiproc_dir = tempfile.TemporaryDirectory()\n            os.environ[\n                \"PROMETHEUS_MULTIPROC_DIR\"] = prometheus_multiproc_dir.name\n        else:\n            logger.warning(\n                \"Found PROMETHEUS_MULTIPROC_DIR was set by user. \"\n                \"This directory must be wiped between vLLM runs or \"\n                \"you will find inaccurate metrics. Unset the variable \"\n                \"and vLLM will properly handle cleanup.\")\n\n        # Select random path for IPC.\n        ipc_path = get_open_zmq_ipc_path()\n        logger.debug(\"Multiprocessing frontend to use %s for IPC Path.\",\n                     ipc_path)\n\n        # Start RPCServer in separate process (holds the LLMEngine).\n        # the current process might have CUDA context,\n        # so we need to spawn a new process\n        context = multiprocessing.get_context(\"spawn\")\n\n        # The Process can raise an exception during startup, which may\n        # not actually result in an exitcode being reported. As a result\n        # we use a shared variable to communicate the information.\n        engine_alive = multiprocessing.Value('b', True, lock=False)\n        engine_process = context.Process(target=run_mp_engine,\n                                         args=(engine_args,\n                                               UsageContext.OPENAI_API_SERVER,\n                                               ipc_path, engine_alive))\n        engine_process.start()\n        engine_pid = engine_process.pid\n        assert engine_pid is not None, \"Engine process failed to start.\"\n        logger.info(\"Started engine process with PID %d\", engine_pid)\n\n        def _cleanup_ipc_path():\n            socket_path = ipc_path.replace(\"ipc://\", \"\")\n            if os.path.exists(socket_path):\n                os.remove(socket_path)\n\n        # Ensure we clean up the local IPC socket file on exit.\n        atexit.register(_cleanup_ipc_path)\n\n        # Build RPCClient, which conforms to EngineClient Protocol.\n        engine_config = engine_args.create_engine_config()\n        build_client = partial(MQLLMEngineClient, ipc_path, engine_config,\n                               engine_pid)\n        mq_engine_client = await asyncio.get_running_loop().run_in_executor(\n            None, build_client)\n        try:\n            while True:\n                try:\n                    await mq_engine_client.setup()\n                    break\n                except TimeoutError:\n                    if (not engine_process.is_alive()\n                            or not engine_alive.value):\n                        raise RuntimeError(\n                            \"Engine process failed to start. See stack \"\n                            \"trace for the root cause.\") from None\n\n            yield mq_engine_client  # type: ignore[misc]\n        finally:\n            # Ensure rpc server process was terminated\n            engine_process.terminate()\n\n            # Close all open connections to the backend\n            mq_engine_client.close()\n\n            # Wait for engine process to join\n            engine_process.join(4)\n            if engine_process.exitcode is None:\n                # Kill if taking longer than 5 seconds to stop\n                engine_process.kill()\n\n            # Lazy import for prometheus multiprocessing.\n            # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n            # before prometheus_client is imported.\n            # See https://prometheus.github.io/client_python/multiprocess/\n            from prometheus_client import multiprocess\n            multiprocess.mark_process_dead(engine_process.pid)\n\n\nrouter = APIRouter()\n\n\ndef mount_metrics(app: FastAPI):\n    # Lazy import for prometheus multiprocessing.\n    # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n    # before prometheus_client is imported.\n    # See https://prometheus.github.io/client_python/multiprocess/\n    from prometheus_client import (CollectorRegistry, make_asgi_app,\n                                   multiprocess)\n\n    prometheus_multiproc_dir_path = os.getenv(\"PROMETHEUS_MULTIPROC_DIR\", None)\n    if prometheus_multiproc_dir_path is not None:\n        logger.debug(\"vLLM to use %s as PROMETHEUS_MULTIPROC_DIR\",\n                     prometheus_multiproc_dir_path)\n        registry = CollectorRegistry()\n        multiprocess.MultiProcessCollector(registry)\n\n        # Add prometheus asgi middleware to route /metrics requests\n        metrics_route = Mount(\"/metrics\", make_asgi_app(registry=registry))\n    else:\n        # Add prometheus asgi middleware to route /metrics requests\n        metrics_route = Mount(\"/metrics\", make_asgi_app())\n\n    # Workaround for 307 Redirect for /metrics\n    metrics_route.path_regex = re.compile(\"^/metrics(?P<path>.*)$\")\n    app.routes.append(metrics_route)\n\n\ndef base(request: Request) -> OpenAIServing:\n    # Reuse the existing instance\n    return tokenization(request)\n\n\ndef models(request: Request) -> OpenAIServingModels:\n    return request.app.state.openai_serving_models\n\n\ndef chat(request: Request) -> Optional[OpenAIServingChat]:\n    return request.app.state.openai_serving_chat\n\n\ndef completion(request: Request) -> Optional[OpenAIServingCompletion]:\n    return request.app.state.openai_serving_completion\n\n\ndef pooling(request: Request) -> Optional[OpenAIServingPooling]:\n    return request.app.state.openai_serving_pooling\n\n\ndef embedding(request: Request) -> Optional[OpenAIServingEmbedding]:\n    return request.app.state.openai_serving_embedding\n\n\ndef score(request: Request) -> Optional[OpenAIServingScores]:\n    return request.app.state.openai_serving_scores\n\n\ndef tokenization(request: Request) -> OpenAIServingTokenization:\n    return request.app.state.openai_serving_tokenization\n\n\ndef engine_client(request: Request) -> EngineClient:\n    return request.app.state.engine_client\n\n\n@router.get(\"/health\")\nasync def health(raw_request: Request) -> Response:\n    \"\"\"Health check.\"\"\"\n    await engine_client(raw_request).check_health()\n    return Response(status_code=200)\n\n\n@router.api_route(\"/ping\", methods=[\"GET\", \"POST\"])\nasync def ping(raw_request: Request) -> Response:\n    \"\"\"Ping check. Endpoint required for SageMaker\"\"\"\n    return await health(raw_request)\n\n\n@router.post(\"/tokenize\")\n@with_cancellation\nasync def tokenize(request: TokenizeRequest, raw_request: Request):\n    handler = tokenization(raw_request)\n\n    generator = await handler.create_tokenize(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, TokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/detokenize\")\n@with_cancellation\nasync def detokenize(request: DetokenizeRequest, raw_request: Request):\n    handler = tokenization(raw_request)\n\n    generator = await handler.create_detokenize(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, DetokenizeResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.get(\"/v1/models\")\nasync def show_available_models(raw_request: Request):\n    handler = models(raw_request)\n\n    models_ = await handler.show_available_models()\n    return JSONResponse(content=models_.model_dump())\n\n\n@router.get(\"/version\")\nasync def show_version():\n    ver = {\"version\": VLLM_VERSION}\n    return JSONResponse(content=ver)\n\n\n@router.post(\"/v1/chat/completions\")\n@with_cancellation\nasync def create_chat_completion(request: ChatCompletionRequest,\n                                 raw_request: Request):\n    handler = chat(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Chat Completions API\")\n\n    generator = await handler.create_chat_completion(request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n\n    elif isinstance(generator, ChatCompletionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/completions\")\n@with_cancellation\nasync def create_completion(request: CompletionRequest, raw_request: Request):\n    handler = completion(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Completions API\")\n\n    generator = await handler.create_completion(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, CompletionResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    return StreamingResponse(content=generator, media_type=\"text/event-stream\")\n\n\n@router.post(\"/v1/embeddings\")\n@with_cancellation\nasync def create_embedding(request: EmbeddingRequest, raw_request: Request):\n    handler = embedding(raw_request)\n    if handler is None:\n        fallback_handler = pooling(raw_request)\n        if fallback_handler is None:\n            return base(raw_request).create_error_response(\n                message=\"The model does not support Embeddings API\")\n\n        logger.warning(\n            \"Embeddings API will become exclusive to embedding models \"\n            \"in a future release. To return the hidden states directly, \"\n            \"use the Pooling API (`/pooling`) instead.\")\n\n        res = await fallback_handler.create_pooling(request, raw_request)\n\n        generator: Union[ErrorResponse, EmbeddingResponse]\n        if isinstance(res, PoolingResponse):\n            generator = EmbeddingResponse(\n                id=res.id,\n                object=res.object,\n                created=res.created,\n                model=res.model,\n                data=[\n                    EmbeddingResponseData(\n                        index=d.index,\n                        embedding=d.data,  # type: ignore\n                    ) for d in res.data\n                ],\n                usage=res.usage,\n            )\n        else:\n            generator = res\n    else:\n        generator = await handler.create_embedding(request, raw_request)\n\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, EmbeddingResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/pooling\")\n@with_cancellation\nasync def create_pooling(request: PoolingRequest, raw_request: Request):\n    handler = pooling(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Pooling API\")\n\n    generator = await handler.create_pooling(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, PoolingResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/score\")\n@with_cancellation\nasync def create_score(request: ScoreRequest, raw_request: Request):\n    handler = score(raw_request)\n    if handler is None:\n        return base(raw_request).create_error_response(\n            message=\"The model does not support Score API\")\n\n    generator = await handler.create_score(request, raw_request)\n    if isinstance(generator, ErrorResponse):\n        return JSONResponse(content=generator.model_dump(),\n                            status_code=generator.code)\n    elif isinstance(generator, ScoreResponse):\n        return JSONResponse(content=generator.model_dump())\n\n    assert_never(generator)\n\n\n@router.post(\"/v1/score\")\n@with_cancellation\nasync def create_score_v1(request: ScoreRequest, raw_request: Request):\n    logger.warning(\n        \"To indicate that Score API is not part of standard OpenAI API, we \"\n        \"have moved it to `/score`. Please update your client accordingly.\")\n\n    return await create_score(request, raw_request)\n\n\nTASK_HANDLERS: Dict[str, Dict[str, tuple]] = {\n    \"generate\": {\n        \"messages\": (ChatCompletionRequest, create_chat_completion),\n        \"default\": (CompletionRequest, create_completion),\n    },\n    \"embed\": {\n        \"messages\": (EmbeddingChatRequest, create_embedding),\n        \"default\": (EmbeddingCompletionRequest, create_embedding),\n    },\n    \"score\": {\n        \"default\": (ScoreRequest, create_score),\n    },\n    \"reward\": {\n        \"messages\": (PoolingChatRequest, create_pooling),\n        \"default\": (PoolingCompletionRequest, create_pooling),\n    },\n    \"classify\": {\n        \"messages\": (PoolingChatRequest, create_pooling),\n        \"default\": (PoolingCompletionRequest, create_pooling),\n    },\n}\n\nif envs.VLLM_SERVER_DEV_MODE:\n\n    @router.post(\"/reset_prefix_cache\")\n    async def reset_prefix_cache(raw_request: Request):\n        \"\"\"\n        Reset the prefix cache. Note that we currently do not check if the\n        prefix cache is successfully reset in the API server.\n        \"\"\"\n        logger.info(\"Resetting prefix cache...\")\n        await engine_client(raw_request).reset_prefix_cache()\n        return Response(status_code=200)\n\n\n@router.post(\"/invocations\")\nasync def invocations(raw_request: Request):\n    \"\"\"\n    For SageMaker, routes requests to other handlers based on model `task`.\n    \"\"\"\n    body = await raw_request.json()\n    task = raw_request.app.state.task\n\n    if task not in TASK_HANDLERS:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Unsupported task: '{task}' for '/invocations'. \"\n            f\"Expected one of {set(TASK_HANDLERS.keys())}\")\n\n    handler_config = TASK_HANDLERS[task]\n    if \"messages\" in body:\n        request_model, handler = handler_config[\"messages\"]\n    else:\n        request_model, handler = handler_config[\"default\"]\n\n    # this is required since we lose the FastAPI automatic casting\n    request = request_model.model_validate(body)\n    return await handler(request, raw_request)\n\n\nif envs.VLLM_TORCH_PROFILER_DIR:\n    logger.warning(\n        \"Torch Profiler is enabled in the API server. This should ONLY be \"\n        \"used for local development!\")\n\n    @router.post(\"/start_profile\")\n    async def start_profile(raw_request: Request):\n        logger.info(\"Starting profiler...\")\n        await engine_client(raw_request).start_profile()\n        logger.info(\"Profiler started.\")\n        return Response(status_code=200)\n\n    @router.post(\"/stop_profile\")\n    async def stop_profile(raw_request: Request):\n        logger.info(\"Stopping profiler...\")\n        await engine_client(raw_request).stop_profile()\n        logger.info(\"Profiler stopped.\")\n        return Response(status_code=200)\n\n\nif envs.VLLM_ALLOW_RUNTIME_LORA_UPDATING:\n    logger.warning(\n        \"Lora dynamic loading & unloading is enabled in the API server. \"\n        \"This should ONLY be used for local development!\")\n\n    @router.post(\"/v1/load_lora_adapter\")\n    async def load_lora_adapter(request: LoadLoraAdapterRequest,\n                                raw_request: Request):\n        handler = models(raw_request)\n        response = await handler.load_lora_adapter(request)\n        if isinstance(response, ErrorResponse):\n            return JSONResponse(content=response.model_dump(),\n                                status_code=response.code)\n\n        return Response(status_code=200, content=response)\n\n    @router.post(\"/v1/unload_lora_adapter\")\n    async def unload_lora_adapter(request: UnloadLoraAdapterRequest,\n                                  raw_request: Request):\n        handler = models(raw_request)\n        response = await handler.unload_lora_adapter(request)\n        if isinstance(response, ErrorResponse):\n            return JSONResponse(content=response.model_dump(),\n                                status_code=response.code)\n\n        return Response(status_code=200, content=response)\n\n\ndef build_app(args: Namespace) -> FastAPI:\n    if args.disable_fastapi_docs:\n        app = FastAPI(openapi_url=None,\n                      docs_url=None,\n                      redoc_url=None,\n                      lifespan=lifespan)\n    else:\n        app = FastAPI(lifespan=lifespan)\n    app.include_router(router)\n    app.root_path = args.root_path\n\n    mount_metrics(app)\n\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=args.allowed_origins,\n        allow_credentials=args.allow_credentials,\n        allow_methods=args.allowed_methods,\n        allow_headers=args.allowed_headers,\n    )\n\n    @app.exception_handler(RequestValidationError)\n    async def validation_exception_handler(_, exc):\n        err = ErrorResponse(message=str(exc),\n                            type=\"BadRequestError\",\n                            code=HTTPStatus.BAD_REQUEST)\n        return JSONResponse(err.model_dump(),\n                            status_code=HTTPStatus.BAD_REQUEST)\n\n    if token := envs.VLLM_API_KEY or args.api_key:\n\n        @app.middleware(\"http\")\n        async def authentication(request: Request, call_next):\n            if request.method == \"OPTIONS\":\n                return await call_next(request)\n            url_path = request.url.path\n            if app.root_path and url_path.startswith(app.root_path):\n                url_path = url_path[len(app.root_path):]\n            if not url_path.startswith(\"/v1\"):\n                return await call_next(request)\n            if request.headers.get(\"Authorization\") != \"Bearer \" + token:\n                return JSONResponse(content={\"error\": \"Unauthorized\"},\n                                    status_code=401)\n            return await call_next(request)\n\n    if args.enable_request_id_headers:\n        logger.warning(\n            \"CAUTION: Enabling X-Request-Id headers in the API Server. \"\n            \"This can harm performance at high QPS.\")\n\n        @app.middleware(\"http\")\n        async def add_request_id(request: Request, call_next):\n            request_id = request.headers.get(\n                \"X-Request-Id\") or uuid.uuid4().hex\n            response = await call_next(request)\n            response.headers[\"X-Request-Id\"] = request_id\n            return response\n\n    for middleware in args.middleware:\n        module_path, object_name = middleware.rsplit(\".\", 1)\n        imported = getattr(importlib.import_module(module_path), object_name)\n        if inspect.isclass(imported):\n            app.add_middleware(imported)  # type: ignore[arg-type]\n        elif inspect.iscoroutinefunction(imported):\n            app.middleware(\"http\")(imported)\n        else:\n            raise ValueError(f\"Invalid middleware {middleware}. \"\n                             f\"Must be a function or a class.\")\n\n    return app\n\n\nasync def init_app_state(\n    engine_client: EngineClient,\n    model_config: ModelConfig,\n    state: State,\n    args: Namespace,\n) -> None:\n    if args.served_model_name is not None:\n        served_model_names = args.served_model_name\n    else:\n        served_model_names = [args.model]\n\n    if args.disable_log_requests:\n        request_logger = None\n    else:\n        request_logger = RequestLogger(max_log_len=args.max_log_len)\n\n    base_model_paths = [\n        BaseModelPath(name=name, model_path=args.model)\n        for name in served_model_names\n    ]\n\n    state.engine_client = engine_client\n    state.log_stats = not args.disable_log_stats\n\n    resolved_chat_template = load_chat_template(args.chat_template)\n    logger.info(\"Using supplied chat template:\\n%s\", resolved_chat_template)\n\n    state.openai_serving_models = OpenAIServingModels(\n        engine_client=engine_client,\n        model_config=model_config,\n        base_model_paths=base_model_paths,\n        lora_modules=args.lora_modules,\n        prompt_adapters=args.prompt_adapters,\n    )\n    await state.openai_serving_models.init_static_loras()\n    state.openai_serving_chat = OpenAIServingChat(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        args.response_role,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n        return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n        enable_auto_tools=args.enable_auto_tool_choice,\n        tool_parser=args.tool_call_parser,\n        enable_prompt_tokens_details=args.enable_prompt_tokens_details,\n    ) if model_config.runner_type == \"generate\" else None\n    state.openai_serving_completion = OpenAIServingCompletion(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        return_tokens_as_token_ids=args.return_tokens_as_token_ids,\n    ) if model_config.runner_type == \"generate\" else None\n    state.openai_serving_pooling = OpenAIServingPooling(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    ) if model_config.runner_type == \"pooling\" else None\n    state.openai_serving_embedding = OpenAIServingEmbedding(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    ) if model_config.task == \"embed\" else None\n    state.openai_serving_scores = OpenAIServingScores(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger\n    ) if model_config.task == \"score\" else None\n    state.openai_serving_tokenization = OpenAIServingTokenization(\n        engine_client,\n        model_config,\n        state.openai_serving_models,\n        request_logger=request_logger,\n        chat_template=resolved_chat_template,\n        chat_template_content_format=args.chat_template_content_format,\n    )\n    state.task = model_config.task\n\n\ndef create_server_socket(addr: Tuple[str, int]) -> socket.socket:\n    family = socket.AF_INET\n    if is_valid_ipv6_address(addr[0]):\n        family = socket.AF_INET6\n\n    sock = socket.socket(family=family, type=socket.SOCK_STREAM)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind(addr)\n\n    return sock\n\n\nasync def run_server(args, **uvicorn_kwargs) -> None:\n    logger.info(\"vLLM API server version %s\", VLLM_VERSION)\n    logger.info(\"args: %s\", args)\n\n    if args.tool_parser_plugin and len(args.tool_parser_plugin) > 3:\n        ToolParserManager.import_tool_parser(args.tool_parser_plugin)\n\n    valid_tool_parses = ToolParserManager.tool_parsers.keys()\n    if args.enable_auto_tool_choice \\\n        and args.tool_call_parser not in valid_tool_parses:\n        raise KeyError(f\"invalid tool call parser: {args.tool_call_parser} \"\n                       f\"(chose from {{ {','.join(valid_tool_parses)} }})\")\n\n    # workaround to make sure that we bind the port before the engine is set up.\n    # This avoids race conditions with ray.\n    # see https://github.com/vllm-project/vllm/issues/8204\n    sock_addr = (args.host or \"\", args.port)\n    sock = create_server_socket(sock_addr)\n\n    # workaround to avoid footguns where uvicorn drops requests with too\n    # many concurrent requests active\n    set_ulimit()\n\n    def signal_handler(*_) -> None:\n        # Interrupt server on sigterm while initializing\n        raise KeyboardInterrupt(\"terminated\")\n\n    signal.signal(signal.SIGTERM, signal_handler)\n\n    async with build_async_engine_client(args) as engine_client:\n        app = build_app(args)\n\n        model_config = await engine_client.get_model_config()\n        await init_app_state(engine_client, model_config, app.state, args)\n\n        shutdown_task = await serve_http(\n            app,\n            host=args.host,\n            port=args.port,\n            log_level=args.uvicorn_log_level,\n            timeout_keep_alive=TIMEOUT_KEEP_ALIVE,\n            ssl_keyfile=args.ssl_keyfile,\n            ssl_certfile=args.ssl_certfile,\n            ssl_ca_certs=args.ssl_ca_certs,\n            ssl_cert_reqs=args.ssl_cert_reqs,\n            # Workaround to work on macOS\n            fd=sock.fileno() if sys.platform.startswith(\"darwin\") else None,\n            **uvicorn_kwargs,\n        )\n\n    # NB: Await server shutdown only after the backend context is exited\n    await shutdown_task\n\n    sock.close()\n\n\nif __name__ == \"__main__\":\n    # NOTE(simon):\n    # This section should be in sync with vllm/scripts.py for CLI entrypoints.\n    parser = FlexibleArgumentParser(\n        description=\"vLLM OpenAI-Compatible RESTful API server.\")\n    parser = make_arg_parser(parser)\n    args = parser.parse_args()\n    validate_parsed_serve_args(args)\n\n    uvloop.run(run_server(args))\n",
      "diff": "diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py\nindex 9bb11907f..f510c4150 100644\n--- a/vllm/entrypoints/openai/api_server.py\n+++ b/vllm/entrypoints/openai/api_server.py\n@@ -1,5 +1,6 @@\n import asyncio\n import atexit\n+import gc\n import importlib\n import inspect\n import multiprocessing\n@@ -104,6 +105,11 @@ async def lifespan(app: FastAPI):\n             task.add_done_callback(_running_tasks.remove)\n         else:\n             task = None\n+\n+        # Mark the startup heap as static so that it's ignored by GC.\n+        # Reduces pause times of oldest generation collections.\n+        gc.collect()\n+        gc.freeze()\n         try:\n             yield\n         finally:",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/entrypoints/openai/protocol.py",
      "old_content": "# Adapted from\n# https://github.com/lm-sys/FastChat/blob/168ccc29d3f7edc50823016105c024fe2282732a/fastchat/protocol/openai_api_protocol.py\nimport re\nimport time\nfrom argparse import Namespace\nfrom typing import Any, Dict, List, Literal, Optional, Union\n\nimport torch\nfrom pydantic import BaseModel, ConfigDict, Field, model_validator\nfrom typing_extensions import Annotated\n\nfrom vllm.entrypoints.chat_utils import ChatCompletionMessageParam\nfrom vllm.logger import init_logger\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.sampling_params import (BeamSearchParams, GuidedDecodingParams,\n                                  RequestOutputKind, SamplingParams)\nfrom vllm.sequence import Logprob\nfrom vllm.utils import random_uuid, resolve_obj_by_qualname\n\nlogger = init_logger(__name__)\n\n# torch is mocked during docs generation,\n# so we have to provide the values as literals\n_MOCK_LONG_INFO = Namespace(min=-9223372036854775808, max=9223372036854775807)\n_LONG_INFO: Union[\"torch.iinfo\", Namespace]\n\ntry:\n    from sphinx.ext.autodoc.mock import _MockModule\n\n    if isinstance(torch, _MockModule):\n        _LONG_INFO = _MOCK_LONG_INFO\n    else:\n        _LONG_INFO = torch.iinfo(torch.long)\nexcept ModuleNotFoundError:\n    _LONG_INFO = torch.iinfo(torch.long)\n\nassert _LONG_INFO.min == _MOCK_LONG_INFO.min\nassert _LONG_INFO.max == _MOCK_LONG_INFO.max\n\n\nclass OpenAIBaseModel(BaseModel):\n    # OpenAI API does allow extra fields\n    model_config = ConfigDict(extra=\"allow\")\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def __log_extra_fields__(cls, data):\n        if isinstance(data, dict):\n            # Get all class field names and their potential aliases\n            field_names = set()\n            for field_name, field in cls.model_fields.items():\n                field_names.add(field_name)\n                if hasattr(field, 'alias') and field.alias:\n                    field_names.add(field.alias)\n\n            # Compare against both field names and aliases\n            extra_fields = data.keys() - field_names\n            if extra_fields:\n                logger.warning(\n                    \"The following fields were present in the request \"\n                    \"but ignored: %s\", extra_fields)\n        return data\n\n\nclass ErrorResponse(OpenAIBaseModel):\n    object: str = \"error\"\n    message: str\n    type: str\n    param: Optional[str] = None\n    code: int\n\n\nclass ModelPermission(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"modelperm-{random_uuid()}\")\n    object: str = \"model_permission\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    allow_create_engine: bool = False\n    allow_sampling: bool = True\n    allow_logprobs: bool = True\n    allow_search_indices: bool = False\n    allow_view: bool = True\n    allow_fine_tuning: bool = False\n    organization: str = \"*\"\n    group: Optional[str] = None\n    is_blocking: bool = False\n\n\nclass ModelCard(OpenAIBaseModel):\n    id: str\n    object: str = \"model\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    owned_by: str = \"vllm\"\n    root: Optional[str] = None\n    parent: Optional[str] = None\n    max_model_len: Optional[int] = None\n    permission: List[ModelPermission] = Field(default_factory=list)\n\n\nclass ModelList(OpenAIBaseModel):\n    object: str = \"list\"\n    data: List[ModelCard] = Field(default_factory=list)\n\n\nclass PromptTokenUsageInfo(OpenAIBaseModel):\n    cached_tokens: Optional[int] = None\n\n\nclass UsageInfo(OpenAIBaseModel):\n    prompt_tokens: int = 0\n    total_tokens: int = 0\n    completion_tokens: Optional[int] = 0\n    prompt_tokens_details: Optional[PromptTokenUsageInfo] = None\n\n\nclass RequestResponseMetadata(BaseModel):\n    request_id: str\n    final_usage_info: Optional[UsageInfo] = None\n\n\nclass JsonSchemaResponseFormat(OpenAIBaseModel):\n    name: str\n    description: Optional[str] = None\n    # schema is the field in openai but that causes conflicts with pydantic so\n    # instead use json_schema with an alias\n    json_schema: Optional[Dict[str, Any]] = Field(default=None, alias='schema')\n    strict: Optional[bool] = None\n\n\nclass ResponseFormat(OpenAIBaseModel):\n    # type must be \"json_schema\", \"json_object\" or \"text\"\n    type: Literal[\"text\", \"json_object\", \"json_schema\"]\n    json_schema: Optional[JsonSchemaResponseFormat] = None\n\n\nclass StreamOptions(OpenAIBaseModel):\n    include_usage: Optional[bool] = True\n    continuous_usage_stats: Optional[bool] = False\n\n\nclass FunctionDefinition(OpenAIBaseModel):\n    name: str\n    description: Optional[str] = None\n    parameters: Optional[Dict[str, Any]] = None\n\n\nclass ChatCompletionToolsParam(OpenAIBaseModel):\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionDefinition\n\n\nclass ChatCompletionNamedFunction(OpenAIBaseModel):\n    name: str\n\n\nclass ChatCompletionNamedToolChoiceParam(OpenAIBaseModel):\n    function: ChatCompletionNamedFunction\n    type: Literal[\"function\"] = \"function\"\n\n\nclass LogitsProcessorConstructor(BaseModel):\n    qualname: str\n    args: Optional[List[Any]] = None\n    kwargs: Optional[Dict[str, Any]] = None\n\n\nLogitsProcessors = List[Union[str, LogitsProcessorConstructor]]\n\n\ndef get_logits_processors(processors: Optional[LogitsProcessors],\n                          pattern: Optional[str]) -> Optional[List[Any]]:\n    if processors and pattern:\n        logits_processors = []\n        for processor in processors:\n            qualname = processor if isinstance(processor,\n                                               str) else processor.qualname\n            if not re.match(pattern, qualname):\n                raise ValueError(\n                    f\"Logits processor '{qualname}' is not allowed by this \"\n                    \"server. See --logits-processor-pattern engine argument \"\n                    \"for more information.\")\n            try:\n                logits_processor = resolve_obj_by_qualname(qualname)\n            except Exception as e:\n                raise ValueError(\n                    f\"Logits processor '{qualname}' could not be resolved: {e}\"\n                ) from e\n            if isinstance(processor, LogitsProcessorConstructor):\n                logits_processor = logits_processor(*processor.args or [],\n                                                    **processor.kwargs or {})\n            logits_processors.append(logits_processor)\n        return logits_processors\n    elif processors:\n        raise ValueError(\n            \"The `logits_processors` argument is not supported by this \"\n            \"server. See --logits-processor-pattern engine argugment \"\n            \"for more information.\")\n    return None\n\n\nclass ChatCompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/chat/create\n    messages: List[ChatCompletionMessageParam]\n    model: str\n    frequency_penalty: Optional[float] = 0.0\n    logit_bias: Optional[Dict[str, float]] = None\n    logprobs: Optional[bool] = False\n    top_logprobs: Optional[int] = 0\n    # TODO(#9845): remove max_tokens when field is removed from OpenAI API\n    max_tokens: Optional[int] = Field(\n        default=None,\n        deprecated=\n        'max_tokens is deprecated in favor of the max_completion_tokens field')\n    max_completion_tokens: Optional[int] = None\n    n: Optional[int] = 1\n    presence_penalty: Optional[float] = 0.0\n    response_format: Optional[ResponseFormat] = None\n    seed: Optional[int] = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)\n    stream: Optional[bool] = False\n    stream_options: Optional[StreamOptions] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    tools: Optional[List[ChatCompletionToolsParam]] = None\n    tool_choice: Optional[Union[Literal[\"none\"], Literal[\"auto\"],\n                                ChatCompletionNamedToolChoiceParam]] = \"none\"\n\n    # NOTE this will be ignored by VLLM -- the model determines the behavior\n    parallel_tool_calls: Optional[bool] = False\n    user: Optional[str] = None\n\n    # doc: begin-chat-completion-sampling-params\n    best_of: Optional[int] = None\n    use_beam_search: bool = False\n    top_k: Optional[int] = None\n    min_p: Optional[float] = None\n    repetition_penalty: Optional[float] = None\n    length_penalty: float = 1.0\n    stop_token_ids: Optional[List[int]] = Field(default_factory=list)\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None\n    prompt_logprobs: Optional[int] = None\n    # doc: end-chat-completion-sampling-params\n\n    # doc: begin-chat-completion-extra-params\n    echo: bool = Field(\n        default=False,\n        description=(\n            \"If true, the new message will be prepended with the last message \"\n            \"if they belong to the same role.\"),\n    )\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=\n        (\"If true, the generation prompt will be added to the chat template. \"\n         \"This is a parameter used by chat template in tokenizer config of the \"\n         \"model.\"),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=\n        (\"If this is set, the chat will be formatted so that the final \"\n         \"message in the chat is open-ended, without any EOS tokens. The \"\n         \"model will continue this message rather than starting a new one. \"\n         \"This allows you to \\\"prefill\\\" part of the model's response for it. \"\n         \"Cannot be used at the same time as `add_generation_prompt`.\"),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"),\n    )\n    documents: Optional[List[Dict[str, str]]] = Field(\n        default=None,\n        description=\n        (\"A list of dicts representing documents that will be accessible to \"\n         \"the model if it is performing RAG (retrieval-augmented generation).\"\n         \" If the template does not support RAG, this argument will have no \"\n         \"effect. We recommend that each document should be a dict containing \"\n         \"\\\"title\\\" and \\\"text\\\" keys.\"),\n    )\n    chat_template: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"),\n    )\n    chat_template_kwargs: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the template renderer. \"\n                     \"Will be accessible by the chat template.\"),\n    )\n    guided_json: Optional[Union[str, dict, BaseModel]] = Field(\n        default=None,\n        description=(\"If specified, the output will follow the JSON schema.\"),\n    )\n    guided_regex: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will follow the regex pattern.\"),\n    )\n    guided_choice: Optional[List[str]] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will be exactly one of the choices.\"),\n    )\n    guided_grammar: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will follow the context free grammar.\"),\n    )\n    guided_decoding_backend: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, will override the default guided decoding backend \"\n            \"of the server for this specific request. If set, must be either \"\n            \"'outlines' / 'lm-format-enforcer'\"))\n    guided_whitespace_pattern: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, will override the default whitespace pattern \"\n            \"for guided json decoding.\"))\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"))\n    request_id: str = Field(\n        default_factory=lambda: f\"{random_uuid()}\",\n        description=(\n            \"The request_id related to this request. If the caller does \"\n            \"not set it, a random_uuid will be generated. This id is used \"\n            \"through out the inference process and return in response.\"))\n    logits_processors: Optional[LogitsProcessors] = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"))\n\n    # doc: end-chat-completion-extra-params\n\n    # Default sampling parameters for chat completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": -1,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n            self,\n            default_max_tokens: int,\n            default_sampling_params: Optional[dict] = None\n    ) -> BeamSearchParams:\n        # TODO(#9845): remove max_tokens when field is removed from OpenAI API\n        max_tokens = self.max_completion_tokens or self.max_tokens\n        if max_tokens is None:\n            max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        n = self.n if self.n is not None else 1\n\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"])\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output)\n\n    def to_sampling_params(\n            self,\n            default_max_tokens: int,\n            logits_processor_pattern: Optional[str],\n            default_sampling_params: Optional[dict] = None) -> SamplingParams:\n        # TODO(#9845): remove max_tokens when field is removed from OpenAI API\n        max_tokens = self.max_completion_tokens or self.max_tokens\n        if max_tokens is None:\n            max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"])\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"])\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"])\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"])\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.top_logprobs\n\n        guided_json_object = None\n        if self.response_format is not None:\n            if self.response_format.type == \"json_object\":\n                guided_json_object = True\n            elif self.response_format.type == \"json_schema\":\n                json_schema = self.response_format.json_schema\n                assert json_schema is not None\n                self.guided_json = json_schema.json_schema\n                if self.guided_decoding_backend is None:\n                    self.guided_decoding_backend = \"xgrammar\"\n\n        guided_decoding = GuidedDecodingParams.from_optional(\n            json=self._get_guided_json_from_tool() or self.guided_json,\n            regex=self.guided_regex,\n            choice=self.guided_choice,\n            grammar=self.guided_grammar,\n            json_object=guided_json_object,\n            backend=self.guided_decoding_backend,\n            whitespace_pattern=self.guided_whitespace_pattern)\n\n        return SamplingParams.from_optional(\n            n=self.n,\n            best_of=self.best_of,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.top_logprobs if self.logprobs else None,\n            prompt_logprobs=prompt_logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens,\n            min_tokens=self.min_tokens,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            logits_processors=get_logits_processors(self.logits_processors,\n                                                    logits_processor_pattern),\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA if self.stream \\\n                else RequestOutputKind.FINAL_ONLY,\n            guided_decoding=guided_decoding,\n            logit_bias=self.logit_bias)\n\n    def _get_guided_json_from_tool(\n            self) -> Optional[Union[str, dict, BaseModel]]:\n        # user has chosen to not use any tool\n        if self.tool_choice == \"none\" or self.tools is None:\n            return None\n\n        # user has chosen to use a named tool\n        if type(self.tool_choice) is ChatCompletionNamedToolChoiceParam:\n            tool_name = self.tool_choice.function.name\n            tools = {tool.function.name: tool.function for tool in self.tools}\n            if tool_name not in tools:\n                raise ValueError(\n                    f\"Tool '{tool_name}' has not been passed in `tools`.\")\n            tool = tools[tool_name]\n            return tool.parameters\n\n        return None\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise ValueError(\n                \"Stream options can only be defined when `stream=True`.\")\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and prompt_logprobs > 0:\n                raise ValueError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\")\n\n            if prompt_logprobs < 0:\n                raise ValueError(\"`prompt_logprobs` must be a positive value.\")\n\n        if (top_logprobs := data.get(\"top_logprobs\")) is not None:\n            if top_logprobs < 0:\n                raise ValueError(\"`top_logprobs` must be a positive value.\")\n\n            if not data.get(\"logprobs\"):\n                raise ValueError(\n                    \"when using `top_logprobs`, `logprobs` must be set to true.\"\n                )\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_guided_decoding_count(cls, data):\n        if isinstance(data, ValueError):\n            raise data\n\n        guide_count = sum([\n            \"guided_json\" in data and data[\"guided_json\"] is not None,\n            \"guided_regex\" in data and data[\"guided_regex\"] is not None,\n            \"guided_choice\" in data and data[\"guided_choice\"] is not None\n        ])\n        # you can only use one kind of guided decoding\n        if guide_count > 1:\n            raise ValueError(\n                \"You can only use one kind of guided decoding \"\n                \"('guided_json', 'guided_regex' or 'guided_choice').\")\n        # you can only either use guided decoding or tools, not both\n        if guide_count > 1 and data.get(\"tool_choice\",\n                                        \"none\") not in (\"none\", \"auto\"):\n            raise ValueError(\n                \"You can only either use guided decoding or tools, not both.\")\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_tool_usage(cls, data):\n\n        # if \"tool_choice\" is not specified but tools are provided,\n        # default to \"auto\" tool_choice\n        if \"tool_choice\" not in data and data.get(\"tools\"):\n            data[\"tool_choice\"] = \"auto\"\n\n        # if \"tool_choice\" is \"none\" -- ignore tools if present\n        if \"tool_choice\" in data and data[\"tool_choice\"] == \"none\":\n            # ensure that no tools are present\n            data.pop(\"tools\", None)\n            return data\n\n        # if \"tool_choice\" is specified -- validation\n        if \"tool_choice\" in data:\n\n            # ensure that if \"tool choice\" is specified, tools are present\n            if \"tools\" not in data or data[\"tools\"] is None:\n                raise ValueError(\n                    \"When using `tool_choice`, `tools` must be set.\")\n\n            # make sure that tool choice is either a named tool\n            # OR that it's set to \"auto\"\n            if data[\"tool_choice\"] != \"auto\" and not isinstance(\n                    data[\"tool_choice\"], dict):\n                raise ValueError(\n                    \"`tool_choice` must either be a named tool, \\\"auto\\\", \"\n                    \"or \\\"none\\\".\")\n\n            # ensure that if \"tool_choice\" is specified as an object,\n            # it matches a valid tool\n            if isinstance(data[\"tool_choice\"], dict):\n                valid_tool = False\n                specified_function = data[\"tool_choice\"].get(\"function\")\n                if not specified_function:\n                    raise ValueError(\n                        \"Expected field `function` in `tool_choice`.\"\n                        \" Correct usage: `{\\\"type\\\": \\\"function\\\",\"\n                        \" \\\"function\\\": {\\\"name\\\": \\\"my_function\\\"}}`\")\n                specified_function_name = specified_function.get(\"name\")\n                if not specified_function_name:\n                    raise ValueError(\n                        \"Expected field `name` in `function` in `tool_choice`.\"\n                        \"Correct usage: `{\\\"type\\\": \\\"function\\\", \"\n                        \"\\\"function\\\": {\\\"name\\\": \\\"my_function\\\"}}`\")\n                for tool in data[\"tools\"]:\n                    if tool[\"function\"][\"name\"] == specified_function_name:\n                        valid_tool = True\n                        break\n                if not valid_tool:\n                    raise ValueError(\n                        \"The tool specified in `tool_choice` does not match any\"\n                        \" of the specified `tools`\")\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\n                \"add_generation_prompt\"):\n            raise ValueError(\"Cannot set both `continue_final_message` and \"\n                             \"`add_generation_prompt` to True.\")\n        return data\n\n\nclass CompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/completions/create\n    model: str\n    prompt: Union[List[int], List[List[int]], str, List[str]]\n    best_of: Optional[int] = None\n    echo: Optional[bool] = False\n    frequency_penalty: Optional[float] = 0.0\n    logit_bias: Optional[Dict[str, float]] = None\n    logprobs: Optional[int] = None\n    max_tokens: Optional[int] = 16\n    n: int = 1\n    presence_penalty: Optional[float] = 0.0\n    seed: Optional[int] = Field(None, ge=_LONG_INFO.min, le=_LONG_INFO.max)\n    stop: Optional[Union[str, List[str]]] = Field(default_factory=list)\n    stream: Optional[bool] = False\n    stream_options: Optional[StreamOptions] = None\n    suffix: Optional[str] = None\n    temperature: Optional[float] = None\n    top_p: Optional[float] = None\n    user: Optional[str] = None\n\n    # doc: begin-completion-sampling-params\n    use_beam_search: bool = False\n    top_k: Optional[int] = None\n    min_p: Optional[float] = None\n    repetition_penalty: Optional[float] = None\n    length_penalty: float = 1.0\n    stop_token_ids: Optional[List[int]] = Field(default_factory=list)\n    include_stop_str_in_output: bool = False\n    ignore_eos: bool = False\n    min_tokens: int = 0\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None\n    allowed_token_ids: Optional[List[int]] = None\n    prompt_logprobs: Optional[int] = None\n    # doc: end-completion-sampling-params\n\n    # doc: begin-completion-extra-params\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"),\n    )\n    response_format: Optional[ResponseFormat] = Field(\n        default=None,\n        description=\n        (\"Similar to chat completion, this parameter specifies the format of \"\n         \"output. Only {'type': 'json_object'}, {'type': 'json_schema'} or \"\n         \"{'type': 'text' } is supported.\"),\n    )\n    guided_json: Optional[Union[str, dict, BaseModel]] = Field(\n        default=None,\n        description=\"If specified, the output will follow the JSON schema.\",\n    )\n    guided_regex: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will follow the regex pattern.\"),\n    )\n    guided_choice: Optional[List[str]] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will be exactly one of the choices.\"),\n    )\n    guided_grammar: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, the output will follow the context free grammar.\"),\n    )\n    guided_decoding_backend: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, will override the default guided decoding backend \"\n            \"of the server for this specific request. If set, must be one of \"\n            \"'outlines' / 'lm-format-enforcer'\"))\n    guided_whitespace_pattern: Optional[str] = Field(\n        default=None,\n        description=(\n            \"If specified, will override the default whitespace pattern \"\n            \"for guided json decoding.\"))\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"))\n    logits_processors: Optional[LogitsProcessors] = Field(\n        default=None,\n        description=(\n            \"A list of either qualified names of logits processors, or \"\n            \"constructor objects, to apply when sampling. A constructor is \"\n            \"a JSON object with a required 'qualname' field specifying the \"\n            \"qualified name of the processor class/factory, and optional \"\n            \"'args' and 'kwargs' fields containing positional and keyword \"\n            \"arguments. For example: {'qualname': \"\n            \"'my_module.MyLogitsProcessor', 'args': [1, 2], 'kwargs': \"\n            \"{'param': 'value'}}.\"))\n\n    # doc: end-completion-extra-params\n\n    # Default sampling parameters for completion requests\n    _DEFAULT_SAMPLING_PARAMS: dict = {\n        \"repetition_penalty\": 1.0,\n        \"temperature\": 1.0,\n        \"top_p\": 1.0,\n        \"top_k\": -1,\n        \"min_p\": 0.0,\n    }\n\n    def to_beam_search_params(\n            self,\n            default_max_tokens: int,\n            default_sampling_params: Optional[dict] = None\n    ) -> BeamSearchParams:\n        max_tokens = self.max_tokens\n        if max_tokens is None:\n            max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        n = self.n if self.n is not None else 1\n\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\"temperature\", 1.0)\n\n        return BeamSearchParams(\n            beam_width=n,\n            max_tokens=max_tokens,\n            ignore_eos=self.ignore_eos,\n            temperature=temperature,\n            length_penalty=self.length_penalty,\n            include_stop_str_in_output=self.include_stop_str_in_output)\n\n    def to_sampling_params(\n            self,\n            default_max_tokens: int,\n            logits_processor_pattern: Optional[str],\n            default_sampling_params: Optional[dict] = None) -> SamplingParams:\n        max_tokens = self.max_tokens\n        if max_tokens is None:\n            max_tokens = default_max_tokens\n\n        if default_sampling_params is None:\n            default_sampling_params = {}\n        # Default parameters\n        if (repetition_penalty := self.repetition_penalty) is None:\n            repetition_penalty = default_sampling_params.get(\n                \"repetition_penalty\",\n                self._DEFAULT_SAMPLING_PARAMS[\"repetition_penalty\"],\n            )\n        if (temperature := self.temperature) is None:\n            temperature = default_sampling_params.get(\n                \"temperature\", self._DEFAULT_SAMPLING_PARAMS[\"temperature\"])\n        if (top_p := self.top_p) is None:\n            top_p = default_sampling_params.get(\n                \"top_p\", self._DEFAULT_SAMPLING_PARAMS[\"top_p\"])\n        if (top_k := self.top_k) is None:\n            top_k = default_sampling_params.get(\n                \"top_k\", self._DEFAULT_SAMPLING_PARAMS[\"top_k\"])\n        if (min_p := self.min_p) is None:\n            min_p = default_sampling_params.get(\n                \"min_p\", self._DEFAULT_SAMPLING_PARAMS[\"min_p\"])\n\n        prompt_logprobs = self.prompt_logprobs\n        if prompt_logprobs is None and self.echo:\n            prompt_logprobs = self.logprobs\n\n        echo_without_generation = self.echo and self.max_tokens == 0\n\n        guided_json_object = None\n        if (self.response_format is not None\n                and self.response_format.type == \"json_object\"):\n            guided_json_object = True\n\n        guided_decoding = GuidedDecodingParams.from_optional(\n            json=self.guided_json,\n            regex=self.guided_regex,\n            choice=self.guided_choice,\n            grammar=self.guided_grammar,\n            json_object=guided_json_object,\n            backend=self.guided_decoding_backend,\n            whitespace_pattern=self.guided_whitespace_pattern)\n\n        return SamplingParams.from_optional(\n            n=self.n,\n            best_of=self.best_of,\n            presence_penalty=self.presence_penalty,\n            frequency_penalty=self.frequency_penalty,\n            repetition_penalty=repetition_penalty,\n            temperature=temperature,\n            top_p=top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=self.seed,\n            stop=self.stop,\n            stop_token_ids=self.stop_token_ids,\n            logprobs=self.logprobs,\n            ignore_eos=self.ignore_eos,\n            max_tokens=max_tokens if not echo_without_generation else 1,\n            min_tokens=self.min_tokens,\n            prompt_logprobs=prompt_logprobs,\n            skip_special_tokens=self.skip_special_tokens,\n            spaces_between_special_tokens=self.spaces_between_special_tokens,\n            include_stop_str_in_output=self.include_stop_str_in_output,\n            logits_processors=get_logits_processors(self.logits_processors,\n                                                    logits_processor_pattern),\n            truncate_prompt_tokens=self.truncate_prompt_tokens,\n            output_kind=RequestOutputKind.DELTA if self.stream \\\n                else RequestOutputKind.FINAL_ONLY,\n            guided_decoding=guided_decoding,\n            logit_bias=self.logit_bias,\n            allowed_token_ids=self.allowed_token_ids)\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_guided_decoding_count(cls, data):\n        guide_count = sum([\n            \"guided_json\" in data and data[\"guided_json\"] is not None,\n            \"guided_regex\" in data and data[\"guided_regex\"] is not None,\n            \"guided_choice\" in data and data[\"guided_choice\"] is not None\n        ])\n        if guide_count > 1:\n            raise ValueError(\n                \"You can only use one kind of guided decoding \"\n                \"('guided_json', 'guided_regex' or 'guided_choice').\")\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_logprobs(cls, data):\n        if (prompt_logprobs := data.get(\"prompt_logprobs\")) is not None:\n            if data.get(\"stream\") and prompt_logprobs > 0:\n                raise ValueError(\n                    \"`prompt_logprobs` are not available when `stream=True`.\")\n\n            if prompt_logprobs < 0:\n                raise ValueError(\"`prompt_logprobs` must be a positive value.\")\n\n        if (logprobs := data.get(\"logprobs\")) is not None and logprobs < 0:\n            raise ValueError(\"`logprobs` must be a positive value.\")\n\n        return data\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def validate_stream_options(cls, data):\n        if data.get(\"stream_options\") and not data.get(\"stream\"):\n            raise ValueError(\n                \"Stream options can only be defined when `stream=True`.\")\n\n        return data\n\n\nclass EmbeddingCompletionRequest(OpenAIBaseModel):\n    # Ordered by official OpenAI API documentation\n    # https://platform.openai.com/docs/api-reference/embeddings\n    model: str\n    input: Union[List[int], List[List[int]], str, List[str]]\n    encoding_format: Literal[\"float\", \"base64\"] = \"float\"\n    dimensions: Optional[int] = None\n    user: Optional[str] = None\n    truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None\n\n    # doc: begin-embedding-pooling-params\n    additional_data: Optional[Any] = None\n    # doc: end-embedding-pooling-params\n\n    # doc: begin-embedding-extra-params\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"))\n\n    # doc: end-embedding-extra-params\n\n    def to_pooling_params(self):\n        return PoolingParams(additional_data=self.additional_data)\n\n\nclass EmbeddingChatRequest(OpenAIBaseModel):\n    model: str\n    messages: List[ChatCompletionMessageParam]\n\n    encoding_format: Literal[\"float\", \"base64\"] = \"float\"\n    dimensions: Optional[int] = None\n    user: Optional[str] = None\n    truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None\n\n    # doc: begin-chat-embedding-pooling-params\n    additional_data: Optional[Any] = None\n    # doc: end-chat-embedding-pooling-params\n\n    # doc: begin-chat-embedding-extra-params\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"),\n    )\n    chat_template: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"),\n    )\n    chat_template_kwargs: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the template renderer. \"\n                     \"Will be accessible by the chat template.\"),\n    )\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"))\n    # doc: end-chat-embedding-extra-params\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\n                \"add_generation_prompt\"):\n            raise ValueError(\"Cannot set both `continue_final_message` and \"\n                             \"`add_generation_prompt` to True.\")\n        return data\n\n    def to_pooling_params(self):\n        return PoolingParams(additional_data=self.additional_data)\n\n\nEmbeddingRequest = Union[EmbeddingCompletionRequest, EmbeddingChatRequest]\n\nPoolingCompletionRequest = EmbeddingCompletionRequest\nPoolingChatRequest = EmbeddingChatRequest\nPoolingRequest = Union[PoolingCompletionRequest, PoolingChatRequest]\n\n\nclass ScoreRequest(OpenAIBaseModel):\n    model: str\n    text_1: Union[List[str], str]\n    text_2: Union[List[str], str]\n    truncate_prompt_tokens: Optional[Annotated[int, Field(ge=1)]] = None\n\n    # doc: begin-score-pooling-params\n    additional_data: Optional[Any] = None\n    # doc: end-score-pooling-params\n\n    # doc: begin-score-extra-params\n    priority: int = Field(\n        default=0,\n        description=(\n            \"The priority of the request (lower means earlier handling; \"\n            \"default: 0). Any priority other than 0 will raise an error \"\n            \"if the served model does not use priority scheduling.\"))\n\n    # doc: end-score-extra-params\n\n    def to_pooling_params(self):\n        return PoolingParams(additional_data=self.additional_data)\n\n\nclass CompletionLogProbs(OpenAIBaseModel):\n    text_offset: List[int] = Field(default_factory=list)\n    token_logprobs: List[Optional[float]] = Field(default_factory=list)\n    tokens: List[str] = Field(default_factory=list)\n    top_logprobs: List[Optional[Dict[str,\n                                     float]]] = Field(default_factory=list)\n\n\nclass CompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: Optional[CompletionLogProbs] = None\n    finish_reason: Optional[str] = None\n    stop_reason: Optional[Union[int, str]] = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"),\n    )\n    prompt_logprobs: Optional[List[Optional[Dict[int, Logprob]]]] = None\n\n\nclass CompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseChoice]\n    usage: UsageInfo\n\n\nclass CompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    text: str\n    logprobs: Optional[CompletionLogProbs] = None\n    finish_reason: Optional[str] = None\n    stop_reason: Optional[Union[int, str]] = Field(\n        default=None,\n        description=(\n            \"The stop string or token id that caused the completion \"\n            \"to stop, None if the completion finished for some other reason \"\n            \"including encountering the EOS token\"),\n    )\n\n\nclass CompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"cmpl-{random_uuid()}\")\n    object: str = \"text_completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[CompletionResponseStreamChoice]\n    usage: Optional[UsageInfo] = Field(default=None)\n\n\nclass EmbeddingResponseData(OpenAIBaseModel):\n    index: int\n    object: str = \"embedding\"\n    embedding: Union[List[float], str]\n\n\nclass EmbeddingResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"embd-{random_uuid()}\")\n    object: str = \"list\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    data: List[EmbeddingResponseData]\n    usage: UsageInfo\n\n\nclass PoolingResponseData(OpenAIBaseModel):\n    index: int\n    object: str = \"pooling\"\n    data: Union[List[List[float]], List[float], str]\n\n\nclass PoolingResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"pool-{random_uuid()}\")\n    object: str = \"list\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    data: List[PoolingResponseData]\n    usage: UsageInfo\n\n\nclass ScoreResponseData(OpenAIBaseModel):\n    index: int\n    object: str = \"score\"\n    score: float\n\n\nclass ScoreResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"embd-{random_uuid()}\")\n    object: str = \"list\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    data: List[ScoreResponseData]\n    usage: UsageInfo\n\n\nclass FunctionCall(OpenAIBaseModel):\n    name: str\n    arguments: str\n\n\nclass ToolCall(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-tool-{random_uuid()}\")\n    type: Literal[\"function\"] = \"function\"\n    function: FunctionCall\n\n\nclass DeltaFunctionCall(BaseModel):\n    name: Optional[str] = None\n    arguments: Optional[str] = None\n\n\n# a tool call delta where everything is optional\nclass DeltaToolCall(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-tool-{random_uuid()}\")\n    type: Literal[\"function\"] = \"function\"\n    index: int\n    function: Optional[DeltaFunctionCall] = None\n\n\nclass ExtractedToolCallInformation(BaseModel):\n    # indicate if tools were called\n    tools_called: bool\n\n    # extracted tool calls\n    tool_calls: List[ToolCall]\n\n    # content - per OpenAI spec, content AND tool calls can be returned rarely\n    # But some models will do this intentionally\n    content: Optional[str] = None\n\n\nclass ChatMessage(OpenAIBaseModel):\n    role: str\n    content: Optional[str] = None\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n\n\nclass ChatCompletionLogProb(OpenAIBaseModel):\n    token: str\n    logprob: float = -9999.0\n    bytes: Optional[List[int]] = None\n\n\nclass ChatCompletionLogProbsContent(ChatCompletionLogProb):\n    top_logprobs: List[ChatCompletionLogProb] = Field(default_factory=list)\n\n\nclass ChatCompletionLogProbs(OpenAIBaseModel):\n    content: Optional[List[ChatCompletionLogProbsContent]] = None\n\n\nclass ChatCompletionResponseChoice(OpenAIBaseModel):\n    index: int\n    message: ChatMessage\n    logprobs: Optional[ChatCompletionLogProbs] = None\n    # per OpenAI spec this is the default\n    finish_reason: Optional[str] = \"stop\"\n    # not part of the OpenAI spec but included in vLLM for legacy reasons\n    stop_reason: Optional[Union[int, str]] = None\n\n\nclass ChatCompletionResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion\"] = \"chat.completion\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseChoice]\n    usage: UsageInfo\n    prompt_logprobs: Optional[List[Optional[Dict[int, Logprob]]]] = None\n\n\nclass DeltaMessage(OpenAIBaseModel):\n    role: Optional[str] = None\n    content: Optional[str] = None\n    tool_calls: List[DeltaToolCall] = Field(default_factory=list)\n\n\nclass ChatCompletionResponseStreamChoice(OpenAIBaseModel):\n    index: int\n    delta: DeltaMessage\n    logprobs: Optional[ChatCompletionLogProbs] = None\n    finish_reason: Optional[str] = None\n    stop_reason: Optional[Union[int, str]] = None\n\n\nclass ChatCompletionStreamResponse(OpenAIBaseModel):\n    id: str = Field(default_factory=lambda: f\"chatcmpl-{random_uuid()}\")\n    object: Literal[\"chat.completion.chunk\"] = \"chat.completion.chunk\"\n    created: int = Field(default_factory=lambda: int(time.time()))\n    model: str\n    choices: List[ChatCompletionResponseStreamChoice]\n    usage: Optional[UsageInfo] = Field(default=None)\n\n\nclass BatchRequestInput(OpenAIBaseModel):\n    \"\"\"\n    The per-line object of the batch input file.\n\n    NOTE: Currently only the `/v1/chat/completions` endpoint is supported.\n    \"\"\"\n\n    # A developer-provided per-request id that will be used to match outputs to\n    # inputs. Must be unique for each request in a batch.\n    custom_id: str\n\n    # The HTTP method to be used for the request. Currently only POST is\n    # supported.\n    method: str\n\n    # The OpenAI API relative URL to be used for the request. Currently\n    # /v1/chat/completions is supported.\n    url: str\n\n    # The parameters of the request.\n    body: Union[ChatCompletionRequest, EmbeddingRequest]\n\n\nclass BatchResponseData(OpenAIBaseModel):\n    # HTTP status code of the response.\n    status_code: int = 200\n\n    # An unique identifier for the API request.\n    request_id: str\n\n    # The body of the response.\n    body: Optional[Union[ChatCompletionResponse, EmbeddingResponse]] = None\n\n\nclass BatchRequestOutput(OpenAIBaseModel):\n    \"\"\"\n    The per-line object of the batch output and error files\n    \"\"\"\n\n    id: str\n\n    # A developer-provided per-request id that will be used to match outputs to\n    # inputs.\n    custom_id: str\n\n    response: Optional[BatchResponseData]\n\n    # For requests that failed with a non-HTTP error, this will contain more\n    # information on the cause of the failure.\n    error: Optional[Any]\n\n\nclass TokenizeCompletionRequest(OpenAIBaseModel):\n    model: str\n    prompt: str\n\n    add_special_tokens: bool = Field(\n        default=True,\n        description=(\n            \"If true (the default), special tokens (e.g. BOS) will be added to \"\n            \"the prompt.\"),\n    )\n\n\nclass TokenizeChatRequest(OpenAIBaseModel):\n    model: str\n    messages: List[ChatCompletionMessageParam]\n\n    add_generation_prompt: bool = Field(\n        default=True,\n        description=\n        (\"If true, the generation prompt will be added to the chat template. \"\n         \"This is a parameter used by chat template in tokenizer config of the \"\n         \"model.\"),\n    )\n    continue_final_message: bool = Field(\n        default=False,\n        description=\n        (\"If this is set, the chat will be formatted so that the final \"\n         \"message in the chat is open-ended, without any EOS tokens. The \"\n         \"model will continue this message rather than starting a new one. \"\n         \"This allows you to \\\"prefill\\\" part of the model's response for it. \"\n         \"Cannot be used at the same time as `add_generation_prompt`.\"),\n    )\n    add_special_tokens: bool = Field(\n        default=False,\n        description=(\n            \"If true, special tokens (e.g. BOS) will be added to the prompt \"\n            \"on top of what is added by the chat template. \"\n            \"For most models, the chat template takes care of adding the \"\n            \"special tokens so this should be set to false (as is the \"\n            \"default).\"),\n    )\n    chat_template: Optional[str] = Field(\n        default=None,\n        description=(\n            \"A Jinja template to use for this conversion. \"\n            \"As of transformers v4.44, default chat template is no longer \"\n            \"allowed, so you must provide a chat template if the tokenizer \"\n            \"does not define one.\"),\n    )\n    chat_template_kwargs: Optional[Dict[str, Any]] = Field(\n        default=None,\n        description=(\"Additional kwargs to pass to the template renderer. \"\n                     \"Will be accessible by the chat template.\"),\n    )\n\n    @model_validator(mode=\"before\")\n    @classmethod\n    def check_generation_prompt(cls, data):\n        if data.get(\"continue_final_message\") and data.get(\n                \"add_generation_prompt\"):\n            raise ValueError(\"Cannot set both `continue_final_message` and \"\n                             \"`add_generation_prompt` to True.\")\n        return data\n\n\nTokenizeRequest = Union[TokenizeCompletionRequest, TokenizeChatRequest]\n\n\nclass TokenizeResponse(OpenAIBaseModel):\n    count: int\n    max_model_len: int\n    tokens: List[int]\n\n\nclass DetokenizeRequest(OpenAIBaseModel):\n    model: str\n    tokens: List[int]\n\n\nclass DetokenizeResponse(OpenAIBaseModel):\n    prompt: str\n\n\nclass LoadLoraAdapterRequest(BaseModel):\n    lora_name: str\n    lora_path: str\n\n\nclass UnloadLoraAdapterRequest(BaseModel):\n    lora_name: str\n    lora_int_id: Optional[int] = Field(default=None)\n",
      "diff": "diff --git a/vllm/entrypoints/openai/protocol.py b/vllm/entrypoints/openai/protocol.py\nindex 14e41346d..80403f77d 100644\n--- a/vllm/entrypoints/openai/protocol.py\n+++ b/vllm/entrypoints/openai/protocol.py\n@@ -3,7 +3,7 @@\n import re\n import time\n from argparse import Namespace\n-from typing import Any, Dict, List, Literal, Optional, Union\n+from typing import Any, ClassVar, Dict, List, Literal, Optional, Set, Union\n \n import torch\n from pydantic import BaseModel, ConfigDict, Field, model_validator\n@@ -42,23 +42,31 @@ class OpenAIBaseModel(BaseModel):\n     # OpenAI API does allow extra fields\n     model_config = ConfigDict(extra=\"allow\")\n \n+    # Cache class field names\n+    field_names: ClassVar[Optional[Set[str]]] = None\n+\n     @model_validator(mode=\"before\")\n     @classmethod\n     def __log_extra_fields__(cls, data):\n-        if isinstance(data, dict):\n+\n+        field_names = cls.field_names\n+        if field_names is None:\n+            if not isinstance(data, dict):\n+                return data\n             # Get all class field names and their potential aliases\n             field_names = set()\n             for field_name, field in cls.model_fields.items():\n                 field_names.add(field_name)\n-                if hasattr(field, 'alias') and field.alias:\n-                    field_names.add(field.alias)\n-\n-            # Compare against both field names and aliases\n-            extra_fields = data.keys() - field_names\n-            if extra_fields:\n-                logger.warning(\n-                    \"The following fields were present in the request \"\n-                    \"but ignored: %s\", extra_fields)\n+                if alias := getattr(field, 'alias', None):\n+                    field_names.add(alias)\n+            cls.field_names = field_names\n+\n+        # Compare against both field names and aliases\n+        if any(k not in field_names for k in data):\n+            logger.warning(\n+                \"The following fields were present in the request \"\n+                \"but ignored: %s\",\n+                data.keys() - field_names)\n         return data",
      "change_type": "modified",
      "lines_added": 20,
      "lines_removed": 12
    },
    {
      "file_path": "vllm/envs.py",
      "old_content": "import os\nimport tempfile\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n\nif TYPE_CHECKING:\n    VLLM_HOST_IP: str = \"\"\n    VLLM_PORT: Optional[int] = None\n    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()\n    VLLM_USE_MODELSCOPE: bool = False\n    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60\n    VLLM_NCCL_SO_PATH: Optional[str] = None\n    LD_LIBRARY_PATH: Optional[str] = None\n    VLLM_USE_TRITON_FLASH_ATTN: bool = False\n    LOCAL_RANK: int = 0\n    CUDA_VISIBLE_DEVICES: Optional[str] = None\n    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60\n    VLLM_API_KEY: Optional[str] = None\n    S3_ACCESS_KEY_ID: Optional[str] = None\n    S3_SECRET_ACCESS_KEY: Optional[str] = None\n    S3_ENDPOINT_URL: Optional[str] = None\n    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")\n    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")\n    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"\n    VLLM_NO_USAGE_STATS: bool = False\n    VLLM_DO_NOT_TRACK: bool = False\n    VLLM_USAGE_SOURCE: str = \"\"\n    VLLM_CONFIGURE_LOGGING: int = 1\n    VLLM_LOGGING_LEVEL: str = \"INFO\"\n    VLLM_LOGGING_PREFIX: str = \"\"\n    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None\n    VLLM_TRACE_FUNCTION: int = 0\n    VLLM_ATTENTION_BACKEND: Optional[str] = None\n    VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None\n    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False\n    VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False\n    VLLM_PP_LAYER_PARTITION: Optional[str] = None\n    VLLM_CPU_KVCACHE_SPACE: int = 0\n    VLLM_CPU_OMP_THREADS_BIND: str = \"\"\n    VLLM_OPENVINO_DEVICE: str = \"CPU\"\n    VLLM_OPENVINO_KVCACHE_SPACE: int = 0\n    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None\n    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False\n    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")\n    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024\n    VLLM_USE_RAY_SPMD_WORKER: bool = False\n    VLLM_USE_RAY_COMPILED_DAG: bool = False\n    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True\n    VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False\n    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"\n    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")\n    VLLM_IMAGE_FETCH_TIMEOUT: int = 5\n    VLLM_VIDEO_FETCH_TIMEOUT: int = 30\n    VLLM_AUDIO_FETCH_TIMEOUT: int = 10\n    VLLM_TARGET_DEVICE: str = \"cuda\"\n    MAX_JOBS: Optional[str] = None\n    NVCC_THREADS: Optional[str] = None\n    VLLM_USE_PRECOMPILED: bool = False\n    VLLM_NO_DEPRECATION_WARNING: bool = False\n    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False\n    CMAKE_BUILD_TYPE: Optional[str] = None\n    VERBOSE: bool = False\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False\n    VLLM_TEST_FORCE_FP8_MARLIN: bool = False\n    VLLM_RPC_TIMEOUT: int = 10000  # ms\n    VLLM_PLUGINS: Optional[List[str]] = None\n    VLLM_TORCH_PROFILER_DIR: Optional[str] = None\n    VLLM_USE_TRITON_AWQ: bool = False\n    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n    VLLM_SKIP_P2P_CHECK: bool = False\n    VLLM_DISABLED_KERNELS: List[str] = []\n    VLLM_USE_V1: bool = False\n    VLLM_ENABLE_V1_MULTIPROCESSING: bool = True\n    VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n    VLLM_DISABLE_COMPILE_CACHE: bool = False\n    VLLM_SERVER_DEV_MODE: bool = False\n\n\ndef get_default_cache_root():\n    return os.getenv(\n        \"XDG_CACHE_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n    )\n\n\ndef get_default_config_root():\n    return os.getenv(\n        \"XDG_CONFIG_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n    )\n\n\n# The begin-* and end* here are used by the documentation generator\n# to extract the used env vars.\n\n# begin-env-vars-definition\n\nenvironment_variables: Dict[str, Callable[[], Any]] = {\n\n    # ================== Installation Time Env Vars ==================\n\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, neuron, cpu, openvino]\n    \"VLLM_TARGET_DEVICE\":\n    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),\n\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\":\n    lambda: os.getenv(\"MAX_JOBS\", None),\n\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\":\n    lambda: os.getenv(\"NVCC_THREADS\", None),\n\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\":\n    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")) or bool(\n        os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),\n\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\":\n    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),\n\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\":\n    lambda: bool(int(os.getenv('VERBOSE', '0'))),\n\n    # Root directory for VLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )),\n\n    # ================== Runtime Env Vars ==================\n\n    # Root directory for VLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )),\n\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    'VLLM_HOST_IP':\n    lambda: os.getenv('VLLM_HOST_IP', \"\"),\n\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    # '0' is used to make mypy happy\n    'VLLM_PORT':\n    lambda: int(os.getenv('VLLM_PORT', '0'))\n    if 'VLLM_PORT' in os.environ else None,\n\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    'VLLM_RPC_BASE_PATH':\n    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),\n\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\":\n    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),\n\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\":\n    lambda: os.environ.get(\"CUDA_HOME\", None),\n\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\":\n    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\":\n    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n\n    # flag to control if vllm should use triton flash attention\n    \"VLLM_USE_TRITON_FLASH_ATTN\":\n    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Internal flag to enable Dynamo fullgraph capture\n    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":\n    lambda: bool(\n        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),\n\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\":\n    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\":\n    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":\n    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),\n\n    # API key for VLLM API server\n    \"VLLM_API_KEY\":\n    lambda: os.environ.get(\"VLLM_API_KEY\", None),\n\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\":\n    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\":\n    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\":\n    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\":\n    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),\n    \"VLLM_NO_USAGE_STATS\":\n    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DO_NOT_TRACK\":\n    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(\n        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",\n    \"VLLM_USAGE_SOURCE\":\n    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\":\n    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),\n    \"VLLM_LOGGING_CONFIG_PATH\":\n    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\":\n    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),\n\n    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages\n    \"VLLM_LOGGING_PREFIX\":\n    lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\":\n    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n\n    # Backend for attention computation\n    # Available options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"XFORMERS\": use XFormers\n    # - \"ROCM_FLASH\": use ROCmFlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    \"VLLM_ATTENTION_BACKEND\":\n    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),\n\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\":\n    lambda: bool(int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"]))\n    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ else None,\n\n    # If set, vllm will force flashinfer to use tensor cores;\n    # otherwise will use heuristic based on model architecture.\n    \"VLLM_FLASHINFER_FORCE_TENSOR_CORES\":\n    lambda: bool(int(os.getenv(\"VLLM_FLASHINFER_FORCE_TENSOR_CORES\", \"0\"))),\n\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\":\n    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n\n    # (CPU backend only) CPU key-value cache space.\n    # default is 4GB\n    \"VLLM_CPU_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),\n\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\":\n    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),\n\n    # OpenVINO device selection\n    # default is CPU\n    \"VLLM_OPENVINO_DEVICE\":\n    lambda: os.getenv(\"VLLM_OPENVINO_DEVICE\", \"CPU\").upper(),\n\n    # OpenVINO key-value cache space\n    # default is 4GB\n    \"VLLM_OPENVINO_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),\n\n    # OpenVINO KV cache precision\n    # default is bf16 if natively supported by platform, otherwise f16\n    # To enable KV cache compression, please, explicitly specify u8\n    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":\n    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),\n\n    # Enables weights compression during model export via HF Optimum\n    # default is False\n    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":\n    lambda: bool(os.getenv(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", False)),\n\n    # If the env var is set, then all workers will execute as separate\n    # processes from the engine, and we use the same mechanism to trigger\n    # execution on all workers.\n    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.\n    \"VLLM_USE_RAY_SPMD_WORKER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),\n\n    # If the env var is set, it uses the Ray's compiled DAG API\n    # which optimizes the control plane overhead.\n    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n    \"VLLM_USE_RAY_COMPILED_DAG\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),\n\n    # If the env var is set, it uses NCCL for communication in\n    # Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))\n                 ),\n\n    # If the env var is set, it enables GPU communication overlap\n    # (experimental feature) in Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))\n                 ),\n\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\":\n    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),\n\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )),\n\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n\n    # Timeout for fetching videos when serving multimodal models\n    # Default is 15 seconds\n    \"VLLM_VIDEO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"15\")),\n\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 10 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")),\n\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),\n\n    # If set, vllm will skip the deprecation warnings.\n    \"VLLM_NO_DEPRECATION_WARNING\":\n    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),\n\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":\n    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),\n\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\":\n    lambda:\n    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\":\n    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),\n\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\":\n    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[\n        \"VLLM_PLUGINS\"].split(\",\"),\n\n    # Enables torch profiler if set. Path to the directory where torch profiler\n    # traces are saved. Note that it must be an absolute path.\n    \"VLLM_TORCH_PROFILER_DIR\":\n    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os\n             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),\n\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # By default, vLLM will check the peer-to-peer capability itself,\n    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    # If this env var is set to 1, vLLM will skip the peer-to-peer check,\n    # and trust the driver's peer-to-peer capability report.\n    \"VLLM_SKIP_P2P_CHECK\":\n    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\":\n    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[\n        \"VLLM_DISABLED_KERNELS\"].split(\",\"),\n\n    # If set, use the V1 code path.\n    \"VLLM_USE_V1\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"0\"))),\n\n    # If set, enable multiprocessing in LLM for the V1 code path.\n    \"VLLM_ENABLE_V1_MULTIPROCESSING\":\n    lambda: bool(int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))),\n    \"VLLM_LOG_BATCHSIZE_INTERVAL\":\n    lambda: float(os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")),\n    \"VLLM_DISABLE_COMPILE_CACHE\":\n    lambda: bool(int(os.getenv(\"VLLM_DISABLE_COMPILE_CACHE\", \"0\"))),\n\n    # If set, vllm will run in development mode, which will enable\n    # some additional endpoints for developing and debugging,\n    # e.g. `/reset_prefix_cache`\n    \"VLLM_SERVER_DEV_MODE\":\n    lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n}\n\n# end-env-vars-definition\n\n\ndef __getattr__(name: str):\n    # lazy evaluation of environment variables\n    if name in environment_variables:\n        return environment_variables[name]()\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef __dir__():\n    return list(environment_variables.keys())\n",
      "diff": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 1e68326b2..3a15e00e7 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -73,6 +73,7 @@ if TYPE_CHECKING:\n     VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n     VLLM_DISABLE_COMPILE_CACHE: bool = False\n     VLLM_SERVER_DEV_MODE: bool = False\n+    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n \n \n def get_default_cache_root():\n@@ -474,6 +475,16 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     # e.g. `/reset_prefix_cache`\n     \"VLLM_SERVER_DEV_MODE\":\n     lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n+\n+    # Controls the maximum number of requests to handle in a\n+    # single asyncio task when processing per-token outputs in the\n+    # V1 AsyncLLM interface. It is applicable when handling a high\n+    # concurrency of streaming requests.\n+    # Setting this too high can result in a higher variance of\n+    # inter-message latencies. Setting it too low can negatively impact\n+    # TTFT and overall throughput.\n+    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n+    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n }\n \n # end-env-vars-definition",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/v1/engine/async_llm.py",
      "old_content": "import asyncio\nimport os\nfrom typing import AsyncGenerator, List, Mapping, Optional, Type, Union\n\nfrom vllm.config import ModelConfig, VllmConfig\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.protocol import EngineClient\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry, PromptType\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.outputs import RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import kill_process_tree\nfrom vllm.v1.engine.core_client import EngineCoreClient\nfrom vllm.v1.engine.output_processor import OutputProcessor\nfrom vllm.v1.engine.processor import Processor\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.metrics.loggers import LoggingStatLogger, StatLoggerBase\nfrom vllm.v1.metrics.stats import IterationStats, SchedulerStats\n\nlogger = init_logger(__name__)\n\n\nclass AsyncLLM(EngineClient):\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        executor_class: Type[Executor],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        use_cached_outputs: bool = False,\n        log_requests: bool = True,\n        start_engine_loop: bool = True,\n    ) -> None:\n\n        assert start_engine_loop\n\n        self.log_requests = log_requests\n        self.log_stats = log_stats\n        self.stat_loggers: List[StatLoggerBase] = [\n            LoggingStatLogger(),\n            # TODO(rob): PrometheusStatLogger(),\n        ]\n        self.model_config = vllm_config.model_config\n\n        # Tokenizer (+ ensure liveness if running in another process).\n        self.tokenizer = init_tokenizer_from_configs(\n            model_config=vllm_config.model_config,\n            scheduler_config=vllm_config.scheduler_config,\n            parallel_config=vllm_config.parallel_config,\n            lora_config=vllm_config.lora_config)\n        self.tokenizer.ping()\n\n        # Processor (converts Inputs --> EngineCoreRequests).\n        self.processor = Processor(\n            model_config=vllm_config.model_config,\n            cache_config=vllm_config.cache_config,\n            lora_config=vllm_config.lora_config,\n            tokenizer=self.tokenizer,\n            input_registry=input_registry,\n        )\n\n        # OutputProcessor (converts EngineCoreOutputs --> RequestOutput).\n        self.output_processor = OutputProcessor(self.tokenizer,\n                                                log_stats=self.log_stats)\n\n        # EngineCore (starts the engine in background process).\n        self.engine_core = EngineCoreClient.make_client(\n            multiprocess_mode=True,\n            asyncio_mode=True,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n        )\n\n        self.output_handler: Optional[asyncio.Task] = None\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: AsyncEngineArgs,\n        engine_config: Optional[VllmConfig] = None,\n        start_engine_loop: bool = True,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n    ) -> \"AsyncLLM\":\n        \"\"\"Create an AsyncLLM from the EngineArgs.\"\"\"\n\n        # Create the engine configs.\n        if engine_config is None:\n            vllm_config = engine_args.create_engine_config(usage_context)\n        else:\n            vllm_config = engine_config\n\n        executor_class = Executor.get_class(vllm_config)\n\n        # Create the AsyncLLM.\n        return cls(\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_requests=not engine_args.disable_log_requests,\n            log_stats=not engine_args.disable_log_stats,\n            start_engine_loop=start_engine_loop,\n            usage_context=usage_context,\n        )\n\n    def shutdown(self):\n        \"\"\"Shutdown, cleaning up the background proc and IPC.\"\"\"\n\n        if engine_core := getattr(self, \"engine_core\", None):\n            engine_core.shutdown()\n\n        if handler := getattr(self, \"output_handler\", None):\n            handler.cancel()\n\n    async def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> asyncio.Queue[RequestOutput]:\n        \"\"\"Add new request to the AsyncLLM.\"\"\"\n\n        # 1) Create a new output queue for the request.\n        if self.output_processor.is_request_active(request_id):\n            raise ValueError(f\"Request id {request_id} already running.\")\n        queue: asyncio.Queue[RequestOutput] = asyncio.Queue()\n\n        # 2) Convert Input --> Request.\n        request = self.processor.process_inputs(request_id, prompt, params,\n                                                arrival_time, lora_request,\n                                                trace_headers,\n                                                prompt_adapter_request,\n                                                priority)\n\n        # 3) Add the request to OutputProcessor (this process).\n        self.output_processor.add_request(request, queue)\n\n        # 4) Add the EngineCoreRequest to EngineCore (separate process).\n        await self.engine_core.add_request_async(request)\n\n        if self.log_requests:\n            logger.info(\"Added request %s.\", request_id)\n\n        return queue\n\n    # TODO: we should support multiple prompts in one call, as you\n    # can do with LLM.generate. So that for multi-prompt completion\n    # requests we don't need to send multiple messages to core proc,\n    # and so we don't need multiple streams which then get\n    # re-multiplexed in the API server anyhow.\n    async def generate(\n        self,\n        prompt: PromptType,\n        sampling_params: SamplingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        \"\"\"\n        Main function called by the API server to kick off a request\n            * 1) Making an AsyncStream corresponding to the Request.\n            * 2) Processing the Input.\n            * 3) Adding the Request to the Detokenizer.\n            * 4) Adding the Request to the EngineCore (separate process).\n\n        A separate output_handler loop runs in a background AsyncIO task, \n        pulling outputs from EngineCore and putting them into the \n        per-request AsyncStream.\n\n        The caller of generate() iterates the returned AsyncGenerator,\n        returning the RequestOutput back to the caller.\n        \"\"\"\n\n        try:\n            # We start the output_handler on the first call to generate() so\n            # we can call __init__ before the event loop, which enables us\n            # to handle startup failure gracefully in the OpenAI server.\n            if self.output_handler is None:\n                self.output_handler = asyncio.create_task(\n                    self._run_output_handler())\n\n            q = await self.add_request(\n                request_id,\n                prompt,\n                sampling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority,\n            )\n\n            # The output_handler task pushes items into the queue.\n            # This task pulls from the queue and yields to caller.\n            while True:\n                # Note: drain queue without await if possible (avoids\n                # task switching under load which helps performance).\n                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n\n                # Note: both OutputProcessor and EngineCore handle their\n                # own request cleanup based on finished.\n                if out.finished:\n                    yield out\n                    break\n\n                yield out\n\n        # If the request is disconnected by the client, the\n        # generate() task will be canceled. So, we abort the\n        # request if we end up here.\n        except asyncio.CancelledError:\n            await self.abort(request_id)\n            raise\n\n    async def _run_output_handler(self):\n        \"\"\"Background loop: pulls from EngineCore and pushes to AsyncStreams.\"\"\"\n\n        try:\n            while True:\n                # 1) Pull EngineCoreOutputs from the EngineCore.\n                outputs = await self.engine_core.get_output_async()\n\n                # 2) Process EngineCoreOutputs.\n                processed_outputs = self.output_processor.process_outputs(\n                    outputs.outputs)\n                # NOTE: RequestOutputs are pushed to their queues.\n                assert len(processed_outputs.request_outputs) == 0\n\n                # 3) Abort any reqs that finished due to stop strings.\n                await self.engine_core.abort_requests_async(\n                    processed_outputs.reqs_to_abort)\n\n                # 4) Logging.\n                # TODO(rob): make into a coroutine and launch it in\n                # background thread once we add Prometheus.\n                self._log_stats(\n                    scheduler_stats=outputs.scheduler_stats,\n                    iteration_stats=processed_outputs.iteration_stats,\n                )\n\n        except Exception as e:\n            logger.exception(\"EngineCore output handler hit an error: %s\", e)\n            kill_process_tree(os.getpid())\n\n    async def abort(self, request_id: str) -> None:\n        \"\"\"Abort RequestId in OutputProcessor and EngineCore.\"\"\"\n\n        request_ids = [request_id]\n        await self.engine_core.abort_requests_async(request_ids)\n        self.output_processor.abort_requests(request_ids)\n\n        if self.log_requests:\n            logger.info(\"Aborted request %s.\", request_id)\n\n    def _log_stats(\n        self,\n        scheduler_stats: SchedulerStats,\n        iteration_stats: IterationStats,\n    ):\n        if not self.log_stats:\n            return\n\n        for logger in self.stat_loggers:\n            logger.log(scheduler_stats=scheduler_stats)\n\n    def encode(\n        self,\n        prompt: PromptType,\n        pooling_params: PoolingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ):\n        raise ValueError(\"Not Supported on V1 yet.\")\n\n    async def get_model_config(self) -> ModelConfig:\n        return self.model_config\n\n    async def get_decoding_config(self):\n        raise ValueError(\"Not Supported on V1 yet.\")\n\n    async def get_input_preprocessor(self) -> InputPreprocessor:\n        return self.processor.input_preprocessor\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.tokenizer.get_lora_tokenizer(lora_request)\n\n    async def is_tracing_enabled(self) -> bool:\n        return False\n\n    async def do_log_stats(\n        self,\n        scheduler_outputs=None,\n        model_output=None,\n    ) -> None:\n        logger.debug(\"Called do_log_stats.\")\n\n    async def check_health(self) -> None:\n        logger.debug(\"Called check_health.\")\n\n    async def start_profile(self) -> None:\n        await self.engine_core.profile_async(True)\n\n    async def stop_profile(self) -> None:\n        await self.engine_core.profile_async(False)\n\n    async def reset_prefix_cache(self) -> None:\n        await self.engine_core.reset_prefix_cache_async()\n\n    @property\n    def is_running(self) -> bool:\n        return True\n\n    @property\n    def is_stopped(self) -> bool:\n        return False\n\n    @property\n    def errored(self) -> bool:\n        return False\n\n    @property\n    def dead_error(self) -> BaseException:\n        return Exception()  # TODO: implement\n\n    async def add_lora(self, lora_request: LoRARequest) -> None:\n        \"\"\"Load a new LoRA adapter into the engine for future requests.\"\"\"\n        raise NotImplementedError(\"LoRA not yet supported in V1\")\n",
      "diff": "diff --git a/vllm/v1/engine/async_llm.py b/vllm/v1/engine/async_llm.py\nindex b4d3e4411..1505b6250 100644\n--- a/vllm/v1/engine/async_llm.py\n+++ b/vllm/v1/engine/async_llm.py\n@@ -2,9 +2,12 @@ import asyncio\n import os\n from typing import AsyncGenerator, List, Mapping, Optional, Type, Union\n \n+import numpy as np\n+\n from vllm.config import ModelConfig, VllmConfig\n from vllm.engine.arg_utils import AsyncEngineArgs\n from vllm.engine.protocol import EngineClient\n+from vllm.envs import VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\n from vllm.inputs import INPUT_REGISTRY, InputRegistry, PromptType\n from vllm.inputs.preprocess import InputPreprocessor\n from vllm.logger import init_logger\n@@ -16,7 +19,7 @@ from vllm.sampling_params import SamplingParams\n from vllm.transformers_utils.tokenizer import AnyTokenizer\n from vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\n from vllm.usage.usage_lib import UsageContext\n-from vllm.utils import kill_process_tree\n+from vllm.utils import cdiv, kill_process_tree\n from vllm.v1.engine.core_client import EngineCoreClient\n from vllm.v1.engine.output_processor import OutputProcessor\n from vllm.v1.engine.processor import Processor\n@@ -205,17 +208,15 @@ class AsyncLLM(EngineClient):\n \n             # The output_handler task pushes items into the queue.\n             # This task pulls from the queue and yields to caller.\n-            while True:\n+            finished = False\n+            while not finished:\n                 # Note: drain queue without await if possible (avoids\n                 # task switching under load which helps performance).\n-                out = q.get_nowait() if q.qsize() > 0 else await q.get()\n+                out = q.get_nowait() if not q.empty() else await q.get()\n \n                 # Note: both OutputProcessor and EngineCore handle their\n                 # own request cleanup based on finished.\n-                if out.finished:\n-                    yield out\n-                    break\n-\n+                finished = out.finished\n                 yield out\n \n         # If the request is disconnected by the client, the\n@@ -233,22 +234,41 @@ class AsyncLLM(EngineClient):\n                 # 1) Pull EngineCoreOutputs from the EngineCore.\n                 outputs = await self.engine_core.get_output_async()\n \n-                # 2) Process EngineCoreOutputs.\n-                processed_outputs = self.output_processor.process_outputs(\n-                    outputs.outputs)\n-                # NOTE: RequestOutputs are pushed to their queues.\n-                assert len(processed_outputs.request_outputs) == 0\n-\n-                # 3) Abort any reqs that finished due to stop strings.\n-                await self.engine_core.abort_requests_async(\n-                    processed_outputs.reqs_to_abort)\n+                # Split outputs into chunks of at most\n+                # VLLM_V1_OUTPUT_PROC_CHUNK_SIZE, so that we don't block the\n+                # event loop for too long.\n+                num_outputs = len(outputs.outputs)\n+                if num_outputs <= VLLM_V1_OUTPUT_PROC_CHUNK_SIZE:\n+                    slices = (outputs.outputs, )\n+                else:\n+                    slices = np.array_split(\n+                        outputs.outputs,\n+                        cdiv(num_outputs, VLLM_V1_OUTPUT_PROC_CHUNK_SIZE))\n+\n+                iteration_stats = None\n+                for i, outputs_slice in enumerate(slices):\n+                    # 2) Process EngineCoreOutputs.\n+                    processed_outputs = self.output_processor.process_outputs(\n+                        outputs_slice, iteration_stats)\n+                    # NOTE: RequestOutputs are pushed to their queues.\n+                    assert not processed_outputs.request_outputs\n+                    iteration_stats = processed_outputs.iteration_stats\n+\n+                    # Allow other asyncio tasks to run between chunks\n+                    if i + 1 < len(slices):\n+                        await asyncio.sleep(0)\n+\n+                    # 3) Abort any reqs that finished due to stop strings.\n+                    await self.engine_core.abort_requests_async(\n+                        processed_outputs.reqs_to_abort)\n \n                 # 4) Logging.\n                 # TODO(rob): make into a coroutine and launch it in\n                 # background thread once we add Prometheus.\n+                assert iteration_stats is not None\n                 self._log_stats(\n                     scheduler_stats=outputs.scheduler_stats,\n-                    iteration_stats=processed_outputs.iteration_stats,\n+                    iteration_stats=iteration_stats,\n                 )\n \n         except Exception as e:",
      "change_type": "modified",
      "lines_added": 38,
      "lines_removed": 18
    },
    {
      "file_path": "vllm/v1/engine/core_client.py",
      "old_content": "import os\nimport signal\nimport weakref\nfrom abc import ABC, abstractmethod\nfrom typing import List, Type\n\nimport msgspec\nimport zmq\nimport zmq.asyncio\n\nfrom vllm.config import VllmConfig\nfrom vllm.logger import init_logger\nfrom vllm.utils import (get_open_zmq_ipc_path, kill_process_tree,\n                        make_zmq_socket)\nfrom vllm.v1.engine import (EngineCoreOutputs, EngineCoreProfile,\n                            EngineCoreRequest, EngineCoreRequestType,\n                            EngineCoreRequestUnion, EngineCoreResetPrefixCache)\nfrom vllm.v1.engine.core import EngineCore, EngineCoreProc\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.serial_utils import PickleEncoder\nfrom vllm.v1.utils import BackgroundProcHandle\n\nlogger = init_logger(__name__)\n\n\nclass EngineCoreClient(ABC):\n    \"\"\"\n    EngineCoreClient: subclasses handle different methods for pushing \n        and pulling from the EngineCore for asyncio / multiprocessing.\n\n    Subclasses:\n    * InprocClient: In process EngineCore (for V0-style LLMEngine use)\n    * SyncMPClient: ZMQ + background proc EngineCore (for LLM)\n    * AsyncMPClient: ZMQ + background proc EngineCore w/ asyncio (for AsyncLLM)\n    \"\"\"\n\n    @staticmethod\n    def make_client(\n        multiprocess_mode: bool,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: Type[Executor],\n    ) -> \"EngineCoreClient\":\n\n        # TODO: support this for debugging purposes.\n        if asyncio_mode and not multiprocess_mode:\n            raise NotImplementedError(\n                \"Running EngineCore in asyncio without multiprocessing \"\n                \"is not currently supported.\")\n\n        if multiprocess_mode and asyncio_mode:\n            return AsyncMPClient(vllm_config, executor_class)\n\n        if multiprocess_mode and not asyncio_mode:\n            return SyncMPClient(vllm_config, executor_class)\n\n        return InprocClient(vllm_config, executor_class)\n\n    @abstractmethod\n    def shutdown(self):\n        ...\n\n    def get_output(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    def profile(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    def reset_prefix_cache(self) -> None:\n        raise NotImplementedError\n\n    def abort_requests(self, request_ids: List[str]) -> None:\n        raise NotImplementedError\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    async def reset_prefix_cache_async(self) -> None:\n        raise NotImplementedError\n\n    async def abort_requests_async(self, request_ids: List[str]) -> None:\n        raise NotImplementedError\n\n\nclass InprocClient(EngineCoreClient):\n    \"\"\"\n    InprocClient: client for in-process EngineCore. Intended \n    for use in LLMEngine for V0-style add_request() and step()\n        EngineCore setup in this process (no busy loop).\n\n        * pushes EngineCoreRequest directly into the EngineCore\n        * pulls EngineCoreOutputs by stepping the EngineCore\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.engine_core = EngineCore(*args, **kwargs)\n\n    def get_output(self) -> EngineCoreOutputs:\n        return self.engine_core.step()\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        self.engine_core.add_request(request)\n\n    def abort_requests(self, request_ids: List[str]) -> None:\n        if len(request_ids) > 0:\n            self.engine_core.abort_requests(request_ids)\n\n    def shutdown(self) -> None:\n        self.engine_core.shutdown()\n\n    def profile(self, is_start: bool = True) -> None:\n        self.engine_core.profile(is_start)\n\n    def reset_prefix_cache(self) -> None:\n        self.engine_core.reset_prefix_cache()\n\n\nclass MPClient(EngineCoreClient):\n    \"\"\"\n    MPClient: base client for multi-proc EngineCore.\n        EngineCore runs in a background process busy loop, getting\n        new EngineCoreRequests and returning EngineCoreOutputs\n\n        * pushes EngineCoreRequests via input_socket\n        * pulls EngineCoreOutputs via output_socket\n    \n        * AsyncMPClient subclass for AsyncLLM usage\n        * SyncMPClient subclass for LLM usage\n    \"\"\"\n\n    def __init__(\n        self,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: Type[Executor],\n        log_stats: bool,\n    ):\n        # The child processes will send SIGUSR1 when unrecoverable\n        # errors happen. We kill the process tree here so that the\n        # stack trace is very evident.\n        # TODO(rob): rather than killing the main process, we should\n        # figure out how to raise an AsyncEngineDeadError and\n        # handle at the API server level so we can return a better\n        # error code to the clients calling VLLM.\n        def sigusr1_handler(signum, frame):\n            logger.fatal(\"Got fatal signal from worker processes, shutting \"\n                         \"down. See stack trace above for root cause issue.\")\n            kill_process_tree(os.getpid())\n\n        signal.signal(signal.SIGUSR1, sigusr1_handler)\n\n        # Serialization setup.\n        self.encoder = PickleEncoder()\n        self.decoder = msgspec.msgpack.Decoder(EngineCoreOutputs)\n\n        # ZMQ setup.\n        self.ctx = (\n            zmq.asyncio.Context()  # type: ignore[attr-defined]\n            if asyncio_mode else zmq.Context())  # type: ignore[attr-defined]\n\n        # Note(rob): shutdown function cannot be a bound method,\n        # else the gc cannot collect the object.\n        self._finalizer = weakref.finalize(self, lambda x: x.destroy(linger=0),\n                                           self.ctx)\n\n        # Paths and sockets for IPC.\n        output_path = get_open_zmq_ipc_path()\n        input_path = get_open_zmq_ipc_path()\n        self.output_socket = make_zmq_socket(self.ctx, output_path,\n                                             zmq.constants.PULL)\n        self.input_socket = make_zmq_socket(self.ctx, input_path,\n                                            zmq.constants.PUSH)\n\n        # Start EngineCore in background process.\n        self.proc_handle = BackgroundProcHandle(\n            input_path=input_path,\n            output_path=output_path,\n            process_name=\"EngineCore\",\n            target_fn=EngineCoreProc.run_engine_core,\n            process_kwargs={\n                \"vllm_config\": vllm_config,\n                \"executor_class\": executor_class,\n                \"log_stats\": log_stats,\n            })\n\n    def shutdown(self):\n        \"\"\"Clean up background resources.\"\"\"\n        if hasattr(self, \"proc_handle\"):\n            self.proc_handle.shutdown()\n\n        self._finalizer()\n\n\nclass SyncMPClient(MPClient):\n    \"\"\"Synchronous client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig,\n                 executor_class: Type[Executor]):\n        super().__init__(\n            asyncio_mode=False,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=False,\n        )\n\n    def get_output(self) -> EngineCoreOutputs:\n\n        (frame, ) = self.output_socket.recv_multipart(copy=False)\n        return self.decoder.decode(frame.buffer)\n\n    def _send_input(self, request_type: EngineCoreRequestType,\n                    request: EngineCoreRequestUnion) -> None:\n\n        # (RequestType, SerializedRequest)\n        msg = (request_type.value, self.encoder.encode(request))\n        self.input_socket.send_multipart(msg, copy=False)\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        # NOTE: text prompt is not needed in the core engine as it has been\n        # tokenized.\n        request.prompt = None\n        self._send_input(EngineCoreRequestType.ADD, request)\n\n    def abort_requests(self, request_ids: List[str]) -> None:\n        if len(request_ids) > 0:\n            self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    def profile(self, is_start: bool = True) -> None:\n        self._send_input(EngineCoreRequestType.PROFILE,\n                         EngineCoreProfile(is_start))\n\n    def reset_prefix_cache(self) -> None:\n        self._send_input(EngineCoreRequestType.RESET_PREFIX_CACHE,\n                         EngineCoreResetPrefixCache())\n\n\nclass AsyncMPClient(MPClient):\n    \"\"\"Asyncio-compatible client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig,\n                 executor_class: Type[Executor]):\n        super().__init__(\n            asyncio_mode=True,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=True,\n        )\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n\n        frames = await self.output_socket.recv_multipart(copy=False)\n        return self.decoder.decode(frames[0].buffer)\n\n    async def _send_input(self, request_type: EngineCoreRequestType,\n                          request: EngineCoreRequestUnion) -> None:\n\n        msg = (request_type.value, self.encoder.encode(request))\n        await self.input_socket.send_multipart(msg, copy=False)\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        # NOTE: text prompt is not needed in the core engine as it has been\n        # tokenized.\n        request.prompt = None\n        await self._send_input(EngineCoreRequestType.ADD, request)\n\n    async def abort_requests_async(self, request_ids: List[str]) -> None:\n        if len(request_ids) > 0:\n            await self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        await self._send_input(EngineCoreRequestType.PROFILE,\n                               EngineCoreProfile(is_start))\n\n    async def reset_prefix_cache_async(self) -> None:\n        await self._send_input(EngineCoreRequestType.RESET_PREFIX_CACHE,\n                               EngineCoreResetPrefixCache())\n",
      "diff": "diff --git a/vllm/v1/engine/core_client.py b/vllm/v1/engine/core_client.py\nindex 19b89003c..f3b992d68 100644\n--- a/vllm/v1/engine/core_client.py\n+++ b/vllm/v1/engine/core_client.py\n@@ -1,8 +1,9 @@\n+import asyncio\n import os\n import signal\n import weakref\n from abc import ABC, abstractmethod\n-from typing import List, Type\n+from typing import List, Optional, Type\n \n import msgspec\n import zmq\n@@ -255,10 +256,24 @@ class AsyncMPClient(MPClient):\n             log_stats=True,\n         )\n \n+        self.outputs_queue: Optional[asyncio.Queue[bytes]] = None\n+        self.queue_task: Optional[asyncio.Task] = None\n+\n     async def get_output_async(self) -> EngineCoreOutputs:\n+        if self.outputs_queue is None:\n+            # Perform IO in separate task to parallelize as much as possible\n+            self.outputs_queue = asyncio.Queue()\n+\n+            async def process_outputs_socket():\n+                assert self.outputs_queue is not None\n+                while True:\n+                    (frame, ) = await self.output_socket.recv_multipart(\n+                        copy=False)\n+                    self.outputs_queue.put_nowait(frame.buffer)\n+\n+            self.queue_task = asyncio.create_task(process_outputs_socket())\n \n-        frames = await self.output_socket.recv_multipart(copy=False)\n-        return self.decoder.decode(frames[0].buffer)\n+        return self.decoder.decode(await self.outputs_queue.get())\n \n     async def _send_input(self, request_type: EngineCoreRequestType,\n                           request: EngineCoreRequestUnion) -> None:",
      "change_type": "modified",
      "lines_added": 19,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/v1/engine/output_processor.py",
      "old_content": "import asyncio\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nfrom vllm.outputs import RequestOutput\nfrom vllm.transformers_utils.detokenizer_utils import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import BaseTokenizerGroup\nfrom vllm.v1.engine import EngineCoreOutput, EngineCoreRequest\nfrom vllm.v1.engine.detokenizer import (DetokenizerOutput,\n                                        IncrementalDetokenizer)\nfrom vllm.v1.metrics.stats import IterationStats\n\n\n@dataclass\nclass OutputProcessorOutput:\n\n    request_outputs: List[RequestOutput]\n    reqs_to_abort: List[str]\n    iteration_stats: IterationStats\n\n\nclass RequestState:\n\n    def __init__(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        prompt_token_ids: List[int],\n        detokenizer: IncrementalDetokenizer,\n        queue: Optional[asyncio.Queue[RequestOutput]],\n    ):\n        self.request_id = request_id\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.prompt_len = len(prompt_token_ids)\n        self.detokenizer = detokenizer\n        self.is_prefilling = True\n        self.queue = queue\n\n    @classmethod\n    def from_new_request(\n        cls,\n        tokenizer: AnyTokenizer,\n        request: EngineCoreRequest,\n        queue: Optional[asyncio.Queue[RequestOutput]] = None,\n    ) -> \"RequestState\":\n        return cls(\n            request_id=request.request_id,\n            prompt=request.prompt,\n            prompt_token_ids=request.prompt_token_ids,\n            detokenizer=IncrementalDetokenizer.from_new_request(\n                tokenizer=tokenizer,\n                request=request,\n            ),\n            queue=queue,\n        )\n\n\nclass OutputProcessor:\n    \"\"\"Process EngineCoreOutputs into RequestOutputs.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: BaseTokenizerGroup,\n        log_stats: bool,\n    ):\n        self.log_stats = log_stats\n        self.tokenizer = tokenizer\n        self.request_states: Dict[str, RequestState] = {}\n\n    def is_request_active(self, request_id: str) -> bool:\n        return request_id in self.request_states\n\n    def get_num_unfinished_requests(self):\n        return len(self.request_states)\n\n    def has_unfinished_requests(self) -> bool:\n        return len(self.request_states) > 0\n\n    def abort_requests(\n        self,\n        request_ids: List[str],\n    ) -> None:\n        for request_id in request_ids:\n            self.request_states.pop(request_id, None)\n\n    def add_request(\n        self,\n        request: EngineCoreRequest,\n        queue: Optional[asyncio.Queue[RequestOutput]] = None,\n    ) -> None:\n        request_id = request.request_id\n        if request_id in self.request_states:\n            raise ValueError(f\"Request id {request_id} already running.\")\n\n        self.request_states[request_id] = RequestState.from_new_request(\n            tokenizer=self.tokenizer.get_lora_tokenizer(request.lora_request),\n            request=request,\n            queue=queue)\n\n    def process_outputs(\n        self,\n        engine_core_outputs: List[EngineCoreOutput],\n    ) -> OutputProcessorOutput:\n        \"\"\"\n        Process the EngineCoreOutputs:\n        1) Compute stats for logging\n        2) Detokenize\n        3) Create and handle RequestOutput objects:\n            * If there is a queue (for usage with AsyncLLM), \n              put the RequestOutput objects into the queue for\n              handling by the per-request generate() tasks.\n\n            * If there is no queue (for usage with LLMEngine), \n              return a list of RequestOutput objects.\n\n        ****************** NOTE FOR DEVELOPERS ******************\n\n        VLLM V1 minimizes the number of python loops over the full\n        batch to ensure system overheads are minimized. This is the \n        only function that should loop over EngineCoreOutputs.\n\n        If you need to touch every element of the batch, implement a\n        method called XXXClass.update_from_output() to be called\n        within the loop below. For examples, see:\n            * IterationStats.update_from_output()\n            * Detokenizer.update_from_output()\n        \n        TODO(rob): add Protocol makes update_from_output explicit.\n        \n        **********************************************************\n        \"\"\"\n\n        request_outputs: List[RequestOutput] = []\n        reqs_to_abort: List[str] = []\n        iteration_stats = IterationStats(self.log_stats)\n        for engine_core_output in engine_core_outputs:\n            req_id = engine_core_output.request_id\n            req_state = self.request_states.get(req_id)\n            if req_state is None:\n                # Ignore output for already-aborted request.\n                continue\n\n            # 1) Compute stats for this iteration.\n            iteration_stats.update_from_output(engine_core_output,\n                                               req_state.is_prefilling,\n                                               req_state.prompt_len)\n            req_state.is_prefilling = False\n\n            # 2) Detokenize the token ids into text.\n            detokenizer_output = req_state.detokenizer.update_from_output(\n                engine_core_output)\n\n            # 3) Create and handle RequestOutput objects.\n            if request_output := self._make_request_output(\n                    req_state, detokenizer_output):\n                if req_state.queue is not None:\n                    # AsyncLLM: put into queue for handling by generate().\n                    req_state.queue.put_nowait(request_output)\n                else:\n                    # LLMEngine: return list of RequestOutputs.\n                    request_outputs.append(request_output)\n\n                # Free completed requests.\n                if request_output.finished:\n                    self.request_states.pop(req_id)\n                    if not engine_core_output.finished:\n                        # If req not finished in EngineCore, but Detokenizer\n                        # detected stop string, abort needed in EngineCore.\n                        reqs_to_abort.append(req_id)\n\n        return OutputProcessorOutput(\n            request_outputs=request_outputs,\n            reqs_to_abort=reqs_to_abort,\n            iteration_stats=iteration_stats,\n        )\n\n    def _make_request_output(\n        self,\n        request_state: RequestState,\n        detokenizer_output: Optional[DetokenizerOutput],\n    ) -> Optional[RequestOutput]:\n\n        if detokenizer_output is None:\n            return None\n\n        request_output = RequestOutput.new(\n            request_state.request_id,\n            request_state.prompt,\n            request_state.prompt_token_ids,\n            detokenizer_output.output_text,\n            detokenizer_output.token_ids,\n            detokenizer_output.finished,\n        )\n        if detokenizer_output.finished:\n            completion_output = request_output.outputs[0]\n            completion_output.finish_reason = detokenizer_output.finish_reason\n            completion_output.stop_reason = detokenizer_output.stop_reason\n\n        return request_output\n",
      "diff": "diff --git a/vllm/v1/engine/output_processor.py b/vllm/v1/engine/output_processor.py\nindex 749f4f504..564eab51b 100644\n--- a/vllm/v1/engine/output_processor.py\n+++ b/vllm/v1/engine/output_processor.py\n@@ -101,6 +101,7 @@ class OutputProcessor:\n     def process_outputs(\n         self,\n         engine_core_outputs: List[EngineCoreOutput],\n+        iteration_stats: Optional[IterationStats] = None,\n     ) -> OutputProcessorOutput:\n         \"\"\"\n         Process the EngineCoreOutputs:\n@@ -133,7 +134,8 @@ class OutputProcessor:\n \n         request_outputs: List[RequestOutput] = []\n         reqs_to_abort: List[str] = []\n-        iteration_stats = IterationStats(self.log_stats)\n+        if not iteration_stats:\n+            iteration_stats = IterationStats(self.log_stats)\n         for engine_core_output in engine_core_outputs:\n             req_id = engine_core_output.request_id\n             req_state = self.request_states.get(req_id)\n@@ -175,8 +177,8 @@ class OutputProcessor:\n             iteration_stats=iteration_stats,\n         )\n \n+    @staticmethod\n     def _make_request_output(\n-        self,\n         request_state: RequestState,\n         detokenizer_output: Optional[DetokenizerOutput],\n     ) -> Optional[RequestOutput]:",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/v1/request.py",
      "old_content": "import enum\nfrom typing import TYPE_CHECKING, List, Optional, Union\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import RequestMetrics\nfrom vllm.v1.engine import EngineCoreRequest\nfrom vllm.v1.utils import ConstantList\n\nif TYPE_CHECKING:\n    from vllm.multimodal import MultiModalKwargs\n    from vllm.multimodal.inputs import PlaceholderRange\n    from vllm.v1.core.kv_cache_utils import BlockHashType\n\n\nclass Request:\n\n    def __init__(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        prompt_token_ids: List[int],\n        multi_modal_inputs: Optional[List[\"MultiModalKwargs\"]],\n        multi_modal_hashes: Optional[List[str]],\n        multi_modal_placeholders: Optional[List[\"PlaceholderRange\"]],\n        sampling_params: SamplingParams,\n        eos_token_id: Optional[int],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.sampling_params = sampling_params\n        # Because of LoRA, the eos token id can be different for each request.\n        self.eos_token_id = eos_token_id\n        self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                      last_token_time=arrival_time,\n                                      first_scheduled_time=None,\n                                      first_token_time=None,\n                                      time_in_queue=None)\n        self.lora_request = lora_request\n\n        self.status = RequestStatus.WAITING\n        self.stop_reason: Union[int, str, None] = None\n        assert sampling_params.max_tokens is not None\n        self.max_tokens = sampling_params.max_tokens\n\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.num_prompt_tokens = len(self.prompt_token_ids)\n        self._output_token_ids: List[int] = []\n        self._all_token_ids: List[int] = self.prompt_token_ids.copy()\n        self.num_computed_tokens = 0\n\n        # Multi-modal related\n        self.mm_positions = multi_modal_placeholders or []\n        self.mm_inputs = multi_modal_inputs or []\n        self.mm_hashes: List[str] = multi_modal_hashes or []\n\n        # Sanity check\n        assert len(self.mm_inputs) == len(self.mm_positions)\n        assert len(self.mm_inputs) == len(self.mm_hashes)\n\n        # Cache the computed kv block hashes of the request to avoid\n        # recomputing.\n        self._kv_block_hashes: List[BlockHashType] = []\n\n    @classmethod\n    def from_engine_core_request(cls, request: EngineCoreRequest) -> \"Request\":\n        return cls(\n            request_id=request.request_id,\n            prompt=request.prompt,\n            prompt_token_ids=request.prompt_token_ids,\n            multi_modal_inputs=request.mm_inputs,\n            multi_modal_hashes=request.mm_hashes,\n            multi_modal_placeholders=request.mm_placeholders,\n            sampling_params=request.sampling_params,\n            eos_token_id=request.eos_token_id,\n            arrival_time=request.arrival_time,\n            lora_request=request.lora_request,\n        )\n\n    @property\n    def output_token_ids(self) -> ConstantList[int]:\n        # Prevent directly appending to the output_token_ids since\n        # all_token_ids should also be updated simultaneously.\n        return ConstantList(self._output_token_ids)\n\n    @property\n    def all_token_ids(self) -> ConstantList[int]:\n        # Prevent directly appending to the all_token_ids since\n        # output_token_ids should also be updated simultaneously\n        return ConstantList(self._all_token_ids)\n\n    def append_output_token_ids(\n        self,\n        token_ids: Union[int, List[int]],\n    ) -> None:\n        if isinstance(token_ids, int):\n            token_ids = [token_ids]\n        self._output_token_ids.extend(token_ids)\n        self._all_token_ids.extend(token_ids)\n\n    @property\n    def num_tokens(self) -> int:\n        return len(self._all_token_ids)\n\n    @property\n    def num_output_tokens(self) -> int:\n        return len(self._output_token_ids)\n\n    def is_finished(self) -> bool:\n        return RequestStatus.is_finished(self.status)\n\n    def get_finished_reason(self) -> Union[str, None]:\n        return RequestStatus.get_finished_reason(self.status)\n\n    def has_encoder_inputs(self) -> bool:\n        return len(self.mm_inputs) > 0\n\n    @property\n    def num_encoder_inputs(self) -> int:\n        return len(self.mm_positions)\n\n    def get_num_encoder_tokens(self, input_id: int) -> int:\n        assert input_id < len(self.mm_positions)\n        num_tokens = self.mm_positions[input_id][\"length\"]\n        return num_tokens\n\n    @property\n    def kv_block_hashes(self) -> ConstantList[\"BlockHashType\"]:\n        # Prevent directly appending to the kv_block_hashes.\n        return ConstantList(self._kv_block_hashes)\n\n    def set_kv_block_hashes(self, value: List[\"BlockHashType\"]) -> None:\n        self._kv_block_hashes = value\n\n    def append_kv_block_hashes(self, block_hash: \"BlockHashType\") -> None:\n        self._kv_block_hashes.append(block_hash)\n\n\nclass RequestStatus(enum.IntEnum):\n    \"\"\"Status of a request.\"\"\"\n    WAITING = 0\n    RUNNING = 1\n    PREEMPTED = 2\n    # Note: anything after PREEMPTED (2) will be considered\n    # as a finished status.\n    FINISHED_STOPPED = 3\n    FINISHED_LENGTH_CAPPED = 4\n    FINISHED_ABORTED = 5\n    FINISHED_IGNORED = 6\n\n    @staticmethod\n    def is_finished(status: \"RequestStatus\") -> bool:\n        return status > RequestStatus.PREEMPTED\n\n    @staticmethod\n    def get_finished_reason(status: \"RequestStatus\") -> Union[str, None]:\n        return _FINISHED_REASON_MAP.get(status)\n\n\n# Mapping of finished statuses to their finish reasons.\n# NOTE: The ignored requests are the requests whose prompt lengths\n# are longer than the model's length cap. Therefore, the stop\n# reason should also be \"length\" as in OpenAI API.\n_FINISHED_REASON_MAP = {\n    RequestStatus.FINISHED_STOPPED: \"stop\",\n    RequestStatus.FINISHED_LENGTH_CAPPED: \"length\",\n    RequestStatus.FINISHED_ABORTED: \"abort\",\n    RequestStatus.FINISHED_IGNORED: \"length\",\n}\n",
      "diff": "diff --git a/vllm/v1/request.py b/vllm/v1/request.py\nindex 45450165e..eefcdaf29 100644\n--- a/vllm/v1/request.py\n+++ b/vllm/v1/request.py\n@@ -64,6 +64,12 @@ class Request:\n         # recomputing.\n         self._kv_block_hashes: List[BlockHashType] = []\n \n+        # Read-only views\n+        # Prevent directly appending to the these lists since\n+        # they should also be updated simultaneously.\n+        self.output_token_ids = ConstantList(self._output_token_ids)\n+        self.all_token_ids = ConstantList(self._all_token_ids)\n+\n     @classmethod\n     def from_engine_core_request(cls, request: EngineCoreRequest) -> \"Request\":\n         return cls(\n@@ -79,18 +85,6 @@ class Request:\n             lora_request=request.lora_request,\n         )\n \n-    @property\n-    def output_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the output_token_ids since\n-        # all_token_ids should also be updated simultaneously.\n-        return ConstantList(self._output_token_ids)\n-\n-    @property\n-    def all_token_ids(self) -> ConstantList[int]:\n-        # Prevent directly appending to the all_token_ids since\n-        # output_token_ids should also be updated simultaneously\n-        return ConstantList(self._all_token_ids)\n-\n     def append_output_token_ids(\n         self,\n         token_ids: Union[int, List[int]],",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 13
    }
  ],
  "affected_apis": [
    "vllm.v1.engine.core_client.AsyncMPClient.get_output_async",
    "vllm.v1.engine.output_processor.OutputProcessor.process_outputs",
    "vllm.v1.engine.output_processor.OutputProcessor._make_request_output",
    "vllm.v1.request.Request.output_token_ids",
    "vllm.v1.request.Request.all_token_ids"
  ],
  "summary": {
    "total_files": 7,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 7
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_async_llm, benchmark_serving)",
    "is_benchmark_actually_there": "",
    "sample_clues": "api_server, async, async_llm"
  }
}