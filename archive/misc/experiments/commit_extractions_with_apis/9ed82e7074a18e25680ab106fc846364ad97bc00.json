{
  "commit_hash": "9ed82e7074a18e25680ab106fc846364ad97bc00",
  "parent_hash": "51f8aa90ad409cc77bfab208be7f5907bf7d5330",
  "message": "[Misc] Small perf improvements (#6520)",
  "author": "Antoni Baum <antoni.baum@protonmail.com>",
  "date": "2024-07-19 12:10:56 -0700",
  "files_changed": [
    {
      "file_path": "tests/core/block/test_block_manager_v2.py",
      "old_content": "import pytest\n\nfrom vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                                   STR_NOT_IMPL_ENC_DEC_SWA)\nfrom vllm.core.block_manager_v2 import BlockSpaceManagerV2\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.sequence import Logprob, SequenceStatus\nfrom vllm.utils import chunk_list\n\nfrom ..utils import (create_dummy_prompt, create_seq_group,\n                     create_seq_group_encoder_decoder)\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [8, 40, 80])\n@pytest.mark.parametrize(\"num_seqs_per_group\", [1, 4])\n@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\ndef test_can_allocate_seq_group(block_size: int, num_seqs_per_group: int,\n                                num_gpu_blocks: int, watermark: float):\n    block_manager = BlockSpaceManagerV2(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=1024,\n        watermark=watermark,\n    )\n    num_watermark_blocks = int(watermark * num_gpu_blocks)\n\n    num_output_blocks_per_seq = 1\n\n    # NOTE: This should be num_output_blocks_per_seq * num_seqs_per_group, but\n    # the current implementation assumes all seqs are new prompts / don't have\n    # different output lens.\n    num_output_blocks = num_output_blocks_per_seq\n\n    for num_prompt_blocks in range(1, num_gpu_blocks - num_output_blocks):\n        seq_group = create_seq_group(\n            seq_prompt_len=block_size * num_prompt_blocks,\n            seq_output_lens=[\n                block_size * num_output_blocks_per_seq\n                for _ in range(num_seqs_per_group)\n            ],\n        )\n\n        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n\n        can_allocate_result = block_manager.can_allocate(seq_group)\n\n        num_required_blocks = num_prompt_blocks + num_output_blocks\n\n        if num_gpu_blocks - num_required_blocks < num_watermark_blocks:\n            assert can_allocate_result == AllocStatus.NEVER\n        elif num_gpu_blocks >= num_required_blocks:\n            assert can_allocate_result == AllocStatus.OK\n        else:\n            assert can_allocate_result == AllocStatus.LATER\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [16, 80, 160])\n@pytest.mark.parametrize(\"num_seqs_per_group\", [1, 4])\n@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\ndef test_can_allocate_seq_group_encoder_decoder(block_size: int,\n                                                num_seqs_per_group: int,\n                                                num_gpu_blocks: int,\n                                                watermark: float):\n    block_manager = BlockSpaceManagerV2(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=1024,\n        watermark=watermark,\n    )\n    num_watermark_blocks = int(watermark * num_gpu_blocks)\n\n    num_output_blocks_per_seq = 1\n\n    # NOTE: This should be num_output_blocks_per_seq * num_seqs_per_group, but\n    # the current implementation assumes all seqs are new prompts / don't have\n    # different output lens.\n    num_output_blocks = num_output_blocks_per_seq\n\n    for bdx, num_prompt_blocks in enumerate(\n            range(1, num_gpu_blocks - num_output_blocks)):\n        num_cross_blocks_per_seq = num_prompt_blocks\n\n        seq_group = create_seq_group_encoder_decoder(\n            seq_prompt_len=block_size * num_prompt_blocks,\n            seq_output_lens=[\n                block_size * num_output_blocks_per_seq\n                for _ in range(num_seqs_per_group)\n            ],\n            request_id=str(bdx))\n\n        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n\n        can_allocate_result = block_manager.can_allocate(seq_group)\n\n        num_required_blocks = num_prompt_blocks + \\\n                              num_output_blocks + \\\n                              num_cross_blocks_per_seq\n\n        if num_gpu_blocks - num_required_blocks < num_watermark_blocks:\n            assert can_allocate_result == AllocStatus.NEVER\n        elif num_gpu_blocks >= num_required_blocks:\n            assert can_allocate_result == AllocStatus.OK\n        else:\n            assert can_allocate_result == AllocStatus.LATER\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [16])\n@pytest.mark.parametrize(\"num_seqs_per_group\", [1])\n@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\ndef test_can_allocate_encoder_decoder_fails_with_swa(block_size: int,\n                                                     num_seqs_per_group: int,\n                                                     num_gpu_blocks: int,\n                                                     watermark: float):\n    '''\n    SWA short for Sliding Window Attention.\n\n    At time of writing block manager v2 does not support SWA.\n\n    However even when SWA is implemented for block manager v2,\n    there will still most likely be a separate workstream required\n    to enable SWA for encoder/decoder models.\n\n    Therefore this test enforces that one of the following cases\n    hold true:\n    1. Block manager v2 does not support SWA at all (true at time of writing)\n    2. Block manager v2 fails with NotImplementError when SWA is enabled\n       AND a SequenceGroup with an encoder sequence (i.e. in support of an\n       encoder/decoder model) is passed into can_allocate() as an argument\n\n    The setup for this test is stripped down version of\n    test_can_allocate_seq_group_encoder_decoder()\n    '''\n\n    with pytest.raises((NotImplementedError, AssertionError)) as exc_info:\n        block_manager = BlockSpaceManagerV2(\n            block_size=block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=1024,\n            watermark=watermark,\n            sliding_window=5  # SWA\n        )\n\n        num_output_blocks_per_seq = 1\n        num_prompt_blocks = 1\n        num_output_blocks = num_output_blocks_per_seq\n        seq_group = create_seq_group_encoder_decoder(\n            seq_prompt_len=block_size * num_prompt_blocks,\n            seq_output_lens=[\n                block_size * num_output_blocks_per_seq\n                for _ in range(num_seqs_per_group)\n            ],\n            request_id=\"0\")\n\n        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n        block_manager.can_allocate(seq_group)\n\n    # Assert that either\n    # 1. Block manager v2 constructor fails with assertion that sliding window\n    #    is not yet supported (most likely near-term outcome at time of\n    #    writing), or\n    # 2. can_allocate() fails with NotImplementedError due to combination of\n    #    encoder/decoder and sliding window attention\n    if isinstance(exc_info.value, NotImplementedError):\n        assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n    elif isinstance(exc_info.value, AssertionError):\n        assert str(exc_info.value) == \"Sliding window not yet supported\"\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [16])\n@pytest.mark.parametrize(\"num_seqs_per_group\", [1])\n@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\ndef test_can_allocate_encoder_decoder_fails_with_prefix_cache(\n        block_size: int, num_seqs_per_group: int, num_gpu_blocks: int,\n        watermark: float):\n\n    block_manager = BlockSpaceManagerV2(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=1024,\n        watermark=watermark,\n        enable_caching=True  # Prefix cache\n    )\n\n    num_output_blocks_per_seq = 1\n    num_prompt_blocks = 1\n    num_output_blocks = num_output_blocks_per_seq\n    seq_group = create_seq_group_encoder_decoder(\n        seq_prompt_len=block_size * num_prompt_blocks,\n        seq_output_lens=[\n            block_size * num_output_blocks_per_seq\n            for _ in range(num_seqs_per_group)\n        ],\n        request_id=\"0\")\n\n    assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n\n    # Assert that either can_allocate() fails with NotImplementedError\n    # due to combination of encoder/decoder and prefix cache\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.can_allocate(seq_group)\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n\n\n@pytest.mark.parametrize(\"block_size\", [1, 8])\n@pytest.mark.parametrize(\"prompt_len\", [1, 7, 8])\n@pytest.mark.parametrize(\"num_slots_to_append\", [1, 8, 129])\n@pytest.mark.parametrize(\"num_lookahead_slots\", [0, 10])\ndef test_append_slots(block_size, prompt_len, num_slots_to_append,\n                      num_lookahead_slots):\n    \"\"\"Verify append_slots consumes the correct number of blocks from the block\n    table.\n    \"\"\"\n\n    num_gpu_blocks = 1024\n    watermark = 0.1\n    block_manager = BlockSpaceManagerV2(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=0,\n        watermark=watermark,\n    )\n\n    seq_group = create_seq_group(\n        seq_prompt_len=prompt_len,\n        seq_output_lens=[0],\n    )\n\n    # Allocate seq\n    assert block_manager.can_allocate(seq_group)\n    block_manager.allocate(seq_group)\n\n    # Seq seq to RUNNING\n    seq = seq_group.get_seqs()[0]\n    seq.status = SequenceStatus.RUNNING\n\n    # Append tokens to the sequeqnce\n    for token_id in range(num_slots_to_append):\n        seq.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Append slots for new tokens and lookahead slots.\n    free_blocks_before_append = block_manager.get_num_free_gpu_blocks()\n    block_manager.append_slots(seq, num_lookahead_slots)\n    num_consumed_blocks = (free_blocks_before_append -\n                           block_manager.get_num_free_gpu_blocks())\n\n    # Expect consumed blocks to be new blocks required to support the new slots.\n    expected_consumed_blocks = len(\n        chunk_list(\n            list(\n                range(prompt_len + num_slots_to_append + num_lookahead_slots)),\n            block_size)) - len(chunk_list(list(range(prompt_len)), block_size))\n    assert num_consumed_blocks == expected_consumed_blocks\n\n\n@pytest.mark.parametrize(\"block_size\", [8])\n@pytest.mark.parametrize(\"num_cpu_blocks\", [4])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [4])\n@pytest.mark.parametrize(\"num_lookahead_slots\", [0, 2, 10])\n@pytest.mark.parametrize(\"enable_caching\", [False, True])\ndef test_swap(block_size, num_cpu_blocks, num_gpu_blocks, num_lookahead_slots,\n              enable_caching):\n    \"\"\"Verify blocks number on src/desc device is correct after swapping in/out\n        sequence group (not missing or extra blocks).\n    \"\"\"\n    block_manager = BlockSpaceManagerV2(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0,\n                                        enable_caching=enable_caching)\n    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n    prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    prompt.status = SequenceStatus.RUNNING\n    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap seq group from GPU -> CPU.\n    gpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    mapping_keys = [key for key, _ in mapping]\n    assert mapping_keys == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    prompt.status = SequenceStatus.SWAPPED\n\n    # Swap seq group from CPU -> GPU.\n    assert block_manager.can_swap_in(seq_group, num_lookahead_slots)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    cpu_blocks = block_manager.get_block_table(prompt)\n    mapping_keys = [key for key, _ in mapping]\n    assert mapping_keys == [cpu_blocks[0]]\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\n# TODO(cade/kaiyang): add comprehensive tests for swapping at allocator level.\n\n\n@pytest.mark.parametrize(\"block_size\", [8, 16])\n@pytest.mark.parametrize(\"prompt_len\", [10, 300, 1000])\n@pytest.mark.parametrize(\"num_slots_to_append\", [50])\n@pytest.mark.parametrize(\"sliding_window\", [20, 32, 200, 512])\ndef test_sliding_window(block_size, prompt_len, num_slots_to_append,\n                        sliding_window):\n    \"\"\"Verify append_slots consumes the correct number of blocks from the block\n    table.\n    \"\"\"\n\n    num_gpu_blocks = 1024\n    watermark = 0.1\n    block_manager = BlockSpaceManagerV2(\n        block_size=block_size,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=0,\n        watermark=watermark,\n        sliding_window=sliding_window,\n    )\n\n    def check_used(min_n, max_n=None):\n        if max_n is None:\n            max_n = min_n\n        used = num_gpu_blocks - block_manager.get_num_free_gpu_blocks()\n        #print(\"check\", min_n, used, max_n)\n        assert min_n <= used\n        assert used <= max_n\n\n    def num_blocks(num_tokens):\n        return (num_tokens + block_size - 1) // block_size\n\n    check_used(0)\n\n    seq_group = create_seq_group(\n        seq_prompt_len=prompt_len,\n        seq_output_lens=[0],\n    )\n\n    check_used(0)\n\n    # Allocate seq\n    assert block_manager.can_allocate(seq_group)\n    block_manager.allocate(seq_group)\n\n    check_used(num_blocks(prompt_len))\n\n    # Seq seq to RUNNING\n    seq = seq_group.get_seqs()[0]\n    seq.status = SequenceStatus.RUNNING\n\n    seq.data.update_num_computed_tokens(prompt_len)\n    check_used(num_blocks(prompt_len))\n\n    # this is how we compute it in BlockSpaceManagerV2.__init__\n    sliding_blocks = (sliding_window // block_size) + 2\n    # plus one block for null block\n    sliding_blocks += 1\n\n    # Append tokens to the sequeqnce\n    for token_id in range(num_slots_to_append):\n        seq.append_token_id(token_id, {token_id: Logprob(0.0)})\n        seq.data.update_num_computed_tokens(1)\n        block_manager.append_slots(seq, num_lookahead_slots=0)\n        if prompt_len < sliding_window + 10:\n            check_used(0, sliding_blocks + 1)\n        else:\n            check_used(sliding_blocks, sliding_blocks + 1)\n",
      "diff": "diff --git a/tests/core/block/test_block_manager_v2.py b/tests/core/block/test_block_manager_v2.py\nindex d0ca09c4b..d7863a9ae 100644\n--- a/tests/core/block/test_block_manager_v2.py\n+++ b/tests/core/block/test_block_manager_v2.py\n@@ -249,10 +249,13 @@ def test_append_slots(block_size, prompt_len, num_slots_to_append,\n \n     # Expect consumed blocks to be new blocks required to support the new slots.\n     expected_consumed_blocks = len(\n-        chunk_list(\n-            list(\n-                range(prompt_len + num_slots_to_append + num_lookahead_slots)),\n-            block_size)) - len(chunk_list(list(range(prompt_len)), block_size))\n+        list(\n+            chunk_list(\n+                list(\n+                    range(prompt_len + num_slots_to_append +\n+                          num_lookahead_slots)),\n+                block_size))) - len(\n+                    list(chunk_list(list(range(prompt_len)), block_size)))\n     assert num_consumed_blocks == expected_consumed_blocks",
      "change_type": "modified",
      "lines_added": 8,
      "lines_removed": 5
    },
    {
      "file_path": "tests/core/block/test_cpu_gpu_block_allocator.py",
      "old_content": "import pytest\n\nfrom vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\nfrom vllm.utils import Device, chunk_list\n\n\n@pytest.mark.parametrize(\"num_cpu_blocks\", [0, 512])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [1024])\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"allocator_type\", [\"naive\", \"prefix_caching\"])\ndef test_allocate_mutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n                                block_size: int, allocator_type: str):\n    allocator = CpuGpuBlockAllocator.create(\n        allocator_type=allocator_type,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=num_cpu_blocks,\n        block_size=block_size,\n    )\n\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n\n    cpu_blocks = [\n        allocator.allocate_mutable_block(prev_block=None, device=Device.CPU)\n        for _ in range(num_cpu_blocks)\n    ]\n    assert allocator.get_num_free_blocks(Device.CPU) == 0\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n\n    gpu_blocks = [\n        allocator.allocate_mutable_block(prev_block=None, device=Device.GPU)\n        for _ in range(num_gpu_blocks)\n    ]\n    assert allocator.get_num_free_blocks(Device.CPU) == 0\n    assert allocator.get_num_free_blocks(Device.GPU) == 0\n\n    _ = [allocator.free(block) for block in cpu_blocks]\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == 0\n\n    _ = [allocator.free(block) for block in gpu_blocks]\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n\n\n@pytest.mark.parametrize(\"num_cpu_blocks\", [0, 512])\n@pytest.mark.parametrize(\"num_gpu_blocks\", [1024])\n@pytest.mark.parametrize(\"block_size\", [2])\n@pytest.mark.parametrize(\"allocator_type\", [\"naive\", \"prefix_caching\"])\ndef test_allocate_immutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n                                  block_size: int, allocator_type: str):\n    allocator = CpuGpuBlockAllocator.create(\n        allocator_type=allocator_type,\n        num_gpu_blocks=num_gpu_blocks,\n        num_cpu_blocks=num_cpu_blocks,\n        block_size=block_size,\n    )\n\n    unique_token_ids = list(\n        range((num_cpu_blocks + num_gpu_blocks) * block_size))\n    gpu_token_ids = chunk_list(unique_token_ids[:num_gpu_blocks * block_size],\n                               block_size)\n    cpu_token_ids = chunk_list(unique_token_ids[num_gpu_blocks * block_size:],\n                               block_size)\n\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n\n    cpu_blocks = [\n        allocator.allocate_immutable_block(prev_block=None,\n                                           token_ids=token_ids,\n                                           device=Device.CPU)\n        for token_ids in cpu_token_ids\n    ]\n    assert allocator.get_num_free_blocks(Device.CPU) == 0\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n\n    gpu_blocks = [\n        allocator.allocate_immutable_block(prev_block=None,\n                                           token_ids=token_ids,\n                                           device=Device.GPU)\n        for token_ids in gpu_token_ids\n    ]\n    assert allocator.get_num_free_blocks(Device.CPU) == 0\n    assert allocator.get_num_free_blocks(Device.GPU) == 0\n\n    _ = [allocator.free(block) for block in cpu_blocks]\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == 0\n\n    _ = [allocator.free(block) for block in gpu_blocks]\n    assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n    assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks\n",
      "diff": "diff --git a/tests/core/block/test_cpu_gpu_block_allocator.py b/tests/core/block/test_cpu_gpu_block_allocator.py\nindex 15b76d909..a9e38d404 100644\n--- a/tests/core/block/test_cpu_gpu_block_allocator.py\n+++ b/tests/core/block/test_cpu_gpu_block_allocator.py\n@@ -58,10 +58,10 @@ def test_allocate_immutable_block(num_cpu_blocks: int, num_gpu_blocks: int,\n \n     unique_token_ids = list(\n         range((num_cpu_blocks + num_gpu_blocks) * block_size))\n-    gpu_token_ids = chunk_list(unique_token_ids[:num_gpu_blocks * block_size],\n-                               block_size)\n-    cpu_token_ids = chunk_list(unique_token_ids[num_gpu_blocks * block_size:],\n-                               block_size)\n+    gpu_token_ids = list(\n+        chunk_list(unique_token_ids[:num_gpu_blocks * block_size], block_size))\n+    cpu_token_ids = list(\n+        chunk_list(unique_token_ids[num_gpu_blocks * block_size:], block_size))\n \n     assert allocator.get_num_free_blocks(Device.CPU) == num_cpu_blocks\n     assert allocator.get_num_free_blocks(Device.GPU) == num_gpu_blocks",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/core/block/block_table.py",
      "old_content": "from typing import List, Optional\n\nfrom vllm.core.block.common import BlockList\nfrom vllm.core.block.interfaces import Block, DeviceAwareBlockAllocator\nfrom vllm.utils import Device, cdiv, chunk_list\n\n\nclass BlockTable:\n    \"\"\"A class to manage blocks for a specific sequence.\n\n    The BlockTable maps a sequence of tokens to a list of blocks, where each\n    block represents a contiguous memory allocation for a portion of the \n    sequence. The blocks are managed by a DeviceAwareBlockAllocator, which is\n    responsible for allocating and freeing memory for the blocks.\n\n    Args:\n        block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        block_allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]], optional): An optional list of existing\n            blocks to initialize the BlockTable with. If not provided, an empty\n            BlockTable is created.\n        max_block_sliding_window (Optional[int], optional): The number of\n            blocks to keep around for each sequance. If None, all blocks\n            are kept (eg., when sliding window is not used).\n            It should at least fit the sliding window size of the model.\n\n    Attributes:\n        _block_size (int): The maximum number of tokens that can be stored in a\n            single block.\n        _allocator (DeviceAwareBlockAllocator): The block allocator used to\n            manage memory for the blocks.\n        _blocks (Optional[List[Block]]): The list of blocks managed by this\n            BlockTable.\n        _num_full_slots (int): The number of tokens currently stored in the\n            blocks.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        block_allocator: DeviceAwareBlockAllocator,\n        _blocks: Optional[List[Block]] = None,\n        max_block_sliding_window: Optional[int] = None,\n    ):\n        self._block_size = block_size\n        self._allocator = block_allocator\n        if _blocks is None:\n            _blocks = []\n        self._blocks: BlockList = BlockList(_blocks)\n\n        self._max_block_sliding_window = max_block_sliding_window\n        self._num_full_slots = self._get_num_token_ids()\n\n    @staticmethod\n    def get_num_required_blocks(token_ids: List[int], block_size: int) -> int:\n        \"\"\"Calculates the minimum number of blocks required to store a given\n        sequence of token IDs.\n\n        This assumes worst-case scenario, where every block requires a new\n        allocation (e.g. ignoring prefix caching).\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be stored.\n            block_size (int): The maximum number of tokens that can be stored in\n                a single block.\n\n        Returns:\n            int: The minimum number of blocks required to store the given\n                sequence of token IDs.\n        \"\"\"\n        return cdiv(len(token_ids), block_size)\n\n    def allocate(self,\n                 token_ids: List[int],\n                 device: Device = Device.GPU) -> None:\n        \"\"\"Allocates memory blocks for storing the given sequence of token IDs.\n\n        This method allocates the required number of blocks to store the given\n        sequence of token IDs.\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be stored.\n            device (Device, optional): The device on which the blocks should be\n                allocated. Defaults to Device.GPU.\n        \"\"\"\n        assert not self._is_allocated\n        assert token_ids\n        blocks = self._allocate_blocks_for_token_ids(prev_block=None,\n                                                     token_ids=token_ids,\n                                                     device=device)\n        self.update(blocks)\n        self._num_full_slots = len(token_ids)\n\n    def update(self, blocks: List[Block]) -> None:\n        \"\"\"Resets the table to the newly provided blocks \n        (with their corresponding block ids)\n        \"\"\"\n        self._blocks.update(blocks)\n\n    def append_token_ids(self,\n                         token_ids: List[int],\n                         num_lookahead_slots: int = 0,\n                         num_computed_slots: Optional[int] = None) -> None:\n        \"\"\"Appends a sequence of token IDs to the existing blocks in the\n        BlockTable.\n\n        This method appends the given sequence of token IDs to the existing\n        blocks in the BlockTable. If there is not enough space in the existing\n        blocks, new blocks are allocated using the `ensure_num_empty_slots`\n        method to accommodate the additional tokens.\n\n        The token IDs are divided into chunks of size `block_size` (except for\n        the first chunk, which may be smaller), and each chunk is appended to a\n        separate block.\n\n        Args:\n            token_ids (List[int]): The sequence of token IDs to be appended.\n            num_computed_slots (Optional[int]): The number of KV cache slots\n                that are already filled (computed).\n                When sliding window is enabled, this is used to compute how many\n                blocks to drop at the front of the sequence.\n                Without sliding window, None can be passed.\n                Without chunked prefill, it should be the same as\n                _num_full_slots.\n        \"\"\"\n        assert self._is_allocated, \"no blocks have been allocated\"\n        assert len(self._blocks) > 0\n\n        # Drop blocks that are no longer needed due to sliding window\n        if self._max_block_sliding_window is not None:\n            null_block = self._allocator.allocate_or_get_null_block()\n            assert num_computed_slots is not None\n            end_block_idx = (num_computed_slots //\n                             self._block_size) - self._max_block_sliding_window\n            for idx in range(0, end_block_idx):\n                b = self._blocks[idx]\n                if b is not null_block:\n                    self._allocator.free(b)\n                    self._blocks[idx] = null_block\n\n        # Ensure there are enough empty slots for the new tokens plus\n        # lookahead slots\n        self.ensure_num_empty_slots(num_empty_slots=len(token_ids) +\n                                    num_lookahead_slots)\n\n        # Update the blocks with the new tokens\n        first_block_idx = self._num_full_slots // self._block_size\n        token_blocks = self._chunk_token_blocks_for_append(token_ids)\n\n        for i, token_block in enumerate(token_blocks):\n            self._blocks.append_token_ids(first_block_idx + i, token_block)\n\n        self._num_full_slots += len(token_ids)\n\n    def ensure_num_empty_slots(self, num_empty_slots: int) -> None:\n        \"\"\"Ensures that the BlockTable has at least the specified number of\n        empty slots available.\n\n        This method checks if the BlockTable has enough empty slots (i.e.,\n        available space) to accommodate the requested number of tokens. If not,\n        it allocates additional blocks on the GPU to ensure that the required\n        number of empty slots is available.\n\n        Args:\n            num_empty_slots (int): The minimum number of empty slots required.\n        \"\"\"\n        # Currently the block table only supports\n        # appending tokens to GPU blocks.\n        device = Device.GPU\n        assert self._is_allocated\n\n        if self._num_empty_slots >= num_empty_slots:\n            return\n\n        slots_to_allocate = num_empty_slots - self._num_empty_slots\n        blocks_to_allocate = cdiv(slots_to_allocate, self._block_size)\n\n        for _ in range(blocks_to_allocate):\n            assert len(self._blocks) > 0\n            self._blocks.append(\n                self._allocator.allocate_mutable_block(\n                    prev_block=self._blocks[-1], device=device))\n\n    def fork(self) -> \"BlockTable\":\n        \"\"\"Creates a new BlockTable instance with a copy of the blocks from the\n        current instance.\n\n        This method creates a new BlockTable instance with the same block size,\n        block allocator, and a copy of the blocks from the current instance. The\n        new BlockTable has its own independent set of blocks, but shares the\n        same underlying memory allocation with the original BlockTable.\n\n        Returns:\n            BlockTable: A new BlockTable instance with a copy of the blocks from\n                the current instance.\n        \"\"\"\n        assert self._is_allocated\n        assert len(self._blocks) > 0\n        forked_blocks = self._allocator.fork(self._blocks[-1])\n        return BlockTable(\n            block_size=self._block_size,\n            block_allocator=self._allocator,\n            _blocks=forked_blocks,\n            max_block_sliding_window=self._max_block_sliding_window,\n        )\n\n    def free(self) -> None:\n        \"\"\"Frees the memory occupied by the blocks in the BlockTable.\n\n        This method iterates over all the blocks in the `_blocks` list and calls\n        the `free` method of the `_allocator` object to release the memory\n        occupied by each block. After freeing all the blocks, the `_blocks` list\n        is set to `None`.\n        \"\"\"\n        assert self._is_allocated\n        for block in self.blocks:\n            self._allocator.free(block)\n        self._blocks.reset()\n\n    @property\n    def physical_block_ids(self) -> List[int]:\n        \"\"\"Returns a list of physical block indices for the blocks in the\n        BlockTable.\n\n        This property returns a list of integers, where each integer represents\n        the physical block index of a corresponding block in the `_blocks` list.\n        The physical block index is a unique identifier for the memory location\n        occupied by the block.\n\n        Returns:\n            List[int]: A list of physical block indices for the blocks in the\n                BlockTable.\n        \"\"\"\n        assert self._is_allocated\n        return self._blocks.ids()\n\n    def get_unseen_token_ids(self, sequence_token_ids: List[int]) -> List[int]:\n        \"\"\"Get the number of \"unseen\" tokens in the sequence.\n\n        Unseen tokens are tokens in the sequence corresponding to this block\n        table, but are not yet appended to this block table.\n\n        Args:\n            sequence_token_ids (List[int]): The list of token ids in the\n                sequence.\n\n        Returns:\n            List[int]: The postfix of sequence_token_ids that has not yet been\n                appended to the block table.\n        \"\"\"\n\n        # Since the block table is append-only, the unseen token ids are the\n        # ones after the appended ones.\n        return sequence_token_ids[self.num_full_slots:]\n\n    def _allocate_blocks_for_token_ids(self, prev_block: Optional[Block],\n                                       token_ids: List[int],\n                                       device: Device) -> List[Block]:\n        blocks: List[Block] = []\n\n        block_token_ids = []\n        tail_token_ids = []\n        for cur_token_ids in chunk_list(token_ids, self._block_size):\n            if len(cur_token_ids) == self._block_size:\n                block_token_ids.append(cur_token_ids)\n            else:\n                tail_token_ids.append(cur_token_ids)\n\n        if block_token_ids:\n            blocks.extend(\n                self._allocator.allocate_immutable_blocks(\n                    prev_block, block_token_ids=block_token_ids,\n                    device=device))\n            prev_block = blocks[-1]\n\n        if tail_token_ids:\n            assert len(tail_token_ids) == 1\n            cur_token_ids = tail_token_ids[0]\n\n            block = self._allocator.allocate_mutable_block(\n                prev_block=prev_block, device=device)\n            block.append_token_ids(cur_token_ids)\n\n            blocks.append(block)\n\n        return blocks\n\n    def _get_all_token_ids(self) -> List[int]:\n        # NOTE: This function is O(seq_len); use sparingly.\n        token_ids: List[int] = []\n\n        if not self._is_allocated:\n            return token_ids\n\n        for block in self.blocks:\n            token_ids.extend(block.token_ids)\n\n        return token_ids\n\n    def _get_num_token_ids(self) -> int:\n        res = 0\n        for block in self.blocks:\n            res += len(block.token_ids)\n\n        return res\n\n    @property\n    def _is_allocated(self) -> bool:\n        return len(self._blocks) > 0\n\n    @property\n    def blocks(self) -> List[Block]:\n        return self._blocks.list()\n\n    @property\n    def _num_empty_slots(self) -> int:\n        assert self._is_allocated\n        return len(self._blocks) * self._block_size - self._num_full_slots\n\n    @property\n    def num_full_slots(self) -> int:\n        \"\"\"Returns the total number of tokens currently stored in the\n        BlockTable.\n\n        Returns:\n            int: The total number of tokens currently stored in the BlockTable.\n        \"\"\"\n        return self._num_full_slots\n\n    def get_num_blocks_touched_by_append_slots(\n            self, token_ids: List[int], num_lookahead_slots: int) -> int:\n        \"\"\"Determine how many blocks will be \"touched\" by appending the token\n        ids.\n\n        This is required for the scheduler to determine whether a sequence can\n        continue generation, or if it must be preempted.\n        \"\"\"\n\n        all_token_ids = token_ids + [-1] * num_lookahead_slots\n        token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n        return len(token_blocks)\n\n    def _chunk_token_blocks_for_append(\n            self, token_ids: List[int]) -> List[List[int]]:\n        \"\"\"Split the token ids into block-sized chunks so they can be easily\n        appended to blocks. The first such \"token block\" may have less token ids\n        than the block size, since the last allocated block may be partially\n        full.\n        \"\"\"\n        first_chunk_size = self._block_size - (self._num_full_slots %\n                                               self._block_size)\n        token_blocks = [token_ids[:first_chunk_size]] + chunk_list(\n            token_ids[first_chunk_size:], self._block_size)\n        return token_blocks\n",
      "diff": "diff --git a/vllm/core/block/block_table.py b/vllm/core/block/block_table.py\nindex 49e63c231..06b816eb3 100644\n--- a/vllm/core/block/block_table.py\n+++ b/vllm/core/block/block_table.py\n@@ -1,3 +1,4 @@\n+import math\n from typing import List, Optional\n \n from vllm.core.block.common import BlockList\n@@ -337,10 +338,17 @@ class BlockTable:\n         This is required for the scheduler to determine whether a sequence can\n         continue generation, or if it must be preempted.\n         \"\"\"\n+        # Math below is equivalent to:\n+        # all_token_ids = token_ids + [-1] * num_lookahead_slots\n+        # token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n+        # return len(token_blocks)\n \n-        all_token_ids = token_ids + [-1] * num_lookahead_slots\n-        token_blocks = self._chunk_token_blocks_for_append(all_token_ids)\n-        return len(token_blocks)\n+        num_token_ids = len(token_ids) + num_lookahead_slots\n+        first_chunk_size = self._block_size - (self._num_full_slots %\n+                                               self._block_size)\n+        num_token_blocks = (1 + math.ceil(\n+            (num_token_ids - first_chunk_size) / self._block_size))\n+        return num_token_blocks\n \n     def _chunk_token_blocks_for_append(\n             self, token_ids: List[int]) -> List[List[int]]:\n@@ -351,6 +359,7 @@ class BlockTable:\n         \"\"\"\n         first_chunk_size = self._block_size - (self._num_full_slots %\n                                                self._block_size)\n-        token_blocks = [token_ids[:first_chunk_size]] + chunk_list(\n-            token_ids[first_chunk_size:], self._block_size)\n+        token_blocks = [token_ids[:first_chunk_size]]\n+        token_blocks.extend(\n+            chunk_list(token_ids[first_chunk_size:], self._block_size))\n         return token_blocks",
      "change_type": "modified",
      "lines_added": 15,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/core/block/prefix_caching_block.py",
      "old_content": "\"\"\"Token blocks.\"\"\"\n\nfrom os.path import commonprefix\nfrom typing import Dict, FrozenSet, Iterable, List, Optional, Tuple\n\nfrom vllm.core.block.common import (CopyOnWriteTracker,\n                                    get_all_blocks_recursively)\nfrom vllm.core.block.interfaces import Block, BlockAllocator, BlockId, Device\nfrom vllm.core.block.naive_block import (BlockPool, NaiveBlock,\n                                         NaiveBlockAllocator)\nfrom vllm.core.evictor_v2 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.utils import cdiv\n\nPrefixHash = int\n\n# By default, we init our block access time as _DEFAULT_LAST_ACCESSED_TIME\n# so that if we find one block is still hold _DEFAULT_LAST_ACCESSED_TIME,\n# then we know this block hasn't been accessed yet.\n_DEFAULT_LAST_ACCESSED_TIME = -1\n\n\nclass BlockTracker:\n    \"\"\"Used to track the status of a block inside the prefix caching allocator\n    \"\"\"\n    __slots__ = (\"active\", \"last_accessed\", \"computed\")\n\n    def reset(self):\n        self.last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME\n        self.computed: bool = False\n\n    def __init__(self):\n        self.active: bool = False\n        self.reset()\n\n    def enable(self):\n        assert not self.active\n        self.active = True\n        self.reset()\n\n    def disable(self):\n        assert self.active\n        self.active = False\n        self.reset()\n\n\nclass PrefixCachingBlockAllocator(BlockAllocator):\n    \"\"\"A block allocator that implements prefix caching.\n\n    The PrefixCachingBlockAllocator maintains a cache of blocks based on their\n    content hash. It reuses blocks with the same content hash to avoid redundant\n    memory allocation. The allocator also supports copy-on-write operations.\n\n    Args:\n        num_blocks (int): The total number of blocks to manage.\n        block_size (int): The size of each block in tokens.\n        block_ids(Optional[Iterable[int]], optional): An optional iterable of\n            block IDs. If not provided, block IDs will be assigned sequentially\n            from 0 to num_blocks - 1.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_blocks: int,\n        block_size: int,\n        block_ids: Optional[Iterable[int]] = None,\n        eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n    ):\n        if block_ids is None:\n            block_ids = range(num_blocks)\n\n        self._block_size = block_size\n\n        # A mapping of prefix hash to block index. All blocks which have a\n        # prefix hash will be in this dict, even if they have refcount 0.\n        self._cached_blocks: Dict[PrefixHash, BlockId] = {}\n\n        # Used to track status of each physical block id\n        self._block_tracker: Dict[BlockId, BlockTracker] = {}\n        for block_id in block_ids:\n            self._block_tracker[block_id] = BlockTracker()\n\n        # Pre-allocate \"num_blocks * extra_factor\" block objects.\n        # The \"* extra_factor\" is a buffer to allow more block objects\n        # than physical blocks\n        extra_factor = 4\n        self._block_pool = BlockPool(self._block_size, self._create_block,\n                                     self, num_blocks * extra_factor)\n\n        # An allocator for blocks that do not have prefix hashes.\n        self._hashless_allocator = NaiveBlockAllocator(\n            create_block=self._create_block,  # type: ignore\n            num_blocks=num_blocks,\n            block_size=block_size,\n            block_ids=block_ids,\n            block_pool=self._block_pool,  # Share block pool here\n        )\n\n        # Evitor used to maintain how we want to handle those computed blocks\n        # if we find memory pressure is high.\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        # We share the refcounter between allocators. This allows us to promote\n        # blocks originally allocated in the hashless allocator to immutable\n        # blocks.\n        self._refcounter = self._hashless_allocator.refcounter\n\n        self._cow_tracker = CopyOnWriteTracker(\n            refcounter=self._refcounter.as_readonly())\n\n    # Implements Block.Factory.\n    def _create_block(\n        self,\n        prev_block: Optional[Block],\n        token_ids: List[int],\n        block_size: int,\n        allocator: BlockAllocator,\n        block_id: Optional[int] = None,\n        computed: bool = False,\n    ) -> Block:\n        # Bind block to self.\n        allocator = self\n\n        return PrefixCachingBlock(\n            prev_block=prev_block,\n            token_ids=token_ids,\n            block_size=block_size,\n            block_id=block_id,\n            allocator=allocator,\n            computed=computed,\n        )\n\n    def allocate_immutable_block(self,\n                                 prev_block: Optional[Block],\n                                 token_ids: List[int],\n                                 device: Optional[Device] = None) -> Block:\n        \"\"\"Allocates an immutable block with the given token IDs, reusing cached\n        blocks if possible.\n\n        Args:\n            prev_block (Optional[Block]): The previous block in the sequence.\n            token_ids (List[int]): The token IDs to be stored in the block.\n\n        Returns:\n            Block: The allocated immutable block.\n        \"\"\"\n        assert device is None\n        assert_prefix_caching_block_or_none(prev_block)\n\n        # First, try to create a block that points to cached data\n        block = self._block_pool.init_block(prev_block=prev_block,\n                                            token_ids=token_ids,\n                                            block_size=self._block_size,\n                                            physical_block_id=None)\n        assert block.content_hash is not None\n\n        cached_block_id = self._cached_blocks.get(block.content_hash, None)\n        if cached_block_id is not None:\n            block.block_id = cached_block_id\n            self._incr_refcount_cached_block(block)\n            return block\n        self._block_pool.free_block(block)\n\n        # No cached block => Allocate a new block\n        block = self.allocate_mutable_block(prev_block)\n        block.append_token_ids(token_ids)\n        return block\n\n    def allocate_immutable_blocks(\n            self,\n            prev_block: Optional[Block],\n            block_token_ids: List[List[int]],\n            device: Optional[Device] = None) -> List[Block]:\n        blocks = []\n        for token_ids in block_token_ids:\n            prev_block = self.allocate_immutable_block(prev_block=prev_block,\n                                                       token_ids=token_ids,\n                                                       device=device)\n            blocks.append(prev_block)\n        return blocks\n\n    def allocate_mutable_block(self,\n                               prev_block: Optional[Block],\n                               device: Optional[Device] = None) -> Block:\n        \"\"\"Allocates a mutable block. If there are no free blocks, this will\n        evict unused cached blocks.\n\n        Args:\n            prev_block (Block): The previous block in the sequence.\n                None is not allowed unlike it is super class.\n\n        Returns:\n            Block: The allocated mutable block.\n        \"\"\"\n        assert device is None\n        assert_prefix_caching_block_or_none(prev_block)\n\n        block_id = self._allocate_block_id()\n        block = self._block_pool.init_block(prev_block=prev_block,\n                                            token_ids=[],\n                                            block_size=self._block_size,\n                                            physical_block_id=block_id)\n        assert not block.computed\n        assert block.content_hash is None\n        return block\n\n    def _incr_refcount_cached_block(self, block: Block) -> None:\n        # Set this block to be \"computed\" since it is pointing to a\n        # cached block id (which was already computed)\n        block.computed = True\n\n        block_id = block.block_id\n        assert block_id is not None\n\n        refcount = self._refcounter.incr(block_id)\n        if refcount == 1:\n            # In case a cached block was evicted, restore its tracking\n            if block_id in self.evictor:\n                self.evictor.remove(block_id)\n\n            self._track_block_id(block_id, computed=True)\n\n    def _decr_refcount_cached_block(self, block: Block) -> None:\n        # Ensure this is immutable/cached block\n        assert block.content_hash is not None\n\n        block_id = block.block_id\n        assert block_id is not None\n\n        refcount = self._refcounter.decr(block_id)\n        if refcount > 0:\n            block.block_id = None\n            return\n        else:\n            assert refcount == 0\n\n        # No longer used\n        assert block.content_hash in self._cached_blocks\n\n        # Add the cached block to the evictor\n        # (This keeps the cached block around so it can be reused)\n        self.evictor.add(block_id, block.content_hash, block.num_tokens_total,\n                         self._block_tracker[block_id].last_accessed)\n\n        # Stop tracking the block\n        self._untrack_block_id(block_id)\n\n        block.block_id = None\n\n    def _decr_refcount_hashless_block(self, block: Block) -> None:\n        block_id = block.block_id\n        assert block_id is not None\n\n        # We may have a fork case where block is shared,\n        # in which case, we cannot remove it from tracking\n        refcount = self._refcounter.get(block_id)\n        if refcount == 1:\n            self._untrack_block_id(block_id)\n\n        # Decrement refcount of the block_id, but do not free the block object\n        # itself (will be handled by the caller)\n        self._hashless_allocator.free(block, keep_block_object=True)\n\n    def _allocate_block_id(self) -> BlockId:\n        \"\"\"First tries to allocate a block id from the hashless allocator,\n        and if there are no blocks, then tries to evict an unused cached block.\n        \"\"\"\n        hashless_block_id = self._maybe_allocate_hashless_block_id()\n        if hashless_block_id is not None:\n            return hashless_block_id\n\n        evicted_block_id = self._maybe_allocate_evicted_block_id()\n        if evicted_block_id is not None:\n            return evicted_block_id\n\n        # No block available in hashless allocator, nor in unused cache blocks.\n        raise BlockAllocator.NoFreeBlocksError()\n\n    def _maybe_allocate_hashless_block_id(self) -> Optional[BlockId]:\n        try:\n            # Allocate mutable block and extract its block_id\n            block = self._hashless_allocator.allocate_mutable_block(\n                prev_block=None)\n            block_id = block.block_id\n            self._block_pool.free_block(block)\n\n            self._track_block_id(block_id, computed=False)\n            return block_id\n        except BlockAllocator.NoFreeBlocksError:\n            return None\n\n    def _maybe_allocate_evicted_block_id(self) -> Optional[BlockId]:\n        if self.evictor.num_blocks == 0:\n            return None\n\n        # Here we get an evicted block, which is only added\n        # into evictor if its ref counter is 0\n        # and since its content would be changed, we need\n        # to remove it from _cached_blocks's tracking list\n        block_id, content_hash_to_evict = self.evictor.evict()\n\n        # Sanity checks\n        assert content_hash_to_evict in self._cached_blocks\n        _block_id = self._cached_blocks[content_hash_to_evict]\n        assert self._refcounter.get(_block_id) == 0\n        assert _block_id == block_id\n\n        self._cached_blocks.pop(content_hash_to_evict)\n\n        self._refcounter.incr(block_id)\n        self._track_block_id(block_id, computed=False)\n\n        return block_id\n\n    def _free_block_id(self, block: Block) -> None:\n        \"\"\"Decrements the refcount of the block. The block may be in two \n        possible states: (1) immutable/cached or (2) mutable/hashless. \n        In the first case, the refcount is decremented directly and the block\n        may be possibly added to the evictor. In other case, hashless \n        allocator free(..) with keep_block_object=True is called to only free\n        the block id (since the block object may be reused by the caller)\n        \"\"\"\n        block_id = block.block_id\n        assert block_id is not None, \"Freeing unallocated block is undefined\"\n\n        if block.content_hash is not None:\n            # Immutable: This type of block is always cached, and we want to\n            # keep it in the evictor for future reuse\n            self._decr_refcount_cached_block(block)\n        else:\n            # Mutable: This type of block is not cached, so we release it\n            # directly to the hashless allocator\n            self._decr_refcount_hashless_block(block)\n\n        assert block.block_id is None\n\n    def free(self, block: Block, keep_block_object: bool = False) -> None:\n        \"\"\"Release the block (look at free_block_id(..) docs)\n        \"\"\"\n        # Release the physical block index\n        self._free_block_id(block)\n\n        # Release the block object to the pool\n        if not keep_block_object:\n            self._block_pool.free_block(block)\n\n    def fork(self, last_block: Block) -> List[Block]:\n        \"\"\"Creates a new sequence of blocks that shares the same underlying\n        memory as the original sequence.\n\n        Args:\n            last_block (Block): The last block in the original sequence.\n\n        Returns:\n            List[Block]: The new sequence of blocks that shares the same memory\n                as the original sequence.\n        \"\"\"\n        source_blocks = get_all_blocks_recursively(last_block)\n\n        forked_blocks: List[Block] = []\n        prev_block = None\n        for block in source_blocks:\n            block_id = block.block_id\n            assert block_id is not None\n\n            refcount = self._refcounter.incr(block_id)\n            assert refcount != 1, \"can't fork free'd block_id = {}\".format(\n                block_id)\n\n            forked_block = self._block_pool.init_block(\n                prev_block=prev_block,\n                token_ids=block.token_ids,\n                block_size=self._block_size,\n                physical_block_id=block_id)\n\n            forked_blocks.append(forked_block)\n            prev_block = forked_blocks[-1]\n\n        return forked_blocks\n\n    def get_num_free_blocks(self, device: Optional[Device] = None) -> int:\n        assert device is None\n        # The number of free blocks is the number of hashless free blocks\n        # plus the number of blocks evictor could free from its list.\n        return self._hashless_allocator.get_num_free_blocks(\n        ) + self.evictor.num_blocks\n\n    def get_num_total_blocks(self) -> int:\n        return self._hashless_allocator.get_num_total_blocks()\n\n    def get_physical_block_id(self, absolute_id: int) -> int:\n        \"\"\"Returns the zero-offset block id on certain block allocator\n        given the absolute block id.\n\n        Args:\n            absolute_id (int): The absolute block id for the block \n                in whole allocator.\n\n        Returns:\n            int: The rzero-offset block id on certain device.\n        \"\"\"\n        return sorted(self.all_block_ids).index(absolute_id)\n\n    @property\n    def all_block_ids(self) -> FrozenSet[int]:\n        return self._hashless_allocator.all_block_ids\n\n    def is_block_cached(self, block: Block) -> bool:\n        assert block.content_hash is not None\n        if block.content_hash in self._cached_blocks:\n            return True\n        return False\n\n    def promote_to_immutable_block(self, block: Block) -> BlockId:\n        \"\"\"Once a mutable block is full, it can be promoted to an immutable\n        block. This means that its content can be referenced by future blocks\n        having the same prefix.\n\n        Note that if we already have a cached block with the same content, we\n        will replace the newly-promoted block's mapping with the existing cached\n        block id.\n\n        Args:\n            block: The mutable block to be promoted.\n\n        Returns:\n            BlockId: Either the original block index, or the block index of\n                the previously cached block matching the same content.\n        \"\"\"\n        # Ensure block can be promoted\n        assert block.content_hash is not None\n        assert block.block_id is not None\n        assert self._refcounter.get(block.block_id) > 0\n\n        if block.content_hash not in self._cached_blocks:\n            # No cached content hash => Set this block as cached\n            # (Note that this block is not computed yet =>\n            #  Will be computed after free())\n            self._cached_blocks[block.content_hash] = block.block_id\n            return block.block_id\n\n        # Reuse the cached content hash\n        self._decr_refcount_hashless_block(block)\n        block.block_id = self._cached_blocks[block.content_hash]\n\n        # Increment refcount of the cached block and (possibly) restore\n        # it from the evictor.\n        # Note that in this case, the block is marked as computed\n        self._incr_refcount_cached_block(block)\n\n        return block.block_id\n\n    def cow_block_if_not_appendable(self, block: Block) -> BlockId:\n        \"\"\"Performs a copy-on-write operation on the given block if it is not\n        appendable.\n\n        Args:\n            block (Block): The block to check for copy-on-write.\n\n        Returns:\n            BlockId: The block index of the new block if a copy-on-write \n                operation was performed, or the original block index if\n                no copy-on-write was necessary.\n        \"\"\"\n        src_block_id = block.block_id\n        assert src_block_id is not None\n\n        if self._cow_tracker.is_appendable(block):\n            return src_block_id\n\n        self._free_block_id(block)\n        trg_block_id = self._allocate_block_id()\n\n        self._cow_tracker.record_cow(src_block_id, trg_block_id)\n\n        return trg_block_id\n\n    def clear_copy_on_writes(self) -> List[Tuple[BlockId, BlockId]]:\n        \"\"\"Returns the copy-on-write source->destination mapping and clears it.\n\n        Returns:\n            List[Tuple[BlockId, BlockId]]: A list mapping source\n                block indices to destination block indices.\n        \"\"\"\n        return self._cow_tracker.clear_cows()\n\n    def mark_blocks_as_accessed(self, block_ids: List[int],\n                                now: float) -> None:\n        \"\"\"Mark blocks as accessed, used in prefix caching.\n\n        If the block is added into evictor, we need to update corresponding\n        info in evictor's metadata.\n        \"\"\"\n\n        for block_id in block_ids:\n            if self._block_tracker[block_id].active:\n                self._block_tracker[block_id].last_accessed = now\n            elif block_id in self.evictor:\n                self.evictor.update(block_id, now)\n            else:\n                raise ValueError(\n                    \"Mark block as accessed which is not belonged to GPU\")\n\n    def mark_blocks_as_computed(self, block_ids: List[int]) -> None:\n        raise NotImplementedError(\"Marking as computed is incremental\")\n\n    def _track_block_id(self, block_id: Optional[BlockId],\n                        computed: bool) -> None:\n        assert block_id is not None\n        self._block_tracker[block_id].enable()\n        self._block_tracker[block_id].computed = computed\n\n    def _untrack_block_id(self, block_id: Optional[BlockId]) -> None:\n        assert block_id is not None\n        self._block_tracker[block_id].disable()\n\n    def block_is_computed(self, block_id: int) -> bool:\n        if self._block_tracker[block_id].active:\n            return self._block_tracker[block_id].computed\n        else:\n            return block_id in self.evictor\n\n    def get_computed_block_ids(self,\n                               prev_computed_block_ids: List[int],\n                               block_ids: List[int],\n                               skip_last_block_id: bool = True) -> List[int]:\n        prev_prefix_size = len(prev_computed_block_ids)\n        cur_size = len(block_ids)\n        if skip_last_block_id:\n            cur_size -= 1\n\n        # Sanity checks\n        assert cur_size >= 0\n        assert prev_prefix_size <= cur_size\n\n        ret = prev_computed_block_ids\n        for i in range(prev_prefix_size, cur_size):\n            block_id = block_ids[i]\n            if self.block_is_computed(block_id):\n                ret.append(block_id)\n        return ret\n\n    def get_common_computed_block_ids(\n            self, computed_seq_block_ids: List[List[int]]) -> List[int]:\n        \"\"\"Return the block ids that are common for a given sequence group.\n\n        Only those blocks that are immutable and already be marked\n        compyted would be taken consideration.\n        \"\"\"\n\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n\n        # It returns a list of int although type annotation says list of string.\n        return commonprefix([\n            ids for ids in computed_seq_block_ids  # type: ignore\n            if ids != []\n        ])\n\n    def get_num_blocks_touched(self,\n                               blocks: List[Block],\n                               num_lookahead_slots: int = 0) -> int:\n        \"\"\"Determine the number of blocks that will be touched by\n        swapping in/out the given blocks from certain sequence\n        group with the provided num_lookahead_slots.\n\n        Args:\n            blocks (List[Block]): The potential blocks to swap.\n            num_lookahead_slots (int): number of lookahead slots (0 for \n                swap out).\n        \n        Returns:\n            int: the number of blocks that will be touched by\n                swapping in/out the given blocks and num_lookahead_slots.\n        \"\"\"\n        num_touched_blocks = 0\n        for block in blocks:\n            if not block.is_full:\n                if block.num_empty_slots >= num_lookahead_slots:\n                    num_touched_blocks += 1\n                else:\n                    num_touched_blocks += cdiv(\n                        num_lookahead_slots - block.num_empty_slots,\n                        self._block_size)\n            else:\n                if not self.is_block_cached(block):\n                    num_touched_blocks += 1\n        return num_touched_blocks\n\n    def swap_out(self, blocks: List[Block]) -> None:\n        \"\"\"Execute the swap out actions. Basically just free the \n        given blocks.\n\n        Args:\n            blocks: List of blocks to be swapped out.\n        \"\"\"\n        for block in blocks:\n            self._free_block_id(block)\n\n    def swap_in(self, blocks: List[Block]) -> None:\n        \"\"\"Execute the swap in actions. Change the block id from \n        old allocator to current allocator for each block to finish \n        the block table update. \n\n        Args:\n            blocks: List of blocks to be swapped in.\n        \"\"\"\n        for block in blocks:\n            # Here we allocate either immutable or mutable block and then\n            # extract its block_id. Note that the block object is released\n            # and the block_id is assigned to \"block\" to allow reusing the\n            # existing \"block\" object\n            if block.is_full:\n                tmp_block = self.allocate_immutable_block(\n                    prev_block=block.prev_block, token_ids=block.token_ids)\n            else:\n                tmp_block = self.allocate_mutable_block(\n                    prev_block=block.prev_block)\n                tmp_block.append_token_ids(block.token_ids)\n\n            block_id = tmp_block.block_id\n            self._block_pool.free_block(tmp_block)\n\n            block.block_id = block_id  # Assign block_id\n\n\nclass PrefixCachingBlock(Block):\n    \"\"\"A block implementation that supports prefix caching.\n\n    The PrefixCachingBlock class represents a block of token IDs with prefix\n    caching capabilities. It wraps a NaiveBlock internally and provides\n    additional functionality for content hashing and promoting immutable blocks\n    with the prefix caching allocator.\n\n    Args:\n        prev_block (Optional[PrefixCachingBlock]): The previous block in the\n            sequence.\n        token_ids (List[int]): The initial token IDs to be stored in the block.\n        block_size (int): The maximum number of token IDs that can be stored in\n            the block.\n        allocator (BlockAllocator): The prefix\n            caching block allocator associated with this block.\n        block_id (Optional[int], optional): The physical block index\n            of this block. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        prev_block: Optional[Block],\n        token_ids: List[int],\n        block_size: int,\n        allocator: BlockAllocator,\n        block_id: Optional[int] = None,\n        computed: bool = False,\n    ):\n        assert isinstance(allocator, PrefixCachingBlockAllocator), (\n            \"Currently this class is only tested with \"\n            \"PrefixCachingBlockAllocator. Got instead allocator = {}\".format(\n                allocator))\n        assert_prefix_caching_block_or_none(prev_block)\n\n        self._prev_block = prev_block\n        self._cached_content_hash: Optional[int] = None\n        self._cached_num_tokens_total: int = 0\n        self._allocator = allocator\n        self._last_accessed: float = _DEFAULT_LAST_ACCESSED_TIME\n        self._computed = computed\n\n        # On the first time, we create the block object, and next we only\n        # reinitialize it\n        if hasattr(self, \"_block\"):\n            self._block.__init__(  # type: ignore[has-type]\n                prev_block=prev_block,\n                token_ids=token_ids,\n                block_size=block_size,\n                block_id=block_id,\n                allocator=self._allocator)\n        else:\n            self._block = NaiveBlock(prev_block=prev_block,\n                                     token_ids=token_ids,\n                                     block_size=block_size,\n                                     block_id=block_id,\n                                     allocator=self._allocator)\n\n        self._update_num_tokens_total()\n\n    def _update_num_tokens_total(self):\n        \"\"\"Incrementally computes the number of tokens that there is\n        till the current block (included)\n        \"\"\"\n        res = 0\n\n        # Add all previous blocks\n        if self._prev_block is not None:\n            res += self._prev_block.num_tokens_total\n\n        # Add current block\n        res += len(self.token_ids)\n\n        self._cached_num_tokens_total = res\n\n    @property\n    def computed(self) -> bool:\n        return self._computed\n\n    @computed.setter\n    def computed(self, value) -> None:\n        self._computed = value\n\n    @property\n    def last_accessed(self) -> float:\n        return self._last_accessed\n\n    @last_accessed.setter\n    def last_accessed(self, last_accessed_ts: float):\n        self._last_accessed = last_accessed_ts\n\n    def append_token_ids(self, token_ids: List[int]) -> None:\n        \"\"\"Appends the given token IDs to the block and registers the block as\n        immutable if the block becomes full.\n\n        Args:\n            token_ids (List[int]): The token IDs to be appended to the block.\n        \"\"\"\n        # Ensure this is mutable block (not promoted)\n        assert self.content_hash is None\n        assert not self.computed\n\n        if len(token_ids) == 0:\n            return\n\n        # Ensure there are input tokens\n        assert token_ids, \"Got token_ids = {}\".format(token_ids)\n\n        # Naive block handles CoW.\n        self._block.append_token_ids(token_ids)\n        self._update_num_tokens_total()\n\n        # If the content hash is present, then the block can be made immutable.\n        # Register ourselves with the allocator, potentially replacing the\n        # physical block index.\n        if self.content_hash is not None:\n            self.block_id = self._allocator.promote_to_immutable_block(self)\n\n    @property\n    def block_id(self) -> Optional[int]:\n        return self._block.block_id\n\n    @block_id.setter\n    def block_id(self, value) -> None:\n        self._block.block_id = value\n\n    @property\n    def is_full(self) -> bool:\n        return self._block.is_full\n\n    @property\n    def num_empty_slots(self) -> int:\n        return self._block.num_empty_slots\n\n    @property\n    def num_tokens_total(self) -> int:\n        return self._cached_num_tokens_total\n\n    @property\n    def block_size(self) -> int:\n        return self._block.block_size\n\n    @property\n    def token_ids(self) -> List[int]:\n        return self._block.token_ids\n\n    @property\n    def prev_block(self) -> Optional[Block]:\n        return self._prev_block\n\n    @property\n    def content_hash(self) -> Optional[int]:\n        \"\"\"Return the content-based hash of the current block, or None if it is\n        not yet defined.\n\n        For the content-based hash to be defined, the current block must be\n        full.\n        \"\"\"\n        # If the hash is already computed, return it.\n        if self._cached_content_hash is not None:\n            return self._cached_content_hash\n\n        # We cannot compute a hash for the current block because it is not full.\n        if not self.is_full:\n            return None\n\n        is_first_block = self._prev_block is None\n        prev_block_hash = (\n            None if is_first_block else\n            self._prev_block.content_hash  # type: ignore\n        )\n\n        # Previous block exists but does not yet have a hash.\n        # Return no hash in this case.\n        if prev_block_hash is None and not is_first_block:\n            return None\n\n        self._cached_content_hash = PrefixCachingBlock.hash_block_tokens(\n            is_first_block,\n            prev_block_hash,\n            cur_block_token_ids=self.token_ids)\n        return self._cached_content_hash\n\n    @staticmethod\n    def hash_block_tokens(is_first_block: bool, prev_block_hash: Optional[int],\n                          cur_block_token_ids: List[int]) -> int:\n        \"\"\"Computes a hash value corresponding to the contents of a block and\n        the contents of the preceding block(s). The hash value is used for\n        prefix caching.\n\n        NOTE: Content-based hashing does not yet support LoRA.\n\n        Parameters:\n        - is_first_block (bool): A flag indicating if the block is the first in\n            the sequence.\n        - prev_block_hash (Optional[int]): The hash of the previous block. None\n            if this is the first block.\n        - cur_block_token_ids (List[int]): A list of token ids in the current\n            block. The current block is assumed to be full.\n\n        Returns:\n        - int: The computed hash value for the block.\n        \"\"\"\n        assert (prev_block_hash is None) == is_first_block\n        return hash((is_first_block, prev_block_hash, *cur_block_token_ids))\n\n\nclass ComputedBlocksTracker:\n    \"\"\"Handles caching of per-sequence computed block ids. \n        When a sequence appears for the first time, it traverses all of the \n        blocks and detects the prefix of blocks that is computed. On the\n        subsequent times, it only traverses the new blocks that were added \n        and updates the already recorded prefix of blocks with the newly \n        computed blocks.\n\n        To avoid redundant traversals, the algorithm also detects when there\n        is a \"gap\" in the computed prefix. For example, if we have blocks =\n        [1,2,3,4,5], and we have detected [1,2,3] as the computed prefix, then\n        we won't try to add more computed blocks to [1,2,3] in this sequence\n        iteration, and will add more computed blocks only after the sequence is\n        freed and reused again.\n\n        Note that currently, for a given sequence, we also skip the last \n        block id for caching purposes, to avoid caching of a full sequence\n    \"\"\"\n\n    def __init__(self, allocator):\n        self._allocator = allocator\n        self._cached_computed_seq_blocks: Dict[int, Tuple[List[int],\n                                                          bool]] = {}\n\n    def add_seq(self, seq_id: int) -> None:\n        \"\"\"Start tracking seq_id\n        \"\"\"\n        assert seq_id not in self._cached_computed_seq_blocks\n        self._cached_computed_seq_blocks[seq_id] = ([], False)\n\n    def remove_seq(self, seq_id: int) -> None:\n        \"\"\"Stop tracking seq_id\n        \"\"\"\n        assert seq_id in self._cached_computed_seq_blocks\n        del self._cached_computed_seq_blocks[seq_id]\n\n    def get_cached_computed_blocks_and_update(\n            self, seq_id: int, block_ids: List[int]) -> List[int]:\n        \"\"\" Look at the class documentation for details\n        \"\"\"\n        # Ensure seq_id is already tracked\n        assert seq_id in self._cached_computed_seq_blocks\n\n        # Get cached data (may be empty on the first time)\n        prev_computed_block_ids, has_gap = self._cached_computed_seq_blocks[\n            seq_id]\n\n        if has_gap:\n            # When gap is detected, we do not add more computed blocks at this\n            # sequence iteration\n            return prev_computed_block_ids\n\n        # We do not consider the last block id for caching purposes.\n        num_cur_blocks = len(block_ids) - 1\n        assert num_cur_blocks >= 0\n\n        if len(prev_computed_block_ids) >= num_cur_blocks:\n            # Cache HIT\n            assert len(prev_computed_block_ids) == num_cur_blocks\n            return prev_computed_block_ids\n\n        # If here, then we may possibly add more computed blocks. As a result,\n        # traverse the additional blocks after prev_computed_block_ids to\n        # detect more computed blocks and add them.\n\n        # Incremental init for seq_id => Look only at the new blocks\n        computed_block_ids = self._allocator.get_computed_block_ids(  # noqa: E501\n            prev_computed_block_ids,\n            block_ids,\n            skip_last_block_id=\n            True,  # We skip last block id to avoid caching of full seq\n        )\n\n        # Detect if there is a \"gap\"\n        has_gap = len(computed_block_ids) < num_cur_blocks\n\n        # Record\n        self._cached_computed_seq_blocks[seq_id] = (computed_block_ids,\n                                                    has_gap)\n\n        return computed_block_ids\n\n\nclass LastAccessBlocksTracker:\n    \"\"\"Manages the last access time of the tracked sequences, in order to allow\n    an efficient update of allocator's block last access times\n    \"\"\"\n\n    def __init__(self, allocator):\n        self._allocator = allocator\n        self._seq_last_access: Dict[int, Optional[float]] = {}\n\n    def add_seq(self, seq_id: int) -> None:\n        \"\"\"Start tracking seq_id\n        \"\"\"\n        assert seq_id not in self._seq_last_access\n        self._seq_last_access[seq_id] = None\n\n    def remove_seq(self, seq_id: int) -> None:\n        \"\"\"Stop tracking seq_id\n        \"\"\"\n        assert seq_id in self._seq_last_access\n        del self._seq_last_access[seq_id]\n\n    def update_last_access(self, seq_id: int, time: float) -> None:\n        assert seq_id in self._seq_last_access\n        self._seq_last_access[seq_id] = time\n\n    def update_seq_blocks_last_access(self, seq_id: int,\n                                      block_ids: List[int]) -> None:\n        assert seq_id in self._seq_last_access\n\n        ts = self._seq_last_access[seq_id]\n\n        if ts is None:\n            # No last access was recorded, no need to update.\n            return\n\n        self._allocator.mark_blocks_as_accessed(block_ids, ts)\n\n\ndef assert_prefix_caching_block_or_none(block: Optional[Block]):\n    if block is None:\n        return\n    assert isinstance(block,\n                      PrefixCachingBlock), \"Got block = {}\".format(block)\n",
      "diff": "diff --git a/vllm/core/block/prefix_caching_block.py b/vllm/core/block/prefix_caching_block.py\nindex f272e23ee..d102ad404 100644\n--- a/vllm/core/block/prefix_caching_block.py\n+++ b/vllm/core/block/prefix_caching_block.py\n@@ -552,9 +552,12 @@ class PrefixCachingBlockAllocator(BlockAllocator):\n         # runner.\n \n         # It returns a list of int although type annotation says list of string.\n+        if len(computed_seq_block_ids) == 1:\n+            return computed_seq_block_ids[0]\n+\n         return commonprefix([\n             ids for ids in computed_seq_block_ids  # type: ignore\n-            if ids != []\n+            if ids\n         ])\n \n     def get_num_blocks_touched(self,",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/__init__.py",
      "old_content": "import importlib\nfrom typing import Dict, List, Optional, Type\n\nimport torch.nn as nn\n\nfrom vllm.logger import init_logger\nfrom vllm.utils import is_hip\n\nlogger = init_logger(__name__)\n\n# Architecture -> (module, class).\n_GENERATION_MODELS = {\n    \"AquilaModel\": (\"llama\", \"LlamaForCausalLM\"),\n    \"AquilaForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),  # AquilaChat2\n    \"BaiChuanForCausalLM\": (\"baichuan\", \"BaiChuanForCausalLM\"),  # baichuan-7b\n    \"BaichuanForCausalLM\": (\"baichuan\", \"BaichuanForCausalLM\"),  # baichuan-13b\n    \"BloomForCausalLM\": (\"bloom\", \"BloomForCausalLM\"),\n    \"ChatGLMModel\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"ChatGLMForConditionalGeneration\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"CohereForCausalLM\": (\"commandr\", \"CohereForCausalLM\"),\n    \"DbrxForCausalLM\": (\"dbrx\", \"DbrxForCausalLM\"),\n    \"DeciLMForCausalLM\": (\"decilm\", \"DeciLMForCausalLM\"),\n    \"DeepseekForCausalLM\": (\"deepseek\", \"DeepseekForCausalLM\"),\n    \"DeepseekV2ForCausalLM\": (\"deepseek_v2\", \"DeepseekV2ForCausalLM\"),\n    \"FalconForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"FuyuForCausalLM\": (\"fuyu\", \"FuyuForCausalLM\"),\n    \"GemmaForCausalLM\": (\"gemma\", \"GemmaForCausalLM\"),\n    \"Gemma2ForCausalLM\": (\"gemma2\", \"Gemma2ForCausalLM\"),\n    \"GPT2LMHeadModel\": (\"gpt2\", \"GPT2LMHeadModel\"),\n    \"GPTBigCodeForCausalLM\": (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n    \"GPTJForCausalLM\": (\"gpt_j\", \"GPTJForCausalLM\"),\n    \"GPTNeoXForCausalLM\": (\"gpt_neox\", \"GPTNeoXForCausalLM\"),\n    \"InternLMForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"InternLM2ForCausalLM\": (\"internlm2\", \"InternLM2ForCausalLM\"),\n    \"JAISLMHeadModel\": (\"jais\", \"JAISLMHeadModel\"),\n    \"LlamaForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"LlavaForConditionalGeneration\":\n    (\"llava\", \"LlavaForConditionalGeneration\"),\n    \"LlavaNextForConditionalGeneration\":\n    (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n    # For decapoda-research/llama-*\n    \"LLaMAForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"MistralForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"MixtralForCausalLM\": (\"mixtral\", \"MixtralForCausalLM\"),\n    \"QuantMixtralForCausalLM\": (\"mixtral_quant\", \"MixtralForCausalLM\"),\n    # transformers's mpt class has lower case\n    \"MptForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"MPTForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"MiniCPMForCausalLM\": (\"minicpm\", \"MiniCPMForCausalLM\"),\n    \"OlmoForCausalLM\": (\"olmo\", \"OlmoForCausalLM\"),\n    \"OPTForCausalLM\": (\"opt\", \"OPTForCausalLM\"),\n    \"OrionForCausalLM\": (\"orion\", \"OrionForCausalLM\"),\n    \"PersimmonForCausalLM\": (\"persimmon\", \"PersimmonForCausalLM\"),\n    \"PaliGemmaForConditionalGeneration\":\n    (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n    \"PhiForCausalLM\": (\"phi\", \"PhiForCausalLM\"),\n    \"Phi3ForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"Phi3VForCausalLM\": (\"phi3v\", \"Phi3VForCausalLM\"),\n    \"QWenLMHeadModel\": (\"qwen\", \"QWenLMHeadModel\"),\n    \"Qwen2ForCausalLM\": (\"qwen2\", \"Qwen2ForCausalLM\"),\n    \"Qwen2MoeForCausalLM\": (\"qwen2_moe\", \"Qwen2MoeForCausalLM\"),\n    \"RWForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"StableLMEpochForCausalLM\": (\"stablelm\", \"StablelmForCausalLM\"),\n    \"StableLmForCausalLM\": (\"stablelm\", \"StablelmForCausalLM\"),\n    \"Starcoder2ForCausalLM\": (\"starcoder2\", \"Starcoder2ForCausalLM\"),\n    \"ArcticForCausalLM\": (\"arctic\", \"ArcticForCausalLM\"),\n    \"XverseForCausalLM\": (\"xverse\", \"XverseForCausalLM\"),\n    \"Phi3SmallForCausalLM\": (\"phi3_small\", \"Phi3SmallForCausalLM\"),\n    \"MedusaModel\": (\"medusa\", \"Medusa\"),\n    \"MLPSpeculatorPreTrainedModel\": (\"mlp_speculator\", \"MLPSpeculator\"),\n    \"JambaForCausalLM\": (\"jamba\", \"JambaForCausalLM\")\n}\n\n_EMBEDDING_MODELS = {\n    \"MistralModel\": (\"llama_embedding\", \"LlamaEmbeddingModel\"),\n}\n\n_MODELS = {**_GENERATION_MODELS, **_EMBEDDING_MODELS}\n\n# Architecture -> type.\n# out of tree models\n_OOT_MODELS: Dict[str, Type[nn.Module]] = {}\n\n# Models not supported by ROCm.\n_ROCM_UNSUPPORTED_MODELS: List[str] = []\n\n# Models partially supported by ROCm.\n# Architecture -> Reason.\n_ROCM_PARTIALLY_SUPPORTED_MODELS: Dict[str, str] = {\n    \"Qwen2ForCausalLM\":\n    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n    \"MistralForCausalLM\":\n    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n    \"MixtralForCausalLM\":\n    \"Sliding window attention is not yet supported in ROCm's flash attention\",\n}\n\n\nclass ModelRegistry:\n\n    @staticmethod\n    def load_model_cls(model_arch: str) -> Optional[Type[nn.Module]]:\n        if model_arch in _OOT_MODELS:\n            return _OOT_MODELS[model_arch]\n        if model_arch not in _MODELS:\n            return None\n        if is_hip():\n            if model_arch in _ROCM_UNSUPPORTED_MODELS:\n                raise ValueError(\n                    f\"Model architecture {model_arch} is not supported by \"\n                    \"ROCm for now.\")\n            if model_arch in _ROCM_PARTIALLY_SUPPORTED_MODELS:\n                logger.warning(\n                    \"Model architecture %s is partially supported by ROCm: %s\",\n                    model_arch, _ROCM_PARTIALLY_SUPPORTED_MODELS[model_arch])\n\n        module_name, model_cls_name = _MODELS[model_arch]\n        module = importlib.import_module(\n            f\"vllm.model_executor.models.{module_name}\")\n        return getattr(module, model_cls_name, None)\n\n    @staticmethod\n    def get_supported_archs() -> List[str]:\n        return list(_MODELS.keys())\n\n    @staticmethod\n    def register_model(model_arch: str, model_cls: Type[nn.Module]):\n        if model_arch in _MODELS:\n            logger.warning(\n                \"Model architecture %s is already registered, and will be \"\n                \"overwritten by the new model class %s.\", model_arch,\n                model_cls.__name__)\n        global _OOT_MODELS\n        _OOT_MODELS[model_arch] = model_cls\n\n    @staticmethod\n    def is_embedding_model(model_arch: str) -> bool:\n        return model_arch in _EMBEDDING_MODELS\n\n\n__all__ = [\n    \"ModelRegistry\",\n]\n",
      "diff": "diff --git a/vllm/model_executor/models/__init__.py b/vllm/model_executor/models/__init__.py\nindex 87508a116..aa5a70757 100644\n--- a/vllm/model_executor/models/__init__.py\n+++ b/vllm/model_executor/models/__init__.py\n@@ -1,3 +1,4 @@\n+import functools\n import importlib\n from typing import Dict, List, Optional, Type\n \n@@ -98,6 +99,14 @@ _ROCM_PARTIALLY_SUPPORTED_MODELS: Dict[str, str] = {\n \n class ModelRegistry:\n \n+    @staticmethod\n+    @functools.lru_cache(maxsize=128)\n+    def _get_model(model_arch: str):\n+        module_name, model_cls_name = _MODELS[model_arch]\n+        module = importlib.import_module(\n+            f\"vllm.model_executor.models.{module_name}\")\n+        return getattr(module, model_cls_name, None)\n+\n     @staticmethod\n     def load_model_cls(model_arch: str) -> Optional[Type[nn.Module]]:\n         if model_arch in _OOT_MODELS:\n@@ -114,10 +123,7 @@ class ModelRegistry:\n                     \"Model architecture %s is partially supported by ROCm: %s\",\n                     model_arch, _ROCM_PARTIALLY_SUPPORTED_MODELS[model_arch])\n \n-        module_name, model_cls_name = _MODELS[model_arch]\n-        module = importlib.import_module(\n-            f\"vllm.model_executor.models.{module_name}\")\n-        return getattr(module, model_cls_name, None)\n+        return ModelRegistry._get_model(model_arch)\n \n     @staticmethod\n     def get_supported_archs() -> List[str]:",
      "change_type": "modified",
      "lines_added": 11,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/sequence.py",
      "old_content": "\"\"\"Sequence and its related classes.\"\"\"\nimport copy\nimport enum\nimport math\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Set, Tuple, Union\n\nimport torch\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\n\nif TYPE_CHECKING:\n    from vllm.inputs import LLMInputs\n    from vllm.multimodal import MultiModalDataDict\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n\n\n@dataclass\nclass Logprob:\n    \"\"\"Infos for supporting OpenAI compatible logprobs and token ranks.\n\n    Attributes:\n        logprob: The logprob of chosen token\n        rank: The vocab rank of chosen token (>=1)\n        decoded_token: The decoded chosen token index\n    \"\"\"\n    logprob: float\n    rank: Optional[int] = None\n    decoded_token: Optional[str] = None\n\n\n# {token_id -> logprob} per each sequence group. None if the corresponding\n# sequence group doesn't require prompt logprob.\nPromptLogprobs = List[Optional[Dict[int, Logprob]]]\n# {token_id -> logprob} for each sequence group.\nSampleLogprobs = List[Dict[int, Logprob]]\n\n\nclass SequenceStatus(enum.IntEnum):\n    \"\"\"Status of a sequence.\"\"\"\n    WAITING = 0\n    RUNNING = 1\n    SWAPPED = 2\n    # Note: anything after SWAPPED (2) will be considered\n    # as a finished status.\n    FINISHED_STOPPED = 3\n    FINISHED_LENGTH_CAPPED = 4\n    FINISHED_ABORTED = 5\n    FINISHED_IGNORED = 6\n\n    @staticmethod\n    def is_finished(status: \"SequenceStatus\") -> bool:\n        return status > SequenceStatus.SWAPPED\n\n    @staticmethod\n    def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n        if status == SequenceStatus.FINISHED_STOPPED:\n            finish_reason = \"stop\"\n        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n            finish_reason = \"length\"\n        elif status == SequenceStatus.FINISHED_ABORTED:\n            finish_reason = \"abort\"\n        elif status == SequenceStatus.FINISHED_IGNORED:\n            # The ignored sequences are the sequences whose prompt lengths\n            # are longer than the model's length cap. Therefore, the stop\n            # reason should also be \"length\" as in OpenAI API.\n            finish_reason = \"length\"\n        else:\n            finish_reason = None\n        return finish_reason\n\n\nclass SequenceStage(enum.Enum):\n    PREFILL = enum.auto()\n    DECODE = enum.auto()\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Metrics associated with a request.\n\n    Attributes:\n        arrival_time: The time when the request arrived.\n        first_scheduled_time: The time when the request was first scheduled.\n        first_token_time: The time when the first token was generated.\n        time_in_queue: The time the request spent in the queue.\n        finished_time: The time when the request was finished.\n    \"\"\"\n    arrival_time: float\n    last_token_time: float\n    first_scheduled_time: Optional[float]\n    first_token_time: Optional[float]\n    time_in_queue: Optional[float]\n    finished_time: Optional[float] = None\n\n\nclass SequenceData:\n    \"\"\"Data associated with a sequence.\n\n    Args:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output. Set to an empty list if\n            None.\n\n    Attributes:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output.\n        cumulative_logprob: The cumulative log probability of the output.\n    \"\"\"\n\n    def __init__(\n        self,\n        prompt_token_ids: List[int],\n        output_token_ids: Optional[List[int]] = None,\n    ) -> None:\n        self._prompt_token_ids: List[int] = list(prompt_token_ids)\n        self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n        self._output_token_ids: List[int] = (\n            list(output_token_ids) if output_token_ids is not None else [])\n\n        self.cumulative_logprob = 0.0\n        # The number of tokens that are computed (that run against the model).\n        self._num_computed_tokens = 0\n        self._stage: SequenceStage = SequenceStage.PREFILL\n\n        self._update_cached_all_tokens()\n\n    def _update_cached_all_tokens(self):\n        self._cached_all_token_ids: List[int] = (self._prompt_token_ids +\n                                                 self._output_token_ids)\n\n    @property\n    def prompt_token_ids(self) -> Tuple[int, ...]:\n        return self._prompt_token_ids_tuple\n\n    @prompt_token_ids.setter\n    def prompt_token_ids(self, new_prompt_token_ids) -> None:\n        self._prompt_token_ids = list(new_prompt_token_ids)\n        self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n        self._update_cached_all_tokens()\n\n    @property\n    def output_token_ids(self) -> Tuple[int, ...]:\n        return tuple(self._output_token_ids)\n\n    @output_token_ids.setter\n    def output_token_ids(self, new_output_token_ids) -> None:\n        self._output_token_ids = list(new_output_token_ids)\n        self._update_cached_all_tokens()\n\n    def append_token_id(self, token_id: int, logprob: float) -> None:\n        self._output_token_ids.append(token_id)\n        self._cached_all_token_ids.append(token_id)\n        self.cumulative_logprob += logprob\n\n    def get_len(self) -> int:\n        return len(self._output_token_ids) + len(self._prompt_token_ids)\n\n    def get_prompt_len(self) -> int:\n        return len(self._prompt_token_ids)\n\n    def get_output_len(self) -> int:\n        return len(self._output_token_ids)\n\n    def get_token_ids(self) -> List[int]:\n        return self._cached_all_token_ids\n\n    def get_prefix_token_ids(\n            self, num_tokens: int\n    ) -> Tuple[Tuple[int, ...], Optional[Tuple[int, ...]]]:\n        \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n        prompt_length = self.get_prompt_len()\n        if num_tokens > prompt_length:\n            return (self._prompt_token_ids_tuple,\n                    tuple(self._output_token_ids[:num_tokens - prompt_length]))\n        else:\n            return (self._prompt_token_ids_tuple[:num_tokens], None)\n\n    def get_num_computed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are already computed.\"\"\"\n        return self._num_computed_tokens\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        self._num_computed_tokens += num_new_computed_tokens\n        assert self._num_computed_tokens <= self.get_len(), (\n            self._num_computed_tokens, self.get_len())\n        # If all tokens are computed, it means it is in decoding phase.\n        if self.get_num_uncomputed_tokens() == 0:\n            self._stage = SequenceStage.DECODE\n\n    def reset_state_for_recompute(self) -> None:\n        \"\"\"Reset the number of computed tokens from this sequence. It is\n        supposed to be called when a sequence needs to be started from\n        the beginning again (e.g., sequence is preempted).\n        \"\"\"\n        self._num_computed_tokens = 0\n        self._stage = SequenceStage.PREFILL\n\n    def get_num_uncomputed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are not computed.\"\"\"\n        # we use `get_len()` which includes prompt_len + output_len instead\n        # of prompt_len here. This is because during recompute we need to\n        # prefill for both prompt and output.\n        return self.get_len() - self.get_num_computed_tokens()\n\n    def get_last_token_id(self) -> int:\n        if not self._output_token_ids:\n            return self._prompt_token_ids[-1]\n        return self._output_token_ids[-1]\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.prompt_token_ids\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.output_token_ids\n\n    @property\n    def stage(self) -> SequenceStage:\n        return self._stage\n\n    def __repr__(self) -> str:\n        return (f\"SequenceData(\"\n                f\"prompt_token_ids={self._prompt_token_ids}, \"\n                f\"output_token_ids={self._output_token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob})\")\n\n\nclass Sequence:\n    \"\"\"Stores the data, status, and block information of a sequence.\n\n    Args:\n        seq_id: The ID of the sequence.\n        inputs: The inputs of the sequence.\n        block_size: The block size of the sequence. Should be the same as the\n            block size used by the block manager and cache engine.\n        lora_request: LoRA request.\n        prompt_adapter_request: Prompt Adapter request.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            seq_id: int,\n            inputs: \"LLMInputs\",\n            block_size: int,\n            eos_token_id: Optional[int] = None,\n            lora_request: Optional[LoRARequest] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None\n    ) -> None:\n        self.seq_id = seq_id\n        self.inputs = inputs\n        self.block_size = block_size\n        self.eos_token_id = eos_token_id\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n\n        self.data = SequenceData(self.prompt_token_ids)\n        self.output_logprobs: SampleLogprobs = []\n        self.output_text = \"\"\n\n        self.status = SequenceStatus.WAITING\n        self.stop_reason: Union[int, str, None] = None\n\n        # Used for incremental detokenization\n        self.prefix_offset = 0\n        self.read_offset = 0\n        # Input + output tokens\n        self.tokens: Optional[List[str]] = None\n\n    @property\n    def n_blocks(self) -> int:\n        return math.ceil(self.get_len() / self.block_size)\n\n    @property\n    def prompt(self) -> Optional[str]:\n        return self.inputs.get(\"prompt\")\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        return self.inputs[\"prompt_token_ids\"]\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        return self.inputs.get(\"multi_modal_data\") or {}\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    def get_output_text_to_return(self, buffer_length: int):\n        # We return the full output text if the sequence is finished.\n        truncate = buffer_length and not self.is_finished()\n        return self.output_text[:-buffer_length] if truncate else (\n            self.output_text)\n\n    def hash_of_block(self, logical_idx: int) -> int:\n        # TODO This can produce incorrect hash when block size > prompt size\n\n        # Compute the number of tokens in the sequence\n        # TODO: The current hashing function is O(L^2). We should optimize\n        # this in the future.\n        num_tokens = self.num_hashed_tokens_of_block(logical_idx)\n        hashed_tokens = self.data.get_prefix_token_ids(num_tokens)\n        return hash((hashed_tokens, self.lora_int_id))\n\n    def num_hashed_tokens_of_block(self, logical_idx: int):\n        return logical_idx * self.block_size + self.block_size\n\n    def reset_state_for_recompute(self):\n        \"\"\"Reset the sequence states for recomputation.\"\"\"\n        self.data.reset_state_for_recompute()\n\n    def append_token_id(\n        self,\n        token_id: int,\n        logprobs: Dict[int, Logprob],\n    ) -> None:\n        assert token_id in logprobs\n        self.output_logprobs.append(logprobs)\n        self.data.append_token_id(token_id, logprobs[token_id].logprob)\n\n    def get_len(self) -> int:\n        return self.data.get_len()\n\n    def get_prompt_len(self) -> int:\n        return self.data.get_prompt_len()\n\n    def get_output_len(self) -> int:\n        return self.data.get_output_len()\n\n    def get_token_ids(self) -> List[int]:\n        return self.data.get_token_ids()\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_prompt_token_ids()\n\n    def get_last_token_id(self) -> int:\n        return self.data.get_last_token_id()\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_output_token_ids()\n\n    def get_cumulative_logprob(self) -> float:\n        return self.data.cumulative_logprob\n\n    def get_beam_search_score(self,\n                              length_penalty: float = 1.0,\n                              seq_len: Optional[int] = None,\n                              eos_token_id: Optional[int] = None) -> float:\n        \"\"\"Calculate the beam search score with length penalty.\n\n        Adapted from\n\n        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938\n        \"\"\"\n        if seq_len is None:\n            seq_len = self.get_len()\n            # NOTE: HF implementation does not count the EOS token\n            # towards the length, we align with that here for testing.\n            if (eos_token_id is not None\n                    and self.get_last_token_id() == eos_token_id):\n                seq_len -= 1\n        return self.get_cumulative_logprob() / (seq_len**length_penalty)\n\n    def is_finished(self) -> bool:\n        return SequenceStatus.is_finished(self.status)\n\n    def fork(self, new_seq_id: int) -> \"Sequence\":\n        new_seq = copy.deepcopy(self)\n        new_seq.seq_id = new_seq_id\n        return new_seq\n\n    def get_num_new_tokens(self) -> int:\n        \"\"\"Get the number of new tokens to be computed.\n\n        Returns:\n            The new number of tokens to be computed. I.e., 1 for decode, or\n            the remaining prompt size for prefill.\n        \"\"\"\n        if self.data.stage == SequenceStage.DECODE:\n            return 1\n        return self.data.get_num_uncomputed_tokens()\n\n    def is_prefill(self) -> bool:\n        return self.data.stage == SequenceStage.PREFILL\n\n    def __repr__(self) -> str:\n        return (f\"Sequence(seq_id={self.seq_id}, \"\n                f\"status={self.status.name}, \"\n                f\"num_blocks={self.n_blocks}, \")\n\n\n@dataclass\nclass SequenceGroupState:\n    \"\"\"Mutable state tied to a specific sequence group\"\"\"\n\n    # torch.Generator used in seeded sampling\n    generator: Optional = None  # type: ignore\n\n\nclass SequenceGroup:\n    \"\"\"A group of sequences that are generated from the same prompt.\n\n    Args:\n        request_id: The ID of the request.\n        seqs: The list of sequences.\n        sampling_params: The sampling parameters used to generate the outputs.\n        arrival_time: The arrival time of the request.\n        lora_request: LoRA request.\n        embeddings: The embeddings vectors of the prompt of the sequence group\n            for an embedding model.\n        pooling_params: The pooling parameters used to generate the pooling\n            for an embedding model.\n        encoder_seq: Optional, the single encoder sequence. Should be None\n                     unless you are working with an encoder/decoder model.\n        trace_headers: OpenTelemetry trace headers.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        seqs: List[Sequence],\n        arrival_time: float,\n        sampling_params: Optional[SamplingParams] = None,\n        lora_request: Optional[LoRARequest] = None,\n        embeddings: Optional[List[float]] = None,\n        pooling_params: Optional[PoolingParams] = None,\n        encoder_seq: Optional[Sequence] = None,\n        trace_headers: Optional[Dict[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n        self.sampling_params = sampling_params\n        self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                      last_token_time=arrival_time,\n                                      first_scheduled_time=None,\n                                      first_token_time=None,\n                                      time_in_queue=None)\n        self.lora_request = lora_request\n        self.prompt_logprobs: Optional[PromptLogprobs] = None\n        self.state = SequenceGroupState()\n        self.embeddings = embeddings\n        self.pooling_params = pooling_params\n        self.prompt_adapter_request = prompt_adapter_request\n        self.encoder_seq = encoder_seq\n        self.trace_headers = trace_headers\n\n    @property\n    def prompt(self) -> Optional[str]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return next(iter(self.seqs_dict.values())).prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return next(iter(self.seqs_dict.values())).prompt_token_ids\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        # All sequences in the group should have the same multi-modal data.\n        # We use the multi-modal data of an arbitrary sequence.\n        return next(iter(self.seqs_dict.values())).multi_modal_data\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens\\\n                         if self.prompt_adapter_request else 0\n\n    def get_last_latency(self, now: float) -> Optional[float]:\n        \"\"\"Sets the last token time for Request level timings.\"\"\"\n        # If still in prefill phase, raise Error.\n        if self.is_prefill():\n            raise ValueError(\n                \"seq_group.get_last_latency() should not be called \"\n                \"if the seq_group is in prefill phase.\")\n\n        # Otherwise return token latency.\n        latency = now - self.metrics.last_token_time\n        self.metrics.last_token_time = now\n        return latency\n\n    def maybe_set_first_token_time(self, time: float) -> None:\n        \"\"\"Sets the first token time for Request level timings.\"\"\"\n        # Note: in a case where a sequence_group is swapped and\n        #   recomputed, the time between iterations is counted\n        #   in TPOT, rather than recalculating TTFT (since from the )\n        #   POV of the user, there is simply a long generation delay.\n        if (self.metrics.first_token_time is None\n                and self.get_seqs()[0].get_output_len() == 1):\n            self.metrics.first_token_time = time\n\n    def maybe_set_first_scheduled_time(self, time: float) -> None:\n        \"\"\"Sets the first scheduled time and time in queue for Request\n        level timings.\"\"\"\n        if self.metrics.first_scheduled_time is None:\n            self.metrics.first_scheduled_time = time\n            self.metrics.time_in_queue = time - self.metrics.arrival_time\n\n    def set_finished_time(self, time: Optional[float]) -> None:\n        \"\"\"Sets the finished time for Request level timings.\"\"\"\n        self.metrics.finished_time = time\n\n    def get_max_num_running_seqs(self) -> int:\n        \"\"\"The maximum number of sequences running in parallel in the remaining\n        lifetime of the request.\"\"\"\n        if self.sampling_params and self.sampling_params.use_beam_search:\n            # For beam search, maximally there will always be `best_of` beam\n            # candidates running in the future.\n            return self.sampling_params.best_of\n        else:\n            if (self.sampling_params\n                    and self.sampling_params.best_of > self.num_seqs()):\n                # At prompt stage, the sequence group is not yet filled up\n                # and only have one sequence running. However, in the\n                # generation stage, we will have `best_of` sequences running.\n                return self.sampling_params.best_of\n            # At sampling stages, return the number of actual sequences\n            # that are not finished yet.\n            return self.num_unfinished_seqs()\n\n    def get_seqs(\n        self,\n        status: Optional[SequenceStatus] = None,\n    ) -> List[Sequence]:\n        return list(self.seqs_dict.values()) if status is None else [\n            seq for seq in self.seqs_dict.values() if seq.status == status\n        ]\n\n    def is_encoder_decoder(self) -> bool:\n        return self.encoder_seq is not None\n\n    def get_encoder_seq(self) -> Optional[Sequence]:\n        return self.encoder_seq\n\n    def get_unfinished_seqs(self) -> List[Sequence]:\n        return [\n            seq for seq in self.seqs_dict.values() if not seq.is_finished()\n        ]\n\n    def get_finished_seqs(self) -> List[Sequence]:\n        return [seq for seq in self.seqs_dict.values() if seq.is_finished()]\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        for seq in self.seqs_dict.values():\n            if not seq.is_finished():\n                seq.data.update_num_computed_tokens(num_new_computed_tokens)\n\n    def get_num_uncomputed_tokens(self) -> int:\n        num_uncomputed_tokens = 0\n        for seq in self.get_seqs():\n            if not seq.is_finished():\n                num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n        return num_uncomputed_tokens\n\n    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:\n        # Optimization. We don't need to call get_seqs if we don't need to\n        # filter by states.\n        if status is None:\n            return len(self.seqs_dict)\n\n        return len(self.get_seqs(status))\n\n    def num_unfinished_seqs(self) -> int:\n        return len(self.get_unfinished_seqs())\n\n    def num_finished_seqs(self) -> int:\n        return len(self.get_finished_seqs())\n\n    def find(self, seq_id: int) -> Sequence:\n        if seq_id not in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        return self.seqs_dict[seq_id]\n\n    def add(self, seq: Sequence) -> None:\n        if seq.seq_id in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n        self.seqs_dict[seq.seq_id] = seq\n\n    def remove(self, seq_id: int) -> None:\n        if seq_id not in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        del self.seqs_dict[seq_id]\n\n    def is_finished(self) -> bool:\n        return all(seq.is_finished() for seq in self.get_seqs())\n\n    def is_prefill(self) -> bool:\n        # Every sequence should be in the same stage.\n        return self.get_seqs()[0].is_prefill()\n\n    def __repr__(self) -> str:\n        return (f\"SequenceGroup(request_id={self.request_id}, \"\n                f\"sampling_params={self.sampling_params}, \"\n                f\"num_seqs={len(self.seqs_dict)})\")\n\n\nclass SequenceGroupMetadata:\n    \"\"\"Metadata for a sequence group. Used to create `AttentionMetadata`.\n\n    Args:\n        request_id: The ID of the request.\n        is_prompt: Whether the request is at prompt stage.\n        seq_data: The sequence data. (Seq id -> sequence data)\n        sampling_params: The sampling parameters used to generate the outputs.\n        block_tables: The block tables. (Seq id -> list of physical block\n            numbers)\n        do_sample: True if sampling is required. Sampling is not required when\n            e.g., prefill is chunked, and the current iteration only computes\n            query tokens for prefill, we don't need sampling.\n        token_chunk_size: The number of tokens to be processed (per sequence).\n            None if chunking is not required.\n        lora_request: LoRA request.\n        computed_block_nums: The block numbers that are already computed,\n            used in prefix caching.\n        state: Internal state tied to this sequence group.\n        multi_modal_data: Multi modal data.\n        encoder_seq_data: Optional sequence data for encoder prompt\n                          (SequenceGroup.encoder_seq). Should be None \n                          unless you are working with an encoder/decoder\n                          model.\n        cross_block_table: Optional cross-attention block table associated\n                           with the encoder prompt\n                           (SequenceGroup.encoder_seq). Should be None\n                           unless you are working with an encoder/decoder\n                           model.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        is_prompt: bool,\n        seq_data: Dict[int, SequenceData],\n        sampling_params: SamplingParams,\n        block_tables: Dict[int, List[int]],\n        do_sample: bool = True,\n        pooling_params: Optional[PoolingParams] = None,\n        token_chunk_size: Optional[int] = None,\n        lora_request: Optional[LoRARequest] = None,\n        computed_block_nums: Optional[List[int]] = None,\n        state: Optional[SequenceGroupState] = None,\n        multi_modal_data: Optional[\"MultiModalDataDict\"] = None,\n        encoder_seq_data: Optional[SequenceData] = None,\n        cross_block_table: Optional[List[int]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.is_prompt = is_prompt\n        self.seq_data = seq_data\n        self.sampling_params = sampling_params\n        self.block_tables = block_tables\n        self.pooling_params = pooling_params\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n        self.computed_block_nums = computed_block_nums\n        self.multi_modal_data = multi_modal_data\n        self.state = SequenceGroupState() if state is None else state\n        self.encoder_seq_data = encoder_seq_data\n        self.cross_block_table = cross_block_table\n        self._token_chunk_size = token_chunk_size\n        self.do_sample = do_sample\n\n        # The number of speculative tokens adopted in this request.\n        # None means specuative decoding is not used.\n        # Zero means speculative decoding is disabled for some reasons.\n        # TODO: We should maintain this states out of the sequence group.\n        self.num_speculative_tokens = None\n\n        if self._token_chunk_size is None:\n            if is_prompt:\n                self._token_chunk_size = list(seq_data.values())[0].get_len()\n            else:\n                self._token_chunk_size = 1\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def token_chunk_size(self) -> int:\n        \"\"\"Return the number of tokens to be processed (chunk size).\"\"\"\n        assert self._token_chunk_size is not None\n        return self._token_chunk_size\n\n\nclass SequenceOutput:\n    \"\"\"The model output associated with a sequence.\n\n    Args:\n        parent_seq_id: The ID of the parent sequence (for forking in beam\n            search).\n        output_token: The output token ID.\n        logprobs: The logprobs of the output token.\n            (Token id -> logP(x_i+1 | x_0, ..., x_i))\n    \"\"\"\n\n    def __init__(\n        self,\n        parent_seq_id: int,\n        output_token: int,\n        logprobs: Dict[int, Logprob],\n    ) -> None:\n        self.parent_seq_id = parent_seq_id\n        self.output_token = output_token\n        self.logprobs = logprobs\n\n    def __repr__(self) -> str:\n        return (f\"SequenceOutput(parent_seq_id={self.parent_seq_id}, \"\n                f\"output_token={self.output_token}, \"\n                f\"logprobs={self.logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, SequenceOutput):\n            raise NotImplementedError()\n        equal = (self.parent_seq_id == other.parent_seq_id\n                 and self.output_token == other.output_token)\n        log_probs_equal = other.logprobs == self.logprobs\n        return equal and log_probs_equal\n\n\nclass SequenceGroupOutput(ABC):\n    \"\"\"The base class for model outputs associated with a sequence group.\"\"\"\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        pass\n\n    @abstractmethod\n    def __eq__(self, other: object) -> bool:\n        pass\n\n\nclass CompletionSequenceGroupOutput(SequenceGroupOutput):\n    \"\"\"The model output associated with a completion sequence group.\"\"\"\n\n    def __init__(\n        self,\n        samples: List[SequenceOutput],\n        prompt_logprobs: Optional[PromptLogprobs],\n    ) -> None:\n        self.samples = samples\n        # Prompt logprob for each prompt query token.\n        self.prompt_logprobs = prompt_logprobs\n\n    def __repr__(self) -> str:\n        return (f\"CompletionSequenceGroupOutput(samples={self.samples}, \"\n                f\"prompt_logprobs={self.prompt_logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, CompletionSequenceGroupOutput):\n            raise NotImplementedError()\n        return (self.samples == other.samples\n                and self.prompt_logprobs == other.prompt_logprobs)\n\n\nclass EmbeddingSequenceGroupOutput(SequenceGroupOutput):\n    \"\"\"The model output associated with an embedding sequence group.\"\"\"\n\n    def __init__(\n        self,\n        embeddings: List[float],\n    ) -> None:\n        self.embeddings = embeddings\n\n    def __repr__(self) -> str:\n        return (f\"EmbeddingSequenceGroupOutput(\"\n                f\"embeddings_shape={len(self.embeddings)})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, EmbeddingSequenceGroupOutput):\n            raise NotImplementedError()\n        return self.embeddings == other.embeddings\n\n\n@dataclass\nclass IntermediateTensors:\n    \"\"\"For all pipeline stages except the last, we need to return the hidden\n    states and residuals to be sent to the next stage. This data structure\n    contains the hidden states and residuals for a request.\n    \"\"\"\n\n    tensors: Dict[str, torch.Tensor]\n\n    def __getitem__(self, key: Union[str, slice]):\n        if isinstance(key, str):\n            return self.tensors[key]\n        elif isinstance(key, slice):\n            return self.__class__({k: v[key] for k, v in self.tensors.items()})\n\n    def __setitem__(self, key: str, value):\n        self.tensors[key] = value\n\n    def __len__(self):\n        return len(self.tensors)\n\n    def __eq__(self, other: object):\n        return isinstance(other, self.__class__) and self\n\n    def __repr__(self) -> str:\n        return f\"IntermediateTensors(tensors={self.tensors})\"\n\n\n@dataclass\nclass SamplerOutput:\n    \"\"\"For each sequence group, we generate a list of SequenceOutput object,\n    each of which contains one possible candidate for the next token.\n\n    This data structure implements methods, so it can be used like a list, but\n    also has optional fields for device tensors.\n    \"\"\"\n\n    outputs: List[CompletionSequenceGroupOutput]\n\n    # On-device tensor containing probabilities of each token.\n    sampled_token_probs: Optional[torch.Tensor] = None\n\n    # On-device tensor containing the logprobs of each token.\n    logprobs: Optional[\"torch.Tensor\"] = None\n\n    # On-device tensor containing the sampled token ids.\n    sampled_token_ids: Optional[torch.Tensor] = None\n\n    # Spec decode metrics populated by workers.\n    spec_decode_worker_metrics: Optional[\"SpecDecodeWorkerMetrics\"] = None\n\n    # Optional last hidden states from the model.\n    hidden_states: Optional[torch.Tensor] = None\n\n    def __getitem__(self, idx: int):\n        return self.outputs[idx]\n\n    def __setitem__(self, idx: int, value):\n        self.outputs[idx] = value\n\n    def __len__(self):\n        return len(self.outputs)\n\n    def __eq__(self, other: object):\n        return isinstance(other,\n                          self.__class__) and self.outputs == other.outputs\n\n    def __repr__(self) -> str:\n        \"\"\"Show the shape of a tensor instead of its values to reduce noise.\n        \"\"\"\n        sampled_token_probs_repr = (\"None\" if self.sampled_token_probs is None\n                                    else self.sampled_token_probs.shape)\n        sampled_token_ids_repr = (\"None\" if self.sampled_token_ids is None else\n                                  self.sampled_token_ids.shape)\n        return (\n            f\"SamplerOutput(outputs={self.outputs}, \"\n            f\"sampled_token_probs={sampled_token_probs_repr}, \"\n            f\"sampled_token_ids={sampled_token_ids_repr}, \"\n            f\"spec_decode_worker_metrics={self.spec_decode_worker_metrics})\")\n\n\n@dataclass\nclass PoolerOutput:\n    \"\"\"The output from a pooling operation in the embedding model.\"\"\"\n    outputs: List[EmbeddingSequenceGroupOutput]\n\n    spec_decode_worker_metrics: Optional[\"SpecDecodeWorkerMetrics\"] = None\n\n    def __getitem__(self, idx: int):\n        return self.outputs[idx]\n\n    def __setitem__(self, idx: int, value):\n        self.outputs[idx] = value\n\n    def __len__(self):\n        return len(self.outputs)\n\n    def __eq__(self, other: object):\n        return isinstance(other,\n                          self.__class__) and self.outputs == other.outputs\n\n\ndef get_all_seq_ids(\n        seq_group_metadata_list: List[SequenceGroupMetadata]) -> List[int]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    return [seq_id for sg in seq_group_metadata_list for seq_id in sg.seq_data]\n\n\ndef get_all_seq_ids_and_request_ids(\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n) -> Tuple[List[int], Dict[str, Set[int]]]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    seq_ids: List[int] = []\n    request_id_seq_ids_mapping: Dict[str, Set[int]] = defaultdict(set)\n    for sg in seq_group_metadata_list:\n        for seq_id in sg.seq_data:\n            seq_ids.append(seq_id)\n            request_id_seq_ids_mapping[sg.request_id].add(seq_id)\n    return seq_ids, request_id_seq_ids_mapping\n\n\nclass HiddenStates:\n    \"\"\"Hidden states corresponding to in-progress sequences.\n    Used in speculative decoding to pass hidden states from\n    the target model to the proposer model in the subsequent step.\n\n    seq_ids are the sequence ids of each entry of the batch\n    dimension of the hidden_states tensor\"\"\"\n\n    def __init__(self, seq_group_metadata_list: List[SequenceGroupMetadata],\n                 hidden_states: torch.Tensor):\n        assert len(seq_group_metadata_list) == len(hidden_states)\n        self.seq_ids: List[int] = get_all_seq_ids(seq_group_metadata_list)\n        self.hidden_states: torch.Tensor = hidden_states\n\n    def update(self, seq_group_metadata_list: List[SequenceGroupMetadata],\n               hidden_states: torch.Tensor) -> None:\n        \"\"\"Update hidden states from target model invocation.\"\"\"\n        assert len(seq_group_metadata_list) == len(hidden_states)\n        self.seq_ids.extend(get_all_seq_ids(seq_group_metadata_list))\n        self.hidden_states = torch.cat([self.hidden_states, hidden_states])\n\n    def prune(self,\n              seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        \"\"\"Prune to provided list of sequence ids.\"\"\"\n        seq_ids = get_all_seq_ids(seq_group_metadata_list)\n        if seq_ids != self.seq_ids:\n            # Batch contents changed - prune removed sequences.\n            index = [self.seq_ids.index(seq_id) for seq_id in seq_ids]\n            self.hidden_states = self.hidden_states[index]\n            self.seq_ids = seq_ids\n\n\n@dataclass\nclass ExecuteModelRequest:\n    \"\"\"The model execution request, containing CPU metadata only. The LLM\n    engine should create an instance of this class for each request batch.\"\"\"\n    # The sequence group metadata list.\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]] = field(default_factory=list)\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]] = field(default_factory=list)\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]] = field(default_factory=list)\n    # Virtual engine ID for pipeline parallel.\n    virtual_engine: int = 0\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int = 0\n    # The number of requests in the running queue.\n    running_queue_size: int = 0\n    # Optional hidden states from prior step.\n    previous_hidden_states: Optional[HiddenStates] = None\n    # The number of forward steps to run.\n    num_steps: int = 1\n    # Finished request ids since last step.\n    finished_requests_ids: List[str] = field(default_factory=list)\n\n    def clone(\n        self, seq_group_metadata_list: List[SequenceGroupMetadata]\n    ) -> \"ExecuteModelRequest\":\n        \"\"\"Clone the request with a new sequence group metadata list.\"\"\"\n        return ExecuteModelRequest(\n            seq_group_metadata_list=seq_group_metadata_list,\n            blocks_to_swap_in=self.blocks_to_swap_in.copy(),\n            blocks_to_swap_out=self.blocks_to_swap_out.copy(),\n            blocks_to_copy=self.blocks_to_copy.copy(),\n            virtual_engine=self.virtual_engine,\n            num_lookahead_slots=self.num_lookahead_slots,\n            running_queue_size=self.running_queue_size,\n            previous_hidden_states=self.previous_hidden_states,\n            num_steps=self.num_steps,\n            finished_requests_ids=self.finished_requests_ids)\n",
      "diff": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex 1cebf68d4..6c12a01bd 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -457,24 +457,25 @@ class SequenceGroup:\n         self.prompt_adapter_request = prompt_adapter_request\n         self.encoder_seq = encoder_seq\n         self.trace_headers = trace_headers\n+        self._first_seq = next(iter(self.seqs_dict.values()))\n \n     @property\n     def prompt(self) -> Optional[str]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return next(iter(self.seqs_dict.values())).prompt\n+        return self._first_seq.prompt\n \n     @property\n     def prompt_token_ids(self) -> List[int]:\n         # All sequences in the group should have the same prompt.\n         # We use the prompt of an arbitrary sequence.\n-        return next(iter(self.seqs_dict.values())).prompt_token_ids\n+        return self._first_seq.prompt_token_ids\n \n     @property\n     def multi_modal_data(self) -> \"MultiModalDataDict\":\n         # All sequences in the group should have the same multi-modal data.\n         # We use the multi-modal data of an arbitrary sequence.\n-        return next(iter(self.seqs_dict.values())).multi_modal_data\n+        return self._first_seq.multi_modal_data\n \n     @property\n     def lora_int_id(self) -> int:",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/utils.py",
      "old_content": "import argparse\nimport asyncio\nimport contextlib\nimport datetime\nimport enum\nimport gc\nimport os\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nimport uuid\nimport warnings\nfrom collections import defaultdict\nfrom functools import lru_cache, partial, wraps\nfrom platform import uname\nfrom typing import (Any, AsyncIterator, Awaitable, Callable, Dict, Generic,\n                    Hashable, List, Optional, OrderedDict, Set, Tuple, TypeVar,\n                    Union)\n\nimport numpy as np\nimport psutil\nimport torch\nimport torch.types\nfrom typing_extensions import ParamSpec\n\nimport vllm.envs as envs\nfrom vllm import _custom_ops as ops\nfrom vllm.logger import enable_trace_function_call, init_logger\n\nlogger = init_logger(__name__)\n\nSTR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.half,\n    \"bfloat16\": torch.bfloat16,\n    \"float\": torch.float,\n    \"fp8\": torch.uint8,\n    \"fp8_e4m3\": torch.uint8,\n    \"fp8_e5m2\": torch.uint8,\n}\n\nP = ParamSpec('P')\nK = TypeVar(\"K\")\nT = TypeVar(\"T\")\n\n\nclass _Sentinel:\n    ...\n\n\nALL_PINNED_SENTINEL = _Sentinel()\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\nclass LRUCache(Generic[T]):\n\n    def __init__(self, capacity: int):\n        self.cache: OrderedDict[Hashable, T] = OrderedDict()\n        self.pinned_items: Set[Hashable] = set()\n        self.capacity = capacity\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self.cache\n\n    def __len__(self) -> int:\n        return len(self.cache)\n\n    def __getitem__(self, key: Hashable) -> Optional[T]:\n        return self.get(key)\n\n    def __setitem__(self, key: Hashable, value: T) -> None:\n        self.put(key, value)\n\n    def __delitem__(self, key: Hashable) -> None:\n        self.pop(key)\n\n    def touch(self, key: Hashable) -> None:\n        self.cache.move_to_end(key)\n\n    def get(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        if key in self.cache:\n            value: Optional[T] = self.cache[key]\n            self.cache.move_to_end(key)\n        else:\n            value = default_value\n        return value\n\n    def put(self, key: Hashable, value: T) -> None:\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n        self._remove_old_if_needed()\n\n    def pin(self, key: Hashable) -> None:\n        \"\"\"\n        Pins a key in the cache preventing it from being\n        evicted in the LRU order.\n        \"\"\"\n        if key not in self.cache:\n            raise ValueError(f\"Cannot pin key: {key} not in cache.\")\n        self.pinned_items.add(key)\n\n    def _unpin(self, key: Hashable) -> None:\n        self.pinned_items.remove(key)\n\n    def _on_remove(self, key: Hashable, value: Optional[T]):\n        pass\n\n    def remove_oldest(self, remove_pinned=False):\n        if not self.cache:\n            return\n\n        if not remove_pinned:\n            # pop the oldest item in the cache that is not pinned\n            lru_key = next(\n                (key for key in self.cache if key not in self.pinned_items),\n                ALL_PINNED_SENTINEL)\n            if lru_key is ALL_PINNED_SENTINEL:\n                raise RuntimeError(\"All items are pinned, \"\n                                   \"cannot remove oldest from the cache.\")\n        else:\n            lru_key = next(iter(self.cache))\n        self.pop(lru_key)\n\n    def _remove_old_if_needed(self) -> None:\n        while len(self.cache) > self.capacity:\n            self.remove_oldest()\n\n    def pop(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        run_on_remove = key in self.cache\n        value: Optional[T] = self.cache.pop(key, default_value)\n        # remove from pinned items\n        if key in self.pinned_items:\n            self._unpin(key)\n        if run_on_remove:\n            self._on_remove(key, value)\n        return value\n\n    def clear(self):\n        while len(self.cache) > 0:\n            self.remove_oldest(remove_pinned=True)\n        self.cache.clear()\n\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\n\n@lru_cache(maxsize=None)\ndef is_cpu() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"cpu\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_openvino() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"openvino\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_neuron() -> bool:\n    try:\n        import transformers_neuronx\n    except ImportError:\n        transformers_neuronx = None\n    return transformers_neuronx is not None\n\n\n@lru_cache(maxsize=None)\ndef is_tpu() -> bool:\n    try:\n        import libtpu\n    except ImportError:\n        libtpu = None\n    return libtpu is not None\n\n\n@lru_cache(maxsize=None)\ndef is_xpu() -> bool:\n    from importlib.metadata import version\n    is_xpu_flag = \"xpu\" in version(\"vllm\")\n    # vllm is not build with xpu\n    if not is_xpu_flag:\n        return False\n    try:\n        import intel_extension_for_pytorch as ipex  # noqa: F401\n        _import_ipex = True\n    except ImportError as e:\n        logger.warning(\"Import Error for IPEX: %s\", e.msg)\n        _import_ipex = False\n    # ipex dependency is not ready\n    if not _import_ipex:\n        logger.warning(\"not found ipex lib\")\n        return False\n    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n\n\n@lru_cache(maxsize=None)\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    max_shared_mem = (\n        ops.get_max_shared_memory_per_block_device_attribute(gpu))\n    # value 0 will cause MAX_SEQ_LEN become negative and test_attention.py\n    # will fail\n    assert max_shared_mem > 0, \"max_shared_mem can not be zero\"\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\n@lru_cache(maxsize=None)\ndef get_vllm_instance_id() -> str:\n    \"\"\"\n    If the environment variable VLLM_INSTANCE_ID is set, return it.\n    Otherwise, return a random UUID.\n    Instance id represents an instance of the VLLM. All processes in the same\n    instance should have the same instance id.\n    \"\"\"\n    return envs.VLLM_INSTANCE_ID or f\"vllm-instance-{random_uuid()}\"\n\n\n@lru_cache(maxsize=None)\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\ndef make_async(func: Callable[P, T]) -> Callable[P, Awaitable[T]]:\n    \"\"\"Take a blocking function, and run it on in an executor thread.\n\n    This function prevents the blocking function from blocking the\n    asyncio event loop.\n    The code in this function needs to be thread safe.\n    \"\"\"\n\n    def _async_wrapper(*args: P.args, **kwargs: P.kwargs) -> asyncio.Future:\n        loop = asyncio.get_event_loop()\n        p_func = partial(func, *args, **kwargs)\n        return loop.run_in_executor(executor=None, func=p_func)\n\n    return _async_wrapper\n\n\ndef merge_async_iterators(\n        *iterators: AsyncIterator[T]) -> AsyncIterator[Tuple[int, T]]:\n    \"\"\"Merge multiple asynchronous iterators into a single iterator.\n\n    This method handle the case where some iterators finish before others.\n    When it yields, it yields a tuple (i, item) where i is the index of the\n    iterator that yields the item.\n    \"\"\"\n    queue: asyncio.Queue[Union[Tuple[int, T], Exception]] = asyncio.Queue()\n\n    finished = [False] * len(iterators)\n\n    async def producer(i: int, iterator: AsyncIterator[T]):\n        try:\n            async for item in iterator:\n                await queue.put((i, item))\n        except Exception as e:\n            await queue.put(e)\n        finished[i] = True\n\n    _tasks = [\n        asyncio.create_task(producer(i, iterator))\n        for i, iterator in enumerate(iterators)\n    ]\n\n    async def consumer():\n        try:\n            while not all(finished) or not queue.empty():\n                item = await queue.get()\n                if isinstance(item, Exception):\n                    raise item\n                yield item\n        except (Exception, asyncio.CancelledError) as e:\n            for task in _tasks:\n                if sys.version_info >= (3, 9):\n                    # msg parameter only supported in Python 3.9+\n                    task.cancel(e)\n                else:\n                    task.cancel()\n            raise e\n        await asyncio.gather(*_tasks)\n\n    return consumer()\n\n\ndef get_ip() -> str:\n    host_ip = envs.VLLM_HOST_IP\n    if host_ip:\n        return host_ip\n\n    # IP is not set, try to get it from the network interface\n\n    # try ipv4\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        s.connect((\"8.8.8.8\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    # try ipv6\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        # Google's public DNS server, see\n        # https://developers.google.com/speed/public-dns/docs/using#addresses\n        s.connect((\"2001:4860:4860::8888\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    warnings.warn(\n        \"Failed to get the IP address, using 0.0.0.0 by default.\"\n        \"The value can be set by the environment variable\"\n        \" VLLM_HOST_IP or HOST_IP.\",\n        stacklevel=2)\n    return \"0.0.0.0\"\n\n\ndef get_distributed_init_method(ip: str, port: int) -> str:\n    # Brackets are not permitted in ipv4 addresses,\n    # see https://github.com/python/cpython/issues/103848\n    return f\"tcp://[{ip}]:{port}\" if \":\" in ip else f\"tcp://{ip}:{port}\"\n\n\ndef get_open_port() -> int:\n    port = envs.VLLM_PORT\n    if port is not None:\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\",\n                            port - 1, port)\n    # try ipv4\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n    except OSError:\n        # try ipv6\n        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n\n\ndef update_environment_variables(envs: Dict[str, str]):\n    for k, v in envs.items():\n        if k in os.environ and os.environ[k] != v:\n            logger.warning(\n                \"Overwriting environment variable %s \"\n                \"from '%s' to '%s'\", k, os.environ[k], v)\n        os.environ[k] = v\n\n\ndef init_kmp_env():\n    if not is_cpu():\n        return\n\n    ld_prealod_str = os.getenv(\"LD_PRELOAD\", \"\")\n    if \"libiomp5.so\" not in ld_prealod_str:\n        return\n\n    # The time(milliseconds) that a thread should wait after completing the\n    # execution of a parallel region, before sleeping.\n    os.environ['KMP_BLOCKTIME'] = \"1\"\n    # dump settings on start up\n    os.environ['KMP_SETTINGS'] = \"1\"\n    # Prevents the CPU to run into low performance state\n    os.environ['KMP_TPAUSE'] = \"0\"\n    # Provides fine granularity parallelism\n    os.environ['KMP_FORKJOIN_BARRIER_PATTERN'] = \"dist,dist\"\n    os.environ['KMP_PLAIN_BARRIER_PATTERN'] = \"dist,dist\"\n    os.environ['KMP_REDUCTION_BARRIER_PATTERN'] = \"dist,dist\"\n\n\ndef chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:\n    \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n\n\ndef cdiv(a: int, b: int) -> int:\n    \"\"\"Ceiling division.\"\"\"\n    return -(a // -b)\n\n\ndef _generate_random_fp8(\n    tensor: torch.Tensor,\n    low: float,\n    high: float,\n) -> None:\n    # NOTE(zhaoyang): Due to NaN and Inf representation for fp8 data type,\n    # it may occur Inf or NaN if we directly use torch.randint\n    # to generate random data for fp8 data.\n    # For example, s.11111.00 in fp8e5m2 format represents Inf.\n    #     | E4M3        | E5M2\n    #-----|-------------|-------------------\n    # Inf | N/A         | s.11111.00\n    # NaN | s.1111.111  | s.11111.{01,10,11}\n    from vllm import _custom_ops as ops\n    tensor_tmp = torch.empty_like(tensor, dtype=torch.float16)\n    tensor_tmp.uniform_(low, high)\n    ops.convert_fp8(tensor, tensor_tmp)\n    del tensor_tmp\n\n\ndef get_kv_cache_torch_dtype(\n        cache_dtype: Optional[Union[str, torch.dtype]],\n        model_dtype: Optional[Union[str, torch.dtype]] = None) -> torch.dtype:\n    if isinstance(cache_dtype, str):\n        if cache_dtype == \"auto\":\n            if isinstance(model_dtype, str):\n                torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[model_dtype]\n            elif isinstance(model_dtype, torch.dtype):\n                torch_dtype = model_dtype\n            else:\n                raise ValueError(f\"Invalid model dtype: {model_dtype}\")\n        elif cache_dtype in [\"half\", \"bfloat16\", \"float\"]:\n            torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_dtype]\n        elif cache_dtype == \"fp8\":\n            torch_dtype = torch.uint8\n        else:\n            raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    elif isinstance(cache_dtype, torch.dtype):\n        torch_dtype = cache_dtype\n    else:\n        raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    return torch_dtype\n\n\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: int = 0,\n    device: Optional[str] = \"cuda\",\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    assert cache_dtype != \"fp8\"\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n    key_value_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n    scale = head_size**-0.5\n\n    key_caches: List[torch.Tensor] = []\n    value_caches: List[torch.Tensor] = []\n\n    for _ in range(num_layers):\n        key_value_cache = torch.empty(size=key_value_cache_shape,\n                                      dtype=torch_dtype,\n                                      device=device)\n        key_value_cache.uniform_(-scale, scale)\n        key_caches.append(key_value_cache[:, 0])\n        value_caches.append(key_value_cache[:, 1])\n    return key_caches, value_caches\n\n\ndef create_kv_caches_with_random(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: int = 0,\n    device: Optional[str] = \"cuda\",\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n\n    scale = head_size**-0.5\n    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_caches: List[torch.Tensor] = []\n    for _ in range(num_layers):\n        key_cache = torch.empty(size=key_cache_shape,\n                                dtype=torch_dtype,\n                                device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_cache)\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_caches: List[torch.Tensor] = []\n    for _ in range(num_layers):\n        value_cache = torch.empty(size=value_cache_shape,\n                                  dtype=torch_dtype,\n                                  device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support value cache of type {cache_dtype}\")\n        value_caches.append(value_cache)\n    return key_caches, value_caches\n\n\n@lru_cache\ndef print_warning_once(msg: str) -> None:\n    logger.warning(msg)\n\n\n@lru_cache(maxsize=None)\ndef is_pin_memory_available() -> bool:\n\n    if in_wsl():\n        # Pinning memory in WSL is not supported.\n        # https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications\n        print_warning_once(\"Using 'pin_memory=False' as WSL is detected. \"\n                           \"This may slow down the performance.\")\n        return False\n    elif is_xpu():\n        print_warning_once(\"Pin memory is not supported on XPU.\")\n        return False\n    elif is_neuron():\n        print_warning_once(\"Pin memory is not supported on Neuron.\")\n        return False\n    elif is_cpu() or is_openvino():\n        return False\n    return True\n\n\nclass CudaMemoryProfiler:\n\n    def __init__(self, device: Optional[torch.types.Device] = None):\n        self.device = device\n\n    def current_memory_usage(self) -> float:\n        # Return the memory usage in bytes.\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats(self.device)\n            mem = torch.cuda.max_memory_allocated(self.device)\n        elif is_xpu():\n            torch.xpu.reset_peak_memory_stats(self.device)\n            mem = torch.xpu.max_memory_allocated(self.device)\n        return mem\n\n    def __enter__(self):\n        self.initial_memory = self.current_memory_usage()\n        # This allows us to call methods of the context manager if needed\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.final_memory = self.current_memory_usage()\n        self.consumed_memory = self.final_memory - self.initial_memory\n\n        # Force garbage collection\n        gc.collect()\n\n\ndef str_to_int_tuple(s: str) -> Tuple[int, ...]:\n    \"\"\"Convert a string to a tuple of integers.\"\"\"\n    try:\n        return tuple(map(int, s.split(\",\")))\n    except ValueError as e:\n        raise ValueError(\n            \"String must be a series of integers separated by commas \"\n            f\"(e.g., 1, 2, 3). Given input: {s}\") from e\n\n\ndef make_tensor_with_pad(\n    x: List[List[int]],\n    max_len: int,\n    pad: int,\n    dtype: torch.dtype,\n    device: Optional[Union[str, torch.device]],\n) -> torch.Tensor:\n    \"\"\"Make a padded tensor of a 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    padded_x = np.zeros([len(x), max_len], dtype=np.int32) + pad\n    for ind, blocktb in enumerate(x):\n        assert len(blocktb) <= max_len\n        padded_x[ind, :len(blocktb)] = blocktb\n    return torch.tensor(padded_x, dtype=dtype, device=device)\n\n\ndef async_tensor_h2d(\n    data: list,\n    dtype: torch.dtype,\n    target_device: Union[str, torch.device],\n    pin_memory: bool,\n) -> torch.Tensor:\n    \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n    return t.to(device=target_device, non_blocking=True)\n\n\ndef maybe_expand_dim(tensor: torch.Tensor,\n                     target_dims: int,\n                     size: int = 1) -> torch.Tensor:\n    \"\"\"Expand the tensor to the target_dims.\"\"\"\n    if tensor.ndim < target_dims:\n        tensor = tensor.view(-1, *([size] * (target_dims - tensor.ndim)))\n    return tensor\n\n\ndef get_dtype_size(dtype: torch.dtype) -> int:\n    \"\"\"Get the size of the data type in bytes.\"\"\"\n    return torch.tensor([], dtype=dtype).element_size()\n\n\ndef merge_dicts(dict1: Dict[K, List[T]],\n                dict2: Dict[K, List[T]]) -> Dict[K, List[T]]:\n    \"\"\"Merge 2 dicts that have key -> List of items.\n\n    When a key conflicts, the values in dict1 is prioritized.\n    \"\"\"\n    merged_dict: Dict[K, List[T]] = defaultdict(list)\n\n    for key, value in dict1.items():\n        merged_dict[key].extend(value)\n\n    for key, value in dict2.items():\n        merged_dict[key].extend(value)\n\n    return dict(merged_dict)\n\n\ndef init_cached_hf_modules() -> None:\n    \"\"\"\n    Lazy initialization of the Hugging Face modules.\n    \"\"\"\n    from transformers.dynamic_module_utils import init_hf_modules\n    init_hf_modules()\n\n\n@lru_cache(maxsize=None)\ndef find_library(lib_name: str) -> str:\n    \"\"\"\n    Find the library file in the system.\n    `lib_name` is full filename, with both prefix and suffix.\n    This function resolves `lib_name` to the full path of the library.\n    \"\"\"\n    # Adapted from https://github.com/openai/triton/blob/main/third_party/nvidia/backend/driver.py#L19 # noqa\n    # According to https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard\n    # `/sbin/ldconfig` should exist in all Linux systems.\n    # `/sbin/ldconfig` searches the library in the system\n    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n    # each line looks like the following:\n    # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n    locs = [line.split()[-1] for line in libs.splitlines() if lib_name in line]\n    # `LD_LIBRARY_PATH` searches the library in the user-defined paths\n    env_ld_library_path = envs.LD_LIBRARY_PATH\n    if not locs and env_ld_library_path:\n        locs = [\n            os.path.join(dir, lib_name)\n            for dir in env_ld_library_path.split(\":\")\n            if os.path.exists(os.path.join(dir, lib_name))\n        ]\n    if not locs:\n        raise ValueError(f\"Cannot find {lib_name} in the system.\")\n    return locs[0]\n\n\ndef find_nccl_library() -> str:\n    \"\"\"\n    We either use the library file specified by the `VLLM_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.VLLM_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable VLLM_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return so_file\n\n\ndef enable_trace_function_call_for_thread() -> None:\n    \"\"\"Set up function tracing for the current thread,\n    if enabled via the VLLM_TRACE_FUNCTION environment variable\n    \"\"\"\n\n    if envs.VLLM_TRACE_FUNCTION:\n        tmp_dir = tempfile.gettempdir()\n        filename = (f\"VLLM_TRACE_FUNCTION_for_process_{os.getpid()}\"\n                    f\"_thread_{threading.get_ident()}_\"\n                    f\"at_{datetime.datetime.now()}.log\").replace(\" \", \"_\")\n        log_path = os.path.join(tmp_dir, \"vllm\", get_vllm_instance_id(),\n                                filename)\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n        enable_trace_function_call(log_path)\n\n\ndef identity(value: T) -> T:\n    return value\n\n\nF = TypeVar('F', bound=Callable[..., Any])\n\n\ndef deprecate_kwargs(\n        *kws: str,\n        is_deprecated: Union[bool, Callable[[], bool]] = True,\n        additional_message: Optional[str] = None) -> Callable[[F], F]:\n    deprecated_kws = set(kws)\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_kwargs = kwargs.keys() & deprecated_kws\n                if deprecated_kwargs:\n                    msg = (\n                        f\"The keyword arguments {deprecated_kwargs} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\n@lru_cache(maxsize=8)\ndef _cuda_device_count_stateless(\n        cuda_visible_devices: Optional[str] = None) -> int:\n    # Note: cuda_visible_devices is not used, but we keep it as an argument for\n    # LRU Cache purposes.\n\n    # Code below is based on\n    # https://github.com/pytorch/pytorch/blob/\n    # c1cd946818442aca8c7f812b16d187ce1586c3bc/\n    # torch/cuda/__init__.py#L831C1-L831C17\n    import torch.cuda\n    import torch.version\n\n    if not torch.cuda._is_compiled():\n        return 0\n    if is_hip():\n        # ROCm uses amdsmi instead of nvml for stateless device count\n        # This requires a sufficiently modern version of Torch 2.4.0\n        raw_count = torch.cuda._device_count_amdsmi() if (hasattr(\n            torch.cuda, \"_device_count_amdsmi\")) else -1\n    else:\n        raw_count = torch.cuda._device_count_nvml()\n    r = torch._C._cuda_getDeviceCount() if raw_count < 0 else raw_count\n    return r\n\n\ndef cuda_device_count_stateless() -> int:\n    \"\"\"Get number of CUDA devices, caching based on the value of\n    CUDA_VISIBLE_DEVICES at the time of call.\n    \n    This should be used instead of torch.cuda.device_count()\n    unless CUDA_VISIBLE_DEVICES has already been set to the desired\n    value.\"\"\"\n\n    # This can be removed and simply replaced with torch.cuda.get_device_count\n    # after https://github.com/pytorch/pytorch/pull/122815 is released.\n    return _cuda_device_count_stateless(envs.CUDA_VISIBLE_DEVICES)\n\n\ndef error_on_invalid_device_count_status():\n    cache_entries = 0\n    with contextlib.suppress(Exception):\n        # future pytorch will fix the issue, device_count will not be cached\n        # at that time, `.cache_info().currsize` will error out\n        cache_entries = torch.cuda.device_count.cache_info().currsize\n    if cache_entries != 0:\n        # the function is already called, and the result is cached\n        remembered = torch.cuda.device_count()\n        current = cuda_device_count_stateless()\n        if remembered > current:\n            raise RuntimeError(\n                \"The number of CUDA devices has changed since the first \"\n                \"call to torch.cuda.device_count(). This is not allowed \"\n                \"and may result in undefined behavior. Please check out \"\n                \"https://github.com/vllm-project/vllm/issues/6056 to \"\n                \"find the first call to torch.cuda.device_count() \"\n                \"and defer it until the engine is up. Or you can set \"\n                \"CUDA_VISIBLE_DEVICES to the GPUs you want to use.\")\n\n\n# NVML utils\n# Note that NVML is not affected by `CUDA_VISIBLE_DEVICES`,\n# all the related functions work on real physical device ids.\n# the major benefit of using NVML is that it will not initialize CUDA\n\ntry:\n    import pynvml\nexcept ImportError:\n    # For non-NV devices\n    pynvml = None\n\n\ndef with_nvml_context(fn):\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        if pynvml is not None:\n            pynvml.nvmlInit()\n        try:\n            return fn(*args, **kwargs)\n        finally:\n            if pynvml is not None:\n                pynvml.nvmlShutdown()\n\n    return wrapper\n\n\n@with_nvml_context\ndef is_full_nvlink(device_ids: List[int]) -> bool:\n    \"\"\"\n    query if the set of gpus are fully connected by nvlink (1 hop)\n    \"\"\"\n    handles = [pynvml.nvmlDeviceGetHandleByIndex(i) for i in device_ids]\n    for i, handle in enumerate(handles):\n        for j, peer_handle in enumerate(handles):\n            if i < j:\n                try:\n                    p2p_status = pynvml.nvmlDeviceGetP2PStatus(\n                        handle, peer_handle, pynvml.NVML_P2P_CAPS_INDEX_NVLINK)\n                    if p2p_status != pynvml.NVML_P2P_STATUS_OK:\n                        return False\n                except pynvml.NVMLError as error:\n                    logger.error(\n                        \"NVLink detection failed. This is normal if your\"\n                        \" machine has no NVLink equipped.\",\n                        exc_info=error)\n                    return False\n    return True\n\n\n#From: https://stackoverflow.com/a/4104188/2749989\ndef run_once(f):\n\n    def wrapper(*args, **kwargs) -> Any:\n        if not wrapper.has_run:  # type: ignore[attr-defined]\n            wrapper.has_run = True  # type: ignore[attr-defined]\n            return f(*args, **kwargs)\n\n    wrapper.has_run = False  # type: ignore[attr-defined]\n    return wrapper\n\n\nclass FlexibleArgumentParser(argparse.ArgumentParser):\n    \"\"\"ArgumentParser that allows both underscore and dash in names.\"\"\"\n\n    def parse_args(self, args=None, namespace=None):\n        if args is None:\n            args = sys.argv[1:]\n\n        # Convert underscores to dashes and vice versa in argument names\n        processed_args = []\n        for arg in args:\n            if arg.startswith('--'):\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    key = '--' + key[len('--'):].replace('_', '-')\n                    processed_args.append(f'{key}={value}')\n                else:\n                    processed_args.append('--' +\n                                          arg[len('--'):].replace('_', '-'))\n            else:\n                processed_args.append(arg)\n\n        return super().parse_args(processed_args, namespace)\n\n\nasync def _run_task_with_lock(task: Callable, lock: asyncio.Lock, *args,\n                              **kwargs):\n    \"\"\"Utility function to run async task in a lock\"\"\"\n    async with lock:\n        return await task(*args, **kwargs)\n",
      "diff": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex f3025a68d..f906d8258 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -415,9 +415,10 @@ def init_kmp_env():\n     os.environ['KMP_REDUCTION_BARRIER_PATTERN'] = \"dist,dist\"\n \n \n-def chunk_list(lst: List[T], chunk_size: int) -> List[List[T]]:\n+def chunk_list(lst: List[T], chunk_size: int):\n     \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n-    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n+    for i in range(0, len(lst), chunk_size):\n+        yield lst[i:i + chunk_size]\n \n \n def cdiv(a: int, b: int) -> int:",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 7,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 7
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_block_manager_v2, test_cpu_gpu_block_allocator, test_block_table, test_prefix_caching_block, test_sequence)",
    "is_benchmark_actually_there": "",
    "sample_clues": "__init__, block_table, blocks"
  }
}