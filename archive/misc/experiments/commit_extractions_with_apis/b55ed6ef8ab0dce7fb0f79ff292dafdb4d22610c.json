{
  "commit_hash": "b55ed6ef8ab0dce7fb0f79ff292dafdb4d22610c",
  "parent_hash": "2f385183f35497e030ef22c9820d83b83bc4f6db",
  "message": "[V1][Minor] Optimize token_ids_cpu copy (#11692)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2025-01-02 12:04:58 -0700",
  "files_changed": [
    {
      "file_path": "vllm/v1/worker/gpu_input_batch.py",
      "old_content": "# Datastructures defining an input batch\n\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Set\n\nimport numpy as np\nimport torch\n\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.v1.sample.metadata import SamplingMetadata\n\nif TYPE_CHECKING:\n    from vllm.multimodal.inputs import PlaceholderRange\n\n\n@dataclass\nclass CachedRequestState:\n\n    req_id: str\n    prompt_token_ids: List[int]\n    prompt: Optional[str]\n    mm_inputs: List[MultiModalKwargs]\n    mm_positions: List[\"PlaceholderRange\"]\n    sampling_params: SamplingParams\n    generator: Optional[torch.Generator]\n\n    block_ids: List[int]\n    num_computed_tokens: int\n    output_token_ids: List[int]\n\n    @property\n    def num_tokens(self) -> int:\n        return len(self.prompt_token_ids) + len(self.output_token_ids)\n\n\nclass InputBatch:\n\n    def __init__(\n        self,\n        max_num_reqs: int,\n        max_model_len: int,\n        max_num_blocks_per_req: int,\n        device: torch.device,\n        pin_memory: bool,\n        vocab_size: int,\n    ):\n        self.max_num_reqs = max_num_reqs\n        self.max_model_len = max_model_len\n        self.max_num_blocks_per_req = max_num_blocks_per_req\n        self.device = device\n        self.pin_memory = pin_memory\n        self.vocab_size = vocab_size\n\n        self.req_ids: List[Optional[str]] = [None] * max_num_reqs\n        self.req_id_to_index: Dict[str, int] = {}\n\n        # TODO(woosuk): This buffer could be too large if max_model_len is big.\n        # Find a way to reduce the CPU memory usage.\n        # This buffer is not directly transferred to the GPU, so it does not\n        # need to be pinned.\n        self.token_ids_cpu_tensor = torch.zeros(\n            (max_num_reqs, max_model_len),\n            device=\"cpu\",\n            dtype=torch.int32,\n            pin_memory=False,\n        )\n        self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n        self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n\n        # Attention-related.\n        self.block_table = torch.zeros(\n            (max_num_reqs, max_num_blocks_per_req),\n            device=self.device,\n            dtype=torch.int32,\n        )\n        self.block_table_cpu_tensor = torch.zeros(\n            (max_num_reqs, max_num_blocks_per_req),\n            device=\"cpu\",\n            dtype=torch.int32,\n            pin_memory=pin_memory,\n        )\n        self.block_table_cpu = self.block_table_cpu_tensor.numpy()\n\n        # Sampling-related.\n        self.temperature = torch.empty((max_num_reqs, ),\n                                       dtype=torch.float32,\n                                       device=device)\n        self.temperature_cpu_tensor = torch.empty((max_num_reqs, ),\n                                                  dtype=torch.float32,\n                                                  device=\"cpu\",\n                                                  pin_memory=pin_memory)\n        self.temperature_cpu = self.temperature_cpu_tensor.numpy()\n        self.greedy_reqs: Set[str] = set()\n        self.random_reqs: Set[str] = set()\n\n        self.top_p = torch.empty((max_num_reqs, ),\n                                 dtype=torch.float32,\n                                 device=device)\n        self.top_p_cpu_tensor = torch.empty((max_num_reqs, ),\n                                            dtype=torch.float32,\n                                            device=\"cpu\",\n                                            pin_memory=pin_memory)\n        self.top_p_cpu = self.top_p_cpu_tensor.numpy()\n        self.top_p_reqs: Set[str] = set()\n\n        self.top_k = torch.empty((max_num_reqs, ),\n                                 dtype=torch.int32,\n                                 device=device)\n        self.top_k_cpu_tensor = torch.empty((max_num_reqs, ),\n                                            dtype=torch.int32,\n                                            device=\"cpu\",\n                                            pin_memory=pin_memory)\n        self.top_k_cpu = self.top_k_cpu_tensor.numpy()\n        self.top_k_reqs: Set[str] = set()\n\n        # Frequency penalty related data structures\n        self.frequency_penalties = torch.empty((max_num_reqs, ),\n                                               dtype=torch.float,\n                                               device=device)\n        self.frequency_penalties_cpu_tensor = torch.empty(\n            (max_num_reqs, ),\n            dtype=torch.float,\n            device=\"cpu\",\n            pin_memory=pin_memory)\n        self.frequency_penalties_cpu = \\\n            self.frequency_penalties_cpu_tensor.numpy()\n        self.frequency_penalties_reqs: Set[str] = set()\n\n        # Presence penalty related data structures\n        self.presence_penalties = torch.empty((max_num_reqs, ),\n                                              dtype=torch.float,\n                                              device=device)\n        self.presence_penalties_cpu_tensor = torch.empty((max_num_reqs, ),\n                                                         dtype=torch.float,\n                                                         device=\"cpu\",\n                                                         pin_memory=pin_memory)\n        self.presence_penalties_cpu = \\\n            self.presence_penalties_cpu_tensor.numpy()\n        self.presence_penalties_reqs: Set[str] = set()\n\n        # Repetition penalty related data structures\n        self.repetition_penalties = torch.empty((max_num_reqs, ),\n                                                dtype=torch.float,\n                                                device=device)\n        self.repetition_penalties_cpu_tensor = torch.empty(\n            (max_num_reqs, ),\n            dtype=torch.float,\n            device=\"cpu\",\n            pin_memory=pin_memory)\n        self.repetition_penalties_cpu = \\\n            self.repetition_penalties_cpu_tensor.numpy()\n        self.repetition_penalties_reqs: Set[str] = set()\n\n        self.min_tokens: List[int] = [0] * max_num_reqs\n        self.stop_token_ids: List[Set[int]] = [\n            set() for _ in range(max_num_reqs)\n        ]\n        self.prompt_token_ids: Optional[torch.Tensor] = None\n\n        # req_index -> generator\n        # NOTE(woosuk): The indices of the requests that do not have their own\n        # generator should not be included in the dictionary.\n        self.generators: Dict[int, torch.Generator] = {}\n\n        self.num_logprobs: Dict[str, int] = {}\n        self.prompt_logprob_reqs: Set[str] = set()\n\n    def add_request(\n        self,\n        request: \"CachedRequestState\",\n        req_index: Optional[int] = None,\n    ) -> None:\n        if req_index is None:\n            req_index = self.num_reqs\n        assert req_index < self.max_num_reqs\n\n        req_id = request.req_id\n        self.req_ids[req_index] = req_id\n        self.req_id_to_index[req_id] = req_index\n\n        # Copy the prompt token ids and output token ids.\n        num_prompt_tokens = len(request.prompt_token_ids)\n        self.num_prompt_tokens[req_index] = num_prompt_tokens\n        self.token_ids_cpu[\n            req_index, :num_prompt_tokens] = request.prompt_token_ids\n        start_idx = num_prompt_tokens\n        end_idx = start_idx + len(request.output_token_ids)\n        self.token_ids_cpu[req_index,\n                           start_idx:end_idx] = request.output_token_ids\n\n        self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens\n        num_blocks = len(request.block_ids)\n        self.block_table_cpu[req_index, :num_blocks] = request.block_ids\n\n        sampling_params = request.sampling_params\n        self.temperature_cpu[req_index] = sampling_params.temperature\n        if sampling_params.sampling_type == SamplingType.GREEDY:\n            self.greedy_reqs.add(req_id)\n        else:\n            self.random_reqs.add(req_id)\n\n        self.top_p_cpu[req_index] = sampling_params.top_p\n        if sampling_params.top_p < 1:\n            self.top_p_reqs.add(req_id)\n        self.top_k_cpu[req_index] = sampling_params.top_k\n        if sampling_params.top_k > 0:\n            self.top_k_reqs.add(req_id)\n        self.frequency_penalties_cpu[req_index] = \\\n            sampling_params.frequency_penalty\n        if sampling_params.frequency_penalty != 0.0:\n            self.frequency_penalties_reqs.add(req_id)\n        self.presence_penalties_cpu[req_index] = \\\n            sampling_params.presence_penalty\n        if sampling_params.presence_penalty != 0.0:\n            self.presence_penalties_reqs.add(req_id)\n        self.repetition_penalties_cpu[req_index] = \\\n            sampling_params.repetition_penalty\n        if sampling_params.repetition_penalty != 1.0:\n            self.repetition_penalties_reqs.add(req_id)\n        self.min_tokens[req_index] = sampling_params.min_tokens\n        self.stop_token_ids[req_index] = sampling_params.all_stop_token_ids\n\n        # NOTE(woosuk): self.generators should not include the requests that\n        # do not have their own generator.\n        if request.generator is not None:\n            self.generators[req_index] = request.generator\n\n        num_logprobs = sampling_params.logprobs\n        if num_logprobs is not None and num_logprobs > 0:\n            self.num_logprobs[req_id] = num_logprobs\n        if sampling_params.prompt_logprobs:\n            self.prompt_logprob_reqs.add(req_id)\n\n    def remove_request(self, req_id: str) -> Optional[int]:\n        req_index = self.req_id_to_index.pop(req_id, None)\n        if req_index is None:\n            return None\n        self.req_ids[req_index] = None\n\n        self.greedy_reqs.discard(req_id)\n        self.random_reqs.discard(req_id)\n        self.top_p_reqs.discard(req_id)\n        self.top_k_reqs.discard(req_id)\n        self.frequency_penalties_reqs.discard(req_id)\n        self.presence_penalties_reqs.discard(req_id)\n        self.repetition_penalties_reqs.discard(req_id)\n        self.generators.pop(req_index, None)\n        self.num_logprobs.pop(req_id, None)\n        self.prompt_logprob_reqs.discard(req_id)\n        return req_index\n\n    def clear(self) -> None:\n        self.req_ids = [None] * self.max_num_reqs\n        self.req_id_to_index.clear()\n        self.greedy_reqs.clear()\n        self.random_reqs.clear()\n        self.top_p_reqs.clear()\n        self.top_k_reqs.clear()\n        self.frequency_penalties_reqs.clear()\n        self.presence_penalties_reqs.clear()\n        self.repetition_penalties_reqs.clear()\n        self.generators.clear()\n        self.num_logprobs.clear()\n        self.prompt_logprob_reqs.clear()\n\n    def condense(self, empty_req_indices: List[int]) -> None:\n        if self.num_reqs == 0:\n            # The batched states are empty.\n            return\n\n        # NOTE(woosuk): This function assumes that the empty_req_indices\n        # is sorted in descending order.\n        last_req_index = self.num_reqs + len(empty_req_indices) - 1\n        while empty_req_indices:\n            # Find the largest non-empty index.\n            while last_req_index in empty_req_indices:\n                last_req_index -= 1\n\n            # Find the smallest empty index.\n            empty_index = empty_req_indices.pop()\n            if empty_index >= last_req_index:\n                break\n\n            # Swap the states.\n            req_id = self.req_ids[last_req_index]\n            assert req_id is not None\n            self.req_ids[empty_index] = req_id\n            self.req_ids[last_req_index] = None\n            self.req_id_to_index[req_id] = empty_index\n\n            # TODO(woosuk): Optimize the copy of token_ids_cpu and\n            # block_table_cpu.\n            self.token_ids_cpu[empty_index] = self.token_ids_cpu[\n                last_req_index]\n            self.num_prompt_tokens[empty_index] = \\\n                self.num_prompt_tokens[last_req_index]\n            self.num_computed_tokens_cpu[\n                empty_index] = self.num_computed_tokens_cpu[last_req_index]\n            self.block_table_cpu[empty_index] = self.block_table_cpu[\n                last_req_index]\n            self.temperature_cpu[empty_index] = self.temperature_cpu[\n                last_req_index]\n            self.top_p_cpu[empty_index] = self.top_p_cpu[last_req_index]\n            self.top_k_cpu[empty_index] = self.top_k_cpu[last_req_index]\n            self.frequency_penalties_cpu[empty_index] = \\\n                self.frequency_penalties_cpu[last_req_index]\n            self.presence_penalties_cpu[empty_index] = \\\n                self.presence_penalties_cpu[last_req_index]\n            self.repetition_penalties_cpu[empty_index] = \\\n                self.repetition_penalties_cpu[last_req_index]\n            self.min_tokens[empty_index] = self.min_tokens[last_req_index]\n            self.stop_token_ids[empty_index] = \\\n                self.stop_token_ids[last_req_index]\n            generator = self.generators.pop(last_req_index, None)\n            if generator is not None:\n                self.generators[empty_index] = generator\n\n            # Decrement last_req_index since it is now empty.\n            last_req_index -= 1\n\n    def make_sampling_metadata(\n        self,\n        req_id_output_token_ids: Dict[str, List[int]],\n        skip_copy: bool = False,\n    ) -> SamplingMetadata:\n        if not skip_copy:\n            self.temperature[:self.num_reqs].copy_(\n                self.temperature_cpu_tensor[:self.num_reqs], non_blocking=True)\n            self.top_p[:self.num_reqs].copy_(\n                self.top_p_cpu_tensor[:self.num_reqs], non_blocking=True)\n            self.top_k[:self.num_reqs].copy_(\n                self.top_k_cpu_tensor[:self.num_reqs], non_blocking=True)\n            if not self.no_penalties:\n                # Since syncing these tensors is expensive only copy them\n                # if necessary i.e. if there are requests which require\n                # penalties to be applied during sampling.\n                self.frequency_penalties[:self.num_reqs].copy_(\n                    self.frequency_penalties_cpu_tensor[:self.num_reqs],\n                    non_blocking=True)\n                self.presence_penalties[:self.num_reqs].copy_(\n                    self.presence_penalties_cpu_tensor[:self.num_reqs],\n                    non_blocking=True)\n                self.repetition_penalties[:self.num_reqs].copy_(\n                    self.repetition_penalties_cpu_tensor[:self.num_reqs],\n                    non_blocking=True)\n                # The prompt tokens are used only for applying penalties during\n                # the sampling process. Hence copy these tensors only when\n                # there are requests which need penalties to be applied.\n                self.prompt_token_ids = self._make_prompt_token_ids_tensor()\n\n        output_token_ids: List[List[int]] = []\n\n        for req_id in self.req_ids[:self.num_reqs]:\n            assert req_id is not None\n            # Currently we create a tensor for output_token_ids from scratch\n            # at each step. However, for the penalties computation what we\n            # need is stats about the token ids present in the output. This\n            # stats can be maintained incrementally instead of computing it\n            # from scratch at each step.\n            # TODO - Replace this with incremental update to output token\n            # statistics.\n            output_token_ids.append(req_id_output_token_ids[req_id])\n\n        return SamplingMetadata(\n            temperature=self.temperature[:self.num_reqs],\n            all_greedy=self.all_greedy,\n            all_random=self.all_random,\n            top_p=self.top_p[:self.num_reqs],\n            top_k=self.top_k[:self.num_reqs],\n            no_top_p=self.no_top_p,\n            no_top_k=self.no_top_k,\n            generators=self.generators,\n            max_num_logprobs=self.max_num_logprobs,\n            prompt_token_ids=self.prompt_token_ids,\n            frequency_penalties=self.frequency_penalties[:self.num_reqs],\n            presence_penalties=self.presence_penalties[:self.num_reqs],\n            repetition_penalties=self.repetition_penalties[:self.num_reqs],\n            output_token_ids=output_token_ids,\n            min_tokens=self.min_tokens[:self.num_reqs],\n            stop_token_ids=self.stop_token_ids[:self.num_reqs],\n            no_penalties=self.no_penalties,\n        )\n\n    def _make_prompt_token_ids_tensor(self) -> torch.Tensor:\n        max_prompt_len = self.num_prompt_tokens[:self.num_reqs].max()\n        prompt_token_ids_cpu_tensor = torch.empty(\n            (self.num_reqs, max_prompt_len),\n            device=\"cpu\",\n            dtype=torch.int64,\n            pin_memory=self.pin_memory)\n        prompt_token_ids = prompt_token_ids_cpu_tensor.numpy()\n        prompt_token_ids[:] = (\n            self.token_ids_cpu[:self.num_reqs, :max_prompt_len])\n        # Use the value of vocab_size as a pad since we don't have a\n        # token_id of this value.\n        for i in range(self.num_reqs):\n            prompt_token_ids[i, self.num_prompt_tokens[i]:] = self.vocab_size\n        return prompt_token_ids_cpu_tensor.to(device=self.device,\n                                              non_blocking=True)\n\n    @property\n    def num_reqs(self) -> int:\n        return len(self.req_id_to_index)\n\n    @property\n    def all_greedy(self) -> bool:\n        return len(self.random_reqs) == 0\n\n    @property\n    def all_random(self) -> bool:\n        return len(self.greedy_reqs) == 0\n\n    @property\n    def no_top_p(self) -> bool:\n        return len(self.top_p_reqs) == 0\n\n    @property\n    def no_top_k(self) -> bool:\n        return len(self.top_k_reqs) == 0\n\n    @property\n    def no_penalties(self) -> bool:\n        return (len(self.presence_penalties_reqs) == 0\n                and len(self.frequency_penalties_reqs) == 0\n                and len(self.repetition_penalties_reqs) == 0)\n\n    @property\n    def max_num_logprobs(self) -> int:\n        return max(self.num_logprobs.values()) if self.num_logprobs else 0\n\n    @property\n    def no_logprob(self) -> bool:\n        return len(self.num_logprobs) == 0\n\n    @property\n    def no_prompt_logprob(self) -> bool:\n        return len(self.prompt_logprob_reqs) == 0\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_input_batch.py b/vllm/v1/worker/gpu_input_batch.py\nindex e79145300..f8a1427c6 100644\n--- a/vllm/v1/worker/gpu_input_batch.py\n+++ b/vllm/v1/worker/gpu_input_batch.py\n@@ -66,8 +66,9 @@ class InputBatch:\n             pin_memory=False,\n         )\n         self.token_ids_cpu = self.token_ids_cpu_tensor.numpy()\n-        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n+        self.num_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n         self.num_prompt_tokens = np.zeros(max_num_reqs, dtype=np.int32)\n+        self.num_computed_tokens_cpu = np.empty(max_num_reqs, dtype=np.int32)\n \n         # Attention-related.\n         self.block_table = torch.zeros(\n@@ -189,6 +190,7 @@ class InputBatch:\n         end_idx = start_idx + len(request.output_token_ids)\n         self.token_ids_cpu[req_index,\n                            start_idx:end_idx] = request.output_token_ids\n+        self.num_tokens[req_index] = request.num_tokens\n \n         self.num_computed_tokens_cpu[req_index] = request.num_computed_tokens\n         num_blocks = len(request.block_ids)\n@@ -290,14 +292,15 @@ class InputBatch:\n             self.req_ids[last_req_index] = None\n             self.req_id_to_index[req_id] = empty_index\n \n-            # TODO(woosuk): Optimize the copy of token_ids_cpu and\n-            # block_table_cpu.\n-            self.token_ids_cpu[empty_index] = self.token_ids_cpu[\n-                last_req_index]\n+            num_tokens = self.num_tokens[last_req_index]\n+            self.token_ids_cpu[empty_index, :num_tokens] = self.token_ids_cpu[\n+                last_req_index, :num_tokens]\n+            self.num_tokens[empty_index] = num_tokens\n             self.num_prompt_tokens[empty_index] = \\\n                 self.num_prompt_tokens[last_req_index]\n             self.num_computed_tokens_cpu[\n                 empty_index] = self.num_computed_tokens_cpu[last_req_index]\n+            # TODO(woosuk): Optimize the copy of block_table_cpu.\n             self.block_table_cpu[empty_index] = self.block_table_cpu[\n                 last_req_index]\n             self.temperature_cpu[empty_index] = self.temperature_cpu[",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/v1/worker/gpu_model_runner.py",
      "old_content": "import gc\nimport time\nfrom typing import TYPE_CHECKING, Dict, List, Tuple, cast\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nfrom vllm.config import CompilationLevel, VllmConfig\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.forward_context import set_forward_context\nfrom vllm.inputs import INPUT_REGISTRY\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalKwargs\nfrom vllm.sampling_params import SamplingType\nfrom vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,\n                        LayerBlockType, cdiv, is_pin_memory_available)\nfrom vllm.v1.attention.backends.flash_attn import (FlashAttentionBackend,\n                                                   FlashAttentionMetadata)\nfrom vllm.v1.engine.mm_input_mapper import MMHasher, MMInputMapperClient\nfrom vllm.v1.outputs import ModelRunnerOutput\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch\n\nif TYPE_CHECKING:\n    from vllm.v1.core.scheduler import SchedulerOutput\n\nlogger = init_logger(__name__)\n\n\nclass GPUModelRunner:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        device: torch.device,\n    ):\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.load_config = vllm_config.load_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.speculative_config = vllm_config.speculative_config\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config\n        self.observability_config = vllm_config.observability_config\n\n        model_config = self.model_config\n        cache_config = self.cache_config\n        scheduler_config = self.scheduler_config\n        parallel_config = self.parallel_config\n        self.device = device\n        self.pin_memory = is_pin_memory_available()\n        self.dtype = self.model_config.dtype\n        if cache_config.cache_dtype == \"auto\":\n            self.kv_cache_dtype = self.dtype\n        else:\n            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[\n                cache_config.cache_dtype]\n\n        self.is_multimodal_model = model_config.is_multimodal_model\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_model_len = model_config.max_model_len\n        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n        self.max_num_tokens = scheduler_config.max_num_batched_tokens\n        self.max_num_reqs = scheduler_config.max_num_seqs\n\n        # Model-related.\n        self.num_attn_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n        self.num_query_heads = model_config.get_num_attention_heads(\n            parallel_config)\n        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\n        self.head_size = model_config.get_head_size()\n        self.hidden_size = model_config.get_hidden_size()\n\n        # Multi-modal data support\n        self.input_registry = INPUT_REGISTRY\n        self.mm_registry = MULTIMODAL_REGISTRY\n\n        # NOTE: mm_input_mapper_client and mm_hasher are only used for memory\n        # profiling.\n        self.mm_input_mapper_client = MMInputMapperClient(self.model_config)\n        self.mm_hasher = MMHasher()\n        self.use_hash = (not model_config.disable_mm_preprocessor_cache) or \\\n            cache_config.enable_prefix_caching\n\n        self.max_num_encoder_input_tokens = self.scheduler_config.max_num_encoder_input_tokens  # noqa: E501\n        self.encoder_cache_size = self.scheduler_config.encoder_cache_size\n\n        # Lazy initialization\n        # self.model: nn.Module  # Set after load_model\n        self.kv_caches: List[torch.Tensor] = []\n        # req_id -> (input_id -> encoder_output)\n        self.encoder_cache: Dict[str, Dict[int, torch.Tensor]] = {}\n\n        # Request states.\n        self.requests: Dict[str, CachedRequestState] = {}\n        # Persistent batch.\n        self.input_batch = InputBatch(\n            max_num_reqs=self.max_num_reqs,\n            max_model_len=self.max_model_len,\n            max_num_blocks_per_req=self.max_num_blocks_per_req,\n            device=self.device,\n            pin_memory=self.pin_memory,\n            vocab_size=model_config.get_vocab_size(),\n        )\n\n        self.use_cuda_graph = (self.vllm_config.compilation_config.level\n                               == CompilationLevel.PIECEWISE\n                               and not self.model_config.enforce_eager)\n        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.\n        # The convention is different.\n        # self.cudagraph_batch_sizes sorts in ascending order.\n        # The batch sizes in the config are in descending order.\n        self.cudagraph_batch_sizes = list(\n            reversed(self.vllm_config.compilation_config.capture_sizes))\n\n        # Cache the device properties.\n        self.device_properties = torch.cuda.get_device_properties(self.device)\n        self.num_sms = self.device_properties.multi_processor_count\n\n        # Persistent buffers for CUDA graphs.\n        self.input_ids = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int32,\n                                     device=self.device)\n        self.positions = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int64,\n                                     device=self.device)\n        self.inputs_embeds = torch.zeros(\n            (self.max_num_tokens, self.hidden_size),\n            dtype=self.dtype,\n            device=self.device)\n\n        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n        self.arange_np = np.arange(max(self.max_num_reqs + 1,\n                                       self.max_model_len),\n                                   dtype=np.int32)\n        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n        # a faster version of creating a new tensor every time. Thus, we should\n        # not make any assumptions about the values in these tensors.\n        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.input_ids_np = self.input_ids_cpu.numpy()\n        self.positions_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.positions_np = self.positions_cpu.numpy()\n        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n                                            dtype=torch.int32,\n                                            device=\"cpu\",\n                                            pin_memory=self.pin_memory)\n        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n                                               dtype=torch.int32,\n                                               device=\"cpu\",\n                                               pin_memory=self.pin_memory)\n        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n\n    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n        # Remove stopped requests from the cached states.\n        # Keep the states of the pre-empted requests.\n        for req_id in scheduler_output.finished_req_ids:\n            self.requests.pop(req_id, None)\n            self.encoder_cache.pop(req_id, None)\n\n        # Free the cached encoder outputs.\n        for req_id, input_id in scheduler_output.free_encoder_input_ids:\n            encoder_outputs = self.encoder_cache.get(req_id)\n            if encoder_outputs is not None:\n                encoder_outputs.pop(input_id, None)\n                if not encoder_outputs:\n                    self.encoder_cache.pop(req_id, None)\n\n        # Remove the requests from the persistent batch.\n        stopped_req_ids = set().union(\n            scheduler_output.preempted_req_ids,\n            scheduler_output.finished_req_ids,\n        )\n        removed_req_indices: List[int] = []\n        for req_id in stopped_req_ids:\n            req_index = self.input_batch.remove_request(req_id)\n            if req_index is not None:\n                removed_req_indices.append(req_index)\n\n        # Update the states of the running requests.\n        for req_data in scheduler_output.scheduled_running_reqs:\n            req_id = req_data.req_id\n            req_state = self.requests[req_id]\n            req_index = self.input_batch.req_id_to_index[req_id]\n\n            # Update the num_computed_tokens.\n            req_state.num_computed_tokens = req_data.num_computed_tokens\n            self.input_batch.num_computed_tokens_cpu[req_index] = (\n                req_data.num_computed_tokens)\n\n            # Update the block table.\n            num_new_blocks = len(req_data.new_block_ids)\n            if num_new_blocks == 0:\n                continue\n            start_index = len(req_state.block_ids)\n            end_index = start_index + num_new_blocks\n            req_state.block_ids.extend(req_data.new_block_ids)\n            self.input_batch.block_table_cpu[\n                req_index, start_index:end_index] = req_data.new_block_ids\n\n        req_ids_to_add: List[str] = []\n        # Add new requests to the cached states.\n        for new_req_data in scheduler_output.scheduled_new_reqs:\n            req_id = new_req_data.req_id\n            sampling_params = new_req_data.sampling_params\n            if sampling_params.sampling_type == SamplingType.RANDOM_SEED:\n                generator = torch.Generator(device=self.device)\n                generator.manual_seed(sampling_params.seed)\n            else:\n                generator = None\n\n            self.requests[req_id] = CachedRequestState(\n                req_id=req_id,\n                prompt_token_ids=new_req_data.prompt_token_ids,\n                prompt=new_req_data.prompt,\n                mm_inputs=new_req_data.mm_inputs,\n                mm_positions=new_req_data.mm_positions,\n                sampling_params=sampling_params,\n                generator=generator,\n                block_ids=new_req_data.block_ids,\n                num_computed_tokens=new_req_data.num_computed_tokens,\n                output_token_ids=[],\n            )\n            req_ids_to_add.append(req_id)\n\n        # Update the cached states of the resumed requests.\n        for res_req_data in scheduler_output.scheduled_resumed_reqs:\n            req_id = res_req_data.req_id\n            req_state = self.requests[req_id]\n\n            req_state.block_ids = res_req_data.block_ids\n            req_state.num_computed_tokens = res_req_data.num_computed_tokens\n            req_ids_to_add.append(req_id)\n\n        # Add the new or resumed requests to the persistent batch.\n        # The smaller empty indices are filled first.\n        removed_req_indices = sorted(removed_req_indices, reverse=True)\n        for req_id in req_ids_to_add:\n            req_state = self.requests[req_id]\n            if removed_req_indices:\n                # Fill the empty index.\n                req_index = removed_req_indices.pop()\n            else:\n                # Append to the end.\n                req_index = None\n            self.input_batch.add_request(req_state, req_index)\n\n        # Condense the batched states if there are empty indices.\n        if removed_req_indices:\n            self.input_batch.condense(removed_req_indices)\n\n    def _prepare_inputs(self, scheduler_output: \"SchedulerOutput\"):\n        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        assert total_num_scheduled_tokens > 0\n        num_reqs = self.input_batch.num_reqs\n        assert num_reqs > 0\n\n        # OPTIMIZATION: Start copying the block table first.\n        # This way, we can overlap the copy with the following CPU operations.\n        self.input_batch.block_table[:num_reqs].copy_(\n            self.input_batch.block_table_cpu_tensor[:num_reqs],\n            non_blocking=True)\n\n        # Get the number of scheduled tokens for each request.\n        # TODO: The Python loop can be slow. Optimize.\n        num_scheduled_tokens = []\n        max_num_scheduled_tokens = 0\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_scheduled_tokens.append(num_tokens)\n            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n                                           num_tokens)\n        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n        assert max_num_scheduled_tokens > 0\n\n        # Get request indices.\n        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n        req_indices = np.repeat(self.arange_np[:num_reqs],\n                                num_scheduled_tokens)\n\n        # Get batched arange.\n        # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        arange = np.concatenate(\n            [self.arange_np[:n] for n in num_scheduled_tokens])\n\n        # Get positions.\n        positions_np = self.positions_np[:total_num_scheduled_tokens]\n        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n               arange,\n               out=positions_np)\n\n        # Get token indices.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]\n        # where M is the max_model_len.\n        token_indices = (positions_np +\n                         req_indices * self.input_batch.token_ids_cpu.shape[1])\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                           0,\n                           torch.from_numpy(token_indices),\n                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n\n        # Calculate the slot mapping.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]\n        # where K is the max_num_blocks_per_req and the block size is 2.\n        # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n        # because M (max_model_len) is not necessarily divisible by block_size.\n        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n                               positions_np // self.block_size)\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n                         [block_table_indices].numpy())\n        block_offsets = positions_np % self.block_size\n        np.add(block_numbers * self.block_size,\n               block_offsets,\n               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n\n        # Prepare the attention metadata.\n        self.query_start_loc_np[0] = 0\n        np.cumsum(num_scheduled_tokens,\n                  out=self.query_start_loc_np[1:num_reqs + 1])\n\n        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                    num_scheduled_tokens)\n        max_seq_len = seq_lens.max()\n        self.seq_start_loc_np[0] = 0\n        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])\n\n        # Copy the tensors to the GPU.\n        self.input_ids[:total_num_scheduled_tokens].copy_(\n            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n        self.positions[:total_num_scheduled_tokens].copy_(\n            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n            self.device, non_blocking=True)\n        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n            self.device, non_blocking=True)\n        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n            self.device, non_blocking=True).long()\n\n        # Prepare for cascade attention if needed.\n        common_prefix_len = (scheduler_output.num_common_prefix_blocks *\n                             self.block_size)\n        if common_prefix_len == 0:\n            # Common case.\n            use_cascade = False\n        else:\n            # NOTE(woosuk): Cascade attention uses two attention kernels: one\n            # for the common prefix and the other for the rest. For the first\n            # kernel, we concatenate all the query tokens (possibly from\n            # different requests) and treat them as if they are from the same\n            # request. Then, we use bi-directional attention to process the\n            # common prefix in the KV cache. Importantly, this means that the\n            # first kernel does not do any masking.\n\n            # Consider the following example:\n            # Request 1's input query: [D, E, X]\n            # Request 1's kv cache: [A, B, C, D, E, X]\n            # Request 1's num_computed_tokens: 3 (i.e., [A, B, C])\n            # Request 2's input query: [E, Y]\n            # Request 2's kv cache: [A, B, C, D, E, Y]\n            # Request 2's num_computed_tokens: 4 (i.e., [A, B, C, D])\n\n            # If we use [A, B, C, D, E] as the common prefix, then the\n            # first kernel will compute the bi-directional attention between\n            # input query [D, E, X, E, Y] and common prefix [A, B, C, D, E].\n            # However, this is wrong because D in Request 1 should not attend to\n            # E in the common prefix (i.e., we need masking).\n            # To avoid this, [A, B, C, D] should be the common prefix.\n            # That is, the common prefix should be capped by the minimum\n            # num_computed_tokens among the requests, and plus one to include\n            # the first token of the query.\n\n            # In practice, we use [A, B, C] as the common prefix, instead of\n            # [A, B, C, D] (i.e., the common prefix is capped by the minimum\n            # num_computed_tokens, without plus one).\n            # This is because of an implementation detail: We want to always\n            # use two kernels for cascade attention. Let's imagine:\n            # Request 3's input query: [D]\n            # Request 3's kv cache: [A, B, C, D]\n            # Request 3's num_computed_tokens: 4 (i.e., [A, B, C, D])\n            # If we use [A, B, C, D] as the common prefix for Request 1-3,\n            # then Request 3 will be processed only by the first kernel,\n            # and the second kernel will get an empty input. While this is not\n            # a fundamental problem, our current implementation does not support\n            # this case.\n            common_prefix_len = min(\n                common_prefix_len,\n                self.input_batch.num_computed_tokens_cpu[:num_reqs].min())\n            # common_prefix_len should be a multiple of the block size.\n            common_prefix_len = (common_prefix_len // self.block_size *\n                                 self.block_size)\n            use_cascade = FlashAttentionBackend.use_cascade_attention(\n                common_prefix_len=common_prefix_len,\n                query_lens=num_scheduled_tokens,\n                num_query_heads=self.num_query_heads,\n                num_kv_heads=self.num_kv_heads,\n                use_alibi=False,  # FIXME\n                use_sliding_window=self.sliding_window is not None,\n                num_sms=self.num_sms,\n            )\n\n        if use_cascade:\n            # TODO: Optimize.\n            cu_prefix_query_lens = torch.tensor(\n                [0, total_num_scheduled_tokens],\n                dtype=torch.int32,\n                device=self.device)\n            cu_prefix_kv_lens = torch.tensor([0, common_prefix_len],\n                                             dtype=torch.int32,\n                                             device=self.device)\n            cu_suffix_kv_lens = (\n                self.seq_start_loc_np[:num_reqs + 1] -\n                self.arange_np[:num_reqs + 1] * common_prefix_len)\n            cu_suffix_kv_lens = torch.from_numpy(cu_suffix_kv_lens).to(\n                self.device)\n        else:\n            cu_prefix_query_lens = None\n            cu_prefix_kv_lens = None\n            cu_suffix_kv_lens = None\n\n        attn_metadata = FlashAttentionMetadata(\n            num_actual_tokens=total_num_scheduled_tokens,\n            max_query_len=max_num_scheduled_tokens,\n            query_start_loc=query_start_loc,\n            max_seq_len=max_seq_len,\n            seq_start_loc=seq_start_loc,\n            block_table=self.input_batch.block_table[:num_reqs],\n            slot_mapping=slot_mapping,\n            use_cascade=use_cascade,\n            common_prefix_len=common_prefix_len,\n            cu_prefix_query_lens=cu_prefix_query_lens,\n            cu_prefix_kv_lens=cu_prefix_kv_lens,\n            cu_suffix_kv_lens=cu_suffix_kv_lens,\n        )\n        # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial\n        # request in the batch. While we should not sample any token from this\n        # partial request, we do so for simplicity. We will ignore the sampled\n        # token from the partial request.\n        # TODO: Support prompt logprobs.\n        logits_indices = query_start_loc[1:] - 1\n        return attn_metadata, logits_indices\n\n    def _prepare_sampling(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> SamplingMetadata:\n        skip_copy = True\n        if (scheduler_output.finished_req_ids\n                or scheduler_output.preempted_req_ids):\n            skip_copy = False\n        if (scheduler_output.scheduled_new_reqs\n                or scheduler_output.scheduled_resumed_reqs):\n            skip_copy = False\n        # Create the sampling metadata.\n        req_id_output_token_ids: Dict[str, List[int]] = \\\n            {req_id: req.output_token_ids \\\n                for req_id, req in self.requests.items()}\n\n        sampling_metadata = self.input_batch.make_sampling_metadata(\n            req_id_output_token_ids, skip_copy)\n        return sampling_metadata\n\n    def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n        scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n        if not scheduled_encoder_inputs:\n            return\n\n        # Batch the multi-modal inputs.\n        mm_inputs: List[MultiModalKwargs] = []\n        req_input_ids: List[Tuple[str, int]] = []\n        for req_id, encoder_input_ids in scheduled_encoder_inputs.items():\n            req_state = self.requests[req_id]\n            for input_id in encoder_input_ids:\n                mm_inputs.append(req_state.mm_inputs[input_id])\n                req_input_ids.append((req_id, input_id))\n        batched_mm_inputs = MultiModalKwargs.batch(mm_inputs)\n        batched_mm_inputs = MultiModalKwargs.as_kwargs(batched_mm_inputs,\n                                                       device=self.device)\n\n        # Run the encoder.\n        # `encoder_outputs` is either of the following:\n        # 1. A tensor of shape [num_images, feature_size, hidden_size]\n        # in case when feature_size is fixed across all images.\n        # 2. A list (length: num_images) of tensors, each of shape\n        # [feature_size, hidden_size] in case when the feature size is\n        # dynamic depending on input images.\n        encoder_outputs = self.model.get_multimodal_embeddings(\n            **batched_mm_inputs)\n\n        # Cache the encoder outputs.\n        for (req_id, input_id), output in zip(req_input_ids, encoder_outputs):\n            if req_id not in self.encoder_cache:\n                self.encoder_cache[req_id] = {}\n            self.encoder_cache[req_id][input_id] = output\n\n    def _gather_encoder_outputs(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> List[torch.Tensor]:\n        encoder_outputs: List[torch.Tensor] = []\n        num_reqs = self.input_batch.num_reqs\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[\n                req_id]\n            req_state = self.requests[req_id]\n            num_computed_tokens = req_state.num_computed_tokens\n            mm_positions = req_state.mm_positions\n            for i, pos_info in enumerate(mm_positions):\n                start_pos = pos_info[\"offset\"]\n                num_encoder_tokens = pos_info[\"length\"]\n\n                # The encoder output is needed if the two ranges overlap:\n                # [num_computed_tokens,\n                #  num_computed_tokens + num_scheduled_tokens) and\n                # [start_pos, start_pos + num_encoder_tokens)\n                if start_pos >= num_computed_tokens + num_scheduled_tokens:\n                    # The encoder output is not needed in this step.\n                    break\n                if start_pos + num_encoder_tokens <= num_computed_tokens:\n                    # The encoder output is already processed and stored\n                    # in the decoder's KV cache.\n                    continue\n\n                start_idx = max(num_computed_tokens - start_pos, 0)\n                end_idx = min(\n                    num_computed_tokens - start_pos + num_scheduled_tokens,\n                    num_encoder_tokens)\n                assert start_idx < end_idx\n                assert req_id in self.encoder_cache\n                assert i in self.encoder_cache[req_id]\n                encoder_output = self.encoder_cache[req_id][i]\n                encoder_outputs.append(encoder_output[start_idx:end_idx])\n        return encoder_outputs\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> ModelRunnerOutput:\n        self._update_states(scheduler_output)\n\n        if self.is_multimodal_model:\n            # Run the multimodal encoder if any.\n            self._execute_encoder(scheduler_output)\n            encoder_outputs = self._gather_encoder_outputs(scheduler_output)\n        else:\n            encoder_outputs = []\n\n        # Prepare the decoder inputs.\n        attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\n        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        if (self.use_cuda_graph\n                and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n            # Use piecewise CUDA graphs.\n            # Add padding to the batch size.\n            num_input_tokens = self.vllm_config.pad_for_cudagraph(\n                num_scheduled_tokens)\n        else:\n            # Eager mode.\n            num_input_tokens = num_scheduled_tokens\n        attn_metadata.num_input_tokens = num_input_tokens\n\n        if self.is_multimodal_model:\n            # NOTE(woosuk): To unify token ids and soft tokens (vision\n            # embeddings), we always use embeddings (rather than token ids)\n            # as input to the multimodal model, even when the input is text.\n            input_ids = self.input_ids[:num_scheduled_tokens]\n            if encoder_outputs:\n                inputs_embeds = self.model.get_input_embeddings(\n                    input_ids, encoder_outputs)\n            else:\n                inputs_embeds = self.model.get_input_embeddings(input_ids)\n            # TODO(woosuk): Avoid the copy. Optimize.\n            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)\n            inputs_embeds = self.inputs_embeds[:num_input_tokens]\n            input_ids = None\n        else:\n            # For text-only models, we use token ids as input.\n            # While it is possible to use embeddings as input just like the\n            # multimodal models, it is not desirable for performance since\n            # then the embedding layer is not included in the CUDA graph.\n            input_ids = self.input_ids[:num_input_tokens]\n            inputs_embeds = None\n\n        # Run the decoder.\n        # Use persistent buffers for CUDA graphs.\n        with set_forward_context(attn_metadata, self.vllm_config):\n            hidden_states = self.model(\n                input_ids=input_ids,\n                positions=self.positions[:num_input_tokens],\n                kv_caches=self.kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        hidden_states = hidden_states[:num_scheduled_tokens]\n        hidden_states = hidden_states[logits_indices]\n        logits = self.model.compute_logits(hidden_states, None)\n\n        # Sample the next token and get logprobs if needed.\n        sampling_metadata = self._prepare_sampling(scheduler_output)\n        sampler_output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        sampled_token_ids = sampler_output.sampled_token_ids\n        # TODO(woosuk): The following loop can be slow since it iterates over\n        # the requests one by one. Optimize.\n        num_reqs = self.input_batch.num_reqs\n        for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n            assert req_id is not None\n            req_state = self.requests[req_id]\n            seq_len = (req_state.num_computed_tokens +\n                       scheduler_output.num_scheduled_tokens[req_id])\n            assert seq_len <= req_state.num_tokens\n            if seq_len == req_state.num_tokens:\n                # Append the sampled token to the output token ids.\n                token_id = sampled_token_ids[i]\n                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                req_state.output_token_ids.append(token_id)\n            else:\n                # Ignore the sampled token from the partial request.\n                # Rewind the generator state as if the token was not sampled.\n                generator = self.input_batch.generators.get(i)\n                if generator is not None:\n                    # This relies on cuda-specific torch-internal impl details\n                    generator.set_offset(generator.get_offset() - 4)\n\n        if sampler_output.logprob_token_ids is None:\n            logprob_token_ids = None\n        else:\n            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n        if sampler_output.logprobs is None:\n            logprobs = None\n        else:\n            logprobs = sampler_output.logprobs.cpu()\n\n        # num_reqs entries should be non-None\n        assert all(\n            req_id is not None for req_id in\n            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n\n        model_runner_output = ModelRunnerOutput(\n            req_ids=req_ids,\n            req_id_to_index=self.input_batch.req_id_to_index,\n            sampled_token_ids=sampled_token_ids,\n            logprob_token_ids_cpu=logprob_token_ids,\n            logprobs_cpu=logprobs,\n        )\n        return model_runner_output\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with DeviceMemoryProfiler() as m:  # noqa: SIM117\n            self.model = get_model(vllm_config=self.vllm_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n    @torch.inference_mode()\n    def _dummy_run(\n        self,\n        model: nn.Module,\n        num_tokens: int,\n        kv_caches: List[torch.Tensor],\n    ) -> torch.Tensor:\n        if self.is_multimodal_model:\n            input_ids = None\n            inputs_embeds = self.inputs_embeds[:num_tokens]\n        else:\n            input_ids = self.input_ids[:num_tokens]\n            inputs_embeds = None\n        with set_forward_context(None, self.vllm_config):\n            hidden_states = model(\n                input_ids=input_ids,\n                positions=self.positions[:num_tokens],\n                kv_caches=kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        return hidden_states\n\n    def profile_run(self) -> None:\n        # use an empty tensor instead of `None`` to force Dynamo to pass\n        # it by reference, rather by specializing on the value `None`.\n        # the `dtype` argument does not matter, and we use `float32` as\n        # a placeholder (it has wide hardware support).\n        # it is important to create tensors inside the loop, rather than\n        # multiplying the list, to avoid Dynamo from treating them as\n        # tensor aliasing.\n        dummy_kv_caches = [\n            torch.tensor([], dtype=torch.float32, device=self.device)\n            for _ in range(self.num_attn_layers)\n        ]\n\n        # Profile with multimodal encoder & encoder cache.\n        # TODO (ywang96): generalize this beyond image modality since\n        # mm_input_mapper only supports image inputs.\n        if self.is_multimodal_model:\n\n            # Create dummy batch of multimodal inputs.\n            dummy_request_data = self.input_registry.dummy_data_for_profiling(\n                model_config=self.model_config,\n                seq_len=self.max_num_tokens,\n                mm_registry=self.mm_registry,\n            )\n            dummy_mm_data = dummy_request_data.multi_modal_data\n\n            # NOTE: Currently model is profiled with a single non-text\n            # modality even when it supports multiple.\n            max_tokens_per_mm_item = max(\n                self.mm_registry.get_max_tokens_per_item_by_modality(\n                    self.model_config).values())\n\n            max_num_mm_items_encoder_budget = min(\n                self.max_num_encoder_input_tokens,\n                self.encoder_cache_size) // max_tokens_per_mm_item\n\n            max_mm_items_per_req = max(\n                self.mm_registry.get_mm_limits_per_prompt(\n                    self.model_config).values())\n\n            # NOTE: We do not consider max_num_batched_tokens on purpose\n            # because the multimodal embeddings can be generated in advance\n            # and chunked prefilled.\n            max_num_mm_items_decoder_budget = self.max_num_reqs * \\\n                max_mm_items_per_req\n\n            max_num_mm_items = min(max_num_mm_items_encoder_budget,\n                                   max_num_mm_items_decoder_budget)\n\n            # Dummy data definition in V0 may contain multiple multimodal items\n            # (e.g, multiple images) for a single request, therefore here we\n            # always replicate first item by max_num_mm_items times since in V1\n            # they are scheduled to be processed separately.\n\n            # Case when models have a merged processor, their dummy data is\n            # already batched `MultiModalKwargs`, therefore we need to \"unbatch\"\n            # and take the first item in each batched tensor.\n            # TODO (ywang96): This is somewhat hacky. Refactor this to be\n            # consistent with the other case.\n            if isinstance(dummy_mm_data, MultiModalKwargs):\n                dummy_mm_kwargs = {\n                    k: v[0].unsqueeze(0)\n                    for k, v in dummy_mm_data.items()\n                }\n\n            # Case when models have dummy data explicitly defined as\n            # `MultiModalDataDict`, so they need to be processed through input\n            # mapper.\n            else:\n                # Compute MM hashes (if enabled)\n                mm_hashes = None\n                if self.use_hash:\n                    mm_hashes = self.mm_hasher.hash_dummy_mm_data(\n                        dummy_mm_data)\n\n                mm_kwargs_list = self.mm_input_mapper_client.process_inputs(\n                    mm_data=dummy_mm_data,\n                    mm_hashes=mm_hashes,\n                    mm_processor_kwargs=None,\n                    precomputed_mm_inputs=None)\n\n                # Take the first `MultiModalKwargs`\n                dummy_mm_kwargs = mm_kwargs_list[0]\n\n            batched_dummy_mm_inputs = MultiModalKwargs.batch(\n                [dummy_mm_kwargs] * max_num_mm_items)\n            batched_dummy_mm_inputs = MultiModalKwargs.as_kwargs(\n                batched_dummy_mm_inputs, device=self.device)\n\n            # Run multimodal encoder.\n            dummy_encoder_outputs = self.model.get_multimodal_embeddings(\n                **batched_dummy_mm_inputs)\n            assert len(dummy_encoder_outputs) == max_num_mm_items, (\n                \"Expected dimension 0 of encoder outputs to match the number \"\n                f\"of multimodal data items: {max_num_mm_items}, got \"\n                f\"{len(dummy_encoder_outputs)=} instead. This is most likely \"\n                \"due to the 'get_multimodal_embeddings' method of the model \"\n                \"not implemented correctly.\")\n\n            # Cache the dummy encoder outputs.\n            self.encoder_cache[\"tmp\"] = dict(enumerate(dummy_encoder_outputs))\n\n        # Trigger compilation for general shape.\n        hidden_states = self._dummy_run(self.model, self.max_num_tokens,\n                                        dummy_kv_caches)\n        logits = self.model.compute_logits(hidden_states, None)\n        logits = logits[:self.max_num_tokens]\n        # TODO(woosuk): Consider the memory usage of the sampler.\n        torch.cuda.synchronize()\n        del hidden_states, logits\n        self.encoder_cache.clear()\n        gc.collect()\n\n    def capture_model(self) -> None:\n        if not self.use_cuda_graph:\n            logger.warning(\n                \"Skipping CUDA graph capture. Please add \"\n                \"-O %s to use CUDA graphs.\", CompilationLevel.PIECEWISE)\n            return\n\n        start_time = time.perf_counter()\n        start_free_gpu_memory = torch.cuda.mem_get_info()[0]\n\n        # Trigger CUDA graph capture for specific shapes.\n        # Capture the large shapes first so that the smaller shapes\n        # can reuse the memory pool allocated for the large shapes.\n        with graph_capture():\n            for num_tokens in reversed(self.cudagraph_batch_sizes):\n                for _ in range(self.vllm_config.compilation_config.\n                               cudagraph_num_of_warmups):\n                    self._dummy_run(self.model, num_tokens, self.kv_caches)\n                self._dummy_run(self.model, num_tokens, self.kv_caches)\n\n        end_time = time.perf_counter()\n        end_free_gpu_memory = torch.cuda.mem_get_info()[0]\n        elapsed_time = end_time - start_time\n        cuda_graph_size = start_free_gpu_memory - end_free_gpu_memory\n        # This usually takes 5~20 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs, took %.2f GiB\",\n                    elapsed_time, cuda_graph_size / (1 << 30))\n\n    def initialize_kv_cache(self, num_blocks: int) -> None:\n        assert len(self.kv_caches) == 0\n        kv_cache_shape = FlashAttentionBackend.get_kv_cache_shape(\n            num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n        for _ in range(self.num_attn_layers):\n            self.kv_caches.append(\n                torch.zeros(kv_cache_shape,\n                            dtype=self.kv_cache_dtype,\n                            device=self.device))\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 995de54e8..75098b033 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -644,6 +644,7 @@ class GPUModelRunner:\n                 # Append the sampled token to the output token ids.\n                 token_id = sampled_token_ids[i]\n                 self.input_batch.token_ids_cpu[i, seq_len] = token_id\n+                self.input_batch.num_tokens[i] += 1\n                 req_state.output_token_ids.append(token_id)\n             else:\n                 # Ignore the sampled token from the partial request.",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 1
    }
  ],
  "affected_apis": [
    "InputBatch",
    "GPUModelRunner"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 2
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "NO (cpu)",
    "is_benchmark_actually_there": "",
    "sample_clues": "add, condense, gpu_input_batch"
  }
}