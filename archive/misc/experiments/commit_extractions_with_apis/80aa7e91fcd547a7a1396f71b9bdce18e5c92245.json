{
  "commit_hash": "80aa7e91fcd547a7a1396f71b9bdce18e5c92245",
  "parent_hash": "bd43973522ea17be50e10fbb222a22f673c8067e",
  "message": "[Hardware][Intel] Optimize CPU backend and add more performance tips (#4971)\n\nCo-authored-by: Jianan Gu <jianan.gu@intel.com>",
  "author": "Li, Jiang <jiang1.li@intel.com>",
  "date": "2024-06-13 09:33:14 -0700",
  "files_changed": [
    {
      "file_path": "Dockerfile.cpu",
      "old_content": "# This vLLM Dockerfile is used to construct image that can build and run vLLM on x86 CPU platform.\n\nFROM ubuntu:22.04 AS cpu-test-1\n\nRUN apt-get update  -y \\\n    && apt-get install -y git wget vim numactl gcc-12 g++-12 python3 python3-pip \\\n    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n\nRUN pip install --upgrade pip \\\n    && pip install wheel packaging ninja \"setuptools>=49.4.0\" numpy\n\nFROM cpu-test-1 AS build\n\nCOPY ./ /workspace/vllm\n\nWORKDIR /workspace/vllm\n\nRUN pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n\nRUN VLLM_TARGET_DEVICE=cpu python3 setup.py install\n\nWORKDIR /workspace/\n\nRUN ln -s /workspace/vllm/tests  && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n\nCMD [\"/bin/bash\"]\n",
      "diff": "diff --git a/Dockerfile.cpu b/Dockerfile.cpu\nindex 403a1cd03..777bb0829 100644\n--- a/Dockerfile.cpu\n+++ b/Dockerfile.cpu\n@@ -3,9 +3,13 @@\n FROM ubuntu:22.04 AS cpu-test-1\n \n RUN apt-get update  -y \\\n-    && apt-get install -y git wget vim numactl gcc-12 g++-12 python3 python3-pip \\\n+    && apt-get install -y git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 \\\n     && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n \n+RUN echo 'export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD' >> ~/.bashrc\n+\n+RUN pip install https://intel-extension-for-pytorch.s3.amazonaws.com/ipex_dev/cpu/intel_extension_for_pytorch-2.3.100%2Bgit0eb3473-cp310-cp310-linux_x86_64.whl\n+\n RUN pip install --upgrade pip \\\n     && pip install wheel packaging ninja \"setuptools>=49.4.0\" numpy\n \n@@ -21,6 +25,6 @@ RUN VLLM_TARGET_DEVICE=cpu python3 setup.py install\n \n WORKDIR /workspace/\n \n-RUN ln -s /workspace/vllm/tests  && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n+RUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n \n CMD [\"/bin/bash\"]",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 3
    },
    {
      "file_path": "README.md",
      "old_content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://vllm.ai\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://discord.gg/jz7wjKhh6g\"><b>Discord</b></a> |\n\n</p>\n\n---\n\n**Ray Summit CPF is Open (June 4th to June 20th)!**\n\nThere will be a track for vLLM at the Ray Summit (09/30-10/02, SF) this year!\nIf you have cool projects related to vLLM or LLM inference, we would love to see your proposals.\nThis will be a great chance for everyone in the community to get together and learn.\nPlease submit your proposal [here](https://raysummit.anyscale.com/flow/anyscale/raysummit2024/landing/page/eventsite)\n\n**The Fourth vLLM Bay Area Meetup (June 11th 5:30pm-8pm PT)**\n\nWe are thrilled to announce our fourth vLLM Meetup!\nThe vLLM team will share recent updates and roadmap.\nWe will also have vLLM collaborators from BentoML and Cloudflare coming up to the stage to discuss their experience in deploying LLMs with vLLM.\nPlease register [here](https://lu.ma/agivllm) and join us!\n\n---\n\n*Latest News* ðŸ”¥\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) in SF! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2024/01] Added ROCm 6.0 support to vLLM.\n- [2023/12] Added ROCm 5.7 support to vLLM.\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) in SF! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/09] We created our [Discord server](https://discord.gg/jz7wjKhh6g)! Join us to discuss vLLM and LLM serving! We will also post the latest announcements and updates there.\n- [2023/09] We released our [PagedAttention paper](https://arxiv.org/abs/2309.06180) on arXiv!\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/07] Added support for LLaMA-2! You can run and serve 7B/13B/70B LLaMA-2s on vLLM with a single command!\n- [2023/06] Serving vLLM On any Cloud with SkyPilot. Check out a 1-click [example](https://github.com/skypilot-org/skypilot/blob/master/llm/vllm) to start the vLLM demo, and the [blog post](https://blog.skypilot.co/serving-llm-24x-faster-on-the-cloud-with-vllm-and-skypilot/) for the story behind vLLM development on the clouds.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n---\n## About\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with **PagedAttention**\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantization: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [SqueezeLLM](https://arxiv.org/abs/2306.07629), FP8 KV Cache\n- Optimized CUDA kernels\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs and AMD GPUs\n- (Experimental) Prefix caching support\n- (Experimental) Multi-lora support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\nInstall vLLM with pip or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):\n\n```bash\npip install vllm\n```\n\nVisit our [documentation](https://vllm.readthedocs.io/en/latest/) to learn more.\n- [Installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)\n- [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n- [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.\n\n## Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with docs/source/community/sponsors.md -->\n\n- a16z\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Dropbox\n- Lambda Lab\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Sequoia Capital\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n",
      "diff": "diff --git a/README.md b/README.md\nindex 57374d279..8e4480ac2 100644\n--- a/README.md\n+++ b/README.md\n@@ -65,7 +65,7 @@ vLLM is flexible and easy to use with:\n - Tensor parallelism support for distributed inference\n - Streaming outputs\n - OpenAI-compatible API server\n-- Support NVIDIA GPUs and AMD GPUs\n+- Support NVIDIA GPUs, AMD GPUs, and Intel CPUs\n - (Experimental) Prefix caching support\n - (Experimental) Multi-lora support",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "docs/source/getting_started/cpu-installation.rst",
      "old_content": ".. _installation_cpu:\n\nInstallation with CPU\n========================\n\nvLLM initially supports basic model inferencing and serving on x86 CPU platform, with data types FP32 and BF16.\n\nTable of contents:\n\n#. :ref:`Requirements <cpu_backend_requirements>`\n#. :ref:`Quick start using Dockerfile <cpu_backend_quick_start_dockerfile>`\n#. :ref:`Build from source <build_cpu_backend_from_source>`\n#. :ref:`Performance tips <cpu_backend_performance_tips>`\n\n.. _cpu_backend_requirements:\n\nRequirements\n------------\n\n* OS: Linux\n* Compiler: gcc/g++>=12.3.0 (recommended)\n* Instruction set architecture (ISA) requirement: AVX512 is required.\n\n.. _cpu_backend_quick_start_dockerfile:\n\nQuick start using Dockerfile\n----------------------------\n\n.. code-block:: console\n\n    $ docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n    $ docker run -it \\\n                 --rm \\\n                 --network=host \\\n                 --cpuset-cpus=<cpu-id-list, optional> \\\n                 --cpuset-mems=<memory-node, optional> \\\n                 vllm-cpu-env\n\n.. _build_cpu_backend_from_source:\n\nBuild from source\n-----------------\n\n- First, install required compiler. We recommend to use ``gcc/g++ >= 12.3.0`` as the default compiler to avoid potential problems. For example, on Ubuntu 22.4, you can run:\n\n.. code-block:: console\n\n    $ sudo apt-get update  -y\n    $ sudo apt-get install -y gcc-12 g++-12\n    $ sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n\n- Second, install Python packages for vLLM CPU backend building:\n\n.. code-block:: console\n\n    $ pip install --upgrade pip\n    $ pip install wheel packaging ninja \"setuptools>=49.4.0\" numpy\n    $ pip install -v -r requirements-cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu\n\n- Finally, build and install vLLM CPU backend: \n\n.. code-block:: console\n\n    $ VLLM_TARGET_DEVICE=cpu python setup.py install\n\n.. note::\n    - BF16 is the default data type in the current CPU backend (that means the backend will cast FP16 to BF16), and is compatible will all CPUs with AVX512 ISA support. \n\n    - AVX512_BF16 is an extension ISA provides native BF16 data type conversion and vector product instructions, will brings some performance improvement compared with pure AVX512. The CPU backend build script will check the host CPU flags to determine whether to enable AVX512_BF16. \n    \n    - If you want to force enable AVX512_BF16 for the cross-compilation, please set environment variable VLLM_CPU_AVX512BF16=1 before the building.    \n\n.. _cpu_backend_performance_tips:\n\nPerformance tips\n-----------------\n\n- vLLM CPU backend uses environment variable ``VLLM_CPU_KVCACHE_SPACE`` to specify the KV Cache size (e.g, ``VLLM_CPU_KVCACHE_SPACE=40`` means 40 GB space for KV cache), larger setting will allow vLLM running more requests in parallel. This parameter should be set based on the hardware configuration and memory management pattern of users.\n\n- vLLM CPU backend uses OpenMP for thread-parallel computation. If you want the best performance on CPU, it will be very critical to isolate CPU cores for OpenMP threads with other thread pools (like web-service event-loop), to avoid CPU oversubscription. \n\n- If using vLLM CPU backend on a bare-metal machine, it is recommended to disable the hyper-threading.\n\n- If using vLLM CPU backend on a multi-socket machine with NUMA, be aware to set CPU cores and memory nodes, to avoid the remote memory node access. ``numactl`` is an useful tool for CPU core and memory binding on NUMA platform. Besides, ``--cpuset-cpus`` and ``--cpuset-mems`` arguments of ``docker run`` are also useful.\n\n\n\n",
      "diff": "diff --git a/docs/source/getting_started/cpu-installation.rst b/docs/source/getting_started/cpu-installation.rst\nindex 5270253ca..a9544e8a5 100644\n--- a/docs/source/getting_started/cpu-installation.rst\n+++ b/docs/source/getting_started/cpu-installation.rst\n@@ -10,6 +10,7 @@ Table of contents:\n #. :ref:`Requirements <cpu_backend_requirements>`\n #. :ref:`Quick start using Dockerfile <cpu_backend_quick_start_dockerfile>`\n #. :ref:`Build from source <build_cpu_backend_from_source>`\n+#. :ref:`Intel Extension for PyTorch <ipex_guidance>`\n #. :ref:`Performance tips <cpu_backend_performance_tips>`\n \n .. _cpu_backend_requirements:\n@@ -18,7 +19,7 @@ Requirements\n ------------\n \n * OS: Linux\n-* Compiler: gcc/g++>=12.3.0 (recommended)\n+* Compiler: gcc/g++>=12.3.0 (optional, recommended)\n * Instruction set architecture (ISA) requirement: AVX512 is required.\n \n .. _cpu_backend_quick_start_dockerfile:\n@@ -41,7 +42,7 @@ Quick start using Dockerfile\n Build from source\n -----------------\n \n-- First, install required compiler. We recommend to use ``gcc/g++ >= 12.3.0`` as the default compiler to avoid potential problems. For example, on Ubuntu 22.4, you can run:\n+- First, install recommended compiler. We recommend to use ``gcc/g++ >= 12.3.0`` as the default compiler to avoid potential problems. For example, on Ubuntu 22.4, you can run:\n \n .. code-block:: console\n \n@@ -70,6 +71,15 @@ Build from source\n     \n     - If you want to force enable AVX512_BF16 for the cross-compilation, please set environment variable VLLM_CPU_AVX512BF16=1 before the building.    \n \n+.. _ipex_guidance:\n+\n+Intel Extension for PyTorch\n+---------------------------\n+\n+- `Intel Extension for PyTorch (IPEX) <https://github.com/intel/intel-extension-for-pytorch>`_ extends PyTorch with up-to-date features optimizations for an extra performance boost on Intel hardware.\n+\n+- IPEX after the ``2.3.0`` can be enabled in the CPU backend by default if it is installed.\n+\n .. _cpu_backend_performance_tips:\n \n Performance tips\n@@ -77,6 +87,15 @@ Performance tips\n \n - vLLM CPU backend uses environment variable ``VLLM_CPU_KVCACHE_SPACE`` to specify the KV Cache size (e.g, ``VLLM_CPU_KVCACHE_SPACE=40`` means 40 GB space for KV cache), larger setting will allow vLLM running more requests in parallel. This parameter should be set based on the hardware configuration and memory management pattern of users.\n \n+- We highly recommend to use TCMalloc for high performance memory allocation and better cache locality. For example, on Ubuntu 22.4, you can run:\n+\n+.. code-block:: console\n+\n+    $ sudo apt-get install libtcmalloc-minimal4 # install TCMalloc library\n+    $ find / -name *libtcmalloc* # find the dynamic link library path\n+    $ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:$LD_PRELOAD # prepend the library to LD_PRELOAD\n+    $ python examples/offline_inference.py # run vLLM\n+\n - vLLM CPU backend uses OpenMP for thread-parallel computation. If you want the best performance on CPU, it will be very critical to isolate CPU cores for OpenMP threads with other thread pools (like web-service event-loop), to avoid CPU oversubscription. \n \n - If using vLLM CPU backend on a bare-metal machine, it is recommended to disable the hyper-threading.",
      "change_type": "modified",
      "lines_added": 22,
      "lines_removed": 3
    },
    {
      "file_path": "requirements-cpu.txt",
      "old_content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for x86_64 CPUs\ntorch == 2.3.0+cpu\ntriton >= 2.2.0  # FIXME(woosuk): This is a hack to avoid import error.",
      "diff": "diff --git a/requirements-cpu.txt b/requirements-cpu.txt\nindex b739642d8..8b7d86e68 100644\n--- a/requirements-cpu.txt\n+++ b/requirements-cpu.txt\n@@ -2,5 +2,5 @@\n -r requirements-common.txt\n \n # Dependencies for x86_64 CPUs\n-torch == 2.3.0+cpu\n+torch == 2.3.1+cpu\n triton >= 2.2.0  # FIXME(woosuk): This is a hack to avoid import error.\n\\ No newline at end of file",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/attention/backends/torch_sdpa.py",
      "old_content": "\"\"\" Attention layer with torch scaled_dot_product_attention\n    and PagedAttention.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple, Type\n\nimport torch\nfrom torch.nn.functional import scaled_dot_product_attention\n\nfrom vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                              AttentionMetadata)\nfrom vllm.attention.ops.paged_attn import (PagedAttention,\n                                           PagedAttentionMetadata)\n\n\nclass TorchSDPABackend(AttentionBackend):\n\n    @staticmethod\n    def get_name() -> str:\n        return \"torch-sdpa\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"TorchSDPABackendImpl\"]:\n        return TorchSDPABackendImpl\n\n    @staticmethod\n    def make_metadata(*args, **kwargs) -> \"TorchSDPAMetadata\":\n        return TorchSDPAMetadata(*args, **kwargs)\n\n    @staticmethod\n    def get_kv_cache_shape(\n        num_blocks: int,\n        block_size: int,\n        num_kv_heads: int,\n        head_size: int,\n    ) -> Tuple[int, ...]:\n        return PagedAttention.get_kv_cache_shape(num_blocks, block_size,\n                                                 num_kv_heads, head_size)\n\n    @staticmethod\n    def swap_blocks(\n        src_kv_cache: torch.Tensor,\n        dst_kv_cache: torch.Tensor,\n        src_to_dst: torch.Tensor,\n    ) -> None:\n        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)\n\n    @staticmethod\n    def copy_blocks(\n        kv_caches: List[torch.Tensor],\n        src_to_dists: torch.Tensor,\n    ) -> None:\n        PagedAttention.copy_blocks(kv_caches, src_to_dists)\n\n\n@dataclass\nclass TorchSDPAMetadata(AttentionMetadata, PagedAttentionMetadata):\n    \"\"\"Metadata for TorchSDPABackend.\n    \"\"\"\n    # Currently, input sequences can only contain all prompts\n    # or all decoding. True if all sequences are prompts.\n    is_prompt: bool\n    slot_mapping: torch.Tensor\n    seq_lens: Optional[List[int]]\n\n    def __post_init__(self):\n        # Set during the execution of the first attention op.\n        # It is a list because it is needed to set per prompt\n        # when alibi slopes is used. It is because of the limitation\n        # from xformer API.\n        # will not appear in the __repr__ and __init__\n        self.attn_bias: Optional[List[torch.Tensor]] = None\n\n    @property\n    def prefill_metadata(self) -> Optional[\"TorchSDPAMetadata\"]:\n        # Currently chunked prefill is not supported\n        if self.num_decode_tokens == 0:\n            assert self.num_prefills > 0\n            return self\n\n        return None\n\n    @property\n    def decode_metadata(self) -> Optional[\"TorchSDPAMetadata\"]:\n        # Currently chunked prefill is not supported\n        if self.num_prefills > 0:\n            assert self.num_decode_tokens == 0\n            return None\n\n        return self\n\n\nclass TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        assert blocksparse_params is None, ValueError(\n            \"Torch SPDA does not support block-sparse attention.\")\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        if alibi_slopes is not None:\n            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n        self.alibi_slopes = alibi_slopes\n        self.sliding_window = sliding_window\n        self.kv_cache_dtype = kv_cache_dtype\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n        self.need_mask = (self.alibi_slopes is not None\n                          or self.sliding_window is not None)\n\n        supported_head_sizes = PagedAttention.get_supported_head_sizes()\n        if head_size not in supported_head_sizes:\n            raise ValueError(\n                f\"Head size {head_size} is not supported by PagedAttention. \"\n                f\"Supported head sizes are: {supported_head_sizes}.\")\n        if kv_cache_dtype != \"auto\":\n            raise NotImplementedError(\n                \"Torch SDPA backend does not support FP8 KV cache. \"\n                \"Please use xFormers backend instead.\")\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: Optional[torch.Tensor],\n        attn_metadata: TorchSDPAMetadata,  # type: ignore\n        kv_scale: float = 1.0,\n    ) -> torch.Tensor:\n        \"\"\"Forward pass with torch SDPA and PagedAttention.\n\n        Args:\n            query: shape = [num_tokens, num_heads * head_size]\n            key: shape = [num_tokens, num_kv_heads * head_size]\n            value: shape = [num_tokens, num_kv_heads * head_size]\n            kv_cache = [2, num_blocks, block_size * num_kv_heads * head_size]\n            attn_metadata: Metadata for attention.\n        Returns:\n            shape = [num_tokens, num_heads * head_size]\n        \"\"\"\n        assert kv_scale == 1.0\n        num_tokens, hidden_size = query.shape\n        # Reshape the query, key, and value tensors.\n        query = query.view(-1, self.num_heads, self.head_size)\n        key = key.view(-1, self.num_kv_heads, self.head_size)\n        value = value.view(-1, self.num_kv_heads, self.head_size)\n\n        if kv_cache is not None:\n            key_cache, value_cache = PagedAttention.split_kv_cache(\n                kv_cache, self.num_kv_heads, self.head_size)\n            PagedAttention.write_to_paged_cache(key, value, key_cache,\n                                                value_cache,\n                                                attn_metadata.slot_mapping,\n                                                self.kv_cache_dtype, kv_scale)\n\n        if attn_metadata.is_prompt:\n            assert attn_metadata.seq_lens is not None\n            if (kv_cache is None or attn_metadata.block_tables.numel() == 0):\n                if self.num_kv_heads != self.num_heads:\n                    key = key.repeat_interleave(self.num_queries_per_kv, dim=1)\n                    value = value.repeat_interleave(self.num_queries_per_kv,\n                                                    dim=1)\n\n                if attn_metadata.attn_bias is None:\n                    if self.alibi_slopes is not None:\n                        att_masks = _make_alibi_bias(\n                            self.alibi_slopes, query.dtype,\n                            attn_metadata.seq_lens)  # type: ignore\n                    elif self.sliding_window is not None:\n                        att_masks = _make_sliding_window_bias(\n                            attn_metadata.seq_lens, self.sliding_window,\n                            query.dtype)  # type: ignore\n                    else:\n                        att_masks = [None] * len(attn_metadata.seq_lens)\n                    attn_metadata.attn_bias = att_masks\n\n                query = query.movedim(0, query.dim() - 2)\n                key = key.movedim(0, key.dim() - 2)\n                value = value.movedim(0, value.dim() - 2)\n\n                start = 0\n                output = torch.empty(\n                    (num_tokens, self.num_heads, self.head_size),\n                    dtype=query.dtype)\n                for seq_len, mask in zip(attn_metadata.seq_lens,\n                                         attn_metadata.attn_bias):\n                    end = start + seq_len\n                    sub_out = scaled_dot_product_attention(\n                        query[:, start:end, :],\n                        key[:, start:end, :],\n                        value[:, start:end, :],\n                        attn_mask=mask,\n                        dropout_p=0.0,\n                        is_causal=not self.need_mask,\n                        scale=self.scale).movedim(query.dim() - 2, 0)\n                    output[start:end, :, :] = sub_out\n                    start = end\n            else:\n                # prefix-enabled attention\n                raise RuntimeError(\n                    \"Torch SDPA backend doesn't support prefix decoding.\")\n\n        else:\n            # Decoding run.\n            output = PagedAttention.forward_decode(\n                query,\n                key_cache,\n                value_cache,\n                attn_metadata.block_tables,\n                attn_metadata.seq_lens_tensor,\n                attn_metadata.max_decode_seq_len,\n                self.kv_cache_dtype,\n                self.num_kv_heads,\n                self.scale,\n                self.alibi_slopes,\n                kv_scale,\n            )\n\n        # Reshape the output tensor.\n        return output.view(-1, self.num_heads * self.head_size)\n\n\ndef _make_alibi_bias(\n    alibi_slopes: torch.Tensor,\n    dtype: torch.dtype,\n    seq_lens: List[int],\n) -> List[torch.Tensor]:\n    attn_biases = []\n    for seq_len in seq_lens:\n        bias = torch.arange(seq_len, dtype=dtype)\n        # NOTE(zhuohan): HF uses\n        #     `bias = bias[None, :].repeat(seq_len, 1)`\n        # here. We find that both biases give the same results, but\n        # the bias below more accurately follows the original ALiBi\n        # paper.\n        bias = bias[None, :] - bias[:, None]\n\n        num_heads = alibi_slopes.shape[0]\n        bias = bias[None, :].repeat((num_heads, 1, 1))\n        bias.mul_(alibi_slopes[:, None, None])\n        inf_mask = torch.empty(\n            (1, seq_len, seq_len),\n            dtype=bias.dtype).fill_(-torch.inf).triu_(diagonal=1)\n        attn_biases.append((bias + inf_mask).to(dtype))\n\n    return attn_biases\n\n\ndef _make_sliding_window_bias(\n    seq_lens: List[int],\n    window_size: Optional[int],\n    dtype: torch.dtype,\n) -> List[torch.Tensor]:\n    attn_biases = []\n    for seq_len in seq_lens:\n        tensor = torch.full(\n            (1, seq_len, seq_len),\n            dtype=dtype,\n            fill_value=1,\n        )\n        shift = 0\n        mask = torch.tril(tensor, diagonal=shift).to(dtype)  # type: ignore\n        if window_size is not None:\n            mask = torch.triu(mask, diagonal=shift - window_size + 1)\n        mask = torch.log(mask)\n        attn_biases.append(mask.to(dtype))\n\n    return attn_biases\n",
      "diff": "diff --git a/vllm/attention/backends/torch_sdpa.py b/vllm/attention/backends/torch_sdpa.py\nindex 9b50adec5..4b08cce99 100644\n--- a/vllm/attention/backends/torch_sdpa.py\n+++ b/vllm/attention/backends/torch_sdpa.py\n@@ -8,8 +8,16 @@ from torch.nn.functional import scaled_dot_product_attention\n \n from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                               AttentionMetadata)\n-from vllm.attention.ops.paged_attn import (PagedAttention,\n-                                           PagedAttentionMetadata)\n+from vllm.attention.ops.paged_attn import PagedAttentionMetadata\n+from vllm.utils import is_cpu\n+\n+if is_cpu():\n+    try:\n+        from vllm.attention.ops.ipex_attn import PagedAttention\n+    except ImportError:\n+        from vllm.attention.ops.paged_attn import PagedAttention\n+else:\n+    from vllm.attention.ops.paged_attn import PagedAttention\n \n \n class TorchSDPABackend(AttentionBackend):\n@@ -197,13 +205,14 @@ class TorchSDPABackendImpl(AttentionImpl[TorchSDPAMetadata]):\n                                          attn_metadata.attn_bias):\n                     end = start + seq_len\n                     sub_out = scaled_dot_product_attention(\n-                        query[:, start:end, :],\n-                        key[:, start:end, :],\n-                        value[:, start:end, :],\n+                        query[None, :, start:end, :],\n+                        key[None, :, start:end, :],\n+                        value[None, :, start:end, :],\n                         attn_mask=mask,\n                         dropout_p=0.0,\n                         is_causal=not self.need_mask,\n-                        scale=self.scale).movedim(query.dim() - 2, 0)\n+                        scale=self.scale).squeeze(0).movedim(\n+                            query.dim() - 2, 0)\n                     output[start:end, :, :] = sub_out\n                     start = end\n             else:\n@@ -248,7 +257,7 @@ def _make_alibi_bias(\n \n         num_heads = alibi_slopes.shape[0]\n         bias = bias[None, :].repeat((num_heads, 1, 1))\n-        bias.mul_(alibi_slopes[:, None, None])\n+        bias.mul_(alibi_slopes[:, None, None]).unsqueeze_(0)\n         inf_mask = torch.empty(\n             (1, seq_len, seq_len),\n             dtype=bias.dtype).fill_(-torch.inf).triu_(diagonal=1)",
      "change_type": "modified",
      "lines_added": 17,
      "lines_removed": 8
    },
    {
      "file_path": "vllm/attention/ops/ipex_attn.py",
      "old_content": "",
      "diff": "diff --git a/vllm/attention/ops/ipex_attn.py b/vllm/attention/ops/ipex_attn.py\nnew file mode 100644\nindex 000000000..5a5317b65\n--- /dev/null\n+++ b/vllm/attention/ops/ipex_attn.py\n@@ -0,0 +1,120 @@\n+from typing import Dict, List, Optional, Tuple\n+\n+import intel_extension_for_pytorch.llm.modules as ipex_modules\n+import torch\n+\n+from vllm import _custom_ops as ops\n+\n+\n+class PagedAttention:\n+\n+    @staticmethod\n+    def get_supported_head_sizes() -> List[int]:\n+        return [64, 80, 96, 112, 128, 256]\n+\n+    @staticmethod\n+    def get_kv_cache_shape(\n+        num_blocks: int,\n+        block_size: int,\n+        num_kv_heads: int,\n+        head_size: int,\n+        *args,\n+    ) -> Tuple[int, ...]:\n+        return (2, num_blocks, block_size * num_kv_heads * head_size)\n+\n+    @staticmethod\n+    def split_kv_cache(\n+        kv_cache: torch.Tensor,\n+        num_kv_heads: int,\n+        head_size: int,\n+        *args,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        num_blocks = kv_cache.shape[1]\n+\n+        key_cache = kv_cache[0]\n+        key_cache = key_cache.view(num_blocks, num_kv_heads, -1, head_size)\n+        value_cache = kv_cache[1]\n+        value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)\n+        return key_cache, value_cache\n+\n+    @staticmethod\n+    def write_to_paged_cache(\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        slot_mapping: torch.Tensor,\n+        kv_cache_dtype: str,\n+        kv_scale: float,\n+        *args,\n+    ) -> None:\n+        ipex_modules.PagedAttention.reshape_and_cache(\n+            key, value, key_cache, value_cache,\n+            slot_mapping.flatten().int())\n+\n+    @staticmethod\n+    def forward_decode(\n+        query: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        block_tables: torch.Tensor,\n+        context_lens: torch.Tensor,\n+        max_context_len: int,\n+        kv_cache_dtype: str,\n+        num_kv_heads: int,\n+        scale: float,\n+        alibi_slopes: Optional[torch.Tensor],\n+        kv_scale: float,\n+        *args,\n+    ) -> torch.Tensor:\n+        output = torch.empty_like(query)\n+        block_size = value_cache.shape[2]\n+        head_mapping = torch.arange(\n+            0,\n+            num_kv_heads,\n+            device=\"cpu\",\n+            dtype=torch.int32,\n+        ).view(num_kv_heads,\n+               1).repeat_interleave(query.size(1) // num_kv_heads).flatten()\n+        ipex_modules.PagedAttention.single_query_cached_kv_attention(\n+            output, query.contiguous(), key_cache, value_cache, head_mapping,\n+            scale, block_tables, context_lens, block_size, max_context_len,\n+            alibi_slopes)\n+\n+        return output\n+\n+    @staticmethod\n+    def forward_prefix(\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        block_tables: torch.Tensor,\n+        subquery_start_loc: torch.Tensor,\n+        prompt_lens_tensor: torch.Tensor,\n+        context_lens: torch.Tensor,\n+        max_subquery_len: int,\n+        alibi_slopes: Optional[torch.Tensor],\n+        *args,\n+    ) -> torch.Tensor:\n+        raise NotImplementedError\n+\n+    @staticmethod\n+    def swap_blocks(\n+        src_kv_cache: torch.Tensor,\n+        dst_kv_cache: torch.Tensor,\n+        src_to_dst: Dict[int, int],\n+        *args,\n+    ) -> None:\n+        raise NotImplementedError\n+\n+    @staticmethod\n+    def copy_blocks(\n+        kv_caches: List[torch.Tensor],\n+        src_to_dists: Dict[int, List[int]],\n+        *args,\n+    ) -> None:\n+        key_caches = [kv_cache[0] for kv_cache in kv_caches]\n+        value_caches = [kv_cache[1] for kv_cache in kv_caches]\n+        ops.copy_blocks(key_caches, value_caches, src_to_dists)",
      "change_type": "added",
      "lines_added": 121,
      "lines_removed": 1
    }
  ],
  "affected_apis": [
    "vllm.attention.backends.torch_sdpa.TorchSDPABackend",
    "vllm.attention.backends.torch_sdpa.TorchSDPABackendImpl",
    "vllm.attention.ops.ipex_attn.PagedAttention",
    "vllm.attention.ops.paged_attn.PagedAttention"
  ],
  "summary": {
    "total_files": 6,
    "files_added": 1,
    "files_deleted": 0,
    "files_modified": 5
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (but cpu)",
    "is_benchmark_actually_there": "",
    "sample_clues": "attention, attn, backends"
  }
}