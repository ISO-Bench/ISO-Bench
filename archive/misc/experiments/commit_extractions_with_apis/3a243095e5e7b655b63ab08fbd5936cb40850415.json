{
  "commit_hash": "3a243095e5e7b655b63ab08fbd5936cb40850415",
  "parent_hash": "64172a976c8d975b3aec946f1675716d2532d94f",
  "message": "Optimize `_get_ranks` in Sampler (#3623)",
  "author": "Antoni Baum <antoni.baum@protonmail.com>",
  "date": "2024-03-25 16:03:02 -0700",
  "files_changed": [
    {
      "file_path": "vllm/model_executor/layers/sampler.py",
      "old_content": "\"\"\"A layer that samples the next tokens from the model's outputs.\"\"\"\nimport itertools\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom vllm.model_executor.layers.ops.sample import sample as sample_triton\nfrom vllm.model_executor.sampling_metadata import (SamplingMetadata,\n                                                   SamplingTensors)\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import (Logprob, PromptLogprobs, SampleLogprobs,\n                           SamplerOutput, SequenceData, SequenceGroupOutput,\n                           SequenceOutput)\n\n\nclass Sampler(nn.Module):\n    \"\"\"Samples the next tokens from the model's outputs.\n\n    This layer does the following:\n    1. Discard the hidden states that are not used for sampling (i.e., all\n        tokens except the final one in each prompt).\n    2. Compute the logits for the next tokens.\n    3. Apply presence, frequency and repetition penalties.\n    4. Apply temperature scaling.\n    5. Apply top-p and top-k truncation.\n    6. Sample the next tokens.\n    Here, each sequence group within the batch can have different sampling\n    parameters (e.g., sampling method, temperature, top-p, top-k, etc.).\n    \"\"\"\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> Optional[SamplerOutput]:\n        assert logits is not None\n        _, vocab_size = logits.shape\n\n        # Apply min_tokens penalty which sets stop tokens to -inf if min_tokens\n        # have not been generated yet\n        logits = _apply_min_tokens_penalty(logits, sampling_metadata)\n\n        # Prepare sampling tensors with pinned memory to avoid blocking.\n        (sampling_tensors, do_penalties, do_top_p_top_k,\n         do_min_p) = SamplingTensors.from_sampling_metadata(\n             sampling_metadata, vocab_size, logits.device, logits.dtype)\n\n        # Apply presence and frequency penalties.\n        if do_penalties:\n            logits = _apply_penalties(logits, sampling_tensors.prompt_tokens,\n                                      sampling_tensors.output_tokens,\n                                      sampling_tensors.presence_penalties,\n                                      sampling_tensors.frequency_penalties,\n                                      sampling_tensors.repetition_penalties)\n\n        # Apply temperature scaling.\n        # Use in-place division to avoid creating a new tensor.\n        logits.div_(sampling_tensors.temperatures.unsqueeze_(dim=1))\n\n        if do_top_p_top_k:\n            logits = _apply_top_k_top_p(logits, sampling_tensors.top_ps,\n                                        sampling_tensors.top_ks)\n\n        if do_min_p:\n            logits = _apply_min_p(logits, sampling_tensors.min_ps)\n\n        # We use float32 for probabilities and log probabilities.\n        # Compute the probabilities.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n        # Compute the log probabilities.\n        # Use log_softmax to ensure numerical stability.\n        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n\n        # Sample the next tokens.\n        sample_results = _sample(probs, logprobs, sampling_metadata,\n                                 sampling_tensors)\n        # Get the logprobs query results.\n        prompt_logprobs, sample_logprobs = _get_logprobs(\n            logprobs, sampling_metadata, sample_results)\n        return _build_sampler_output(sample_results, sampling_metadata,\n                                     prompt_logprobs, sample_logprobs)\n\n\ndef _get_bin_counts_and_mask(\n    tokens: torch.Tensor,\n    vocab_size: int,\n    num_seqs: int,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # Compute the bin counts for the tokens.\n    # vocab_size + 1 for padding.\n    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                             dtype=torch.long,\n                             device=tokens.device)\n    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n    bin_counts = bin_counts[:, :vocab_size]\n    mask = bin_counts > 0\n\n    return bin_counts, mask\n\n\ndef _apply_min_tokens_penalty(\n    logits: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n) -> torch.Tensor:\n    # list of indices in logits that will be set to -inf\n    logits_to_penalize = []\n    start_idx = 0\n    for seq_ids, sampling_params in sampling_metadata.seq_groups:\n        min_tokens = sampling_params.min_tokens\n        if min_tokens > 0:\n            seqs_to_penalize = []\n            for i, seq_id in enumerate(seq_ids):\n                seq_data = sampling_metadata.seq_data[seq_id]\n                if len(seq_data.output_token_ids) < min_tokens:\n                    seqs_to_penalize.append(i)\n\n            if seqs_to_penalize:\n                # convert to the index into logits\n                seqs_to_penalize = [start_idx + i for i in seqs_to_penalize]\n                # use set() to remove any duplicates\n                token_ids_to_penalize = set(sampling_params.stop_token_ids +\n                                            [sampling_params.eos_token_id])\n                # itertools.product pairs each seq index with every token id\n                logits_to_penalize.extend(\n                    itertools.product(seqs_to_penalize, token_ids_to_penalize))\n\n        start_idx += len(seq_ids)\n\n    if logits_to_penalize:\n        # use zip and * to group indices along each dimension\n        # eg. [ (1,2), (1,3), (5,6) ] -> ( (1,1,5), (2,3,6) )\n        logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\n\n    return logits\n\n\ndef _apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                     output_tokens_tensor: torch.Tensor,\n                     presence_penalties: torch.Tensor,\n                     frequency_penalties: torch.Tensor,\n                     repetition_penalties: torch.Tensor) -> torch.Tensor:\n    num_seqs, vocab_size = logits.shape\n    _, prompt_mask = _get_bin_counts_and_mask(prompt_tokens_tensor, vocab_size,\n                                              num_seqs)\n    output_bin_counts, output_mask = _get_bin_counts_and_mask(\n        output_tokens_tensor, vocab_size, num_seqs)\n\n    repetition_penalties = repetition_penalties[:, None].repeat(1, vocab_size)\n    repetition_penalties[~(prompt_mask | output_mask)] = 1.0\n    logits = torch.where(logits > 0, logits / repetition_penalties,\n                         logits * repetition_penalties)\n\n    # We follow the definition in OpenAI API.\n    # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n    logits -= frequency_penalties.unsqueeze_(dim=1) * output_bin_counts\n    logits -= presence_penalties.unsqueeze_(dim=1) * output_mask\n    return logits\n\n\ndef _apply_top_k_top_p(\n    logits: torch.Tensor,\n    p: torch.Tensor,\n    k: torch.Tensor,\n) -> torch.Tensor:\n    logits_sort, logits_idx = logits.sort(dim=-1, descending=False)\n\n    # Apply top-k.\n    top_k_mask = logits_sort.size(1) - k.to(torch.long)\n    # Get all the top_k values.\n    top_k_mask = logits_sort.gather(1, top_k_mask.unsqueeze(dim=1))\n    top_k_mask = logits_sort < top_k_mask\n    logits_sort.masked_fill_(top_k_mask, -float(\"inf\"))\n\n    # Apply top-p.\n    probs_sort = logits_sort.softmax(dim=-1)\n    probs_sum = probs_sort.cumsum(dim=-1)\n    top_p_mask = probs_sum <= 1 - p.unsqueeze(dim=1)\n    # at least one\n    top_p_mask[:, -1] = False\n    logits_sort.masked_fill_(top_p_mask, -float(\"inf\"))\n\n    # Re-sort the probabilities.\n    src = torch.arange(logits_idx.shape[-1],\n                       device=logits_idx.device).expand_as(logits_idx)\n    logits_idx_inv = torch.empty_like(logits_idx).scatter_(dim=-1,\n                                                           index=logits_idx,\n                                                           src=src)\n    logits = torch.gather(logits_sort, dim=-1, index=logits_idx_inv)\n    return logits\n\n\ndef _apply_min_p(\n    logits: torch.Tensor,\n    min_p: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Adapted from\n    https://github.com/oobabooga/text-generation-webui/blob/3146124ec01f02c8fb1650a6517cf1b60b537aaf/modules/sampler_hijack.py#L16C17-L16C17\n    \"\"\"\n    probs = torch.softmax(logits, dim=-1)\n    top_probs, _ = probs.max(dim=-1, keepdim=True)\n    scaled_min_p = min_p.unsqueeze_(dim=1) * top_probs\n    tokens_to_remove = probs < scaled_min_p\n    logits = logits.masked_fill_(tokens_to_remove, -float(\"inf\"))\n\n    return logits\n\n\ndef _greedy_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    samples: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    samples = samples.tolist()\n    sample_idx = 0\n    results = []\n    for seq_group in selected_seq_groups:\n        seq_ids, _ = seq_group\n        num_parent_seqs = len(seq_ids)\n        assert num_parent_seqs == 1, (\n            \"Greedy sampling should have only one seq.\")\n        parent_ids = list(range(num_parent_seqs))\n        next_token_ids = [samples[sample_idx]]\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    return results\n\n\ndef _random_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    is_prompts: List[bool],\n    random_samples: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    # Find the maximum best_of value of the prompt phase requests.\n    random_samples = random_samples.cpu()\n    sample_idx = 0\n    results = []\n    for seq_group, is_prompt in zip(selected_seq_groups, is_prompts):\n        seq_ids, sampling_params = seq_group\n        num_parent_seqs = len(seq_ids)\n        if is_prompt:\n            # Prompt phase.\n            parent_ids = [0] * sampling_params.best_of\n            next_token_ids = random_samples[\n                sample_idx, :sampling_params.best_of].tolist()\n        else:\n            # Generation phase.\n            parent_ids = list(range(num_parent_seqs))\n            next_token_ids = random_samples[sample_idx:sample_idx +\n                                            num_parent_seqs, 0].tolist()\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    return results\n\n\ndef _beam_search_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    is_prompts: List[bool],\n    seq_data: Dict[int, SequenceData],\n    logprobs: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    # We sample 2 * beam_width candidates to make sure that with high\n    # probability we can get `beam_width` candidates in addition to\n    # the finished sequences for the next iteration. See\n    # https://github.com/tensorflow/tensor2tensor/blob/bafdc1b67730430d38d6ab802cbd51f9d053ba2e/tensor2tensor/utils/beam_search.py#L557-L563\n    # for details. See also HF reference:\n    # https://github.com/huggingface/transformers/blob/a4dd53d88e4852f023332d284ff07a01afcd5681/src/transformers/generation/utils.py#L3063-L3065\n    #\n    # NOTE: Beam search is not vectorized, so its speed can be slower than\n    # other sampling methods.\n    sample_idx = 0\n    results = []\n    for seq_group, is_prompt in zip(selected_seq_groups, is_prompts):\n        seq_ids, sampling_params = seq_group\n        num_parent_seqs = len(seq_ids)\n        beam_width = sampling_params.best_of\n        seq_group_logprobs = logprobs[sample_idx:sample_idx + num_parent_seqs]\n        if is_prompt:\n            # Prompt phase.\n            assert num_parent_seqs == 1, (\n                \"Prompt input should have only one seq.\")\n            parent_ids = [0] * (2 * beam_width)\n            _, next_token_ids = torch.topk(seq_group_logprobs[0],\n                                           2 * beam_width)\n            next_token_ids = next_token_ids.tolist()\n        else:\n            # Generation phase.\n            cumulative_logprobs = [\n                seq_data[seq_id].cumulative_logprob for seq_id in seq_ids\n            ]\n            cumulative_logprobs = torch.tensor(\n                cumulative_logprobs,\n                dtype=torch.float,\n                device=seq_group_logprobs.device)\n            seq_group_logprobs = (seq_group_logprobs +\n                                  cumulative_logprobs.unsqueeze(dim=1))\n            _, topk_ids = torch.topk(seq_group_logprobs.flatten(),\n                                     2 * beam_width)\n            topk_ids = topk_ids.tolist()\n            vocab_size = seq_group_logprobs.size(-1)\n            parent_ids = [i // vocab_size for i in topk_ids]\n            next_token_ids = [i % vocab_size for i in topk_ids]\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    assert sample_idx == logprobs.size(0)\n    return results\n\n\n# torch.multinomial forces a GPU<->CPU sync.\n# Therefore, we use an optimized implementation instead.\n# Note that we always sample with replacement.\n# probs will be modified in place, but this is fine, as we pass\n# in a copy already.\ndef _multinomial(\n    probs: torch.Tensor,\n    num_samples: int,\n    seq_groups: Optional[List[Tuple[List[int], SamplingParams]]] = None,\n    generators: Optional[List[torch.Generator]] = None,\n) -> torch.Tensor:\n    if num_samples > 1:\n        # This is equivalent to torch.repeat_interleaved (which also\n        # forces a GPU<->CPU sync).\n        # This allows us to do sampling with replacement by creating\n        # num_samples copies of each row in the tensor, and then\n        # batch sampling the resulting tensor.\n        probs = probs[:, None, :].expand(probs.shape[0], num_samples,\n                                         probs.shape[1]).contiguous().view(\n                                             -1, probs.shape[1])\n    q = torch.empty_like(probs)\n    if seq_groups is None:\n        q.exponential_()\n    else:\n        sample_idx = 0\n        for (seq_ids, _), generator in zip(seq_groups, generators):\n            next_sample_idx = sample_idx + len(seq_ids) * num_samples\n            q[sample_idx:next_sample_idx].exponential_(generator=generator)\n            sample_idx = next_sample_idx\n    return probs.div_(q).argmax(dim=1).view(-1, num_samples)\n\n\ndef _sample_with_torch(\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n) -> List[Tuple[List[int], List[int]]]:\n    categorized_seq_group_ids = {t: [] for t in SamplingType}\n    categorized_sample_indices = sampling_metadata.categorized_sample_indices\n    for i, seq_group in enumerate(sampling_metadata.seq_groups):\n        _, sampling_params = seq_group\n        sampling_type = sampling_params.sampling_type\n        categorized_seq_group_ids[sampling_type].append(i)\n\n    sample_results_dict: Dict[int, Tuple[List[int], List[int]]] = {}\n    sample_metadata = {}\n    multinomial_samples = {}\n\n    # Counterintiutively, having two loops here is actually faster.\n    # The first loop can run without waiting on GPU<->CPU sync.\n    for sampling_type in SamplingType:\n        sample_indices = categorized_sample_indices[sampling_type][:, 0]\n        num_tokens = len(sample_indices)\n        if num_tokens == 0:\n            continue\n        seq_group_ids = categorized_seq_group_ids[sampling_type]\n        seq_groups = [sampling_metadata.seq_groups[i] for i in seq_group_ids]\n        is_prompts = [i < sampling_metadata.num_prompts for i in seq_group_ids]\n        sample_metadata[sampling_type] = (seq_group_ids, seq_groups,\n                                          is_prompts, sample_indices)\n        if sampling_type == SamplingType.GREEDY:\n            greedy_samples = torch.argmax(logprobs[sample_indices.long()],\n                                          dim=-1)\n        elif sampling_type in (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n            max_best_of_in_batch = 1\n            for seq_group, is_prompt in zip(seq_groups, is_prompts):\n                if is_prompt:\n                    _, sampling_params = seq_group\n                    max_best_of_in_batch = max(max_best_of_in_batch,\n                                               sampling_params.best_of)\n            seeded_args = {} if sampling_type == SamplingType.RANDOM else {\n                \"seq_groups\": seq_groups,\n                \"generators\": sampling_metadata.generators,\n            }\n            multinomial_samples[sampling_type] = _multinomial(\n                probs[sample_indices.long()], max_best_of_in_batch,\n                **seeded_args)\n        elif sampling_type == SamplingType.BEAM:\n            beam_search_logprobs = logprobs[sample_indices]\n        else:\n            raise ValueError(f\"Unsupported sampling type: {sampling_type}\")\n\n    # GPU<->CPU sync happens in the loop below.\n\n    for sampling_type in SamplingType:\n        if sampling_type not in sample_metadata:\n            continue\n        seq_group_ids, seq_groups, is_prompts, sample_indices = sample_metadata[\n            sampling_type]\n        if sampling_type == SamplingType.GREEDY:\n            sample_results = _greedy_sample(seq_groups, greedy_samples)\n        elif sampling_type in (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n            sample_results = _random_sample(seq_groups, is_prompts,\n                                            multinomial_samples[sampling_type])\n        elif sampling_type == SamplingType.BEAM:\n            sample_results = _beam_search_sample(seq_groups, is_prompts,\n                                                 sampling_metadata.seq_data,\n                                                 beam_search_logprobs)\n        sample_results_dict.update(zip(seq_group_ids, sample_results))\n\n    sample_results = [\n        sample_results_dict[i]\n        for i in range(len(sampling_metadata.seq_groups))\n    ]\n    return sample_results\n\n\ndef _sample_with_triton_kernel(\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n    sampling_tensors: SamplingTensors,\n) -> List[Tuple[List[int], List[int]]]:\n    categorized_seq_group_ids = {t: [] for t in SamplingType}\n    categorized_sample_indices = sampling_metadata.categorized_sample_indices\n    for i, seq_group in enumerate(sampling_metadata.seq_groups):\n        _, sampling_params = seq_group\n        sampling_type = sampling_params.sampling_type\n        categorized_seq_group_ids[sampling_type].append(i)\n\n    sample_results_dict: Dict[int, Tuple[List[int], List[int]]] = {}\n    sample_metadata = {}\n    max_best_of_in_batch = 1\n\n    # Counterintiutively, having two loops here is actually faster.\n    # The first loop can run without waiting on GPU<->CPU sync.\n    for sampling_type in SamplingType:\n        sample_indices = categorized_sample_indices[sampling_type][:, 0]\n        sampled_token_indices = categorized_sample_indices[sampling_type][:, 1]\n        num_tokens = len(sample_indices)\n        if num_tokens == 0:\n            continue\n        seq_group_ids = categorized_seq_group_ids[sampling_type]\n        seq_groups = [sampling_metadata.seq_groups[i] for i in seq_group_ids]\n        is_prompts = [i < sampling_metadata.num_prompts for i in seq_group_ids]\n        sample_metadata[sampling_type] = (seq_group_ids, seq_groups,\n                                          is_prompts, sample_indices,\n                                          sampled_token_indices)\n        if sampling_type in (SamplingType.GREEDY, SamplingType.RANDOM,\n                             SamplingType.RANDOM_SEED):\n            for seq_group, is_prompt in zip(seq_groups, is_prompts):\n                if is_prompt:\n                    _, sampling_params = seq_group\n                    max_best_of_in_batch = max(max_best_of_in_batch,\n                                               sampling_params.best_of)\n        elif sampling_type == SamplingType.BEAM:\n            beam_search_logprobs = logprobs[sample_indices]\n        else:\n            raise ValueError(f\"Unsupported sampling type: {sampling_type}\")\n\n    sampled_tokens, _, _ = sample_triton(\n        probs=probs,\n        seeds=sampling_tensors.sampling_seeds,\n        max_best_of=max_best_of_in_batch,\n        sample_indices=sampling_tensors.sample_indices,\n        logprobs=logprobs,\n        # don't save logprobs because we have logic for that below\n        # TODO: use this instead of the CPU-based logic below\n        save_logprobs=False,\n    )\n\n    # GPU<->CPU sync happens in the loop below.\n\n    for sampling_type in SamplingType:\n        if sampling_type not in sample_metadata:\n            continue\n        (seq_group_ids, seq_groups, is_prompts, sample_indices,\n         sampled_token_indices) = sample_metadata[sampling_type]\n        if sampling_type == SamplingType.GREEDY:\n            sample_results = _greedy_sample(\n                seq_groups, sampled_tokens[sampled_token_indices][:, 0])\n        elif sampling_type in (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n            sample_results = _random_sample(\n                seq_groups, is_prompts, sampled_tokens[sampled_token_indices])\n        elif sampling_type == SamplingType.BEAM:\n            sample_results = _beam_search_sample(seq_groups, is_prompts,\n                                                 sampling_metadata.seq_data,\n                                                 beam_search_logprobs)\n        sample_results_dict.update(zip(seq_group_ids, sample_results))\n\n    sample_results = [\n        sample_results_dict[i]\n        for i in range(len(sampling_metadata.seq_groups))\n    ]\n    return sample_results\n\n\ndef _sample(\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n    sampling_tensors: SamplingTensors,\n) -> List[Tuple[List[int], List[int]]]:\n    return _sample_with_torch(probs, logprobs, sampling_metadata)\n\n    # TODO: Enable once Triton kernel & associated code is faster.\n    # return _sample_with_triton_kernel(probs, logprobs, sampling_metadata,\n    #                                   sampling_tensors)\n\n\ndef _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n    \"\"\"\n    This function calculates the ranks of the chosen tokens in a logprob tensor.\n\n    Args:\n        x (torch.Tensor): 2D logprob tensor of shape (N, M)\n                        where N is the no. of tokens and M is the vocab dim.\n        indices (List[int]): List of chosen token indices.\n\n    Returns:\n        torch.Tensor: 1D tensor of shape (N,) where N is the no. of tokens.\n                    Each element in the returned tensor represents the rank \n                    of the chosen token in the input logprob tensor.\n    \"\"\"\n    vals = x[range(len(x)), indices]\n    return (x > vals[:, None]).long().sum(1) + 1\n\n\ndef _get_logprobs(\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n    sample_results: List[Tuple[List[int], List[int]]],\n) -> Tuple[List[Optional[List[Optional[Dict[int, float]]]]], List[List[Dict[\n        int, float]]]]:\n    # Prepare query indices\n    batched_logprobs_query_seq_indices: List[int] = []\n    batched_logprobs_query_token_indices: List[int] = []\n    largest_num_logprobs = 0\n    sample_idx = 0\n    for i, (seq_group, sample_result) in enumerate(\n            zip(sampling_metadata.seq_groups, sample_results)):\n        seq_ids, sampling_params = seq_group\n        next_token_ids, parent_ids = sample_result\n        num_parent_seqs = len(seq_ids)\n        if (i < sampling_metadata.num_prompts\n                and sampling_params.prompt_logprobs is not None):\n            largest_num_logprobs = max(largest_num_logprobs,\n                                       sampling_params.prompt_logprobs)\n            prompt_len = sampling_metadata.prompt_lens[i]\n            prompt_tokens = sampling_metadata.seq_data[\n                seq_ids[0]].prompt_token_ids\n            batched_logprobs_query_seq_indices.extend(\n                sample_idx + j for j in range(prompt_len - 1))\n            batched_logprobs_query_token_indices.extend(\n                token_id for token_id in prompt_tokens[1:])\n            sample_idx += prompt_len - 1\n        batched_logprobs_query_seq_indices.extend(\n            [sample_idx + parent_id for parent_id in parent_ids])\n        batched_logprobs_query_token_indices.extend(next_token_ids)\n        if sampling_params.logprobs is not None:\n            largest_num_logprobs = max(largest_num_logprobs,\n                                       sampling_params.logprobs)\n        sample_idx += num_parent_seqs\n    assert sample_idx == logprobs.size(0)\n\n    # Batched query for logprobs of selected token\n    batched_logprobs_query_result = logprobs[[\n        batched_logprobs_query_seq_indices,\n        batched_logprobs_query_token_indices\n    ]]\n\n    # Batched query for logprobs of topk tokens\n    if largest_num_logprobs > 0:\n        top_logprobs, top_token_ids = torch.topk(logprobs,\n                                                 largest_num_logprobs,\n                                                 dim=-1)\n        top_logprobs = top_logprobs.cpu()\n        top_token_ids = top_token_ids.cpu()\n    else:\n        top_logprobs, top_token_ids = None, None\n\n    batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n\n    batched_ranks_query_result = _get_ranks(\n        logprobs[batched_logprobs_query_seq_indices],\n        batched_logprobs_query_token_indices)\n\n    # Gather results\n    result_prompt_logprobs: List[Optional[PromptLogprobs]] = []\n    result_sample_logprobs: List[SampleLogprobs] = []\n    sample_idx = 0\n    query_result_idx = 0\n    for i, (seq_group, sample_result) in enumerate(\n            zip(sampling_metadata.seq_groups, sample_results)):\n        seq_ids, sampling_params = seq_group\n        next_token_ids, parent_ids = sample_result\n\n        # Prompt logprobs\n        if (i < sampling_metadata.num_prompts\n                and sampling_params.prompt_logprobs is not None):\n            num_logprobs = sampling_params.prompt_logprobs\n            prompt_tokens = sampling_metadata.seq_data[\n                seq_ids[0]].prompt_token_ids\n            group_prompt_logprobs: PromptLogprobs = [None]\n            for token_id in prompt_tokens[1:]:\n                prompt_logprobs_dict = {\n                    token_id:\n                    (batched_logprobs_query_result[query_result_idx].item(),\n                     batched_ranks_query_result[query_result_idx].item())\n                }\n                if num_logprobs > 0:\n                    prompt_logprobs_dict.update(\n                        zip(\n                            top_token_ids[sample_idx, :num_logprobs].tolist(),\n                            zip(\n                                top_logprobs[\n                                    sample_idx, :num_logprobs].tolist(),\n                                range(1, num_logprobs + 1))))\n                group_prompt_logprobs.append({\n                    token_id: Logprob(*logprob_rank)\n                    for token_id, logprob_rank in prompt_logprobs_dict.items()\n                })\n                sample_idx += 1\n                query_result_idx += 1\n            result_prompt_logprobs.append(group_prompt_logprobs)\n        else:\n            result_prompt_logprobs.append(None)\n\n        # Sample logprobs\n        num_logprobs = sampling_params.logprobs\n        if num_logprobs is None:\n            num_logprobs = 0\n        group_sample_logprobs: SampleLogprobs = []\n        for next_token_id, parent_id in zip(next_token_ids, parent_ids):\n            sample_logprobs_dict = {\n                next_token_id:\n                (batched_logprobs_query_result[query_result_idx].item(),\n                 batched_ranks_query_result[query_result_idx].item())\n            }\n            query_result_idx += 1\n            if num_logprobs > 0:\n                sample_logprobs_dict.update(\n                    zip(\n                        top_token_ids[sample_idx +\n                                      parent_id, :num_logprobs].tolist(),\n                        zip(\n                            top_logprobs[sample_idx +\n                                         parent_id, :num_logprobs].tolist(),\n                            range(1, num_logprobs + 1))))\n            group_sample_logprobs.append({\n                token_id: Logprob(*logprob_rank)\n                for token_id, logprob_rank in sample_logprobs_dict.items()\n            })\n        result_sample_logprobs.append(group_sample_logprobs)\n        sample_idx += len(seq_ids)\n\n    return result_prompt_logprobs, result_sample_logprobs\n\n\ndef _build_sampler_output(\n    sample_results: List[Tuple[List[int], List[int]]],\n    sampling_metadata: SamplingMetadata,\n    prompt_logprobs: List[Optional[PromptLogprobs]],\n    sample_logprobs: List[SampleLogprobs],\n) -> SamplerOutput:\n    sampler_output = []\n    for (seq_group, sample_result, group_prompt_logprobs,\n         group_sample_logprobs) in zip(sampling_metadata.seq_groups,\n                                       sample_results, prompt_logprobs,\n                                       sample_logprobs):\n        seq_ids, _ = seq_group\n        next_token_ids, parent_ids = sample_result\n        seq_outputs = []\n        for parent_id, next_token_id, logprobs in zip(parent_ids,\n                                                      next_token_ids,\n                                                      group_sample_logprobs):\n            seq_outputs.append(\n                SequenceOutput(seq_ids[parent_id], next_token_id, logprobs))\n        sampler_output.append(\n            SequenceGroupOutput(seq_outputs, group_prompt_logprobs))\n    return SamplerOutput(outputs=sampler_output)\n",
      "diff": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex d07527304..06135192c 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -506,22 +506,23 @@ def _sample(\n     #                                   sampling_tensors)\n \n \n-def _get_ranks(x: torch.Tensor, indices: List[int]) -> torch.Tensor:\n+def _get_ranks(x: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     This function calculates the ranks of the chosen tokens in a logprob tensor.\n \n     Args:\n         x (torch.Tensor): 2D logprob tensor of shape (N, M)\n                         where N is the no. of tokens and M is the vocab dim.\n-        indices (List[int]): List of chosen token indices.\n+        indices (torch.Tensor): List of chosen token indices.\n \n     Returns:\n         torch.Tensor: 1D tensor of shape (N,) where N is the no. of tokens.\n                     Each element in the returned tensor represents the rank \n                     of the chosen token in the input logprob tensor.\n     \"\"\"\n-    vals = x[range(len(x)), indices]\n-    return (x > vals[:, None]).long().sum(1) + 1\n+    vals = x[torch.arange(0, len(x), device=x.device, dtype=indices.dtype),\n+             indices]\n+    return (x > vals[:, None]).long().sum(1).add_(1)\n \n \n def _get_logprobs(\n@@ -561,12 +562,21 @@ def _get_logprobs(\n         sample_idx += num_parent_seqs\n     assert sample_idx == logprobs.size(0)\n \n+    batched_logprobs_query_seq_indices_gpu = torch.tensor(\n+        batched_logprobs_query_seq_indices, device=logprobs.device)\n+    batched_logprobs_query_token_indices_gpu = torch.tensor(\n+        batched_logprobs_query_token_indices, device=logprobs.device)\n+\n     # Batched query for logprobs of selected token\n     batched_logprobs_query_result = logprobs[[\n-        batched_logprobs_query_seq_indices,\n-        batched_logprobs_query_token_indices\n+        batched_logprobs_query_seq_indices_gpu,\n+        batched_logprobs_query_token_indices_gpu\n     ]]\n \n+    batched_ranks_query_result = _get_ranks(\n+        logprobs[batched_logprobs_query_seq_indices_gpu],\n+        batched_logprobs_query_token_indices_gpu)\n+\n     # Batched query for logprobs of topk tokens\n     if largest_num_logprobs > 0:\n         top_logprobs, top_token_ids = torch.topk(logprobs,\n@@ -578,10 +588,7 @@ def _get_logprobs(\n         top_logprobs, top_token_ids = None, None\n \n     batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n-\n-    batched_ranks_query_result = _get_ranks(\n-        logprobs[batched_logprobs_query_seq_indices],\n-        batched_logprobs_query_token_indices)\n+    batched_ranks_query_result = batched_ranks_query_result.cpu()\n \n     # Gather results\n     result_prompt_logprobs: List[Optional[PromptLogprobs]] = []",
      "change_type": "modified",
      "lines_added": 18,
      "lines_removed": 11
    }
  ],
  "affected_apis": [
    "LLM.generate"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_sampler)",
    "is_benchmark_actually_there": "",
    "sample_clues": "executor, get, layers"
  }
}