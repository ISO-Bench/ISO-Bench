{
  "commit_hash": "bd852f2a8b9e9129de69fa7349906a9115538d5a",
  "parent_hash": "ec266536b7c4d4d308566ac928a69fcb9ef94462",
  "message": "[Performance] Enable chunked prefill and prefix caching together (#8120)\n\nCo-authored-by: Tao He <sighingnow@gmail.com>\nCo-authored-by: Juelianqvq <Juelianqvq@noreply.github.com>",
  "author": "Cody Yu <hao.yu.cody@gmail.com>",
  "date": "2024-09-03 10:49:18 -0700",
  "files_changed": [],
  "affected_apis": [
    "GenerationMixin.generate",
    "PreTrainedModel.generate",
    "transformers.pipeline"
  ],
  "summary": {
    "total_files": 0,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 0
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "",
    "is_benchmark_actually_there": "",
    "sample_clues": "cache, llmengine, prefix"
  }
}