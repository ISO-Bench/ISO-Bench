{
  "commit_hash": "81ede99ca44a5b3518932a07ea4a76a719e7416e",
  "parent_hash": "5eda21e773447d81ffc661ac094716420dc7b7cb",
  "message": "[Core] Deprecating block manager v1 and make block manager v2 default (#8704)\n\nRemoving the block manager v1. This is the initial piece of prefix-caching-centric design. In order to achieve prefix-caching-centric design, we need to simplify the code path so that we only use v2 block manager (which has much higher performance on prefix caching).",
  "author": "Kuntai Du <kuntai@uchicago.edu>",
  "date": "2024-10-17 11:38:15 -0500",
  "files_changed": [
    {
      "file_path": ".buildkite/test-pipeline.yaml",
      "old_content": "# In this file, you can add more tests to run either by adding a new step or\n# adding a new command to an existing step. See different options here for examples.\n\n# This script will be feed into Jinja template in `test-template-aws.j2` at\n# https://github.com/vllm-project/buildkite-ci/blob/main/scripts/test-template-aws.j2 \n# to generate the final pipeline yaml file.\n\n# Documentation\n# label(str): the name of the test. emoji allowed.\n# fast_check(bool): whether to run this on each commit on fastcheck pipeline.\n# fast_check_only(bool): run this test on fastcheck pipeline only\n# optional(bool): never run this test by default (i.e. need to unblock manually)\n# command(str): the single command to run for tests. incompatible with commands.\n# commands(list): the list of commands to run for test. incompatbile with command.\n# mirror_hardwares(list): the list of hardwares to run the test on as well. currently only supports [amd]\n# gpu(str): override the GPU selection for the test. default is on L4 GPUs. currently only supports a100\n# num_gpus(int): override the number of GPUs for the test. default to 1 GPU. currently support 2,4.\n# num_nodes(int): whether to simulate multi-node setup by launch multiple containers on one host, \n#     in this case, commands must be specified. the first command runs on first host, the second\n#     command runs on the second host.\n# working_dir(str): specify the place where command should execute, default to /vllm-workspace/tests\n# source_file_dependencies(list): the list of prefix to opt-in the test for, if empty, the test will always run.\n\n# When adding a test\n# - If the test belong to an existing group, add it there\n# - If the test is short, add to any existing step\n# - If the test takes more than 10min, then it is okay to create a new step. \n#   Note that all steps execute in parallel. \n\nsteps:\n##### fast check tests  #####\n\n- label: Documentation Build # 2min\n  working_dir: \"/vllm-workspace/test_docs/docs\"\n  fast_check: true\n  no_gpu: True\n  commands:\n  - pip install -r requirements-docs.txt\n  - SPHINXOPTS=\\\"-W\\\" make html\n  # Check API reference (if it fails, you may have missing mock imports)\n  - grep \\\"sig sig-object py\\\" build/html/dev/sampling_params.html\n\n- label: Async Engine, Inputs, Utils, Worker Test # 24min\n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/mq_llm_engine\n  - tests/async_engine\n  - tests/test_inputs\n  - tests/multimodal\n  - tests/test_utils\n  - tests/worker\n  commands:\n  - pytest -v -s mq_llm_engine # MQLLMEngine\n  - pytest -v -s async_engine # AsyncLLMEngine\n  - NUM_SCHEDULER_STEPS=4 pytest -v -s async_engine/test_async_llm_engine.py\n  - pytest -v -s test_inputs.py\n  - pytest -v -s multimodal\n  - pytest -v -s test_utils.py # Utils\n  - pytest -v -s worker # Worker\n\n- label: Basic Correctness Test # 30min\n  #mirror_hardwares: [amd]\n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/basic_correctness/test_basic_correctness\n  - tests/basic_correctness/test_cpu_offload\n  - tests/basic_correctness/test_preemption\n  commands:\n  - pytest -v -s basic_correctness/test_basic_correctness.py\n  - pytest -v -s basic_correctness/test_cpu_offload.py\n  - VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT=1 pytest -v -s basic_correctness/test_preemption.py\n\n- label: Chunked Prefill Test\n  source_file_dependencies:\n  - vllm/\n  - tests/basic_correctness/test_chunked_prefill\n  commands:\n  - VLLM_ATTENTION_BACKEND=XFORMERS VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n  - VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n\n- label: Core Test # 10min\n  mirror_hardwares: [amd]\n  fast_check: true\n  source_file_dependencies:\n  - vllm/core\n  - vllm/distributed\n  - tests/core\n  commands:\n  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core/test_scheduler.py\n  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/test_chunked_prefill_scheduler.py\n  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness.py\n  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n  - pytest -v -s core --ignore=core/block/e2e/test_correctness.py --ignore=core/test_scheduler.py --ignore=core/test_chunked_prefill_scheduler.py --ignore=core/block/e2e/test_correctness.py --ignore=core/block/e2e/test_correctness_sliding_window.py\n\n- label: Entrypoints Test # 40min\n  working_dir: \"/vllm-workspace/tests\"\n  fast_check: true\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  commands:\n  - pip install -e ./plugins/vllm_add_dummy_model\n  - pytest -v -s entrypoints/llm --ignore=entrypoints/llm/test_lazy_outlines.py --ignore=entrypoints/llm/test_generate.py --ignore=entrypoints/llm/test_generate_multiple_loras.py --ignore=entrypoints/llm/test_guided_generate.py\n  - pytest -v -s entrypoints/llm/test_lazy_outlines.py # it needs a clean process\n  - pytest -v -s entrypoints/llm/test_generate.py # it needs a clean process\n  - pytest -v -s entrypoints/llm/test_generate_multiple_loras.py # it needs a clean process\n  - pytest -v -s entrypoints/llm/test_guided_generate.py # it needs a clean process\n  - pytest -v -s entrypoints/openai --ignore=entrypoints/openai/test_oot_registration.py\n  - pytest -v -s entrypoints/openai/test_oot_registration.py # it needs a clean process\n  - pytest -v -s entrypoints/test_chat_utils.py\n  - pytest -v -s entrypoints/offline_mode # Needs to avoid interference with other tests\n\n- label: Distributed Tests (4 GPUs) # 10min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  fast_check: true\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/core/\n  - tests/distributed\n  - tests/spec_decode/e2e/test_integration_dist_tp4\n  - tests/compile\n  commands:\n  - pytest -v -s compile/test_basic_correctness.py\n  - pytest -v -s distributed/test_pynccl.py\n  - pytest -v -s spec_decode/e2e/test_integration_dist_tp4.py\n\n- label: Metrics, Tracing Test # 10min\n  num_gpus: 2 \n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/metrics\n  - tests/tracing\n  commands:\n  - pytest -v -s metrics \n  - \"pip install \\\n      'opentelemetry-sdk>=1.26.0,<1.27.0' \\\n      'opentelemetry-api>=1.26.0,<1.27.0' \\\n      'opentelemetry-exporter-otlp>=1.26.0,<1.27.0' \\\n      'opentelemetry-semantic-conventions-ai>=0.4.1,<0.5.0'\"\n  - pytest -v -s tracing\n\n##### fast check tests  #####\n#####  1 GPU test  #####\n\n- label: Regression Test # 5min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/test_regression\n  commands:\n  - pip install modelscope\n  - pytest -v -s test_regression.py\n  working_dir: \"/vllm-workspace/tests\" # optional\n\n- label: Engine Test # 10min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/engine\n  - tests/tokenization\n  commands:\n  - pytest -v -s engine test_sequence.py test_config.py test_logger.py\n  # OOM in the CI unless we run this separately\n  - pytest -v -s tokenization\n\n- label: Examples Test # 15min\n  working_dir: \"/vllm-workspace/examples\"\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/entrypoints\n  - examples/\n  commands:\n    - pip install awscli tensorizer # for llava example and tensorizer test\n    - python3 offline_inference.py\n    - python3 cpu_offload.py\n    - python3 offline_inference_chat.py\n    - python3 offline_inference_with_prefix.py\n    - python3 llm_engine_example.py\n    - python3 offline_inference_vision_language.py\n    - python3 offline_inference_vision_language_multi_image.py\n    - python3 tensorize_vllm_model.py --model facebook/opt-125m serialize --serialized-directory /tmp/ --suffix v1 && python3 tensorize_vllm_model.py --model facebook/opt-125m deserialize --path-to-tensors /tmp/vllm/facebook/opt-125m/v1/model.tensors\n    - python3 offline_inference_encoder_decoder.py\n    - python3 offline_profile.py --model facebook/opt-125m\n\n- label: Prefix Caching Test # 9min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/prefix_caching\n  commands:\n    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s prefix_caching/test_prefix_caching.py\n    - pytest -v -s prefix_caching --ignore=prefix_caching/test_prefix_caching.py\n\n- label: Samplers Test # 36min\n  source_file_dependencies:\n  - vllm/model_executor/layers\n  - vllm/sampling_metadata.py\n  - tests/samplers\n  commands:\n    - pytest -v -s samplers\n    - VLLM_USE_FLASHINFER_SAMPLER=1 pytest -v -s samplers\n\n- label: LogitsProcessor Test # 5min\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/model_executor/layers\n  - tests/test_logits_processor\n  command: pytest -v -s test_logits_processor.py\n\n- label: Speculative decoding tests # 30min\n  source_file_dependencies:\n  - vllm/spec_decode\n  - tests/spec_decode\n  commands:\n    - pytest -v -s spec_decode/e2e/test_multistep_correctness.py\n    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s spec_decode/e2e/test_compatibility.py\n    - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py --ignore=spec_decode/e2e/test_compatibility.py\n\n- label: LoRA Test %N # 15min each\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/lora\n  - tests/lora\n  command: pytest -v -s lora --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT --ignore=lora/test_long_context.py\n  parallelism: 4\n\n- label: \"PyTorch Fullgraph Smoke Test\" # 9min\n  fast_check: true\n  source_file_dependencies:\n  - vllm/\n  - tests/compile\n  commands:\n  - pytest -v -s compile/test_basic_correctness.py\n\n# TODO: re-write in comparison tests, and fix symbolic shape\n# for quantization ops.\n# - label: \"PyTorch Fullgraph Test\" # 18min\n#   source_file_dependencies:\n#   - vllm/\n#   - tests/compile\n#   commands:\n#   - pytest -v -s compile/test_full_graph.py\n\n- label: Kernels Test %N # 1h each\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - csrc/\n  - vllm/attention\n  - tests/kernels\n  commands:\n    - pytest -v -s kernels --shard-id=$$BUILDKITE_PARALLEL_JOB --num-shards=$$BUILDKITE_PARALLEL_JOB_COUNT\n  parallelism: 4\n\n- label: Tensorizer Test # 11min\n  mirror_hardwares: [amd]\n  soft_fail: true\n  source_file_dependencies:\n  - vllm/model_executor/model_loader\n  - tests/tensorizer_loader\n  commands:\n    - apt-get update && apt-get install -y curl libsodium23\n    - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n    - pytest -v -s tensorizer_loader\n\n- label: Benchmarks # 9min\n  working_dir: \"/vllm-workspace/.buildkite\"\n  mirror_hardwares: [amd]\n  source_file_dependencies:\n  - benchmarks/\n  commands:\n  - pip install aiohttp\n  - bash run-benchmarks.sh\n\n- label: Quantization Test # 33min\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  - tests/quantization\n  command: VLLM_TEST_FORCE_LOAD_FORMAT=auto pytest -v -s quantization\n\n- label: LM Eval Small Models # 53min\n  working_dir: \"/vllm-workspace/.buildkite/lm-eval-harness\"\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  commands:\n  - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n  - bash ./run-tests.sh -c configs/models-small.txt -t 1\n\n- label: Encoder Decoder tests # 5min\n  source_file_dependencies:\n  - vllm/\n  - tests/encoder_decoder\n  commands:\n    - pytest -v -s encoder_decoder\n\n- label: OpenAI-Compatible Tool Use # 20 min\n  fast_check: false\n  mirror_hardwares: [ amd ]\n  source_file_dependencies:\n    - vllm/\n    - tests/tool_use\n  commands:\n    - pytest -v -s tool_use\n\n#####  models test  #####\n\n- label: Basic Models Test # 3min\n  source_file_dependencies:\n  - vllm/\n  - tests/models\n  commands:\n    - pip install -e ./plugins/vllm_add_dummy_model\n    - pytest -v -s models/test_oot_registration.py # it needs a clean process\n    - pytest -v -s models/*.py --ignore=models/test_oot_registration.py\n\n- label: Decoder-only Language Models Test # 1h36min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/models/decoder_only/language\n  commands:\n    - pytest -v -s models/decoder_only/language\n\n- label: Decoder-only Multi-Modal Models Test # 1h31min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/models/decoder_only/audio_language\n  - tests/models/decoder_only/vision_language\n  commands:\n    - pytest -v -s models/decoder_only/audio_language\n    - pytest -v -s models/decoder_only/vision_language\n\n- label: Other Models Test # 6min\n  #mirror_hardwares: [amd]\n  source_file_dependencies:\n  - vllm/\n  - tests/models/embedding/language\n  - tests/models/embedding/vision_language\n  - tests/models/encoder_decoder/language\n  - tests/models/encoder_decoder/vision_language\n  commands:\n    - pytest -v -s models/embedding/language\n    - pytest -v -s models/embedding/vision_language\n    - pytest -v -s models/encoder_decoder/language\n    - pytest -v -s models/encoder_decoder/vision_language\n\n# This test is used only in PR development phase to test individual models and should never run on main\n- label: Custom Models Test\n  optional: true\n  commands:\n    - echo 'Testing custom models...'\n    # PR authors can temporarily add commands below to test individual models\n    # e.g. pytest -v -s models/encoder_decoder/vision_language/test_mllama.py\n    # *To avoid merge conflicts, remember to REMOVE (not just comment out) them before merging the PR*\n\n#####  1 GPU test  #####\n#####  multi gpus test  #####\n\n- label: Distributed Comm Ops Test # 7min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/distributed\n  - tests/distributed\n  commands:\n  - pytest -v -s distributed/test_comm_ops.py\n  - pytest -v -s distributed/test_shm_broadcast.py\n\n- label: 2 Node Tests (4 GPUs in total) # 16min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  num_nodes: 2\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  commands:\n  - # the following commands are for the first node, with ip 192.168.10.10 (ray environment already set up)\n    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py | grep -q 'Same node test passed'\n    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_multi_node_assignment.py\n    - VLLM_MULTI_NODE=1 pytest -v -s distributed/test_pipeline_parallel.py\n  - # the following commands are for the second node, with ip 192.168.10.11 (ray environment already set up)\n    - VLLM_TEST_SAME_HOST=0 torchrun --nnodes 2 --nproc-per-node=2 --rdzv_backend=c10d --rdzv_endpoint=192.168.10.10 distributed/test_same_node.py | grep -q 'Same node test passed'\n\n- label: Distributed Tests (2 GPUs) # 40min\n  #mirror_hardwares: [amd]\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  - vllm/compilation\n  commands:\n  - pytest -v -s ./compile/test_basic_correctness.py\n  - pytest -v -s ./compile/test_wrapper.py\n  - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py | grep -q 'Same node test passed'\n  - TARGET_TEST_SUITE=L4 VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest basic_correctness/ -v -s -m distributed_2_gpus\n  # Avoid importing model tests that cause CUDA reinitialization error\n  - pytest models/encoder_decoder/language/test_bart.py -v -s -m distributed_2_gpus\n  - pytest models/encoder_decoder/vision_language/test_broadcast.py -v -s -m distributed_2_gpus\n  - pytest models/decoder_only/vision_language/test_broadcast.py -v -s -m distributed_2_gpus\n  - pytest -v -s spec_decode/e2e/test_integration_dist_tp2.py\n  - pip install -e ./plugins/vllm_add_dummy_model\n  - pytest -v -s distributed/test_distributed_oot.py\n  - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s test_sharded_state_loader.py\n  - CUDA_VISIBLE_DEVICES=0,1 pytest -v -s distributed/test_utils.py\n\n- label: Multi-step Tests (4 GPUs) # 36min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/model_executor/layers/sampler.py\n  - vllm/sequence.py\n  - vllm/worker/worker_base.py\n  - vllm/worker/worker.py\n  - vllm/worker/multi_step_worker.py\n  - vllm/worker/model_runner_base.py\n  - vllm/worker/model_runner.py\n  - vllm/worker/multi_step_model_runner.py\n  - vllm/engine\n  - tests/multi_step\n  commands:\n  - pytest -v -s multi_step/test_correctness_async_llm.py\n  - pytest -v -s multi_step/test_correctness_llm.py\n\n- label: Pipeline Parallelism Test # 45min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/distributed/\n  - vllm/engine/\n  - vllm/executor/\n  - vllm/model_executor/models/\n  - tests/distributed/\n  commands:\n  - pytest -v -s distributed/test_pp_cudagraph.py\n  - pytest -v -s distributed/test_pipeline_parallel.py\n\n- label: LoRA Long Context (Distributed) # 11min\n  # This test runs llama 13B, so it is required to run on 4 GPUs.\n  num_gpus: 4\n  soft_fail: true\n  source_file_dependencies:\n  - vllm/lora\n  - tests/lora/test_long_context\n  commands:\n    # FIXIT: find out which code initialize cuda before running the test\n    # before the fix, we need to use spawn to test it\n    - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n    - pytest -v -s -x lora/test_long_context.py\n\n- label: Weight Loading Multiple GPU Test  # 33min\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  source_file_dependencies:\n  - vllm/\n  - tests/weight_loading\n  commands:\n    - bash weight_loading/run_model_weight_loading_test.sh -c weight_loading/models.txt\n\n- label: Weight Loading Multiple GPU Test - Large Models # optional\n  working_dir: \"/vllm-workspace/tests\"\n  num_gpus: 2\n  gpu: a100\n  optional: true\n  source_file_dependencies:\n  - vllm/\n  - tests/weight_loading\n  commands:\n    - bash weight_loading/run_model_weight_loading_test.sh -c weight_loading/models-large.txt \n\n\n##### multi gpus test #####\n##### A100 test #####\n\n- label: Distributed Tests (A100) # optional\n  gpu: a100\n  num_gpus: 4\n  source_file_dependencies:\n  - vllm/\n  commands: \n  # NOTE: don't test llama model here, it seems hf implementation is buggy\n  # see https://github.com/vllm-project/vllm/pull/5689 for details\n  - pytest -v -s distributed/test_custom_all_reduce.py\n  - TARGET_TEST_SUITE=A100 pytest basic_correctness/ -v -s -m distributed_2_gpus\n  - pytest -v -s -x lora/test_mixtral.py\n\n- label: LM Eval Large Models # optional\n  gpu: a100\n  num_gpus: 4\n  working_dir: \"/vllm-workspace/.buildkite/lm-eval-harness\"\n  source_file_dependencies:\n  - csrc/\n  - vllm/model_executor/layers/quantization\n  commands:\n  - export VLLM_WORKER_MULTIPROC_METHOD=spawn\n  - bash ./run-tests.sh -c configs/models-large.txt -t 4\n",
      "diff": "diff --git a/.buildkite/test-pipeline.yaml b/.buildkite/test-pipeline.yaml\nindex 398fdc5f0..d2324d7ce 100644\n--- a/.buildkite/test-pipeline.yaml\n+++ b/.buildkite/test-pipeline.yaml\n@@ -77,8 +77,8 @@ steps:\n   - vllm/\n   - tests/basic_correctness/test_chunked_prefill\n   commands:\n-  - VLLM_ATTENTION_BACKEND=XFORMERS VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n-  - VLLM_ATTENTION_BACKEND=FLASH_ATTN VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=XFORMERS pytest -v -s basic_correctness/test_chunked_prefill.py\n+  - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s basic_correctness/test_chunked_prefill.py\n \n - label: Core Test # 10min\n   mirror_hardwares: [amd]\n@@ -88,11 +88,7 @@ steps:\n   - vllm/distributed\n   - tests/core\n   commands:\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core/test_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/test_chunked_prefill_scheduler.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness.py\n-  - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest -v -s core core/block/e2e/test_correctness_sliding_window.py\n-  - pytest -v -s core --ignore=core/block/e2e/test_correctness.py --ignore=core/test_scheduler.py --ignore=core/test_chunked_prefill_scheduler.py --ignore=core/block/e2e/test_correctness.py --ignore=core/block/e2e/test_correctness_sliding_window.py\n+  - pytest -v -s core\n \n - label: Entrypoints Test # 40min\n   working_dir: \"/vllm-workspace/tests\"\n@@ -192,8 +188,7 @@ steps:\n   - vllm/\n   - tests/prefix_caching\n   commands:\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s prefix_caching/test_prefix_caching.py\n-    - pytest -v -s prefix_caching --ignore=prefix_caching/test_prefix_caching.py\n+    - pytest -v -s prefix_caching\n \n - label: Samplers Test # 36min\n   source_file_dependencies:\n@@ -217,8 +212,7 @@ steps:\n   - tests/spec_decode\n   commands:\n     - pytest -v -s spec_decode/e2e/test_multistep_correctness.py\n-    - VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest -v -s spec_decode/e2e/test_compatibility.py\n-    - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py --ignore=spec_decode/e2e/test_compatibility.py\n+    - VLLM_ATTENTION_BACKEND=FLASH_ATTN pytest -v -s spec_decode --ignore=spec_decode/e2e/test_multistep_correctness.py\n \n - label: LoRA Test %N # 15min each\n   mirror_hardwares: [amd]\n@@ -405,7 +399,7 @@ steps:\n   - pytest -v -s ./compile/test_basic_correctness.py\n   - pytest -v -s ./compile/test_wrapper.py\n   - VLLM_TEST_SAME_HOST=1 torchrun --nproc-per-node=4 distributed/test_same_node.py | grep -q 'Same node test passed'\n-  - TARGET_TEST_SUITE=L4 VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1  pytest basic_correctness/ -v -s -m distributed_2_gpus\n+  - TARGET_TEST_SUITE=L4 pytest basic_correctness/ -v -s -m distributed_2_gpus\n   # Avoid importing model tests that cause CUDA reinitialization error\n   - pytest models/encoder_decoder/language/test_bart.py -v -s -m distributed_2_gpus\n   - pytest models/encoder_decoder/vision_language/test_broadcast.py -v -s -m distributed_2_gpus",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 13
    },
    {
      "file_path": "benchmarks/benchmark_latency.py",
      "old_content": "\"\"\"Benchmark the latency of processing a single batch of requests.\"\"\"\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom vllm import LLM, SamplingParams\nfrom vllm.engine.arg_utils import DEVICE_OPTIONS, EngineArgs\nfrom vllm.inputs import PromptType\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.utils import FlexibleArgumentParser\n\n\ndef main(args: argparse.Namespace):\n    print(args)\n\n    # NOTE(woosuk): If the request cannot be processed in a single batch,\n    # the engine will automatically process the request in multiple batches.\n    llm = LLM(\n        model=args.model,\n        speculative_model=args.speculative_model,\n        num_speculative_tokens=args.num_speculative_tokens,\n        speculative_draft_tensor_parallel_size=\\\n            args.speculative_draft_tensor_parallel_size,\n        tokenizer=args.tokenizer,\n        quantization=args.quantization,\n        tensor_parallel_size=args.tensor_parallel_size,\n        trust_remote_code=args.trust_remote_code,\n        dtype=args.dtype,\n        max_model_len=args.max_model_len,\n        enforce_eager=args.enforce_eager,\n        kv_cache_dtype=args.kv_cache_dtype,\n        quantization_param_path=args.quantization_param_path,\n        device=args.device,\n        ray_workers_use_nsight=args.ray_workers_use_nsight,\n        use_v2_block_manager=args.use_v2_block_manager,\n        enable_chunked_prefill=args.enable_chunked_prefill,\n        download_dir=args.download_dir,\n        block_size=args.block_size,\n        gpu_memory_utilization=args.gpu_memory_utilization,\n        load_format=args.load_format,\n        distributed_executor_backend=args.distributed_executor_backend,\n        otlp_traces_endpoint=args.otlp_traces_endpoint,\n        enable_prefix_caching=args.enable_prefix_caching,\n    )\n\n    sampling_params = SamplingParams(\n        n=args.n,\n        temperature=1.0,\n        top_p=1.0,\n        ignore_eos=True,\n        max_tokens=args.output_len,\n    )\n    print(sampling_params)\n    dummy_prompt_token_ids = np.random.randint(10000,\n                                               size=(args.batch_size,\n                                                     args.input_len))\n    dummy_prompts: List[PromptType] = [{\n        \"prompt_token_ids\": batch\n    } for batch in dummy_prompt_token_ids.tolist()]\n\n    def run_to_completion(profile_dir: Optional[str] = None):\n        if profile_dir:\n            with torch.profiler.profile(\n                    activities=[\n                        torch.profiler.ProfilerActivity.CPU,\n                        torch.profiler.ProfilerActivity.CUDA,\n                    ],\n                    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n                        str(profile_dir))) as p:\n                llm.generate(dummy_prompts,\n                             sampling_params=sampling_params,\n                             use_tqdm=False)\n            print(p.key_averages())\n        else:\n            start_time = time.perf_counter()\n            llm.generate(dummy_prompts,\n                         sampling_params=sampling_params,\n                         use_tqdm=False)\n            end_time = time.perf_counter()\n            latency = end_time - start_time\n            return latency\n\n    print(\"Warming up...\")\n    for _ in tqdm(range(args.num_iters_warmup), desc=\"Warmup iterations\"):\n        run_to_completion(profile_dir=None)\n\n    if args.profile:\n        profile_dir = args.profile_result_dir\n        if not profile_dir:\n            profile_dir = Path(\n                \".\"\n            ) / \"vllm_benchmark_result\" / f\"latency_result_{time.time()}\"\n        print(f\"Profiling (results will be saved to '{profile_dir}')...\")\n        run_to_completion(profile_dir=profile_dir)\n        return\n\n    # Benchmark.\n    latencies = []\n    for _ in tqdm(range(args.num_iters), desc=\"Profiling iterations\"):\n        latencies.append(run_to_completion(profile_dir=None))\n    latencies = np.array(latencies)\n    percentages = [10, 25, 50, 75, 90, 99]\n    percentiles = np.percentile(latencies, percentages)\n    print(f'Avg latency: {np.mean(latencies)} seconds')\n    for percentage, percentile in zip(percentages, percentiles):\n        print(f'{percentage}% percentile latency: {percentile} seconds')\n\n    # Output JSON results if specified\n    if args.output_json:\n        results = {\n            \"avg_latency\": np.mean(latencies),\n            \"latencies\": latencies.tolist(),\n            \"percentiles\": dict(zip(percentages, percentiles.tolist())),\n        }\n        with open(args.output_json, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == '__main__':\n    parser = FlexibleArgumentParser(\n        description='Benchmark the latency of processing a single batch of '\n        'requests till completion.')\n    parser.add_argument('--model', type=str, default='facebook/opt-125m')\n    parser.add_argument('--speculative-model', type=str, default=None)\n    parser.add_argument('--num-speculative-tokens', type=int, default=None)\n    parser.add_argument('--speculative-draft-tensor-parallel-size',\n                        '-spec-draft-tp',\n                        type=int,\n                        default=None)\n    parser.add_argument('--tokenizer', type=str, default=None)\n    parser.add_argument('--quantization',\n                        '-q',\n                        choices=[*QUANTIZATION_METHODS, None],\n                        default=None)\n    parser.add_argument('--tensor-parallel-size', '-tp', type=int, default=1)\n    parser.add_argument('--input-len', type=int, default=32)\n    parser.add_argument('--output-len', type=int, default=128)\n    parser.add_argument('--batch-size', type=int, default=8)\n    parser.add_argument('--n',\n                        type=int,\n                        default=1,\n                        help='Number of generated sequences per prompt.')\n    parser.add_argument('--use-beam-search', action='store_true')\n    parser.add_argument('--num-iters-warmup',\n                        type=int,\n                        default=10,\n                        help='Number of iterations to run for warmup.')\n    parser.add_argument('--num-iters',\n                        type=int,\n                        default=30,\n                        help='Number of iterations to run.')\n    parser.add_argument('--trust-remote-code',\n                        action='store_true',\n                        help='trust remote code from huggingface')\n    parser.add_argument(\n        '--max-model-len',\n        type=int,\n        default=None,\n        help='Maximum length of a sequence (including prompt and output). '\n        'If None, will be derived from the model.')\n    parser.add_argument(\n        '--dtype',\n        type=str,\n        default='auto',\n        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],\n        help='data type for model weights and activations. '\n        'The \"auto\" option will use FP16 precision '\n        'for FP32 and FP16 models, and BF16 precision '\n        'for BF16 models.')\n    parser.add_argument('--enforce-eager',\n                        action='store_true',\n                        help='enforce eager mode and disable CUDA graph')\n    parser.add_argument(\n        '--kv-cache-dtype',\n        type=str,\n        choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n        default=\"auto\",\n        help='Data type for kv cache storage. If \"auto\", will use model '\n        'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n        'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n    parser.add_argument(\n        '--quantization-param-path',\n        type=str,\n        default=None,\n        help='Path to the JSON file containing the KV cache scaling factors. '\n        'This should generally be supplied, when KV cache dtype is FP8. '\n        'Otherwise, KV cache scaling factors default to 1.0, which may cause '\n        'accuracy issues. FP8_E5M2 (without scaling) is only supported on '\n        'cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is '\n        'instead supported for common inference criteria.')\n    parser.add_argument(\n        '--profile',\n        action='store_true',\n        help='profile the generation process of a single batch')\n    parser.add_argument(\n        '--profile-result-dir',\n        type=str,\n        default=None,\n        help=('path to save the pytorch profiler output. Can be visualized '\n              'with ui.perfetto.dev or Tensorboard.'))\n    parser.add_argument(\"--device\",\n                        type=str,\n                        default=\"auto\",\n                        choices=DEVICE_OPTIONS,\n                        help='device type for vLLM execution')\n    parser.add_argument('--block-size',\n                        type=int,\n                        default=16,\n                        help='block size of key/value cache')\n    parser.add_argument(\n        '--enable-chunked-prefill',\n        action='store_true',\n        help='If True, the prefill requests can be chunked based on the '\n        'max_num_batched_tokens')\n    parser.add_argument(\"--enable-prefix-caching\",\n                        action='store_true',\n                        help=\"Enable automatic prefix caching\")\n    parser.add_argument('--use-v2-block-manager',\n                        action='store_true',\n                        default=EngineArgs.use_v2_block_manager)\n    parser.add_argument(\n        \"--ray-workers-use-nsight\",\n        action='store_true',\n        help=\"If specified, use nsight to profile ray workers\",\n    )\n    parser.add_argument('--download-dir',\n                        type=str,\n                        default=None,\n                        help='directory to download and load the weights, '\n                        'default to the default cache dir of huggingface')\n    parser.add_argument(\n        '--output-json',\n        type=str,\n        default=None,\n        help='Path to save the latency results in JSON format.')\n    parser.add_argument('--gpu-memory-utilization',\n                        type=float,\n                        default=0.9,\n                        help='the fraction of GPU memory to be used for '\n                        'the model executor, which can range from 0 to 1.'\n                        'If unspecified, will use the default value of 0.9.')\n    parser.add_argument(\n        '--load-format',\n        type=str,\n        default=EngineArgs.load_format,\n        choices=[\n            'auto', 'pt', 'safetensors', 'npcache', 'dummy', 'tensorizer',\n            'bitsandbytes'\n        ],\n        help='The format of the model weights to load.\\n\\n'\n        '* \"auto\" will try to load the weights in the safetensors format '\n        'and fall back to the pytorch bin format if safetensors format '\n        'is not available.\\n'\n        '* \"pt\" will load the weights in the pytorch bin format.\\n'\n        '* \"safetensors\" will load the weights in the safetensors format.\\n'\n        '* \"npcache\" will load the weights in pytorch format and store '\n        'a numpy cache to speed up the loading.\\n'\n        '* \"dummy\" will initialize the weights with random values, '\n        'which is mainly for profiling.\\n'\n        '* \"tensorizer\" will load the weights using tensorizer from '\n        'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n        'section for more information.\\n'\n        '* \"bitsandbytes\" will load the weights using bitsandbytes '\n        'quantization.\\n')\n    parser.add_argument(\n        '--distributed-executor-backend',\n        choices=['ray', 'mp'],\n        default=None,\n        help='Backend to use for distributed serving. When more than 1 GPU '\n        'is used, will be automatically set to \"ray\" if installed '\n        'or \"mp\" (multiprocessing) otherwise.')\n    parser.add_argument(\n        '--otlp-traces-endpoint',\n        type=str,\n        default=None,\n        help='Target URL to which OpenTelemetry traces will be sent.')\n    args = parser.parse_args()\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_latency.py b/benchmarks/benchmark_latency.py\nindex 79a48b2a1..ea1a7788f 100644\n--- a/benchmarks/benchmark_latency.py\n+++ b/benchmarks/benchmark_latency.py\n@@ -38,7 +38,6 @@ def main(args: argparse.Namespace):\n         quantization_param_path=args.quantization_param_path,\n         device=args.device,\n         ray_workers_use_nsight=args.ray_workers_use_nsight,\n-        use_v2_block_manager=args.use_v2_block_manager,\n         enable_chunked_prefill=args.enable_chunked_prefill,\n         download_dir=args.download_dir,\n         block_size=args.block_size,\n@@ -221,9 +220,6 @@ if __name__ == '__main__':\n     parser.add_argument(\"--enable-prefix-caching\",\n                         action='store_true',\n                         help=\"Enable automatic prefix caching\")\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager)\n     parser.add_argument(\n         \"--ray-workers-use-nsight\",\n         action='store_true',",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 5
    },
    {
      "file_path": "benchmarks/benchmark_prefix_caching.py",
      "old_content": "\"\"\"\nBenchmark the efficiency of prefix caching.\n\nThis script allows you to benchmark the performance of\na model with and without prefix caching using either fixed prompts\nor prompts sampled from the ShareGPT dataset.\n\nFixed example usage:\n    python benchmark_prefix_caching.py \\\n        --model meta-llama/Llama-2-7b-chat-hf \\\n        --enable-prefix-caching \\\n        --num-prompts 1 \\\n        --repeat-count 100\n\nShareGPT example usage:\n    # This command samples 20 prompts with input lengths\n    # between 128 and 256 tokens from the ShareGPT dataset,\n    # then replicates each prompt 5 times.\n    python benchmark_prefix_caching.py \\\n        --model meta-llama/Llama-2-7b-chat-hf \\\n        --dataset-path /path/to/ShareGPT_V3_unfiltered_cleaned_split.json \\\n        --enable-prefix-caching \\\n        --num-prompts 20 \\\n        --repeat-count 5 \\\n        --input-length-range 128:256\n\"\"\"\n\nimport json\nimport random\nimport time\nfrom typing import List, Optional, Tuple\n\nfrom transformers import PreTrainedTokenizerBase\n\nfrom vllm import LLM, SamplingParams\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.utils import FlexibleArgumentParser\n\ntry:\n    from vllm.transformers_utils.tokenizer import get_tokenizer\nexcept ImportError:\n    from backend_request_func import get_tokenizer\n\nPROMPT = \"You are a helpful assistant in recognizes the content of tables in markdown format. Here is a table as fellows. You need to answer my question about the table.\\n# Table\\n|Opening|Opening|Sl. No.|Film|Cast|Director|Music Director|Notes|\\n|----|----|----|----|----|----|----|----|\\n|J A N|9|1|Agni Pushpam|Jayabharathi, Kamalahasan|Jeassy|M. K. Arjunan||\\n|J A N|16|2|Priyamvada|Mohan Sharma, Lakshmi, KPAC Lalitha|K. S. Sethumadhavan|V. Dakshinamoorthy||\\n|J A N|23|3|Yakshagaanam|Madhu, Sheela|Sheela|M. S. Viswanathan||\\n|J A N|30|4|Paalkkadal|Sheela, Sharada|T. K. Prasad|A. T. Ummer||\\n|F E B|5|5|Amma|Madhu, Srividya|M. Krishnan Nair|M. K. Arjunan||\\n|F E B|13|6|Appooppan|Thikkurissi Sukumaran Nair, Kamal Haasan|P. Bhaskaran|M. S. Baburaj||\\n|F E B|20|7|Srishti|Chowalloor Krishnankutty, Ravi Alummoodu|K. T. Muhammad|M. S. Baburaj||\\n|F E B|20|8|Vanadevatha|Prem Nazir, Madhubala|Yusufali Kechery|G. Devarajan||\\n|F E B|27|9|Samasya|Madhu, Kamalahaasan|K. Thankappan|Shyam||\\n|F E B|27|10|Yudhabhoomi|K. P. Ummer, Vidhubala|Crossbelt Mani|R. K. Shekhar||\\n|M A R|5|11|Seemantha Puthran|Prem Nazir, Jayabharathi|A. B. Raj|M. K. Arjunan||\\n|M A R|12|12|Swapnadanam|Rani Chandra, Dr. Mohandas|K. G. George|Bhaskar Chandavarkar||\\n|M A R|19|13|Thulavarsham|Prem Nazir, sreedevi, Sudheer|N. Sankaran Nair|V. Dakshinamoorthy||\\n|M A R|20|14|Aruthu|Kaviyoor Ponnamma, Kamalahasan|Ravi|G. Devarajan||\\n|M A R|26|15|Swimming Pool|Kamal Haasan, M. G. Soman|J. Sasikumar|M. K. Arjunan||\\n\\n# Question\\nWhat' s the content in the (1,1) cells\\n\"  # noqa: E501\n\n\ndef test_prefix(llm=None, sampling_params=None, prompts=None):\n    start_time = time.time()\n\n    llm.generate(prompts, sampling_params=sampling_params)\n\n    end_time = time.time()\n    print(f\"cost time {end_time - start_time}\")\n\n\ndef sample_requests(\n    dataset_path: str,\n    num_requests: int,\n    tokenizer: PreTrainedTokenizerBase,\n    input_length_range: Tuple[int, int],\n    fixed_output_len: Optional[int],\n) -> List[Tuple[str, int, int]]:\n    if fixed_output_len is not None and fixed_output_len < 4:\n        raise ValueError(\"output_len too small\")\n\n    # Load the dataset.\n    with open(dataset_path) as f:\n        dataset = json.load(f)\n    # Filter out the conversations with less than 2 turns.\n    dataset = [data for data in dataset if len(data[\"conversations\"]) >= 2]\n    # Only keep the first two turns of each conversation.\n    dataset = [(data[\"conversations\"][0][\"value\"],\n                data[\"conversations\"][1][\"value\"]) for data in dataset]\n\n    # Shuffle the dataset.\n    random.shuffle(dataset)\n\n    min_len, max_len = input_length_range\n\n    # Filter out sequences that are too long or too short\n    filtered_dataset: List[Tuple[str, int, int]] = []\n    for i in range(len(dataset)):\n        if len(filtered_dataset) == num_requests:\n            break\n\n        # Tokenize the prompts and completions.\n        prompt = dataset[i][0]\n        prompt_token_ids = tokenizer(prompt).input_ids\n        completion = dataset[i][1]\n        completion_token_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_token_ids)\n        output_len = len(completion_token_ids\n                         ) if fixed_output_len is None else fixed_output_len\n        if prompt_len < 4 or output_len < 4:\n            # Prune too short sequences.\n            continue\n        if min_len <= prompt_len <= max_len:\n            filtered_dataset.append((prompt, prompt_len, output_len))\n\n    return filtered_dataset\n\n\ndef repeat_and_sort_requests(requests: List[Tuple[str, int, int]],\n                             repeat_count: int,\n                             sort: bool = False) -> List[str]:\n    repeated_requests = requests * repeat_count\n    if sort:\n        repeated_requests.sort(key=lambda x: x[1])\n    else:\n        random.shuffle(repeated_requests)\n    return [req[0] for req in repeated_requests]\n\n\ndef main(args):\n    tokenizer = get_tokenizer(args.model, trust_remote_code=True)\n    input_length_range = tuple(map(int, args.input_length_range.split(':')))\n    random.seed(args.seed)\n    if args.dataset_path is not None:\n        print(f\"Start to sample {args.num_prompts} prompts\"\n              \"from {args.dataset_path}\")\n        filtered_datasets = sample_requests(\n            dataset_path=args.dataset_path,\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            input_length_range=input_length_range,\n            fixed_output_len=args.output_len,\n        )\n    else:\n        prompt_len = len(tokenizer(PROMPT).input_ids)\n        filtered_datasets = [(PROMPT, prompt_len, args.output_len)\n                             ] * args.num_prompts\n\n    llm = LLM(model=args.model,\n              tokenizer_mode='auto',\n              trust_remote_code=True,\n              enforce_eager=True,\n              use_v2_block_manager=args.use_v2_block_manager,\n              tensor_parallel_size=args.tensor_parallel_size,\n              enable_prefix_caching=args.enable_prefix_caching)\n\n    sampling_params = SamplingParams(temperature=0, max_tokens=args.output_len)\n\n    print(\"Testing filtered datasets\")\n    prompts = repeat_and_sort_requests(filtered_datasets,\n                                       repeat_count=args.repeat_count,\n                                       sort=args.sort)\n\n    print(\"------warm up------\")\n    test_prefix(\n        llm=llm,\n        prompts=prompts,\n        sampling_params=sampling_params,\n    )\n\n    print(\"------start generating------\")\n    test_prefix(\n        llm=llm,\n        prompts=prompts,\n        sampling_params=sampling_params,\n    )\n\n\nif __name__ == \"__main__\":\n    parser = FlexibleArgumentParser(\n        description=\n        'Benchmark the performance with or without automatic prefix caching.')\n    parser.add_argument('--model',\n                        type=str,\n                        default='baichuan-inc/Baichuan2-13B-Chat')\n    parser.add_argument(\"--dataset-path\",\n                        type=str,\n                        default=None,\n                        help=\"Path to the dataset.\")\n    parser.add_argument('--tensor-parallel-size', '-tp', type=int, default=1)\n    parser.add_argument('--output-len', type=int, default=10)\n    parser.add_argument('--enable-prefix-caching',\n                        action='store_true',\n                        help='enable prefix caching')\n    parser.add_argument('--use-v2-block-manager',\n                        action='store_true',\n                        default=EngineArgs.use_v2_block_manager,\n                        help='Use BlockSpaceMangerV2')\n    parser.add_argument('--num-prompts',\n                        type=int,\n                        default=1,\n                        help=\"Number of the prompts sampled from dataset\")\n    parser.add_argument('--repeat-count',\n                        type=int,\n                        default=100,\n                        help='Number of times to repeat each prompt')\n    parser.add_argument('--sort',\n                        action='store_true',\n                        help='Sort prompts by input length')\n    parser.add_argument('--input-length-range',\n                        type=str,\n                        default='128:256',\n                        help='Range of input lengths for sampling prompts,'\n                        'specified as \"min:max\" (e.g., \"128:256\").')\n    parser.add_argument(\"--seed\",\n                        type=int,\n                        default=0,\n                        help='Random seed for reproducibility')\n    args = parser.parse_args()\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_prefix_caching.py b/benchmarks/benchmark_prefix_caching.py\nindex f14092d34..a354358e4 100644\n--- a/benchmarks/benchmark_prefix_caching.py\n+++ b/benchmarks/benchmark_prefix_caching.py\n@@ -33,7 +33,6 @@ from typing import List, Optional, Tuple\n from transformers import PreTrainedTokenizerBase\n \n from vllm import LLM, SamplingParams\n-from vllm.engine.arg_utils import EngineArgs\n from vllm.utils import FlexibleArgumentParser\n \n try:\n@@ -134,7 +133,6 @@ def main(args):\n               tokenizer_mode='auto',\n               trust_remote_code=True,\n               enforce_eager=True,\n-              use_v2_block_manager=args.use_v2_block_manager,\n               tensor_parallel_size=args.tensor_parallel_size,\n               enable_prefix_caching=args.enable_prefix_caching)\n \n@@ -176,10 +174,6 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help='Use BlockSpaceMangerV2')\n     parser.add_argument('--num-prompts',\n                         type=int,\n                         default=1,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 7
    },
    {
      "file_path": "benchmarks/benchmark_throughput.py",
      "old_content": "\"\"\"Benchmark offline inference throughput.\"\"\"\nimport argparse\nimport json\nimport random\nimport time\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport uvloop\nfrom tqdm import tqdm\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          PreTrainedTokenizerBase)\n\nfrom vllm.engine.arg_utils import DEVICE_OPTIONS, AsyncEngineArgs, EngineArgs\nfrom vllm.entrypoints.openai.api_server import (\n    build_async_engine_client_from_engine_args)\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.sampling_params import BeamSearchParams\nfrom vllm.utils import FlexibleArgumentParser, merge_async_iterators\n\n\ndef sample_requests(\n    dataset_path: str,\n    num_requests: int,\n    tokenizer: PreTrainedTokenizerBase,\n    fixed_output_len: Optional[int],\n) -> List[Tuple[str, int, int]]:\n    if fixed_output_len is not None and fixed_output_len < 4:\n        raise ValueError(\"output_len too small\")\n\n    # Load the dataset.\n    with open(dataset_path) as f:\n        dataset = json.load(f)\n    # Filter out the conversations with less than 2 turns.\n    dataset = [data for data in dataset if len(data[\"conversations\"]) >= 2]\n    # Only keep the first two turns of each conversation.\n    dataset = [(data[\"conversations\"][0][\"value\"],\n                data[\"conversations\"][1][\"value\"]) for data in dataset]\n\n    # Shuffle the dataset.\n    random.shuffle(dataset)\n\n    # Filter out sequences that are too long or too short\n    filtered_dataset: List[Tuple[str, int, int]] = []\n    for i in range(len(dataset)):\n        if len(filtered_dataset) == num_requests:\n            break\n\n        # Tokenize the prompts and completions.\n        prompt = dataset[i][0]\n        prompt_token_ids = tokenizer(prompt).input_ids\n        completion = dataset[i][1]\n        completion_token_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_token_ids)\n        output_len = len(completion_token_ids\n                         ) if fixed_output_len is None else fixed_output_len\n        if prompt_len < 4 or output_len < 4:\n            # Prune too short sequences.\n            continue\n        if prompt_len > 1024 or prompt_len + output_len > 2048:\n            # Prune too long sequences.\n            continue\n        filtered_dataset.append((prompt, prompt_len, output_len))\n\n    return filtered_dataset\n\n\ndef run_vllm(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tokenizer: str,\n    quantization: Optional[str],\n    tensor_parallel_size: int,\n    seed: int,\n    n: int,\n    trust_remote_code: bool,\n    dtype: str,\n    max_model_len: Optional[int],\n    enforce_eager: bool,\n    kv_cache_dtype: str,\n    quantization_param_path: Optional[str],\n    device: str,\n    enable_prefix_caching: bool,\n    enable_chunked_prefill: bool,\n    max_num_batched_tokens: int,\n    distributed_executor_backend: Optional[str],\n    gpu_memory_utilization: float = 0.9,\n    num_scheduler_steps: int = 1,\n    use_v2_block_manager: bool = False,\n    download_dir: Optional[str] = None,\n    load_format: str = EngineArgs.load_format,\n    disable_async_output_proc: bool = False,\n) -> float:\n    from vllm import LLM, SamplingParams\n    llm = LLM(\n        model=model,\n        tokenizer=tokenizer,\n        quantization=quantization,\n        tensor_parallel_size=tensor_parallel_size,\n        seed=seed,\n        trust_remote_code=trust_remote_code,\n        dtype=dtype,\n        max_model_len=max_model_len,\n        gpu_memory_utilization=gpu_memory_utilization,\n        enforce_eager=enforce_eager,\n        kv_cache_dtype=kv_cache_dtype,\n        quantization_param_path=quantization_param_path,\n        device=device,\n        enable_prefix_caching=enable_prefix_caching,\n        download_dir=download_dir,\n        enable_chunked_prefill=enable_chunked_prefill,\n        max_num_batched_tokens=max_num_batched_tokens,\n        distributed_executor_backend=distributed_executor_backend,\n        load_format=load_format,\n        num_scheduler_steps=num_scheduler_steps,\n        use_v2_block_manager=use_v2_block_manager,\n        disable_async_output_proc=disable_async_output_proc,\n    )\n\n    # Add the requests to the engine.\n    prompts: List[str] = []\n    sampling_params: List[SamplingParams] = []\n    for prompt, _, output_len in requests:\n        prompts.append(prompt)\n        sampling_params.append(\n            SamplingParams(\n                n=n,\n                temperature=1.0,\n                top_p=1.0,\n                ignore_eos=True,\n                max_tokens=output_len,\n            ))\n\n    use_beam_search = False\n\n    if not use_beam_search:\n        start = time.perf_counter()\n        llm.generate(prompts, sampling_params, use_tqdm=True)\n        end = time.perf_counter()\n    else:\n        prompts = [prompt for prompt, _, _ in requests]\n        # output_len should be the same for all requests.\n        output_len = requests[0][2]\n        for prompt, input_len, _output_len in requests:\n            assert _output_len == output_len\n        start = time.perf_counter()\n        llm.beam_search(\n            prompts,\n            BeamSearchParams(\n                beam_width=n,\n                max_tokens=output_len,\n                ignore_eos=True,\n            ))\n        end = time.perf_counter()\n    return end - start\n\n\nasync def run_vllm_async(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tokenizer: str,\n    quantization: Optional[str],\n    tensor_parallel_size: int,\n    seed: int,\n    n: int,\n    trust_remote_code: bool,\n    dtype: str,\n    max_model_len: Optional[int],\n    enforce_eager: bool,\n    kv_cache_dtype: str,\n    quantization_param_path: Optional[str],\n    device: str,\n    enable_prefix_caching: bool,\n    enable_chunked_prefill: bool,\n    max_num_batched_tokens: int,\n    distributed_executor_backend: Optional[str],\n    gpu_memory_utilization: float = 0.9,\n    num_scheduler_steps: int = 1,\n    use_v2_block_manager: bool = False,\n    download_dir: Optional[str] = None,\n    load_format: str = EngineArgs.load_format,\n    disable_async_output_proc: bool = False,\n    disable_frontend_multiprocessing: bool = False,\n) -> float:\n    from vllm import SamplingParams\n    engine_args = AsyncEngineArgs(\n        model=model,\n        tokenizer=tokenizer,\n        quantization=quantization,\n        tensor_parallel_size=tensor_parallel_size,\n        seed=seed,\n        trust_remote_code=trust_remote_code,\n        dtype=dtype,\n        max_model_len=max_model_len,\n        gpu_memory_utilization=gpu_memory_utilization,\n        enforce_eager=enforce_eager,\n        kv_cache_dtype=kv_cache_dtype,\n        quantization_param_path=quantization_param_path,\n        device=device,\n        enable_prefix_caching=enable_prefix_caching,\n        download_dir=download_dir,\n        enable_chunked_prefill=enable_chunked_prefill,\n        max_num_batched_tokens=max_num_batched_tokens,\n        distributed_executor_backend=distributed_executor_backend,\n        load_format=load_format,\n        num_scheduler_steps=num_scheduler_steps,\n        use_v2_block_manager=use_v2_block_manager,\n        disable_async_output_proc=disable_async_output_proc,\n        worker_use_ray=False,\n        disable_log_requests=True,\n    )\n\n    async with build_async_engine_client_from_engine_args(\n            engine_args, disable_frontend_multiprocessing) as llm:\n\n        # Add the requests to the engine.\n        prompts: List[str] = []\n        sampling_params: List[SamplingParams] = []\n        for prompt, _, output_len in requests:\n            prompts.append(prompt)\n            sampling_params.append(\n                SamplingParams(\n                    n=n,\n                    temperature=1.0,\n                    top_p=1.0,\n                    ignore_eos=True,\n                    max_tokens=output_len,\n                ))\n\n        generators = []\n        start = time.perf_counter()\n        for i, (prompt, sp) in enumerate(zip(prompts, sampling_params)):\n            generator = llm.generate(prompt, sp, request_id=f\"test{i}\")\n            generators.append(generator)\n        all_gens = merge_async_iterators(*generators)\n        async for i, res in all_gens:\n            pass\n        end = time.perf_counter()\n        return end - start\n\n\ndef run_hf(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tokenizer: PreTrainedTokenizerBase,\n    n: int,\n    max_batch_size: int,\n    trust_remote_code: bool,\n) -> float:\n    llm = AutoModelForCausalLM.from_pretrained(\n        model, torch_dtype=torch.float16, trust_remote_code=trust_remote_code)\n    if llm.config.model_type == \"llama\":\n        # To enable padding in the HF backend.\n        tokenizer.pad_token = tokenizer.eos_token\n    llm = llm.cuda()\n\n    pbar = tqdm(total=len(requests))\n    start = time.perf_counter()\n    batch: List[str] = []\n    max_prompt_len = 0\n    max_output_len = 0\n    for i in range(len(requests)):\n        prompt, prompt_len, output_len = requests[i]\n        # Add the prompt to the batch.\n        batch.append(prompt)\n        max_prompt_len = max(max_prompt_len, prompt_len)\n        max_output_len = max(max_output_len, output_len)\n        if len(batch) < max_batch_size and i != len(requests) - 1:\n            # Check if we can add more requests to the batch.\n            _, next_prompt_len, next_output_len = requests[i + 1]\n            if (max(max_prompt_len, next_prompt_len) +\n                    max(max_output_len, next_output_len)) <= 2048:\n                # We can add more requests to the batch.\n                continue\n\n        # Generate the sequences.\n        input_ids = tokenizer(batch, return_tensors=\"pt\",\n                              padding=True).input_ids\n        llm_outputs = llm.generate(\n            input_ids=input_ids.cuda(),\n            do_sample=True,\n            num_return_sequences=n,\n            temperature=1.0,\n            top_p=1.0,\n            use_cache=True,\n            max_new_tokens=max_output_len,\n        )\n        # Include the decoding time.\n        tokenizer.batch_decode(llm_outputs, skip_special_tokens=True)\n        pbar.update(len(batch))\n\n        # Clear the batch.\n        batch = []\n        max_prompt_len = 0\n        max_output_len = 0\n    end = time.perf_counter()\n    return end - start\n\n\ndef run_mii(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tensor_parallel_size: int,\n    output_len: int,\n) -> float:\n    from mii import client, serve\n    llm = serve(model, tensor_parallel=tensor_parallel_size)\n    prompts = [prompt for prompt, _, _ in requests]\n\n    start = time.perf_counter()\n    llm.generate(prompts, max_new_tokens=output_len)\n    end = time.perf_counter()\n    client = client(model)\n    client.terminate_server()\n    return end - start\n\n\ndef main(args: argparse.Namespace):\n    print(args)\n    random.seed(args.seed)\n\n    # Sample the requests.\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.tokenizer, trust_remote_code=args.trust_remote_code)\n    if args.dataset is None:\n        # Synthesize a prompt with the given input length.\n        prompt = \"hi\" * (args.input_len - 1)\n        requests = [(prompt, args.input_len, args.output_len)\n                    for _ in range(args.num_prompts)]\n    else:\n        requests = sample_requests(args.dataset, args.num_prompts, tokenizer,\n                                   args.output_len)\n\n    if args.backend == \"vllm\":\n        run_args = [\n            requests, args.model, args.tokenizer, args.quantization,\n            args.tensor_parallel_size, args.seed, args.n,\n            args.trust_remote_code, args.dtype, args.max_model_len,\n            args.enforce_eager, args.kv_cache_dtype,\n            args.quantization_param_path, args.device,\n            args.enable_prefix_caching, args.enable_chunked_prefill,\n            args.max_num_batched_tokens, args.distributed_executor_backend,\n            args.gpu_memory_utilization, args.num_scheduler_steps,\n            args.use_v2_block_manager, args.download_dir, args.load_format,\n            args.disable_async_output_proc\n        ]\n\n        if args.async_engine:\n            run_args.append(args.disable_frontend_multiprocessing)\n            elapsed_time = uvloop.run(run_vllm_async(*run_args))\n        else:\n            elapsed_time = run_vllm(*run_args)\n    elif args.backend == \"hf\":\n        assert args.tensor_parallel_size == 1\n        elapsed_time = run_hf(requests, args.model, tokenizer, args.n,\n                              args.hf_max_batch_size, args.trust_remote_code)\n    elif args.backend == \"mii\":\n        elapsed_time = run_mii(requests, args.model, args.tensor_parallel_size,\n                               args.output_len)\n    else:\n        raise ValueError(f\"Unknown backend: {args.backend}\")\n    total_num_tokens = sum(prompt_len + output_len\n                           for _, prompt_len, output_len in requests)\n    print(f\"Throughput: {len(requests) / elapsed_time:.2f} requests/s, \"\n          f\"{total_num_tokens / elapsed_time:.2f} tokens/s\")\n\n    # Output JSON results if specified\n    if args.output_json:\n        results = {\n            \"elapsed_time\": elapsed_time,\n            \"num_requests\": len(requests),\n            \"total_num_tokens\": total_num_tokens,\n            \"requests_per_second\": len(requests) / elapsed_time,\n            \"tokens_per_second\": total_num_tokens / elapsed_time,\n        }\n        with open(args.output_json, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == \"__main__\":\n    parser = FlexibleArgumentParser(description=\"Benchmark the throughput.\")\n    parser.add_argument(\"--backend\",\n                        type=str,\n                        choices=[\"vllm\", \"hf\", \"mii\"],\n                        default=\"vllm\")\n    parser.add_argument(\"--dataset\",\n                        type=str,\n                        default=None,\n                        help=\"Path to the dataset.\")\n    parser.add_argument(\"--input-len\",\n                        type=int,\n                        default=None,\n                        help=\"Input prompt length for each request\")\n    parser.add_argument(\"--output-len\",\n                        type=int,\n                        default=None,\n                        help=\"Output length for each request. Overrides the \"\n                        \"output length from the dataset.\")\n    parser.add_argument(\"--model\", type=str, default=\"facebook/opt-125m\")\n    parser.add_argument(\"--tokenizer\", type=str, default=None)\n    parser.add_argument('--quantization',\n                        '-q',\n                        choices=[*QUANTIZATION_METHODS, None],\n                        default=None)\n    parser.add_argument(\"--tensor-parallel-size\", \"-tp\", type=int, default=1)\n    parser.add_argument(\"--n\",\n                        type=int,\n                        default=1,\n                        help=\"Number of generated sequences per prompt.\")\n    parser.add_argument(\"--num-prompts\",\n                        type=int,\n                        default=1000,\n                        help=\"Number of prompts to process.\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--hf-max-batch-size\",\n                        type=int,\n                        default=None,\n                        help=\"Maximum batch size for HF backend.\")\n    parser.add_argument('--trust-remote-code',\n                        action='store_true',\n                        help='trust remote code from huggingface')\n    parser.add_argument(\n        '--max-model-len',\n        type=int,\n        default=None,\n        help='Maximum length of a sequence (including prompt and output). '\n        'If None, will be derived from the model.')\n    parser.add_argument(\n        '--dtype',\n        type=str,\n        default='auto',\n        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],\n        help='data type for model weights and activations. '\n        'The \"auto\" option will use FP16 precision '\n        'for FP32 and FP16 models, and BF16 precision '\n        'for BF16 models.')\n    parser.add_argument('--gpu-memory-utilization',\n                        type=float,\n                        default=0.9,\n                        help='the fraction of GPU memory to be used for '\n                        'the model executor, which can range from 0 to 1.'\n                        'If unspecified, will use the default value of 0.9.')\n    parser.add_argument(\"--enforce-eager\",\n                        action=\"store_true\",\n                        help=\"enforce eager execution\")\n    parser.add_argument(\n        '--kv-cache-dtype',\n        type=str,\n        choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n        default=\"auto\",\n        help='Data type for kv cache storage. If \"auto\", will use model '\n        'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n        'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n    parser.add_argument(\n        '--quantization-param-path',\n        type=str,\n        default=None,\n        help='Path to the JSON file containing the KV cache scaling factors. '\n        'This should generally be supplied, when KV cache dtype is FP8. '\n        'Otherwise, KV cache scaling factors default to 1.0, which may cause '\n        'accuracy issues. FP8_E5M2 (without scaling) is only supported on '\n        'cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is '\n        'instead supported for common inference criteria.')\n    parser.add_argument(\"--device\",\n                        type=str,\n                        default=\"auto\",\n                        choices=DEVICE_OPTIONS,\n                        help='device type for vLLM execution')\n    parser.add_argument(\n        \"--num-scheduler-steps\",\n        type=int,\n        default=1,\n        help=\"Maximum number of forward steps per scheduler call.\")\n    parser.add_argument(\"--use-v2-block-manager\",\n                        action='store_true',\n                        default=EngineArgs.use_v2_block_manager,\n                        help=\"Enable block manager v2.\")\n    parser.add_argument(\n        \"--enable-prefix-caching\",\n        action='store_true',\n        help=\"Enable automatic prefix caching for vLLM backend.\")\n    parser.add_argument(\"--enable-chunked-prefill\",\n                        action='store_true',\n                        help=\"enable chunked prefill for vLLM backend.\")\n    parser.add_argument('--max-num-batched-tokens',\n                        type=int,\n                        default=None,\n                        help='maximum number of batched tokens per '\n                        'iteration')\n    parser.add_argument('--download-dir',\n                        type=str,\n                        default=None,\n                        help='directory to download and load the weights, '\n                        'default to the default cache dir of huggingface')\n    parser.add_argument(\n        '--output-json',\n        type=str,\n        default=None,\n        help='Path to save the throughput results in JSON format.')\n    parser.add_argument(\n        '--distributed-executor-backend',\n        choices=['ray', 'mp'],\n        default=None,\n        help='Backend to use for distributed serving. When more than 1 GPU '\n        'is used, will be automatically set to \"ray\" if installed '\n        'or \"mp\" (multiprocessing) otherwise.')\n    parser.add_argument(\n        '--load-format',\n        type=str,\n        default=EngineArgs.load_format,\n        choices=[\n            'auto', 'pt', 'safetensors', 'npcache', 'dummy', 'tensorizer',\n            'bitsandbytes'\n        ],\n        help='The format of the model weights to load.\\n\\n'\n        '* \"auto\" will try to load the weights in the safetensors format '\n        'and fall back to the pytorch bin format if safetensors format '\n        'is not available.\\n'\n        '* \"pt\" will load the weights in the pytorch bin format.\\n'\n        '* \"safetensors\" will load the weights in the safetensors format.\\n'\n        '* \"npcache\" will load the weights in pytorch format and store '\n        'a numpy cache to speed up the loading.\\n'\n        '* \"dummy\" will initialize the weights with random values, '\n        'which is mainly for profiling.\\n'\n        '* \"tensorizer\" will load the weights using tensorizer from '\n        'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n        'section for more information.\\n'\n        '* \"bitsandbytes\" will load the weights using bitsandbytes '\n        'quantization.\\n')\n    parser.add_argument(\n        \"--disable-async-output-proc\",\n        action='store_true',\n        default=False,\n        help=\"Disable async output processor for vLLM backend.\")\n    parser.add_argument(\"--async-engine\",\n                        action='store_true',\n                        default=False,\n                        help=\"Use vLLM async engine rather than LLM class.\")\n    parser.add_argument(\"--disable-frontend-multiprocessing\",\n                        action='store_true',\n                        default=False,\n                        help=\"Disable decoupled async engine frontend.\")\n    args = parser.parse_args()\n    if args.tokenizer is None:\n        args.tokenizer = args.model\n    if args.dataset is None:\n        assert args.input_len is not None\n        assert args.output_len is not None\n    else:\n        assert args.input_len is None\n\n    if args.backend == \"vllm\":\n        if args.hf_max_batch_size is not None:\n            raise ValueError(\"HF max batch size is only for HF backend.\")\n    elif args.backend == \"hf\":\n        if args.hf_max_batch_size is None:\n            raise ValueError(\"HF max batch size is required for HF backend.\")\n        if args.quantization is not None:\n            raise ValueError(\"Quantization is only for vLLM backend.\")\n    elif args.backend == \"mii\":\n        if args.dtype != \"auto\":\n            raise ValueError(\"dtype must be auto for MII backend.\")\n        if args.n != 1:\n            raise ValueError(\"n must be 1 for MII backend.\")\n        if args.quantization is not None:\n            raise ValueError(\"Quantization is only for vLLM backend.\")\n        if args.hf_max_batch_size is not None:\n            raise ValueError(\"HF max batch size is only for HF backend.\")\n        if args.tokenizer != args.model:\n            raise ValueError(\"Tokenizer must be the same as the model for MII \"\n                             \"backend.\")\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py\nindex b7bc2a640..e26706af6 100644\n--- a/benchmarks/benchmark_throughput.py\n+++ b/benchmarks/benchmark_throughput.py\n@@ -86,7 +86,6 @@ def run_vllm(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -113,7 +112,6 @@ def run_vllm(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n     )\n \n@@ -176,7 +174,6 @@ async def run_vllm_async(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     num_scheduler_steps: int = 1,\n-    use_v2_block_manager: bool = False,\n     download_dir: Optional[str] = None,\n     load_format: str = EngineArgs.load_format,\n     disable_async_output_proc: bool = False,\n@@ -204,7 +201,6 @@ async def run_vllm_async(\n         distributed_executor_backend=distributed_executor_backend,\n         load_format=load_format,\n         num_scheduler_steps=num_scheduler_steps,\n-        use_v2_block_manager=use_v2_block_manager,\n         disable_async_output_proc=disable_async_output_proc,\n         worker_use_ray=False,\n         disable_log_requests=True,\n@@ -341,8 +337,7 @@ def main(args: argparse.Namespace):\n             args.enable_prefix_caching, args.enable_chunked_prefill,\n             args.max_num_batched_tokens, args.distributed_executor_backend,\n             args.gpu_memory_utilization, args.num_scheduler_steps,\n-            args.use_v2_block_manager, args.download_dir, args.load_format,\n-            args.disable_async_output_proc\n+            args.download_dir, args.load_format, args.disable_async_output_proc\n         ]\n \n         if args.async_engine:\n@@ -471,10 +466,6 @@ if __name__ == \"__main__\":\n         type=int,\n         default=1,\n         help=\"Maximum number of forward steps per scheduler call.\")\n-    parser.add_argument(\"--use-v2-block-manager\",\n-                        action='store_true',\n-                        default=EngineArgs.use_v2_block_manager,\n-                        help=\"Enable block manager v2.\")\n     parser.add_argument(\n         \"--enable-prefix-caching\",\n         action='store_true',",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 11
    },
    {
      "file_path": "benchmarks/overheads/benchmark_hashing.py",
      "old_content": "import cProfile\nimport pstats\n\nfrom vllm import LLM, SamplingParams\nfrom vllm.utils import FlexibleArgumentParser\n\n# A very long prompt, total number of tokens is about 15k.\nLONG_PROMPT = [\"You are an expert in large language models, aren't you?\"\n               ] * 1000\nLONG_PROMPT = ' '.join(LONG_PROMPT)\n\n\ndef main(args):\n    llm = LLM(\n        model=args.model,\n        enforce_eager=True,\n        enable_prefix_caching=True,\n        tensor_parallel_size=args.tensor_parallel_size,\n        use_v2_block_manager=args.use_v2_block_manager,\n    )\n\n    sampling_params = SamplingParams(temperature=0, max_tokens=args.output_len)\n    profiler = cProfile.Profile()\n\n    print(\"------warm up------\")\n    for i in range(3):\n        output = llm.generate(LONG_PROMPT, sampling_params)\n        print(output[0].outputs[0].text)\n\n    print(\"------start generating------\")\n    for i in range(3):\n        profiler.runctx('llm.generate(LONG_PROMPT, sampling_params)',\n                        globals(), locals())\n\n    # analyze the runtime of hashing function\n    stats = pstats.Stats(profiler)\n    stats.sort_stats('cumulative')\n    total_time = 0\n    total_calls = 0\n    for func in stats.stats:\n        if 'hash_of_block' in func[2]:\n            total_time = stats.stats[func][3]\n            total_calls = stats.stats[func][0]\n    percentage = (total_time / stats.total_tt) * 100\n    print(f\"Hashing took {total_time:.2f} seconds,\"\n          f\"{percentage:.2f}% of the total runtime.\")\n\n\nif __name__ == \"__main__\":\n    parser = FlexibleArgumentParser(\n        description='Benchmark the performance of hashing function in'\n        'automatic prefix caching.')\n    parser.add_argument('--model', type=str, default='lmsys/longchat-7b-16k')\n    parser.add_argument('--tensor-parallel-size', '-tp', type=int, default=1)\n    parser.add_argument('--output-len', type=int, default=10)\n    parser.add_argument('--enable-prefix-caching',\n                        action='store_true',\n                        help='enable prefix caching')\n    parser.add_argument('--use-v2-block-manager',\n                        action='store_true',\n                        help='Use BlockSpaceMangerV2')\n    args = parser.parse_args()\n    main(args)\n",
      "diff": "diff --git a/benchmarks/overheads/benchmark_hashing.py b/benchmarks/overheads/benchmark_hashing.py\nindex 203699e9a..d16d6f9fb 100644\n--- a/benchmarks/overheads/benchmark_hashing.py\n+++ b/benchmarks/overheads/benchmark_hashing.py\n@@ -16,7 +16,6 @@ def main(args):\n         enforce_eager=True,\n         enable_prefix_caching=True,\n         tensor_parallel_size=args.tensor_parallel_size,\n-        use_v2_block_manager=args.use_v2_block_manager,\n     )\n \n     sampling_params = SamplingParams(temperature=0, max_tokens=args.output_len)\n@@ -56,8 +55,5 @@ if __name__ == \"__main__\":\n     parser.add_argument('--enable-prefix-caching',\n                         action='store_true',\n                         help='enable prefix caching')\n-    parser.add_argument('--use-v2-block-manager',\n-                        action='store_true',\n-                        help='Use BlockSpaceMangerV2')\n     args = parser.parse_args()\n     main(args)",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 5
    },
    {
      "file_path": "docs/source/models/spec_decode.rst",
      "old_content": ".. _spec_decode:\n\nSpeculative decoding in vLLM\n============================\n\n.. warning::\n    Please note that speculative decoding in vLLM is not yet optimized and does\n    not usually yield inter-token latency reductions for all prompt datasets or sampling parameters. The work\n    to optimize it is ongoing and can be followed in `this issue. <https://github.com/vllm-project/vllm/issues/4630>`_\n\nThis document shows how to use `Speculative Decoding <https://x.com/karpathy/status/1697318534555336961>`_ with vLLM.\nSpeculative decoding is a technique which improves inter-token latency in memory-bound LLM inference.\n\nSpeculating with a draft model\n------------------------------\n\nThe following code configures vLLM in an offline mode to use speculative decoding with a draft model, speculating 5 tokens at a time.\n\n.. code-block:: python\n\n    from vllm import LLM, SamplingParams\n\n    prompts = [\n        \"The future of AI is\",\n    ]\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    llm = LLM(\n        model=\"facebook/opt-6.7b\",\n        tensor_parallel_size=1,\n        speculative_model=\"facebook/opt-125m\",\n        num_speculative_tokens=5,\n        use_v2_block_manager=True,\n    )\n    outputs = llm.generate(prompts, sampling_params)\n\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nTo perform the same with an online mode launch the server:\n\n.. code-block:: bash\n\n    python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 --model facebook/opt-6.7b \\\n        --seed 42 -tp 1 --speculative_model facebook/opt-125m --use-v2-block-manager \\\n        --num_speculative_tokens 5 --gpu_memory_utilization 0.8\n\nThen use a client:\n\n.. code-block:: python\n\n    from openai import OpenAI\n\n    # Modify OpenAI's API key and API base to use vLLM's API server.\n    openai_api_key = \"EMPTY\"\n    openai_api_base = \"http://localhost:8000/v1\"\n\n    client = OpenAI(\n        # defaults to os.environ.get(\"OPENAI_API_KEY\")\n        api_key=openai_api_key,\n        base_url=openai_api_base,\n    )\n\n    models = client.models.list()\n    model = models.data[0].id\n\n    # Completion API\n    stream = False\n    completion = client.completions.create(\n        model=model,\n        prompt=\"The future of AI is\",\n        echo=False,\n        n=1,\n        stream=stream,\n    )\n\n    print(\"Completion results:\")\n    if stream:\n        for c in completion:\n            print(c)\n    else:\n        print(completion)\n\nSpeculating by matching n-grams in the prompt\n---------------------------------------------\n\nThe following code configures vLLM to use speculative decoding where proposals are generated by\nmatching n-grams in the prompt. For more information read `this thread. <https://x.com/joao_gante/status/1747322413006643259>`_\n\n.. code-block:: python\n\n    from vllm import LLM, SamplingParams\n\n    prompts = [\n        \"The future of AI is\",\n    ]\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    llm = LLM(\n        model=\"facebook/opt-6.7b\",\n        tensor_parallel_size=1,\n        speculative_model=\"[ngram]\",\n        num_speculative_tokens=5,\n        ngram_prompt_lookup_max=4,\n        use_v2_block_manager=True,\n    )\n    outputs = llm.generate(prompts, sampling_params)\n\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nSpeculating using MLP speculators\n---------------------------------\n\nThe following code configures vLLM to use speculative decoding where proposals are generated by\ndraft models that conditioning draft predictions on both context vectors and sampled tokens.\nFor more information see `this blog <https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/>`_ or\n`this technical report <https://arxiv.org/abs/2404.19124>`_.\n\n.. code-block:: python\n\n    from vllm import LLM, SamplingParams\n\n    prompts = [\n        \"The future of AI is\",\n    ]\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    llm = LLM(\n        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n        tensor_parallel_size=4,\n        speculative_model=\"ibm-fms/llama3-70b-accelerator\",\n        speculative_draft_tensor_parallel_size=1,\n        use_v2_block_manager=True,\n    )\n    outputs = llm.generate(prompts, sampling_params)\n\n    for output in outputs:\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\nNote that these speculative models currently need to be run without tensor parallelism, although\nit is possible to run the main model using tensor parallelism (see example above). Since the\nspeculative models are relatively small, we still see significant speedups. However, this\nlimitation will be fixed in a future release.\n\nA variety of speculative models of this type are available on HF hub:\n\n* `llama-13b-accelerator <https://huggingface.co/ibm-fms/llama-13b-accelerator>`_\n* `llama3-8b-accelerator <https://huggingface.co/ibm-fms/llama3-8b-accelerator>`_\n* `codellama-34b-accelerator <https://huggingface.co/ibm-fms/codellama-34b-accelerator>`_\n* `llama2-70b-accelerator <https://huggingface.co/ibm-fms/llama2-70b-accelerator>`_\n* `llama3-70b-accelerator <https://huggingface.co/ibm-fms/llama3-70b-accelerator>`_\n* `granite-3b-code-instruct-accelerator <https://huggingface.co/ibm-granite/granite-3b-code-instruct-accelerator>`_\n* `granite-8b-code-instruct-accelerator <https://huggingface.co/ibm-granite/granite-8b-code-instruct-accelerator>`_\n* `granite-7b-instruct-accelerator <https://huggingface.co/ibm-granite/granite-7b-instruct-accelerator>`_\n* `granite-20b-code-instruct-accelerator <https://huggingface.co/ibm-granite/granite-20b-code-instruct-accelerator>`_\n\nLossless guarantees of Speculative Decoding\n-------------------------------------------\nIn vLLM, speculative decoding aims to enhance inference efficiency while maintaining accuracy. This section addresses the lossless guarantees of \nspeculative decoding, breaking down the guarantees into three key areas:\n\n1. **Theoretical Losslessness**\n   - Speculative decoding sampling is theoretically lossless up to the precision limits of hardware numerics. Floating-point errors might \n   cause slight variations in output distributions, as discussed \n   in `Accelerating Large Language Model Decoding with Speculative Sampling <https://arxiv.org/pdf/2302.01318>`_\n\n2. **Algorithmic Losslessness**\n   - vLLMs implementation of speculative decoding is algorithmically validated to be lossless. Key validation tests include:\n\n    - **Rejection Sampler Convergence**: Ensures that samples from vLLMs rejection sampler align with the target \n      distribution. `View Test Code <https://github.com/vllm-project/vllm/blob/47b65a550866c7ffbd076ecb74106714838ce7da/tests/samplers/test_rejection_sampler.py#L252>`_\n\n    - **Greedy Sampling Equality**: Confirms that greedy sampling with speculative decoding matches greedy sampling\n      without it. This verifies that vLLM's speculative decoding framework, when integrated with the vLLM forward pass and the vLLM rejection sampler, \n      provides a lossless guarantee.  Almost all of the tests in `this directory <https://github.com/vllm-project/vllm/tree/b67ae00cdbbe1a58ffc8ff170f0c8d79044a684a/tests/spec_decode/e2e>`_\n      verify this property using `this assertion implementation <https://github.com/vllm-project/vllm/blob/b67ae00cdbbe1a58ffc8ff170f0c8d79044a684a/tests/spec_decode/e2e/conftest.py#L291>`_\n\n3. **vLLM Logprob Stability**\n   - vLLM does not currently guarantee stable token log probabilities (logprobs). This can result in different outputs for the \n   same request across runs. For more details, see the FAQ section \n   titled *Can the output of a prompt vary across runs in vLLM?* in the `FAQs <../serving/faq.rst>`_.\n\n\n**Conclusion**\n\nWhile vLLM strives to ensure losslessness in speculative decoding, variations in generated outputs with and without speculative decoding \ncan occur due to following factors:\n\n- **Floating-Point Precision**: Differences in hardware numerical precision may lead to slight discrepancies in the output distribution.\n\n- **Batch Size and Numerical Stability**: Changes in batch size may cause variations in logprobs and output probabilities, potentially \n  due to non-deterministic behavior in batched operations or numerical instability.\n\n**Mitigation Strategies**\n\nFor mitigation strategies, please refer to the FAQ entry *Can the output of a prompt vary across runs in vLLM?* in the `FAQs <../serving/faq.rst>`_.\n\nResources for vLLM contributors\n-------------------------------\n* `A Hacker's Guide to Speculative Decoding in vLLM <https://www.youtube.com/watch?v=9wNAgpX6z_4>`_\n* `What is Lookahead Scheduling in vLLM? <https://docs.google.com/document/d/1Z9TvqzzBPnh5WHcRwjvK2UEeFeq5zMZb5mFE8jR0HCs/edit#heading=h.1fjfb0donq5a>`_\n* `Information on batch expansion <https://docs.google.com/document/d/1T-JaS2T1NRfdP51qzqpyakoCXxSXTtORppiwaj5asxA/edit#heading=h.kk7dq05lc6q8>`_\n* `Dynamic speculative decoding <https://github.com/vllm-project/vllm/issues/4565>`_\n",
      "diff": "diff --git a/docs/source/models/spec_decode.rst b/docs/source/models/spec_decode.rst\nindex 0dc9cb383..b02c80aeb 100644\n--- a/docs/source/models/spec_decode.rst\n+++ b/docs/source/models/spec_decode.rst\n@@ -30,7 +30,6 @@ The following code configures vLLM in an offline mode to use speculative decodin\n         tensor_parallel_size=1,\n         speculative_model=\"facebook/opt-125m\",\n         num_speculative_tokens=5,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -104,7 +103,6 @@ matching n-grams in the prompt. For more information read `this thread. <https:/\n         speculative_model=\"[ngram]\",\n         num_speculative_tokens=5,\n         ngram_prompt_lookup_max=4,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)\n \n@@ -135,7 +133,6 @@ For more information see `this blog <https://pytorch.org/blog/hitchhikers-guide-\n         tensor_parallel_size=4,\n         speculative_model=\"ibm-fms/llama3-70b-accelerator\",\n         speculative_draft_tensor_parallel_size=1,\n-        use_v2_block_manager=True,\n     )\n     outputs = llm.generate(prompts, sampling_params)",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 4
    },
    {
      "file_path": "examples/offline_inference_mlpspeculator.py",
      "old_content": "import gc\nimport time\nfrom typing import List\n\nfrom vllm import LLM, SamplingParams\n\n\ndef time_generation(llm: LLM, prompts: List[str],\n                    sampling_params: SamplingParams):\n    # Generate texts from the prompts. The output is a list of RequestOutput\n    # objects that contain the prompt, generated text, and other information.\n    # Warmup first\n    llm.generate(prompts, sampling_params)\n    llm.generate(prompts, sampling_params)\n    start = time.time()\n    outputs = llm.generate(prompts, sampling_params)\n    end = time.time()\n    print((end - start) / sum([len(o.outputs[0].token_ids) for o in outputs]))\n    # Print the outputs.\n    for output in outputs:\n        generated_text = output.outputs[0].text\n        print(f\"text: {generated_text!r}\")\n\n\nif __name__ == \"__main__\":\n\n    template = (\n        \"Below is an instruction that describes a task. Write a response \"\n        \"that appropriately completes the request.\\n\\n### Instruction:\\n{}\"\n        \"\\n\\n### Response:\\n\")\n\n    # Sample prompts.\n    prompts = [\n        \"Write about the president of the United States.\",\n    ]\n    prompts = [template.format(prompt) for prompt in prompts]\n    # Create a sampling params object.\n    sampling_params = SamplingParams(temperature=0.0, max_tokens=200)\n\n    # Create an LLM without spec decoding\n    llm = LLM(model=\"meta-llama/Llama-2-13b-chat-hf\")\n\n    print(\"Without speculation\")\n    time_generation(llm, prompts, sampling_params)\n\n    del llm\n    gc.collect()\n\n    # Create an LLM with spec decoding\n    llm = LLM(\n        model=\"meta-llama/Llama-2-13b-chat-hf\",\n        speculative_model=\"ibm-fms/llama-13b-accelerator\",\n        # These are currently required for MLPSpeculator decoding\n        use_v2_block_manager=True,\n    )\n\n    print(\"With speculation\")\n    time_generation(llm, prompts, sampling_params)\n",
      "diff": "diff --git a/examples/offline_inference_mlpspeculator.py b/examples/offline_inference_mlpspeculator.py\nindex 5dec4a76a..8f0eb65e4 100644\n--- a/examples/offline_inference_mlpspeculator.py\n+++ b/examples/offline_inference_mlpspeculator.py\n@@ -50,8 +50,6 @@ if __name__ == \"__main__\":\n     llm = LLM(\n         model=\"meta-llama/Llama-2-13b-chat-hf\",\n         speculative_model=\"ibm-fms/llama-13b-accelerator\",\n-        # These are currently required for MLPSpeculator decoding\n-        use_v2_block_manager=True,\n     )\n \n     print(\"With speculation\")",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 3
    },
    {
      "file_path": "tests/basic_correctness/test_chunked_prefill.py",
      "old_content": "\"\"\"Compare the outputs of HF and vLLM when using greedy sampling.\n\nIt tests chunked prefill. Chunked prefill can be enabled by\nenable_chunked_prefill=True. If prefill size exceeds max_num_batched_tokens,\nprefill requests are chunked.\n\nRun `pytest tests/models/test_chunked_prefill.py`.\n\"\"\"\nimport os\nfrom contextlib import nullcontext\n\nimport pytest\n\nfrom ..models.utils import check_logprobs_close, check_outputs_equal\nfrom ..utils import check_deprecated_block_manager_usage, multi_gpu_test\n\nMODELS = [\n    \"facebook/opt-125m\",\n    \"meta-llama/Llama-2-7b-hf\",\n]\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/basic_correctness/test_chunked_prefill.py')\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [32])\n@pytest.mark.parametrize(\"chunked_prefill_token_size\", [1, 4, 16])\n@pytest.mark.parametrize(\"enforce_eager\", [False, True])\n# NOTE: Increasing this in this suite will fail CI because we currently cannot\n# reset distributed env properly. Use a value > 1 just when you test.\n@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\ndef test_models(\n    hf_runner,\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n    chunked_prefill_token_size: int,\n    enforce_eager: bool,\n    tensor_parallel_size: int,\n) -> None:\n    \"\"\"\n    Checks exact match decode between huggingface model and vllm runner with\n    chunked prefill.\n    \"\"\"\n    max_num_seqs = chunked_prefill_token_size\n    max_num_batched_tokens = chunked_prefill_token_size\n\n    with hf_runner(model, dtype=dtype) as hf_model:\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            max_num_batched_tokens=max_num_batched_tokens,\n            enable_chunked_prefill=True,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n    ) as vllm_model:\n        vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n\n    check_outputs_equal(\n        outputs_0_lst=hf_outputs,\n        outputs_1_lst=vllm_outputs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n\n\n@multi_gpu_test(num_gpus=2)\n@pytest.mark.parametrize(\"distributed_executor_backend\", [\"ray\", \"mp\"])\n@pytest.mark.parametrize(\"model\", MODELS)\ndef test_models_distributed(\n    hf_runner,\n    vllm_runner,\n    example_prompts,\n    model: str,\n    distributed_executor_backend: str,\n) -> None:\n    if (model == \"meta-llama/Llama-2-7b-hf\"\n            and distributed_executor_backend == \"ray\"):\n        # test ray adag\n        os.environ['VLLM_USE_RAY_SPMD_WORKER'] = \"1\"\n        os.environ['VLLM_USE_RAY_COMPILED_DAG'] = \"1\"\n\n    dtype = \"half\"\n    max_tokens = 5\n    chunked_prefill_token_size = 16\n\n    # Add a chunked prefill config.\n    max_num_seqs = min(chunked_prefill_token_size, 256)\n    assert chunked_prefill_token_size != -1\n    enable_chunked_prefill = True\n    max_num_batched_tokens = chunked_prefill_token_size\n\n    # NOTE: take care of the order. run vLLM first, and then run HF.\n    # vLLM needs a fresh new process without cuda initialization.\n    # if we run HF first, the cuda initialization will be done and it\n    # will hurt multiprocessing backend with fork method (the default method).\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            tensor_parallel_size=2,\n            max_num_seqs=max_num_seqs,\n            enable_chunked_prefill=enable_chunked_prefill,\n            max_num_batched_tokens=max_num_batched_tokens,\n            distributed_executor_backend=distributed_executor_backend,\n    ) as vllm_model:\n        vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n\n    with hf_runner(model, dtype=dtype) as hf_model:\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\n\n    check_outputs_equal(\n        outputs_0_lst=hf_outputs,\n        outputs_1_lst=vllm_outputs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n\n\n@pytest.mark.parametrize(\n    \"kv_cache_dtype,model\",\n    [(\"fp8_e4m3\",\n      \"nm-testing/TinyLlama-1.1B-compressed-tensors-kv-cache-scheme\")])\n# Due to low-precision numerical divergence, we only test logprob of 4 tokens\n@pytest.mark.parametrize(\"max_tokens\", [4])\n@pytest.mark.parametrize(\"chunked_prefill_token_size\", [4, 16])\n@pytest.mark.parametrize(\"enforce_eager\", [False, True])\n# NOTE: Increasing this in this suite will fail CI because we currently cannot\n# reset distributed env properly. Use a value > 1 just when you test.\n@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n# Due to low-precision numerical divergence, this test is too sensitive to\n# the async postprocessor\n@pytest.mark.parametrize(\"disable_async_output_proc\", [True])\ndef test_models_with_fp8_kv_cache(\n    vllm_runner,\n    example_prompts,\n    kv_cache_dtype: str,\n    model: str,\n    max_tokens: int,\n    chunked_prefill_token_size: int,\n    enforce_eager: bool,\n    tensor_parallel_size: int,\n    disable_async_output_proc: bool,\n) -> None:\n    \"\"\"\n    Check output logprobs match between no_chunked_prefill and chunked_prefill\n    with fp8 kv cache. General fp8 kv-cache tests are covered in test_fp8.py,\n    so here we only check chunked prefill.\n    \"\"\"\n    NUM_LOG_PROBS = 8\n\n    max_num_seqs = chunked_prefill_token_size\n    max_num_batched_tokens = chunked_prefill_token_size\n\n    with vllm_runner(\n            model,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n            kv_cache_dtype=kv_cache_dtype,\n            disable_async_output_proc=disable_async_output_proc,\n    ) as vllm_model:\n        no_chunked_prefill_outputs = vllm_model.generate_greedy_logprobs(\n            example_prompts, max_tokens, NUM_LOG_PROBS)\n\n    with vllm_runner(\n            model,\n            max_num_batched_tokens=max_num_batched_tokens,\n            enable_chunked_prefill=True,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n            kv_cache_dtype=kv_cache_dtype,\n            disable_async_output_proc=disable_async_output_proc,\n    ) as vllm_model:\n        chunked_prefill_outputs = vllm_model.generate_greedy_logprobs(\n            example_prompts, max_tokens, NUM_LOG_PROBS)\n\n    check_logprobs_close(\n        outputs_0_lst=no_chunked_prefill_outputs,\n        outputs_1_lst=chunked_prefill_outputs,\n        name_0=\"no_chunked_prefill\",\n        name_1=\"chunked_prefill\",\n    )\n\n\n@pytest.mark.parametrize(\"max_tokens\", [16])\n@pytest.mark.parametrize(\"enforce_eager\", [False])\n@pytest.mark.parametrize(\"chunk_size\", [30, 32])\n@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n# NOTE: Increasing this in this suite will fail CI because we currently cannot\n# reset distributed env properly. Use a value > 1 just when you test.\n@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\ndef test_with_prefix_caching(\n    vllm_runner,\n    max_tokens: int,\n    enforce_eager: bool,\n    chunk_size: int,\n    use_v2_block_manager: bool,\n    tensor_parallel_size: int,\n) -> None:\n    \"\"\"\n    Checks exact match decode with and without prefix caching\n    with chunked prefill enabled.\n    \"\"\"\n    model = \"meta-llama/Llama-2-7b-chat-hf\"\n    # The common prompt has 142 tokens with Llama-2 tokenizer.\n    common_prompt = \"You are a helpful AI assistant \" * 20\n    unique_prompts = [\n        \"Question\",  # Warmup\n        \"Question\",  # Fully cached\n        \"Another question\",  # Partial cached\n    ]\n    full_prompts = [f\"{common_prompt}\\n{p}\" for p in unique_prompts]\n\n    max_num_batched_tokens = max_num_seqs = chunk_size\n    outputs = {}  # type: ignore\n    check_result = True\n    for enable in (True, False):\n        with vllm_runner(\n                model,\n                dtype=\"half\",\n                max_num_batched_tokens=max_num_batched_tokens,\n                enable_chunked_prefill=True,\n                enable_prefix_caching=enable,\n                tensor_parallel_size=tensor_parallel_size,\n                use_v2_block_manager=use_v2_block_manager,\n                enforce_eager=enforce_eager,\n                max_num_seqs=max_num_seqs,\n        ) as vllm_model:\n            # It should fail when prefix caching is enable and chunk\n            # size is not a multiple of block size (16).\n            should_fail = chunk_size % 16 != 0 and enable\n            check_result &= not should_fail\n            outputs[enable] = []\n            # Send the request one-by-one to ensure the cache is populated.\n            with pytest.raises(ValueError) if should_fail else nullcontext():\n                for prompt in full_prompts:\n                    outputs[enable] += vllm_model.generate_greedy([prompt],\n                                                                  max_tokens)\n\n    # Check results only if we did not expect a failure.\n    if check_result:\n        check_outputs_equal(\n            outputs_0_lst=outputs[False],\n            outputs_1_lst=outputs[True],\n            name_0=\"w/o prefix caching\",\n            name_1=\"with prefix caching\",\n        )\n",
      "diff": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex e8819688c..c3e3835af 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -12,7 +12,7 @@ from contextlib import nullcontext\n import pytest\n \n from ..models.utils import check_logprobs_close, check_outputs_equal\n-from ..utils import check_deprecated_block_manager_usage, multi_gpu_test\n+from ..utils import multi_gpu_test\n \n MODELS = [\n     \"facebook/opt-125m\",\n@@ -20,12 +20,6 @@ MODELS = [\n ]\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/basic_correctness/test_chunked_prefill.py')\n-\n-\n @pytest.mark.parametrize(\"model\", MODELS)\n @pytest.mark.parametrize(\"dtype\", [\"half\"])\n @pytest.mark.parametrize(\"max_tokens\", [32])\n@@ -197,7 +191,6 @@ def test_models_with_fp8_kv_cache(\n @pytest.mark.parametrize(\"max_tokens\", [16])\n @pytest.mark.parametrize(\"enforce_eager\", [False])\n @pytest.mark.parametrize(\"chunk_size\", [30, 32])\n-@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n # NOTE: Increasing this in this suite will fail CI because we currently cannot\n # reset distributed env properly. Use a value > 1 just when you test.\n @pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n@@ -206,7 +199,6 @@ def test_with_prefix_caching(\n     max_tokens: int,\n     enforce_eager: bool,\n     chunk_size: int,\n-    use_v2_block_manager: bool,\n     tensor_parallel_size: int,\n ) -> None:\n     \"\"\"\n@@ -234,7 +226,6 @@ def test_with_prefix_caching(\n                 enable_chunked_prefill=True,\n                 enable_prefix_caching=enable,\n                 tensor_parallel_size=tensor_parallel_size,\n-                use_v2_block_manager=use_v2_block_manager,\n                 enforce_eager=enforce_eager,\n                 max_num_seqs=max_num_seqs,\n         ) as vllm_model:",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 11
    },
    {
      "file_path": "tests/core/block/e2e/test_correctness.py",
      "old_content": "from itertools import cycle\n\nimport pytest\n\nfrom tests.utils import check_deprecated_block_manager_usage\nfrom vllm import SamplingParams\n\nfrom .conftest import get_token_ids_from_llm_generator\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/core/block/e2e/test_correctness.py')\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        \"model\": \"facebook/opt-125m\",\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n\n        # Allow only 5 sequences of ~1024 tokens in worst case.\n        \"block_size\": 16,\n        \"num_gpu_blocks_override\": 5 * (64 + 1),\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"use_v2_block_manager\": False\n}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n    \"use_v2_block_manager\": True,\n    \"preemption_mode\": \"swap\"\n}, {\n    \"use_v2_block_manager\": True,\n    \"preemption_mode\": \"recompute\"\n}])\n@pytest.mark.parametrize(\"batch_size\", [10])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n                                               test_llm_generator, batch_size):\n    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n    when there is preemption.\n\n    This constructs two LLM, each with limited number of GPU blocks. The limit\n    is decided such that as the sequences in the batch grow, sequences must be\n    preempted and removed from cache.\n\n    If the output token ids are equivalent, then we have confidence that the KV\n    cache is not corrupted in the v2 block manager.\n\n    NOTE: We want a significant number of generated tokens so that any incorrect\n    KV mapping has time to build up error.\n    \"\"\"\n    output_len = 1024\n    temperature = 0.0\n\n    # We want to ensure equality even with preemption.\n    # We force the total block size to be 1 + cdiv(output_len, block_size)\n    # so that only one sequence can fit at a time (once the sequences grow).\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids from block manager v1')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids from block manager v2')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        \"model\": \"facebook/opt-125m\",\n\n        # Our prompts will generate 128 tokens; since the prompts themselves are\n        # small, we don't need much KV space beyond 128.\n        \"max_model_len\": 160,\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n\n        # Lookahead scheduling only supported in v2 block manager.\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        {\n            \"block_size\": 16,\n\n            # Allow only 2 sequences of ~128 tokens in worst case.\n            # Note 8 = 128/block_size\n            \"num_gpu_blocks_override\": 2 * (8 + 1),\n        },\n        {\n            \"block_size\": 8,\n\n            # Allow only 2 sequences of ~128 tokens in worst case.\n            # Note 16 = 128/block_size\n            \"num_gpu_blocks_override\": 2 * (16 + 2),\n        }\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"num_lookahead_slots\": 0,\n}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            # We run one test with block_size < lookahead_slots, one test with\n            # block_size > lookahead_slots\n            \"num_lookahead_slots\": 10,\n            \"preemption_mode\": \"swap\",\n        },\n        {\n            \"num_lookahead_slots\": 10,\n            \"preemption_mode\": \"recompute\",\n        }\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n                                                   test_llm_generator,\n                                                   batch_size):\n    \"\"\"Verify vLLM produces the same output with greedy sampling, when lookahead\n    scheduling is used vs. not.\n\n    Lookahead scheduling is not expected to modify the output, as it simply\n    allocates empty slots ahead of the known token ids in a sliding fashion.\n\n    This test constrains the total number of blocks to force preemption. It also\n    varies the block size so that the lookahead size is less than and greater\n    than the block size.\n    \"\"\"\n    output_len = 128\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids without lookahead scheduling')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids with lookahead scheduling')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [\n        {\n            # Use a small model for a fast test.\n            \"model\": \"facebook/opt-125m\",\n\n            # skip cuda graph creation for fast test.\n            \"enforce_eager\": True,\n            \"enable_chunked_prefill\": True,\n        },\n    ])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\",\n                         [{\n                             \"block_size\": 8,\n                             \"max_num_batched_tokens\": 2,\n                             \"max_num_seqs\": 2,\n                         }, {\n                             \"block_size\": 8,\n                             \"max_num_batched_tokens\": 3,\n                             \"max_num_seqs\": 2,\n                         }, {\n                             \"block_size\": 8,\n                             \"max_num_batched_tokens\": 256,\n                             \"max_num_seqs\": 10,\n                         }])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [\n    {\n        \"use_v2_block_manager\": False,\n    },\n])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"use_v2_block_manager\": True,\n        \"num_lookahead_slots\": 0,\n    },\n    {\n        \"use_v2_block_manager\": True,\n        \"num_lookahead_slots\": 5,\n    },\n])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n                                          test_llm_generator, batch_size):\n    \"\"\"Verify that chunked prefill works with BlockManagerV2, with and without\n    lookahead scheduling.\n    \"\"\"\n    output_len = 32\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        (\"1 + \" * 50) + \" 1 = \",  # Longer prompt.\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids with BlockManagerV1')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids with BlockManagerV2')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        \"model\": \"facebook/opt-125m\",\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n\n        # Allow only 5 sequences of ~1024 tokens in worst case.\n        \"block_size\": 16,\n        \"num_gpu_blocks_override\": 5 * (64 + 1),\n\n        # Enable prefill cache\n        \"enable_prefix_caching\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"use_v2_block_manager\": False\n}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n    \"use_v2_block_manager\": True,\n    \"preemption_mode\": \"swap\"\n}, {\n    \"use_v2_block_manager\": True,\n    \"preemption_mode\": \"recompute\"\n}])\n@pytest.mark.parametrize(\"batch_size\", [10])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n        baseline_llm_generator, test_llm_generator, batch_size):\n    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n    when there is preemption.\n\n    This constructs two LLM, each with limited number of GPU blocks. The limit\n    is decided such that as the sequences in the batch grow, sequences must be\n    preempted and removed from cache.\n\n    If the output token ids are equivalent, then we have confidence that the KV\n    cache is not corrupted in the v2 block manager.\n\n    NOTE: We want a significant number of generated tokens so that any incorrect\n    KV mapping has time to build up error.\n    \"\"\"\n    output_len = 1024\n    temperature = 0.0\n\n    # We want to ensure equality even with preemption.\n    # We force the total block size to be 1 + cdiv(output_len, block_size)\n    # so that only one sequence can fit at a time (once the sequences grow).\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids from block manager v1')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids from block manager v2')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        \"model\": \"facebook/opt-125m\",\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n\n        # Allow only 5 sequences of ~1024 tokens in worst case.\n        \"block_size\": 16,\n        \"num_gpu_blocks_override\": 5 * (64 + 1),\n\n        # Test APC in v2 block\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"enable_prefix_caching\": False\n}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n    \"enable_prefix_caching\": True,\n    \"preemption_mode\": \"swap\"\n}, {\n    \"enable_prefix_caching\": True,\n    \"preemption_mode\": \"recompute\"\n}])\n@pytest.mark.parametrize(\"batch_size\", [10])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_auto_prefix_caching_with_preemption(baseline_llm_generator,\n                                             test_llm_generator, batch_size):\n    \"\"\"Verify block manager v2 with auto prefix caching enabled produces same\n    outputs as auto prefix caching disabled, even when there is preemption.\n\n    This constructs two LLM, each with limited number of GPU blocks. The limit\n    is decided such that as the sequences in the batch grow, sequences must be\n    preempted and removed from cache.\n\n    If the output token ids are equivalent, then we have confidence that auto\n    prefix caching itself at least don't cause result error.\n    \"\"\"\n    output_len = 1024\n    temperature = 0.0\n\n    # We want to ensure equality even with preemption.\n    # We force the total block size to be 1 + cdiv(output_len, block_size)\n    # so that only one sequence can fit at a time (once the sequences grow).\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids with APC disabled')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids with APC enabled')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        \"model\": \"facebook/opt-125m\",\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n\n        # we keep the blocks small, so that hit eviction quickly\n        \"max_model_len\": 48,\n        \"block_size\": 16,\n        \"num_gpu_blocks_override\": 3,\n\n        # Test APC in v2 block\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"enable_prefix_caching\": False\n}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n    \"enable_prefix_caching\": True,\n}])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_auto_prefix_caching_after_evition_start(baseline_llm_generator,\n                                                 test_llm_generator):\n    \"\"\"Verify block manager v2 with auto prefix caching could works normal\n    even when eviction started.\n    With APC enabled, all blocks are held by native block at the beginning.\n    Then blocks are managed by evictor instead. If cache hit at the evitor's\n    block, then it could be reused, or we need to recompute its kv cache.\n    \"\"\"\n    output_len = 10\n    temperature = 0.0\n\n    prompts = [\n        \"You are a helpful assistant. Please answer truthfully and write \"\n        \"out your thinking step by step to be sure you get the right answer. \"\n        \"If you make a mistake, attempt to correct it. who are you?\",\n        \"You are a helpful assistant. Please answer truthfully and write out \"\n        \"your thinking step by step to be sure you get the right answer. You \"\n        \"are helpful and harmless and you follow ethical guidelines. \"\n        \"who are you?\"\n    ]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    print('Getting token ids with APC disabled')\n    baseline_token_ids = get_token_ids_from_llm_generator(\n        baseline_llm_generator, prompts, sampling_params)\n\n    print('Getting token ids with APC enabled')\n    test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                      prompts, sampling_params)\n\n    for expected_token_ids, actual_token_ids in zip(baseline_token_ids,\n                                                    test_token_ids):\n        assert expected_token_ids == actual_token_ids\n\n    assert baseline_token_ids == test_token_ids\n",
      "diff": "diff --git a/tests/core/block/e2e/test_correctness.py b/tests/core/block/e2e/test_correctness.py\nindex b3f626714..86502f613 100644\n--- a/tests/core/block/e2e/test_correctness.py\n+++ b/tests/core/block/e2e/test_correctness.py\n@@ -2,18 +2,11 @@ from itertools import cycle\n \n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import SamplingParams\n \n from .conftest import get_token_ids_from_llm_generator\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/block/e2e/test_correctness.py')\n-\n-\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n@@ -28,32 +21,32 @@ def check_deprecated_block_manager():\n         \"num_gpu_blocks_override\": 5 * (64 + 1),\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"swap\"\n }, {\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"recompute\"\n }])\n @pytest.mark.parametrize(\"batch_size\", [10])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n-                                               test_llm_generator, batch_size):\n-    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n-    when there is preemption.\n+def test_block_manager_with_preemption(baseline_llm_generator,\n+                                       test_llm_generator, batch_size):\n+    \"\"\"Verify block manager produces same outputs even when there is preemption.\n \n     This constructs two LLM, each with limited number of GPU blocks. The limit\n     is decided such that as the sequences in the batch grow, sequences must be\n     preempted and removed from cache.\n \n     If the output token ids are equivalent, then we have confidence that the KV\n-    cache is not corrupted in the v2 block manager.\n+    cache is not corrupted.\n \n     NOTE: We want a significant number of generated tokens so that any incorrect\n     KV mapping has time to build up error.\n+\n+    NOTE(Kuntai): Though we have removed block manager v1, this test is still\n+    useful as it asserts the behavior of block manager v2 (now it is called \n+    SelfAttnBlockSpaceManager) is the same when swapping / preemption, so we  \n+    keep this test.\n     \"\"\"\n     output_len = 1024\n     temperature = 0.0\n@@ -77,11 +70,9 @@ def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids from block manager v1')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids from block manager v2')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -104,9 +95,6 @@ def test_v1_v2_greedy_equality_with_preemption(baseline_llm_generator,\n \n         # skip cuda graph creation for fast test.\n         \"enforce_eager\": True,\n-\n-        # Lookahead scheduling only supported in v2 block manager.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -218,26 +206,22 @@ def test_lookahead_greedy_equality_with_preemption(baseline_llm_generator,\n                              \"max_num_seqs\": 10,\n                          }])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [\n-    {\n-        \"use_v2_block_manager\": False,\n-    },\n+    {},\n ])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [\n     {\n-        \"use_v2_block_manager\": True,\n         \"num_lookahead_slots\": 0,\n     },\n     {\n-        \"use_v2_block_manager\": True,\n         \"num_lookahead_slots\": 5,\n     },\n ])\n @pytest.mark.parametrize(\"batch_size\", [4])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n-                                          test_llm_generator, batch_size):\n-    \"\"\"Verify that chunked prefill works with BlockManagerV2, with and without\n-    lookahead scheduling.\n+def test_chunked_prefill_block_manager(baseline_llm_generator,\n+                                       test_llm_generator, batch_size):\n+    \"\"\"Verify that chunked prefill works with SelfAttnBlockSpaceManager, \n+    with and without lookahead scheduling.\n     \"\"\"\n     output_len = 32\n     temperature = 0.0\n@@ -258,11 +242,11 @@ def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n         temperature=temperature,\n     )\n \n-    print('Getting token ids with BlockManagerV1')\n+    print('Getting token ids with BlockManager')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids with BlockManagerV2')\n+    print('Getting token ids with BlockManager, with lookahead slots.')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -290,32 +274,32 @@ def test_chunked_prefill_block_manager_v2(baseline_llm_generator,\n         \"enable_prefix_caching\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"swap\"\n }, {\n-    \"use_v2_block_manager\": True,\n     \"preemption_mode\": \"recompute\"\n }])\n @pytest.mark.parametrize(\"batch_size\", [10])\n @pytest.mark.parametrize(\"seed\", [1])\n-def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n+def test_block_manager_prefix_caching_enabled_with_preemption(\n         baseline_llm_generator, test_llm_generator, batch_size):\n-    \"\"\"Verify block manager v2 produces same outputs as block manager v1, even\n-    when there is preemption.\n+    \"\"\"Verify block manager produces same outputs even when there is preemption.\n \n     This constructs two LLM, each with limited number of GPU blocks. The limit\n     is decided such that as the sequences in the batch grow, sequences must be\n     preempted and removed from cache.\n \n     If the output token ids are equivalent, then we have confidence that the KV\n-    cache is not corrupted in the v2 block manager.\n+    cache is not corrupted.\n \n     NOTE: We want a significant number of generated tokens so that any incorrect\n     KV mapping has time to build up error.\n+\n+    NOTE(Kuntai): Though we have removed block manager v1, this test is still\n+    useful as it asserts the behavior of block manager v2 (now it is called \n+    SelfAttnBlockSpaceManager) is the same when swapping / preemption, so we  \n+    keep this test.\n     \"\"\"\n     output_len = 1024\n     temperature = 0.0\n@@ -339,11 +323,11 @@ def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n         temperature=temperature,\n     )\n \n-    print('Getting token ids from block manager v1')\n+    print('Getting token ids from block manager')\n     baseline_token_ids = get_token_ids_from_llm_generator(\n         baseline_llm_generator, prompts, sampling_params)\n \n-    print('Getting token ids from block manager v2')\n+    print('Getting token ids from block manager, with preemption')\n     test_token_ids = get_token_ids_from_llm_generator(test_llm_generator,\n                                                       prompts, sampling_params)\n \n@@ -366,9 +350,6 @@ def test_v1_v2_greedy_equality_prefix_caching_enabled_with_preemption(\n         # Allow only 5 sequences of ~1024 tokens in worst case.\n         \"block_size\": 16,\n         \"num_gpu_blocks_override\": 5 * (64 + 1),\n-\n-        # Test APC in v2 block\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n@@ -444,9 +425,6 @@ def test_auto_prefix_caching_with_preemption(baseline_llm_generator,\n         \"max_model_len\": 48,\n         \"block_size\": 16,\n         \"num_gpu_blocks_override\": 3,\n-\n-        # Test APC in v2 block\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{",
      "change_type": "modified",
      "lines_added": 29,
      "lines_removed": 51
    },
    {
      "file_path": "tests/core/block/e2e/test_correctness_sliding_window.py",
      "old_content": "import random\nfrom typing import List\n\nimport pytest\n\nfrom tests.utils import check_deprecated_block_manager_usage\nfrom vllm import LLM, SamplingParams\n\nfrom .conftest import get_text_from_llm_generator\n\n# relatively small model with 4k sliding window\nMODEL = \"bigcode/starcoder2-3b\"\nBLOCK_SIZE = 16\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/core/block/e2e/test_correctness_sliding_window.py')\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model\": MODEL,\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n        \"block_size\": BLOCK_SIZE,\n        # needed due to https://github.com/vllm-project/vllm/issues/1908#issuecomment-2101122008\n        \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n    \"use_v2_block_manager\": False\n}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"use_v2_block_manager\": True}])\n@pytest.mark.parametrize(\"batch_size\", [5])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n                                 batch_size, seed):\n    \"\"\"\n    The test does a bunch of assignments \"x1 = 10\\nx2 = 33\\n...\" and then\n    asks for value of one of them (which is outside the sliding window).\n    If we tell it upfront which we are going to be looking for, then\n    it answers correctly (mostly).\n\n    Additionally, we compare the results of the v1 and v2 managers.\n    \"\"\"\n    sampling_params = SamplingParams(\n        max_tokens=1024,\n        ignore_eos=True,\n        temperature=0.0,\n    )\n\n    prompts, answer, indices = prep_prompts(batch_size)\n\n    print('Getting token ids from block manager v1')\n    baseline_texts = get_text_from_llm_generator(baseline_llm_generator,\n                                                 prompts,\n                                                 sampling_params,\n                                                 llm_cb=check_window(prompts))\n\n    check_answers(indices, answer, baseline_texts)\n\n    print('Getting token ids from block manager v2')\n    test_texts = get_text_from_llm_generator(test_llm_generator, prompts,\n                                             sampling_params)\n    check_answers(indices, answer, test_texts)\n\n    cmp = [\n        expected_text == actual_text\n        for expected_text, actual_text in zip(baseline_texts, test_texts)\n    ]\n    print(cmp)\n    # make sure it's mostly OK; this is possibly because https://github.com/vllm-project/vllm/pull/4768\n    # however, https://github.com/vllm-project/vllm/issues/3385#issuecomment-1995924290\n    # states that xformers and flash_attn have different ideas about the window\n    # size anyways\n    assert sum(cmp) > 0.7 * len(cmp)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model\": MODEL,\n\n        # skip cuda graph creation for fast test.\n        \"enforce_eager\": True,\n        \"block_size\": BLOCK_SIZE,\n        \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n    \"use_v2_block_manager\": True,\n    \"enable_chunked_prefill\": True\n}])\n@pytest.mark.parametrize(\"batch_size\", [5])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_sliding_window_chunked_prefill(test_llm_generator, batch_size, seed):\n    \"\"\"\n    This is similar to test_sliding_window_retrival, however, it doesn't\n    compare against the v1 block manager since v1 doesn't support\n    chunked prefill with sliding window.\n\n    The results with and without chunked prefill are not the same due to\n    numerical instabilities.\n    \"\"\"\n    sampling_params = SamplingParams(\n        max_tokens=10,\n        ignore_eos=True,\n        temperature=0.0,\n    )\n\n    prompts, answer, indices = prep_prompts(batch_size)\n\n    # We don't compare with the baseline model here, since the results\n    # slightly different due to different tailing in attention.\n    test_texts = get_text_from_llm_generator(test_llm_generator,\n                                             prompts,\n                                             sampling_params,\n                                             llm_cb=check_window(prompts))\n    check_answers(indices, answer, test_texts)\n\n\ndef prep_prompts(batch_size: int):\n    \"\"\"\n    Generate prompts which a bunch of assignments,\n    then asking for the value of one of them.\n    The prompt is just under 10k tokens; sliding window is 4k\n    so the answer is outside sliding window, but should still be correct.\n    \"\"\"\n    prompts: List[str] = []\n    answer: List[int] = []\n    indices: List[int] = []\n    random.seed(1)\n    for _ in range(batch_size):\n        idx = random.randint(30, 90)\n        indices.append(idx)\n        prompt = \"```python\\n# We set a number of variables, \" + \\\n                 f\"x{idx} will be important later\\n\"\n        ln = random.randint(800, 1100)\n        for k in range(30, ln):\n            v = random.randint(10, 99)\n            if k == idx:\n                answer.append(v)\n            prompt += f\"x{k} = {v}\\n\"\n        prompt += f\"# Now, we check the value of x{idx}:\\n\"\n        prompt += f\"assert x{idx} == \"\n        prompts.append(prompt)\n    return prompts, answer, indices\n\n\ndef check_answers(indices: List[int], answer: List[int], outputs: List[str]):\n    answer2 = [int(text[0:2].strip()) for text in outputs]\n    print(list(zip(indices, zip(answer, answer2))))\n    numok = 0\n    for a1, a2 in zip(answer, answer2):\n        if a1 == a2:\n            numok += 1\n    frac_ok = numok / len(answer)\n    print(f\"Num OK: {numok}/{len(answer)} {frac_ok}\")\n    assert frac_ok > 0.7\n\n\ndef check_window(prompts: List[str]):\n\n    def inner(llm: LLM):\n        sliding_window = llm.llm_engine.model_config.get_sliding_window()\n        assert sliding_window and sliding_window > 0\n        assert any(\n            len(llm.get_tokenizer().tokenize(prompt)) > sliding_window\n            for prompt in prompts)\n\n    return inner\n",
      "diff": "diff --git a/tests/core/block/e2e/test_correctness_sliding_window.py b/tests/core/block/e2e/test_correctness_sliding_window.py\nindex 731131984..9320a9ef6 100644\n--- a/tests/core/block/e2e/test_correctness_sliding_window.py\n+++ b/tests/core/block/e2e/test_correctness_sliding_window.py\n@@ -3,7 +3,6 @@ from typing import List\n \n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import LLM, SamplingParams\n \n from .conftest import get_text_from_llm_generator\n@@ -13,12 +12,6 @@ MODEL = \"bigcode/starcoder2-3b\"\n BLOCK_SIZE = 16\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/block/e2e/test_correctness_sliding_window.py')\n-\n-\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n@@ -31,10 +24,8 @@ def check_deprecated_block_manager():\n         \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\n-    \"use_v2_block_manager\": False\n-}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"use_v2_block_manager\": True}])\n+@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n+@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"batch_size\", [5])\n @pytest.mark.parametrize(\"seed\", [1])\n def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n@@ -55,7 +46,6 @@ def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n \n     prompts, answer, indices = prep_prompts(batch_size)\n \n-    print('Getting token ids from block manager v1')\n     baseline_texts = get_text_from_llm_generator(baseline_llm_generator,\n                                                  prompts,\n                                                  sampling_params,\n@@ -91,10 +81,7 @@ def test_sliding_window_retrival(baseline_llm_generator, test_llm_generator,\n         \"num_gpu_blocks_override\": 100000 // BLOCK_SIZE,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{\n-    \"use_v2_block_manager\": True,\n-    \"enable_chunked_prefill\": True\n-}])\n+@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"enable_chunked_prefill\": True}])\n @pytest.mark.parametrize(\"batch_size\", [5])\n @pytest.mark.parametrize(\"seed\", [1])\n def test_sliding_window_chunked_prefill(test_llm_generator, batch_size, seed):",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 17
    },
    {
      "file_path": "tests/core/block/test_block_manager.py",
      "old_content": "",
      "diff": "diff --git a/tests/core/block/test_block_manager.py b/tests/core/block/test_block_manager.py\nnew file mode 100644\nindex 000000000..cfd749ad5\n--- /dev/null\n+++ b/tests/core/block/test_block_manager.py\n@@ -0,0 +1,491 @@\n+import pytest\n+\n+from vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n+                                   STR_NOT_IMPL_ENC_DEC_SWA)\n+from vllm.core.block_manager import SelfAttnBlockSpaceManager\n+from vllm.core.interfaces import AllocStatus\n+from vllm.sequence import Logprob, SequenceStatus\n+from vllm.utils import chunk_list\n+\n+from ..utils import (create_dummy_prompt, create_seq_group,\n+                     create_seq_group_encoder_decoder)\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [16])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [8, 40, 80])\n+@pytest.mark.parametrize(\"num_seqs_per_group\", [1, 4])\n+@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\n+def test_can_allocate_seq_group(block_size: int, num_seqs_per_group: int,\n+                                num_gpu_blocks: int, watermark: float):\n+    block_manager = SelfAttnBlockSpaceManager(\n+        block_size=block_size,\n+        num_gpu_blocks=num_gpu_blocks,\n+        num_cpu_blocks=1024,\n+        watermark=watermark,\n+    )\n+    num_watermark_blocks = int(watermark * num_gpu_blocks)\n+\n+    num_output_blocks_per_seq = 1\n+\n+    # NOTE: This should be num_output_blocks_per_seq * num_seqs_per_group, but\n+    # the current implementation assumes all seqs are new prompts / don't have\n+    # different output lens.\n+    num_output_blocks = num_output_blocks_per_seq\n+\n+    for num_prompt_blocks in range(1, num_gpu_blocks - num_output_blocks):\n+        seq_group = create_seq_group(\n+            seq_prompt_len=block_size * num_prompt_blocks,\n+            seq_output_lens=[\n+                block_size * num_output_blocks_per_seq\n+                for _ in range(num_seqs_per_group)\n+            ],\n+        )\n+\n+        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n+\n+        can_allocate_result = block_manager.can_allocate(seq_group)\n+\n+        num_required_blocks = num_prompt_blocks + num_output_blocks\n+\n+        if num_gpu_blocks - num_required_blocks < num_watermark_blocks:\n+            assert can_allocate_result == AllocStatus.NEVER\n+        elif num_gpu_blocks >= num_required_blocks:\n+            assert can_allocate_result == AllocStatus.OK\n+        else:\n+            assert can_allocate_result == AllocStatus.LATER\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [16])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [16, 80, 160])\n+@pytest.mark.parametrize(\"num_seqs_per_group\", [1, 4])\n+@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\n+def test_can_allocate_seq_group_encoder_decoder(block_size: int,\n+                                                num_seqs_per_group: int,\n+                                                num_gpu_blocks: int,\n+                                                watermark: float):\n+    block_manager = SelfAttnBlockSpaceManager(\n+        block_size=block_size,\n+        num_gpu_blocks=num_gpu_blocks,\n+        num_cpu_blocks=1024,\n+        watermark=watermark,\n+    )\n+    num_watermark_blocks = int(watermark * num_gpu_blocks)\n+\n+    num_output_blocks_per_seq = 1\n+\n+    # NOTE: This should be num_output_blocks_per_seq * num_seqs_per_group, but\n+    # the current implementation assumes all seqs are new prompts / don't have\n+    # different output lens.\n+    num_output_blocks = num_output_blocks_per_seq\n+\n+    for bdx, num_prompt_blocks in enumerate(\n+            range(1, num_gpu_blocks - num_output_blocks)):\n+        num_cross_blocks_per_seq = num_prompt_blocks\n+\n+        seq_group = create_seq_group_encoder_decoder(\n+            seq_prompt_len=block_size * num_prompt_blocks,\n+            seq_output_lens=[\n+                block_size * num_output_blocks_per_seq\n+                for _ in range(num_seqs_per_group)\n+            ],\n+            request_id=str(bdx))\n+\n+        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n+\n+        can_allocate_result = block_manager.can_allocate(seq_group)\n+\n+        num_required_blocks = num_prompt_blocks + \\\n+                              num_output_blocks + \\\n+                              num_cross_blocks_per_seq\n+\n+        if num_gpu_blocks - num_required_blocks < num_watermark_blocks:\n+            assert can_allocate_result == AllocStatus.NEVER\n+        elif num_gpu_blocks >= num_required_blocks:\n+            assert can_allocate_result == AllocStatus.OK\n+        else:\n+            assert can_allocate_result == AllocStatus.LATER\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [16])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [16])\n+@pytest.mark.parametrize(\"num_seqs_per_group\", [1])\n+@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\n+def test_can_allocate_encoder_decoder_fails_with_swa(block_size: int,\n+                                                     num_seqs_per_group: int,\n+                                                     num_gpu_blocks: int,\n+                                                     watermark: float):\n+    '''\n+    SWA short for Sliding Window Attention.\n+\n+    At time of writing block manager does not support SWA.\n+\n+    However even when SWA is implemented for block manager,\n+    there will still most likely be a separate workstream required\n+    to enable SWA for encoder/decoder models.\n+\n+    Therefore this test enforces that one of the following cases\n+    hold true:\n+    1. Block manager does not support SWA at all (true at time of writing)\n+    2. Block manager fails with NotImplementError when SWA is enabled\n+       AND a SequenceGroup with an encoder sequence (i.e. in support of an\n+       encoder/decoder model) is passed into can_allocate() as an argument\n+\n+    The setup for this test is stripped down version of\n+    test_can_allocate_seq_group_encoder_decoder()\n+    '''\n+\n+    with pytest.raises((NotImplementedError, AssertionError)) as exc_info:\n+        block_manager = SelfAttnBlockSpaceManager(\n+            block_size=block_size,\n+            num_gpu_blocks=num_gpu_blocks,\n+            num_cpu_blocks=1024,\n+            watermark=watermark,\n+            sliding_window=5  # SWA\n+        )\n+\n+        num_output_blocks_per_seq = 1\n+        num_prompt_blocks = 1\n+        num_output_blocks = num_output_blocks_per_seq\n+        seq_group = create_seq_group_encoder_decoder(\n+            seq_prompt_len=block_size * num_prompt_blocks,\n+            seq_output_lens=[\n+                block_size * num_output_blocks_per_seq\n+                for _ in range(num_seqs_per_group)\n+            ],\n+            request_id=\"0\")\n+\n+        assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n+        block_manager.can_allocate(seq_group)\n+\n+    # Assert that either\n+    # 1. Block manager constructor fails with assertion that sliding window\n+    #    is not yet supported (most likely near-term outcome at time of\n+    #    writing), or\n+    # 2. can_allocate() fails with NotImplementedError due to combination of\n+    #    encoder/decoder and sliding window attention\n+    if isinstance(exc_info.value, NotImplementedError):\n+        assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n+    elif isinstance(exc_info.value, AssertionError):\n+        assert str(exc_info.value) == \"Sliding window not yet supported\"\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [16])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [16])\n+@pytest.mark.parametrize(\"num_seqs_per_group\", [1])\n+@pytest.mark.parametrize(\"watermark\", [0.0, 0.5])\n+def test_can_allocate_encoder_decoder_fails_with_prefix_cache(\n+        block_size: int, num_seqs_per_group: int, num_gpu_blocks: int,\n+        watermark: float):\n+\n+    block_manager = SelfAttnBlockSpaceManager(\n+        block_size=block_size,\n+        num_gpu_blocks=num_gpu_blocks,\n+        num_cpu_blocks=1024,\n+        watermark=watermark,\n+        enable_caching=True  # Prefix cache\n+    )\n+\n+    num_output_blocks_per_seq = 1\n+    num_prompt_blocks = 1\n+    num_output_blocks = num_output_blocks_per_seq\n+    seq_group = create_seq_group_encoder_decoder(\n+        seq_prompt_len=block_size * num_prompt_blocks,\n+        seq_output_lens=[\n+            block_size * num_output_blocks_per_seq\n+            for _ in range(num_seqs_per_group)\n+        ],\n+        request_id=\"0\")\n+\n+    assert num_prompt_blocks + num_output_blocks <= num_gpu_blocks\n+\n+    # Assert that either can_allocate() fails with NotImplementedError\n+    # due to combination of encoder/decoder and prefix cache\n+    with pytest.raises(NotImplementedError) as exc_info:\n+        block_manager.can_allocate(seq_group)\n+    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [1, 8])\n+@pytest.mark.parametrize(\"prompt_len\", [1, 7, 8])\n+@pytest.mark.parametrize(\"num_slots_to_append\", [1, 8, 129])\n+@pytest.mark.parametrize(\"num_lookahead_slots\", [0, 10])\n+def test_append_slots(block_size, prompt_len, num_slots_to_append,\n+                      num_lookahead_slots):\n+    \"\"\"Verify append_slots consumes the correct number of blocks from the block\n+    table.\n+    \"\"\"\n+\n+    num_gpu_blocks = 1024\n+    watermark = 0.1\n+    block_manager = SelfAttnBlockSpaceManager(\n+        block_size=block_size,\n+        num_gpu_blocks=num_gpu_blocks,\n+        num_cpu_blocks=0,\n+        watermark=watermark,\n+    )\n+\n+    seq_group = create_seq_group(\n+        seq_prompt_len=prompt_len,\n+        seq_output_lens=[0],\n+    )\n+\n+    # Allocate seq\n+    assert block_manager.can_allocate(seq_group)\n+    block_manager.allocate(seq_group)\n+\n+    # Seq seq to RUNNING\n+    seq = seq_group.get_seqs()[0]\n+    seq.status = SequenceStatus.RUNNING\n+\n+    # Append tokens to the sequeqnce\n+    for token_id in range(num_slots_to_append):\n+        seq.append_token_id(token_id, {token_id: Logprob(0.0)})\n+\n+    # Append slots for new tokens and lookahead slots.\n+    free_blocks_before_append = block_manager.get_num_free_gpu_blocks()\n+    block_manager.append_slots(seq, num_lookahead_slots)\n+    num_consumed_blocks = (free_blocks_before_append -\n+                           block_manager.get_num_free_gpu_blocks())\n+\n+    # Expect consumed blocks to be new blocks required to support the new slots.\n+    expected_consumed_blocks = len(\n+        list(\n+            chunk_list(\n+                list(\n+                    range(prompt_len + num_slots_to_append +\n+                          num_lookahead_slots)),\n+                block_size))) - len(\n+                    list(chunk_list(list(range(prompt_len)), block_size)))\n+    assert num_consumed_blocks == expected_consumed_blocks\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [8])\n+@pytest.mark.parametrize(\"num_cpu_blocks\", [4])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [4])\n+@pytest.mark.parametrize(\"num_lookahead_slots\", [0, 2, 10])\n+@pytest.mark.parametrize(\"enable_caching\", [False, True])\n+def test_swap(block_size, num_cpu_blocks, num_gpu_blocks, num_lookahead_slots,\n+              enable_caching):\n+    \"\"\"Verify blocks number on src/desc device is correct after swapping in/out\n+        sequence group (not missing or extra blocks).\n+    \"\"\"\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n+    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n+    prompt.status = SequenceStatus.WAITING\n+    block_manager.allocate(seq_group)\n+\n+    # Emulate a forward pass by appending a single token.\n+    # The block manager then knows how many unprocessed\n+    # tokens will be written in the next forward pass.\n+    token_id = 0\n+    prompt.status = SequenceStatus.RUNNING\n+    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n+\n+    # Swap seq group from GPU -> CPU.\n+    gpu_blocks = block_manager.get_block_table(prompt)\n+    assert block_manager.can_swap_out(seq_group)\n+    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    mapping = block_manager.swap_out(seq_group)\n+    mapping_keys = [key for key, _ in mapping]\n+    assert mapping_keys == gpu_blocks\n+    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n+    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n+    prompt.status = SequenceStatus.SWAPPED\n+\n+    # Swap seq group from CPU -> GPU.\n+    assert block_manager.can_swap_in(seq_group, num_lookahead_slots)\n+    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    mapping = block_manager.swap_in(seq_group)\n+    cpu_blocks = block_manager.get_block_table(prompt)\n+    mapping_keys = [key for key, _ in mapping]\n+    assert mapping_keys == [cpu_blocks[0]]\n+    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [8])\n+@pytest.mark.parametrize(\"num_gpu_blocks\", [4])\n+@pytest.mark.parametrize(\"num_lookahead_slots\", [3, 8, 10])\n+@pytest.mark.parametrize(\"enable_caching\", [True, False])\n+def test_can_swap(block_size, num_gpu_blocks, num_lookahead_slots,\n+                  enable_caching):\n+    \"\"\" Verify the block manager can correctly determine if a sequence group\n+        can be swapped in/out.\n+    \"\"\"\n+    num_cpu_blocks = num_gpu_blocks\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n+    prompt, seq_group = create_dummy_prompt(\n+        \"1\", prompt_length=(num_gpu_blocks - 1) * block_size - 1)\n+    prompt.status = SequenceStatus.WAITING\n+    block_manager.allocate(seq_group)\n+    prompt.status = SequenceStatus.RUNNING\n+\n+    # Swap seq group from GPU -> CPU.\n+    gpu_blocks = block_manager.get_block_table(prompt)\n+    assert block_manager.can_swap_out(seq_group)\n+    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    mapping = block_manager.swap_out(seq_group)\n+    mapping_keys = [key for key, _ in mapping]\n+    assert mapping_keys == gpu_blocks\n+    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n+    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n+    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n+    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n+    prompt.status = SequenceStatus.SWAPPED\n+\n+    # At this moment, we still have enough free blocks to swap in the seq group.\n+    if num_lookahead_slots <= block_size:\n+        assert block_manager.can_swap_in(seq_group,\n+                                         num_lookahead_slots) == AllocStatus.OK\n+    else:\n+        assert block_manager.can_swap_in(\n+            seq_group, num_lookahead_slots) == AllocStatus.NEVER\n+\n+    # During Swapped out, 2 cached blocks were evicted from the GPU,\n+    # so the prompt1 can't be swapped in\n+    prompt2_len = 2 * block_size - 1\n+    prompt2, seq_group2 = create_dummy_prompt(\n+        \"2\",\n+        prompt_length=prompt2_len,\n+        prompt_tokens=[10000 + i for i in range(prompt2_len)])\n+    prompt2.status = SequenceStatus.WAITING\n+    block_manager.allocate(seq_group2)\n+\n+    # Swap seq group from CPU -> GPU.\n+    if num_lookahead_slots <= block_size:\n+        assert block_manager.can_swap_in(\n+            seq_group, num_lookahead_slots) == AllocStatus.LATER\n+    else:\n+        assert block_manager.can_swap_in(\n+            seq_group, num_lookahead_slots) == AllocStatus.NEVER\n+\n+\n+@pytest.mark.parametrize(\"num_lookahead_slots\", [0, 2, 10])\n+@pytest.mark.parametrize(\"enable_caching\", [False, True])\n+def test_swap_in_infeasible(num_lookahead_slots, enable_caching):\n+    \"\"\"Verifies that swapping fails if there is not enough free blocks\n+    to account for unseen tokens and lookahead_slots.\n+    \"\"\"\n+    block_size = 8\n+    num_cpu_blocks = 1\n+    num_gpu_blocks = 1\n+    block_manager = SelfAttnBlockSpaceManager(block_size,\n+                                              num_cpu_blocks,\n+                                              num_gpu_blocks,\n+                                              watermark=0,\n+                                              enable_caching=enable_caching)\n+    prompt_length = block_size - 3\n+    assert prompt_length > 0\n+    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=prompt_length)\n+    prompt.status = SequenceStatus.WAITING\n+    block_manager.allocate(seq_group)\n+    # Emulate a forward pass by appending a single token.\n+    # The block manager then knows how many unprocessed\n+    # tokens will be written in the next forward pass.\n+    token_id = 0\n+    prompt.status = SequenceStatus.RUNNING\n+    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n+\n+    # Swap seq group from GPU -> CPU.\n+    assert block_manager.can_swap_out(seq_group)\n+    block_manager.swap_out(seq_group)\n+    prompt.status = SequenceStatus.SWAPPED\n+\n+    # Swap seq group from CPU -> GPU.\n+    # The number of unseen tokens is 1. If the number of existing\n+    # tokens plus the unseen ones and number of lookahead slots exceeds\n+    # the total number of available GPU blocks then the swap\n+    # should fail.\n+    num_unseen_tokens = 1\n+    if (num_lookahead_slots + num_unseen_tokens +\n+            prompt_length) <= (block_size * num_gpu_blocks):\n+        assert block_manager.can_swap_in(seq_group,\n+                                         num_lookahead_slots) == AllocStatus.OK\n+    else:\n+        assert block_manager.can_swap_in(\n+            seq_group, num_lookahead_slots) == AllocStatus.NEVER\n+\n+\n+# TODO(cade/kaiyang): add comprehensive tests for swapping at allocator level.\n+\n+\n+@pytest.mark.parametrize(\"block_size\", [8, 16])\n+@pytest.mark.parametrize(\"prompt_len\", [10, 300, 1000])\n+@pytest.mark.parametrize(\"num_slots_to_append\", [50])\n+@pytest.mark.parametrize(\"sliding_window\", [20, 32, 200, 512])\n+def test_sliding_window(block_size, prompt_len, num_slots_to_append,\n+                        sliding_window):\n+    \"\"\"Verify append_slots consumes the correct number of blocks from the block\n+    table.\n+    \"\"\"\n+\n+    num_gpu_blocks = 1024\n+    watermark = 0.1\n+    block_manager = SelfAttnBlockSpaceManager(\n+        block_size=block_size,\n+        num_gpu_blocks=num_gpu_blocks,\n+        num_cpu_blocks=0,\n+        watermark=watermark,\n+        sliding_window=sliding_window,\n+    )\n+\n+    def check_used(min_n, max_n=None):\n+        if max_n is None:\n+            max_n = min_n\n+        used = num_gpu_blocks - block_manager.get_num_free_gpu_blocks()\n+        assert min_n <= used\n+        assert used <= max_n\n+\n+    def num_blocks(num_tokens):\n+        return (num_tokens + block_size - 1) // block_size\n+\n+    check_used(0)\n+\n+    seq_group = create_seq_group(\n+        seq_prompt_len=prompt_len,\n+        seq_output_lens=[0],\n+    )\n+\n+    check_used(0)\n+\n+    # Allocate seq\n+    assert block_manager.can_allocate(seq_group)\n+    block_manager.allocate(seq_group)\n+\n+    check_used(num_blocks(prompt_len))\n+\n+    # Seq seq to RUNNING\n+    seq = seq_group.get_seqs()[0]\n+    seq.status = SequenceStatus.RUNNING\n+\n+    seq.data.update_num_computed_tokens(prompt_len)\n+    check_used(num_blocks(prompt_len))\n+\n+    # this is how we compute it in SelfAttnBlockSpaceManager.__init__\n+    sliding_blocks = (sliding_window // block_size) + 2\n+    # plus one block for null block\n+    sliding_blocks += 1\n+\n+    # Append tokens to the sequeqnce\n+    for token_id in range(num_slots_to_append):\n+        seq.append_token_id(token_id, {token_id: Logprob(0.0)})\n+        seq.data.update_num_computed_tokens(1)\n+        block_manager.append_slots(seq, num_lookahead_slots=0)\n+        if prompt_len < sliding_window + 10:\n+            check_used(0, sliding_blocks + 1)\n+        else:\n+            check_used(sliding_blocks, sliding_blocks + 1)",
      "change_type": "renamed",
      "lines_added": 492,
      "lines_removed": 1
    },
    {
      "file_path": "tests/core/test_block_manager.py",
      "old_content": "import time\nfrom collections import defaultdict\nfrom typing import List\n\nimport pytest\n\nfrom vllm import SamplingParams\nfrom vllm.block import PhysicalTokenBlock\nfrom vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                                   STR_NOT_IMPL_ENC_DEC_SWA)\nfrom vllm.core.block_manager_v1 import (BlockSpaceManagerV1,\n                                        UncachedBlockAllocator)\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.sequence import Logprob, Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nfrom .utils import create_dummy_prompt, create_dummy_prompt_encoder_decoder\n\n\ndef test_block_allocator_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n                                           num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    num_free = num_cpu_blocks\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        num_free -= 1\n\n        assert block not in cpu_allocator.free_blocks\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n    with pytest.raises(ValueError):\n        cpu_allocator.allocate()\n\n\ndef test_block_allocator_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n                                           num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    blocks: List[PhysicalTokenBlock] = []\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        blocks.append(block)\n        assert block not in cpu_allocator.free_blocks\n\n    # Free all allocated cpu blocks.\n    num_free = 0\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for block in blocks:\n        cpu_allocator.free(block)\n        num_free += 1\n        assert block in cpu_allocator.free_blocks\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n        with pytest.raises(ValueError):\n            cpu_allocator.free(block)\n\n\ndef test_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same sequence group to all available gpu blocks.\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n    # Allocate same sequence group to all available gpu blocks.\n    # Use watermark to reserve one gpu block.\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=1 / num_gpu_blocks)\n    for i in range(num_gpu_blocks - 1):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n\ndef test_allocate_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_req_per_seq_group = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same sequence group to all available gpu blocks.\n    for i in range(num_gpu_blocks // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            str(i),\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n    # Allocate same sequence group to all available gpu blocks.\n    # Use watermark to reserve one gpu block.\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=1 / num_gpu_blocks)\n    for i in range((num_gpu_blocks - 1) // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            str(i),\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n\ndef test_allocate_encoder_decoder_fails_with_swa():\n    # SWA short for sliding window attention\n\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0,\n                                        sliding_window=5)  # swa\n\n    # Allocate same sequence group to all available gpu blocks.\n    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n        \"0\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n\n    # Assert that can_allocate() fails due to SWA\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.can_allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n\n    # Assert that allocate() fails due to SWA\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n\n\ndef test_allocate_encoder_decoder_fails_with_prefix_caching():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0,\n                                        enable_caching=True)  # Prefix cache\n\n    # Allocate same sequence group to all available gpu blocks.\n    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n        \"0\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n\n    # Assert that can_allocate() fails due to prefix caching\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.can_allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n\n    # Assert that allocate() fails due to prefix caching\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n\n\ndef test_append_slot_single_seq():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate single seq to gpu block.\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Nothing to append. Sequence has no new logical blocks.\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slots(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks == after_blocks\n\n    # Add block_size number of new tokens and append slot.\n    for i in range(block_size):\n        token_id = i + 5\n        prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slots(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_append_slot_cow():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size=block_size,\n                                        num_cpu_blocks=num_cpu_blocks,\n                                        num_gpu_blocks=num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate prompt to gpu block. There is one slot left in the block.\n    prompt = Sequence(seq_id=1,\n                      inputs={\n                          \"prompt\": \"one two three\",\n                          \"prompt_token_ids\": [1, 2, 3],\n                      },\n                      block_size=block_size)\n\n    # Fork the sequence, such that a COW will be required when we append a new\n    # token id.\n    child = prompt.fork(new_seq_id=2)\n\n    # Allocate space for the sequence group.\n    seq_group = SequenceGroup(request_id=\"1\",\n                              seqs=[prompt, child],\n                              arrival_time=time.time(),\n                              sampling_params=SamplingParams())\n    block_manager.allocate(seq_group)\n\n    # Fork and append a new token id. We expect a COW to be scheduled.\n    token_id = 4\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.fork(prompt, child)\n\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n\n    cows = block_manager.append_slots(child)\n    assert cows\n    dict_cows = defaultdict(list)\n    for src_block, dst_block in cows:\n        dict_cows[src_block].append(dst_block)\n    for src_block, dst_blocks in dict_cows.items():\n        assert src_block not in dst_blocks\n\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_fork():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\",\n                                            block_size - 1,\n                                            block_size=block_size)\n    block_manager.allocate(seq_group)\n\n    # Fork prompt and copy block tables.\n    child = prompt.fork(2)\n    block_manager.fork(prompt, child)\n    assert block_manager.get_block_table(\n        prompt) == block_manager.get_block_table(child)\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(child)\n    assert block_manager.get_block_table(\n        prompt) != block_manager.get_block_table(child)\n\n\ndef test_swap():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n    prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    prompt.status = SequenceStatus.RUNNING\n    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap seq group from GPU -> CPU.\n    gpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    assert [x[0] for x in mapping] == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    prompt.status = SequenceStatus.SWAPPED\n\n    # Swap seq group from CPU -> GPU.\n    cpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    assert [x[0] for x in mapping] == cpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\ndef test_swap_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    decoder_prompt, encoder_prompt, seq_group = \\\n        create_dummy_prompt_encoder_decoder(\n        \"1\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n    decoder_prompt.status = SequenceStatus.WAITING\n    encoder_prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    decoder_prompt.status = SequenceStatus.RUNNING\n    decoder_prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap encoder/decoder seq group from GPU -> CPU.\n    decoder_gpu_blocks = block_manager.get_block_table(decoder_prompt)\n    cross_gpu_blocks = block_manager.get_cross_block_table(seq_group)\n    gpu_blocks = decoder_gpu_blocks + cross_gpu_blocks\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    assert [x[0] for x in mapping] == gpu_blocks\n    #assert list(mapping.keys()) == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    decoder_prompt.status = SequenceStatus.SWAPPED\n\n    # Swap encoder/decoder seq group from CPU -> GPU.\n    decoder_cpu_blocks = block_manager.get_block_table(decoder_prompt)\n    cross_cpu_blocks = block_manager.get_cross_block_table(seq_group)\n    cpu_blocks = decoder_cpu_blocks + cross_cpu_blocks\n    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    assert [x[0] for x in mapping] == cpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\ndef test_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Free allocated seq.\n    prompt_blocks = len(block_manager.get_block_table(prompt))\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    block_manager.free(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert after_blocks == before_blocks + prompt_blocks\n\n    # Block table for freed seq is deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(prompt)\n\n\ndef test_free_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    decoder_prompt, encoder_prompt, seq_group = \\\n        create_dummy_prompt_encoder_decoder(\n        \"1\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n    block_manager.allocate(seq_group)\n\n    # Free allocated seq.\n    decoder_prompt_blocks = len(block_manager.get_block_table(decoder_prompt))\n    encoder_prompt_blocks = len(block_manager.get_cross_block_table(seq_group))\n    prompt_blocks = decoder_prompt_blocks + encoder_prompt_blocks\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    block_manager.free(decoder_prompt)\n    block_manager.free_cross(seq_group)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert after_blocks == before_blocks + prompt_blocks\n\n    # Block table for freed encoder & decoder seq's are deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(decoder_prompt)\n\n    # Block table for freed encoder & decoder seq's are deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(encoder_prompt)\n\n\ndef test_reset():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same seq group on all available gpu blocks.\n    original_blocks = block_manager.get_num_free_gpu_blocks()\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        block_manager.allocate(seq_group)\n    assert block_manager.get_num_free_gpu_blocks() == 0\n\n    # Resetting block manager frees all allocated blocks.\n    block_manager.reset()\n    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n\n\ndef test_reset_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_req_per_seq_group = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same seq group on all available gpu blocks.\n    original_blocks = block_manager.get_num_free_gpu_blocks()\n    for i in range(num_gpu_blocks // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            f\"{i}\",\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        block_manager.allocate(seq_group)\n    assert block_manager.get_num_free_gpu_blocks() == 0\n\n    # Resetting block manager frees all allocated blocks.\n    block_manager.reset()\n    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n\n\ndef test_sliding_window_multi_seq():\n    \"\"\"\n    Tests that memory allocation and deallocation is handled\n    correctly with multiple sequences that exceed the sliding\n    window's capacity.\n    \"\"\"\n    block_size = 1\n    num_cpu_blocks = 8\n    num_gpu_blocks = 8\n    sliding_window = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        sliding_window=sliding_window,\n                                        watermark=0)\n\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n\n    parent = Sequence(seq_id=1,\n                      inputs={\n                          \"prompt\": \"one two three\",\n                          \"prompt_token_ids\": [0, 1, 2],\n                      },\n                      block_size=block_size)\n    seq_group = SequenceGroup(request_id=\"1\",\n                              seqs=[parent],\n                              arrival_time=time.time(),\n                              sampling_params=SamplingParams(),\n                              lora_request=None)\n    block_manager.allocate(seq_group)\n\n    # assert the number of blocks allocated is correct\n    # the parent seq has len 3, but since sliding_window is 2,\n    # we will use at most 2 blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # Fork prompt and copy block tables.\n    child = parent.fork(2)\n    block_manager.fork(parent, child)\n\n    # assert the number of blocks allocated is correct\n    # forking does not increase memory consumption\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # assert both parent and child share all blocks\n    assert block_manager.get_block_table(\n        parent) == block_manager.get_block_table(child)\n\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(child)\n\n    # assert the number of blocks allocated is correct\n    # we will use now one block more. Each seq will use 2 blocks,\n    # but only one can be shared\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    token_id = 5\n    parent.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(parent)\n\n    # assert the number of blocks allocated is correct\n    # no change, because both sequences are still just sharing one block\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    block_table_parent = block_manager.get_block_table(parent)\n    block_table_child = block_manager.get_block_table(child)\n\n    assert block_table_parent != block_table_child\n\n    # assert both blocks are sharing the second-last block\n    assert block_table_parent[-2] == block_table_child[-2]\n\n    # now let's clean up...\n    block_manager.free(parent)\n\n    # assert the number of blocks allocated is correct\n    # We have freed one seq, reducing the ref count of two blocks by one.\n    # One of the two was only used by the parent seq, so this is now free.\n    # The child seq still consumes sliding_window blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # free all blocks\n    block_manager.free(child)\n\n    # assert all blocks are free now\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n\n\ndef test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n    should only mark a chunk of blocks as computed instead of all blocks.\n    \"\"\"\n\n    block_size = 4\n    num_cpu_blocks = 0\n    num_gpu_blocks = 16\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_gpu_blocks,\n                                        num_cpu_blocks,\n                                        watermark=0,\n                                        enable_caching=True)\n\n    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n    prompt_length = block_size * num_gpu_blocks - 1\n\n    # Allocate (reserve) all blocks.\n    _, seq_group = create_dummy_prompt(\"0\",\n                                       prompt_length,\n                                       block_size=block_size)\n    block_manager.allocate(seq_group)\n    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n\n    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n    token_chunk_size = int(block_size * 2.5)\n    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n    assert len(computed_blocks) == 2\n\n    # Actual computed tokens.\n    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n\n    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n    token_chunk_size = int(block_size * 4.5)\n    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n    assert len(computed_blocks) == 7\n",
      "diff": "diff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\ndeleted file mode 100644\nindex 2ee9f2082..000000000\n--- a/tests/core/test_block_manager.py\n+++ /dev/null\n@@ -1,637 +0,0 @@\n-import time\n-from collections import defaultdict\n-from typing import List\n-\n-import pytest\n-\n-from vllm import SamplingParams\n-from vllm.block import PhysicalTokenBlock\n-from vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n-                                   STR_NOT_IMPL_ENC_DEC_SWA)\n-from vllm.core.block_manager_v1 import (BlockSpaceManagerV1,\n-                                        UncachedBlockAllocator)\n-from vllm.core.interfaces import AllocStatus\n-from vllm.sequence import Logprob, Sequence, SequenceGroup, SequenceStatus\n-from vllm.utils import Device\n-\n-from .utils import create_dummy_prompt, create_dummy_prompt_encoder_decoder\n-\n-\n-def test_block_allocator_allocate():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n-                                           num_cpu_blocks)\n-\n-    # Allocate all available cpu blocks.\n-    num_free = num_cpu_blocks\n-    assert cpu_allocator.get_num_free_blocks() == num_free\n-    for _ in range(num_cpu_blocks):\n-        block = cpu_allocator.allocate()\n-        num_free -= 1\n-\n-        assert block not in cpu_allocator.free_blocks\n-        assert cpu_allocator.get_num_free_blocks() == num_free\n-\n-    with pytest.raises(ValueError):\n-        cpu_allocator.allocate()\n-\n-\n-def test_block_allocator_free():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n-                                           num_cpu_blocks)\n-\n-    # Allocate all available cpu blocks.\n-    blocks: List[PhysicalTokenBlock] = []\n-    for _ in range(num_cpu_blocks):\n-        block = cpu_allocator.allocate()\n-        blocks.append(block)\n-        assert block not in cpu_allocator.free_blocks\n-\n-    # Free all allocated cpu blocks.\n-    num_free = 0\n-    assert cpu_allocator.get_num_free_blocks() == num_free\n-    for block in blocks:\n-        cpu_allocator.free(block)\n-        num_free += 1\n-        assert block in cpu_allocator.free_blocks\n-        assert cpu_allocator.get_num_free_blocks() == num_free\n-\n-        with pytest.raises(ValueError):\n-            cpu_allocator.free(block)\n-\n-\n-def test_allocate():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    for i in range(num_gpu_blocks):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    # Use watermark to reserve one gpu block.\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=1 / num_gpu_blocks)\n-    for i in range(num_gpu_blocks - 1):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-\n-def test_allocate_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_req_per_seq_group = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    for i in range(num_gpu_blocks // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            str(i),\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    # Use watermark to reserve one gpu block.\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=1 / num_gpu_blocks)\n-    for i in range((num_gpu_blocks - 1) // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            str(i),\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n-        block_manager.allocate(seq_group)\n-    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n-\n-\n-def test_allocate_encoder_decoder_fails_with_swa():\n-    # SWA short for sliding window attention\n-\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        sliding_window=5)  # swa\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-        \"0\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-\n-    # Assert that can_allocate() fails due to SWA\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.can_allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n-\n-    # Assert that allocate() fails due to SWA\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n-\n-\n-def test_allocate_encoder_decoder_fails_with_prefix_caching():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=True)  # Prefix cache\n-\n-    # Allocate same sequence group to all available gpu blocks.\n-    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-        \"0\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-\n-    # Assert that can_allocate() fails due to prefix caching\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.can_allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n-\n-    # Assert that allocate() fails due to prefix caching\n-    with pytest.raises(NotImplementedError) as exc_info:\n-        block_manager.allocate(seq_group)\n-\n-    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n-\n-\n-def test_append_slot_single_seq():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate single seq to gpu block.\n-    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Nothing to append. Sequence has no new logical blocks.\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert not block_manager.append_slots(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks == after_blocks\n-\n-    # Add block_size number of new tokens and append slot.\n-    for i in range(block_size):\n-        token_id = i + 5\n-        prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert not block_manager.append_slots(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks - after_blocks == 1\n-\n-\n-def test_append_slot_cow():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size=block_size,\n-                                        num_cpu_blocks=num_cpu_blocks,\n-                                        num_gpu_blocks=num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate prompt to gpu block. There is one slot left in the block.\n-    prompt = Sequence(seq_id=1,\n-                      inputs={\n-                          \"prompt\": \"one two three\",\n-                          \"prompt_token_ids\": [1, 2, 3],\n-                      },\n-                      block_size=block_size)\n-\n-    # Fork the sequence, such that a COW will be required when we append a new\n-    # token id.\n-    child = prompt.fork(new_seq_id=2)\n-\n-    # Allocate space for the sequence group.\n-    seq_group = SequenceGroup(request_id=\"1\",\n-                              seqs=[prompt, child],\n-                              arrival_time=time.time(),\n-                              sampling_params=SamplingParams())\n-    block_manager.allocate(seq_group)\n-\n-    # Fork and append a new token id. We expect a COW to be scheduled.\n-    token_id = 4\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.fork(prompt, child)\n-\n-    assert block_manager.can_append_slots(seq_group)\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-\n-    cows = block_manager.append_slots(child)\n-    assert cows\n-    dict_cows = defaultdict(list)\n-    for src_block, dst_block in cows:\n-        dict_cows[src_block].append(dst_block)\n-    for src_block, dst_blocks in dict_cows.items():\n-        assert src_block not in dst_blocks\n-\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_blocks - after_blocks == 1\n-\n-\n-def test_fork():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\",\n-                                            block_size - 1,\n-                                            block_size=block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Fork prompt and copy block tables.\n-    child = prompt.fork(2)\n-    block_manager.fork(prompt, child)\n-    assert block_manager.get_block_table(\n-        prompt) == block_manager.get_block_table(child)\n-    token_id = 4\n-    # Append token to child. Block is shared so copy on write occurs.\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(child)\n-    assert block_manager.get_block_table(\n-        prompt) != block_manager.get_block_table(child)\n-\n-\n-def test_swap():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n-    prompt.status = SequenceStatus.WAITING\n-    block_manager.allocate(seq_group)\n-\n-    # Emulate a forward pass by appending a single token.\n-    # The block manager then knows how many unprocessed\n-    # tokens will be written in the next forward pass.\n-    token_id = 0\n-    prompt.status = SequenceStatus.RUNNING\n-    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    # Swap seq group from GPU -> CPU.\n-    gpu_blocks = block_manager.get_block_table(prompt)\n-    assert block_manager.can_swap_out(seq_group)\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_out(seq_group)\n-    assert [x[0] for x in mapping] == gpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n-    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n-    prompt.status = SequenceStatus.SWAPPED\n-\n-    # Swap seq group from CPU -> GPU.\n-    cpu_blocks = block_manager.get_block_table(prompt)\n-    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_in(seq_group)\n-    assert [x[0] for x in mapping] == cpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n-    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n-\n-\n-def test_swap_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    decoder_prompt, encoder_prompt, seq_group = \\\n-        create_dummy_prompt_encoder_decoder(\n-        \"1\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-    decoder_prompt.status = SequenceStatus.WAITING\n-    encoder_prompt.status = SequenceStatus.WAITING\n-    block_manager.allocate(seq_group)\n-\n-    # Emulate a forward pass by appending a single token.\n-    # The block manager then knows how many unprocessed\n-    # tokens will be written in the next forward pass.\n-    token_id = 0\n-    decoder_prompt.status = SequenceStatus.RUNNING\n-    decoder_prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n-\n-    # Swap encoder/decoder seq group from GPU -> CPU.\n-    decoder_gpu_blocks = block_manager.get_block_table(decoder_prompt)\n-    cross_gpu_blocks = block_manager.get_cross_block_table(seq_group)\n-    gpu_blocks = decoder_gpu_blocks + cross_gpu_blocks\n-    assert block_manager.can_swap_out(seq_group)\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_out(seq_group)\n-    assert [x[0] for x in mapping] == gpu_blocks\n-    #assert list(mapping.keys()) == gpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n-    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n-    decoder_prompt.status = SequenceStatus.SWAPPED\n-\n-    # Swap encoder/decoder seq group from CPU -> GPU.\n-    decoder_cpu_blocks = block_manager.get_block_table(decoder_prompt)\n-    cross_cpu_blocks = block_manager.get_cross_block_table(seq_group)\n-    cpu_blocks = decoder_cpu_blocks + cross_cpu_blocks\n-    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n-    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    mapping = block_manager.swap_in(seq_group)\n-    assert [x[0] for x in mapping] == cpu_blocks\n-    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n-    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n-    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n-\n-\n-def test_free():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Free allocated seq.\n-    prompt_blocks = len(block_manager.get_block_table(prompt))\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    block_manager.free(prompt)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert after_blocks == before_blocks + prompt_blocks\n-\n-    # Block table for freed seq is deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(prompt)\n-\n-\n-def test_free_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    decoder_prompt, encoder_prompt, seq_group = \\\n-        create_dummy_prompt_encoder_decoder(\n-        \"1\",\n-        decoder_prompt_length=block_size,\n-        encoder_prompt_length=block_size)\n-    block_manager.allocate(seq_group)\n-\n-    # Free allocated seq.\n-    decoder_prompt_blocks = len(block_manager.get_block_table(decoder_prompt))\n-    encoder_prompt_blocks = len(block_manager.get_cross_block_table(seq_group))\n-    prompt_blocks = decoder_prompt_blocks + encoder_prompt_blocks\n-    before_blocks = block_manager.get_num_free_gpu_blocks()\n-    block_manager.free(decoder_prompt)\n-    block_manager.free_cross(seq_group)\n-    after_blocks = block_manager.get_num_free_gpu_blocks()\n-    assert after_blocks == before_blocks + prompt_blocks\n-\n-    # Block table for freed encoder & decoder seq's are deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(decoder_prompt)\n-\n-    # Block table for freed encoder & decoder seq's are deleted.\n-    with pytest.raises(KeyError):\n-        block_manager.get_block_table(encoder_prompt)\n-\n-\n-def test_reset():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same seq group on all available gpu blocks.\n-    original_blocks = block_manager.get_num_free_gpu_blocks()\n-    for i in range(num_gpu_blocks):\n-        _, seq_group = create_dummy_prompt(str(i), block_size)\n-        block_manager.allocate(seq_group)\n-    assert block_manager.get_num_free_gpu_blocks() == 0\n-\n-    # Resetting block manager frees all allocated blocks.\n-    block_manager.reset()\n-    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n-\n-\n-def test_reset_encoder_decoder():\n-    block_size = 4\n-    num_cpu_blocks = 4\n-    num_gpu_blocks = 4\n-    block_req_per_seq_group = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        watermark=0)\n-\n-    # Allocate same seq group on all available gpu blocks.\n-    original_blocks = block_manager.get_num_free_gpu_blocks()\n-    for i in range(num_gpu_blocks // block_req_per_seq_group):\n-        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n-            f\"{i}\",\n-            decoder_prompt_length=block_size,\n-            encoder_prompt_length=block_size)\n-        block_manager.allocate(seq_group)\n-    assert block_manager.get_num_free_gpu_blocks() == 0\n-\n-    # Resetting block manager frees all allocated blocks.\n-    block_manager.reset()\n-    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n-\n-\n-def test_sliding_window_multi_seq():\n-    \"\"\"\n-    Tests that memory allocation and deallocation is handled\n-    correctly with multiple sequences that exceed the sliding\n-    window's capacity.\n-    \"\"\"\n-    block_size = 1\n-    num_cpu_blocks = 8\n-    num_gpu_blocks = 8\n-    sliding_window = 2\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_cpu_blocks,\n-                                        num_gpu_blocks,\n-                                        sliding_window=sliding_window,\n-                                        watermark=0)\n-\n-    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n-\n-    parent = Sequence(seq_id=1,\n-                      inputs={\n-                          \"prompt\": \"one two three\",\n-                          \"prompt_token_ids\": [0, 1, 2],\n-                      },\n-                      block_size=block_size)\n-    seq_group = SequenceGroup(request_id=\"1\",\n-                              seqs=[parent],\n-                              arrival_time=time.time(),\n-                              sampling_params=SamplingParams(),\n-                              lora_request=None)\n-    block_manager.allocate(seq_group)\n-\n-    # assert the number of blocks allocated is correct\n-    # the parent seq has len 3, but since sliding_window is 2,\n-    # we will use at most 2 blocks\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # Fork prompt and copy block tables.\n-    child = parent.fork(2)\n-    block_manager.fork(parent, child)\n-\n-    # assert the number of blocks allocated is correct\n-    # forking does not increase memory consumption\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # assert both parent and child share all blocks\n-    assert block_manager.get_block_table(\n-        parent) == block_manager.get_block_table(child)\n-\n-    token_id = 4\n-    # Append token to child. Block is shared so copy on write occurs.\n-    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(child)\n-\n-    # assert the number of blocks allocated is correct\n-    # we will use now one block more. Each seq will use 2 blocks,\n-    # but only one can be shared\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window - 1\n-\n-    token_id = 5\n-    parent.append_token_id(token_id, {token_id: Logprob(0.0)})\n-    block_manager.append_slots(parent)\n-\n-    # assert the number of blocks allocated is correct\n-    # no change, because both sequences are still just sharing one block\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window - 1\n-\n-    block_table_parent = block_manager.get_block_table(parent)\n-    block_table_child = block_manager.get_block_table(child)\n-\n-    assert block_table_parent != block_table_child\n-\n-    # assert both blocks are sharing the second-last block\n-    assert block_table_parent[-2] == block_table_child[-2]\n-\n-    # now let's clean up...\n-    block_manager.free(parent)\n-\n-    # assert the number of blocks allocated is correct\n-    # We have freed one seq, reducing the ref count of two blocks by one.\n-    # One of the two was only used by the parent seq, so this is now free.\n-    # The child seq still consumes sliding_window blocks\n-    assert block_manager.get_num_free_gpu_blocks(\n-    ) == num_gpu_blocks - sliding_window\n-\n-    # free all blocks\n-    block_manager.free(child)\n-\n-    # assert all blocks are free now\n-    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n-\n-\n-def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n-    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n-    should only mark a chunk of blocks as computed instead of all blocks.\n-    \"\"\"\n-\n-    block_size = 4\n-    num_cpu_blocks = 0\n-    num_gpu_blocks = 16\n-    block_manager = BlockSpaceManagerV1(block_size,\n-                                        num_gpu_blocks,\n-                                        num_cpu_blocks,\n-                                        watermark=0,\n-                                        enable_caching=True)\n-\n-    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n-    prompt_length = block_size * num_gpu_blocks - 1\n-\n-    # Allocate (reserve) all blocks.\n-    _, seq_group = create_dummy_prompt(\"0\",\n-                                       prompt_length,\n-                                       block_size=block_size)\n-    block_manager.allocate(seq_group)\n-    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n-\n-    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n-    token_chunk_size = int(block_size * 2.5)\n-    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n-    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n-    assert len(computed_blocks) == 2\n-\n-    # Actual computed tokens.\n-    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n-\n-    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n-    token_chunk_size = int(block_size * 4.5)\n-    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n-    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n-    assert len(computed_blocks) == 7",
      "change_type": "deleted",
      "lines_added": 1,
      "lines_removed": 638
    },
    {
      "file_path": "tests/core/test_chunked_prefill_scheduler.py",
      "old_content": "from typing import List\nfrom unittest.mock import MagicMock\n\nimport pytest  # noqa\n\nfrom vllm.config import CacheConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.core.scheduler import Scheduler\nfrom vllm.sequence import Logprob, SequenceGroup\n\nfrom ..utils import check_deprecated_block_manager_usage\nfrom .utils import create_dummy_prompt\n\n\ndef get_sequence_groups(scheduler_output):\n    return [s.seq_group for s in scheduler_output.scheduled_seq_groups]\n\n\ndef append_new_token(seq_group, token_id: int):\n    for seq in seq_group.get_seqs():\n        seq.append_token_id(token_id, {token_id: Logprob(token_id)})\n\n\ndef schedule_and_update_computed_tokens(scheduler):\n    metas, out, _ = scheduler.schedule()\n    for s, meta in zip(out.scheduled_seq_groups, metas):\n        s.seq_group.update_num_computed_tokens(meta.token_chunk_size)\n    return metas, out\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/core/test_chunked_prefill_scheduler.py')\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_simple(use_v2_block_manager: bool):\n    \"\"\"Verify basic scheduling works.\"\"\"\n    block_size = 4\n    num_seq_group = 4\n    max_model_len = 16\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        num_seq_group,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=block_size,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Schedule seq groups prompts.\n    num_tokens = block_size * num_seq_group\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_tokens\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    for s in running:\n        append_new_token(s, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_seq_group\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_chunk(use_v2_block_manager: bool):\n    \"\"\"Verify prefills are chunked properly.\"\"\"\n    block_size = 4\n    max_seqs = 60\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 32\n    cache_config.num_gpu_blocks = 32\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Verify the second request is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    print()\n    assert set(get_sequence_groups(out)) == set(running)\n    assert seq_group_meta[0].token_chunk_size == 60\n    # Verify it is chunked.\n    assert seq_group_meta[1].token_chunk_size == 4\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # One chunked prefill, and one decoding.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    # The first one is prefill. Scheduler guarantees ordering.\n    assert seq_group_meta[0].token_chunk_size == 56\n    # The second one is a chunked prefill.\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 57\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_complex(use_v2_block_manager: bool):\n    block_size = 4\n    max_seqs = 60\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 64\n    cache_config.num_gpu_blocks = 64\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n        assert seq_group.is_prefill()\n\n    # Verify the second request is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n\n    assert set(get_sequence_groups(out)) == set(running)\n    assert seq_group_meta[0].token_chunk_size == 60\n    # Verify it is chunked.\n    assert seq_group_meta[1].token_chunk_size == 4\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # Add 2 more requests.\n    for i in range(2, 4):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Decoding & chunked prefill & first chunk of 3rd request is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 3\n    # The first one is the first chunked prefill.\n    assert seq_group_meta[0].token_chunk_size == 7\n    # The second one is the second new chunked prefill.\n    assert seq_group_meta[1].token_chunk_size == 56\n    # The last one is decode.\n    assert seq_group_meta[2].token_chunk_size == 1\n    # Two of them are in chunked prefill.\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # The first 2 requests are now in decodine phase.\n    append_new_token(running[0], 1)\n    assert not running[0].is_prefill()\n    append_new_token(running[1], 1)\n    assert not running[1].is_prefill()\n    # The third request is still in prefill stage.\n    assert running[2].is_prefill()\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_maximal_decoding(use_v2_block_manager: bool):\n    \"\"\"Verify decoding requests are prioritized.\"\"\"\n    block_size = 4\n    max_seqs = 2\n    max_model_len = 8\n    max_num_batched_tokens = 2\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=2,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n        assert seq_group.is_prefill()\n\n    # The first prefill is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 1\n    assert seq_group_meta[0].token_chunk_size == 2\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # Create one more seq_group.\n    _, seq_group = create_dummy_prompt(\"3\",\n                                       prompt_length=2,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n    # The first decoding + second chunk is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert running[2].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n\n    # Decoding + running prefill is prioritized.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n    append_new_token(running[1], 1)\n\n    # Only decoding is prioritized.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n    assert out.num_prefill_groups == 0\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n    append_new_token(running[1], 1)\n\n    # After aborting the decoding request, the fcfs new prefill is prioritized.\n    scheduler.abort_seq_group(running[0].request_id)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[1].is_prefill()\n    assert running[2].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prompt_limit(use_v2_block_manager: bool):\n    \"\"\"Verify max_num_batched_tokens < max_model_len is possible.\"\"\"\n    block_size = 4\n    max_seqs = 32\n    max_model_len = 64\n    max_num_batched_tokens = 32\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 16\n    cache_config.num_gpu_blocks = 16\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=48,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n\n    # The prompt length > max_num_batched_tokens should be still scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 1\n    assert seq_group_meta[0].token_chunk_size == 32\n    assert running[0].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 32\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prompt_limit_exceed(use_v2_block_manager: bool):\n    block_size = 4\n    max_seqs = 64\n    max_model_len = 32\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 16\n    cache_config.num_gpu_blocks = 16\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n    _, seq_group = create_dummy_prompt(\"2\",\n                                       prompt_length=48,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.ignored_seq_groups) == 1\n    assert out.ignored_seq_groups[0] == seq_group\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_swap(use_v2_block_manager: bool):\n    \"\"\"Verify swapping works with chunked prefill requests\"\"\"\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 16\n    cache_config.num_gpu_blocks = 16\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=60,\n                                       best_of=2,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # The running prefill is now swapped.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out != []\n    assert out.blocks_to_swap_in == []\n\n    # Add 1 more task. Swap should be prioritized over new prefill.\n    _, seq_group = create_dummy_prompt(\"2\", prompt_length=60)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in != []\n    assert out.blocks_to_swap_out == []\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 32\n    cache_config.num_gpu_blocks = 32\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=60,\n                                       best_of=2,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # The running prefill is now swapped.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out != []\n    assert out.blocks_to_swap_in == []\n\n    # Add 1 more task. Swap is not possible, so prefill is running.\n    scheduler.block_manager.can_swap_in = MagicMock()\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.LATER\n\n    _, seq_group2 = create_dummy_prompt(\"2\",\n                                        prompt_length=60,\n                                        block_size=block_size)\n    scheduler.add_seq_group(seq_group2)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n\n    # Now although swap is possible, running prefill is prioritized.\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.OK\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert not seq_group2.is_prefill()\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n    append_new_token(seq_group2, 1)\n\n    # Decoding is prioritized.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 1\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert not seq_group2.is_prefill()\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n    append_new_token(seq_group2, 1)\n\n    # Since we abort the sequence group, we can finally swap.\n    scheduler.abort_seq_group(seq_group2.request_id)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in != []\n    assert out.blocks_to_swap_out == []\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_chunked_prefill_preempt(use_v2_block_manager: bool):\n    \"\"\"Verify preempt works with chunked prefill requests\"\"\"\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 16\n    cache_config.num_gpu_blocks = 16\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=60,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The request should be preempted.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group1(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group1)\n\n    # The running prefill is now preempted.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out == []\n    assert out.blocks_to_swap_in == []\n\n    # Make sure we can reschedule preempted request.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n    assert seq_group.get_num_uncomputed_tokens() == 30\n\n    # We should be able to run prefill twice as it is chunked.\n    def cannot_append_second_group2(seq_group, num_lookahead_slots):\n        return True\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group2)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert not seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n    block_size = 4\n    max_seqs = 2\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 128\n    cache_config.num_gpu_blocks = 128\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=65,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    # The first prefill is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert seq_group_meta[0].token_chunk_size == max_num_batched_tokens\n    assert len(get_sequence_groups(out)) == 1\n\n    # Add new requests.\n    for i in range(4):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=65,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Make sure only 2 requests are scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_batched_tokens == max_num_batched_tokens\n    assert len(get_sequence_groups(out)) == 2\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    append_new_token(running[0], 1)\n\n    # Although we have enough token budget, we can only schedule max_seqs.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert seq_group_meta[0].token_chunk_size == 2\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert out.num_batched_tokens == 3\n    assert len(get_sequence_groups(out)) == max_seqs\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_perfix_caching(use_v2_block_manager: bool):\n    \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n    block_size = 4\n    max_seqs = 10\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(\n        max_num_batched_tokens,\n        max_seqs,\n        max_model_len,\n        enable_chunked_prefill=True,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size,\n                               1.0,\n                               1,\n                               \"auto\",\n                               enable_prefix_caching=True)\n    cache_config.num_cpu_blocks = 0\n    cache_config.num_gpu_blocks = 32\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           block_size=block_size,\n                                           prompt_length=50)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert seq_group_meta[0].token_chunk_size == 50\n    # Verify it is chunked. Note that although the budget is 64-50=14,\n    # we only allocate full blocks for prefix caching, so only 4*(14//4)=12\n    # tokens are allocated.\n    assert seq_group_meta[1].token_chunk_size == 12\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 62\n",
      "diff": "diff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex c9495fd50..f97caa06f 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -8,7 +8,6 @@ from vllm.core.interfaces import AllocStatus\n from vllm.core.scheduler import Scheduler\n from vllm.sequence import Logprob, SequenceGroup\n \n-from ..utils import check_deprecated_block_manager_usage\n from .utils import create_dummy_prompt\n \n \n@@ -28,25 +27,16 @@ def schedule_and_update_computed_tokens(scheduler):\n     return metas, out\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/core/test_chunked_prefill_scheduler.py')\n-\n-\n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_simple(use_v2_block_manager: bool):\n+def test_simple():\n     \"\"\"Verify basic scheduling works.\"\"\"\n     block_size = 4\n     num_seq_group = 4\n     max_model_len = 16\n     max_num_batched_tokens = 64\n-    scheduler_config = SchedulerConfig(\n-        max_num_batched_tokens,\n-        num_seq_group,\n-        max_model_len,\n-        enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n+                                       num_seq_group,\n+                                       max_model_len,\n+                                       enable_chunked_prefill=True)\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -81,8 +71,7 @@ def test_simple(use_v2_block_manager: bool):\n     assert len(seq_group_meta) == num_seq_group\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunk(use_v2_block_manager: bool):\n+def test_chunk():\n     \"\"\"Verify prefills are chunked properly.\"\"\"\n     block_size = 4\n     max_seqs = 60\n@@ -93,7 +82,7 @@ def test_chunk(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 32\n     cache_config.num_gpu_blocks = 32\n@@ -131,8 +120,7 @@ def test_chunk(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 57\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_complex(use_v2_block_manager: bool):\n+def test_complex():\n     block_size = 4\n     max_seqs = 60\n     max_model_len = 80\n@@ -142,7 +130,7 @@ def test_complex(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 64\n     cache_config.num_gpu_blocks = 64\n@@ -201,8 +189,7 @@ def test_complex(use_v2_block_manager: bool):\n     assert running[2].is_prefill()\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_maximal_decoding(use_v2_block_manager: bool):\n+def test_maximal_decoding():\n     \"\"\"Verify decoding requests are prioritized.\"\"\"\n     block_size = 4\n     max_seqs = 2\n@@ -213,7 +200,7 @@ def test_maximal_decoding(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -295,8 +282,7 @@ def test_maximal_decoding(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 2\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prompt_limit(use_v2_block_manager: bool):\n+def test_prompt_limit():\n     \"\"\"Verify max_num_batched_tokens < max_model_len is possible.\"\"\"\n     block_size = 4\n     max_seqs = 32\n@@ -307,7 +293,7 @@ def test_prompt_limit(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -330,8 +316,7 @@ def test_prompt_limit(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == 32\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prompt_limit_exceed(use_v2_block_manager: bool):\n+def test_prompt_limit_exceed():\n     block_size = 4\n     max_seqs = 64\n     max_model_len = 32\n@@ -356,8 +341,7 @@ def test_prompt_limit_exceed(use_v2_block_manager: bool):\n     assert out.ignored_seq_groups[0] == seq_group\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_swap(use_v2_block_manager: bool):\n+def test_swap():\n     \"\"\"Verify swapping works with chunked prefill requests\"\"\"\n     block_size = 4\n     max_seqs = 30\n@@ -368,7 +352,7 @@ def test_swap(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -414,8 +398,7 @@ def test_swap(use_v2_block_manager: bool):\n     assert out.blocks_to_swap_out == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n+def test_running_prefill_prioritized_over_swap():\n     block_size = 4\n     max_seqs = 30\n     max_model_len = 200\n@@ -425,7 +408,7 @@ def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 32\n     cache_config.num_gpu_blocks = 32\n@@ -508,8 +491,7 @@ def test_running_prefill_prioritized_over_swap(use_v2_block_manager: bool):\n     assert out.blocks_to_swap_out == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n+def test_chunked_prefill_preempt():\n     \"\"\"Verify preempt works with chunked prefill requests\"\"\"\n     block_size = 4\n     max_seqs = 30\n@@ -520,7 +502,7 @@ def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -575,8 +557,7 @@ def test_chunked_prefill_preempt(use_v2_block_manager: bool):\n     assert out.num_batched_tokens == max_num_batched_tokens\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n+def test_chunked_prefill_max_seqs():\n     block_size = 4\n     max_seqs = 2\n     max_model_len = 80\n@@ -586,7 +567,7 @@ def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 128\n     cache_config.num_gpu_blocks = 128\n@@ -629,8 +610,7 @@ def test_chunked_prefill_max_seqs(use_v2_block_manager: bool):\n     assert not running[1].is_prefill()\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_perfix_caching(use_v2_block_manager: bool):\n+def test_perfix_caching():\n     \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n     block_size = 4\n     max_seqs = 10\n@@ -641,7 +621,7 @@ def test_perfix_caching(use_v2_block_manager: bool):\n         max_seqs,\n         max_model_len,\n         enable_chunked_prefill=True,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size,\n                                1.0,\n                                1,",
      "change_type": "modified",
      "lines_added": 25,
      "lines_removed": 45
    },
    {
      "file_path": "tests/core/test_num_computed_tokens_update.py",
      "old_content": "import pytest\n\nfrom tests.conftest import VllmRunner\nfrom tests.core.utils import create_dummy_prompt\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.platforms import current_platform\nfrom vllm.sequence import SequenceGroup\n\nMODEL = \"JackFram/llama-160m\"\n\n\ndef add_seq_group_to_engine(engine: LLMEngine, seq_group: SequenceGroup):\n    scheduler = engine.scheduler[0]\n    scheduler.add_seq_group(seq_group)\n\n\n@pytest.mark.parametrize(\"num_scheduler_steps\", [1, 8])\n@pytest.mark.parametrize(\"enable_chunked_prefill\", [False, True])\n@pytest.mark.parametrize(\"enforce_eager\", [False, True])\ndef test_num_computed_tokens_update(num_scheduler_steps: int,\n                                    enable_chunked_prefill: bool,\n                                    enforce_eager: bool):\n\n    is_multi_step = num_scheduler_steps > 1\n    is_multi_step_chunked_prefill = is_multi_step and enable_chunked_prefill\n\n    if is_multi_step_chunked_prefill and current_platform.is_rocm():\n        pytest.skip(\"Multi-step with Chunked-Prefill does not support \"\n                    \"rocm_flash_attn backend\")\n\n    # Make a vllm engine\n    runner = VllmRunner(model_name=MODEL,\n                        gpu_memory_utilization=0.7,\n                        use_v2_block_manager=True,\n                        num_scheduler_steps=num_scheduler_steps,\n                        enable_chunked_prefill=enable_chunked_prefill,\n                        enforce_eager=enforce_eager)\n    engine: LLMEngine = runner.model.llm_engine\n\n    # In multi-step + chunked-prefill there is no separate single prompt step.\n    # What is scheduled will run for num_scheduler_steps always.\n    num_prompt_steps = num_scheduler_steps \\\n        if is_multi_step_chunked_prefill else 1\n\n    num_output_tokens_list = [4, 8, 12, 15, 16, 17]\n\n    # Create sequence and add to engine\n    prompt_len = 10\n\n    for req_idx, num_output_tokens in enumerate(num_output_tokens_list):\n        seq, seq_group = create_dummy_prompt(request_id=str(req_idx),\n                                             prompt_length=prompt_len,\n                                             min_tokens=num_output_tokens,\n                                             max_tokens=num_output_tokens)\n        add_seq_group_to_engine(engine, seq_group)\n\n        assert seq.data.get_num_computed_tokens() == 0\n\n        for _ in range(num_prompt_steps):\n            # prompt steps\n            engine.step()\n\n        if not seq.is_finished():\n            prompt_num_computed_tokens = seq.data.get_num_computed_tokens()\n            # Test correctness of num_computed_tokens after the prompt steps\n            assert prompt_num_computed_tokens == \\\n                        prompt_len + num_prompt_steps - 1\n\n            decode_step_counter = 0\n            while not seq.is_finished():\n                # Test correctness of num_computed_tokens after the decode steps\n                assert seq.data.get_num_computed_tokens(\n                ) == prompt_num_computed_tokens + decode_step_counter\n                for _ in range(num_scheduler_steps):\n                    # decode step\n                    engine.step()\n                    decode_step_counter += 1\n\n        # Test correctness of num_computed_tokens after the sequence finish.\n        assert seq.data.get_num_computed_tokens(\n        ) == prompt_len + num_output_tokens - 1\n",
      "diff": "diff --git a/tests/core/test_num_computed_tokens_update.py b/tests/core/test_num_computed_tokens_update.py\nindex f3ec24e7b..bd4accab7 100644\n--- a/tests/core/test_num_computed_tokens_update.py\n+++ b/tests/core/test_num_computed_tokens_update.py\n@@ -31,7 +31,6 @@ def test_num_computed_tokens_update(num_scheduler_steps: int,\n     # Make a vllm engine\n     runner = VllmRunner(model_name=MODEL,\n                         gpu_memory_utilization=0.7,\n-                        use_v2_block_manager=True,\n                         num_scheduler_steps=num_scheduler_steps,\n                         enable_chunked_prefill=enable_chunked_prefill,\n                         enforce_eager=enforce_eager)",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 2
    },
    {
      "file_path": "tests/core/test_scheduler.py",
      "old_content": "import time\nfrom collections import deque\nfrom typing import List, Set, Tuple\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom torch import Use  # noqa\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.core.scheduler import Scheduler, SchedulingBudget\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import SequenceGroup, SequenceStatus\n\nfrom ..utils import check_deprecated_block_manager_usage\nfrom .utils import (append_new_token, append_new_token_seq_group,\n                    create_dummy_prompt, get_sequence_groups,\n                    schedule_and_update_computed_tokens)\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        \"tests/core/test_chunked_prefill_scheduler.py\")\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_add_seq_group(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler_config = SchedulerConfig(\n        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, cache_dtype=\"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq group to scheduler.\n    num_seq_group = 4\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           block_size,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        assert scheduler.get_num_unfinished_seq_groups() == i + 1\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_abort_seq_group(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler_config = SchedulerConfig(\n        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 4\n    cache_config.num_gpu_blocks = 4\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add multiple seq groups to scheduler.\n    num_seq_group = 4\n    request_ids: Set[str] = set()\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        scheduler.add_seq_group(seq_group)\n        request_ids.add(str(i))\n\n    # Abort all added seq groups.\n    assert scheduler.get_num_unfinished_seq_groups() == num_seq_group\n    scheduler.abort_seq_group(request_ids)\n    assert scheduler.get_num_unfinished_seq_groups() == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_schedule_simple(use_v2_block_manager: bool):\n    block_size = 4\n    num_seq_group = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(\n        64,\n        num_seq_group,\n        max_model_len,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=block_size,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Schedule seq groups prompts.\n    num_tokens = block_size * num_seq_group\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_tokens\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_seq_group\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    append_new_token(out, 1)\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n    \"\"\"Verify running batched tokens are not applied to prefill requests.\"\"\"\n    block_size = 4\n    max_model_len = 30\n    max_batched_num_tokens = 30\n    scheduler_config = SchedulerConfig(\n        max_batched_num_tokens,\n        2,\n        max_model_len,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 16\n    cache_config.num_gpu_blocks = 16\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    _, seq_group_a = create_dummy_prompt(\"1\", 1, block_size=block_size)\n    scheduler.add_seq_group(seq_group_a)\n\n    # Schedule seq groups prompts.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a]\n\n    # Add a new prefill request B.\n    _, seq_group_b = create_dummy_prompt(\"2\", 30, block_size=block_size)\n    scheduler.add_seq_group(seq_group_b)\n\n    # Verify prefill requests are prioritized. Since max_batched_num_tokens\n    # is 1, new prefill request has to be scheduled first.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_b]\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_schedule_preempt_abort(use_v2_block_manager: bool):\n    block_size = 4\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(\n        64, 2, max_model_len, use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 2\n    cache_config.num_gpu_blocks = 2\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # Add seq groups to scheduler.\n    seq_a, seq_group_a = create_dummy_prompt(\"1\",\n                                             block_size,\n                                             block_size=block_size)\n    seq_b, seq_group_b = create_dummy_prompt(\"2\",\n                                             block_size,\n                                             block_size=block_size)\n    scheduler.add_seq_group(seq_group_a)\n    scheduler.add_seq_group(seq_group_b)\n\n    # Schedule seq groups prompts.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a, seq_group_b]\n    assert out.num_batched_tokens == block_size * 2  # seq_a and seq_b\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 2\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n\n    # Append \"generated\" tokens, allowing the sequence to mark prompt tokens as\n    # processed.\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation and preempt seq group b.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_a]\n    assert out.num_batched_tokens == 1\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 2\n    assert out.preempted == 1\n\n    # Abort seq group a. Re-schedule seq group b prompt with recomputation.\n    scheduler.abort_seq_group(\"1\")\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert get_sequence_groups(out) == [seq_group_b]\n    assert out.num_batched_tokens == 5  # 4 prompt + 1 generation.\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == 1\n    assert scheduler.get_num_unfinished_seq_groups() == 1\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_max_seqs(use_v2_block_manager: bool):\n    block_size = 4\n    num_seq_group = 4\n    max_seq_group = 2\n    max_model_len = 16\n    scheduler_config = SchedulerConfig(\n        64,\n        max_seq_group,\n        max_model_len,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    all_seq_groups: List[SequenceGroup] = []\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=block_size,\n                                           block_size=block_size)\n        all_seq_groups.append(seq_group)\n\n    # Append 1 seq group\n    scheduler.add_seq_group(all_seq_groups[0])\n\n    # Schedule seq groups prompts.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[0]])\n    append_new_token(out, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[0]])\n    append_new_token(out, 1)\n\n    # Append 2 more seq group\n    scheduler.add_seq_group(all_seq_groups[1])\n    scheduler.add_seq_group(all_seq_groups[2])\n\n    # Schedule seq groups prompts.\n    # Only 1 seq group should be scheduled since max_seq_group is 2\n    # and one is prompting.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set([all_seq_groups[1]])\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_scheduler_delay_factor(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler_config = SchedulerConfig(\n        100,\n        64,\n        16,\n        delay_factor=0.5,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    # schedule first prompt\n    seq_group_meta, seq_group = create_dummy_prompt(\"0\",\n                                                    prompt_length=block_size,\n                                                    block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups > 0\n    assert seq_group_meta[0].request_id == '0'\n    append_new_token(out, 1)\n\n    # wait for a second before scheduling next prompt\n    time.sleep(1)\n    seq_group_meta, seq_group = create_dummy_prompt(\"1\",\n                                                    prompt_length=block_size,\n                                                    block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n\n    # second prompt should *not* be scheduled\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups == 0\n    assert seq_group_meta[0].request_id == '0'\n    append_new_token(out, 1)\n\n    # wait for more than 0.5 second and try again\n    time.sleep(0.6)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_prefill_groups > 0\n    assert seq_group_meta[0].request_id == '1'\n    append_new_token(out, 1)\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_swapped_out_prioritized(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(max_num_seqs=6,\n                                     block_size=block_size,\n                                     use_v2_block_manager=use_v2_block_manager,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    # best_of=2 * 3 == 6 sequences.\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           best_of=2,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 3\n    append_new_token(out, 1)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"2\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 2\n    assert out.num_batched_tokens == 2\n    assert out.blocks_to_swap_out != []\n    assert out.blocks_to_swap_in == []\n    append_new_token(out, 1)\n\n    # Add 1 more task. Swap should be prioritized over prefill.\n    _, seq_group = create_dummy_prompt(str(i),\n                                       prompt_length=60,\n                                       best_of=2,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    append_new_token(out, 1)\n    assert len(out.scheduled_seq_groups) == 3\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 3\n    assert out.blocks_to_swap_in != []\n    assert out.blocks_to_swap_out == []\n\n\ndef initialize_scheduler(\n    *,\n    max_num_seqs=1000,\n    max_token_budget=1000,\n    max_model_len=1000,\n    lora_config=None,\n    use_v2_block_manager=False,\n    block_size=4,\n    num_cpu_blocks=8,\n    num_gpu_blocks=8,\n):\n    block_size = block_size\n    scheduler_config = SchedulerConfig(\n        max_token_budget,\n        max_num_seqs,\n        max_model_len,\n        use_v2_block_manager=use_v2_block_manager)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = num_cpu_blocks\n    cache_config.num_gpu_blocks = num_gpu_blocks\n    scheduler = Scheduler(scheduler_config, cache_config, lora_config)\n    return scheduler\n\n\ndef create_token_budget(token_budget: int = 10000,\n                        max_num_seqs: int = 10000) -> SchedulingBudget:\n    return SchedulingBudget(\n        token_budget=token_budget,\n        max_num_seqs=max_num_seqs,\n    )\n\n\ndef add_token_budget(budget: SchedulingBudget,\n                     num_batched_tokens: int = 0,\n                     num_curr_seqs: int = 0):\n    mock_seq_group = create_dummy_prompt('10', prompt_length=60)[1]\n    budget.add_num_batched_tokens(mock_seq_group.request_id,\n                                  num_batched_tokens)\n    budget.add_num_seqs(mock_seq_group.request_id, num_curr_seqs)\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prefill_schedule_max_prompt_len(use_v2_block_manager: bool):\n    \"\"\"\n    Test prompt longer than max_prompt_len is aborted.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(max_model_len=30,\n                                     use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size)\n    _, seq_group = create_dummy_prompt(\"0\",\n                                       prompt_length=60,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    budget = create_token_budget()\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 1\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n    \"\"\"\n    Test token budget respected.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    budget = create_token_budget(token_budget=0)\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n\n    # 0 token budget == nothing is scheduled.\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 2\n\n    # 60 token budget == 1 request scheduled.\n    budget = create_token_budget(token_budget=60)\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 1\n    assert budget.num_batched_tokens == 60\n    assert budget.num_curr_seqs == 1\n    assert len(remaining_waiting) == 1\n\n    # Test when current_batched_tokens respected.\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=16,\n                                     num_gpu_blocks=16)\n    budget = create_token_budget(token_budget=60)\n    add_token_budget(budget, 30, 0)\n    _, seq_group = create_dummy_prompt(str(i),\n                                       prompt_length=60,\n                                       block_size=block_size)\n    # Cannot schedule a prompt that doesn't fit the budget.\n    scheduler.add_seq_group(seq_group)\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 30\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 1\n    budget = create_token_budget(token_budget=90)\n    add_token_budget(budget, 30, 0)\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.seq_groups) == 1\n    assert budget.num_batched_tokens == 90\n    assert budget.num_curr_seqs == 1\n    assert len(remaining_waiting) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prefill_schedule_max_seqs(use_v2_block_manager: bool):\n    \"\"\"\n    Test max seq respected.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    budget = create_token_budget(max_num_seqs=2)\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 2\n    assert budget.num_batched_tokens == 120\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 1\n\n    # Verify curr_num_seqs respected.\n    scheduler.waiting = deque()\n    budget = create_token_budget(max_num_seqs=2)\n    add_token_budget(budget, 0, 2)\n    _, seq_group = create_dummy_prompt(str(i),\n                                       prompt_length=60,\n                                       block_size=block_size)\n    scheduler.add_seq_group(seq_group)\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 1\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prefill_schedule_max_lora(use_v2_block_manager: bool):\n    \"\"\"\n    Test max lora is respected and prioritized.\n    \"\"\"\n    block_size = 4\n    lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n    scheduler = initialize_scheduler(lora_config=lora_config,\n                                     use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    budget = create_token_budget(token_budget=120)\n    curr_loras: Set[int] = set()\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size,\n                                           lora_request=LoRARequest(\n                                               lora_name=str(i),\n                                               lora_int_id=i + 1,\n                                               lora_path=\"abc\"))\n        scheduler.add_seq_group(seq_group)\n    # Add two more requests to verify lora is prioritized.\n    # 0: Lora, 1: Lora, 2: regular, 3: regular\n    # In the first iteration, index 0, 2 is scheduled.\n    # If a request is not scheduled because it hits max lora, it is\n    # prioritized. Verify that.\n    for i in range(2, 4):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n    # Schedule 2 requests (0 and 2)\n    output = scheduler._schedule_prefills(budget, curr_loras)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 2\n    assert budget.num_batched_tokens == 120\n    assert budget.num_curr_seqs == 2\n    assert len(remaining_waiting) == 2\n    assert len(curr_loras) == 1\n    # The second lora request is scheduled next as FCFS policy.\n    # Reset curr_loras so that it can be scheduled.\n    curr_loras = set()\n    budget = create_token_budget(token_budget=60)\n    output = scheduler._schedule_prefills(budget, curr_loras)\n    remaining_waiting = scheduler.waiting\n    assert len(output.seq_groups) == 1\n    assert output.seq_groups[0].seq_group.request_id == \"1\"\n    assert len(remaining_waiting) == 1\n    assert len(curr_loras) == 1\n    assert budget.num_batched_tokens == 60\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_prefill_schedule_no_block_manager_capacity(use_v2_block_manager):\n    \"\"\"\n    Test sequence cannot be scheduled due to block manager has no capacity.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_gpu_blocks=128,\n                                     num_cpu_blocks=128)\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n    scheduler.block_manager.can_allocate = MagicMock()\n    scheduler.block_manager.can_allocate.return_value = AllocStatus.LATER\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 0\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 3\n\n    scheduler = initialize_scheduler()\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler.add_seq_group(seq_group)\n    scheduler.block_manager.can_allocate = MagicMock()\n    scheduler.block_manager.can_allocate.return_value = AllocStatus.NEVER\n    output = scheduler._schedule_prefills(budget, None)\n    remaining_waiting = scheduler.waiting\n    assert len(output.ignored_seq_groups) == 3\n    assert len(output.seq_groups) == 0\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(remaining_waiting) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_decode_schedule_preempted(use_v2_block_manager: bool):\n    \"\"\"\n    Test decodes cannot be scheduled and preempted.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    curr_loras = None\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size)\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._add_seq_group_to_running(seq_group)\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # 1 cannot be scheduled, and the lowest priority (request 2)\n    # should be preempted. 1 will also be preempted.\n    budget = create_token_budget()\n    output = scheduler._schedule_running(budget, curr_loras)\n    remainig_running = scheduler.running\n    assert len(remainig_running) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert output.decode_seq_groups[0].seq_group.request_id == \"0\"\n    assert len(output.preempted) == 2\n    # Verify budgets are updated.\n    assert budget.num_batched_tokens == 1\n    # NOTE: When enable_chunk is False, num_seqs budget is not updated.\n    # assert budget.num_curr_seqs == 1\n    # Both should be preempted, not swapped.\n    assert output.blocks_to_swap_out == []\n    # Nothing is copied.\n    assert output.blocks_to_copy == []\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_decode_swap_beam_search(use_v2_block_manager: bool):\n    \"\"\"\n    Test best_of > 1 swap out blocks\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_gpu_blocks=64,\n                                     num_cpu_blocks=64)\n    curr_loras = None\n    budget = create_token_budget()\n    for i in range(3):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           best_of=2,\n                                           block_size=block_size)\n        scheduler._allocate_and_set_running(seq_group)\n        scheduler._add_seq_group_to_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        budget.add_num_seqs(seq_group.request_id,\n                            seq_group.get_max_num_running_seqs())\n        budget.add_num_batched_tokens(\n            seq_group.request_id, seq_group.num_seqs(SequenceStatus.RUNNING))\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"2\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n    scheduler.block_manager.swap_out = MagicMock()\n    expected_swap_mapping = [(\"5\", \"7\")]\n    scheduler.block_manager.swap_out.return_value = expected_swap_mapping\n\n    output = scheduler._schedule_running(budget, curr_loras)\n    remainig_running = scheduler.running\n    assert len(remainig_running) == 0\n    assert len(output.decode_seq_groups) == 2\n    assert len(output.prefill_seq_groups) == 0\n    assert output.decode_seq_groups[0].seq_group.request_id == \"0\"\n    assert output.decode_seq_groups[1].seq_group.request_id == \"1\"\n    assert len(output.preempted) == 0\n    assert len(output.swapped_out) == 1\n    # Budget should refledct preempted requests.\n    assert budget.num_batched_tokens == 2\n    # since there are 2 sequences, 2 should be subtracted.\n    assert budget.num_curr_seqs == 4\n    # Both should be preempted, not swapped.\n    assert output.blocks_to_swap_out == expected_swap_mapping\n    # Nothing is copied.\n    assert output.blocks_to_copy == []\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_decode_blocks_to_copy_update(use_v2_block_manager: bool):\n    \"\"\"\n    Verify blocks_to_copy is updated.\n    \"\"\"\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=4,\n                                     num_cpu_blocks=16,\n                                     num_gpu_blocks=16)\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=60,\n                                       best_of=2,\n                                       block_size=block_size)\n    curr_loras = None\n    scheduler._allocate_and_set_running(seq_group)\n    append_new_token_seq_group(60, seq_group, 1)\n    scheduler._add_seq_group_to_running(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.append_slots = MagicMock()\n    scheduler.block_manager.append_slots.return_value = [(2, 3)]\n\n    budget = create_token_budget()\n    output = scheduler._schedule_running(budget, curr_loras)\n    remaining_running = scheduler.running\n    assert len(remaining_running) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert len(output.preempted) == 0\n    assert len(output.swapped_out) == 0\n    # Nothing is preempted.\n    assert output.blocks_to_swap_out == []\n    # Since append_slot returns the source -> dist mapping, it should\n    # applied.\n    assert output.blocks_to_copy == [(2, 3)]\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_simple(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size)\n    curr_loras = None\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=4,\n                                       best_of=2,\n                                       block_size=block_size)\n    scheduler._allocate_and_set_running(seq_group)\n    append_new_token_seq_group(4, seq_group, 1)\n    scheduler._swap_out(seq_group, blocks_to_swap_out)\n    scheduler._add_seq_group_to_swapped(seq_group)\n\n    budget = create_token_budget()\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 0\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    # swap in is the reverse of swap out\n    blocks_to_swap_in_reverse = []\n    for swapin, swapout in output.blocks_to_swap_in:\n        blocks_to_swap_in_reverse.append((swapout, swapin))\n    assert blocks_to_swap_out == blocks_to_swap_in_reverse\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_max_token_budget(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=32,\n                                     num_gpu_blocks=32)\n    curr_loras = None\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60, best_of=2)\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        scheduler._add_seq_group_to_swapped(seq_group)\n\n    budget = create_token_budget(token_budget=1)\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n\n    # Verify num_batched_tokens are respected.\n    budget = create_token_budget(token_budget=1)\n    add_token_budget(budget, 1, 0)\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 0\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_max_seqs(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=64,\n                                     num_gpu_blocks=64)\n    curr_loras = None\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    for i in range(4):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=4)\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        scheduler._add_seq_group_to_swapped(seq_group)\n\n    budget = create_token_budget(max_num_seqs=2)\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 2\n    assert len(output.prefill_seq_groups) == 0\n\n    # Verify num_curr_seqs are respected.\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.num_curr_seqs == 2\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_max_loras(use_v2_block_manager: bool):\n    block_size = 4\n    lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n    scheduler = initialize_scheduler(lora_config=lora_config,\n                                     use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=32,\n                                     num_gpu_blocks=32)\n    curr_loras: Set[int] = set()\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           block_size=block_size,\n                                           lora_request=LoRARequest(\n                                               lora_name=str(i),\n                                               lora_int_id=i + 1,\n                                               lora_path=\"abc\"))\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        scheduler._add_seq_group_to_swapped(seq_group)\n\n    budget = create_token_budget()\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 1\n    assert budget.num_batched_tokens == 1\n    assert budget.num_curr_seqs == 1\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert len(curr_loras) == 1\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_cannot_swap_in(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=32,\n                                     num_gpu_blocks=32)\n    curr_loras = None\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           best_of=2,\n                                           block_size=block_size)\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        scheduler._add_seq_group_to_swapped(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_swap_in = MagicMock()\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.LATER\n    # Since we cannot swap in, none of the requests are swapped in.\n    budget = create_token_budget()\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 2\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_infeasible_swap(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=32,\n                                     num_gpu_blocks=32)\n    curr_loras = None\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i),\n                                           prompt_length=60,\n                                           best_of=2,\n                                           block_size=block_size)\n        scheduler._allocate_and_set_running(seq_group)\n        append_new_token_seq_group(60, seq_group, 1)\n        scheduler._swap_out(seq_group, blocks_to_swap_out)\n        scheduler._add_seq_group_to_swapped(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_swap_in = MagicMock()\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.NEVER\n    # Since we cannot swap in, none of the requests are swapped in.\n    budget = create_token_budget()\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 0\n    assert len(output.infeasible_seq_groups) == 2\n    assert budget.num_batched_tokens == 0\n    assert budget.num_curr_seqs == 0\n    assert len(output.decode_seq_groups) == 0\n    assert len(output.prefill_seq_groups) == 0\n\n\n@pytest.mark.parametrize('use_v2_block_manager', [True, False])\ndef test_schedule_swapped_blocks_to_copy(use_v2_block_manager: bool):\n    block_size = 4\n    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n                                     block_size=block_size,\n                                     num_cpu_blocks=32,\n                                     num_gpu_blocks=32)\n    curr_loras = None\n    _, seq_group = create_dummy_prompt(\"1\",\n                                       prompt_length=60,\n                                       best_of=2,\n                                       block_size=block_size)\n    scheduler._allocate_and_set_running(seq_group)\n    append_new_token_seq_group(60, seq_group, 1)\n    blocks_to_swap_out: List[Tuple[int, int]] = []\n    scheduler._swap_out(seq_group, blocks_to_swap_out)\n    scheduler._add_seq_group_to_swapped(seq_group)\n\n    # The last request should be swapped out.\n    scheduler.block_manager.append_slots = MagicMock()\n    scheduler.block_manager.append_slots.return_value = [(2, 3)]\n\n    budget = create_token_budget()\n    output = scheduler._schedule_swapped(budget, curr_loras)\n    remaining_swapped = scheduler.swapped\n    assert len(remaining_swapped) == 0\n    assert len(output.decode_seq_groups) == 1\n    assert len(output.prefill_seq_groups) == 0\n    assert output.blocks_to_copy == [(2, 3)]\n\n\ndef test_scheduling_budget():\n    TOKEN_BUDGET = 4\n    MAX_SEQS = 4\n    budget = SchedulingBudget(token_budget=TOKEN_BUDGET, max_num_seqs=MAX_SEQS)\n    assert budget.can_schedule(num_new_tokens=1, num_new_seqs=1)\n    assert budget.can_schedule(num_new_tokens=4, num_new_seqs=4)\n    assert not budget.can_schedule(num_new_tokens=1, num_new_seqs=5)\n    assert not budget.can_schedule(num_new_tokens=5, num_new_seqs=1)\n    assert not budget.can_schedule(num_new_tokens=5, num_new_seqs=5)\n    assert budget.remaining_token_budget() == TOKEN_BUDGET\n\n    # Verify add/subtract num batched tokens.\n    _, seq_group = create_dummy_prompt(\"1\", 3)\n    budget.add_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 2\n    assert budget.num_batched_tokens == 2\n    assert budget.can_schedule(num_new_tokens=2, num_new_seqs=1)\n    assert not budget.can_schedule(num_new_tokens=3, num_new_seqs=1)\n    # Verify adding another seq group is no-op.\n    budget.add_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 2\n    assert budget.num_batched_tokens == 2\n    budget.subtract_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 4\n    assert budget.num_batched_tokens == 0\n    budget.subtract_num_batched_tokens(seq_group.request_id, 2)\n    assert budget.remaining_token_budget() == 4\n    assert budget.num_batched_tokens == 0\n\n    # Verify add/subtract max seqs.\n    _, seq_group = create_dummy_prompt(\"1\", 3)\n    budget.add_num_seqs(seq_group.request_id, 2)\n    assert budget.can_schedule(num_new_tokens=1, num_new_seqs=2)\n    assert not budget.can_schedule(num_new_tokens=1, num_new_seqs=3)\n    assert budget.num_curr_seqs == 2\n    # Verify adding another seq group is no-op.\n    budget.add_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 2\n    budget.subtract_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 0\n    budget.subtract_num_seqs(seq_group.request_id, 2)\n    assert budget.num_curr_seqs == 0\n",
      "diff": "diff --git a/tests/core/test_scheduler.py b/tests/core/test_scheduler.py\nindex 5cdf743a4..defa6c1bd 100644\n--- a/tests/core/test_scheduler.py\n+++ b/tests/core/test_scheduler.py\n@@ -3,7 +3,7 @@ from collections import deque\n from typing import List, Set, Tuple\n from unittest.mock import MagicMock\n \n-import pytest\n+import pytest  # noqa\n from torch import Use  # noqa\n \n from vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\n@@ -12,23 +12,18 @@ from vllm.core.scheduler import Scheduler, SchedulingBudget\n from vllm.lora.request import LoRARequest\n from vllm.sequence import SequenceGroup, SequenceStatus\n \n-from ..utils import check_deprecated_block_manager_usage\n from .utils import (append_new_token, append_new_token_seq_group,\n                     create_dummy_prompt, get_sequence_groups,\n                     schedule_and_update_computed_tokens)\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        \"tests/core/test_chunked_prefill_scheduler.py\")\n-\n-\n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_add_seq_group(use_v2_block_manager: bool):\n+def test_scheduler_add_seq_group():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n-        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n+        100,\n+        64,\n+        1,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, cache_dtype=\"auto\")\n     cache_config.num_cpu_blocks = 4\n     cache_config.num_gpu_blocks = 4\n@@ -44,11 +39,13 @@ def test_scheduler_add_seq_group(use_v2_block_manager: bool):\n         assert scheduler.get_num_unfinished_seq_groups() == i + 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_abort_seq_group(use_v2_block_manager: bool):\n+def test_scheduler_abort_seq_group():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n-        100, 64, 1, use_v2_block_manager=use_v2_block_manager)\n+        100,\n+        64,\n+        1,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 4\n     cache_config.num_gpu_blocks = 4\n@@ -68,8 +65,7 @@ def test_scheduler_abort_seq_group(use_v2_block_manager: bool):\n     assert scheduler.get_num_unfinished_seq_groups() == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n+def test_scheduler_schedule_simple():\n     block_size = 4\n     num_seq_group = 4\n     max_model_len = 16\n@@ -77,7 +73,7 @@ def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n         64,\n         num_seq_group,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -112,8 +108,7 @@ def test_scheduler_schedule_simple(use_v2_block_manager: bool):\n     append_new_token(out, 1)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n+def test_scheduler_prefill_prioritized():\n     \"\"\"Verify running batched tokens are not applied to prefill requests.\"\"\"\n     block_size = 4\n     max_model_len = 30\n@@ -122,7 +117,7 @@ def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n         max_batched_num_tokens,\n         2,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 16\n     cache_config.num_gpu_blocks = 16\n@@ -146,12 +141,14 @@ def test_scheduler_prefill_prioritized(use_v2_block_manager: bool):\n     assert get_sequence_groups(out) == [seq_group_b]\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_schedule_preempt_abort(use_v2_block_manager: bool):\n+def test_scheduler_schedule_preempt_abort():\n     block_size = 4\n     max_model_len = 16\n     scheduler_config = SchedulerConfig(\n-        64, 2, max_model_len, use_v2_block_manager=use_v2_block_manager)\n+        64,\n+        2,\n+        max_model_len,\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 2\n     cache_config.num_gpu_blocks = 2\n@@ -201,8 +198,7 @@ def test_scheduler_schedule_preempt_abort(use_v2_block_manager: bool):\n     assert scheduler.get_num_unfinished_seq_groups() == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_max_seqs(use_v2_block_manager: bool):\n+def test_scheduler_max_seqs():\n     block_size = 4\n     num_seq_group = 4\n     max_seq_group = 2\n@@ -211,7 +207,7 @@ def test_scheduler_max_seqs(use_v2_block_manager: bool):\n         64,\n         max_seq_group,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -249,15 +245,14 @@ def test_scheduler_max_seqs(use_v2_block_manager: bool):\n     assert set(get_sequence_groups(out)) == set([all_seq_groups[1]])\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_scheduler_delay_factor(use_v2_block_manager: bool):\n+def test_scheduler_delay_factor():\n     block_size = 4\n     scheduler_config = SchedulerConfig(\n         100,\n         64,\n         16,\n         delay_factor=0.5,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = 8\n     cache_config.num_gpu_blocks = 8\n@@ -294,12 +289,10 @@ def test_scheduler_delay_factor(use_v2_block_manager: bool):\n     append_new_token(out, 1)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_swapped_out_prioritized(use_v2_block_manager: bool):\n+def test_swapped_out_prioritized():\n     block_size = 4\n     scheduler = initialize_scheduler(max_num_seqs=6,\n                                      block_size=block_size,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     # best_of=2 * 3 == 6 sequences.\n@@ -351,7 +344,6 @@ def initialize_scheduler(\n     max_token_budget=1000,\n     max_model_len=1000,\n     lora_config=None,\n-    use_v2_block_manager=False,\n     block_size=4,\n     num_cpu_blocks=8,\n     num_gpu_blocks=8,\n@@ -361,7 +353,7 @@ def initialize_scheduler(\n         max_token_budget,\n         max_num_seqs,\n         max_model_len,\n-        use_v2_block_manager=use_v2_block_manager)\n+    )\n     cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n     cache_config.num_cpu_blocks = num_cpu_blocks\n     cache_config.num_gpu_blocks = num_gpu_blocks\n@@ -386,15 +378,12 @@ def add_token_budget(budget: SchedulingBudget,\n     budget.add_num_seqs(mock_seq_group.request_id, num_curr_seqs)\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_prompt_len(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_prompt_len():\n     \"\"\"\n     Test prompt longer than max_prompt_len is aborted.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(max_model_len=30,\n-                                     use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size)\n+    scheduler = initialize_scheduler(max_model_len=30, block_size=block_size)\n     _, seq_group = create_dummy_prompt(\"0\",\n                                        prompt_length=60,\n                                        block_size=block_size)\n@@ -409,14 +398,12 @@ def test_prefill_schedule_max_prompt_len(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n+def test_prefill_schedule_token_budget():\n     \"\"\"\n     Test token budget respected.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     budget = create_token_budget(token_budget=0)\n@@ -446,8 +433,7 @@ def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 1\n \n     # Test when current_batched_tokens respected.\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=16,\n                                      num_gpu_blocks=16)\n     budget = create_token_budget(token_budget=60)\n@@ -474,14 +460,12 @@ def test_prefill_schedule_token_budget(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_seqs(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_seqs():\n     \"\"\"\n     Test max seq respected.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     budget = create_token_budget(max_num_seqs=2)\n@@ -515,15 +499,13 @@ def test_prefill_schedule_max_seqs(use_v2_block_manager: bool):\n     assert len(remaining_waiting) == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_max_lora(use_v2_block_manager: bool):\n+def test_prefill_schedule_max_lora():\n     \"\"\"\n     Test max lora is respected and prioritized.\n     \"\"\"\n     block_size = 4\n     lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n     scheduler = initialize_scheduler(lora_config=lora_config,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n@@ -570,14 +552,12 @@ def test_prefill_schedule_max_lora(use_v2_block_manager: bool):\n     assert budget.num_batched_tokens == 60\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_prefill_schedule_no_block_manager_capacity(use_v2_block_manager):\n+def test_prefill_schedule_no_block_manager_capacity():\n     \"\"\"\n     Test sequence cannot be scheduled due to block manager has no capacity.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_gpu_blocks=128,\n                                      num_cpu_blocks=128)\n     budget = create_token_budget()\n@@ -614,14 +594,12 @@ def test_prefill_schedule_no_block_manager_capacity(use_v2_block_manager):\n     assert len(remaining_waiting) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_decode_schedule_preempted(use_v2_block_manager: bool):\n+def test_decode_schedule_preempted():\n     \"\"\"\n     Test decodes cannot be scheduled and preempted.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     curr_loras = None\n@@ -660,14 +638,12 @@ def test_decode_schedule_preempted(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_decode_swap_beam_search(use_v2_block_manager: bool):\n+def test_decode_swap_beam_search():\n     \"\"\"\n     Test best_of > 1 swap out blocks\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_gpu_blocks=64,\n                                      num_cpu_blocks=64)\n     curr_loras = None\n@@ -716,14 +692,12 @@ def test_decode_swap_beam_search(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == []\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_decode_blocks_to_copy_update(use_v2_block_manager: bool):\n+def test_schedule_decode_blocks_to_copy_update():\n     \"\"\"\n     Verify blocks_to_copy is updated.\n     \"\"\"\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=4,\n+    scheduler = initialize_scheduler(block_size=4,\n                                      num_cpu_blocks=16,\n                                      num_gpu_blocks=16)\n     _, seq_group = create_dummy_prompt(\"1\",\n@@ -754,11 +728,9 @@ def test_schedule_decode_blocks_to_copy_update(use_v2_block_manager: bool):\n     assert output.blocks_to_copy == [(2, 3)]\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_simple(use_v2_block_manager: bool):\n+def test_schedule_swapped_simple():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size)\n+    scheduler = initialize_scheduler(block_size=block_size)\n     curr_loras = None\n     blocks_to_swap_out: List[Tuple[int, int]] = []\n     _, seq_group = create_dummy_prompt(\"1\",\n@@ -785,11 +757,9 @@ def test_schedule_swapped_simple(use_v2_block_manager: bool):\n     assert blocks_to_swap_out == blocks_to_swap_in_reverse\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_token_budget(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_token_budget():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -822,11 +792,9 @@ def test_schedule_swapped_max_token_budget(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_seqs(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_seqs():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=64,\n                                      num_gpu_blocks=64)\n     curr_loras = None\n@@ -859,12 +827,10 @@ def test_schedule_swapped_max_seqs(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_max_loras(use_v2_block_manager: bool):\n+def test_schedule_swapped_max_loras():\n     block_size = 4\n     lora_config = LoRAConfig(max_lora_rank=8, max_loras=1)\n     scheduler = initialize_scheduler(lora_config=lora_config,\n-                                     use_v2_block_manager=use_v2_block_manager,\n                                      block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n@@ -894,11 +860,9 @@ def test_schedule_swapped_max_loras(use_v2_block_manager: bool):\n     assert len(curr_loras) == 1\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_cannot_swap_in(use_v2_block_manager: bool):\n+def test_schedule_swapped_cannot_swap_in():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -927,11 +891,9 @@ def test_schedule_swapped_cannot_swap_in(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_infeasible_swap(use_v2_block_manager: bool):\n+def test_infeasible_swap():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None\n@@ -961,11 +923,9 @@ def test_infeasible_swap(use_v2_block_manager: bool):\n     assert len(output.prefill_seq_groups) == 0\n \n \n-@pytest.mark.parametrize('use_v2_block_manager', [True, False])\n-def test_schedule_swapped_blocks_to_copy(use_v2_block_manager: bool):\n+def test_schedule_swapped_blocks_to_copy():\n     block_size = 4\n-    scheduler = initialize_scheduler(use_v2_block_manager=use_v2_block_manager,\n-                                     block_size=block_size,\n+    scheduler = initialize_scheduler(block_size=block_size,\n                                      num_cpu_blocks=32,\n                                      num_gpu_blocks=32)\n     curr_loras = None",
      "change_type": "modified",
      "lines_added": 56,
      "lines_removed": 96
    },
    {
      "file_path": "tests/metrics/test_metrics.py",
      "old_content": "import time\nfrom typing import List\n\nimport pytest\nimport ray\nfrom prometheus_client import REGISTRY\n\nfrom vllm import EngineArgs, LLMEngine\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\nfrom vllm.engine.metrics import RayPrometheusStatLogger\nfrom vllm.sampling_params import SamplingParams\n\nfrom ..conftest import cleanup\n\nMODELS = [\n    \"facebook/opt-125m\",\n]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"float\"])\n@pytest.mark.parametrize(\"max_tokens\", [128])\ndef test_metric_counter_prompt_tokens(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n) -> None:\n    with vllm_runner(model,\n                     dtype=dtype,\n                     disable_log_stats=False,\n                     gpu_memory_utilization=0.4) as vllm_model:\n        tokenizer = vllm_model.model.get_tokenizer()\n        prompt_token_counts = [\n            len(tokenizer.encode(p)) for p in example_prompts\n        ]\n        # This test needs at least 2 prompts in a batch of different lengths to\n        # verify their token count is correct despite padding.\n        assert len(example_prompts) > 1, \"at least 2 prompts are required\"\n        assert prompt_token_counts[0] != prompt_token_counts[1], (\n            \"prompts of different lengths are required\")\n        vllm_prompt_token_count = sum(prompt_token_counts)\n\n        _ = vllm_model.generate_greedy(example_prompts, max_tokens)\n        stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n        metric_count = stat_logger.metrics.counter_prompt_tokens.labels(\n            **stat_logger.labels)._value.get()\n\n    assert vllm_prompt_token_count == metric_count, (\n        f\"prompt token count: {vllm_prompt_token_count!r}\\n\"\n        f\"metric: {metric_count!r}\")\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"float\"])\n@pytest.mark.parametrize(\"max_tokens\", [128])\ndef test_metric_counter_generation_tokens(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n) -> None:\n    with vllm_runner(model,\n                     dtype=dtype,\n                     disable_log_stats=False,\n                     gpu_memory_utilization=0.4) as vllm_model:\n        vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n        tokenizer = vllm_model.model.get_tokenizer()\n        stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n        metric_count = stat_logger.metrics.counter_generation_tokens.labels(\n            **stat_logger.labels)._value.get()\n        vllm_generation_count = 0\n        for i in range(len(example_prompts)):\n            vllm_output_ids, vllm_output_str = vllm_outputs[i]\n            prompt_ids = tokenizer.encode(example_prompts[i])\n            # vllm_output_ids contains both prompt tokens and generation tokens.\n            # We're interested only in the count of the generation tokens.\n            vllm_generation_count += len(vllm_output_ids) - len(prompt_ids)\n\n    assert vllm_generation_count == metric_count, (\n        f\"generation token count: {vllm_generation_count!r}\\n\"\n        f\"metric: {metric_count!r}\")\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"float\"])\n@pytest.mark.parametrize(\n    \"served_model_name\",\n    [None, [], [\"ModelName0\"], [\"ModelName0\", \"ModelName1\", \"ModelName2\"]])\ndef test_metric_set_tag_model_name(vllm_runner, model: str, dtype: str,\n                                   served_model_name: List[str]) -> None:\n    with vllm_runner(model,\n                     dtype=dtype,\n                     disable_log_stats=False,\n                     gpu_memory_utilization=0.3,\n                     served_model_name=served_model_name) as vllm_model:\n        stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n        metrics_tag_content = stat_logger.labels[\"model_name\"]\n\n    if served_model_name is None or served_model_name == []:\n        assert metrics_tag_content == model, (\n            f\"Metrics tag model_name is wrong! expect: {model!r}\\n\"\n            f\"actual: {metrics_tag_content!r}\")\n    else:\n        assert metrics_tag_content == served_model_name[0], (\n            f\"Metrics tag model_name is wrong! expect: \"\n            f\"{served_model_name[0]!r}\\n\"\n            f\"actual: {metrics_tag_content!r}\")\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [4])\n@pytest.mark.parametrize(\"disable_log_stats\", [True, False])\n@pytest.mark.asyncio\nasync def test_async_engine_log_metrics_regression(\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n    disable_log_stats: bool,\n) -> None:\n    \"\"\"\n    Regression test ensuring async engine generates metrics\n    when disable_log_stats=False\n    (see: https://github.com/vllm-project/vllm/pull/4150#pullrequestreview-2008176678)\n    \"\"\"\n    engine_args = AsyncEngineArgs(model=model,\n                                  dtype=dtype,\n                                  disable_log_stats=disable_log_stats)\n    async_engine = AsyncLLMEngine.from_engine_args(engine_args)\n    for i, prompt in enumerate(example_prompts):\n        results = async_engine.generate(\n            prompt,\n            SamplingParams(max_tokens=max_tokens),\n            f\"request-id-{i}\",\n        )\n        # Exhaust the async iterator to make the async engine work\n        async for _ in results:\n            pass\n\n    assert_metrics(async_engine.engine, disable_log_stats,\n                   len(example_prompts))\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [4])\n@pytest.mark.parametrize(\"disable_log_stats\", [True, False])\ndef test_engine_log_metrics_regression(\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n    disable_log_stats: bool,\n) -> None:\n    engine_args = EngineArgs(model=model,\n                             dtype=dtype,\n                             disable_log_stats=disable_log_stats)\n    engine = LLMEngine.from_engine_args(engine_args)\n    for i, prompt in enumerate(example_prompts):\n        engine.add_request(\n            f\"request-id-{i}\",\n            prompt,\n            SamplingParams(max_tokens=max_tokens),\n        )\n    while engine.has_unfinished_requests():\n        engine.step()\n\n    assert_metrics(engine, disable_log_stats, len(example_prompts))\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [10])\ndef test_metric_spec_decode(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n) -> None:\n    k = 5\n\n    with vllm_runner(model,\n                     dtype=dtype,\n                     disable_log_stats=False,\n                     gpu_memory_utilization=0.4,\n                     speculative_model=model,\n                     num_speculative_tokens=k,\n                     use_v2_block_manager=True) as vllm_model:\n\n        # Force log interval to be 0 to catch all metrics.\n        stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n        stat_logger.local_interval = 0\n\n        # Note that the purpose of this test is to verify spec decode\n        # metrics instead of functional correctness, so the expected values\n        # are intended to be loose.\n        metric_name_to_expected_fn = {\n            \"gauge_spec_decode_draft_acceptance_rate\": lambda v: 0 <= v <= 1,\n            \"gauge_spec_decode_efficiency\": lambda v: 0 <= v <= 1,\n            \"counter_spec_decode_num_accepted_tokens\": lambda v: 0 <= v <= k,\n            \"counter_spec_decode_num_draft_tokens\": lambda v: v == k,\n            \"counter_spec_decode_num_emitted_tokens\":\n            lambda v: 0 <= v <= k + 1,\n        }\n\n        # Use one request to better inspect the metrics.\n        prompts = example_prompts[:1]\n\n        _ = vllm_model.generate_greedy(prompts, max_tokens)\n        for metric_name, is_expected in metric_name_to_expected_fn.items():\n            metric_val = getattr(\n                stat_logger.metrics,\n                metric_name).labels(**stat_logger.labels)._value.get()\n            assert is_expected(metric_val), (\n                f\"the value of metric {metric_name} ({metric_val}) \"\n                \"does not meet expectation\")\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [10])\n@pytest.mark.parametrize(\"log_interval\", [1, 3, 5, 7])\ndef test_metric_spec_decode_interval(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n    log_interval: int,\n) -> None:\n    k = 5\n\n    engine_args = EngineArgs(model=model,\n                             dtype=dtype,\n                             disable_log_stats=False,\n                             gpu_memory_utilization=0.4,\n                             speculative_model=model,\n                             num_speculative_tokens=k,\n                             use_v2_block_manager=True,\n                             enforce_eager=True)\n\n    engine = LLMEngine.from_engine_args(engine_args)\n\n    try:\n\n        engine.add_request(\n            \"request-id-0\",\n            example_prompts[0],\n            SamplingParams(max_tokens=max_tokens),\n        )\n\n        # set log internal\n        stat_logger = engine.stat_loggers['prometheus']\n        stat_logger.local_interval = log_interval\n\n        # prefill\n        engine.step()\n\n        # wait for 5 seconds to ensure that spec decode metrics\n        # get triggered in first decode step\n        time.sleep(5)\n\n        # first decode step should trigger async collection of metrics\n        engine.step()\n\n        # wait one second to allow H2D transfer to finish\n        time.sleep(1)\n\n        # second decode step should now be able to collect the spec\n        # decode stats and the request should also be finished\n        engine.step()\n\n        # must have finisehd now\n        assert not engine.has_unfinished_requests()\n\n        # wait to ensure logging occurs\n        time.sleep(log_interval)\n\n        # force logging\n        engine.step()\n\n        # Note that the purpose of this test is to verify spec decode\n        # metrics instead of functional correctness, so the expected values\n        # are intended to be loose.\n        metric_name_to_expected_fn = {\n            \"gauge_spec_decode_draft_acceptance_rate\": lambda v: 0 <= v <= 1,\n            \"gauge_spec_decode_efficiency\": lambda v: 0 <= v <= 1,\n            \"counter_spec_decode_num_accepted_tokens\": lambda v: 0 <= v <= k,\n            \"counter_spec_decode_num_draft_tokens\": lambda v: v == k,\n            \"counter_spec_decode_num_emitted_tokens\":\n            lambda v: 0 <= v <= k + 1,\n        }\n\n        for metric_name, is_expected in metric_name_to_expected_fn.items():\n            metric_val = getattr(\n                stat_logger.metrics,\n                metric_name).labels(**stat_logger.labels)._value.get()\n            assert is_expected(metric_val), (\n                f\"the value of metric {metric_name} ({metric_val}) \"\n                \"does not meet expectation\")\n\n    finally:\n        del engine\n        cleanup()\n\n\ndef assert_metrics(engine: LLMEngine, disable_log_stats: bool,\n                   num_requests: int) -> None:\n    if disable_log_stats:\n        with pytest.raises(AttributeError):\n            _ = engine.stat_loggers\n    else:\n        assert (engine.stat_loggers\n                is not None), \"engine.stat_loggers should be set\"\n        # Ensure the count bucket of request-level histogram metrics matches\n        # the number of requests as a simple sanity check to ensure metrics are\n        # generated\n        labels = {'model_name': engine.model_config.model}\n        request_histogram_metrics = [\n            \"vllm:e2e_request_latency_seconds\",\n            \"vllm:request_prompt_tokens\",\n            \"vllm:request_generation_tokens\",\n            \"vllm:request_params_n\",\n        ]\n        for metric_name in request_histogram_metrics:\n            metric_value = REGISTRY.get_sample_value(f\"{metric_name}_count\",\n                                                     labels)\n            assert (\n                metric_value == num_requests), \"Metrics should be collected\"\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [16])\ndef test_engine_log_metrics_ray(\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n) -> None:\n    # This test is quite weak - it only checks that we can use\n    # RayPrometheusStatLogger without exceptions.\n    # Checking whether the metrics are actually emitted is unfortunately\n    # non-trivial.\n\n    # We have to run in a Ray task for Ray metrics to be emitted correctly\n    @ray.remote(num_gpus=1)\n    def _inner():\n\n        class _RayPrometheusStatLogger(RayPrometheusStatLogger):\n\n            def __init__(self, *args, **kwargs):\n                self._i = 0\n                super().__init__(*args, **kwargs)\n\n            def log(self, *args, **kwargs):\n                self._i += 1\n                return super().log(*args, **kwargs)\n\n        engine_args = EngineArgs(\n            model=model,\n            dtype=dtype,\n            disable_log_stats=False,\n        )\n        engine = LLMEngine.from_engine_args(engine_args)\n        logger = _RayPrometheusStatLogger(\n            local_interval=0.5,\n            labels=dict(model_name=engine.model_config.served_model_name),\n            max_model_len=engine.model_config.max_model_len)\n        engine.add_logger(\"ray\", logger)\n        for i, prompt in enumerate(example_prompts):\n            engine.add_request(\n                f\"request-id-{i}\",\n                prompt,\n                SamplingParams(max_tokens=max_tokens),\n            )\n        while engine.has_unfinished_requests():\n            engine.step()\n        assert logger._i > 0, \".log must be called at least once\"\n\n    ray.get(_inner.remote())\n",
      "diff": "diff --git a/tests/metrics/test_metrics.py b/tests/metrics/test_metrics.py\nindex f1003221a..8798ff078 100644\n--- a/tests/metrics/test_metrics.py\n+++ b/tests/metrics/test_metrics.py\n@@ -185,13 +185,14 @@ def test_metric_spec_decode(\n ) -> None:\n     k = 5\n \n-    with vllm_runner(model,\n-                     dtype=dtype,\n-                     disable_log_stats=False,\n-                     gpu_memory_utilization=0.4,\n-                     speculative_model=model,\n-                     num_speculative_tokens=k,\n-                     use_v2_block_manager=True) as vllm_model:\n+    with vllm_runner(\n+            model,\n+            dtype=dtype,\n+            disable_log_stats=False,\n+            gpu_memory_utilization=0.4,\n+            speculative_model=model,\n+            num_speculative_tokens=k,\n+    ) as vllm_model:\n \n         # Force log interval to be 0 to catch all metrics.\n         stat_logger = vllm_model.model.llm_engine.stat_loggers['prometheus']\n@@ -242,7 +243,6 @@ def test_metric_spec_decode_interval(\n                              gpu_memory_utilization=0.4,\n                              speculative_model=model,\n                              num_speculative_tokens=k,\n-                             use_v2_block_manager=True,\n                              enforce_eager=True)\n \n     engine = LLMEngine.from_engine_args(engine_args)",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 9
    },
    {
      "file_path": "tests/multi_step/test_correctness_async_llm.py",
      "old_content": "# Test the AsyncLLMEngine with multi-step-decoding\nfrom typing import List, Optional\n\nimport pytest\n\nfrom tests.kernels.utils import override_backend_env_variable\n\nfrom ..models.utils import check_logprobs_close\nfrom ..utils import (completions_with_server_args, get_client_text_generations,\n                     get_client_text_logprob_generations)\n\nMODELS = [\n    \"JackFram/llama-160m\",\n]\nNUM_SCHEDULER_STEPS = [8]  # Multi-step decoding steps\nNUM_PROMPTS = [10]\n\nDEFAULT_SERVER_ARGS: List[str] = [\n    \"--disable-log-requests\",\n    \"--use-v2-block-manager\",\n    \"--worker-use-ray\",\n    \"--gpu-memory-utilization\",\n    \"0.85\",\n    \"--swap-space\",\n    \"16\",\n]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize((\"tp_size, pp_size\"), [\n    (1, 1),\n    (2, 2),\n])\n@pytest.mark.parametrize(\"eager_mode\", [False, True])\n@pytest.mark.parametrize(\"num_scheduler_steps\", NUM_SCHEDULER_STEPS)\n@pytest.mark.parametrize(\"num_prompts\", NUM_PROMPTS)\n@pytest.mark.parametrize(\"num_logprobs\", [5])\n@pytest.mark.parametrize(\"is_async\", [True])\n@pytest.mark.parametrize(\"attention_backend\", [\"FLASHINFER\", \"FLASH_ATTN\"])\n@pytest.mark.parametrize(\"enable_chunked_prefill\", [True, False])\n@pytest.mark.asyncio\nasync def test_multi_step(\n    example_prompts,\n    model: str,\n    tp_size: int,\n    pp_size: int,\n    eager_mode: int,\n    num_scheduler_steps: int,\n    num_prompts: int,\n    is_async: bool,\n    num_logprobs: Optional[int],\n    attention_backend: str,\n    enable_chunked_prefill: bool,\n    monkeypatch,\n) -> None:\n    \"\"\"Test vLLM engine with multi-step scheduling in an OpenAI-protocol\n    client/server environment.\n\n    Set up an engine with single-step scheduling as a ground-truth reference.\n\n    Send a completions API request to both engines with the same prompts.\n\n    Validate:\n    * Generated tokens match\n    * Generated logprobs are all very close\n\n    Args:\n      example_prompts: test fixture providing example prompts\n      model: model under test (same for single- and multi-step engines)\n      tp_size: degree of tensor-parallelism\n      pp_size: degree of pipeline-parallelism\n      eager_mode\n      num_scheduler_steps: for multi-step scheduling, GPU-side steps per\n                           GPU -> CPU output transfer\n      num_prompts: number of example prompts under test\n      num_logprobs: corresponds to the `logprobs` argument to the OpenAI\n                    completions endpoint; `None` -> no logprobs\n    \"\"\"\n    if enable_chunked_prefill and \\\n        (pp_size > 1 or attention_backend != \"FLASH_ATTN\"):\n        pytest.skip(\"Multi-step with Chunked-Prefill only supports\"\n                    \"PP=1 and FLASH_ATTN backend\")\n\n    override_backend_env_variable(monkeypatch, attention_backend)\n\n    prompts = example_prompts\n    if len(prompts) < num_prompts:\n        prompts = prompts * ((num_prompts // len(prompts)) + 1)\n    prompts = prompts[:num_prompts]\n    assert len(prompts) == num_prompts\n\n    server_args = DEFAULT_SERVER_ARGS + [\"--enforce-eager\"]\n    ms_server_args = DEFAULT_SERVER_ARGS + \\\n        [\"--num-scheduler-steps\", f\"{num_scheduler_steps}\"]\n\n    if not is_async:\n        ms_server_args += [\"--disable-async-output-proc\"]\n\n    if eager_mode:\n        ms_server_args.append(\"--enforce-eager\")\n\n    if enable_chunked_prefill:\n        ms_server_args.append(\"--enable-chunked-prefill\")\n\n    distributed_args = [\n        \"--tensor-parallel-size\",\n        str(tp_size),\n        \"--pipeline-parallel-size\",\n        str(pp_size),\n    ]\n\n    # Spin up client/server & issue completion API requests.\n    # Default `max_wait_seconds` is 240 but was empirically\n    # was raised 3x to 720 *just for this test* due to\n    # observed timeouts in GHA CI\n    ref_completions = await completions_with_server_args(\n        prompts,\n        model,\n        server_args + distributed_args,\n        num_logprobs,\n        max_wait_seconds=5 * 240)\n    test_completions = await completions_with_server_args(\n        prompts,\n        model,\n        ms_server_args + distributed_args,\n        num_logprobs,\n        max_wait_seconds=5 * 240)\n\n    # Assert multi-step scheduling produces identical tokens\n    # to single-step scheduling.\n    ref_generations = get_client_text_generations(ref_completions)\n    test_generations = get_client_text_generations(test_completions)\n    assert ref_generations == test_generations\n\n    # Assert multi-step scheduling produces nearly-identical logprobs\n    # to single-step scheduling.\n    ref_text_logprobs = get_client_text_logprob_generations(ref_completions)\n    test_text_logprobs = get_client_text_logprob_generations(test_completions)\n    check_logprobs_close(\n        outputs_0_lst=ref_text_logprobs,\n        outputs_1_lst=test_text_logprobs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n\n\n@pytest.mark.parametrize((\"tp_size, pp_size\"), [\n    (1, 2),\n])\n@pytest.mark.asyncio\nasync def test_multi_step_pp_smoke(\n    tp_size: int,\n    pp_size: int,\n    monkeypatch,\n) -> None:\n    \"\"\"\n    Smoke test for the vLLM engine with multi-step scheduling in an\n    OpenAI-protocol client/server environment.\n\n    This tests compares the outputs between multi-step scheduling and\n    single-step scheduling. Notably, this test lets the engines generate\n    more tokens (default is 5) and test for an exact match over all the\n    tokens.\n\n    Args:\n      tp_size: degree of tensor-parallelism\n      pp_size: degree of pipeline-parallelism\n      eager_mode\n    \"\"\"\n\n    model = \"JackFram/llama-160m\"\n    num_scheduler_steps = 8\n    attention_backend = \"FLASH_ATTN\"\n    max_num_seqs = 3\n\n    override_backend_env_variable(monkeypatch, attention_backend)\n\n    # Prompt from the ShareGPT dataset\n    prompts = [\n        \"in the jtbd context whats a push?\",  # codespell:ignore\n        \"in the jtbd context whats a push?\",  # codespell:ignore\n        \"in the jtbd context whats a push?\",  # codespell:ignore\n        \"in the jtbd context whats a push?\",  # codespell:ignore\n    ]\n    # Use varying max_tokens to introduce scheduling randomness.\n    max_tokens = [10 * i for i in range(1, len(prompts) + 1)]\n    assert len(prompts) == len(max_tokens)\n\n    test_args = [\n        \"--tensor-parallel-size\",\n        str(tp_size), \"--pipeline-parallel-size\",\n        str(pp_size), \"--max-num-seqs\",\n        str(max_num_seqs)\n    ]\n\n    server_args = DEFAULT_SERVER_ARGS + test_args\n    ms_server_args = DEFAULT_SERVER_ARGS + \\\n       [\"--num-scheduler-steps\", f\"{num_scheduler_steps}\"] + \\\n       test_args\n\n    # Spin up client/server & issue completion API requests.\n    # Default `max_wait_seconds` is 240 but was empirically\n    # was raised 3x to 720 *just for this test* due to\n    # observed timeouts in GHA CI\n    ref_completions = await completions_with_server_args(\n        prompts=prompts,\n        model_name=model,\n        server_cli_args=server_args,\n        num_logprobs=None,\n        max_wait_seconds=5 * 240,\n        max_tokens=max_tokens)\n\n    test_completions = await completions_with_server_args(\n        prompts=prompts,\n        model_name=model,\n        server_cli_args=ms_server_args,\n        num_logprobs=None,\n        max_wait_seconds=5 * 240,\n        max_tokens=max_tokens)\n\n    # Assert multi-step scheduling produces identical tokens\n    # to single-step scheduling.\n    ref_generations = get_client_text_generations(ref_completions)\n    test_generations = get_client_text_generations(test_completions)\n\n    assert ref_generations == test_generations\n",
      "diff": "diff --git a/tests/multi_step/test_correctness_async_llm.py b/tests/multi_step/test_correctness_async_llm.py\nindex 000c923ef..7203d635c 100644\n--- a/tests/multi_step/test_correctness_async_llm.py\n+++ b/tests/multi_step/test_correctness_async_llm.py\n@@ -17,7 +17,6 @@ NUM_PROMPTS = [10]\n \n DEFAULT_SERVER_ARGS: List[str] = [\n     \"--disable-log-requests\",\n-    \"--use-v2-block-manager\",\n     \"--worker-use-ray\",\n     \"--gpu-memory-utilization\",\n     \"0.85\",",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 2
    },
    {
      "file_path": "tests/multi_step/test_correctness_llm.py",
      "old_content": "# Test the LLMEngine with multi-step-decoding\n\nimport copy\nfrom typing import Optional\n\nimport pytest\n\nfrom ..models.utils import check_logprobs_close, check_outputs_equal\n\nMODELS = [\n    \"JackFram/llama-160m\",\n]\nNUM_SCHEDULER_STEPS = [8]  # Multi-step decoding steps\nNUM_PROMPTS = [10]\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"tp_size\", [1])\n@pytest.mark.parametrize(\"enable_chunked_prefill\", [False, True])\n@pytest.mark.parametrize(\"max_tokens\", [5])\n@pytest.mark.parametrize(\"enforce_eager\", [True])\n@pytest.mark.parametrize(\"num_scheduler_steps\", NUM_SCHEDULER_STEPS)\n@pytest.mark.parametrize(\"num_prompts\", NUM_PROMPTS)\n@pytest.mark.parametrize(\"num_logprobs\", [None, 5])\ndef test_multi_step_llm(\n    hf_runner,\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    tp_size: int,\n    enable_chunked_prefill: bool,\n    max_tokens: int,\n    enforce_eager: int,\n    num_scheduler_steps: int,\n    num_prompts: int,\n    num_logprobs: Optional[int],\n) -> None:\n    \"\"\"Test vLLM engine with multi-step scheduling via sync LLM Engine.\n\n    Set up a HuggingFace (HF) transformers model as a ground-truth reference.\n\n    Prompt them with the same example prompts.\n\n    Validate:\n    * Generated tokens match\n    * Generated logprobs are all very close\n\n    Args:\n      hf_runner: HF transformers model runner fixture\n      vllm_runner: vLLM model runner fixture\n      example_prompts: test fixture providing example prompts\n      model: model under test (same for single- and multi-step engines)\n      dtype: tensor datatype for engine to utilize\n      tp_size: degree of tensor-parallelism\n      enable_chunked_prefill: chunked-prefill on/off\n      max_tokens: the maximum number of tokens to generate\n      enforce_eager\n      num_scheduler_steps: for multi-step scheduling, GPU-side steps per\n                           GPU -> CPU output transfer\n      num_prompts: number of example prompts under test\n      num_logprobs: corresponds to the `logprobs` argument to the OpenAI\n                    completions endpoint; `None` -> 1 logprob returned.\n    \"\"\"\n\n    prompts = example_prompts\n    if len(prompts) < num_prompts:\n        prompts = prompts * ((num_prompts // len(prompts)) + 1)\n    prompts = prompts[:num_prompts]\n    assert len(prompts) == num_prompts\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enforce_eager=enforce_eager,\n            gpu_memory_utilization=0.7,\n            tensor_parallel_size=tp_size,\n            use_v2_block_manager=True,\n            enable_chunked_prefill=enable_chunked_prefill,\n            num_scheduler_steps=num_scheduler_steps,\n    ) as vllm_model:\n        vllm_outputs = (vllm_model.generate_greedy(prompts, max_tokens)\n                        if num_logprobs is None else\n                        vllm_model.generate_greedy_logprobs(\n                            prompts, max_tokens, num_logprobs))\n\n    with hf_runner(model, dtype=dtype) as hf_model:\n        hf_outputs = (hf_model.generate_greedy(prompts, max_tokens)\n                      if num_logprobs is None else\n                      hf_model.generate_greedy_logprobs_limit(\n                          prompts, max_tokens, num_logprobs))\n\n    if num_logprobs is None:\n        check_outputs_equal(\n            outputs_0_lst=hf_outputs,\n            outputs_1_lst=vllm_outputs,\n            name_0=\"hf\",\n            name_1=\"vllm\",\n        )\n    else:\n        check_logprobs_close(\n            outputs_0_lst=hf_outputs,\n            outputs_1_lst=vllm_outputs,\n            name_0=\"hf\",\n            name_1=\"vllm\",\n        )\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"tp_size\", [1])\n@pytest.mark.parametrize(\"max_tokens\", [5])\n@pytest.mark.parametrize(\"enforce_eager\", [True])\n@pytest.mark.parametrize(\"num_scheduler_steps\", NUM_SCHEDULER_STEPS)\n@pytest.mark.parametrize(\"num_prompts\", NUM_PROMPTS)\n@pytest.mark.parametrize(\"num_logprobs,num_prompt_logprobs\", [(5, 5)])\ndef test_multi_step_llm_w_prompt_logprobs(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    tp_size: int,\n    max_tokens: int,\n    enforce_eager: int,\n    num_scheduler_steps: int,\n    num_prompts: int,\n    num_logprobs: Optional[int],\n    num_prompt_logprobs: Optional[int],\n) -> None:\n    \"\"\"Test prompt logprobs with multi-step scheduling via sync LLM Engine.\n\n    Set up a vLLM engine instance w/ single-step scheduling as a ground-truth\n    reference.\n\n    Prompt them with the same example prompts.\n\n    Validate:\n    * All generated logprobs are all very close\n\n    Args:\n      hf_runner: HF transformers model runner fixture\n      vllm_runner: vLLM model runner fixture\n      example_prompts: test fixture providing example prompts\n      model: model under test (same for single- and multi-step engines)\n      dtype: tensor datatype for engine to utilize\n      tp_size: degree of tensor-parallelism\n      max_tokens: the maximum number of tokens to generate\n      enforce_eager\n      num_scheduler_steps: for multi-step scheduling, GPU-side steps per\n                           GPU -> CPU output transfer\n      num_prompts: number of example prompts under test\n      num_logprobs: corresponds to the `logprobs` argument to the OpenAI\n                    completions endpoint; `None` -> no logprobs\n      num_prompt_logprobs: number of logprobs to return for each prompt token;\n                           note that this argument is not supported by the\n                           OpenAI completions endpoint.\n    \"\"\"\n\n    prompts = example_prompts\n    if len(prompts) < num_prompts:\n        prompts = prompts * ((num_prompts // len(prompts)) + 1)\n    prompts = prompts[:num_prompts]\n    assert len(prompts) == num_prompts\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enforce_eager=enforce_eager,\n            gpu_memory_utilization=0.7,\n            tensor_parallel_size=tp_size,\n            use_v2_block_manager=True,\n            num_scheduler_steps=num_scheduler_steps,\n    ) as vllm_model:\n        vllm_outputs = vllm_model.generate_greedy_logprobs(\n            prompts,\n            max_tokens,\n            num_logprobs,\n            num_prompt_logprobs=num_prompt_logprobs)\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enforce_eager=enforce_eager,\n            gpu_memory_utilization=0.7,\n            tensor_parallel_size=tp_size,\n    ) as vllm_model:\n        single_step_vllm_outputs = vllm_model.generate_greedy_logprobs(\n            prompts,\n            max_tokens,\n            num_logprobs,\n            num_prompt_logprobs=num_prompt_logprobs)\n\n    check_logprobs_close(\n        outputs_0_lst=single_step_vllm_outputs,\n        outputs_1_lst=vllm_outputs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"tp_size\", [1])\n@pytest.mark.parametrize(\"max_tokens\", [5])\n@pytest.mark.parametrize(\"enforce_eager\", [True])\n@pytest.mark.parametrize(\"num_scheduler_steps\", NUM_SCHEDULER_STEPS)\n@pytest.mark.parametrize(\"num_prompts\", NUM_PROMPTS)\n@pytest.mark.parametrize(\"num_logprobs\", [None, 5])\ndef test_multi_step_llm_chunked_prefill_prefix_cache(\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    tp_size: int,\n    max_tokens: int,\n    enforce_eager: int,\n    num_scheduler_steps: int,\n    num_prompts: int,\n    num_logprobs: Optional[int],\n) -> None:\n    \"\"\"Test vLLM engine with multi-step+\"single-step chunked prefill\"+APC.\n\n    Set up contrived scenario which tests for a possible failure mode of\n    scheduling with multi-step+\"single-step chunked prefill\"+APC\n\n    \"single-step chunked prefill\" here refers to the current vLLM multi-step+\n    chunked-prefill implementation, which requires that a prefill may only\n    be scheduled in the same step as decodes if the prefill prompt fits in a\n    single chunk (note that \"complete\" multi-step+chunked-prefill would allow\n    a prefill to span multiple chunks & multiple steps but that is not yet\n    the case.)\n\n    \"APC\" is short for \"automatic prefix caching\".\n\n    This test creates a scenario where the scheduler must decide whether/how\n    to schedule a prefill with a prompt that exceeds the available token budget.\n    The correct behavior for multi-step+\"single-step chunked prefill\"+APC is to\n    put off scheduling the prefill until a future step.\n\n    Validate that:\n    * Multi-step kernels do not raise an exception due to incorrect scheduler\n      behavior\n    * Generated tokens match between\n      multi-step+\"single-step chunked prefill\"+APC and\n      single-step scheduling.\n    * (If logprobs are enabled) check logprobs are close enough\n\n    Args:\n      vllm_runner: vLLM model runner fixture\n      example_prompts: test fixture providing example prompts\n      model: model under test (same for single- and multi-step engines)\n      dtype: tensor datatype for engine to utilize\n      tp_size: degree of tensor-parallelism\n      max_tokens: the maximum number of tokens to generate\n      enforce_eager\n      num_scheduler_steps: for multi-step scheduling, GPU-side steps per\n                           GPU -> CPU output transfer\n      num_prompts: number of example prompts under test\n      num_logprobs: corresponds to the `logprobs` argument to the OpenAI\n                    completions endpoint; `None` -> 1 logprob returned.\n    \"\"\"\n\n    # Set up contrived test for correct scheduling behavior with\n    # multi-step+\"single-step chunked prefill\"+APC.\n    #\n    # Assume block_size=16\n    #\n    # Assume max_num_batched_tokens=48\n    #   => Per-step token budget=48\n    #\n    # 1. Scheduler schedules 0th prompt (24 tokens)\n    #      => Remaining token budget=24\n    # 2. Scheduler attempts to schedule 1st prompt (30 tokens)\n    #    * 30 tokens exceeds 24 token remaining budget\n    #    * Correct behavior: do not schedule this prompt in this step\n    #    * Incorrect behavior: schedule prompt chunk\n    #      * `do_sample=False` for this prompt in this step\n    #      * Chunk size = (remaining tokens // block size) * block size\n    #\n    # The Incorrect scheduling behavior - if it occurs - will cause an exception\n    # in the model runner resulting from `do_sample=False`.\n    assert len(example_prompts) >= 2\n    challenge_prompts = copy.deepcopy(example_prompts)\n    challenge_prompts[0] = ('vLLM is a high-throughput and memory-efficient '\n                            'inference and serving engine for LLMs.\\n'\n                            )  # 24 tok\n    challenge_prompts[1] = (\n        'Briefly describe the major milestones in the '\n        'development of artificial intelligence from 1950 to 2020.\\n'\n    )  # 30 tok\n\n    # If necessary, adjust the length of `challenge_prompts` to match\n    # `num_prompts`\n    if len(challenge_prompts) < num_prompts:\n        challenge_prompts = (challenge_prompts *\n                             ((num_prompts // len(challenge_prompts)) + 1))\n    challenge_prompts = challenge_prompts[:num_prompts]\n    assert len(challenge_prompts) == num_prompts\n\n    # Single-step scheduler baseline\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enforce_eager=enforce_eager,\n            gpu_memory_utilization=0.7,\n            tensor_parallel_size=tp_size,\n            use_v2_block_manager=True,\n            num_scheduler_steps=num_scheduler_steps,\n            max_model_len=48,\n            max_num_batched_tokens=48,\n            max_num_seqs=4,\n            block_size=16,\n    ) as vllm_model:\n        outputs_baseline = (vllm_model.generate_greedy(\n            challenge_prompts, max_tokens) if num_logprobs is None else\n                            vllm_model.generate_greedy_logprobs(\n                                challenge_prompts, max_tokens, num_logprobs))\n\n    # multi-step+\"single-step chunked prefill\"+APC\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enforce_eager=enforce_eager,\n            gpu_memory_utilization=0.7,\n            tensor_parallel_size=tp_size,\n            use_v2_block_manager=True,\n            enable_chunked_prefill=True,\n            enable_prefix_caching=True,\n            num_scheduler_steps=num_scheduler_steps,\n            max_model_len=48,\n            max_num_batched_tokens=48,\n            max_num_seqs=4,\n            block_size=16,\n    ) as vllm_model:\n        outputs_w_features = (vllm_model.generate_greedy(\n            challenge_prompts, max_tokens) if num_logprobs is None else\n                              vllm_model.generate_greedy_logprobs(\n                                  challenge_prompts, max_tokens, num_logprobs))\n\n    if num_logprobs is None:\n        # No-logprobs test\n        check_outputs_equal(\n            outputs_0_lst=outputs_baseline,\n            outputs_1_lst=outputs_w_features,\n            name_0=\"multi-step\",\n            name_1=\"multi-step+features\",\n        )\n    else:\n        # Yes-logprobs test\n        check_logprobs_close(\n            outputs_0_lst=outputs_baseline,\n            outputs_1_lst=outputs_w_features,\n            name_0=\"multi-step\",\n            name_1=\"multi-step+features\",\n        )\n",
      "diff": "diff --git a/tests/multi_step/test_correctness_llm.py b/tests/multi_step/test_correctness_llm.py\nindex f45428675..cc1fd1925 100644\n--- a/tests/multi_step/test_correctness_llm.py\n+++ b/tests/multi_step/test_correctness_llm.py\n@@ -76,7 +76,6 @@ def test_multi_step_llm(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             enable_chunked_prefill=enable_chunked_prefill,\n             num_scheduler_steps=num_scheduler_steps,\n     ) as vllm_model:\n@@ -169,7 +168,6 @@ def test_multi_step_llm_w_prompt_logprobs(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             num_scheduler_steps=num_scheduler_steps,\n     ) as vllm_model:\n         vllm_outputs = vllm_model.generate_greedy_logprobs(\n@@ -305,7 +303,6 @@ def test_multi_step_llm_chunked_prefill_prefix_cache(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             num_scheduler_steps=num_scheduler_steps,\n             max_model_len=48,\n             max_num_batched_tokens=48,\n@@ -324,7 +321,6 @@ def test_multi_step_llm_chunked_prefill_prefix_cache(\n             enforce_eager=enforce_eager,\n             gpu_memory_utilization=0.7,\n             tensor_parallel_size=tp_size,\n-            use_v2_block_manager=True,\n             enable_chunked_prefill=True,\n             enable_prefix_caching=True,\n             num_scheduler_steps=num_scheduler_steps,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 5
    },
    {
      "file_path": "tests/prefix_caching/test_prefix_caching.py",
      "old_content": "\"\"\"Compare the with and without prefix caching.\n\nRun `pytest tests/prefix_caching/test_prefix_caching.py`.\n\"\"\"\nfrom typing import List\n\nimport pytest\n\nfrom tests.kernels.utils import override_backend_env_variable\nfrom tests.utils import check_deprecated_block_manager_usage\nfrom vllm.block import PhysicalTokenBlock\nfrom vllm.core.block_manager_v1 import CachedBlockAllocator\nfrom vllm.utils import Device\n\nfrom ..models.utils import check_outputs_equal\n\nMODELS = [\n    \"facebook/opt-125m\",\n]\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/prefix_caching/test_prefix_caching.py')\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_blocks\", [16])\ndef test_block_allocator(\n    block_size: int,\n    num_blocks: int,\n):\n    block_hash = 1\n    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n\n    # Allocate two PysicalTokenBlocks with the same hash and check\n    # that they are the same PhysicalTokenBlock\n    first_block = block_allocator.allocate(block_hash, 0)\n    second_block = block_allocator.allocate(block_hash, 0)\n    assert (first_block == second_block)\n    assert (second_block.ref_count == 2)\n\n    # Check metric: 1 hit of 2 queries\n    assert block_allocator.get_prefix_cache_hit_rate() == 0.5\n\n    # Free the first_block and confirm that the ref_count is correctly\n    # decremented on the second block\n    block_allocator.free(first_block)\n    assert (second_block.ref_count == 1)\n\n    # Free the second block\n    block_allocator.free(second_block)\n\n    # Reallocate the first block and confirm that, even after the block\n    # had its ref_count go to 0, we still get the same block back\n    first_block = block_allocator.allocate(block_hash, 0)\n    assert (first_block == second_block)\n    assert (first_block.block_hash == block_hash)\n\n    # Allocate one more time to get 3/4 hit rate for easy checking\n    block_allocator.allocate(block_hash, 0)\n    assert block_allocator.get_prefix_cache_hit_rate() == 0.75\n\n\n@pytest.mark.parametrize(\"num_blocks\", [16])\ndef test_eviction(num_blocks: int, ):\n    block_size = 16\n    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n    blocks: List[PhysicalTokenBlock] = []\n\n    for i in range(num_blocks):\n        # use i as the block_hash\n        blocks.append(block_allocator.allocate(i, 0))\n\n    #Free all blocks\n    for block in blocks:\n        block_allocator.free(block)\n\n    # Allocate a new block and confirm that it's the first block freed.\n    # I.E The Least Recently Used block\n    new_block_hash = block_size\n    new_block = block_allocator.allocate(new_block_hash, 0)\n    assert (new_block == blocks[0])\n    assert (new_block.block_hash == new_block_hash)\n\n    # Reallocate the second in blocks to remove it from the free list\n    realloc_block_hash = 1\n    realloc_block = block_allocator.allocate(realloc_block_hash, 0)\n    assert (realloc_block == blocks[realloc_block_hash])\n    assert (realloc_block.block_hash == realloc_block_hash)\n\n    # Allocate a new block and confirm that it's not the realloc_block,\n    # since the realloc_block shouldn't be in the free list\n    new_block_hash = block_size + 1\n    new_block = block_allocator.allocate(new_block_hash, 0)\n    assert (realloc_block != new_block)\n    assert (new_block.block_hash == new_block_hash)\n    assert (new_block.block_number == 2)\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"backend\", [\"FLASH_ATTN\", \"FLASHINFER\", \"XFORMERS\"])\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [5])\n@pytest.mark.parametrize(\"cached_position\", [0, 1])\n@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\ndef test_mixed_requests(\n    hf_runner,\n    vllm_runner,\n    example_prompts,\n    model: str,\n    backend: str,\n    dtype: str,\n    max_tokens: int,\n    cached_position: int,\n    use_v2_block_manager: bool,\n    monkeypatch,\n) -> None:\n    \"\"\"\n    Test the case when some sequences have the prefix cache hit\n    and the others don't. The cached position determines where \n    the sequence is at among the batch of prefills.\n    \"\"\"\n    override_backend_env_variable(monkeypatch, backend)\n\n    with hf_runner(model, dtype=dtype) as hf_model:\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\n\n    cached_prompt = example_prompts[cached_position]\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            enable_prefix_caching=True,\n            use_v2_block_manager=use_v2_block_manager,\n    ) as vllm_model:\n        # Run the first prompt so the cache is populated\n        vllm_outputs = vllm_model.generate_greedy([cached_prompt], max_tokens)\n\n        # Run all the promopts\n        vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n\n    check_outputs_equal(\n        outputs_0_lst=hf_outputs,\n        outputs_1_lst=vllm_outputs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n",
      "diff": "diff --git a/tests/prefix_caching/test_prefix_caching.py b/tests/prefix_caching/test_prefix_caching.py\nindex 88437425f..366b030ea 100644\n--- a/tests/prefix_caching/test_prefix_caching.py\n+++ b/tests/prefix_caching/test_prefix_caching.py\n@@ -2,15 +2,9 @@\n \n Run `pytest tests/prefix_caching/test_prefix_caching.py`.\n \"\"\"\n-from typing import List\n-\n import pytest\n \n from tests.kernels.utils import override_backend_env_variable\n-from tests.utils import check_deprecated_block_manager_usage\n-from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager_v1 import CachedBlockAllocator\n-from vllm.utils import Device\n \n from ..models.utils import check_outputs_equal\n \n@@ -19,92 +13,11 @@ MODELS = [\n ]\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/prefix_caching/test_prefix_caching.py')\n-\n-\n-@pytest.mark.parametrize(\"block_size\", [16])\n-@pytest.mark.parametrize(\"num_blocks\", [16])\n-def test_block_allocator(\n-    block_size: int,\n-    num_blocks: int,\n-):\n-    block_hash = 1\n-    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n-\n-    # Allocate two PysicalTokenBlocks with the same hash and check\n-    # that they are the same PhysicalTokenBlock\n-    first_block = block_allocator.allocate(block_hash, 0)\n-    second_block = block_allocator.allocate(block_hash, 0)\n-    assert (first_block == second_block)\n-    assert (second_block.ref_count == 2)\n-\n-    # Check metric: 1 hit of 2 queries\n-    assert block_allocator.get_prefix_cache_hit_rate() == 0.5\n-\n-    # Free the first_block and confirm that the ref_count is correctly\n-    # decremented on the second block\n-    block_allocator.free(first_block)\n-    assert (second_block.ref_count == 1)\n-\n-    # Free the second block\n-    block_allocator.free(second_block)\n-\n-    # Reallocate the first block and confirm that, even after the block\n-    # had its ref_count go to 0, we still get the same block back\n-    first_block = block_allocator.allocate(block_hash, 0)\n-    assert (first_block == second_block)\n-    assert (first_block.block_hash == block_hash)\n-\n-    # Allocate one more time to get 3/4 hit rate for easy checking\n-    block_allocator.allocate(block_hash, 0)\n-    assert block_allocator.get_prefix_cache_hit_rate() == 0.75\n-\n-\n-@pytest.mark.parametrize(\"num_blocks\", [16])\n-def test_eviction(num_blocks: int, ):\n-    block_size = 16\n-    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n-    blocks: List[PhysicalTokenBlock] = []\n-\n-    for i in range(num_blocks):\n-        # use i as the block_hash\n-        blocks.append(block_allocator.allocate(i, 0))\n-\n-    #Free all blocks\n-    for block in blocks:\n-        block_allocator.free(block)\n-\n-    # Allocate a new block and confirm that it's the first block freed.\n-    # I.E The Least Recently Used block\n-    new_block_hash = block_size\n-    new_block = block_allocator.allocate(new_block_hash, 0)\n-    assert (new_block == blocks[0])\n-    assert (new_block.block_hash == new_block_hash)\n-\n-    # Reallocate the second in blocks to remove it from the free list\n-    realloc_block_hash = 1\n-    realloc_block = block_allocator.allocate(realloc_block_hash, 0)\n-    assert (realloc_block == blocks[realloc_block_hash])\n-    assert (realloc_block.block_hash == realloc_block_hash)\n-\n-    # Allocate a new block and confirm that it's not the realloc_block,\n-    # since the realloc_block shouldn't be in the free list\n-    new_block_hash = block_size + 1\n-    new_block = block_allocator.allocate(new_block_hash, 0)\n-    assert (realloc_block != new_block)\n-    assert (new_block.block_hash == new_block_hash)\n-    assert (new_block.block_number == 2)\n-\n-\n @pytest.mark.parametrize(\"model\", MODELS)\n @pytest.mark.parametrize(\"backend\", [\"FLASH_ATTN\", \"FLASHINFER\", \"XFORMERS\"])\n @pytest.mark.parametrize(\"dtype\", [\"half\"])\n @pytest.mark.parametrize(\"max_tokens\", [5])\n @pytest.mark.parametrize(\"cached_position\", [0, 1])\n-@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n def test_mixed_requests(\n     hf_runner,\n     vllm_runner,\n@@ -114,7 +27,6 @@ def test_mixed_requests(\n     dtype: str,\n     max_tokens: int,\n     cached_position: int,\n-    use_v2_block_manager: bool,\n     monkeypatch,\n ) -> None:\n     \"\"\"\n@@ -132,7 +44,6 @@ def test_mixed_requests(\n             model,\n             dtype=dtype,\n             enable_prefix_caching=True,\n-            use_v2_block_manager=use_v2_block_manager,\n     ) as vllm_model:\n         # Run the first prompt so the cache is populated\n         vllm_outputs = vllm_model.generate_greedy([cached_prompt], max_tokens)",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 90
    },
    {
      "file_path": "tests/spec_decode/e2e/test_compatibility.py",
      "old_content": "import pytest\n\nfrom tests.utils import check_deprecated_block_manager_usage\nfrom vllm import SamplingParams\n\nfrom .conftest import get_output_from_llm_generator\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef check_deprecated_block_manager():\n    check_deprecated_block_manager_usage(\n        'tests/spec_decode/e2e/test_compatibility.py')\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model\": \"JackFram/llama-68m\",\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"enable_chunked_prefill\": True,\n    },\n])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_spec_decode_xfail_chunked_prefill(test_llm_generator):\n    \"\"\"Verify that speculative decoding with chunked prefill fails.\n    \"\"\"\n    output_len = 128\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n    ]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    with pytest.raises(ValueError,\n                       match=\"Speculative decoding and chunked prefill\"):\n        get_output_from_llm_generator(test_llm_generator, prompts,\n                                      sampling_params)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        {\n            # Speculative max model len > overridden max model len should raise.\n            \"max_model_len\": 128,\n            \"speculative_max_model_len\": 129,\n        },\n        {\n            # Speculative max model len > draft max model len should raise.\n            # https://huggingface.co/JackFram/llama-68m/blob/3b606af5198a0b26762d589a3ee3d26ee6fa6c85/config.json#L12\n            \"speculative_max_model_len\": 2048 + 1,\n        },\n        {\n            # Speculative max model len > target max model len should raise.\n            # https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/blob/f5db02db724555f92da89c216ac04704f23d4590/config.json#L12\n            \"speculative_max_model_len\": 4096 + 1,\n        },\n    ])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_spec_decode_xfail_spec_max_model_len(test_llm_generator):\n    \"\"\"Verify that speculative decoding validates speculative_max_model_len.\n    \"\"\"\n    output_len = 128\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n    ]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    with pytest.raises(ValueError, match=\"cannot be larger than\"):\n        get_output_from_llm_generator(test_llm_generator, prompts,\n                                      sampling_params)\n\n\n@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n    \"model\": \"JackFram/llama-68m\",\n    \"speculative_model\": \"JackFram/llama-68m\",\n    \"num_speculative_tokens\": 5,\n    \"use_v2_block_manager\": False,\n}])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_spec_decode_xfail_block_manager_v1(test_llm_generator):\n    \"\"\"Verify that speculative decoding with block manager v1 fails.\n    \"\"\"\n    output_len = 128\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n    ]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    with pytest.raises(ValueError,\n                       match=\"Speculative decoding requires usage of the V2\"):\n        get_output_from_llm_generator(test_llm_generator, prompts,\n                                      sampling_params)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_compatibility.py b/tests/spec_decode/e2e/test_compatibility.py\nindex 69ea81cff..629074188 100644\n--- a/tests/spec_decode/e2e/test_compatibility.py\n+++ b/tests/spec_decode/e2e/test_compatibility.py\n@@ -1,27 +1,15 @@\n import pytest\n \n-from tests.utils import check_deprecated_block_manager_usage\n from vllm import SamplingParams\n \n from .conftest import get_output_from_llm_generator\n \n \n-@pytest.fixture(scope=\"module\", autouse=True)\n-def check_deprecated_block_manager():\n-    check_deprecated_block_manager_usage(\n-        'tests/spec_decode/e2e/test_compatibility.py')\n-\n-\n-@pytest.mark.parametrize(\n-    \"common_llm_kwargs\",\n-    [{\n-        \"model\": \"JackFram/llama-68m\",\n-        \"speculative_model\": \"JackFram/llama-68m\",\n-        \"num_speculative_tokens\": 5,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n-    }])\n+@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n+    \"model\": \"JackFram/llama-68m\",\n+    \"speculative_model\": \"JackFram/llama-68m\",\n+    \"num_speculative_tokens\": 5,\n+}])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n         \"enable_chunked_prefill\": True,\n@@ -51,16 +39,11 @@ def test_spec_decode_xfail_chunked_prefill(test_llm_generator):\n                                       sampling_params)\n \n \n-@pytest.mark.parametrize(\n-    \"common_llm_kwargs\",\n-    [{\n-        \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n-        \"speculative_model\": \"JackFram/llama-68m\",\n-        \"num_speculative_tokens\": 5,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n-    }])\n+@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n+    \"model\": \"meta-llama/Llama-2-7b-chat-hf\",\n+    \"speculative_model\": \"JackFram/llama-68m\",\n+    \"num_speculative_tokens\": 5,\n+}])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n     [\n@@ -101,34 +84,3 @@ def test_spec_decode_xfail_spec_max_model_len(test_llm_generator):\n     with pytest.raises(ValueError, match=\"cannot be larger than\"):\n         get_output_from_llm_generator(test_llm_generator, prompts,\n                                       sampling_params)\n-\n-\n-@pytest.mark.parametrize(\"common_llm_kwargs\", [{\n-    \"model\": \"JackFram/llama-68m\",\n-    \"speculative_model\": \"JackFram/llama-68m\",\n-    \"num_speculative_tokens\": 5,\n-    \"use_v2_block_manager\": False,\n-}])\n-@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n-@pytest.mark.parametrize(\"seed\", [1])\n-def test_spec_decode_xfail_block_manager_v1(test_llm_generator):\n-    \"\"\"Verify that speculative decoding with block manager v1 fails.\n-    \"\"\"\n-    output_len = 128\n-    temperature = 0.0\n-\n-    prompts = [\n-        \"Hello, my name is\",\n-    ]\n-\n-    sampling_params = SamplingParams(\n-        max_tokens=output_len,\n-        ignore_eos=True,\n-        temperature=temperature,\n-    )\n-\n-    with pytest.raises(ValueError,\n-                       match=\"Speculative decoding requires usage of the V2\"):\n-        get_output_from_llm_generator(test_llm_generator, prompts,\n-                                      sampling_params)",
      "change_type": "modified",
      "lines_added": 11,
      "lines_removed": 59
    },
    {
      "file_path": "tests/spec_decode/e2e/test_eagle_correctness.py",
      "old_content": "\"\"\"This docstring details important information on the testing methodology.\n\nMost of the tests rely on \"greedy equality\", where we expect the output of\nspeculative decoding on a sequence to exactly match the output of normal non-\nspeculative decoding.\n\nSince speculative decoding with rejection sampling guarantees that the output\ndistribution matches the target model's output distribution (up to hardware\nnumerics, see https://arxiv.org/pdf/2302.01318.pdf), we can expect greedy\nequality.\n\nHowever, we still need to verify below scenario could be passed:\n    * Batch size 1 greedy equality\n    * Batch size >1 greedy equality\n    * Test greedy equality under preemption\n    * Test greedy equality under various number of speculative tokens.\n\nWith those tests, we can say at least, EAGLE would not break the\ncorrectess for the target model outputs.\n\"\"\"\n\nimport pytest\n\nfrom .conftest import run_equality_correctness_test\n\n# main model\nMAIN_MODEL = \"JackFram/llama-68m\"\n\n# speculative model\nSPEC_MODEL = \"abhigoyal/vllm-eagle-llama-68m-random\"\n\n# max. number of speculative tokens: this corresponds to\n# num_heads in the config.json of the speculator model.\nMAX_SPEC_TOKENS = 4\n\n# precision\nPRECISION = \"float32\"\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_eagle_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n                                      per_test_common_llm_kwargs,\n                                      baseline_llm_kwargs, test_llm_kwargs,\n                                      batch_size: int, output_len: int,\n                                      seed: int):\n\n    run_equality_correctness_test(vllm_runner, common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs, test_llm_kwargs,\n                                  batch_size, output_len, seed)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n        \"disable_logprobs_during_spec_decoding\": False,\n    },\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n        \"disable_logprobs_during_spec_decoding\": True,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_eagle_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n                                   per_test_common_llm_kwargs,\n                                   baseline_llm_kwargs, test_llm_kwargs,\n                                   batch_size: int, output_len: int, seed: int,\n                                   logprobs: int):\n\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  output_len,\n                                  seed,\n                                  logprobs=logprobs,\n                                  prompt_logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"enforce_eager\": False,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_eagle_e2e_greedy_correctness_cuda_graph(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality with cuda graph enabled and different\n    batch sizes.\"\"\"\n    run_equality_correctness_test(vllm_runner, common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs, test_llm_kwargs,\n                                  batch_size, output_len, seed)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        128,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_eagle_e2e_greedy_correctness_with_preemption(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality, even when some sequences are preempted mid-\n    generation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner, common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs, test_llm_kwargs,\n                                  batch_size, output_len, seed)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": SPEC_MODEL,\n            \"num_speculative_tokens\": k,\n        }\n        # Try a range of num. speculative tokens\n        for k in range(1, 1 + MAX_SPEC_TOKENS)\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_eagle_different_k(vllm_runner, common_llm_kwargs,\n                           per_test_common_llm_kwargs, baseline_llm_kwargs,\n                           test_llm_kwargs, batch_size: int, output_len: int,\n                           seed: int):\n    \"\"\"Verify that eagle speculative decoding produces exact equality\n    to without spec decode with different values of num_speculative_tokens.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner, common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs, test_llm_kwargs,\n                                  batch_size, output_len, seed)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": SPEC_MODEL,\n                             \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n                             \"speculative_disable_by_batch_size\": 4\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_eagle_disable_queue(vllm_runner, common_llm_kwargs,\n                             per_test_common_llm_kwargs, baseline_llm_kwargs,\n                             test_llm_kwargs, batch_size: int, output_len: int,\n                             seed: int):\n    \"\"\"Verify that eagle speculative decoding produces exact equality\n    to without spec decode when speculation is disabled for large\n    batch sizes.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner, common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs, test_llm_kwargs,\n                                  batch_size, output_len, seed)\n\n\nif __name__ == \"__main__\":\n    import pytest\n    pytest.main([__file__])\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_eagle_correctness.py b/tests/spec_decode/e2e/test_eagle_correctness.py\nindex d7ca8815e..5bc70de9d 100644\n--- a/tests/spec_decode/e2e/test_eagle_correctness.py\n+++ b/tests/spec_decode/e2e/test_eagle_correctness.py\n@@ -43,9 +43,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -86,9 +83,6 @@ def test_eagle_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -143,9 +137,6 @@ def test_eagle_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n     [{\n         \"enforce_eager\": False,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -191,9 +182,6 @@ def test_eagle_e2e_greedy_correctness_cuda_graph(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -235,9 +223,6 @@ def test_eagle_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -283,9 +268,6 @@ def test_eagle_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 19
    },
    {
      "file_path": "tests/spec_decode/e2e/test_integration.py",
      "old_content": "\"\"\"Tests which cover integration of the speculative decoding framework with\nother features, e.g. cuda graphs.\n\"\"\"\n\nimport pytest\n\nfrom .conftest import run_equality_correctness_test\n\nMAIN_MODEL = \"JackFram/llama-68m\"\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Verify equality when cuda graphs allowed.\n        \"enforce_eager\": False,\n        \"model_name\": \"JackFram/llama-68m\",\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        {\n            # Identical models.\n            \"speculative_model\": \"JackFram/llama-68m\",\n            \"num_speculative_tokens\": 5,\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"output_len\", [32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_spec_decode_cuda_graph(vllm_runner, common_llm_kwargs,\n                                per_test_common_llm_kwargs,\n                                baseline_llm_kwargs, test_llm_kwargs,\n                                batch_size: int, output_len: int, seed: int):\n    \"\"\"Verify spec decode equality when cuda graphs are enabled.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"speculative_model\": \"LnL-AI/TinyLlama-1.1B-Chat-v1.0-GPTQ-4bit\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        # Explicitly specify draft model quantization\n        {\n            \"speculative_model_quantization\": \"gptq\",\n        },\n        # Explicitly specify GPTQ-based draft model to use marlin quantization\n        {\n            \"speculative_model_quantization\": \"marlin\",\n        },\n        # Not explicitly specify draft model quantization\n        {\n            \"speculative_model_quantization\": None,\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_speculative_model_quantization_config(vllm_runner, common_llm_kwargs,\n                                               per_test_common_llm_kwargs,\n                                               baseline_llm_kwargs,\n                                               test_llm_kwargs,\n                                               batch_size: int, seed: int):\n    \"\"\"Verify spec decode works well with draft model quantization configs.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=32,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": MAIN_MODEL,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 3,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_disable_mqa_scorer\": True,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mqa_scorer(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n                    baseline_llm_kwargs, test_llm_kwargs, batch_size: int,\n                    output_len: int, seed: int):\n    \"\"\"Verify that ngram speculative decoding generates the same output \n    with batch expansion scorer and mqa scorer.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_integration.py b/tests/spec_decode/e2e/test_integration.py\nindex d04e31268..b89e58497 100644\n--- a/tests/spec_decode/e2e/test_integration.py\n+++ b/tests/spec_decode/e2e/test_integration.py\n@@ -12,8 +12,6 @@ MAIN_MODEL = \"JackFram/llama-68m\"\n @pytest.mark.parametrize(\n     \"common_llm_kwargs\",\n     [{\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n \n         # Verify equality when cuda graphs allowed.\n         \"enforce_eager\": False,\n@@ -57,9 +55,6 @@ def test_spec_decode_cuda_graph(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -111,9 +106,6 @@ def test_speculative_model_quantization_config(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": \"JackFram/llama-68m\",\n         \"num_speculative_tokens\": 3,\n     }])",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 9
    },
    {
      "file_path": "tests/spec_decode/e2e/test_integration_dist_tp2.py",
      "old_content": "\"\"\"Tests which cover integration of the speculative decoding framework with\ntensor parallelism.\n\"\"\"\n\nimport pytest\nimport torch\n\nfrom vllm.utils import is_hip\n\nfrom .conftest import run_equality_correctness_test_tp\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2,\n                    reason=\"Need at least 2 GPUs to run the test.\")\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [[\n        # Skip cuda graph recording for fast test.\n        \"--enforce-eager\",\n\n        # Required for spec decode.\n        \"--use-v2-block-manager\",\n        \"--tensor-parallel-size\",\n        \"2\"\n    ]])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    [\n        \"--speculative-model\",\n        \"JackFram/llama-68m\",\n        \"--num-speculative-tokens\",\n        \"3\",\n    ],\n    [\n        \"--speculative-model\",\n        \"[ngram]\",\n        \"--num-speculative-tokens\",\n        \"5\",\n        \"--ngram-prompt-lookup-max\",\n        \"3\",\n    ],\n])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_target_model_tp_gt_1(common_llm_kwargs, per_test_common_llm_kwargs,\n                              baseline_llm_kwargs, test_llm_kwargs,\n                              batch_size: int, output_len: int, seed: int):\n    \"\"\"Verify greedy equality when tensor parallelism is used.\n    \"\"\"\n    if is_hip():\n        pytest.skip(\"hip is not well-supported yet\")\n    run_equality_correctness_test_tp(\"JackFram/llama-68m\",\n                                     common_llm_kwargs,\n                                     per_test_common_llm_kwargs,\n                                     baseline_llm_kwargs,\n                                     test_llm_kwargs,\n                                     batch_size,\n                                     output_len,\n                                     seed,\n                                     temperature=0.0)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 2,\n                    reason=\"Need at least 2 GPUs to run the test.\")\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [[\n        # Skip cuda graph recording for fast test.\n        \"--enforce-eager\",\n\n        # Required for spec decode.\n        \"--use_v2_block_manager\",\n        \"--tensor_parallel_size\",\n        \"2\",\n\n        # precision\n        \"--dtype\",\n        \"bfloat16\",\n    ]])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\"model, test_llm_kwargs\",\n                         [(\"JackFram/llama-68m\", [\n                             \"--speculative-model\",\n                             \"JackFram/llama-68m\",\n                             \"--num_speculative-tokens\",\n                             \"5\",\n                             \"--speculative-draft-tensor-parallel-size\",\n                             \"1\",\n                         ]),\n                          (\"ibm-granite/granite-3b-code-instruct\", [\n                              \"--speculative-model\",\n                              \"ibm-granite/granite-3b-code-instruct\",\n                              \"--num_speculative-tokens\",\n                              \"5\",\n                              \"--speculative-draft-tensor-parallel-size\",\n                              \"1\",\n                          ])])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_draft_model_tp_lt_target_model_tp2(model, common_llm_kwargs,\n                                            per_test_common_llm_kwargs,\n                                            baseline_llm_kwargs,\n                                            test_llm_kwargs, batch_size: int,\n                                            seed: int):\n    \"\"\"Verify spec decode works well with smaller tp for draft models.\n    \"\"\"\n    run_equality_correctness_test_tp(model,\n                                     common_llm_kwargs,\n                                     per_test_common_llm_kwargs,\n                                     baseline_llm_kwargs,\n                                     test_llm_kwargs,\n                                     batch_size,\n                                     max_output_len=32,\n                                     seed=seed,\n                                     temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_integration_dist_tp2.py b/tests/spec_decode/e2e/test_integration_dist_tp2.py\nindex 679a6ded9..b829d1a5b 100644\n--- a/tests/spec_decode/e2e/test_integration_dist_tp2.py\n+++ b/tests/spec_decode/e2e/test_integration_dist_tp2.py\n@@ -17,9 +17,6 @@ from .conftest import run_equality_correctness_test_tp\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"2\"\n     ]])\n@@ -74,9 +71,6 @@ def test_target_model_tp_gt_1(common_llm_kwargs, per_test_common_llm_kwargs,\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use_v2_block_manager\",\n         \"--tensor_parallel_size\",\n         \"2\",",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 7
    },
    {
      "file_path": "tests/spec_decode/e2e/test_integration_dist_tp4.py",
      "old_content": "\"\"\"Tests which cover integration of the speculative decoding framework with\ntensor parallelism.\n\"\"\"\n\nimport openai\nimport pytest\nimport torch\n\nfrom .conftest import run_equality_correctness_test_tp\n\nMAIN_MODEL = \"JackFram/llama-68m\"\nSPEC_MODEL = \"JackFram/llama-68m\"\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 4,\n                    reason=\"Need at least 4 GPUs to run the test.\")\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [[\n        # Skip cuda graph recording for fast test.\n        \"--enforce_eager\",\n\n        # Required for spec decode.\n        \"--use-v2-block-manager\",\n        \"--tensor-parallel-size\",\n        \"4\",\n    ]])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    [\n        \"--speculative-model\",\n        f\"{SPEC_MODEL}\",\n        \"--num-speculative-tokens\",\n        \"5\",\n    ],\n])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        #TODO(wooyeon): add spec_draft_dp=2 case\n        [\n            \"--speculative-draft-tensor-parallel-size\",\n            \"1\",\n        ],\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_draft_model_tp_lt_target_model_tp4(common_llm_kwargs,\n                                            per_test_common_llm_kwargs,\n                                            baseline_llm_kwargs,\n                                            test_llm_kwargs, batch_size: int,\n                                            seed: int):\n    \"\"\"Verify spec decode works well with smaller tp for draft models.\n    \"\"\"\n    run_equality_correctness_test_tp(MAIN_MODEL,\n                                     common_llm_kwargs,\n                                     per_test_common_llm_kwargs,\n                                     baseline_llm_kwargs,\n                                     test_llm_kwargs,\n                                     batch_size,\n                                     max_output_len=32,\n                                     seed=seed,\n                                     temperature=0.0)\n\n\n@pytest.mark.skipif(torch.cuda.device_count() < 4,\n                    reason=\"Need at least 4 GPUs to run the test.\")\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [[\n\n        # Skip cuda graph recording for fast test.\n        \"--enforce-eager\",\n\n        # Required for spec decode.\n        \"--use-v2-block-manager\",\n        \"--tensor-parallel-size\",\n        \"4\",\n    ]])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [[]])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        [\n            \"--speculative-model\",\n            f\"{SPEC_MODEL}\",\n            \"--num-speculative-tokens\",\n            \"5\",\n\n            # Artificially limit the draft model max model len; this forces vLLM\n            # to skip speculation once the sequences grow beyond 32-k tokens.\n            \"--speculative-max-model-len\",\n            \"32\",\n        ],\n    ])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # This must be a good bit larger than speculative_max_model_len so that\n        # we can test the case where all seqs are skipped, but still small to\n        # ensure fast test.\n        64,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_skip_speculation(common_llm_kwargs, per_test_common_llm_kwargs,\n                          baseline_llm_kwargs, test_llm_kwargs,\n                          batch_size: int, output_len: int, seed: int):\n    \"\"\"Verify job failure with RuntimeError when all sequences skip speculation.\n    We do this by setting the max model len of the draft model to an\n    artificially low value, such that when the sequences grow beyond it, they\n    are skipped in speculative decoding.\n\n    TODO: fix it to pass without raising Error. (#5814)\n    \"\"\"\n    with pytest.raises(openai.APIConnectionError):\n        run_equality_correctness_test_tp(MAIN_MODEL,\n                                         common_llm_kwargs,\n                                         per_test_common_llm_kwargs,\n                                         baseline_llm_kwargs,\n                                         test_llm_kwargs,\n                                         batch_size,\n                                         output_len,\n                                         seed,\n                                         temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_integration_dist_tp4.py b/tests/spec_decode/e2e/test_integration_dist_tp4.py\nindex 3f7c5d749..555aef992 100644\n--- a/tests/spec_decode/e2e/test_integration_dist_tp4.py\n+++ b/tests/spec_decode/e2e/test_integration_dist_tp4.py\n@@ -19,9 +19,6 @@ SPEC_MODEL = \"JackFram/llama-68m\"\n     [[\n         # Skip cuda graph recording for fast test.\n         \"--enforce_eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"4\",\n     ]])\n@@ -71,9 +68,6 @@ def test_draft_model_tp_lt_target_model_tp4(common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"--enforce-eager\",\n-\n-        # Required for spec decode.\n-        \"--use-v2-block-manager\",\n         \"--tensor-parallel-size\",\n         \"4\",\n     ]])",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 7
    },
    {
      "file_path": "tests/spec_decode/e2e/test_logprobs.py",
      "old_content": "from itertools import cycle\n\nimport pytest\n\nfrom vllm import SamplingParams\n\nfrom .conftest import run_equality_correctness_test\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": \"JackFram/llama-160m\",\n                             \"num_speculative_tokens\": 3,\n                             \"disable_logprobs_during_spec_decoding\": False,\n                         }, {\n                             \"speculative_model\": \"JackFram/llama-160m\",\n                             \"num_speculative_tokens\": 3,\n                             \"disable_logprobs_during_spec_decoding\": True,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        7,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_logprobs_equality(vllm_runner, common_llm_kwargs,\n                           per_test_common_llm_kwargs, baseline_llm_kwargs,\n                           test_llm_kwargs, batch_size: int, output_len: int,\n                           seed: int, logprobs: int):\n    \"\"\"Verify output logprobs are equal with and without speculative decoding.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  output_len,\n                                  seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  prompt_logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": \"JackFram/llama-160m\",\n                             \"num_speculative_tokens\": 3,\n                             \"disable_logprobs_during_spec_decoding\": False,\n                         }, {\n                             \"speculative_model\": \"JackFram/llama-160m\",\n                             \"num_speculative_tokens\": 6,\n                             \"disable_logprobs_during_spec_decoding\": False,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_logprobs_different_k(vllm_runner, common_llm_kwargs,\n                              per_test_common_llm_kwargs, baseline_llm_kwargs,\n                              test_llm_kwargs, batch_size: int,\n                              output_len: int, seed: int, logprobs: int):\n    \"\"\"Veriy logprob greedy equality with different speculation lens.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  output_len,\n                                  seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [{\n        \"speculative_model\": \"JackFram/llama-160m\",\n        \"num_speculative_tokens\": 3,\n        \"disable_logprobs_during_spec_decoding\": False,\n\n        # Artificially limit the draft model max model len; this forces vLLM\n        # to skip speculation once the sequences grow beyond 32-k tokens.\n        \"speculative_max_model_len\": 32,\n    }])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1])\ndef test_logprobs_when_skip_speculation(vllm_runner, common_llm_kwargs,\n                                        per_test_common_llm_kwargs,\n                                        baseline_llm_kwargs, test_llm_kwargs,\n                                        batch_size: int, output_len: int,\n                                        seed: int, logprobs: int):\n    \"\"\"Verify logprobs greedy equality when some sequences skip speculation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  output_len,\n                                  seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": \"JackFram/llama-160m\",\n                             \"num_speculative_tokens\": 3,\n                             \"disable_logprobs_during_spec_decoding\": False,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [6])\ndef test_logprobs_temp_1(vllm_runner, common_llm_kwargs,\n                         per_test_common_llm_kwargs, baseline_llm_kwargs,\n                         test_llm_kwargs, batch_size: int, output_len: int,\n                         seed: int, logprobs: int):\n    \"\"\"Verify at least one logprob result has num_logprobs+1, which tests the\n    case where the sampled token is not in top-k logprobs.\n\n    Ideally, this test should validate equality with non-spec by getting\n    logprobs. This is left as future improvement.\n    \"\"\"\n    temperature = 1.0\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n        \"San Francisco is know for its\",\n        \"Facebook was created in 2004 by\",\n        \"Curious George is a\",\n        \"Python 3.11 brings improvements to its\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n        logprobs=logprobs,\n    )\n\n    sd_args = {\n        **common_llm_kwargs,\n        **per_test_common_llm_kwargs,\n        **test_llm_kwargs,\n    }\n\n    with vllm_runner(**sd_args) as vllm_model:\n        sd_outputs = vllm_model.generate_w_logprobs(prompts, sampling_params)\n\n    num_returned_logprobs = [\n        len(seq_logprobs) for seq_logprobs in sd_outputs[-1]\n    ]\n\n    # Assert one of the returned logprobs has > num_logprobs (indicating the\n    # sampled token is not in top-k).\n    assert any(\n        [num_returned > logprobs for num_returned in num_returned_logprobs])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": \"JackFram/llama-68m\",\n                             \"num_speculative_tokens\": 3,\n                             \"disable_logprobs_during_spec_decoding\": True,\n                         }])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"logprobs\", [0])\ndef test_logprobs_disabled(vllm_runner, common_llm_kwargs,\n                           per_test_common_llm_kwargs, baseline_llm_kwargs,\n                           test_llm_kwargs, batch_size: int, output_len: int,\n                           seed: int, logprobs: int):\n    \"\"\"Check the behavior when logprobs are disabled.\n    Token choices should match with the base model.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  output_len,\n                                  seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_logprobs.py b/tests/spec_decode/e2e/test_logprobs.py\nindex b7d54991e..4cfca8b78 100644\n--- a/tests/spec_decode/e2e/test_logprobs.py\n+++ b/tests/spec_decode/e2e/test_logprobs.py\n@@ -14,9 +14,6 @@ from .conftest import run_equality_correctness_test\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -67,9 +64,6 @@ def test_logprobs_equality(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -119,9 +113,6 @@ def test_logprobs_different_k(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -173,9 +164,6 @@ def test_logprobs_when_skip_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -251,8 +239,6 @@ def test_logprobs_temp_1(vllm_runner, common_llm_kwargs,\n         \"model_name\": \"JackFram/llama-160m\",\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 15
    },
    {
      "file_path": "tests/spec_decode/e2e/test_medusa_correctness.py",
      "old_content": "\"\"\"This docstring details important information on the testing methodology.\n\nMost of the tests rely on \"greedy equality\", where we expect the output of\nspeculative decoding on a sequence to exactly match the output of normal non-\nspeculative decoding.\n\nSince speculative decoding with rejection sampling guarantees that the output\ndistribution matches the target model's output distribution (up to hardware\nnumerics, see https://arxiv.org/pdf/2302.01318.pdf), we can expect greedy\nequality.\n\nHowever, we still need to verify below scenario could be passed:\n    * Batch size 1 greedy equality\n    * Batch size >1 greedy equality\n    * Test greedy equality under preemption\n    * Test greedy equality under various number of speculative tokens.\n\nWith those tests, we can say at least, Medusa would not break the\ncorrectess for the target model outputs.\n\"\"\"\n\nimport pytest\n\nfrom .conftest import run_equality_correctness_test\n\n# main model\n# lmsys/vicuna-7b-v1.3 was to be used but it's causing\n# OOM in CI pipeline, so using a smaller model.\nMAIN_MODEL = \"JackFram/llama-68m\"\n\n# speculative model\nSPEC_MODEL = \"abhigoyal/vllm-medusa-llama-68m-random\"\n\n# max number of speculative tokens: this corresponds to\n# num_heads in the config.json of the speculator model.\nMAX_SPEC_TOKENS = 5\n\n# precision\nPRECISION = \"float32\"\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_medusa_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n                                       per_test_common_llm_kwargs,\n                                       baseline_llm_kwargs, test_llm_kwargs,\n                                       batch_size: int, output_len: int,\n                                       seed: int):\n    \"\"\"Verify greedy equality with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n        \"disable_logprobs_during_spec_decoding\": False,\n    },\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n        \"disable_logprobs_during_spec_decoding\": True,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    8,\n])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_medusa_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n                                    per_test_common_llm_kwargs,\n                                    baseline_llm_kwargs, test_llm_kwargs,\n                                    batch_size: int, output_len: int,\n                                    seed: int, logprobs: int):\n    \"\"\"Verify greedy equality with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  prompt_logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"enforce_eager\": False,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_medusa_e2e_greedy_correctness_cuda_graph(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality with cuda graph enabled and different \n    batch sizes.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        128,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_medusa_e2e_greedy_correctness_with_preemption(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality, even when some sequences are preempted mid-\n    generation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": SPEC_MODEL,\n            \"num_speculative_tokens\": k,\n        }\n        # Try a range of num. speculative tokens\n        for k in range(1, 1 + MAX_SPEC_TOKENS)\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_medusa_different_k(vllm_runner, common_llm_kwargs,\n                            per_test_common_llm_kwargs, baseline_llm_kwargs,\n                            test_llm_kwargs, batch_size: int, output_len: int,\n                            seed: int):\n    \"\"\"Verify that medusa speculative decoding produces exact equality\n    to without spec decode with different values of num_speculative_tokens.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": SPEC_MODEL,\n                             \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n                             \"speculative_disable_by_batch_size\": 4\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_medusa_disable_queue(vllm_runner, common_llm_kwargs,\n                              per_test_common_llm_kwargs, baseline_llm_kwargs,\n                              test_llm_kwargs, batch_size: int,\n                              output_len: int, seed: int):\n    \"\"\"Verify that medusa speculative decoding produces exact equality\n    to without spec decode when speculation is disabled for large\n    batch sizes.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n        \"speculative_model\": SPEC_MODEL,\n        \"num_speculative_tokens\": MAX_SPEC_TOKENS,\n        \"speculative_disable_by_batch_size\": 4\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_disable_mqa_scorer\": True,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mqa_scorer(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n                    baseline_llm_kwargs, test_llm_kwargs, batch_size: int,\n                    output_len: int, seed: int):\n    \"\"\"Verify that speculative decoding generates the same output \n    with batch expansion scorer and mqa scorer.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\nif __name__ == \"__main__\":\n    import pytest\n    pytest.main([__file__])\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_medusa_correctness.py b/tests/spec_decode/e2e/test_medusa_correctness.py\nindex 0b36e712a..b8965606b 100644\n--- a/tests/spec_decode/e2e/test_medusa_correctness.py\n+++ b/tests/spec_decode/e2e/test_medusa_correctness.py\n@@ -45,9 +45,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -93,9 +90,6 @@ def test_medusa_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -151,9 +145,6 @@ def test_medusa_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n     [{\n         \"enforce_eager\": False,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -204,9 +195,6 @@ def test_medusa_e2e_greedy_correctness_cuda_graph(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -253,9 +241,6 @@ def test_medusa_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -306,9 +291,6 @@ def test_medusa_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -356,9 +338,6 @@ def test_medusa_disable_queue(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 22
    },
    {
      "file_path": "tests/spec_decode/e2e/test_mlp_correctness.py",
      "old_content": "\"\"\"This docstring details important information on the testing methodology.\n\nMost of the tests rely on \"greedy equality\", where we expect the output of\nspeculative decoding on a sequence to exactly match the output of normal non-\nspeculative decoding.\n\nSince speculative decoding with rejection sampling guarantees that the output\ndistribution matches the target model's output distribution (up to hardware\nnumerics, see https://arxiv.org/pdf/2302.01318.pdf), we can expect greedy\nequality.\n\nHowever, we still need to verify below scenario could be passed:\n    * Batch size 1 greedy equality\n    * Batch size >1 greedy equality\n    * Test greedy equality under preemption\n    * Test greedy equality under various number of speculative tokens.\n\nWith those tests, we can say at least, MLPSpeculator would not break the\ncorrectness for the target model outputs.\n\"\"\"\n\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom vllm.model_executor.layers.vocab_parallel_embedding import pad_vocab_size\n\nfrom .conftest import run_equality_correctness_test\n\n# main model\nMAIN_MODEL = \"JackFram/llama-160m\"\n\n# speculative model\nSPEC_MODEL = \"ibm-fms/llama-160m-accelerator\"\n\n# max. number of speculative tokens: this corresponds to\n# n_predict in the config.json of the speculator model.\nMAX_SPEC_TOKENS = 3\n\n# precision\nPRECISION = \"float32\"\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    128,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n                                    per_test_common_llm_kwargs,\n                                    baseline_llm_kwargs, test_llm_kwargs,\n                                    batch_size: int, output_len: int,\n                                    seed: int):\n    \"\"\"Verify greedy equality with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"disable_logprobs_during_spec_decoding\": False,\n    },\n    {\n        \"speculative_model\": SPEC_MODEL,\n        \"disable_logprobs_during_spec_decoding\": True,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [8])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_mlp_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n                                 per_test_common_llm_kwargs,\n                                 baseline_llm_kwargs, test_llm_kwargs,\n                                 batch_size: int, output_len: int, seed: int,\n                                 logprobs: int):\n    \"\"\"Verify greedy equality with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  prompt_logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [2048])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_e2e_acceptance_rate(vllm_runner, common_llm_kwargs,\n                                 per_test_common_llm_kwargs,\n                                 baseline_llm_kwargs, test_llm_kwargs,\n                                 batch_size: int, output_len: int, seed: int):\n    \"\"\"Verify acceptance rate with different batch size and large output \n    length.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  temperature=0.0,\n                                  seed=seed,\n                                  expected_acceptance_rate=0.48)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n\n        # Speculative model\n        \"speculative_model\": SPEC_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\"seed\": 1}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"seed\": 5}])\n@pytest.mark.parametrize(\"output_len\", [64])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"temperature\", [0.1, 1.0])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_e2e_seeded_correctness(vllm_runner, common_llm_kwargs,\n                                    per_test_common_llm_kwargs,\n                                    baseline_llm_kwargs, test_llm_kwargs,\n                                    batch_size: int, output_len: int,\n                                    temperature: float, seed: int):\n    \"\"\"Verify seeded runs produce the same output.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  temperature=temperature,\n                                  seed=seed)\n\n    # Ensure this same test does fail if we _don't_ include per-request seeds\n    with pytest.raises(AssertionError):\n        run_equality_correctness_test(vllm_runner,\n                                      common_llm_kwargs,\n                                      per_test_common_llm_kwargs,\n                                      baseline_llm_kwargs,\n                                      test_llm_kwargs,\n                                      batch_size,\n                                      max_output_len=output_len,\n                                      temperature=temperature,\n                                      seed=seed,\n                                      disable_seed=True)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        128,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_e2e_greedy_correctness_with_preemption(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality, even when some sequences are preempted mid-\n    generation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": SPEC_MODEL,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        128,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_e2e_greedy_correctness_with_padding(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality when the vocab dimension is padded\n    \"\"\"\n\n    # Default pad_to is 64, test model has vocab_size of 32000\n    def patched_pad_vocab_size(vocab_size, pad_to=None):\n        return pad_vocab_size(vocab_size, pad_to=32064)\n\n    with patch(\n            \"vllm.model_executor.layers.vocab_parallel_embedding.pad_vocab_size\",\n            patched_pad_vocab_size):\n        run_equality_correctness_test(vllm_runner,\n                                      common_llm_kwargs,\n                                      per_test_common_llm_kwargs,\n                                      baseline_llm_kwargs,\n                                      test_llm_kwargs,\n                                      batch_size,\n                                      max_output_len=output_len,\n                                      seed=seed,\n                                      temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": SPEC_MODEL,\n            \"num_speculative_tokens\": k,\n        }\n        # Try a range of num. speculative tokens\n        for k in range(1, 1 + MAX_SPEC_TOKENS)\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_different_k(vllm_runner, common_llm_kwargs,\n                         per_test_common_llm_kwargs, baseline_llm_kwargs,\n                         test_llm_kwargs, batch_size: int, seed: int,\n                         output_len: int):\n    \"\"\"Verify that mlp speculative decoding produces exact equality\n    to without spec decode with different values of num_speculative_tokens.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Precision\n        \"dtype\": PRECISION,\n\n        # Main model\n        \"model_name\": MAIN_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": SPEC_MODEL,\n                             \"speculative_disable_by_batch_size\": 4\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mlp_disable_queue(vllm_runner, common_llm_kwargs,\n                           per_test_common_llm_kwargs, baseline_llm_kwargs,\n                           test_llm_kwargs, batch_size: int, seed: int,\n                           output_len: int):\n    \"\"\"Verify that mlp speculative decoding produces exact equality\n    to without spec decode when speculation is disabled for large\n    batch sizes.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": MAIN_MODEL,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n        \"speculative_model\": SPEC_MODEL,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_disable_mqa_scorer\": True,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_mqa_scorer(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n                    baseline_llm_kwargs, test_llm_kwargs, batch_size: int,\n                    output_len: int, seed: int):\n    \"\"\"Verify that speculative decoding generates the same output \n    with batch expansion scorer and mqa scorer.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_mlp_correctness.py b/tests/spec_decode/e2e/test_mlp_correctness.py\nindex 52b48a33c..5ecc0d4e9 100644\n--- a/tests/spec_decode/e2e/test_mlp_correctness.py\n+++ b/tests/spec_decode/e2e/test_mlp_correctness.py\n@@ -47,9 +47,6 @@ PRECISION = \"float32\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -94,9 +91,6 @@ def test_mlp_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -149,9 +143,6 @@ def test_mlp_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -195,9 +186,6 @@ def test_mlp_e2e_acceptance_rate(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n \n@@ -258,9 +246,6 @@ def test_mlp_e2e_seeded_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -311,9 +296,6 @@ def test_mlp_e2e_greedy_correctness_with_preemption(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -366,9 +348,6 @@ def test_mlp_e2e_greedy_correctness_with_padding(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -419,9 +398,6 @@ def test_mlp_different_k(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Precision\n         \"dtype\": PRECISION,\n \n@@ -469,9 +445,6 @@ def test_mlp_disable_queue(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": SPEC_MODEL,\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 28
    },
    {
      "file_path": "tests/spec_decode/e2e/test_multistep_correctness.py",
      "old_content": "\"\"\"The tests in this file verify end-to-end speculative decoding correctness.\n\nThis docstring details important information on the testing methodology.\n\nMost of the tests rely on \"greedy equality\", where we expect the output of\nspeculative decoding on a sequence to exactly match the output of normal non-\nspeculative decoding.\n\nSince speculative decoding with rejection sampling guarantees that the output\ndistribution matches the target model's output distribution (up to hardware\nnumerics, see https://arxiv.org/pdf/2302.01318.pdf), we can expect greedy\nequality. This gives us good coverage of temp=0.\n\nAt temp=0, the TypicalAcceptanceSampler ensures that only the tokens with the\nhighest probability in the target distribution are accepted. Therefore, we can \nexpect greedy equality for the TypicalAcceptanceSampler at temp=0.\n\nFor temp>0, we rely on unit tests on the rejection sampler to verify that the\noutput distribution is the same with spec decode vs. no spec decode (this would\nbe prohibitively expensive to run with a real model). Similarly, for the\nTypicalAcceptance sampler also, we rely on unit tests to validate temp>0\ntest cases.\n\nNOTE: Speculative decoding's distribution equality requires that the measured\ndistributions of the target model and proposal model be deterministic given the\nsame input. vLLM largely guarantees this.\n\n@cadedaniel has seen cases where the output probabilities of a draft/target\nmodel change slightly with certain batch sizes or prompts, even with Torch\ndeterminism flags set. It is unclear if this is a bug in vLLM, due to non-\ndeterminism in on-device batched operations, a bug in vLLM's spec decode\nimplementation, or the \"hardware numerics\" limitations. Either way, rejection\nsampling ensures the output distribution matches the target model, but it breaks\ngreedy-equality tests for those batch sizes/prompts.\n\"\"\"\n\nfrom itertools import cycle\n\nimport pytest\nfrom transformers import AutoTokenizer\n\nfrom vllm import SamplingParams\n\nfrom ...utils import fork_new_process_for_each_test\nfrom .conftest import (get_output_from_llm_generator,\n                       run_equality_correctness_test)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Use a small model for a fast test.\n        # Note this is repeated in the test body; to initialize a tokenizer.\n        \"model\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": \"JackFram/llama-68m\",\n            \"num_speculative_tokens\": 5,\n        },\n        {\n            # Verify the detokenizer assertions in the test work when spec\n            # decode is disabled.\n        },\n    ])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_with_detokenization(test_llm_generator,\n                                             batch_size: int):\n    \"\"\"Run generation with speculative decoding on a batch. Verify the engine\n    generates the correct number of tokens (via ignore_eos=True), and that the\n    detokenization matches HF transformers.\n    \"\"\"\n    output_len = 32\n    temperature = 0.0\n\n    prompts = [\n        \"Hello, my name is\",\n        \"The president of the United States is\",\n        \"The capital of France is\",\n        \"The future of AI is\",\n    ]\n\n    prompts = [prompt for prompt, _ in zip(cycle(prompts), range(batch_size))]\n\n    sampling_params = SamplingParams(\n        max_tokens=output_len,\n        ignore_eos=True,\n        temperature=temperature,\n    )\n\n    batch_tokens, batch_token_ids, _ = get_output_from_llm_generator(\n        test_llm_generator, prompts, sampling_params)\n\n    # Expect a generation for each prompt in the batch.\n    assert len(batch_token_ids) == len(prompts)\n\n    # Expect each generation to have expected number of tokens (note ignore_eos\n    # is True).\n    assert [len(token_ids)\n            for token_ids in batch_token_ids] == ([output_len] * batch_size)\n\n    # Expect detokenized string to match.\n    tok = AutoTokenizer.from_pretrained(\"JackFram/llama-68m\")\n    for actual_tokens, actual_token_ids in zip(batch_tokens, batch_token_ids):\n        expected_tokens = tok.decode(actual_token_ids)\n        print(f\"{actual_token_ids=}\")\n        assert actual_tokens.strip() == expected_tokens.strip()\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        # Try two different tiny base models.\n        # Note that one is equal to the draft model, another isn't.\n        {\n            \"model_name\": \"JackFram/llama-68m\",\n        },\n        {\n            \"model_name\": \"JackFram/llama-160m\",\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use long output len for the small model test.\n        10,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [1])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_tiny_model_bs1(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality on a tiny model with batch size of one.\n\n    Since this test is cheaper than other e2e correctness tests, we generate\n    with a higher output_len.\n\n    When the draft model is the same as the target model, we further check\n    whether all speculative tokens are accepted.\n    \"\"\"\n    ensure_all_accepted = per_test_common_llm_kwargs.get(\n        \"model_name\") == test_llm_kwargs.get(\"speculative_model\")\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0,\n                                  ensure_all_accepted=ensure_all_accepted)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        # Try two different tiny base models.\n        # Note that one is equal to the draft model, another isn't.\n        {\n            \"model_name\": \"JackFram/llama-68m\",\n        },\n        {\n            \"model_name\": \"JackFram/llama-160m\",\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        256,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [64])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality on a tiny model and large batch size.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        # Try two different tiny base models.\n        # Note that one is equal to the draft model, another isn't.\n        {\n            \"model_name\": \"JackFram/llama-68m\",\n        },\n        {\n            \"model_name\": \"JackFram/llama-160m\",\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\"max_output_len\", [\n    256,\n])\n@pytest.mark.parametrize(\"batch_size\", [32])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs_diff_output_len(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int,\n        max_output_len: int, seed: int):\n    \"\"\"Verify greedy equality on a tiny model, with a large batch size, and when\n    sampling respects the EOS token.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len,\n                                  seed=seed,\n                                  temperature=0.0,\n                                  ignore_eos=False)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # A \"real\" model (not tiny).\n        \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\"batch_size\", [1])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use decently long output len for a high quality test.\n        256,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_real_model_bs1(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality on a \"real\" model and batch size of 1. This is\n    separate from large BS tests to make identifying the source of bugs easier.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # A \"real\" model (not tiny).\n        \"model_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\"batch_size\", [32])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        64,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_real_model_large_bs(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality with a \"real\" model on a nontrivial batch size.\n    This is the closest test to a real production workload.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"model_name\": \"JackFram/llama-160m\",\n    },\n])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        256,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_e2e_greedy_correctness_with_preemption(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality, even when some sequences are preempted mid-\n    generation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\n    \"per_test_common_llm_kwargs\",\n    [\n        # As of this writing, vLLM only compiles with these 3 block sizes by\n        # default.\n        {\n            \"block_size\": 8,\n        },\n        {\n            \"block_size\": 16,\n        },\n        {\n            \"block_size\": 32,\n        },\n    ])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n    },\n])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_spec_decode_different_block_size(vllm_runner, common_llm_kwargs,\n                                          per_test_common_llm_kwargs,\n                                          baseline_llm_kwargs, test_llm_kwargs,\n                                          batch_size: int, output_len: int,\n                                          seed: int):\n    \"\"\"Verify greedy equality over different block sizes.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": \"JackFram/llama-68m\",\n            \"num_speculative_tokens\": 5,\n\n            # Artificially limit the draft model max model len; this forces vLLM\n            # to skip speculation once the sequences grow beyond 32-k tokens.\n            \"speculative_max_model_len\": 32,\n        },\n    ])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # This must be a good bit larger than speculative_max_model_len so that\n        # we can test the case where all seqs are skipped, but still small to\n        # ensure fast test.\n        64,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_skip_speculation(vllm_runner, common_llm_kwargs,\n                          per_test_common_llm_kwargs, baseline_llm_kwargs,\n                          test_llm_kwargs, batch_size: int, output_len: int,\n                          seed: int):\n    \"\"\"Verify greedy equality when some (or all) sequences skip speculation.\n    We do this by setting the max model len of the draft model to an\n    artificially low value, such that when the sequences grow beyond it, they\n    are skipped in speculative decoding.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"JackFram/llama-68m\",\n        \"num_speculative_tokens\": 5,\n        \"speculative_disable_by_batch_size\": 2,\n    },\n])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"output_len\", [10])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_disable_speculation(vllm_runner, common_llm_kwargs,\n                             per_test_common_llm_kwargs, baseline_llm_kwargs,\n                             test_llm_kwargs, batch_size: int, output_len: int,\n                             seed: int):\n    \"\"\"Verify greedy equality when all sequences disable speculation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": \"JackFram/llama-68m\",\n            \"num_speculative_tokens\": k,\n        }\n        # Try a range of common k, as well as large speculation.\n        for k in [1, 2, 3, 4, 5, 6, 7, 8, 9, 63]\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_many_k(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n                baseline_llm_kwargs, test_llm_kwargs, batch_size: int,\n                output_len: int, seed: int):\n    \"\"\"Verify that speculative decoding produces exact equality to without spec\n    decode with many different values of k.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-160m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": \"JackFram/llama-68m\",\n            \"num_speculative_tokens\": k,\n            \"spec_decoding_acceptance_method\": \"typical_acceptance_sampler\"\n        }\n        # Try a range of common k.\n        for k in [1, 2, 3]\n    ])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\n@fork_new_process_for_each_test\ndef test_typical_acceptance_sampling(vllm_runner, common_llm_kwargs,\n                                     per_test_common_llm_kwargs,\n                                     baseline_llm_kwargs, test_llm_kwargs,\n                                     batch_size: int, output_len: int,\n                                     seed: int):\n    \"\"\"Verify that speculative decoding produces exact equality to without spec\n    decode with TypicalAcceptanceSampler as the draft token acceptance\n    sampling method.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_multistep_correctness.py b/tests/spec_decode/e2e/test_multistep_correctness.py\nindex df6f12d57..5f240d42d 100644\n--- a/tests/spec_decode/e2e/test_multistep_correctness.py\n+++ b/tests/spec_decode/e2e/test_multistep_correctness.py\n@@ -55,9 +55,6 @@ from .conftest import (get_output_from_llm_generator,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -124,9 +121,6 @@ def test_spec_decode_e2e_with_detokenization(test_llm_generator,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -190,9 +184,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_bs1(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -246,9 +237,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs(\n     [{\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -303,9 +291,6 @@ def test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs_diff_output_len(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -353,9 +338,6 @@ def test_spec_decode_e2e_greedy_correctness_real_model_bs1(\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -404,9 +386,6 @@ def test_spec_decode_e2e_greedy_correctness_real_model_large_bs(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -454,9 +433,6 @@ def test_spec_decode_e2e_greedy_correctness_with_preemption(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\n     \"per_test_common_llm_kwargs\",\n@@ -514,9 +490,6 @@ def test_spec_decode_different_block_size(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -570,9 +543,6 @@ def test_skip_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -611,9 +581,6 @@ def test_disable_speculation(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -660,9 +627,6 @@ def test_many_k(vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 37
    },
    {
      "file_path": "tests/spec_decode/e2e/test_ngram_correctness.py",
      "old_content": "\"\"\"This docstring details important information on the testing methodology.\n\nMost of the tests rely on \"greedy equality\", where we expect the output of\nspeculative decoding on a sequence to exactly match the output of normal non-\nspeculative decoding.\n\nSince speculative decoding with rejection sampling guarantees that the output\ndistribution matches the target model's output distribution (up to hardware\nnumerics, see https://arxiv.org/pdf/2302.01318.pdf), we can expect greedy\nequality.\n\nFor ngram lookup, its idea comes from https://github.com/apoorvumang/prompt-lookup-decoding,\nand is merged into transform code base: https://github.com/huggingface/transformers/pull/27775.\nSince there is no model is needed for generate the proposal, we could make\nthe testcase much simpler than drafter multi-step one.\n\nHowever, we still need to verify below scenario could be passed:\n    * Batch size 1 greedy equality\n    * Batch size >1 greedy equality\n    * Test greedy equality under preemption\n    * Test greedy equality under various ngram sizes / speculative sizes\n\nWith those tests, we can say at least, ngram spec would not break the correctess\nfor the target model outputs.\n\"\"\"\n\nimport pytest\n\nfrom .conftest import run_equality_correctness_test\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"model_name\": \"JackFram/llama-68m\",\n    },\n])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"[ngram]\",\n        \"num_speculative_tokens\": 5,\n        \"ngram_prompt_lookup_max\": 3,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    256,\n])\n@pytest.mark.parametrize(\"batch_size\", [1, 32])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_ngram_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n                                      per_test_common_llm_kwargs,\n                                      baseline_llm_kwargs, test_llm_kwargs,\n                                      batch_size: int, output_len: int,\n                                      seed: int):\n    \"\"\"Verify greedy equality on a tiny model with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # Print spec metrics.\n        \"disable_log_stats\": False,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"model_name\": \"JackFram/llama-68m\",\n    },\n])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"[ngram]\",\n        \"num_speculative_tokens\": 5,\n        \"ngram_prompt_lookup_max\": 3,\n        \"disable_logprobs_during_spec_decoding\": False,\n    },\n    {\n        \"speculative_model\": \"[ngram]\",\n        \"num_speculative_tokens\": 5,\n        \"ngram_prompt_lookup_max\": 3,\n        \"disable_logprobs_during_spec_decoding\": True,\n    },\n])\n@pytest.mark.parametrize(\"output_len\", [\n    8,\n])\n@pytest.mark.parametrize(\"batch_size\", [8])\n@pytest.mark.parametrize(\"seed\", [1])\n@pytest.mark.parametrize(\"logprobs\", [1, 6])\ndef test_ngram_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n                                   per_test_common_llm_kwargs,\n                                   baseline_llm_kwargs, test_llm_kwargs,\n                                   batch_size: int, output_len: int, seed: int,\n                                   logprobs: int):\n    \"\"\"Verify greedy equality on a tiny model with different batch size.\"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0,\n                                  logprobs=logprobs,\n                                  prompt_logprobs=logprobs,\n                                  disable_logprobs=test_llm_kwargs[\n                                      'disable_logprobs_during_spec_decoding'])\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"block_size\": 8,\n        # 2 for small prompt, 256//8 for generated.\n        \"num_gpu_blocks_override\": 2 + 256 // 8,\n        \"max_model_len\": (2 + 256 // 8) * 8,\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n    {\n        \"model_name\": \"JackFram/llama-160m\",\n    },\n])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [\n    {\n        \"speculative_model\": \"[ngram]\",\n        \"num_speculative_tokens\": 5,\n        \"ngram_prompt_lookup_max\": 3,\n    },\n])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use small output len for fast test.\n        256,\n    ])\n@pytest.mark.parametrize(\"batch_size\", [4])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_ngram_e2e_greedy_correctness_with_preemption(\n        vllm_runner, common_llm_kwargs, per_test_common_llm_kwargs,\n        baseline_llm_kwargs, test_llm_kwargs, batch_size: int, output_len: int,\n        seed: int):\n    \"\"\"Verify greedy equality, even when some sequences are preempted mid-\n    generation.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  temperature=0,\n                                  seed=seed)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\n    \"test_llm_kwargs\",\n    [\n        {\n            \"speculative_model\": \"[ngram]\",\n            \"num_speculative_tokens\": k,\n            \"ngram_prompt_lookup_max\": 3,\n        }\n        # Try a range of common k, as well as large speculation.\n        for k in [1, 3, 5]\n    ] + [\n        {\n            \"speculative_model\": \"[ngram]\",\n            \"num_speculative_tokens\": k,\n            \"ngram_prompt_lookup_max\": 1,\n        }\n        # Try a range of common k, as well as large speculation.\n        for k in [1, 3, 5]\n    ])\n@pytest.mark.parametrize(\"batch_size\", [2])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_ngram_different_k(vllm_runner, common_llm_kwargs,\n                           per_test_common_llm_kwargs, baseline_llm_kwargs,\n                           test_llm_kwargs, batch_size: int, output_len: int,\n                           seed: int):\n    \"\"\"Verify that ngram speculative decoding produces exact equality\n    to without spec decode with many different values of k and\n    different ngram_prompt_lookup_max.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_model\": \"[ngram]\",\n                             \"num_speculative_tokens\": 5,\n                             \"ngram_prompt_lookup_max\": 3,\n                             \"speculative_disable_by_batch_size\": 4\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_ngram_disable_queue(vllm_runner, common_llm_kwargs,\n                             per_test_common_llm_kwargs, baseline_llm_kwargs,\n                             test_llm_kwargs, batch_size: int, output_len: int,\n                             seed: int):\n    \"\"\"Verify that ngram speculative decoding produces exact equality\n    to without spec decode with many different values of k and\n    different ngram_prompt_lookup_max.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n        \"speculative_model\": \"[ngram]\",\n        \"num_speculative_tokens\": 5,\n        \"ngram_prompt_lookup_max\": 3,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"test_llm_kwargs\",\n                         [{\n                             \"speculative_disable_mqa_scorer\": True,\n                         }])\n@pytest.mark.parametrize(\"batch_size\", [1, 5])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        32,\n    ])\n@pytest.mark.parametrize(\"seed\", [1])\ndef test_ngram_scorer(vllm_runner, common_llm_kwargs,\n                      per_test_common_llm_kwargs, baseline_llm_kwargs,\n                      test_llm_kwargs, batch_size: int, output_len: int,\n                      seed: int):\n    \"\"\"Verify that ngram speculative decoding generates the same output \n    with batch expansion scorer and mqa scorer.\n    \"\"\"\n    run_equality_correctness_test(vllm_runner,\n                                  common_llm_kwargs,\n                                  per_test_common_llm_kwargs,\n                                  baseline_llm_kwargs,\n                                  test_llm_kwargs,\n                                  batch_size,\n                                  max_output_len=output_len,\n                                  seed=seed,\n                                  temperature=0.0)\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_ngram_correctness.py b/tests/spec_decode/e2e/test_ngram_correctness.py\nindex 586245938..31bedad48 100644\n--- a/tests/spec_decode/e2e/test_ngram_correctness.py\n+++ b/tests/spec_decode/e2e/test_ngram_correctness.py\n@@ -35,9 +35,6 @@ from .conftest import run_equality_correctness_test\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -82,9 +79,6 @@ def test_ngram_e2e_greedy_correctness(vllm_runner, common_llm_kwargs,\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # Print spec metrics.\n         \"disable_log_stats\": False,\n     }])\n@@ -145,9 +139,6 @@ def test_ngram_e2e_greedy_logprobs(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [\n     {\n@@ -195,9 +186,6 @@ def test_ngram_e2e_greedy_correctness_with_preemption(\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -254,9 +242,6 @@ def test_ngram_different_k(vllm_runner, common_llm_kwargs,\n \n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n-\n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True\n     }])\n @pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n @pytest.mark.parametrize(\"baseline_llm_kwargs\", [{}])\n@@ -303,7 +288,6 @@ def test_ngram_disable_queue(vllm_runner, common_llm_kwargs,\n         \"enforce_eager\": True,\n \n         # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n         \"speculative_model\": \"[ngram]\",\n         \"num_speculative_tokens\": 5,\n         \"ngram_prompt_lookup_max\": 3,",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 17
    },
    {
      "file_path": "tests/spec_decode/e2e/test_seed.py",
      "old_content": "import pytest\n\nfrom .conftest import run_equality_correctness_test\n\n# main model\nMAIN_MODEL = \"JackFram/llama-68m\"\n\n# speculative model\nSPEC_MODEL = \"JackFram/llama-160m\"\n\n\n@pytest.mark.parametrize(\n    \"common_llm_kwargs\",\n    [{\n        \"model_name\": \"JackFram/llama-68m\",\n\n        # Skip cuda graph recording for fast test.\n        \"enforce_eager\": True,\n\n        # Required for spec decode.\n        \"use_v2_block_manager\": True,\n\n        # speculative model\n        \"speculative_model\": \"JackFram/llama-160m\",\n\n        # num speculative tokens\n        \"num_speculative_tokens\": 3,\n    }])\n@pytest.mark.parametrize(\"per_test_common_llm_kwargs\", [{}])\n@pytest.mark.parametrize(\"baseline_llm_kwargs\", [{\"seed\": 1}])\n@pytest.mark.parametrize(\"test_llm_kwargs\", [{\"seed\": 5}])\n@pytest.mark.parametrize(\"batch_size\", [1, 8, 32])\n@pytest.mark.parametrize(\"temperature\", [0.1, 1.0])\n@pytest.mark.parametrize(\n    \"output_len\",\n    [\n        # Use smaller output len for fast test.\n        20,\n    ])\ndef test_seeded_consistency(vllm_runner, common_llm_kwargs,\n                            per_test_common_llm_kwargs, baseline_llm_kwargs,\n                            test_llm_kwargs, batch_size: int,\n                            temperature: float, output_len: int):\n    \"\"\"Verify outputs are consistent across multiple runs with same seed\n    \"\"\"\n    run_equality_correctness_test(\n        vllm_runner,\n        common_llm_kwargs,\n        per_test_common_llm_kwargs,\n        baseline_llm_kwargs,\n        test_llm_kwargs,\n        batch_size,\n        max_output_len=output_len,\n        temperature=temperature,\n        disable_seed=False,\n    )\n\n    # Ensure this same test does fail if we _don't_ include per-request seeds\n    with pytest.raises(AssertionError):\n        run_equality_correctness_test(\n            vllm_runner,\n            common_llm_kwargs,\n            per_test_common_llm_kwargs,\n            baseline_llm_kwargs,\n            test_llm_kwargs,\n            batch_size,\n            max_output_len=output_len,\n            temperature=temperature,\n            disable_seed=True,\n        )\n",
      "diff": "diff --git a/tests/spec_decode/e2e/test_seed.py b/tests/spec_decode/e2e/test_seed.py\nindex b17013216..e42cf416b 100644\n--- a/tests/spec_decode/e2e/test_seed.py\n+++ b/tests/spec_decode/e2e/test_seed.py\n@@ -17,9 +17,6 @@ SPEC_MODEL = \"JackFram/llama-160m\"\n         # Skip cuda graph recording for fast test.\n         \"enforce_eager\": True,\n \n-        # Required for spec decode.\n-        \"use_v2_block_manager\": True,\n-\n         # speculative model\n         \"speculative_model\": \"JackFram/llama-160m\",",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 4
    },
    {
      "file_path": "tests/utils.py",
      "old_content": "import asyncio\nimport functools\nimport os\nimport signal\nimport subprocess\nimport sys\nimport time\nimport warnings\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Any, Callable, Dict, List, Literal, Optional, Union\n\nimport openai\nimport pytest\nimport requests\nfrom openai.types.completion import Completion\nfrom typing_extensions import ParamSpec, assert_never\n\nimport vllm.envs as envs\nfrom tests.models.utils import TextTextLogprobs\nfrom vllm.distributed import (ensure_model_parallel_initialized,\n                              init_distributed_environment)\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.entrypoints.openai.cli_args import make_arg_parser\nfrom vllm.model_executor.model_loader.loader import get_model_loader\nfrom vllm.platforms import current_platform\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\nfrom vllm.utils import (FlexibleArgumentParser, GB_bytes,\n                        cuda_device_count_stateless, get_open_port, is_hip)\n\nif current_platform.is_rocm():\n    from amdsmi import (amdsmi_get_gpu_vram_usage,\n                        amdsmi_get_processor_handles, amdsmi_init,\n                        amdsmi_shut_down)\n\n    @contextmanager\n    def _nvml():\n        try:\n            amdsmi_init()\n            yield\n        finally:\n            amdsmi_shut_down()\nelif current_platform.is_cuda():\n    from pynvml import (nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo,\n                        nvmlInit, nvmlShutdown)\n\n    @contextmanager\n    def _nvml():\n        try:\n            nvmlInit()\n            yield\n        finally:\n            nvmlShutdown()\nelse:\n\n    @contextmanager\n    def _nvml():\n        yield\n\n\nVLLM_PATH = Path(__file__).parent.parent\n\"\"\"Path to root of the vLLM repository.\"\"\"\n\n\nclass RemoteOpenAIServer:\n    DUMMY_API_KEY = \"token-abc123\"  # vLLM's OpenAI server does not need API key\n\n    def __init__(self,\n                 model: str,\n                 vllm_serve_args: List[str],\n                 *,\n                 env_dict: Optional[Dict[str, str]] = None,\n                 auto_port: bool = True,\n                 max_wait_seconds: Optional[float] = None) -> None:\n        if auto_port:\n            if \"-p\" in vllm_serve_args or \"--port\" in vllm_serve_args:\n                raise ValueError(\"You have manually specified the port \"\n                                 \"when `auto_port=True`.\")\n\n            # Don't mutate the input args\n            vllm_serve_args = vllm_serve_args + [\n                \"--port\", str(get_open_port())\n            ]\n\n        parser = FlexibleArgumentParser(\n            description=\"vLLM's remote OpenAI server.\")\n        parser = make_arg_parser(parser)\n        args = parser.parse_args([\"--model\", model, *vllm_serve_args])\n        self.host = str(args.host or 'localhost')\n        self.port = int(args.port)\n\n        # download the model before starting the server to avoid timeout\n        is_local = os.path.isdir(model)\n        if not is_local:\n            engine_args = AsyncEngineArgs.from_cli_args(args)\n            model_config = engine_args.create_model_config()\n            load_config = engine_args.create_load_config()\n\n            model_loader = get_model_loader(load_config)\n            model_loader.download_model(model_config)\n\n        env = os.environ.copy()\n        # the current process might initialize cuda,\n        # to be safe, we should use spawn method\n        env['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'\n        if env_dict is not None:\n            env.update(env_dict)\n        self.proc = subprocess.Popen(\n            [\"vllm\", \"serve\", model, *vllm_serve_args],\n            env=env,\n            stdout=sys.stdout,\n            stderr=sys.stderr,\n        )\n        max_wait_seconds = max_wait_seconds or 240\n        self._wait_for_server(url=self.url_for(\"health\"),\n                              timeout=max_wait_seconds)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.proc.terminate()\n        try:\n            self.proc.wait(8)\n        except subprocess.TimeoutExpired:\n            # force kill if needed\n            self.proc.kill()\n\n    def _wait_for_server(self, *, url: str, timeout: float):\n        # run health check\n        start = time.time()\n        while True:\n            try:\n                if requests.get(url).status_code == 200:\n                    break\n            except Exception as err:\n                result = self.proc.poll()\n                if result is not None and result != 0:\n                    raise RuntimeError(\"Server exited unexpectedly.\") from err\n\n                time.sleep(0.5)\n                if time.time() - start > timeout:\n                    raise RuntimeError(\n                        \"Server failed to start in time.\") from err\n\n    @property\n    def url_root(self) -> str:\n        return f\"http://{self.host}:{self.port}\"\n\n    def url_for(self, *parts: str) -> str:\n        return self.url_root + \"/\" + \"/\".join(parts)\n\n    def get_client(self):\n        return openai.OpenAI(\n            base_url=self.url_for(\"v1\"),\n            api_key=self.DUMMY_API_KEY,\n        )\n\n    def get_async_client(self):\n        return openai.AsyncOpenAI(\n            base_url=self.url_for(\"v1\"),\n            api_key=self.DUMMY_API_KEY,\n            max_retries=0,\n        )\n\n\ndef _test_completion(\n    client: openai.OpenAI,\n    model: str,\n    prompt: str,\n    token_ids: List[int],\n):\n    results = []\n\n    # test with text prompt\n    completion = client.completions.create(model=model,\n                                           prompt=prompt,\n                                           max_tokens=5,\n                                           temperature=0.0)\n\n    results.append({\n        \"test\": \"single_completion\",\n        \"text\": completion.choices[0].text,\n        \"finish_reason\": completion.choices[0].finish_reason,\n        \"usage\": completion.usage,\n    })\n\n    # test using token IDs\n    completion = client.completions.create(\n        model=model,\n        prompt=token_ids,\n        max_tokens=5,\n        temperature=0.0,\n    )\n\n    results.append({\n        \"test\": \"token_ids\",\n        \"text\": completion.choices[0].text,\n        \"finish_reason\": completion.choices[0].finish_reason,\n        \"usage\": completion.usage,\n    })\n\n    # test seeded random sampling\n    completion = client.completions.create(model=model,\n                                           prompt=prompt,\n                                           max_tokens=5,\n                                           seed=33,\n                                           temperature=1.0)\n\n    results.append({\n        \"test\": \"seeded_sampling\",\n        \"text\": completion.choices[0].text,\n        \"finish_reason\": completion.choices[0].finish_reason,\n        \"usage\": completion.usage,\n    })\n\n    # test seeded random sampling with multiple prompts\n    completion = client.completions.create(model=model,\n                                           prompt=[prompt, prompt],\n                                           max_tokens=5,\n                                           seed=33,\n                                           temperature=1.0)\n\n    results.append({\n        \"test\":\n        \"seeded_sampling\",\n        \"text\": [choice.text for choice in completion.choices],\n        \"finish_reason\":\n        [choice.finish_reason for choice in completion.choices],\n        \"usage\":\n        completion.usage,\n    })\n\n    # test simple list\n    batch = client.completions.create(\n        model=model,\n        prompt=[prompt, prompt],\n        max_tokens=5,\n        temperature=0.0,\n    )\n\n    results.append({\n        \"test\": \"simple_list\",\n        \"text0\": batch.choices[0].text,\n        \"text1\": batch.choices[1].text,\n    })\n\n    # test streaming\n    batch = client.completions.create(\n        model=model,\n        prompt=[prompt, prompt],\n        max_tokens=5,\n        temperature=0.0,\n        stream=True,\n    )\n\n    texts = [\"\"] * 2\n    for chunk in batch:\n        assert len(chunk.choices) == 1\n        choice = chunk.choices[0]\n        texts[choice.index] += choice.text\n\n    results.append({\n        \"test\": \"streaming\",\n        \"texts\": texts,\n    })\n\n    return results\n\n\ndef _test_embeddings(\n    client: openai.OpenAI,\n    model: str,\n    text: str,\n):\n    results = []\n\n    # test with text input\n    embeddings = client.embeddings.create(\n        model=model,\n        input=text,\n        encoding_format=\"float\",\n    )\n\n    results.append({\n        \"test\": \"single_embedding\",\n        \"embedding\": embeddings.data[0].embedding,\n        \"usage\": embeddings.usage,\n    })\n\n    return results\n\n\ndef compare_two_settings(model: str,\n                         arg1: List[str],\n                         arg2: List[str],\n                         env1: Optional[Dict[str, str]] = None,\n                         env2: Optional[Dict[str, str]] = None,\n                         *,\n                         method: Literal[\"generate\", \"encode\"] = \"generate\",\n                         max_wait_seconds: Optional[float] = None) -> None:\n    \"\"\"\n    Launch API server with two different sets of arguments/environments\n    and compare the results of the API calls.\n\n    Args:\n        model: The model to test.\n        arg1: The first set of arguments to pass to the API server.\n        arg2: The second set of arguments to pass to the API server.\n        env1: The first set of environment variables to pass to the API server.\n        env2: The second set of environment variables to pass to the API server.\n    \"\"\"\n\n    compare_all_settings(\n        model,\n        [arg1, arg2],\n        [env1, env2],\n        method=method,\n        max_wait_seconds=max_wait_seconds,\n    )\n\n\ndef compare_all_settings(model: str,\n                         all_args: List[List[str]],\n                         all_envs: List[Optional[Dict[str, str]]],\n                         *,\n                         method: Literal[\"generate\", \"encode\"] = \"generate\",\n                         max_wait_seconds: Optional[float] = None) -> None:\n    \"\"\"\n    Launch API server with several different sets of arguments/environments\n    and compare the results of the API calls with the first set of arguments.\n    Args:\n        model: The model to test.\n        all_args: A list of argument lists to pass to the API server.\n        all_envs: A list of environment dictionaries to pass to the API server.\n    \"\"\"\n\n    trust_remote_code = False\n    for args in all_args:\n        if \"--trust-remote-code\" in args:\n            trust_remote_code = True\n            break\n\n    tokenizer_mode = \"auto\"\n    for args in all_args:\n        if \"--tokenizer-mode\" in args:\n            tokenizer_mode = args[args.index(\"--tokenizer-mode\") + 1]\n            break\n\n    tokenizer = get_tokenizer(\n        model,\n        trust_remote_code=trust_remote_code,\n        tokenizer_mode=tokenizer_mode,\n    )\n\n    can_force_load_format = True\n\n    for args in all_args:\n        if \"--load-format\" in args:\n            can_force_load_format = False\n            break\n\n    prompt = \"Hello, my name is\"\n    token_ids = tokenizer(prompt).input_ids\n    ref_results: List = []\n    for i, (args, env) in enumerate(zip(all_args, all_envs)):\n        if can_force_load_format:\n            # we are comparing the results and\n            # usually we don't need real weights.\n            # we force to use dummy weights by default,\n            # and it should work for most of the cases.\n            # if not, we can use VLLM_TEST_FORCE_LOAD_FORMAT\n            # environment variable to force the load format,\n            # e.g. in quantization tests.\n            args = args + [\"--load-format\", envs.VLLM_TEST_FORCE_LOAD_FORMAT]\n        compare_results: List = []\n        results = ref_results if i == 0 else compare_results\n        with RemoteOpenAIServer(model,\n                                args,\n                                env_dict=env,\n                                max_wait_seconds=max_wait_seconds) as server:\n            client = server.get_client()\n\n            # test models list\n            models = client.models.list()\n            models = models.data\n            served_model = models[0]\n            results.append({\n                \"test\": \"models_list\",\n                \"id\": served_model.id,\n                \"root\": served_model.root,\n            })\n\n            if method == \"generate\":\n                results += _test_completion(client, model, prompt, token_ids)\n            elif method == \"encode\":\n                results += _test_embeddings(client, model, prompt)\n            else:\n                assert_never(method)\n\n            if i > 0:\n                # if any setting fails, raise an error early\n                ref_args = all_args[0]\n                ref_envs = all_envs[0]\n                compare_args = all_args[i]\n                compare_envs = all_envs[i]\n                for ref_result, compare_result in zip(ref_results,\n                                                      compare_results):\n                    assert ref_result == compare_result, (\n                        f\"Results for {model=} are not the same.\\n\"\n                        f\"{ref_args=} {ref_envs=}\\n\"\n                        f\"{compare_args=} {compare_envs=}\\n\"\n                        f\"{ref_result=}\\n\"\n                        f\"{compare_result=}\\n\")\n\n\ndef init_test_distributed_environment(\n    tp_size: int,\n    pp_size: int,\n    rank: int,\n    distributed_init_port: str,\n    local_rank: int = -1,\n) -> None:\n    distributed_init_method = f\"tcp://localhost:{distributed_init_port}\"\n    init_distributed_environment(\n        world_size=pp_size * tp_size,\n        rank=rank,\n        distributed_init_method=distributed_init_method,\n        local_rank=local_rank)\n    ensure_model_parallel_initialized(tp_size, pp_size)\n\n\ndef multi_process_parallel(\n    tp_size: int,\n    pp_size: int,\n    test_target: Any,\n) -> None:\n    import ray\n\n    # Using ray helps debugging the error when it failed\n    # as compared to multiprocessing.\n    # NOTE: We need to set working_dir for distributed tests,\n    # otherwise we may get import errors on ray workers\n    ray.init(runtime_env={\"working_dir\": VLLM_PATH})\n\n    distributed_init_port = get_open_port()\n    refs = []\n    for rank in range(tp_size * pp_size):\n        refs.append(\n            test_target.remote(tp_size, pp_size, rank, distributed_init_port))\n    ray.get(refs)\n\n    ray.shutdown()\n\n\n@contextmanager\ndef error_on_warning():\n    \"\"\"\n    Within the scope of this context manager, tests will fail if any warning\n    is emitted.\n    \"\"\"\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"error\")\n\n        yield\n\n\ndef get_physical_device_indices(devices):\n    visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n    if visible_devices is None:\n        return devices\n\n    visible_indices = [int(x) for x in visible_devices.split(\",\")]\n    index_mapping = {i: physical for i, physical in enumerate(visible_indices)}\n    return [index_mapping[i] for i in devices if i in index_mapping]\n\n\n@_nvml()\ndef wait_for_gpu_memory_to_clear(devices: List[int],\n                                 threshold_bytes: int,\n                                 timeout_s: float = 120) -> None:\n    # Use nvml instead of pytorch to reduce measurement error from torch cuda\n    # context.\n    devices = get_physical_device_indices(devices)\n    start_time = time.time()\n    while True:\n        output: Dict[int, str] = {}\n        output_raw: Dict[int, float] = {}\n        for device in devices:\n            if is_hip():\n                dev_handle = amdsmi_get_processor_handles()[device]\n                mem_info = amdsmi_get_gpu_vram_usage(dev_handle)\n                gb_used = mem_info[\"vram_used\"] / 2**10\n            else:\n                dev_handle = nvmlDeviceGetHandleByIndex(device)\n                mem_info = nvmlDeviceGetMemoryInfo(dev_handle)\n                gb_used = mem_info.used / 2**30\n            output_raw[device] = gb_used\n            output[device] = f'{gb_used:.02f}'\n\n        print('gpu memory used (GB): ', end='')\n        for k, v in output.items():\n            print(f'{k}={v}; ', end='')\n        print('')\n\n        dur_s = time.time() - start_time\n        if all(v <= (threshold_bytes / 2**30) for v in output_raw.values()):\n            print(f'Done waiting for free GPU memory on devices {devices=} '\n                  f'({threshold_bytes/2**30=}) {dur_s=:.02f}')\n            break\n\n        if dur_s >= timeout_s:\n            raise ValueError(f'Memory of devices {devices=} not free after '\n                             f'{dur_s=:.02f} ({threshold_bytes/2**30=})')\n\n        time.sleep(5)\n\n\n_P = ParamSpec(\"_P\")\n\n\ndef fork_new_process_for_each_test(\n        f: Callable[_P, None]) -> Callable[_P, None]:\n    \"\"\"Decorator to fork a new process for each test function.\n    See https://github.com/vllm-project/vllm/issues/7053 for more details.\n    \"\"\"\n\n    @functools.wraps(f)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> None:\n        # Make the process the leader of its own process group\n        # to avoid sending SIGTERM to the parent process\n        os.setpgrp()\n        from _pytest.outcomes import Skipped\n        pid = os.fork()\n        print(f\"Fork a new process to run a test {pid}\")\n        if pid == 0:\n            try:\n                f(*args, **kwargs)\n            except Skipped as e:\n                # convert Skipped to exit code 0\n                print(str(e))\n                os._exit(0)\n            except Exception:\n                import traceback\n                traceback.print_exc()\n                os._exit(1)\n            else:\n                os._exit(0)\n        else:\n            pgid = os.getpgid(pid)\n            _pid, _exitcode = os.waitpid(pid, 0)\n            # ignore SIGTERM signal itself\n            old_signal_handler = signal.signal(signal.SIGTERM, signal.SIG_IGN)\n            # kill all child processes\n            os.killpg(pgid, signal.SIGTERM)\n            # restore the signal handler\n            signal.signal(signal.SIGTERM, old_signal_handler)\n            assert _exitcode == 0, (f\"function {f} failed when called with\"\n                                    f\" args {args} and kwargs {kwargs}\")\n\n    return wrapper\n\n\ndef large_gpu_test(*, min_gb: int):\n    \"\"\"\n    Decorate a test to be skipped if no GPU is available or it does not have\n    sufficient memory.\n\n    Currently, the CI machine uses L4 GPU which has 24 GB VRAM.\n    \"\"\"\n    try:\n        if current_platform.is_cpu():\n            memory_gb = 0\n        else:\n            memory_gb = current_platform.get_device_total_memory() / GB_bytes\n    except Exception as e:\n        warnings.warn(\n            f\"An error occurred when finding the available memory: {e}\",\n            stacklevel=2,\n        )\n\n        memory_gb = 0\n\n    test_skipif = pytest.mark.skipif(\n        memory_gb < min_gb,\n        reason=f\"Need at least {memory_gb}GB GPU memory to run the test.\",\n    )\n\n    def wrapper(f: Callable[_P, None]) -> Callable[_P, None]:\n        return test_skipif(fork_new_process_for_each_test(f))\n\n    return wrapper\n\n\ndef multi_gpu_test(*, num_gpus: int):\n    \"\"\"\n    Decorate a test to be run only when multiple GPUs are available.\n    \"\"\"\n    test_selector = getattr(pytest.mark, f\"distributed_{num_gpus}_gpus\")\n    test_skipif = pytest.mark.skipif(\n        cuda_device_count_stateless() < num_gpus,\n        reason=f\"Need at least {num_gpus} GPUs to run the test.\",\n    )\n\n    def wrapper(f: Callable[_P, None]) -> Callable[_P, None]:\n        return test_selector(test_skipif(fork_new_process_for_each_test(f)))\n\n    return wrapper\n\n\nasync def completions_with_server_args(\n    prompts: List[str],\n    model_name: str,\n    server_cli_args: List[str],\n    num_logprobs: Optional[int],\n    max_wait_seconds: int = 240,\n    max_tokens: Union[int, list] = 5,\n) -> List[Completion]:\n    '''Construct a remote OpenAI server, obtain an async client to the\n    server & invoke the completions API to obtain completions.\n\n    Args:\n      prompts: test prompts\n      model_name: model to spin up on the vLLM server\n      server_cli_args: CLI args for starting the server\n      num_logprobs: Number of logprobs to report (or `None`)\n      max_wait_seconds: timeout interval for bringing up server.\n                        Default: 240sec\n      max_tokens: max_tokens value for each of the given input prompts.\n        if only one max_token value is given, the same value is used\n        for all the prompts.\n\n    Returns:\n      OpenAI Completion instance\n    '''\n\n    if isinstance(max_tokens, int):\n        max_tokens = [max_tokens] * len(prompts)\n\n    assert len(max_tokens) == len(prompts)\n\n    outputs = None\n    max_wait_seconds = 240 * 3  # 240 is default\n    with RemoteOpenAIServer(model_name,\n                            server_cli_args,\n                            max_wait_seconds=max_wait_seconds) as server:\n        client = server.get_async_client()\n        outputs = [ client.completions.create(model=model_name,\n                                              prompt=[p],\n                                              temperature=0,\n                                              stream=False,\n                                              max_tokens=max_tok,\n                                              logprobs=num_logprobs) \\\n                    for p, max_tok in zip(prompts, max_tokens) ]\n        outputs = await asyncio.gather(*outputs)\n\n    assert outputs is not None, \"Completion API call failed.\"\n\n    return outputs\n\n\ndef get_client_text_generations(completions: List[Completion]) -> List[str]:\n    '''Extract generated tokens from the output of a\n    request made to an Open-AI-protocol completions endpoint.\n    '''\n    assert all([len(x.choices) == 1 for x in completions])\n    return [x.choices[0].text for x in completions]\n\n\ndef get_client_text_logprob_generations(\n        completions: List[Completion]) -> List[TextTextLogprobs]:\n    '''Operates on the output of a request made to an Open-AI-protocol\n    completions endpoint; obtains top-rank logprobs for each token in\n    each :class:`SequenceGroup`\n    '''\n    text_generations = get_client_text_generations(completions)\n    text = ''.join(text_generations)\n    return [(text_generations, text,\n             (None if x.logprobs is None else x.logprobs.top_logprobs))\n            for completion in completions for x in completion.choices]\n\n\ndef check_deprecated_block_manager_usage(test_name: str):\n    assert envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1 is True, (\n        f\"To allow the use of deprecated BlockSpaceManagerV1, set the \"\n        f\"environment variable VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. \"\n        f\"You can run the tests with: \"\n        f\"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest {test_name}`\"  #noqa\n    )\n",
      "diff": "diff --git a/tests/utils.py b/tests/utils.py\nindex 924465057..115cab806 100644\n--- a/tests/utils.py\n+++ b/tests/utils.py\n@@ -678,12 +678,3 @@ def get_client_text_logprob_generations(\n     return [(text_generations, text,\n              (None if x.logprobs is None else x.logprobs.top_logprobs))\n             for completion in completions for x in completion.choices]\n-\n-\n-def check_deprecated_block_manager_usage(test_name: str):\n-    assert envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1 is True, (\n-        f\"To allow the use of deprecated BlockSpaceManagerV1, set the \"\n-        f\"environment variable VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. \"\n-        f\"You can run the tests with: \"\n-        f\"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1 pytest {test_name}`\"  #noqa\n-    )",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 10
    },
    {
      "file_path": "vllm/attention/backends/flash_attn.py",
      "old_content": "\"\"\"Attention layer with FlashAttention.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type\n\nimport torch\n\nfrom vllm import _custom_ops as ops\nfrom vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                              AttentionMetadata,\n                                              AttentionMetadataBuilder,\n                                              AttentionType)\nfrom vllm.attention.backends.utils import (PAD_SLOT_ID, CommonAttentionState,\n                                           compute_slot_mapping,\n                                           compute_slot_mapping_start_idx,\n                                           is_block_tables_empty)\nfrom vllm.forward_context import get_forward_context\nfrom vllm.utils import async_tensor_h2d, make_tensor_with_pad\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import (ModelInputForGPUBuilder,\n                                          ModelInputForGPUWithSamplingMetadata)\n\nfrom vllm.vllm_flash_attn import (flash_attn_varlen_func,\n                                  flash_attn_with_kvcache)\n\n\nclass FlashAttentionBackend(AttentionBackend):\n\n    @staticmethod\n    def get_supported_head_sizes() -> List[int]:\n        return [32, 64, 96, 128, 160, 192, 224, 256]\n\n    @staticmethod\n    def get_name() -> str:\n        return \"flash-attn\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"FlashAttentionImpl\"]:\n        return FlashAttentionImpl\n\n    @staticmethod\n    def get_metadata_cls() -> Type[\"AttentionMetadata\"]:\n        return FlashAttentionMetadata\n\n    @staticmethod\n    def get_builder_cls() -> Type[\"FlashAttentionMetadataBuilder\"]:\n        return FlashAttentionMetadataBuilder\n\n    @staticmethod\n    def get_state_cls() -> Type[\"CommonAttentionState\"]:\n        return CommonAttentionState\n\n    @staticmethod\n    def get_kv_cache_shape(\n        num_blocks: int,\n        block_size: int,\n        num_kv_heads: int,\n        head_size: int,\n    ) -> Tuple[int, ...]:\n        if block_size % 16 != 0:\n            raise ValueError(\"Block size must be a multiple of 16.\")\n        return (2, num_blocks, block_size, num_kv_heads, head_size)\n\n    @staticmethod\n    def swap_blocks(\n        src_kv_cache: torch.Tensor,\n        dst_kv_cache: torch.Tensor,\n        src_to_dst: torch.Tensor,\n    ) -> None:\n        src_key_cache = src_kv_cache[0]\n        dst_key_cache = dst_kv_cache[0]\n        ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)\n\n        src_value_cache = src_kv_cache[1]\n        dst_value_cache = dst_kv_cache[1]\n        ops.swap_blocks(src_value_cache, dst_value_cache, src_to_dst)\n\n    @staticmethod\n    def copy_blocks(\n        kv_caches: List[torch.Tensor],\n        src_to_dists: torch.Tensor,\n    ) -> None:\n        key_caches = [kv_cache[0] for kv_cache in kv_caches]\n        value_caches = [kv_cache[1] for kv_cache in kv_caches]\n        ops.copy_blocks(key_caches, value_caches, src_to_dists)\n\n\n@dataclass\nclass FlashAttentionMetadata(AttentionMetadata):\n    \"\"\"Metadata for FlashAttentionBackend.\n\n    NOTE: Any python object stored here is not updated when it is\n    cuda-graph replayed. If you have values that need to be changed\n    dynamically, it should be stored in tensor. The tensor has to be\n    updated from `CUDAGraphRunner.forward` API.\n    \"\"\"\n    # (batch_size,). The sequence length per sequence. Sequence length means\n    # the computed tokens + new tokens None if it is a decoding.\n    seq_lens: Optional[List[int]]\n    # seq_lens stored as a tensor.\n    seq_lens_tensor: Optional[torch.Tensor]\n\n    # NOTE(sang): Definition of context_len, query_len, and seq_len.\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ---------------------|\n    #                                   |-- query_len ---|\n\n    # Maximum query length in the batch.\n    max_query_len: Optional[int]\n\n    # Max number of query tokens among request in the batch.\n    max_decode_query_len: Optional[int]\n\n    # Maximum sequence length among prefill batch. 0 if there are decoding\n    # requests only.\n    max_prefill_seq_len: int\n    # Maximum sequence length among decode batch. 0 if there are prefill\n    # requests only.\n    max_decode_seq_len: int\n    # (batch_size + 1,). The cumulative subquery lengths of the sequences in\n    # the batch, used to index into subquery. E.g., if the subquery length\n    # is [4, 6], it is [0, 4, 10].\n    query_start_loc: Optional[torch.Tensor]\n    # (batch_size + 1,). The cumulative sequence lengths of the sequences in\n    # the batch, used to index into sequence. E.g., if the sequence length is\n    # [4, 6], it is [0, 4, 10].\n    seq_start_loc: Optional[torch.Tensor]\n    # (batch_size,) A tensor of context lengths (tokens that are computed\n    # so far).\n    context_lens_tensor: Optional[torch.Tensor]\n\n    # (batch_size, max_blocks_per_seq).\n    # Block addresses per sequence. (Seq id -> list of physical block)\n    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks\n    # in the kv cache. Each block can contain up to block_size tokens.\n    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph\n    # captured.\n    block_tables: Optional[torch.Tensor]\n\n    # Whether or not if cuda graph is enabled.\n    # Cuda-graph is currently enabled for decoding only.\n    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.\n    use_cuda_graph: bool\n\n    _cached_prefill_metadata: Optional[\"FlashAttentionMetadata\"] = None\n    _cached_decode_metadata: Optional[\"FlashAttentionMetadata\"] = None\n\n    @property\n    def prefill_metadata(self) -> Optional[\"FlashAttentionMetadata\"]:\n        if self.num_prefills == 0:\n            return None\n\n        if self._cached_prefill_metadata is not None:\n            return self._cached_prefill_metadata\n\n        assert self.seq_lens is not None\n        assert self.seq_lens_tensor is not None\n        assert self.query_start_loc is not None\n        assert self.context_lens_tensor is not None\n        assert self.block_tables is not None\n        assert self.seq_start_loc is not None\n\n        self._cached_prefill_metadata = FlashAttentionMetadata(\n            num_prefills=self.num_prefills,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=0,\n            slot_mapping=self.slot_mapping[:self.num_prefill_tokens],\n            seq_lens=self.seq_lens[:self.num_prefills],\n            seq_lens_tensor=self.seq_lens_tensor[:self.num_prefills],\n            max_query_len=self.max_query_len,\n            max_prefill_seq_len=self.max_prefill_seq_len,\n            max_decode_query_len=0,\n            max_decode_seq_len=0,\n            query_start_loc=self.query_start_loc[:self.num_prefills + 1],\n            seq_start_loc=self.seq_start_loc[:self.num_prefills + 1],\n            context_lens_tensor=self.context_lens_tensor[:self.num_prefills],\n            block_tables=self.block_tables[:self.num_prefills],\n            use_cuda_graph=False,\n        )\n        return self._cached_prefill_metadata\n\n    @property\n    def decode_metadata(self) -> Optional[\"FlashAttentionMetadata\"]:\n        if self.num_decode_tokens == 0:\n            return None\n\n        if self._cached_decode_metadata is not None:\n            return self._cached_decode_metadata\n        assert self.block_tables is not None\n        assert self.seq_lens_tensor is not None\n\n        self._cached_decode_metadata = FlashAttentionMetadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=self.num_decode_tokens,\n            slot_mapping=self.slot_mapping[self.num_prefill_tokens:],\n            seq_lens=None,\n            seq_lens_tensor=self.seq_lens_tensor[self.num_prefills:],\n            max_decode_query_len=self.max_decode_query_len,\n            max_query_len=self.max_query_len,\n            max_prefill_seq_len=0,\n            max_decode_seq_len=self.max_decode_seq_len,\n            query_start_loc=self.query_start_loc[self.num_prefills:]\n            if self.query_start_loc is not None else None,\n            seq_start_loc=self.seq_start_loc[self.num_prefills:]\n            if self.seq_start_loc is not None else None,\n            context_lens_tensor=None,\n            block_tables=self.block_tables[self.num_prefills:],\n            use_cuda_graph=self.use_cuda_graph,\n        )\n        return self._cached_decode_metadata\n\n    def advance_step(self,\n                     model_input: \"ModelInputForGPUWithSamplingMetadata\",\n                     sampled_token_ids: Optional[torch.Tensor],\n                     block_size: int,\n                     num_seqs: int,\n                     num_queries: int,\n                     turn_prefills_into_decodes: bool = False):\n        \"\"\"\n        Update metadata in-place to advance one decode step.\n        \"\"\"\n        # When using cudagraph, the num_seqs is padded to the next captured\n        # batch sized, but num_queries tracks the actual number of requests in\n        # the batch. For --enforce-eager mode, num_seqs == num_queries\n        if num_seqs != num_queries:\n            assert num_seqs > num_queries\n            assert self.use_cuda_graph\n\n        if turn_prefills_into_decodes:\n            # When Mutli-Step is enabled with Chunked-Prefill, prefills and\n            # decodes are scheduled together. In the first step, all the\n            # prefills turn into decodes. This update reflects that\n            # conversion.\n            assert self.num_decode_tokens + self.num_prefills == num_seqs\n            self.num_decode_tokens += self.num_prefills\n            self.num_prefills = 0\n            self.num_prefill_tokens = 0\n            self.max_prefill_seq_len = 0\n            self.max_query_len = 1\n\n            self.slot_mapping = self.slot_mapping[:num_seqs]\n        else:\n            assert self.seq_lens is not None\n            assert self.max_decode_seq_len == max(self.seq_lens)\n\n        assert self.num_prefills == 0\n        assert self.num_prefill_tokens == 0\n        assert self.num_decode_tokens == num_seqs\n        assert self.slot_mapping.shape == (num_seqs, )\n\n        assert self.seq_lens is not None\n        assert len(self.seq_lens) == num_seqs\n        assert self.seq_lens_tensor is not None\n        assert self.seq_lens_tensor.shape == (num_seqs, )\n        assert self.max_query_len == 1\n        assert self.max_prefill_seq_len == 0\n\n        assert self.query_start_loc is not None\n        assert self.query_start_loc.shape == (num_queries + 1, )\n        assert self.seq_start_loc is not None\n        assert self.seq_start_loc.shape == (num_seqs + 1, )\n\n        assert self.context_lens_tensor is not None\n        assert self.context_lens_tensor.shape == (num_queries, )\n\n        assert self.block_tables is not None\n        assert self.block_tables.shape[0] == num_seqs\n\n        # Update query lengths. Note that we update only queries and not seqs,\n        # since tensors may be padded due to captured cuda graph batch size\n        for i in range(num_queries):\n            self.seq_lens[i] += 1\n        self.max_decode_seq_len = max(self.seq_lens)\n\n        ops.advance_step_flashattn(num_seqs=num_seqs,\n                                   num_queries=num_queries,\n                                   block_size=block_size,\n                                   input_tokens=model_input.input_tokens,\n                                   sampled_token_ids=sampled_token_ids,\n                                   input_positions=model_input.input_positions,\n                                   seq_lens=self.seq_lens_tensor,\n                                   slot_mapping=self.slot_mapping,\n                                   block_tables=self.block_tables)\n\n\nclass FlashAttentionMetadataBuilder(\n        AttentionMetadataBuilder[FlashAttentionMetadata]):\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n        self.has_prefix_cache_hit = False\n\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n        self.use_v2_block_manager = (\n            input_builder.scheduler_config.use_v2_block_manager)\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool, prefix_cache_hit: bool):\n        \"\"\"Add a sequence group to the metadata. Specifically update/append\n        1. context length.\n        2. block table.\n        3. slot mapping.\n        \"\"\"\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks):\n            self.context_lens.append(context_len)\n\n            if is_prompt:\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if prefix_cache_hit:\n                # NOTE(woosuk): For flash-attn, the block table should\n                # include the entries for the incoming prefill tokens.\n                block_table = block_tables[seq_id]\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                if curr_sliding_window_block == 0:\n                    block_table = block_tables[seq_id]\n                else:\n                    block_table = block_tables[seq_id][\n                        -curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            # Compute slot mapping.\n            is_profile_run = is_block_tables_empty(block_tables)\n            start_idx = compute_slot_mapping_start_idx(\n                is_prompt, query_len, context_len, self.sliding_window,\n                self.use_v2_block_manager)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n    def _get_graph_runner_block_tables(\n            self, num_seqs: int,\n            block_tables: List[List[int]]) -> torch.Tensor:\n        # The shape of graph_block_tables is\n        # [max batch size, max context len // block size].\n        max_batch_size, max_blocks = self.runner.graph_block_tables.shape\n        assert max_batch_size >= num_seqs\n\n        graph_block_tables = self.runner.graph_block_tables[:num_seqs]\n        for i, block_table in enumerate(block_tables):\n            if block_table:\n                num_blocks = len(block_table)\n                if num_blocks <= max_blocks:\n                    graph_block_tables[i, :num_blocks] = block_table\n                else:\n                    # It may be possible to have more blocks allocated due\n                    # to lookahead slots of multi-step, however, they are\n                    # not used anyway, so can be safely ignored.\n                    graph_block_tables[\n                        i, :max_blocks] = block_table[:max_blocks]\n\n        return torch.from_numpy(graph_block_tables).to(\n            device=self.runner.device, non_blocking=True)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        prefix_cache_hit = any([\n            inter_data.prefix_cache_hit\n            for inter_data in self.input_builder.inter_data_list\n        ])\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled,\n                                prefix_cache_hit)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_query_len = max(query_lens)\n        decode_query_lens = query_lens[self.num_prefills:]\n        if len(decode_query_lens) > 0:\n            max_decode_query_len = max(decode_query_lens)\n        else:\n            max_decode_query_len = 1\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        max_decode_seq_len = max(self.curr_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n\n        num_seqs = len(seq_lens)\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size - self.num_prefill_tokens\n            block_tables = self._get_graph_runner_block_tables(\n                num_seqs, self.block_tables)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n        assert max_query_len > 0, (\"query_lens: {}\".format(query_lens))\n\n        assert device is not None\n        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,\n                                               device, self.runner.pin_memory)\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,\n                                             self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=device)\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        return FlashAttentionMetadata(\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            seq_lens=seq_lens,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=max_query_len,\n            max_decode_query_len=max_decode_query_len,\n            max_prefill_seq_len=max_prefill_seq_len,\n            max_decode_seq_len=max_decode_seq_len,\n            query_start_loc=query_start_loc,\n            seq_start_loc=seq_start_loc,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n        )\n\n\nclass FlashAttentionImpl(AttentionImpl):\n    \"\"\"\n    If the input tensors contain prompt tokens, the layout is as follows:\n    |<--------------- num_prefill_tokens ----------------->|\t\n    |<--prefill_0-->|<--prefill_1-->|...|<--prefill_N-1--->|\n\n    Otherwise, the layout is as follows:\t\n    |<----------------- num_decode_tokens ------------------>|\t\n    |<--decode_0-->|..........|<--decode_M-1-->|<--padding-->|\n\n    Generation tokens can contain padding when cuda-graph is used.\n    Currently, prompt tokens don't contain any padding.\n\n    The prompts might have different lengths, while the generation tokens\n    always have length 1.\n\n    If chunked prefill is enabled, prefill tokens and decode tokens can be\n    batched together in a flattened 1D query.\n\n    |<----- num_prefill_tokens ---->|<------- num_decode_tokens --------->|\n    |<-prefill_0->|...|<-prefill_N-1->|<--decode_0-->|...|<--decode_M-1-->|\n\n    Currently, cuda graph is disabled for chunked prefill, meaning there's no\n    padding between prefill and decode tokens.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n        logits_soft_cap: Optional[float] = None,\n    ) -> None:\n        if blocksparse_params is not None:\n            raise ValueError(\n                \"FlashAttention does not support block-sparse attention.\")\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        if alibi_slopes is not None:\n            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n        self.alibi_slopes = alibi_slopes\n        self.sliding_window = ((sliding_window, sliding_window)\n                               if sliding_window is not None else (-1, -1))\n        self.kv_cache_dtype = kv_cache_dtype\n        if logits_soft_cap is None:\n            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.\n            logits_soft_cap = 0\n        self.logits_soft_cap = logits_soft_cap\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        if sliding_window is not None:\n            # NOTE(woosuk): flash-attn's sliding window does not work with\n            # paged KV cache.\n            raise ValueError(\n                \"Sliding window is not supported in FlashAttention.\")\n\n        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()\n        if head_size not in support_head_sizes:\n            raise ValueError(\n                f\"Head size {head_size} is not supported by FlashAttention. \"\n                f\"Supported head sizes are: {support_head_sizes}.\")\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: FlashAttentionMetadata,\n        k_scale: float = 1.0,\n        v_scale: float = 1.0,\n        attn_type: AttentionType = AttentionType.DECODER,\n    ) -> torch.Tensor:\n        \"\"\"Forward pass with FlashAttention.\n\n        Args:\n            query: shape = [num_tokens, num_heads * head_size]\n            key: shape = [num_tokens, num_kv_heads * head_size]\n            value: shape = [num_tokens, num_kv_heads * head_size]\n            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]\n                NOTE: kv_cache will be an empty tensor with shape [0]\n                for profiling run.\n            attn_metadata: Metadata for attention.\n        Returns:\n            shape = [num_tokens, num_heads * head_size]\n        \"\"\"\n        if attn_type != AttentionType.DECODER:\n            raise NotImplementedError(\"Encoder self-attention and \"\n                                      \"encoder/decoder cross-attention \"\n                                      \"are not implemented for \"\n                                      \"FlashAttentionImpl\")\n\n        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.\n        assert k_scale == 1.0 and v_scale == 1.0, (\n            \"key/v_scale is not supported in FlashAttention.\")\n\n        output = torch.ops.vllm.unified_flash_attention(\n            query,\n            key,\n            value,\n            self.num_heads,\n            self.head_size,\n            self.num_kv_heads,\n            kv_cache,\n            self.kv_cache_dtype,\n            k_scale,\n            v_scale,\n            self.scale,\n            self.sliding_window,\n            self.alibi_slopes,\n            self.logits_soft_cap,\n        )\n\n        return output\n\n\n@torch.library.custom_op(\"vllm::unified_flash_attention\",\n                         mutates_args=[\"kv_cache\"])\ndef unified_flash_attention(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> torch.Tensor:\n\n    current_metadata = get_forward_context()\n    assert current_metadata is not None\n    assert isinstance(current_metadata, FlashAttentionMetadata)\n    attn_metadata: FlashAttentionMetadata = current_metadata\n\n    num_tokens, hidden_size = query.shape\n    # Reshape the query, key, and value tensors.\n    query = query.view(-1, num_heads, head_size)\n    key = key.view(-1, num_kv_heads, head_size)\n    value = value.view(-1, num_kv_heads, head_size)\n\n    if kv_cache.numel() > 0:\n        key_cache = kv_cache[0]\n        value_cache = kv_cache[1]\n\n        # Reshape the input keys and values and store them in the cache.\n        # If kv_cache is not provided, the new key and value tensors are\n        # not cached. This happens during the initial memory profiling run.\n        torch.ops._C_cache_ops.reshape_and_cache_flash(\n            key,\n            value,\n            kv_cache[0],\n            kv_cache[1],\n            attn_metadata.slot_mapping.flatten(),\n            kv_cache_dtype,\n            k_scale,\n            v_scale,\n        )\n\n    num_prefill_tokens = attn_metadata.num_prefill_tokens\n    num_decode_tokens = attn_metadata.num_decode_tokens\n    assert key.shape[0] == num_prefill_tokens + num_decode_tokens, \\\n                f\"key : {key.shape} : #prefill tokens {num_prefill_tokens} : #decode tokens {num_decode_tokens}\" # noqa\n    assert value.shape[0] == num_prefill_tokens + num_decode_tokens, \\\n                f\"value : {value.shape} : #prefill toks {num_prefill_tokens} : #decode toks {num_decode_tokens}\" # noqa\n\n    # Query for decode. KV is not needed because it is already cached.\n    decode_query = query[num_prefill_tokens:]\n    # QKV for prefill.\n    query = query[:num_prefill_tokens]\n    key = key[:num_prefill_tokens]\n    value = value[:num_prefill_tokens]\n\n    assert query.shape[0] == num_prefill_tokens\n    assert decode_query.shape[0] == num_decode_tokens\n\n    prefill_output: Optional[torch.Tensor] = None\n    decode_output: Optional[torch.Tensor] = None\n\n    if prefill_meta := attn_metadata.prefill_metadata:\n        # Prompt run.\n        if (kv_cache.numel() == 0 or prefill_meta.block_tables is None\n                or prefill_meta.block_tables.numel() == 0):\n            # normal attention\n            # When block_tables are not filled, it means q and k are the\n            # prompt, and they have the same length.\n            prefill_output = flash_attn_varlen_func(\n                q=query,\n                k=key,\n                v=value,\n                cu_seqlens_q=prefill_meta.seq_start_loc,\n                cu_seqlens_k=prefill_meta.seq_start_loc,\n                max_seqlen_q=prefill_meta.max_prefill_seq_len,\n                max_seqlen_k=prefill_meta.max_prefill_seq_len,\n                softmax_scale=softmax_scale,\n                causal=True,\n                window_size=window_size,\n                alibi_slopes=alibi_slopes,\n                softcap=logits_soft_cap,\n            )\n        else:\n            # prefix-enabled attention\n            assert prefill_meta.seq_lens is not None\n            max_seq_len = max(prefill_meta.seq_lens)\n            prefill_output = flash_attn_varlen_func(  # noqa\n                q=query,\n                k=key_cache,\n                v=value_cache,\n                cu_seqlens_q=prefill_meta.query_start_loc,\n                max_seqlen_q=prefill_meta.max_query_len,\n                cu_seqlens_k=prefill_meta.seq_start_loc,\n                max_seqlen_k=max_seq_len,\n                softmax_scale=softmax_scale,\n                causal=True,\n                alibi_slopes=alibi_slopes,\n                block_table=prefill_meta.block_tables,\n                softcap=logits_soft_cap,\n            )\n\n    if decode_meta := attn_metadata.decode_metadata:\n        # Decoding run.\n        # Use flash_attn_varlen_func kernel for speculative decoding\n        # because different queries might have different lengths.\n        assert decode_meta.max_decode_query_len is not None\n        if decode_meta.max_decode_query_len > 1:\n            decode_output = flash_attn_varlen_func(\n                q=decode_query,\n                k=key_cache,\n                v=value_cache,\n                cu_seqlens_q=decode_meta.query_start_loc,\n                max_seqlen_q=decode_meta.max_decode_query_len,\n                cu_seqlens_k=decode_meta.seq_start_loc,\n                max_seqlen_k=decode_meta.max_decode_seq_len,\n                softmax_scale=softmax_scale,\n                causal=True,\n                alibi_slopes=alibi_slopes,\n                softcap=logits_soft_cap,\n                block_table=decode_meta.block_tables,\n            )\n        else:\n            # Use flash_attn_with_kvcache for normal decoding.\n            decode_output = flash_attn_with_kvcache(\n                q=decode_query.unsqueeze(1),\n                k_cache=key_cache,\n                v_cache=value_cache,\n                block_table=decode_meta.block_tables,\n                cache_seqlens=decode_meta.seq_lens_tensor,\n                softmax_scale=softmax_scale,\n                causal=True,\n                alibi_slopes=alibi_slopes,\n                softcap=logits_soft_cap,\n            ).squeeze(1)\n\n    if prefill_output is None:\n        assert decode_output is not None\n        return decode_output.view(num_decode_tokens, hidden_size)\n    if decode_output is None:\n        assert prefill_output is not None\n        return prefill_output.view(num_prefill_tokens, hidden_size)\n\n    # Chunked prefill does not work with speculative decoding.\n    # Therefore, the query length for decode should be 1 in chunked prefill.\n    assert decode_meta is not None\n    decode_output = decode_output.squeeze(1)\n    output = torch.cat([prefill_output, decode_output], dim=0)\n    return output.view(num_tokens, hidden_size)\n\n\n@unified_flash_attention.register_fake\ndef _(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> torch.Tensor:\n    return torch.empty_like(query)\n",
      "diff": "diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py\nindex 8457bde06..d54dbdcb1 100644\n--- a/vllm/attention/backends/flash_attn.py\n+++ b/vllm/attention/backends/flash_attn.py\n@@ -305,8 +305,6 @@ class FlashAttentionMetadataBuilder(\n         self.runner = input_builder.runner\n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n     def _add_seq_group(\n             self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n@@ -355,9 +353,9 @@ class FlashAttentionMetadataBuilder(\n \n             # Compute slot mapping.\n             is_profile_run = is_block_tables_empty(block_tables)\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/attention/backends/flashinfer.py",
      "old_content": "from contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Type\n\ntry:\n    from flashinfer import BatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper\n\n    from vllm.vllm_flash_attn import flash_attn_varlen_func\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024\nexcept ImportError:\n    BatchDecodeWithPagedKVCacheWrapper = None\n    CUDAGraphBatchDecodeWithPagedKVCacheWrapper = None\n    BatchPrefillWithPagedKVCacheWrapper = None\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 0\n\nimport torch\n\nfrom vllm import _custom_ops as ops\nfrom vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                              AttentionMetadata,\n                                              AttentionMetadataBuilder,\n                                              AttentionState, AttentionType)\nfrom vllm.attention.backends.utils import (PAD_SLOT_ID, compute_slot_mapping,\n                                           compute_slot_mapping_start_idx,\n                                           is_block_tables_empty)\nfrom vllm.attention.ops.paged_attn import PagedAttention\nfrom vllm.forward_context import get_forward_context\nfrom vllm.utils import (async_tensor_h2d, get_kv_cache_torch_dtype,\n                        make_tensor_with_pad)\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import (ModelInputForGPUBuilder,\n                                          ModelInputForGPUWithSamplingMetadata)\n\n\nclass FlashInferBackend(AttentionBackend):\n\n    @staticmethod\n    def get_name() -> str:\n        return \"flashinfer\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"FlashInferImpl\"]:\n        return FlashInferImpl\n\n    @staticmethod\n    def get_metadata_cls() -> Type[\"AttentionMetadata\"]:\n        return FlashInferMetadata\n\n    @staticmethod\n    def get_builder_cls() -> Type[\"FlashInferMetadataBuilder\"]:\n        return FlashInferMetadataBuilder\n\n    @staticmethod\n    def get_state_cls() -> Type[\"FlashInferState\"]:\n        return FlashInferState\n\n    @staticmethod\n    def get_kv_cache_shape(\n        num_blocks: int,\n        block_size: int,\n        num_kv_heads: int,\n        head_size: int,\n    ) -> Tuple[int, ...]:\n        return (num_blocks, 2, block_size, num_kv_heads, head_size)\n\n    @staticmethod\n    def swap_blocks(\n        src_kv_cache: torch.Tensor,\n        dst_kv_cache: torch.Tensor,\n        src_to_dst: torch.Tensor,\n    ) -> None:\n        PagedAttention.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)\n\n    @staticmethod\n    def copy_blocks(\n        kv_caches: List[torch.Tensor],\n        src_to_dists: torch.Tensor,\n    ) -> None:\n        PagedAttention.copy_blocks(kv_caches, src_to_dists)\n\n    @staticmethod\n    def get_supported_head_sizes() -> List[int]:\n        return [64, 128, 256]\n\n    @staticmethod\n    def get_fp8_dtype_for_flashinfer(kv_cache_dtype: str) -> torch.dtype:\n        if kv_cache_dtype in (\"fp8\", \"fp8_e4m3\"):\n            return torch.float8_e4m3fn\n        elif kv_cache_dtype == \"fp8_e5m2\":\n            return torch.float8_e5m2\n        else:\n            raise ValueError(f\"Unrecognized FP8 dtype: {kv_cache_dtype}\")\n\n\nclass FlashInferState(AttentionState):\n\n    def __init__(self, runner):\n        self.runner = runner\n        self._is_graph_capturing = False\n        self._workspace_buffer = None\n        self._decode_wrapper = None\n        self._prefill_wrapper = None\n\n    def _get_workspace_buffer(self):\n        if self._workspace_buffer is None:\n            self._workspace_buffer = torch.empty(\n                FLASHINFER_WORKSPACE_BUFFER_SIZE,\n                dtype=torch.uint8,\n                device=self.runner.device)\n        return self._workspace_buffer\n\n    def _get_prefill_wrapper(self):\n        if self._prefill_wrapper is None:\n            self._prefill_wrapper = BatchPrefillWithPagedKVCacheWrapper(\n                self._get_workspace_buffer(), \"NHD\")\n        return self._prefill_wrapper\n\n    def _get_decode_wrapper(self):\n        if self._decode_wrapper is None:\n            num_qo_heads = (self.runner.model_config.get_num_attention_heads(\n                self.runner.parallel_config))\n            num_kv_heads = self.runner.model_config.get_num_kv_heads(\n                self.runner.parallel_config)\n            use_tensor_cores = num_qo_heads // num_kv_heads > 4\n            self._decode_wrapper = BatchDecodeWithPagedKVCacheWrapper(\n                self._get_workspace_buffer(),\n                \"NHD\",\n                use_tensor_cores=use_tensor_cores)\n        return self._decode_wrapper\n\n    @contextmanager\n    def graph_capture(self, max_batch_size: int):\n        self._is_graph_capturing = True\n        self._graph_decode_wrapper = None\n        self._graph_slot_mapping = torch.full((max_batch_size, ),\n                                              PAD_SLOT_ID,\n                                              dtype=torch.long,\n                                              device=self.runner.device)\n        self._graph_seq_lens = torch.ones(max_batch_size,\n                                          dtype=torch.int32,\n                                          device=self.runner.device)\n        self._graph_block_tables = torch.from_numpy(\n            self.runner.graph_block_tables).to(device=self.runner.device)\n        self._graph_decode_workspace_buffer = self._get_workspace_buffer()\n        self._graph_indices_buffer = torch.empty(\n            max_batch_size * self.runner.cache_config.num_gpu_blocks,\n            dtype=torch.int32,\n            device=self.runner.device)\n        self._graph_indptr_buffer = torch.empty(max_batch_size + 1,\n                                                dtype=torch.int32,\n                                                device=self.runner.device)\n        self._graph_last_page_len_buffer = torch.empty(\n            max_batch_size, dtype=torch.int32, device=self.runner.device)\n        yield\n        self._is_graph_capturing = False\n        del self._graph_slot_mapping\n        del self._graph_seq_lens\n        del self._graph_block_tables\n        del self._graph_decode_workspace_buffer\n        del self._graph_indices_buffer\n        del self._graph_indptr_buffer\n        del self._graph_last_page_len_buffer\n        del self._graph_decode_wrapper\n\n    def graph_clone(self, batch_size: int):\n        assert self._is_graph_capturing\n        state = self.__class__(self.runner)\n        state._workspace_buffer = self._graph_decode_workspace_buffer\n        state._decode_wrapper = self._graph_decode_wrapper\n        state._prefill_wrapper = self._get_prefill_wrapper()\n        return state\n\n    def graph_capture_get_metadata_for_batch(\n            self, batch_size: int, is_encoder_decoder_model: bool = False):\n        assert self._is_graph_capturing\n        _indptr_buffer = self._graph_indptr_buffer[:batch_size + 1]\n        _last_page_len_buffer = self._graph_last_page_len_buffer[:batch_size]\n\n        num_qo_heads = (self.runner.model_config.get_num_attention_heads(\n            self.runner.parallel_config))\n        num_kv_heads = self.runner.model_config.get_num_kv_heads(\n            self.runner.parallel_config)\n        use_tensor_cores = num_qo_heads // num_kv_heads > 4\n        self._graph_decode_wrapper = \\\n            CUDAGraphBatchDecodeWithPagedKVCacheWrapper(\n            self._graph_decode_workspace_buffer, _indptr_buffer,\n            self._graph_indices_buffer, _last_page_len_buffer, \"NHD\",\n            use_tensor_cores)\n        if self.runner.kv_cache_dtype.startswith(\"fp8\"):\n            kv_cache_dtype = FlashInferBackend.get_fp8_dtype_for_flashinfer(\n                self.runner.kv_cache_dtype)\n        else:\n            kv_cache_dtype = get_kv_cache_torch_dtype(\n                self.runner.kv_cache_dtype, self.runner.model_config.dtype)\n\n        paged_kv_indptr_tensor_host = torch.arange(0,\n                                                   batch_size + 1,\n                                                   dtype=torch.int32)\n        paged_kv_indices_tensor_host = torch.arange(0,\n                                                    batch_size,\n                                                    dtype=torch.int32)\n        paged_kv_last_page_len_tensor_host = torch.full((batch_size, ),\n                                                        self.runner.block_size,\n                                                        dtype=torch.int32)\n        query_start_loc_host = torch.arange(0,\n                                            batch_size + 1,\n                                            dtype=torch.int32)\n\n        attn_metadata = self.runner.attn_backend.make_metadata(\n            num_prefills=0,\n            slot_mapping=self._graph_slot_mapping[:batch_size],\n            num_prefill_tokens=0,\n            num_decode_tokens=batch_size,\n            max_prefill_seq_len=0,\n            block_tables=self._graph_block_tables,\n            paged_kv_indptr=paged_kv_indptr_tensor_host,\n            paged_kv_indices=paged_kv_indices_tensor_host,\n            paged_kv_last_page_len=paged_kv_last_page_len_tensor_host,\n            num_qo_heads=num_qo_heads,\n            num_kv_heads=num_kv_heads,\n            head_dim=self.runner.model_config.get_head_size(),\n            page_size=self.runner.block_size,\n            seq_start_loc=None,\n            query_start_loc=query_start_loc_host,\n            device=self.runner.device,\n            data_type=kv_cache_dtype,\n            q_data_type=self.runner.model_config.dtype,\n            use_cuda_graph=True,\n            decode_wrapper=self._graph_decode_wrapper,\n            prefill_wrapper=None)\n        attn_metadata.begin_forward()\n        return attn_metadata\n\n    def get_graph_input_buffers(self,\n                                attn_metadata,\n                                is_encoder_decoder_model: bool = False):\n        return {\n            \"slot_mapping\": attn_metadata.slot_mapping,\n        }\n\n    def prepare_graph_input_buffers(self,\n                                    input_buffers,\n                                    attn_metadata,\n                                    is_encoder_decoder_model: bool = False):\n        return\n\n    def begin_forward(self, model_input):\n        assert not self._is_graph_capturing\n        state = self\n        if model_input.attn_metadata.use_cuda_graph:\n            batch_size = model_input.input_tokens.shape[0]\n            state = (self.runner.graph_runners[model_input.virtual_engine]\n                     [batch_size].attn_state)\n        model_input.attn_metadata.prefill_wrapper = state._get_prefill_wrapper(\n        )\n        model_input.attn_metadata.decode_wrapper = state._get_decode_wrapper()\n        model_input.attn_metadata.begin_forward()\n\n\n@dataclass\nclass FlashInferMetadata(AttentionMetadata):\n    # Maximum sequence length among prefill batch. 0 if there are decoding\n    # requests only.\n    max_prefill_seq_len: int\n\n    use_cuda_graph: bool = True\n\n    prefill_wrapper: Optional[BatchPrefillWithPagedKVCacheWrapper] = None\n    decode_wrapper: Optional[BatchDecodeWithPagedKVCacheWrapper] = None\n\n    # Metadata for the prefill stage\n    seq_start_loc: Optional[torch.Tensor] = None\n    query_start_loc: Optional[torch.Tensor] = None\n    block_tables: Optional[torch.Tensor] = None\n\n    # used for GPU in-place advance_step\n    seq_lens_tensor: Optional[torch.Tensor] = None\n    block_table_bound: Optional[torch.Tensor] = None\n\n    # An example for paged_kv_indices, paged_kv_indptr:\n    # request 1, page indices [0, 5, 8]\n    # request 2, page indices [1, 6, 7]\n    # request 3, page indices [3, 4]\n    # paged_kv_indices is a concatenation of page indices of all requests:\n    # [0, 5, 8, 1, 6, 7, 3, 4]\n    # paged_kv_indptr is used to index into paged_kv_indices:\n    # [0, 3, 6, 8]\n    # The indptr of the paged kv cache, shape: [batch_size + 1]\n    paged_kv_indptr: Optional[torch.Tensor] = None\n    # The page indices of the paged kv cache\n    paged_kv_indices: Optional[torch.Tensor] = None\n    # The number of entries in the last page of each request in\n    # the paged kv cache, shape: [batch_size]\n    paged_kv_last_page_len: Optional[torch.Tensor] = None\n    # The number of query/output heads\n    num_qo_heads: Optional[int] = None\n    # The number of key/value heads\n    num_kv_heads: Optional[int] = None\n    # The dimension of the attention heads\n    head_dim: Optional[int] = None\n    # Block size of vllm\n    page_size: Optional[int] = None\n    # The data type of the paged kv cache\n    data_type: torch.dtype = None\n    # The data type of the query\n    q_data_type: torch.dtype = None\n    device: torch.device = torch.device(\"cuda\")\n    is_profile_run: bool = False\n\n    def __post_init__(self):\n        # Refer to\n        # https://github.com/flashinfer-ai/flashinfer/blob/3d55c71a62052c590c130897d3a3db49b14fcc34/include/flashinfer/utils.cuh#L157\n        supported_head_sizes = FlashInferBackend.get_supported_head_sizes()\n        if self.head_dim is not None and self.head_dim \\\n                not in supported_head_sizes:\n            raise ValueError(\n                f\"Only {supported_head_sizes} are supported for head_dim,\",\n                f\"received {self.head_dim}.\")\n\n    def begin_forward(self):\n        if self.num_prefill_tokens > 0:\n            if self.paged_kv_indices is None:\n                return\n\n            assert self.prefill_wrapper is not None\n            assert self.query_start_loc is not None\n            assert self.paged_kv_indices is not None\n            assert self.paged_kv_indptr is not None\n            assert self.paged_kv_last_page_len is not None\n            assert self.block_table_bound is not None\n            assert self.seq_lens_tensor is not None\n            batch_size = self.query_start_loc.shape[0] - 1\n            assert batch_size >= 0\n            # We will use flash attention for profiling to\n            # determine the number of blocks. Therefore,\n            # we don't need to prepare the input for flashinfer for profile run.\n            if not self.is_profile_run:\n                self.paged_kv_indptr = self.paged_kv_indptr.to(self.device)\n                self.paged_kv_last_page_len = self.paged_kv_last_page_len.to(\n                    self.device)\n                self.block_table_bound = self.block_table_bound.to(self.device)\n                self.seq_lens_tensor = self.seq_lens_tensor.to(self.device)\n                self.paged_kv_indices = self.paged_kv_indices.to(self.device)\n                self.prefill_wrapper.end_forward()\n                self.prefill_wrapper.begin_forward(\n                    self.query_start_loc, self.paged_kv_indptr,\n                    self.paged_kv_indices, self.paged_kv_last_page_len,\n                    self.num_qo_heads, self.num_kv_heads, self.head_dim,\n                    self.page_size)\n        else:\n            assert self.paged_kv_indices is not None\n            assert self.paged_kv_indptr is not None\n            assert self.paged_kv_last_page_len is not None\n            self.paged_kv_indices = self.paged_kv_indices.to(self.device)\n            self.paged_kv_indptr = self.paged_kv_indptr.to(self.device)\n            self.paged_kv_last_page_len = self.paged_kv_last_page_len.to(\n                self.device)\n            # handle model warmup path\n            if self.block_table_bound is not None:\n                self.block_table_bound = self.block_table_bound.to(self.device)\n            if self.seq_lens_tensor is not None:\n                self.seq_lens_tensor = self.seq_lens_tensor.to(self.device)\n\n            assert self.decode_wrapper is not None\n            self.decode_wrapper.end_forward()\n            self.decode_wrapper.begin_forward(\n                self.paged_kv_indptr,\n                self.paged_kv_indices,\n                self.paged_kv_last_page_len,\n                self.num_qo_heads,\n                self.num_kv_heads,\n                self.head_dim,\n                self.page_size,\n                # Disable flashinfer's pos encoding and use vllm's rope.\n                pos_encoding_mode=\"NONE\",\n                # kv-cache data type.\n                data_type=self.data_type,\n                # query data type.\n                q_data_type=self.q_data_type)\n\n    def asdict_zerocopy(self,\n                        skip_fields: Optional[Set[str]] = None\n                        ) -> Dict[str, Any]:\n        if skip_fields is None:\n            skip_fields = set()\n        # We need to skip the prefill/decode_wrapper field since it cannot be\n        # broadcasted with nccl when TP is enabled.\n        skip_fields.add('prefill_wrapper')\n        skip_fields.add('decode_wrapper')\n        return super().asdict_zerocopy(skip_fields)\n\n    @property\n    def prefill_metadata(self) -> Optional[\"FlashInferMetadata\"]:\n        # Currently chunked prefill is not supported\n        if self.num_decode_tokens == 0:\n            assert self.num_prefills > 0\n            return self\n\n        return None\n\n    @property\n    def decode_metadata(self) -> Optional[\"FlashInferMetadata\"]:\n        # Currently chunked prefill is not supported\n        if self.num_prefills > 0:\n            assert self.num_decode_tokens == 0, (\n                \"Chunked prefill is not supported with flashinfer yet.\")\n            return None\n\n        return self\n\n    def advance_step(self,\n                     model_input: \"ModelInputForGPUWithSamplingMetadata\",\n                     sampled_token_ids: Optional[torch.Tensor],\n                     block_size: int,\n                     num_seqs: int,\n                     num_queries: int,\n                     turn_prefills_into_decodes: bool = False):\n        \"\"\"\n        Update metadata in-place to advance one decode step.\n        \"\"\"\n\n        assert not turn_prefills_into_decodes, \\\n            (\"Chunked prefill is not supported with flashinfer yet.\"\n             \"turn_prefills_into_decodes is a Multi-Step + Chunked-Prefill \"\n             \"specific parameter.\")\n\n        assert num_seqs > 0\n        assert num_queries > 0\n        assert model_input.attn_metadata is not None\n        assert sampled_token_ids is not None\n\n        # When using cudagraph, the num_seqs is padded to the next captured\n        # batch sized, but num_queries tracks the actual number of requests in\n        # the batch. For --enforce-eager mode, num_seqs == num_queries\n        if num_seqs != num_queries:\n            assert num_seqs > num_queries\n            assert self.use_cuda_graph\n\n        model_input.input_tokens[:num_queries] = sampled_token_ids.flatten()\n\n        # Update GPU tensors\n        ops.advance_step_flashinfer(\n            num_seqs=num_seqs,\n            num_queries=num_queries,\n            block_size=block_size,\n            input_tokens=model_input.input_tokens,\n            sampled_token_ids=model_input.input_tokens,\n            input_positions=model_input.input_positions,\n            seq_lens=self.seq_lens_tensor,\n            slot_mapping=self.slot_mapping,\n            block_tables=self.block_tables,\n            paged_kv_indices=self.paged_kv_indices,\n            paged_kv_indptr=self.paged_kv_indptr,\n            paged_kv_last_page_len=self.paged_kv_last_page_len,\n            block_table_bound=self.block_table_bound)\n\n\nclass FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n        self.use_v2_block_manager = (\n            input_builder.scheduler_config.use_v2_block_manager)\n\n        # Please follow https://docs.flashinfer.ai/tutorials/kv_layout.html#page-layout\n        # for the precise definition of the following fields.\n        # An example:\n        # request 1, page indices [0, 5, 8]\n        # request 2, page indices [1, 6, 7]\n        # request 3, page indices [3, 4]\n        # paged_kv_indices is a concatenation of page indices of all requests:\n        # [0, 5, 8, 1, 6, 7, 3, 4]\n        # paged_kv_indptr is used to index into paged_kv_indices:\n        # [0, 3, 6, 8]\n        self.paged_kv_indices: List[int] = []\n        # 0 at the beginning of paged_kv_indptr indicates the start of the\n        # first requests page indices in the paged_kv_indices list.\n        self.paged_kv_indptr: List[int] = [0]\n        # paged_kv_last_page_len is the length of the last page of each request\n        self.paged_kv_last_page_len: List[int] = []\n        self.total_blocks = 0\n        self.is_profile_run: bool = False\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool):\n        \"\"\"Add a sequence group to the metadata. Specifically update/append\n        1. context length.\n        2. block table.\n        3. slot mapping.\n        \"\"\"\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n        computed_block_nums = inter_data.computed_block_nums\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks):\n            self.context_lens.append(context_len)\n            if is_prompt:\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                assert query_len == 1, (\n                    \"seq_len: {}, context_len: {}, query_len: {}\".format(\n                        seq_len, context_len, query_len))\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if inter_data.prefix_cache_hit:\n                block_table = computed_block_nums\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                block_table = block_tables[seq_id][-curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            is_profile_run = is_block_tables_empty(block_tables)\n\n            # Compute slot mapping.\n            start_idx = compute_slot_mapping_start_idx(\n                is_prompt, query_len, context_len, self.sliding_window,\n                self.use_v2_block_manager)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n            # It is not necessary to add paged_kv_indices, paged_kv_indptr,\n            # and paged_kv_last_page_len for profile run because we will\n            # create dummy inputs.\n            if is_profile_run:\n                self.is_profile_run = is_profile_run\n                return\n\n            block_table = block_tables[seq_id]\n            self._update_paged_kv_tensors(block_table, seq_len)\n\n    def _update_paged_kv_tensors(self, block_table: List[int], seq_len: int):\n        # Get the number of valid blocks based on sequence length.\n        # If seq_len = 16, block_size = 16,\n        # block_table_bound is 1 with 1 valid block.\n        # If seq_len = 15, block_size = 16,\n        # block_table_bound is 0 + 1 with 1 valid block.\n        self.total_blocks += len(block_table)\n        block_table_bound = seq_len // self.block_size + 1 \\\n                            if seq_len % self.block_size != 0 \\\n                            else seq_len // self.block_size\n        self.paged_kv_indices.extend(block_table[:block_table_bound])\n        self.paged_kv_indptr.append(self.paged_kv_indptr[-1] +\n                                    block_table_bound)\n\n        last_page_len = seq_len % self.block_size\n        if last_page_len == 0:\n            last_page_len = self.block_size\n        self.paged_kv_last_page_len.append(last_page_len)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size\n\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.runner.graph_block_tables[:batch_size]\n            max_blocks = input_block_tables.shape[1]\n            for i, block_table in enumerate(self.block_tables):\n                if block_table:\n                    num_blocks = len(block_table)\n                    if num_blocks <= max_blocks:\n                        input_block_tables[i, :num_blocks] = block_table\n                    else:\n                        # It may be possible to have more blocks allocated due\n                        # to lookahead slots of multi-step, however, they are\n                        # not used anyway, so can be safely ignored.\n                        input_block_tables[\n                            i, :max_blocks] = block_table[:max_blocks]\n\n            block_tables = torch.from_numpy(input_block_tables).to(\n                device, non_blocking=True)\n\n            last_paged_kv_indptr = self.paged_kv_indptr[-1]\n            self.paged_kv_indptr.extend([last_paged_kv_indptr] *\n                                        cuda_graph_pad_size)\n            self.paged_kv_last_page_len.extend([0] * cuda_graph_pad_size)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n\n        assert device is not None\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,\n                                             self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=device)\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        if len(self.paged_kv_indptr) > 0:\n            # extend to the maximum number of blocks as returned by the\n            # scheduler\n            self.paged_kv_indices.extend(\n                [0] * (self.total_blocks - len(self.paged_kv_indices)))\n            paged_kv_indices_tensor = torch.tensor(self.paged_kv_indices,\n                                                   device=\"cpu\",\n                                                   dtype=torch.int)\n            paged_kv_indptr_tensor = torch.tensor(self.paged_kv_indptr,\n                                                  device=\"cpu\",\n                                                  dtype=torch.int)\n            paged_kv_last_page_len_tensor = torch.tensor(\n                self.paged_kv_last_page_len, device=\"cpu\", dtype=torch.int)\n            block_table_bound_tensor = torch.zeros(len(self.paged_kv_indptr) -\n                                                   1,\n                                                   device=\"cpu\",\n                                                   dtype=torch.int)\n        else:\n            paged_kv_indices_tensor = None\n            paged_kv_indptr_tensor = None\n            paged_kv_last_page_len_tensor = None\n            block_table_bound_tensor = None\n\n        if self.runner.kv_cache_dtype.startswith(\"fp8\"):\n            kv_cache_dtype = FlashInferBackend.get_fp8_dtype_for_flashinfer(\n                self.runner.kv_cache_dtype)\n        else:\n            kv_cache_dtype = get_kv_cache_torch_dtype(\n                self.runner.kv_cache_dtype, self.runner.model_config.dtype)\n\n        return FlashInferMetadata(\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            max_prefill_seq_len=max_prefill_seq_len,\n            block_tables=block_tables,\n            paged_kv_indptr=paged_kv_indptr_tensor,\n            paged_kv_indices=paged_kv_indices_tensor,\n            paged_kv_last_page_len=paged_kv_last_page_len_tensor,\n            block_table_bound=block_table_bound_tensor,\n            seq_lens_tensor=seq_lens_tensor,\n            num_qo_heads=self.runner.model_config.get_num_attention_heads(\n                self.runner.parallel_config),\n            num_kv_heads=self.runner.model_config.get_num_kv_heads(\n                self.runner.parallel_config),\n            head_dim=self.runner.model_config.get_head_size(),\n            page_size=self.block_size,\n            seq_start_loc=seq_start_loc,\n            query_start_loc=query_start_loc,\n            device=device,\n            data_type=kv_cache_dtype,\n            q_data_type=self.runner.model_config.dtype,\n            use_cuda_graph=use_captured_graph,\n            is_profile_run=self.is_profile_run)\n\n\nclass FlashInferImpl(AttentionImpl):\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n        logits_soft_cap: Optional[float] = None,\n    ) -> None:\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        if alibi_slopes is not None:\n            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n        self.alibi_slopes = alibi_slopes\n        if sliding_window is not None:\n            raise ValueError(\"Sliding window is not supported in FlashInfer.\")\n        self.sliding_window = (-1, -1)\n        self.kv_cache_dtype = kv_cache_dtype\n        self.logits_soft_cap = logits_soft_cap\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: FlashInferMetadata,\n        k_scale: float = 1.0,\n        v_scale: float = 1.0,\n        attn_type: AttentionType = AttentionType.DECODER,\n    ) -> torch.Tensor:\n        assert k_scale == 1.0 and v_scale == 1.0, (\n            \"key/v_scale is not supported in FlashInfer.\")\n        if attn_type != AttentionType.DECODER:\n            raise NotImplementedError(\"Encoder self-attention and \"\n                                      \"encoder/decoder cross-attention \"\n                                      \"are not implemented for \"\n                                      \"FlashInferImpl\")\n\n        return torch.ops.vllm.unified_flash_infer(\n            query,\n            key,\n            value,\n            self.num_heads,\n            self.head_size,\n            self.num_kv_heads,\n            kv_cache,\n            self.kv_cache_dtype,\n            k_scale,\n            v_scale,\n            self.scale,\n            self.sliding_window,\n            self.alibi_slopes,\n            self.logits_soft_cap,\n        )\n\n\n@torch.library.custom_op(\"vllm::unified_flash_infer\",\n                         mutates_args=[\"kv_cache\"])\ndef unified_flash_infer(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> torch.Tensor:\n\n    current_metadata = get_forward_context()\n    assert current_metadata is not None\n    assert isinstance(current_metadata, FlashInferMetadata)\n    attn_metadata: FlashInferMetadata = current_metadata\n\n    num_tokens, hidden_size = query.shape\n    query = query.view(-1, num_heads, head_size)\n    key = key.view(-1, num_kv_heads, head_size)\n    value = value.view(-1, num_kv_heads, head_size)\n\n    if attn_metadata.num_prefill_tokens > 0:\n        assert attn_metadata.num_decode_tokens == 0, (\n            \"Chunked prefill is not supported with flashinfer yet.\")\n    if attn_metadata.num_decode_tokens > 0:\n        assert attn_metadata.num_prefill_tokens == 0, (\n            \"Chunked prefill is not supported with flashinfer yet.\")\n    if kv_cache.numel() > 0:\n        # Use the same reshape and cache kernel as flash attention.\n        ops.reshape_and_cache_flash(\n            key,\n            value,\n            kv_cache[:, 0],\n            kv_cache[:, 1],\n            attn_metadata.slot_mapping.flatten(),\n            kv_cache_dtype,\n            k_scale,\n            v_scale,\n        )\n        # The FlashInfer api requires data to be in fp8_e4m3 or fp8_e5m2\n        # to process the cache when the kv_cache_dtype is fp8\n        if kv_cache_dtype.startswith(\"fp8\"):\n            torch_dtype = FlashInferBackend.get_fp8_dtype_for_flashinfer(\n                kv_cache_dtype)\n            kv_cache = kv_cache.view(torch_dtype)\n\n    query = query.contiguous()  # Flashinfer requires query to be contiguous\n    if prefill_meta := attn_metadata.prefill_metadata:\n        # We will use flash attention for prefill\n        # when kv_cache is not provided.\n        # This happens when vllm runs the profiling to\n        # determine the number of blocks.\n        if kv_cache.numel() == 0:\n            output = flash_attn_varlen_func(\n                q=query,\n                k=key,\n                v=value,\n                cu_seqlens_q=prefill_meta.seq_start_loc,\n                cu_seqlens_k=prefill_meta.seq_start_loc,\n                max_seqlen_q=prefill_meta.max_prefill_seq_len,\n                max_seqlen_k=prefill_meta.max_prefill_seq_len,\n                softmax_scale=softmax_scale,\n                causal=True,\n                window_size=window_size,\n                alibi_slopes=alibi_slopes,\n            )\n        else:\n            assert prefill_meta is not None\n            assert prefill_meta.prefill_wrapper is not None\n            output = prefill_meta.prefill_wrapper.forward(\n                query, kv_cache, logits_soft_cap=logits_soft_cap, causal=True)\n    else:\n        assert attn_metadata.decode_metadata is not None\n        assert attn_metadata.decode_metadata.decode_wrapper is not None\n        output = attn_metadata.decode_metadata.decode_wrapper.forward(\n            query,\n            kv_cache,\n            sm_scale=softmax_scale,\n            logits_soft_cap=logits_soft_cap,\n            k_scale=k_scale,\n            v_scale=v_scale)\n    return output.view(num_tokens, hidden_size)\n\n\n@unified_flash_infer.register_fake\ndef _(\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    num_heads: int,\n    head_size: int,\n    num_kv_heads: int,\n    kv_cache: torch.Tensor,\n    kv_cache_dtype: str,\n    k_scale: float,\n    v_scale: float,\n    softmax_scale: float,\n    window_size: Optional[List[int]] = None,\n    alibi_slopes: Optional[torch.Tensor] = None,\n    logits_soft_cap: Optional[float] = None,\n) -> torch.Tensor:\n    return torch.empty_like(query).contiguous()\n",
      "diff": "diff --git a/vllm/attention/backends/flashinfer.py b/vllm/attention/backends/flashinfer.py\nindex ba9b2d043..dd9a0fb9d 100644\n--- a/vllm/attention/backends/flashinfer.py\n+++ b/vllm/attention/backends/flashinfer.py\n@@ -475,8 +475,6 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n \n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n         # Please follow https://docs.flashinfer.ai/tutorials/kv_layout.html#page-layout\n         # for the precise definition of the following fields.\n@@ -542,9 +540,9 @@ class FlashInferMetadataBuilder(AttentionMetadataBuilder[FlashInferMetadata]):\n             is_profile_run = is_block_tables_empty(block_tables)\n \n             # Compute slot mapping.\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/attention/backends/utils.py",
      "old_content": "\"\"\"Attention backend utils\"\"\"\nfrom contextlib import contextmanager\nfrom typing import TYPE_CHECKING, Any, Dict, List, Type, TypeVar, Union\n\nimport numpy as np\nimport torch\n\nfrom vllm.attention import (AttentionMetadata, AttentionMetadataBuilder,\n                            AttentionState)\nfrom vllm.utils import async_tensor_h2d, make_tensor_with_pad\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner_base import ModelRunnerBase\n\n# Error string(s) for encoder/decoder\n# unsupported attention scenarios\nSTR_NOT_IMPL_ENC_DEC_ROCM_HIP = (\"ROCm/HIP is not currently supported \"\n                                 \"with encoder/decoder models.\")\n\nPAD_SLOT_ID = -1\n\n# Switch to numpy implementation of compute_slot_mapping\n# if we have at least this many elements. Could be tuned further.\n_COMPUTE_SLOT_MAPPING_NUMPY_NUMEL = 256\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import ModelInputForGPUBuilder\n\n\ndef is_block_tables_empty(block_tables: Union[None, Dict]):\n    \"\"\"\n    Check if block_tables is None or a dictionary with all None values.\n    \"\"\"\n    if block_tables is None:\n        return True\n    return (isinstance(block_tables, dict)\n            and all(value is None for value in block_tables.values()))\n\n\ndef compute_slot_mapping_start_idx(is_prompt: bool, query_len: int,\n                                   context_len: int, sliding_window: int,\n                                   use_v2_block_manager: bool):\n    \"\"\"\n    Compute the start index of slot mapping.\n    \"\"\"\n    start_idx = 0\n    if is_prompt and sliding_window is not None:\n        assert use_v2_block_manager or context_len == 0, (\n            \"Prefix caching is currently not supported with \"\n            \"sliding window attention in V1 block manager\")\n        # When prefill, we use it to not write slots to kv cache\n        # to save memory.\n        start_idx = max(0, query_len - sliding_window)\n    return start_idx\n\n\ndef _compute_slot_mapping_python(slot_mapping: List[int],\n                                 block_table: List[int], range_start: int,\n                                 range_end: int, block_size: int):\n    for i in range(range_start, range_end):\n        block_number = block_table[i // block_size]\n        block_offset = i % block_size\n        slot = block_number * block_size + block_offset\n        slot_mapping.append(slot)\n\n\ndef _compute_slot_mapping_numpy(slot_mapping: List[int],\n                                block_table: List[int], range_start: int,\n                                range_end: int, block_size: int):\n    block_table_array = np.array(block_table)\n    idx = np.arange(range_start, range_end)\n    block_offset = idx % block_size\n    idx //= block_size\n    seq_slot_mapping_array = block_table_array[idx]\n    seq_slot_mapping_array *= block_size\n    seq_slot_mapping_array += block_offset\n    slot_mapping.extend(seq_slot_mapping_array)\n\n\ndef compute_slot_mapping(is_profile_run: bool, slot_mapping: List[int],\n                         seq_id: int, seq_len: int, context_len: int,\n                         start_idx: int, block_size: int,\n                         block_tables: Dict[int, List[int]]):\n    \"\"\"\n    Compute slot mapping.\n    \"\"\"\n    if is_profile_run:\n        # During memory profiling, the block tables are not\n        # initialized yet. In this case, we just use a dummy\n        # slot mapping.\n        # In embeddings, the block tables are {seq_id: None}.\n        slot_mapping.extend([PAD_SLOT_ID] * seq_len)\n        return\n\n    # Mask the [0, start_idx) tokens of the prompt with\n    # PAD_SLOT_ID, where start_idx is max(0, seq_len -\n    # sliding_window). For example, if the prompt len is 10,\n    # sliding window is 8, and block size is 4, the first two\n    # tokens are masked and the slot mapping will be\n    # [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n    padding_mask_len = max(0, start_idx - context_len)\n    slot_mapping.extend([PAD_SLOT_ID] * padding_mask_len)\n\n    range_start = max(start_idx, context_len)\n    range_end = seq_len\n    numel = range_end - range_start\n    block_table = block_tables[seq_id]\n\n    # numpy implementation will be faster than python if we have\n    # many elements, otherwise it will be slower.\n    if numel < _COMPUTE_SLOT_MAPPING_NUMPY_NUMEL:\n        _compute_slot_mapping_python(slot_mapping, block_table, range_start,\n                                     range_end, block_size)\n    else:\n        _compute_slot_mapping_numpy(slot_mapping, block_table, range_start,\n                                    range_end, block_size)\n\n\nTAttentionMetadata = TypeVar(\"TAttentionMetadata\", bound='AttentionMetadata')\n\n\nclass CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n\n    _metadata_cls: Type[TAttentionMetadata]\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n        self.use_v2_block_manager = (\n            input_builder.scheduler_config.use_v2_block_manager)\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool):\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n        computed_block_nums = inter_data.computed_block_nums\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks):\n            self.context_lens.append(context_len)\n            if is_prompt:\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                assert query_len == 1, (\n                    \"seq_len: {}, context_len: {}, query_len: {}\".format(\n                        seq_len, context_len, query_len))\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if inter_data.prefix_cache_hit:\n                block_table = computed_block_nums\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                block_table = block_tables[seq_id][-curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            # Compute slot mapping.\n            is_profile_run = is_block_tables_empty(block_tables)\n            start_idx = compute_slot_mapping_start_idx(\n                is_prompt, query_len, context_len, self.sliding_window,\n                self.use_v2_block_manager)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_query_len = max(query_lens)\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        max_decode_seq_len = max(self.curr_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size\n\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.runner.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(self.block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.from_numpy(input_block_tables).to(\n                device, non_blocking=True)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n        assert max_query_len > 0, \"query_lens: {}\".format(query_lens)\n\n        assert device is not None\n        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,\n                                               device, self.runner.pin_memory)\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,\n                                             self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=device)\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        return self._metadata_cls(  # type: ignore\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            seq_lens=seq_lens,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=max_query_len,\n            max_prefill_seq_len=max_prefill_seq_len,\n            max_decode_seq_len=max_decode_seq_len,\n            query_start_loc=query_start_loc,\n            seq_start_loc=seq_start_loc,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n        )\n\n\nclass CommonAttentionState(AttentionState):\n\n    def __init__(self, runner: \"ModelRunnerBase\"):\n        self.runner = runner\n        self._is_graph_capturing = False\n\n    @contextmanager\n    def graph_capture(self, max_batch_size: int):\n        self._is_graph_capturing = True\n        self._graph_slot_mapping = torch.full((max_batch_size, ),\n                                              PAD_SLOT_ID,\n                                              dtype=torch.long,\n                                              device=self.runner.device)\n        self._graph_seq_lens = torch.ones(max_batch_size,\n                                          dtype=torch.int32,\n                                          device=self.runner.device)\n        self._graph_block_tables = torch.from_numpy(\n            self.runner.graph_block_tables).to(device=self.runner.device)\n        yield\n        self._is_graph_capturing = False\n        del self._graph_slot_mapping\n        del self._graph_seq_lens\n        del self._graph_block_tables\n\n    def graph_clone(self, batch_size: int) -> \"CommonAttentionState\":\n        assert self._is_graph_capturing\n        return self.__class__(self.runner)\n\n    def graph_capture_get_metadata_for_batch(\n            self, batch_size: int, is_encoder_decoder_model: bool = False):\n        assert self._is_graph_capturing\n        attn_metadata = self.runner.attn_backend.make_metadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=batch_size,\n            slot_mapping=self._graph_slot_mapping[:batch_size],\n            seq_lens=None,\n            seq_lens_tensor=self._graph_seq_lens[:batch_size],\n            max_query_len=1,\n            max_decode_query_len=1,\n            max_prefill_seq_len=0,\n            max_decode_seq_len=self.runner.max_seq_len_to_capture,\n            query_start_loc=None,\n            seq_start_loc=None,\n            context_lens_tensor=None,\n            block_tables=self._graph_block_tables[:batch_size],\n            use_cuda_graph=True,\n        )\n        if is_encoder_decoder_model:\n            # The encoder decoder model works only with XFormers backend.\n            # Assert the same.\n            assert self.runner.attn_backend.get_name() == \"xformers\", \\\n            f\"Expected attn_backend name to be 'xformers', but \"\\\n            f\" got '{self.runner.attn_backend.get_name()}'\"\n            self._update_captured_metadata_for_enc_dec_model(\n                batch_size=batch_size, attn_metadata=attn_metadata)\n\n        return attn_metadata\n\n    def get_graph_input_buffers(\n            self,\n            attn_metadata,\n            is_encoder_decoder_model: bool = False) -> Dict[str, Any]:\n        input_buffers = {\n            \"slot_mapping\": attn_metadata.slot_mapping,\n            \"seq_lens_tensor\": attn_metadata.decode_metadata.seq_lens_tensor,\n            \"block_tables\": attn_metadata.decode_metadata.block_tables,\n        }\n        if is_encoder_decoder_model:\n            # The encoder decoder model works only with XFormers backend.\n            # Assert the same.\n            assert self.runner.attn_backend.get_name() == \"xformers\", \\\n            f\"Expected attn_backend name to be 'xformers', but \"\\\n            f\" got '{self.runner.attn_backend.get_name()}'\"\n            self._add_additonal_input_buffers_for_enc_dec_model(\n                attn_metadata=attn_metadata, input_buffers=input_buffers)\n        return input_buffers\n\n    def prepare_graph_input_buffers(\n            self,\n            input_buffers,\n            attn_metadata,\n            is_encoder_decoder_model: bool = False) -> None:\n        input_buffers[\"seq_lens_tensor\"].copy_(\n            attn_metadata.decode_metadata.seq_lens_tensor, non_blocking=True)\n        input_buffers[\"block_tables\"].copy_(\n            attn_metadata.decode_metadata.block_tables, non_blocking=True)\n        if is_encoder_decoder_model:\n            # The encoder decoder model works only with XFormers backend.\n            # Assert the same.\n            assert self.runner.attn_backend.get_name() == \"xformers\", \\\n            f\"Expected attn_backend name to be 'xformers', but \"\\\n            f\" got '{self.runner.attn_backend.get_name()}'\"\n            self._prepare_input_buffers_for_enc_dec_model(\n                attn_metadata, input_buffers)\n\n    def begin_forward(self, model_input) -> None:\n        return\n\n    def _update_captured_metadata_for_enc_dec_model(self, batch_size: int,\n                                                    attn_metadata):\n        \"\"\"\n        Updates the attention metadata parameters for CUDA graph capture in an\n        encoder-decoder model.\n\n        This method modifies attention-related tensors and metadata required\n        for CUDA graph capture in encoder-decoder models. Specifically, it\n        updates the cross-attention and encoder sequence tensors in the \n        AttentionMetadata object.\n        \"\"\"\n        # During decode phase the cross_slot_mapping will be empty. Hence set\n        # an empty tensor for CUDA Graph capture.\n        attn_metadata.cross_slot_mapping = torch.tensor(\n            [], dtype=torch.int).cuda()\n        attn_metadata.cross_block_tables = torch.full(\n            (batch_size, self.runner.get_max_block_per_batch()),\n            1,\n            dtype=torch.int).cuda()\n        attn_metadata.encoder_seq_lens = torch.full((batch_size, ),\n                                                    1,\n                                                    dtype=torch.int).cuda()\n        attn_metadata.encoder_seq_lens_tensor = torch.full(\n            (batch_size, ), 1, dtype=torch.int).cuda()\n        attn_metadata.max_encoder_seq_len = self.runner.max_seq_len_to_capture\n\n    def _add_additonal_input_buffers_for_enc_dec_model(\n            self, attn_metadata, input_buffers: Dict[str, Any]):\n        \"\"\"\n        Saves additional input buffers specific to the encoder-decoder model\n        from the attention metadata.\n\n        This method extracts and stores encoder-decoder related input buffers\n        from the `attn_metadata` into the `input_buffers` dictionary. The\n        buffers include encoder sequence lengths, cross-slot mappings, and\n        cross-block tables, which are essential for the encoder-decoder model\n        during CUDA graph replay.\n        \"\"\"\n        input_buffers[\"encoder_seq_lens_tensor\"] = (\n            attn_metadata.decode_metadata.encoder_seq_lens_tensor)\n        input_buffers[\"cross_slot_mapping\"] = (\n            attn_metadata.decode_metadata.cross_slot_mapping)\n        input_buffers[\"cross_block_tables\"] = (\n            attn_metadata.decode_metadata.cross_block_tables)\n\n    def _prepare_input_buffers_for_enc_dec_model(self, attn_metadata,\n                                                 input_buffers: Dict[str,\n                                                                     Any]):\n        \"\"\"\n        Populates input buffers with data from the encoder-decoder model's\n        attention metadata.\n\n        This method fills the input buffers with encoder-decoder specific\n        tensors. It copies data from the `attn_metadata` and keyword arguments\n        (`kwargs`) into corresponding buffers in the `input_buffers` dictionary.\n        The copied data includes attention-related metadata as well as input \n        IDs and positional information for the encoder.\n        \"\"\"\n        input_buffers[\"encoder_seq_lens_tensor\"].copy_(\n            attn_metadata.decode_metadata.encoder_seq_lens_tensor,\n            non_blocking=True)\n        input_buffers[\"cross_slot_mapping\"].copy_(\n            attn_metadata.decode_metadata.cross_slot_mapping,\n            non_blocking=True)\n        input_buffers[\"cross_block_tables\"].copy_(\n            attn_metadata.decode_metadata.cross_block_tables,\n            non_blocking=True)\n",
      "diff": "diff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py\nindex 53e3a53ba..358a223e7 100644\n--- a/vllm/attention/backends/utils.py\n+++ b/vllm/attention/backends/utils.py\n@@ -38,18 +38,12 @@ def is_block_tables_empty(block_tables: Union[None, Dict]):\n \n \n def compute_slot_mapping_start_idx(is_prompt: bool, query_len: int,\n-                                   context_len: int, sliding_window: int,\n-                                   use_v2_block_manager: bool):\n+                                   context_len: int, sliding_window: int):\n     \"\"\"\n     Compute the start index of slot mapping.\n     \"\"\"\n     start_idx = 0\n     if is_prompt and sliding_window is not None:\n-        assert use_v2_block_manager or context_len == 0, (\n-            \"Prefix caching is currently not supported with \"\n-            \"sliding window attention in V1 block manager\")\n-        # When prefill, we use it to not write slots to kv cache\n-        # to save memory.\n         start_idx = max(0, query_len - sliding_window)\n     return start_idx\n \n@@ -138,8 +132,6 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n \n         self.sliding_window = input_builder.sliding_window\n         self.block_size = input_builder.block_size\n-        self.use_v2_block_manager = (\n-            input_builder.scheduler_config.use_v2_block_manager)\n \n     def _add_seq_group(\n             self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n@@ -180,9 +172,9 @@ class CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n \n             # Compute slot mapping.\n             is_profile_run = is_block_tables_empty(block_tables)\n-            start_idx = compute_slot_mapping_start_idx(\n-                is_prompt, query_len, context_len, self.sliding_window,\n-                self.use_v2_block_manager)\n+            start_idx = compute_slot_mapping_start_idx(is_prompt, query_len,\n+                                                       context_len,\n+                                                       self.sliding_window)\n             compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                  seq_len, context_len, start_idx,\n                                  self.block_size, inter_data.block_tables)",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 13
    },
    {
      "file_path": "vllm/commit_id.py",
      "old_content": "",
      "diff": "diff --git a/vllm/commit_id.py b/vllm/commit_id.py\nnew file mode 100644\nindex 000000000..d857066f1\n--- /dev/null\n+++ b/vllm/commit_id.py\n@@ -0,0 +1 @@\n+__commit__ = \"93ec62b8556e279d2c050bdc1c3247831bd39466\"",
      "change_type": "added",
      "lines_added": 2,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/config.py",
      "old_content": "import enum\nimport json\nfrom dataclasses import dataclass, field, fields\nfrom typing import (TYPE_CHECKING, Any, ClassVar, Dict, List, Mapping,\n                    Optional, Tuple, Type, Union)\n\nimport torch\nfrom transformers import PretrainedConfig\n\nimport vllm.envs as envs\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import current_platform\nfrom vllm.tracing import is_otel_available, otel_import_error_traceback\nfrom vllm.transformers_utils.config import (ConfigFormat, get_config,\n                                            get_hf_image_processor_config,\n                                            get_hf_text_config)\nfrom vllm.utils import (GiB_bytes, cuda_device_count_stateless, get_cpu_memory,\n                        is_hip, is_neuron, is_openvino, is_xpu,\n                        print_warning_once)\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.model_loader.loader import BaseModelLoader\n    from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n        BaseTokenizerGroup)\n\nlogger = init_logger(__name__)\n\n_EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n_MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 5120\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n            It is also used as the content for `model_name` tag in metrics \n            output when `served_model_name` is not specified. \n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, \"slow\" will always use the slow tokenizer, and\n            \"mistral\" will always use the tokenizer from `mistral_common`.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        code_revision: The specific revision to use for the model code on\n            Hugging Face Hub. It can be a branch name, a tag name, or a\n            commit id. If unspecified, will use the default version.\n        rope_scaling: Dictionary containing the scaling configuration for the\n            RoPE embeddings. When using this flag, don't update\n            `max_position_embeddings` to the expected new maximum.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n        quantization_param_path: Path to JSON file containing scaling factors.\n            Used to load KV cache scaling factors into the model when KV cache\n            type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also\n            be used to load activation and weight scaling factors when the\n            model dtype is FP8_E4M3 on ROCm.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n            If None, the user did not specify, so default to False.\n        max_context_len_to_capture: Maximum context len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode (DEPRECATED. Use max_seq_len_to_capture instead).\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode. Additionally for encoder-decoder models, if the\n            sequence length of the encoder input is larger than this, we fall\n            back to the eager mode.\n        disable_sliding_window: Whether to disable sliding window. If True,\n            we will disable the sliding window functionality of the model.\n            If the model does not support sliding window, this argument is\n            ignored.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer.\n        served_model_name: The model name used in metrics tag `model_name`,\n            matches the model name exposed via the APIs. If multiple model \n            names provided, the first name will be used. If not specified, \n            the model name will be the same as `model`.\n        limit_mm_per_prompt: Maximum number of data instances per modality \n            per prompt. Only applicable for multimodal models.\n        override_neuron_config: Initialize non default neuron config or \n            override default neuron config that are specific to Neuron devices, \n            this argument will be used to configure the neuron config that \n            can not be gathered from the vllm arguments. \n        config_format: The config format which shall be loaded.\n            Defaults to 'auto' which defaults to 'hf'.\n        mm_processor_kwargs: Arguments to be forwarded to the model's processor\n            for multi-modal data, e.g., image processor.\n    \"\"\"\n\n    def __init__(self,\n                 model: str,\n                 tokenizer: str,\n                 tokenizer_mode: str,\n                 trust_remote_code: bool,\n                 dtype: Union[str, torch.dtype],\n                 seed: int,\n                 revision: Optional[str] = None,\n                 code_revision: Optional[str] = None,\n                 rope_scaling: Optional[dict] = None,\n                 rope_theta: Optional[float] = None,\n                 tokenizer_revision: Optional[str] = None,\n                 max_model_len: Optional[int] = None,\n                 spec_target_max_model_len: Optional[int] = None,\n                 quantization: Optional[str] = None,\n                 quantization_param_path: Optional[str] = None,\n                 enforce_eager: Optional[bool] = None,\n                 max_context_len_to_capture: Optional[int] = None,\n                 max_seq_len_to_capture: Optional[int] = None,\n                 max_logprobs: int = 20,\n                 disable_sliding_window: bool = False,\n                 skip_tokenizer_init: bool = False,\n                 served_model_name: Optional[Union[str, List[str]]] = None,\n                 limit_mm_per_prompt: Optional[Mapping[str, int]] = None,\n                 use_async_output_proc: bool = True,\n                 override_neuron_config: Optional[Dict[str, Any]] = None,\n                 config_format: ConfigFormat = ConfigFormat.AUTO,\n                 mm_processor_kwargs: Optional[Dict[str, Any]] = None) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.seed = seed\n        self.revision = revision\n        self.code_revision = code_revision\n        self.rope_scaling = rope_scaling\n        self.rope_theta = rope_theta\n        # The tokenizer version is consistent with the model version by default.\n        if tokenizer_revision is None:\n            self.tokenizer_revision = revision\n        else:\n            self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n        self.quantization_param_path = quantization_param_path\n        self.enforce_eager = enforce_eager\n        if max_context_len_to_capture is not None:\n            raise ValueError(\"`max_context_len_to_capture` is deprecated. \"\n                             \"Use `max_seq_len_to_capture` instead.\")\n        self.max_seq_len_to_capture = max_seq_len_to_capture\n        self.max_logprobs = max_logprobs\n        self.disable_sliding_window = disable_sliding_window\n        self.skip_tokenizer_init = skip_tokenizer_init\n\n        self.hf_config = get_config(self.model, trust_remote_code, revision,\n                                    code_revision, rope_scaling, rope_theta,\n                                    config_format)\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.hf_image_processor_config = get_hf_image_processor_config(\n            self.model, revision)\n        self.dtype = _get_and_verify_dtype(self.hf_text_config, dtype)\n        self.use_async_output_proc = use_async_output_proc\n        self.mm_processor_kwargs = mm_processor_kwargs\n\n        # Set enforce_eager to False if the value is unset.\n        if self.enforce_eager is None:\n            self.enforce_eager = False\n\n        sliding_window = getattr(self.hf_text_config, \"sliding_window\", None)\n        has_interleaved_attention = (sliding_window is not None) and (\n            isinstance(sliding_window, list) or\n            (self.hf_text_config.model_type in [\"gemma2\"]))\n\n        if (not self.disable_sliding_window and has_interleaved_attention):\n            sliding_window_len_min = get_min_sliding_window(\n                self.hf_text_config.sliding_window)\n\n            print_warning_once(\n                f\"{self.hf_text_config.model_type} has interleaved attention, \"\n                \"which is currently not supported by vLLM. Disabling sliding \"\n                \"window and capping the max length to the sliding window size \"\n                f\"({sliding_window_len_min}).\")\n            self.disable_sliding_window = True\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window(),\n            spec_target_max_model_len=spec_target_max_model_len)\n        self.served_model_name = get_served_model_name(model,\n                                                       served_model_name)\n        self.multimodal_config = self._init_multimodal_config(\n            limit_mm_per_prompt)\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n\n        self.is_attention_free = self._init_attention_free()\n        self.has_inner_state = self._init_has_inner_state()\n\n        self.override_neuron_config = override_neuron_config if is_neuron(\n        ) else None\n        self._verify_embedding_mode()\n        self._verify_quantization()\n        self._verify_cuda_graph()\n        self._verify_bnb_config()\n\n    def _init_multimodal_config(\n        self, limit_mm_per_prompt: Optional[Mapping[str, int]]\n    ) -> Optional[\"MultiModalConfig\"]:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if ModelRegistry.is_multimodal_model(architectures):\n            return MultiModalConfig(limit_per_prompt=limit_mm_per_prompt or {})\n\n        if limit_mm_per_prompt:\n            raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\n                             \"multimodal models.\")\n\n        return None\n\n    def _init_attention_free(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_attention_free_model(architectures)\n\n    def _init_has_inner_state(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.model_has_inner_state(architectures)\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\", \"mistral\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto', 'slow' or 'mistral'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _verify_embedding_mode(self) -> None:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n\n        # TODO: Allow the same model architecture to be specified as either\n        # generation or embedding model\n        if \"Phi3VForCausalLM\" in architectures:\n            # Match both remote and local names\n            embedding_mode = \"/VLM2Vec\" in self.model\n        else:\n            embedding_mode = ModelRegistry.is_embedding_model(architectures)\n\n        self.embedding_mode = embedding_mode\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = [*QUANTIZATION_METHODS]\n        rocm_supported_quantization = [\n            \"awq\", \"gptq\", \"fp8\", \"compressed_tensors\", \"compressed-tensors\",\n            \"fbgemm_fp8\"\n        ]\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"modelopt\", \"gptq_marlin_24\", \"gptq_marlin\",\n            \"awq_marlin\", \"fbgemm_fp8\", \"compressed_tensors\",\n            \"compressed-tensors\", \"experts_int8\"\n        ]\n        tpu_supported_quantization = [\"tpu_int8\"]\n        neuron_supported_quantization = [\"neuron_quant\"]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n\n            # Detect which checkpoint is it\n            for _, method in QUANTIZATION_METHODS.items():\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override:\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            if is_hip(\n            ) and self.quantization not in rocm_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in ROCm.\")\n            if current_platform.is_tpu(\n            ) and self.quantization not in tpu_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in TPU Backend.\")\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n            if (self.quantization == \"awq\" and is_hip()\n                    and not envs.VLLM_USE_TRITON_AWQ):\n                logger.warning(\n                    \"Using AWQ quantization with ROCm, but VLLM_USE_TRITON_AWQ\"\n                    \" is not set, enabling VLLM_USE_TRITON_AWQ.\")\n                envs.VLLM_USE_TRITON_AWQ = True\n            if is_neuron(\n            ) and self.quantization not in neuron_supported_quantization:\n                raise ValueError(\n                    f\"{self.quantization} quantization is currently not \"\n                    f\"supported in Neuron Backend.\")\n\n    def _verify_cuda_graph(self) -> None:\n        if self.max_seq_len_to_capture is None:\n            self.max_seq_len_to_capture = self.max_model_len\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n\n    def _verify_bnb_config(self) -> None:\n        \"\"\"\n        The current version of bitsandbytes (0.44.0) with 8-bit models does not \n        yet support CUDA graph.\n        \"\"\"\n        is_bitsandbytes = self.quantization == \"bitsandbytes\"\n        has_quantization_config = (getattr(self.hf_config,\n                                           \"quantization_config\", None)\n                                   is not None)\n        is_8bit = (self.hf_config.quantization_config.get(\n            \"load_in_8bit\", False) if has_quantization_config else False)\n        if all([\n                is_bitsandbytes,\n                has_quantization_config,\n                is_8bit,\n                not self.enforce_eager,\n        ]):\n            logger.warning(\n                \"CUDA graph is not supported on BitAndBytes 8bit yet, \"\n                \"fallback to the eager mode.\")\n            self.enforce_eager = True\n\n    def verify_async_output_proc(self, parallel_config, speculative_config,\n                                 device_config) -> None:\n        if not self.use_async_output_proc:\n            # Nothing to check\n            return\n\n        if parallel_config.pipeline_parallel_size > 1:\n            logger.warning(\"Async output processing can not be enabled \"\n                           \"with pipeline parallel\")\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if device_config.device_type not in (\"cuda\", \"tpu\", \"xpu\"):\n            logger.warning(\n                \"Async output processing is only supported for CUDA, TPU, XPU. \"\n                \"Disabling it for other platforms.\")\n            self.use_async_output_proc = False\n            return\n\n        if envs.VLLM_USE_RAY_SPMD_WORKER:\n            logger.warning(\n                \"Async output processing can not be enabled with ray spmd\")\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if device_config.device_type == \"cuda\" and self.enforce_eager:\n            logger.warning(\n                \"To see benefits of async output processing, enable CUDA \"\n                \"graph. Since, enforce-eager is enabled, async output \"\n                \"processor cannot be used\")\n            self.use_async_output_proc = not self.enforce_eager\n            return\n\n        # Async postprocessor is not necessary with embedding mode\n        # since there is no token generation\n        if self.embedding_mode:\n            self.use_async_output_proc = False\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if speculative_config:\n            logger.warning(\"Async output processing is not supported with\"\n                           \" speculative decoding currently.\")\n            self.use_async_output_proc = False\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if pipeline_parallel_size > 1:\n            architectures = getattr(self.hf_config, \"architectures\", [])\n            if not ModelRegistry.is_pp_supported_model(architectures):\n                raise NotImplementedError(\n                    \"Pipeline parallelism is not supported for this model. \"\n                    \"Supported models implement the `SupportsPP` interface.\")\n\n            if self.use_async_output_proc:\n                logger.warning(\"Async output processor is not supported with \"\n                               \"pipeline parallelism currently. Disabling it.\")\n                self.use_async_output_proc = False\n\n    def get_hf_config_sliding_window(\n            self) -> Union[Optional[int], List[Optional[int]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n    def get_sliding_window(self) -> Optional[Union[int, List[Optional[int]]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n        # If user disables sliding window, return None.\n        if self.disable_sliding_window:\n            return None\n        # Otherwise get the value from the hf config.\n        return self.get_hf_config_sliding_window()\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if hasattr(self.hf_text_config, \"model_type\"\n                   ) and self.hf_text_config.model_type == 'deepseek_v2':\n            # FlashAttention supports only head_size 32, 64, 128, 256,\n            # we need to pad head_size 192 to 256\n            return 256\n\n        if self.is_attention_free:\n            return 0\n\n        if hasattr(self.hf_text_config, \"head_dim\"):\n            return self.hf_text_config.head_dim\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        if self.is_attention_free:\n            return 0\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        from vllm.distributed.utils import get_pp_indices\n        total_num_hidden_layers = getattr(self.hf_text_config,\n                                          \"num_hidden_layers\", 0)\n        pp_rank = parallel_config.rank // parallel_config.tensor_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return end - start\n\n    def get_num_attention_layers(self,\n                                 parallel_config: \"ParallelConfig\") -> int:\n        if self.is_attention_free:\n            return 0\n\n        num_layers = self.get_num_layers(parallel_config)\n\n        # Transformers supports layers_block_type @property\n        layers = getattr(self.hf_config, \"layers_block_type\",\n                         [\"attention\"] * num_layers)\n        return len([t for t in layers if t == \"attention\"])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    @property\n    def is_encoder_decoder_model(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return getattr(self.hf_config, \"is_encoder_decoder\", False) or (\n            (hasattr(self.hf_config, \"text_config\") and getattr(\n                self.hf_config.text_config, \"is_encoder_decoder\", False)))\n\n    @property\n    def is_embedding_model(self) -> bool:\n        \"\"\"Extract the embedding model flag.\"\"\"\n        return self.embedding_mode\n\n    @property\n    def is_multimodal_model(self) -> bool:\n        return self.multimodal_config is not None\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n        cache_dtype: Data type for kv cache storage.\n        num_gpu_blocks_override: Number of GPU blocks to use. This overrides the\n            profiled num_gpu_blocks if specified. Does nothing if None.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: float,\n        cache_dtype: str,\n        is_attention_free: bool = False,\n        num_gpu_blocks_override: Optional[int] = None,\n        sliding_window: Optional[int] = None,\n        enable_prefix_caching: bool = False,\n        cpu_offload_gb: float = 0,\n    ) -> None:\n        self.block_size = block_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.swap_space_bytes = swap_space * GiB_bytes\n        self.num_gpu_blocks_override = num_gpu_blocks_override\n        self.cache_dtype = cache_dtype\n        self.is_attention_free = is_attention_free\n        self.sliding_window = sliding_window\n        self.enable_prefix_caching = enable_prefix_caching\n        self.cpu_offload_gb = cpu_offload_gb\n\n        self._verify_args()\n        self._verify_cache_dtype()\n        self._verify_prefix_caching()\n\n        # Will be set after profiling.\n        self.num_gpu_blocks: Optional[int] = None\n        self.num_cpu_blocks: Optional[int] = None\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype in (\"fp8\", \"fp8_e4m3\", \"fp8_e5m2\"):\n            logger.info(\n                \"Using fp8 data type to store kv cache. It reduces the GPU \"\n                \"memory footprint and boosts the performance. \"\n                \"Meanwhile, it may cause accuracy drop without a proper \"\n                \"scaling factor\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def _verify_prefix_caching(self) -> None:\n        if not self.enable_prefix_caching:\n            return\n\n        if self.sliding_window is not None:\n            raise NotImplementedError(\n                \"Prefix caching is not supported with sliding window. \"\n                \"Run with --disable-sliding-window to use prefix caching.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / GiB_bytes:.2f} GiB out of the \"\n               f\"{total_cpu_memory / GiB_bytes:.2f} GiB total CPU memory \"\n               \"is allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. %s\", msg)\n\n\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"Configuration for the tokenizer pool.\n\n    Args:\n        pool_size: Number of tokenizer workers in the pool.\n        pool_type: Type of the pool.\n        extra_config: Additional config for the pool.\n            The way the config will be used depends on the\n            pool type.\n    \"\"\"\n    pool_size: int\n    pool_type: Union[str, Type[\"BaseTokenizerGroup\"]]\n    extra_config: dict\n\n    def __post_init__(self):\n        if self.pool_type not in (\"ray\", ) and not isinstance(\n                self.pool_type, type):\n            raise ValueError(f\"Unknown pool type: {self.pool_type}\")\n        if not isinstance(self.extra_config, dict):\n            raise ValueError(\"extra_config must be a dictionary.\")\n\n    @classmethod\n    def create_config(\n        cls, tokenizer_pool_size: int,\n        tokenizer_pool_type: Union[str, Type[\"BaseTokenizerGroup\"]],\n        tokenizer_pool_extra_config: Optional[Union[str, dict]]\n    ) -> Optional[\"TokenizerPoolConfig\"]:\n        \"\"\"Create a TokenizerPoolConfig from the given parameters.\n\n        If tokenizer_pool_size is 0, return None.\n\n        Args:\n            tokenizer_pool_size: Number of tokenizer workers in the pool.\n            tokenizer_pool_type: Type of the pool.\n            tokenizer_pool_extra_config: Additional config for the pool.\n                The way the config will be used depends on the\n                pool type. This can be a JSON string (will be parsed).\n        \"\"\"\n        if tokenizer_pool_size:\n            if isinstance(tokenizer_pool_extra_config, str):\n                tokenizer_pool_extra_config_parsed = json.loads(\n                    tokenizer_pool_extra_config)\n            else:\n                tokenizer_pool_extra_config_parsed = (\n                    tokenizer_pool_extra_config or {})\n            tokenizer_pool_config = cls(tokenizer_pool_size,\n                                        tokenizer_pool_type,\n                                        tokenizer_pool_extra_config_parsed)\n        else:\n            tokenizer_pool_config = None\n        return tokenizer_pool_config\n\n\nclass LoadFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    PT = \"pt\"\n    SAFETENSORS = \"safetensors\"\n    NPCACHE = \"npcache\"\n    DUMMY = \"dummy\"\n    TENSORIZER = \"tensorizer\"\n    SHARDED_STATE = \"sharded_state\"\n    GGUF = \"gguf\"\n    BITSANDBYTES = \"bitsandbytes\"\n    MISTRAL = \"mistral\"\n\n\n@dataclass\nclass LoadConfig:\n    \"\"\"\n        download_dir: Directory to download and load the weights, default to the\n            default cache directory of huggingface.\n        load_format: The format of the model weights to load:\n            \"auto\" will try to load the weights in the safetensors format and\n                fall back to the pytorch bin format if safetensors format is\n                not available.\n            \"pt\" will load the weights in the pytorch bin format.\n            \"safetensors\" will load the weights in the safetensors format.\n            \"npcache\" will load the weights in pytorch format and store\n                a numpy cache to speed up the loading.\n            \"dummy\" will initialize the weights with random values, which is\n                mainly for profiling.\n            \"tensorizer\" will use CoreWeave's tensorizer library for\n                fast weight loading.\n            \"bitsandbytes\" will load nf4 type weights.\n        ignore_patterns: The list of patterns to ignore when loading the model.\n            Default to \"original/**/*\" to avoid repeated loading of llama's \n            checkpoints.\n\n    \"\"\"\n\n    load_format: Union[str, LoadFormat, \"BaseModelLoader\"] = LoadFormat.AUTO\n    download_dir: Optional[str] = None\n    model_loader_extra_config: Optional[Union[str, dict]] = field(\n        default_factory=dict)\n    ignore_patterns: Optional[Union[List[str], str]] = None\n\n    def __post_init__(self):\n        model_loader_extra_config = self.model_loader_extra_config or {}\n        if isinstance(model_loader_extra_config, str):\n            self.model_loader_extra_config = json.loads(\n                model_loader_extra_config)\n        self._verify_load_format()\n\n        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:\n            logger.info(\n                \"Ignoring the following patterns when downloading weights: %s\",\n                self.ignore_patterns)\n        else:\n            self.ignore_patterns = [\"original/**/*\"]\n\n    def _verify_load_format(self) -> None:\n        if not isinstance(self.load_format, str):\n            return\n\n        load_format = self.load_format.lower()\n        self.load_format = LoadFormat(load_format)\n\n        rocm_not_supported_load_format: List[str] = []\n        if is_hip() and load_format in rocm_not_supported_load_format:\n            rocm_supported_load_format = [\n                f for f in LoadFormat.__members__\n                if (f not in rocm_not_supported_load_format)\n            ]\n            raise ValueError(\n                f\"load format '{load_format}' is not supported in ROCm. \"\n                f\"Supported load formats are \"\n                f\"{rocm_supported_load_format}\")\n\n\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\n\n    Args:\n        pipeline_parallel_size: Number of pipeline parallel groups.\n        tensor_parallel_size: Number of tensor parallel groups.\n        worker_use_ray: Deprecated, use distributed_executor_backend instead.\n        max_parallel_loading_workers: Maximum number of multiple batches\n            when load model sequentially. To avoid RAM OOM when using tensor\n            parallel and large models.\n        disable_custom_all_reduce: Disable the custom all-reduce kernel and\n            fall back to NCCL.\n        tokenizer_pool_config: Config for the tokenizer pool.\n            If None, will use synchronous tokenization.\n        ray_workers_use_nsight: Whether to profile Ray workers with nsight, see\n            https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\n        placement_group: ray distributed model workers placement group.\n        distributed_executor_backend: Backend to use for distributed model\n            workers, either \"ray\" or \"mp\" (multiprocessing). If either\n            pipeline_parallel_size or tensor_parallel_size is greater than 1,\n            will default to \"ray\" if Ray is installed or \"mp\" otherwise.\n    \"\"\"\n\n    def __init__(\n        self,\n        pipeline_parallel_size: int,\n        tensor_parallel_size: int,\n        worker_use_ray: Optional[bool] = None,\n        max_parallel_loading_workers: Optional[int] = None,\n        disable_custom_all_reduce: bool = False,\n        tokenizer_pool_config: Optional[TokenizerPoolConfig] = None,\n        ray_workers_use_nsight: bool = False,\n        placement_group: Optional[\"PlacementGroup\"] = None,\n        distributed_executor_backend: Optional[Union[\n            str, Type[\"ExecutorBase\"]]] = None,\n    ) -> None:\n        self.pipeline_parallel_size = pipeline_parallel_size\n        self.tensor_parallel_size = tensor_parallel_size\n        self.distributed_executor_backend = distributed_executor_backend\n        self.max_parallel_loading_workers = max_parallel_loading_workers\n        self.disable_custom_all_reduce = disable_custom_all_reduce\n        self.tokenizer_pool_config = tokenizer_pool_config\n        self.ray_workers_use_nsight = ray_workers_use_nsight\n        self.placement_group = placement_group\n        self.world_size = pipeline_parallel_size * self.tensor_parallel_size\n\n        if worker_use_ray:\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            elif not self.use_ray:\n                raise ValueError(f\"worker-use-ray can't be used with \"\n                                 f\"distributed executor backend \"\n                                 f\"'{self.distributed_executor_backend}'.\")\n\n        if current_platform.is_tpu() and self.world_size > 1:\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            if self.distributed_executor_backend != \"ray\":\n                raise ValueError(\n                    \"TPU backend only supports Ray for distributed inference.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.executor import ray_utils\n            backend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if (current_platform.is_cuda()\n                    and cuda_device_count_stateless() < self.world_size):\n                if not ray_found:\n                    raise ValueError(\"Unable to load Ray which is \"\n                                     \"required for multi-node inference, \"\n                                     \"please install Ray with `pip install \"\n                                     \"ray`.\") from ray_utils.ray_import_err\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.info(\"Defaulting to use %s for distributed inference\",\n                        backend)\n\n        self._verify_args()\n        self.rank: int = 0\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and self.distributed_executor_backend.uses_ray)\n\n    def _verify_args(self) -> None:\n        # Lazy import to avoid circular import\n        from vllm.executor.executor_base import ExecutorBase\n\n        if self.distributed_executor_backend not in (\n                \"ray\", \"mp\", None) and not (isinstance(\n                    self.distributed_executor_backend, type) and issubclass(\n                        self.distributed_executor_backend, ExecutorBase)):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' or custom ExecutorBase subclass.\")\n        if self.use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n        if is_hip():\n            self.disable_custom_all_reduce = True\n            logger.info(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on AMD GPUs.\")\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\n\n    Args:\n        max_num_batched_tokens: Maximum number of tokens to be processed in\n            a single iteration.\n        max_num_seqs: Maximum number of sequences to be processed in a single\n            iteration.\n        max_model_len: Maximum length of a sequence (including prompt\n            and generated text).\n        use_v2_block_manager: Whether to use the BlockSpaceManagerV2 or not.\n        num_lookahead_slots: The number of slots to allocate per sequence per\n            step, beyond the known token ids. This is used in speculative\n            decoding to store KV activations of tokens which may or may not be\n            accepted.\n        delay_factor: Apply a delay (of delay factor multiplied by previous\n            prompt latency) before scheduling next prompt.\n        enable_chunked_prefill: If True, prefill requests can be chunked based\n            on the remaining max_num_batched_tokens.\n        embedding_mode: Whether the running model is for embedding.\n        preemption_mode: Whether to perform preemption by swapping or \n            recomputation. If not specified, we determine the mode as follows:\n            We use recomputation by default since it incurs lower overhead than\n            swapping. However, when the sequence group has multiple sequences\n            (e.g., beam search), recomputation is not currently supported. In\n            such a case, we use swapping instead.\n        send_delta_data: Private API. If used, scheduler sends delta data to\n            workers instead of an entire data. It should be enabled only\n            when SPMD worker architecture is enabled. I.e.,\n            VLLM_USE_RAY_SPMD_WORKER=1\n        policy: The scheduling policy to use. \"fcfs\" (default) or \"priority\".\n    \"\"\"\n\n    def __init__(self,\n                 max_num_batched_tokens: Optional[int],\n                 max_num_seqs: int,\n                 max_model_len: int,\n                 use_v2_block_manager: bool = True,\n                 num_lookahead_slots: int = 0,\n                 delay_factor: float = 0.0,\n                 enable_chunked_prefill: bool = False,\n                 embedding_mode: bool = False,\n                 is_multimodal_model: bool = False,\n                 preemption_mode: Optional[str] = None,\n                 num_scheduler_steps: int = 1,\n                 multi_step_stream_outputs: bool = False,\n                 send_delta_data: bool = False,\n                 policy: str = \"fcfs\") -> None:\n        if max_num_batched_tokens is None:\n            if enable_chunked_prefill:\n                if num_scheduler_steps > 1:\n                    # Multi-step Chunked-Prefill doesn't allow prompt-chunking\n                    # for now. Have max_num_batched_tokens set to max_model_len\n                    # so we don't reject sequences on account of a short\n                    # max_num_batched_tokens.\n                    max_num_batched_tokens = max(max_model_len, 2048)\n                else:\n                    # It is the values that have the best balance between ITL\n                    # and TTFT on A100. Note it is not optimized for throughput.\n                    max_num_batched_tokens = 512\n            else:\n                # If max_model_len is too short, use 2048 as the default value\n                # for higher throughput.\n                max_num_batched_tokens = max(max_model_len, 2048)\n\n            if embedding_mode:\n                # For embedding, choose specific value for higher throughput\n                max_num_batched_tokens = max(\n                    max_num_batched_tokens,\n                    _EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n            if is_multimodal_model:\n                # The value needs to be at least the number of multimodal tokens\n                max_num_batched_tokens = max(\n                    max_num_batched_tokens,\n                    _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n\n        self.max_num_batched_tokens = max_num_batched_tokens\n\n        if enable_chunked_prefill:\n            logger.info(\n                \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                self.max_num_batched_tokens)\n\n        self.max_num_seqs = max_num_seqs\n        self.max_model_len = max_model_len\n        self.use_v2_block_manager = use_v2_block_manager\n        self.num_lookahead_slots = num_lookahead_slots\n        self.delay_factor = delay_factor\n        self.chunked_prefill_enabled = enable_chunked_prefill\n        self.embedding_mode = embedding_mode\n        self.preemption_mode = preemption_mode\n        self.num_scheduler_steps = num_scheduler_steps\n        self.multi_step_stream_outputs = multi_step_stream_outputs\n        self.send_delta_data = send_delta_data\n        self.policy = policy\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if (self.max_num_batched_tokens < self.max_model_len\n                and not self.chunked_prefill_enabled):\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n        if self.num_lookahead_slots < 0:\n            raise ValueError(\n                \"num_lookahead_slots \"\n                f\"({self.num_lookahead_slots}) must be greater than or \"\n                \"equal to 0.\")\n\n        if self.num_scheduler_steps < 1:\n            raise ValueError(\n                \"num_scheduler_steps \"\n                f\"({self.num_scheduler_steps}) must be greater than or \"\n                \"equal to 1.\")\n\n        if (not self.use_v2_block_manager \\\n            and not envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1):\n            raise ValueError(\n                \"The use of BlockSpaceManagerV1 is deprecated and will \"\n                \"be removed in a future release. Please switch to \"\n                \"BlockSpaceManagerV2 by setting --use-v2-block-manager to \"\n                \"True. If you wish to suppress this error temporarily, \"\n                \"you can set the environment variable \"\n                \"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. If your use \"\n                \"case is not supported in BlockSpaceManagerV2, please \"\n                \"file an issue with detailed information.\")\n\n    @property\n    def is_multi_step(self) -> bool:\n        return self.num_scheduler_steps > 1\n\n\nclass DeviceConfig:\n    device: Optional[torch.device]\n\n    def __init__(self, device: str = \"auto\") -> None:\n        if device == \"auto\":\n            # Automated device type detection\n            if current_platform.is_cuda_alike():\n                self.device_type = \"cuda\"\n            elif is_neuron():\n                self.device_type = \"neuron\"\n            elif is_openvino():\n                self.device_type = \"openvino\"\n            elif current_platform.is_tpu():\n                self.device_type = \"tpu\"\n            elif current_platform.is_cpu():\n                self.device_type = \"cpu\"\n            elif is_xpu():\n                self.device_type = \"xpu\"\n            else:\n                raise RuntimeError(\"Failed to infer device type\")\n        else:\n            # Device type is assigned explicitly\n            self.device_type = device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\", \"openvino\"]:\n            self.device = torch.device(\"cpu\")\n        elif self.device_type in [\"tpu\"]:\n            self.device = None\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\nclass SpeculativeConfig:\n    \"\"\"Configuration for speculative decoding.\n\n    The configuration is currently specialized to draft-model speculative\n    decoding with top-1 proposals.\n    \"\"\"\n\n    @staticmethod\n    def maybe_create_spec_config(\n        target_model_config: ModelConfig,\n        target_parallel_config: ParallelConfig,\n        target_dtype: str,\n        speculative_model: Optional[str],\n        speculative_model_quantization: Optional[str],\n        speculative_draft_tensor_parallel_size: Optional[int],\n        num_speculative_tokens: Optional[int],\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_max_model_len: Optional[int],\n        enable_chunked_prefill: bool,\n        use_v2_block_manager: bool,\n        disable_log_stats: bool,\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: Optional[float],\n        typical_acceptance_sampler_posterior_alpha: Optional[float],\n        disable_logprobs: Optional[bool],\n    ) -> Optional[\"SpeculativeConfig\"]:\n        \"\"\"Create a SpeculativeConfig if possible, else return None.\n\n        This function attempts to create a SpeculativeConfig object based on the\n        provided parameters. If the necessary conditions are met, it returns an\n        instance of SpeculativeConfig. Otherwise, it returns None.\n\n        Args:\n            target_model_config (ModelConfig): The configuration of the target\n                model.\n            target_parallel_config (ParallelConfig): The parallel configuration\n                for the target model.\n            target_dtype (str): The data type used for the target model.\n            speculative_model (Optional[str]): The name of the speculative\n                model, if provided.\n            speculative_model_quantization (Optional[str]): Quantization method\n                that was used to quantize the speculative model weights. If\n                None, we assume the model weights are not quantized.\n            speculative_draft_tensor_parallel_size (Optional[int]): The degree\n                of the tensor parallelism for the draft model.\n            num_speculative_tokens (Optional[int]): The number of speculative\n                tokens, if provided. Will default to the number in the draft\n                model config if present, otherwise is required.\n            speculative_disable_mqa_scorer (Optional[bool]): Disable the MQA\n                scorer for the speculative model and fall back to batch\n                expansion for scoring.\n            speculative_max_model_len (Optional[int]): The maximum model len of\n                the speculative model. Used when testing the ability to skip\n                speculation for some sequences.\n            enable_chunked_prefill (bool): Whether vLLM is configured to use\n                chunked prefill or not. Used for raising an error since its not\n                yet compatible with spec decode.\n            use_v2_block_manager (bool): Whether vLLM is configured to use the\n                v2 block manager or not. Used for raising an error since the v2\n                block manager is required with spec decode.\n            speculative_disable_by_batch_size (Optional[int]): Disable\n                speculative decoding for new incoming requests when the number\n                of enqueue requests  is larger than this value, if provided.\n            ngram_prompt_lookup_max (Optional[int]): Max size of ngram token\n                window, if provided.\n            ngram_prompt_lookup_min (Optional[int]): Min size of ngram token\n                window, if provided.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the \n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs (Optional[bool]): If set to True, token log\n                probabilities are not returned during speculative decoding.\n                If set to False, token log probabilities are returned\n                according to the log probability settings in SamplingParams.\n                If not specified, it defaults to True.\n    \n        Returns:\n            Optional[\"SpeculativeConfig\"]: An instance of SpeculativeConfig if\n                the necessary conditions are met, else None.\n        \"\"\"\n\n        if speculative_model is None:\n            if num_speculative_tokens is not None:\n                raise ValueError(\"num_speculative_tokens was provided without \"\n                                 \"speculative_model.\")\n            return None\n\n        if (speculative_disable_by_batch_size is not None\n                and speculative_disable_by_batch_size < 2):\n            raise ValueError(\"Expect the batch size threshold of disabling \"\n                             \"speculative decoding is > 1, but got \"\n                             f\"{speculative_disable_by_batch_size=}\")\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if enable_chunked_prefill:\n            raise ValueError(\n                \"Speculative decoding and chunked prefill are \"\n                f\"currently mutually exclusive ({enable_chunked_prefill=}).\")\n\n        if not use_v2_block_manager:\n            raise ValueError(\n                \"Speculative decoding requires usage of the V2 \"\n                \"block manager. Enable it with --use-v2-block-manager.\")\n\n        # TODO: The user should be able to specify revision/max model len\n        # for the draft model. It is not currently supported.\n        draft_revision = None\n        draft_code_revision = None\n        draft_quantization = speculative_model_quantization\n\n        if speculative_model == \"[ngram]\":\n            if ngram_prompt_lookup_min is None:\n                ngram_prompt_lookup_min = 1\n            if ngram_prompt_lookup_max is None or ngram_prompt_lookup_max < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_max=} must be > 0\")\n            if ngram_prompt_lookup_min < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} must be > 0\")\n            if ngram_prompt_lookup_min > ngram_prompt_lookup_max:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} cannot be \"\n                                 f\"larger than {ngram_prompt_lookup_max=}\")\n\n            # TODO: current we still need extract vocab_size from target model\n            # config, in future, we may try refactor it out, and set\n            # draft related config as None here.\n            draft_model_config = target_model_config\n            draft_parallel_config = target_parallel_config\n        else:\n            ngram_prompt_lookup_max = 0\n            ngram_prompt_lookup_min = 0\n            draft_model_config = ModelConfig(\n                model=speculative_model,\n                tokenizer=target_model_config.tokenizer,\n                tokenizer_mode=target_model_config.tokenizer_mode,\n                trust_remote_code=target_model_config.trust_remote_code,\n                dtype=target_model_config.dtype,\n                seed=target_model_config.seed,\n                revision=draft_revision,\n                code_revision=draft_code_revision,\n                tokenizer_revision=target_model_config.tokenizer_revision,\n                max_model_len=None,\n                spec_target_max_model_len=target_model_config.max_model_len,\n                quantization=draft_quantization,\n                enforce_eager=target_model_config.enforce_eager,\n                max_seq_len_to_capture=target_model_config.\n                max_seq_len_to_capture,\n                max_logprobs=target_model_config.max_logprobs,\n            )\n\n            draft_hf_config = draft_model_config.hf_config\n\n            if (num_speculative_tokens is not None\n                    and hasattr(draft_hf_config, \"num_lookahead_tokens\")):\n                draft_hf_config.num_lookahead_tokens = num_speculative_tokens\n\n            n_predict = getattr(draft_hf_config, \"n_predict\", None)\n            if n_predict is not None:\n                if num_speculative_tokens is None:\n                    # Default to max value defined in draft model config.\n                    num_speculative_tokens = n_predict\n                elif num_speculative_tokens > n_predict:\n                    # Verify provided value doesn't exceed the maximum\n                    # supported by the draft model.\n                    raise ValueError(\n                        \"This speculative model supports a maximum of \"\n                        f\"num_speculative_tokens={n_predict}, but \"\n                        f\"{num_speculative_tokens=} was provided.\")\n\n            draft_model_config.max_model_len = (\n                SpeculativeConfig._maybe_override_draft_max_model_len(\n                    speculative_max_model_len,\n                    draft_model_config.max_model_len,\n                    target_model_config.max_model_len,\n                ))\n\n            draft_parallel_config = (\n                SpeculativeConfig.create_draft_parallel_config(\n                    target_parallel_config,\n                    speculative_draft_tensor_parallel_size, draft_hf_config))\n\n        if num_speculative_tokens is None:\n            raise ValueError(\n                \"num_speculative_tokens must be provided with \"\n                \"speculative_model unless the draft model config contains an \"\n                \"n_predict parameter.\")\n\n        if typical_acceptance_sampler_posterior_threshold is None:\n            typical_acceptance_sampler_posterior_threshold = 0.09\n        if typical_acceptance_sampler_posterior_alpha is None:\n            typical_acceptance_sampler_posterior_alpha = 0.3\n        if disable_logprobs is None:\n            disable_logprobs = True\n\n        return SpeculativeConfig(\n            draft_model_config,\n            draft_parallel_config,\n            num_speculative_tokens,\n            speculative_disable_mqa_scorer,\n            speculative_disable_by_batch_size,\n            ngram_prompt_lookup_max,\n            ngram_prompt_lookup_min,\n            draft_token_acceptance_method=draft_token_acceptance_method,\n            typical_acceptance_sampler_posterior_threshold=\\\n                typical_acceptance_sampler_posterior_threshold,\n            typical_acceptance_sampler_posterior_alpha=\\\n                typical_acceptance_sampler_posterior_alpha,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n        )\n\n    @staticmethod\n    def _maybe_override_draft_max_model_len(\n        speculative_max_model_len: Optional[int],\n        draft_max_model_len: int,\n        target_max_model_len: int,\n    ) -> int:\n        \"\"\"Determine the max sequence len for the draft model. This is usually\n        the draft_max_model_len, but may be the target_max_model_len if it is\n        less than the draft_max_model_len, or may be speculative_max_model_len\n        if it is specified.\n\n        This is necessary so that sequences do not exceed the capacity of the\n        draft model or the target model.\n\n        speculative_max_model_len is mainly used for testing that sequences can\n        skip speculation.\n        \"\"\"\n\n        if speculative_max_model_len is not None:\n\n            if speculative_max_model_len > draft_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {draft_max_model_len=}\")\n\n            if speculative_max_model_len > target_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {target_max_model_len=}\")\n\n            return speculative_max_model_len\n\n        return min(\n            draft_max_model_len,\n            target_max_model_len,\n        )\n\n    @staticmethod\n    def create_draft_parallel_config(\n        target_parallel_config: ParallelConfig,\n        speculative_draft_tensor_parallel_size: Optional[int],\n        draft_hf_config: PretrainedConfig,\n    ) -> ParallelConfig:\n        \"\"\"Create a parallel config for use by the draft worker.\n\n        This is mostly a copy of the target parallel config, except the tp_size.\n        \"\"\"\n        if speculative_draft_tensor_parallel_size is None:\n            if draft_hf_config.model_type == \"mlp_speculator\":\n                speculative_draft_tensor_parallel_size = 1\n                if target_parallel_config.tensor_parallel_size > 1:\n                    logger.warning(\n                        \"MLPSpeculator cannot currently be run with tp>1; \"\n                        \"setting speculative_draft_tensor_parallel_size=1\")\n            else:\n                speculative_draft_tensor_parallel_size = \\\n                    target_parallel_config.tensor_parallel_size\n        elif speculative_draft_tensor_parallel_size != 1:\n            # TODO(wooyeon): allow tp values larger than 1\n            raise ValueError(\n                f\"{speculative_draft_tensor_parallel_size=} cannot be \"\n                f\"other value than 1\")\n\n        draft_parallel_config = ParallelConfig(\n            pipeline_parallel_size=target_parallel_config.\n            pipeline_parallel_size,\n            tensor_parallel_size=speculative_draft_tensor_parallel_size,\n            distributed_executor_backend=target_parallel_config.\n            distributed_executor_backend,\n            max_parallel_loading_workers=target_parallel_config.\n            max_parallel_loading_workers,\n            disable_custom_all_reduce=target_parallel_config.\n            disable_custom_all_reduce,\n            tokenizer_pool_config=target_parallel_config.tokenizer_pool_config,\n            ray_workers_use_nsight=target_parallel_config.\n            ray_workers_use_nsight,\n            placement_group=target_parallel_config.placement_group,\n        )\n\n        return draft_parallel_config\n\n    def __init__(\n        self,\n        draft_model_config: ModelConfig,\n        draft_parallel_config: ParallelConfig,\n        num_speculative_tokens: int,\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ):\n        \"\"\"Create a SpeculativeConfig object.\n\n        Args:\n            draft_model_config: ModelConfig for the draft model.\n            draft_parallel_config: ParallelConfig for the draft model.\n            num_speculative_tokens: The number of tokens to sample from the\n                draft model before scoring with the target model.\n            speculative_disable_by_batch_size: Disable speculative\n                decoding for new incoming requests when the number of\n                enqueue requests is larger than this value.\n            ngram_prompt_lookup_max: Max size of ngram token window.\n            ngram_prompt_lookup_min: Min size of ngram token window.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the \n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will not\n                be returned even if requested by sampling parameters. This \n                reduces latency by skipping logprob calculation in proposal\n                sampling, target sampling, and after accepted tokens are\n                determined. If set to False, log probabilities will be\n                returned.\n            disable_log_stats: Whether to disable periodic printing of stage\n                times in speculative decoding.\n        \"\"\"\n        self.draft_model_config = draft_model_config\n        self.draft_parallel_config = draft_parallel_config\n        self.num_speculative_tokens = num_speculative_tokens\n        self.speculative_disable_mqa_scorer = speculative_disable_mqa_scorer\n        self.speculative_disable_by_batch_size = \\\n            speculative_disable_by_batch_size\n        self.ngram_prompt_lookup_max = ngram_prompt_lookup_max or 0\n        self.ngram_prompt_lookup_min = ngram_prompt_lookup_min or 0\n        self.draft_token_acceptance_method = draft_token_acceptance_method\n        self.typical_acceptance_sampler_posterior_threshold = \\\n            typical_acceptance_sampler_posterior_threshold\n        self.typical_acceptance_sampler_posterior_alpha = \\\n            typical_acceptance_sampler_posterior_alpha\n        self.disable_logprobs = disable_logprobs\n        self.disable_log_stats = disable_log_stats\n\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.num_speculative_tokens <= 0:\n            raise ValueError(\"Expected num_speculative_tokens to be greater \"\n                             f\"than zero ({self.num_speculative_tokens}).\")\n\n        if self.draft_model_config:\n            self.draft_model_config.verify_with_parallel_config(\n                self.draft_parallel_config)\n            # Validate and set draft token acceptance related settings.\n\n        if (self.draft_token_acceptance_method is None):\n            raise ValueError(\"draft_token_acceptance_method is not set. \"\n                             \"Expected values are rejection_sampler or \"\n                             \"typical_acceptance_sampler.\")\n\n        if (self.draft_token_acceptance_method != 'rejection_sampler'\n                and self.draft_token_acceptance_method !=\n                'typical_acceptance_sampler'):\n            raise ValueError(\n                \"Expected draft_token_acceptance_method to be either \"\n                \"rejection_sampler or typical_acceptance_sampler. Instead it \"\n                f\"is {self.draft_token_acceptance_method}\")\n\n        if (self.typical_acceptance_sampler_posterior_threshold < 0\n                or self.typical_acceptance_sampler_posterior_alpha < 0):\n            raise ValueError(\n                \"Expected typical_acceptance_sampler_posterior_threshold \"\n                \"and typical_acceptance_sampler_posterior_alpha to be > 0. \"\n                \"Instead found \"\n                f\"typical_acceptance_sampler_posterior_threshold = \"\n                f\"{self.typical_acceptance_sampler_posterior_threshold} and \"\n                f\"typical_acceptance_sampler_posterior_alpha = \"\n                f\"{self.typical_acceptance_sampler_posterior_alpha}\")\n\n    @property\n    def num_lookahead_slots(self) -> int:\n        \"\"\"The number of additional slots the scheduler should allocate per\n        step, in addition to the slots allocated for each known token.\n\n        This is equal to the number of speculative tokens, as each speculative\n        token must be scored.\n        \"\"\"\n        return self.num_speculative_tokens\n\n    def __repr__(self) -> str:\n        if self.ngram_prompt_lookup_max > 0:\n            draft_model = \"[ngram]\"\n        else:\n            draft_model = self.draft_model_config.model\n        num_spec_tokens = self.num_speculative_tokens\n        return f\"SpeculativeConfig({draft_model=}, {num_spec_tokens=})\"\n\n\n@dataclass\nclass LoRAConfig:\n    max_lora_rank: int\n    max_loras: int\n    fully_sharded_loras: bool = False\n    max_cpu_loras: Optional[int] = None\n    lora_dtype: Optional[Union[torch.dtype, str]] = None\n    lora_extra_vocab_size: int = 256\n    # This is a constant.\n    lora_vocab_padding_size: ClassVar[int] = 256\n    long_lora_scaling_factors: Optional[Tuple[float]] = None\n\n    def __post_init__(self):\n        # Setting the maximum rank to 256 should be able to satisfy the vast\n        # majority of applications.\n        possible_max_ranks = (8, 16, 32, 64, 128, 256)\n        possible_lora_extra_vocab_size = (0, 256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n        if model_config.quantization and model_config.quantization not in [\n                \"awq\", \"gptq\"\n        ]:\n            # TODO support marlin\n            logger.warning(\"%s quantization is not tested with LoRA yet.\",\n                           model_config.quantization)\n\n    def verify_with_scheduler_config(self, scheduler_config: SchedulerConfig):\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if scheduler_config.chunked_prefill_enabled:\n            raise ValueError(\"LoRA is not supported with chunked prefill yet.\")\n\n\n@dataclass\nclass PromptAdapterConfig:\n    max_prompt_adapters: int\n    max_prompt_adapter_token: int\n    max_cpu_prompt_adapters: Optional[int] = None\n    prompt_adapter_dtype: Optional[torch.dtype] = None\n\n    def __post_init__(self):\n\n        if self.max_prompt_adapters < 1:\n            raise ValueError(f\"max_prompt_adapters \"\n                             f\"({self.max_prompt_adapters}) must be >= 1.\")\n        if self.max_prompt_adapter_token == 0:\n            raise ValueError(\"max_prompt_adapter_token must be set.\")\n        if self.max_cpu_prompt_adapters is None:\n            self.max_cpu_prompt_adapters = self.max_prompt_adapters\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.prompt_adapter_dtype in (None, \"auto\"):\n            self.prompt_adapter_dtype = model_config.dtype\n        elif isinstance(self.prompt_adapter_dtype, str):\n            self.prompt_adapter_dtype = getattr(torch,\n                                                self.prompt_adapter_dtype)\n\n\n@dataclass\nclass MultiModalConfig:\n    \"\"\"Controls the behavior of multimodal models.\"\"\"\n\n    limit_per_prompt: Mapping[str, int] = field(default_factory=dict)\n    \"\"\"\n    The maximum number of multi-modal input instances allowed per prompt\n    for each :class:`~vllm.multimodal.MultiModalPlugin`.\n    \"\"\"\n\n    # TODO: Add configs to init vision tower or not.\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE: List[str] = []  #\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            if config_dtype == torch.float32:\n                if config.model_type == \"gemma2\":\n                    logger.info(\n                        \"For Gemma 2, we downcast float32 to bfloat16 instead \"\n                        \"of float16 by default. Please specify `dtype` if you \"\n                        \"want to use float16.\")\n                    torch_dtype = torch.bfloat16\n                else:\n                    # Following the common practice, we use float16 for float32\n                    # models.\n                    torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            logger.info(\"Upcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            logger.info(\"Downcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(\"Casting %s to %s.\", config_dtype, torch_dtype)\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n    disable_sliding_window: bool,\n    sliding_window_len: Optional[Union[int, List[Optional[int]]]],\n    spec_target_max_model_len: Optional[int] = None,\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Command-R\n        \"model_max_length\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    # Choose the smallest \"max_length\" from the possible keys.\n    max_len_key = None\n    for key in possible_keys:\n        max_len = getattr(hf_config, key, None)\n        if max_len is not None:\n            max_len_key = key if max_len < derived_max_model_len \\\n                else max_len_key\n            derived_max_model_len = min(derived_max_model_len, max_len)\n\n    # If sliding window is manually disabled, max_length should be less\n    # than the sliding window length in the model config.\n    if disable_sliding_window and sliding_window_len is not None:\n\n        sliding_window_len_min = get_min_sliding_window(sliding_window_len)\n        max_len_key = \"sliding_window\" \\\n            if sliding_window_len_min < derived_max_model_len else max_len_key\n        derived_max_model_len = min(derived_max_model_len,\n                                    sliding_window_len_min)\n\n    # If none of the keys were found in the config, use a default and\n    # log a warning.\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        if spec_target_max_model_len is not None:\n            # If this is a speculative draft model, we use the max model len\n            # from the target model.\n            return spec_target_max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            \"%s. Assuming the model's maximum length is %d.\", possible_keys,\n            default_max_len)\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        # No need to consider \"type\" key because of patch_rope_scaling when\n        # loading HF config\n        rope_type = rope_scaling[\"rope_type\"]\n\n        if rope_type not in (\"su\", \"longrope\", \"llama3\"):\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that supports rope_scaling\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"with rope_scaling. Please raise an issue so we can \"\n                    \"investigate.\")\n\n            # NOTE: rope_type == \"default\" does not define factor\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/modeling_rope_utils.py\n            scaling_factor = rope_scaling.get(\"factor\", 1.0)\n\n            if rope_type == \"yarn\":\n                derived_max_model_len = rope_scaling[\n                    \"original_max_position_embeddings\"]\n            derived_max_model_len *= scaling_factor\n\n    # If the user specified a max length, make sure it is smaller than the\n    # derived length from the HF model config.\n    if max_model_len is None:\n        max_model_len = int(derived_max_model_len)\n    elif max_model_len > derived_max_model_len:\n        # Some models might have a separate key for specifying model_max_length\n        # that will be bigger than derived_max_model_len. We compare user input\n        # with model_max_length and allow this override when it's smaller.\n        model_max_length = getattr(hf_config, \"model_max_length\", None)\n        if model_max_length is not None and max_model_len <= model_max_length:\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that has model_max_length\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"model_max_length in the config. Please raise an issue \"\n                    \"so we can investigate.\")\n        else:\n            msg = (\n                f\"User-specified max_model_len ({max_model_len}) is greater \"\n                f\"than the derived max_model_len ({max_len_key}=\"\n                f\"{derived_max_model_len} or model_max_length=\"\n                f\"{model_max_length} in model's config.json). This may lead \"\n                \"to incorrect model outputs or CUDA errors.\")\n            if envs.VLLM_ALLOW_LONG_MAX_MODEL_LEN:\n                logger.warning(\n                    \"%s Make sure the value is correct and within the \"\n                    \"model context size.\", msg)\n            else:\n                raise ValueError(\n                    f\"{msg} To allow overriding this maximum, set \"\n                    \"the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\")\n    return int(max_model_len)\n\n\ndef get_min_sliding_window(\n        sliding_window: Union[int, List[Optional[int]]]) -> int:\n    if isinstance(sliding_window, list):\n        return min(s for s in sliding_window if s is not None)\n\n    return sliding_window\n\n\ndef get_served_model_name(model: str,\n                          served_model_name: Optional[Union[str, List[str]]]):\n    \"\"\"\n    If the input is a non-empty list, the first model_name in \n    `served_model_name` is taken. \n    If the input is a non-empty string, it is used directly. \n    For cases where the input is either an empty string or an \n    empty list, the fallback is to use `self.model`.\n    \"\"\"\n    if not served_model_name:\n        return model\n    if isinstance(served_model_name, list):\n        return served_model_name[0]\n    return served_model_name\n\n\n@dataclass\nclass DecodingConfig:\n    \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n\n    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n    guided_decoding_backend: str = 'outlines'\n\n    def __post_init__(self):\n        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n        backend = self.guided_decoding_backend\n        if backend not in valid_guided_backends:\n            raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\n                             f\"must be one of {valid_guided_backends}\")\n\n\n@dataclass\nclass ObservabilityConfig:\n    \"\"\"Configuration for observability.\"\"\"\n    otlp_traces_endpoint: Optional[str] = None\n\n    # Collecting detailed timing information for each request can be expensive.\n\n    # If set, collects the model forward time for the request.\n    collect_model_forward_time: bool = False\n\n    # If set, collects the model execute time for the request.\n    collect_model_execute_time: bool = False\n\n    def __post_init__(self):\n        if not is_otel_available() and self.otlp_traces_endpoint is not None:\n            raise ValueError(\n                \"OpenTelemetry is not available. Unable to configure \"\n                \"'otlp_traces_endpoint'. Ensure OpenTelemetry packages are \"\n                f\"installed. Original error:\\n{otel_import_error_traceback}\")\n\n        if ((self.collect_model_forward_time\n             or self.collect_model_execute_time)\n                and self.otlp_traces_endpoint is None):\n            raise ValueError(\n                \"collect_model_forward_time or collect_model_execute_time \"\n                \"requires --otlp-traces-endpoint to be set.\")\n\n\n@dataclass(frozen=True)\nclass EngineConfig:\n    \"\"\"Dataclass which contains all engine-related configuration. This\n    simplifies passing around the distinct configurations in the codebase.\n    \"\"\"\n\n    model_config: ModelConfig\n    cache_config: CacheConfig\n    parallel_config: ParallelConfig\n    scheduler_config: SchedulerConfig\n    device_config: DeviceConfig\n    load_config: LoadConfig\n    lora_config: Optional[LoRAConfig]\n    speculative_config: Optional[SpeculativeConfig]\n    decoding_config: Optional[DecodingConfig]\n    observability_config: Optional[ObservabilityConfig]\n    prompt_adapter_config: Optional[PromptAdapterConfig]\n\n    def __post_init__(self):\n        \"\"\"Verify configs are valid & consistent with each other.\n        \"\"\"\n        self.model_config.verify_async_output_proc(self.parallel_config,\n                                                   self.speculative_config,\n                                                   self.device_config)\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def to_dict(self):\n        \"\"\"Return the configs as a dictionary, for use in **kwargs.\n        \"\"\"\n        return dict(\n            (field.name, getattr(self, field.name)) for field in fields(self))\n",
      "diff": "diff --git a/vllm/config.py b/vllm/config.py\nindex 2e98923a3..4533fb017 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -949,7 +949,6 @@ class SchedulerConfig:\n             iteration.\n         max_model_len: Maximum length of a sequence (including prompt\n             and generated text).\n-        use_v2_block_manager: Whether to use the BlockSpaceManagerV2 or not.\n         num_lookahead_slots: The number of slots to allocate per sequence per\n             step, beyond the known token ids. This is used in speculative\n             decoding to store KV activations of tokens which may or may not be\n@@ -976,7 +975,6 @@ class SchedulerConfig:\n                  max_num_batched_tokens: Optional[int],\n                  max_num_seqs: int,\n                  max_model_len: int,\n-                 use_v2_block_manager: bool = True,\n                  num_lookahead_slots: int = 0,\n                  delay_factor: float = 0.0,\n                  enable_chunked_prefill: bool = False,\n@@ -1026,7 +1024,6 @@ class SchedulerConfig:\n \n         self.max_num_seqs = max_num_seqs\n         self.max_model_len = max_model_len\n-        self.use_v2_block_manager = use_v2_block_manager\n         self.num_lookahead_slots = num_lookahead_slots\n         self.delay_factor = delay_factor\n         self.chunked_prefill_enabled = enable_chunked_prefill\n@@ -1067,18 +1064,6 @@ class SchedulerConfig:\n                 f\"({self.num_scheduler_steps}) must be greater than or \"\n                 \"equal to 1.\")\n \n-        if (not self.use_v2_block_manager \\\n-            and not envs.VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1):\n-            raise ValueError(\n-                \"The use of BlockSpaceManagerV1 is deprecated and will \"\n-                \"be removed in a future release. Please switch to \"\n-                \"BlockSpaceManagerV2 by setting --use-v2-block-manager to \"\n-                \"True. If you wish to suppress this error temporarily, \"\n-                \"you can set the environment variable \"\n-                \"`VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1=1. If your use \"\n-                \"case is not supported in BlockSpaceManagerV2, please \"\n-                \"file an issue with detailed information.\")\n-\n     @property\n     def is_multi_step(self) -> bool:\n         return self.num_scheduler_steps > 1\n@@ -1137,7 +1122,6 @@ class SpeculativeConfig:\n         speculative_disable_mqa_scorer: Optional[bool],\n         speculative_max_model_len: Optional[int],\n         enable_chunked_prefill: bool,\n-        use_v2_block_manager: bool,\n         disable_log_stats: bool,\n         speculative_disable_by_batch_size: Optional[int],\n         ngram_prompt_lookup_max: Optional[int],\n@@ -1178,9 +1162,6 @@ class SpeculativeConfig:\n             enable_chunked_prefill (bool): Whether vLLM is configured to use\n                 chunked prefill or not. Used for raising an error since its not\n                 yet compatible with spec decode.\n-            use_v2_block_manager (bool): Whether vLLM is configured to use the\n-                v2 block manager or not. Used for raising an error since the v2\n-                block manager is required with spec decode.\n             speculative_disable_by_batch_size (Optional[int]): Disable\n                 speculative decoding for new incoming requests when the number\n                 of enqueue requests  is larger than this value, if provided.\n@@ -1231,11 +1212,6 @@ class SpeculativeConfig:\n                 \"Speculative decoding and chunked prefill are \"\n                 f\"currently mutually exclusive ({enable_chunked_prefill=}).\")\n \n-        if not use_v2_block_manager:\n-            raise ValueError(\n-                \"Speculative decoding requires usage of the V2 \"\n-                \"block manager. Enable it with --use-v2-block-manager.\")\n-\n         # TODO: The user should be able to specify revision/max model len\n         # for the draft model. It is not currently supported.\n         draft_revision = None",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 25
    },
    {
      "file_path": "vllm/core/block/utils.py",
      "old_content": "\"\"\"Block manager utils.\"\"\"\nfrom vllm.sequence import SequenceGroup\nfrom vllm.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                        STR_NOT_IMPL_ENC_DEC_SWA)\n\n\ndef _get_block_mgr_sliding_window_attr(block_mgr):\n    '''\n    BlockManagerV1 and BlockManagerV2 have slightly different\n    members related to sliding window attention (SWA). This\n    function extracts the appropriate member to use for determining\n    whether SWA is enabled.\n\n    Arguments:\n\n    * block_mgr: BlockManagerV1 or BlockManagerV2 instance\n    '''\n\n    if hasattr(block_mgr, 'block_sliding_window'):\n        return block_mgr.block_sliding_window\n    if hasattr(block_mgr, 'max_block_sliding_window'):\n        return block_mgr.max_block_sliding_window\n\n    raise AttributeError(\"Block manager instance has neither \" + \\\n                         \"block_sliding_window nor \" + \\\n                         \"max_block_sliding_window attributes.\")\n\n\ndef check_no_caching_or_swa_for_blockmgr_encdec(\n        block_mgr, seq_group: SequenceGroup) -> None:\n    '''\n    Enforce that prefix caching & sliding-window attention (SWA)\n    are currently unsupported *specifically* for encoder/decoder models.\n\n    Raises NotImplementedError if unsupported scenario is detected.\n\n    Arguments:\n\n    * block_mgr: BlockSpaceManager instance\n    * seq_group: SequenceGroup passed to block_mgr\n    '''\n\n    if seq_group.is_encoder_decoder():\n        if _get_block_mgr_sliding_window_attr(block_mgr) is not None:\n            raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_SWA)\n\n        if block_mgr.enable_caching:\n            raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE)\n",
      "diff": "diff --git a/vllm/core/block/utils.py b/vllm/core/block/utils.py\nindex 28839437c..1c6578e4c 100644\n--- a/vllm/core/block/utils.py\n+++ b/vllm/core/block/utils.py\n@@ -4,28 +4,6 @@ from vllm.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                         STR_NOT_IMPL_ENC_DEC_SWA)\n \n \n-def _get_block_mgr_sliding_window_attr(block_mgr):\n-    '''\n-    BlockManagerV1 and BlockManagerV2 have slightly different\n-    members related to sliding window attention (SWA). This\n-    function extracts the appropriate member to use for determining\n-    whether SWA is enabled.\n-\n-    Arguments:\n-\n-    * block_mgr: BlockManagerV1 or BlockManagerV2 instance\n-    '''\n-\n-    if hasattr(block_mgr, 'block_sliding_window'):\n-        return block_mgr.block_sliding_window\n-    if hasattr(block_mgr, 'max_block_sliding_window'):\n-        return block_mgr.max_block_sliding_window\n-\n-    raise AttributeError(\"Block manager instance has neither \" + \\\n-                         \"block_sliding_window nor \" + \\\n-                         \"max_block_sliding_window attributes.\")\n-\n-\n def check_no_caching_or_swa_for_blockmgr_encdec(\n         block_mgr, seq_group: SequenceGroup) -> None:\n     '''\n@@ -41,7 +19,7 @@ def check_no_caching_or_swa_for_blockmgr_encdec(\n     '''\n \n     if seq_group.is_encoder_decoder():\n-        if _get_block_mgr_sliding_window_attr(block_mgr) is not None:\n+        if block_mgr.max_block_sliding_window is not None:\n             raise NotImplementedError(STR_NOT_IMPL_ENC_DEC_SWA)\n \n         if block_mgr.enable_caching:",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 24
    },
    {
      "file_path": "vllm/core/block_manager.py",
      "old_content": "",
      "diff": "diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nnew file mode 100644\nindex 000000000..61ed7afba\n--- /dev/null\n+++ b/vllm/core/block_manager.py\n@@ -0,0 +1,505 @@\n+\"\"\"A block manager that manages token blocks.\"\"\"\n+from typing import Dict, List, Optional\n+from typing import Sequence as GenericSequence\n+from typing import Tuple\n+\n+from vllm.core.block.block_table import BlockTable\n+from vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\n+from vllm.core.block.interfaces import Block\n+from vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,\n+                                                  LastAccessBlocksTracker)\n+from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\n+from vllm.core.interfaces import AllocStatus, BlockSpaceManager\n+from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n+from vllm.utils import Device\n+\n+SeqId = int\n+EncoderSeqId = str\n+\n+\n+class SelfAttnBlockSpaceManager(BlockSpaceManager):\n+    \"\"\"BlockSpaceManager which manages the allocation of KV cache.\n+\n+    It owns responsibility for allocation, swapping, allocating memory for\n+    autoregressively-generated tokens, and other advanced features such as\n+    prefix caching, forking/copy-on-write, and sliding-window memory allocation.\n+\n+    This class implements the design described in\n+    https://github.com/vllm-project/vllm/pull/3492.\n+\n+    Lookahead slots\n+        The block manager has the notion of a \"lookahead slot\". These are slots\n+        in the KV cache that are allocated for a sequence. Unlike the other\n+        allocated slots, the content of these slots is undefined -- the worker\n+        may use the memory allocations in any way.\n+\n+        In practice, a worker could use these lookahead slots to run multiple\n+        forward passes for a single scheduler invocation. Each successive\n+        forward pass would write KV activations to the corresponding lookahead\n+        slot. This allows low inter-token latency use-cases, where the overhead\n+        of continuous batching scheduling is amortized over >1 generated tokens.\n+\n+        Speculative decoding uses lookahead slots to store KV activations of\n+        proposal tokens.\n+\n+        See https://github.com/vllm-project/vllm/pull/3250 for more information\n+        on lookahead scheduling.\n+\n+    Args:\n+        block_size (int): The size of each memory block.\n+        num_gpu_blocks (int): The number of memory blocks allocated on GPU.\n+        num_cpu_blocks (int): The number of memory blocks allocated on CPU.\n+        watermark (float, optional): The threshold used for memory swapping.\n+            Defaults to 0.01.\n+        sliding_window (Optional[int], optional): The size of the sliding\n+            window. Defaults to None.\n+        enable_caching (bool, optional): Flag indicating whether caching is\n+            enabled. Defaults to False.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        block_size: int,\n+        num_gpu_blocks: int,\n+        num_cpu_blocks: int,\n+        watermark: float = 0.01,\n+        sliding_window: Optional[int] = None,\n+        enable_caching: bool = False,\n+    ) -> None:\n+        self.block_size = block_size\n+        self.num_total_gpu_blocks = num_gpu_blocks\n+        self.num_total_cpu_blocks = num_cpu_blocks\n+\n+        self.sliding_window = sliding_window\n+        # max_block_sliding_window is the max number of blocks that need to be\n+        # allocated\n+        self.max_block_sliding_window = None\n+        if sliding_window is not None:\n+            # +1 here because // rounds down\n+            num_blocks = sliding_window // block_size + 1\n+            # +1 here because the last block may not be full,\n+            # and so the sequence stretches one more block at the beginning\n+            # For example, if sliding_window is 3 and block_size is 4,\n+            # we may need 2 blocks when the second block only holds 1 token.\n+            self.max_block_sliding_window = num_blocks + 1\n+\n+        self.watermark = watermark\n+        assert watermark >= 0.0\n+\n+        self.enable_caching = enable_caching\n+\n+        self.watermark_blocks = int(watermark * num_gpu_blocks)\n+\n+        self.block_allocator = CpuGpuBlockAllocator.create(\n+            allocator_type=\"prefix_caching\" if enable_caching else \"naive\",\n+            num_gpu_blocks=num_gpu_blocks,\n+            num_cpu_blocks=num_cpu_blocks,\n+            block_size=block_size,\n+        )\n+\n+        self.block_tables: Dict[SeqId, BlockTable] = {}\n+        self.cross_block_tables: Dict[EncoderSeqId, BlockTable] = {}\n+\n+        self._computed_blocks_tracker = ComputedBlocksTracker(\n+            self.block_allocator)\n+        self._last_access_blocks_tracker = LastAccessBlocksTracker(\n+            self.block_allocator)\n+\n+    def can_allocate(self,\n+                     seq_group: SequenceGroup,\n+                     num_lookahead_slots: int = 0) -> AllocStatus:\n+        # FIXME(woosuk): Here we assume that all sequences in the group share\n+        # the same prompt. This may not be true for preempted sequences.\n+\n+        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n+\n+        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n+        num_required_blocks = BlockTable.get_num_required_blocks(\n+            seq.get_token_ids(),\n+            block_size=self.block_size,\n+            num_lookahead_slots=num_lookahead_slots,\n+        )\n+\n+        if seq_group.is_encoder_decoder():\n+            encoder_seq = seq_group.get_encoder_seq()\n+            assert encoder_seq is not None\n+            num_required_blocks += BlockTable.get_num_required_blocks(\n+                encoder_seq.get_token_ids(),\n+                block_size=self.block_size,\n+            )\n+\n+        if self.max_block_sliding_window is not None:\n+            num_required_blocks = min(num_required_blocks,\n+                                      self.max_block_sliding_window)\n+\n+        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n+            device=Device.GPU)\n+\n+        # Use watermark to avoid frequent cache eviction.\n+        if (self.num_total_gpu_blocks - num_required_blocks <\n+                self.watermark_blocks):\n+            return AllocStatus.NEVER\n+        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n+            return AllocStatus.OK\n+        else:\n+            return AllocStatus.LATER\n+\n+    def _allocate_sequence(self, seq: Sequence) -> BlockTable:\n+        block_table = BlockTable(\n+            block_size=self.block_size,\n+            block_allocator=self.block_allocator,\n+            max_block_sliding_window=self.max_block_sliding_window,\n+        )\n+        if seq.get_token_ids():\n+            # Add blocks to the block table only if the sequence is non empty.\n+            block_table.allocate(seq.get_token_ids())\n+\n+        return block_table\n+\n+    def allocate(self, seq_group: SequenceGroup) -> None:\n+\n+        # Allocate self-attention block tables for decoder sequences\n+        waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n+        assert not (set(seq.seq_id for seq in waiting_seqs)\n+                    & self.block_tables.keys()), \"block table already exists\"\n+\n+        # NOTE: Here we assume that all sequences in the group have the same\n+        # prompt.\n+        seq = waiting_seqs[0]\n+        block_table: BlockTable = self._allocate_sequence(seq)\n+        self.block_tables[seq.seq_id] = block_table\n+\n+        # Track seq\n+        self._computed_blocks_tracker.add_seq(seq.seq_id)\n+        self._last_access_blocks_tracker.add_seq(seq.seq_id)\n+\n+        # Assign the block table for each sequence.\n+        for seq in waiting_seqs[1:]:\n+            self.block_tables[seq.seq_id] = block_table.fork()\n+\n+            # Track seq\n+            self._computed_blocks_tracker.add_seq(seq.seq_id)\n+            self._last_access_blocks_tracker.add_seq(seq.seq_id)\n+\n+        # Allocate cross-attention block table for encoder sequence\n+        #\n+        # NOTE: Here we assume that all sequences in the group have the same\n+        # encoder prompt.\n+        request_id = seq_group.request_id\n+\n+        assert (request_id\n+                not in self.cross_block_tables), \\\n+            \"block table already exists\"\n+\n+        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n+\n+        if seq_group.is_encoder_decoder():\n+            encoder_seq = seq_group.get_encoder_seq()\n+            assert encoder_seq is not None\n+            block_table = self._allocate_sequence(encoder_seq)\n+            self.cross_block_tables[request_id] = block_table\n+\n+    def can_append_slots(self, seq_group: SequenceGroup,\n+                         num_lookahead_slots: int) -> bool:\n+        \"\"\"Determine if there is enough space in the GPU KV cache to continue\n+        generation of the specified sequence group.\n+\n+        We use a worst-case heuristic: assume each touched block will require a\n+        new allocation (either via CoW or new block). We can append slots if the\n+        number of touched blocks is less than the number of free blocks.\n+\n+        \"Lookahead slots\" are slots that are allocated in addition to the slots\n+        for known tokens. The contents of the lookahead slots are not defined.\n+        This is used by speculative decoding when speculating future tokens.\n+        \"\"\"\n+\n+        num_touched_blocks = 0\n+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            block_table = self.block_tables[seq.seq_id]\n+\n+            num_touched_blocks += (\n+                block_table.get_num_blocks_touched_by_append_slots(\n+                    token_ids=block_table.get_unseen_token_ids(\n+                        seq.get_token_ids()),\n+                    num_lookahead_slots=num_lookahead_slots,\n+                ))\n+\n+        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n+            Device.GPU)\n+        return num_touched_blocks <= num_free_gpu_blocks\n+\n+    def append_slots(\n+        self,\n+        seq: Sequence,\n+        num_lookahead_slots: int,\n+    ) -> List[Tuple[int, int]]:\n+\n+        block_table = self.block_tables[seq.seq_id]\n+\n+        block_table.append_token_ids(\n+            token_ids=block_table.get_unseen_token_ids(seq.get_token_ids()),\n+            num_lookahead_slots=num_lookahead_slots,\n+            num_computed_slots=seq.data.get_num_computed_tokens(),\n+        )\n+        # Return any new copy-on-writes.\n+        new_cows = self.block_allocator.clear_copy_on_writes()\n+        return new_cows\n+\n+    def free(self, seq: Sequence) -> None:\n+        seq_id = seq.seq_id\n+\n+        if seq_id not in self.block_tables:\n+            # Already freed or haven't been scheduled yet.\n+            return\n+\n+        # Update seq block ids with the latest access time\n+        self._last_access_blocks_tracker.update_seq_blocks_last_access(\n+            seq_id, self.block_tables[seq.seq_id].physical_block_ids)\n+\n+        # Untrack seq\n+        self._last_access_blocks_tracker.remove_seq(seq_id)\n+        self._computed_blocks_tracker.remove_seq(seq_id)\n+\n+        # Free table/blocks\n+        self.block_tables[seq_id].free()\n+        del self.block_tables[seq_id]\n+\n+    def free_cross(self, seq_group: SequenceGroup) -> None:\n+        request_id = seq_group.request_id\n+        if request_id not in self.cross_block_tables:\n+            # Already freed or hasn't been scheduled yet.\n+            return\n+        self.cross_block_tables[request_id].free()\n+        del self.cross_block_tables[request_id]\n+\n+    def get_block_table(self, seq: Sequence) -> List[int]:\n+        block_ids = self.block_tables[seq.seq_id].physical_block_ids\n+        return block_ids  # type: ignore\n+\n+    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n+        request_id = seq_group.request_id\n+        assert request_id in self.cross_block_tables\n+        block_ids = self.cross_block_tables[request_id].physical_block_ids\n+        assert all(b is not None for b in block_ids)\n+        return block_ids  # type: ignore\n+\n+    def access_all_blocks_in_seq(self, seq: Sequence, now: float):\n+        if self.enable_caching:\n+            # Record the latest access time for the sequence. The actual update\n+            # of the block ids is deferred to the sequence free(..) call, since\n+            # only during freeing of block ids, the blocks are actually added to\n+            # the evictor (which is when the most updated time is required)\n+            # (This avoids expensive calls to mark_blocks_as_accessed(..))\n+            self._last_access_blocks_tracker.update_last_access(\n+                seq.seq_id, now)\n+\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n+        # If prefix caching is enabled, mark immutable blocks as computed\n+        # right after they have been scheduled (for prefill). This assumes\n+        # the scheduler is synchronous so blocks are actually computed when\n+        # scheduling the next batch.\n+        self.block_allocator.mark_blocks_as_computed([])\n+\n+    def get_common_computed_block_ids(\n+            self, seqs: List[Sequence]) -> GenericSequence[int]:\n+        \"\"\"Determine which blocks for which we skip prefill.\n+\n+        With prefix caching we can skip prefill for previously-generated blocks.\n+        Currently, the attention implementation only supports skipping cached\n+        blocks if they are a contiguous prefix of cached blocks.\n+\n+        This method determines which blocks can be safely skipped for all\n+        sequences in the sequence group.\n+        \"\"\"\n+        computed_seq_block_ids = []\n+        for seq in seqs:\n+            computed_seq_block_ids.append(\n+                self._computed_blocks_tracker.\n+                get_cached_computed_blocks_and_update(\n+                    seq.seq_id,\n+                    self.block_tables[seq.seq_id].physical_block_ids))\n+\n+        # NOTE(sang): This assumes seq_block_ids doesn't contain any None.\n+        return self.block_allocator.get_common_computed_block_ids(\n+            computed_seq_block_ids)  # type: ignore\n+\n+    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n+        if parent_seq.seq_id not in self.block_tables:\n+            # Parent sequence has either been freed or never existed.\n+            return\n+        src_block_table = self.block_tables[parent_seq.seq_id]\n+        self.block_tables[child_seq.seq_id] = src_block_table.fork()\n+\n+        # Track child seq\n+        self._computed_blocks_tracker.add_seq(child_seq.seq_id)\n+        self._last_access_blocks_tracker.add_seq(child_seq.seq_id)\n+\n+    def can_swap_in(self, seq_group: SequenceGroup,\n+                    num_lookahead_slots: int) -> AllocStatus:\n+        \"\"\"Returns the AllocStatus for the given sequence_group \n+        with num_lookahead_slots.\n+\n+        Args:\n+            sequence_group (SequenceGroup): The sequence group to swap in.\n+            num_lookahead_slots (int): Number of lookahead slots used in \n+                speculative decoding, default to 0.\n+\n+        Returns:\n+            AllocStatus: The AllocStatus for the given sequence group.\n+        \"\"\"\n+        return self._can_swap(seq_group, Device.GPU, SequenceStatus.SWAPPED,\n+                              num_lookahead_slots)\n+\n+    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n+        \"\"\"Returns the block id mapping (from CPU to GPU) generated by\n+        swapping in the given seq_group with num_lookahead_slots.\n+\n+        Args:\n+            seq_group (SequenceGroup): The sequence group to swap in.\n+\n+        Returns:\n+            List[Tuple[int, int]]: The mapping of swapping block from CPU \n+                to GPU.\n+        \"\"\"\n+        physical_block_id_mapping = []\n+        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n+            blocks = self.block_tables[seq.seq_id].blocks\n+            if len(blocks) == 0:\n+                continue\n+\n+            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n+                                                         src_device=Device.CPU,\n+                                                         dst_device=Device.GPU)\n+\n+            # Refresh the block ids of the table (post-swap)\n+            self.block_tables[seq.seq_id].update(blocks)\n+\n+            seq_physical_block_id_mapping = {\n+                self.block_allocator.get_physical_block_id(\n+                    Device.CPU, cpu_block_id):\n+                self.block_allocator.get_physical_block_id(\n+                    Device.GPU, gpu_block_id)\n+                for cpu_block_id, gpu_block_id in seq_swap_mapping.items()\n+            }\n+\n+            physical_block_id_mapping.extend(\n+                list(seq_physical_block_id_mapping.items()))\n+\n+        return physical_block_id_mapping\n+\n+    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n+        \"\"\"Returns whether we can swap out the given sequence_group \n+        with num_lookahead_slots.\n+\n+        Args:\n+            seq_group (SequenceGroup): The sequence group to swap in.\n+            num_lookahead_slots (int): Number of lookahead slots used in \n+                speculative decoding, default to 0.\n+\n+        Returns:\n+            bool: Whether it's possible to swap out current sequence group.\n+        \"\"\"\n+        alloc_status = self._can_swap(seq_group, Device.CPU,\n+                                      SequenceStatus.RUNNING)\n+        return alloc_status == AllocStatus.OK\n+\n+    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n+        \"\"\"Returns the block id mapping (from GPU to CPU) generated by\n+        swapping out the given sequence_group with num_lookahead_slots.\n+\n+        Args:\n+            sequence_group (SequenceGroup): The sequence group to swap in.\n+\n+        Returns:\n+            List[Tuple[int, int]]: The mapping of swapping block from \n+                GPU to CPU.\n+        \"\"\"\n+        physical_block_id_mapping = []\n+        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n+            blocks = self.block_tables[seq.seq_id].blocks\n+            if len(blocks) == 0:\n+                continue\n+\n+            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n+                                                         src_device=Device.GPU,\n+                                                         dst_device=Device.CPU)\n+\n+            # Refresh the block ids of the table (post-swap)\n+            self.block_tables[seq.seq_id].update(blocks)\n+\n+            seq_physical_block_id_mapping = {\n+                self.block_allocator.get_physical_block_id(\n+                    Device.GPU, gpu_block_id):\n+                self.block_allocator.get_physical_block_id(\n+                    Device.CPU, cpu_block_id)\n+                for gpu_block_id, cpu_block_id in seq_swap_mapping.items()\n+            }\n+\n+            physical_block_id_mapping.extend(\n+                list(seq_physical_block_id_mapping.items()))\n+\n+        return physical_block_id_mapping\n+\n+    def get_num_free_gpu_blocks(self) -> int:\n+        return self.block_allocator.get_num_free_blocks(Device.GPU)\n+\n+    def get_num_free_cpu_blocks(self) -> int:\n+        return self.block_allocator.get_num_free_blocks(Device.CPU)\n+\n+    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n+        return self.block_allocator.get_prefix_cache_hit_rate(device)\n+\n+    def _can_swap(self,\n+                  seq_group: SequenceGroup,\n+                  device: Device,\n+                  status: SequenceStatus,\n+                  num_lookahead_slots: int = 0) -> AllocStatus:\n+        \"\"\"Returns the AllocStatus for swapping in/out the given sequence_group \n+        on to the 'device'.\n+\n+        Args:\n+            sequence_group (SequenceGroup): The sequence group to swap in.\n+            device (Device): device to swap the 'seq_group' on.\n+            status (SequenceStatus): The status of sequence which is needed\n+                for action. RUNNING for swap out and SWAPPED for swap in\n+            num_lookahead_slots (int): Number of lookahead slots used in \n+                speculative decoding, default to 0.\n+\n+        Returns:\n+            AllocStatus: The AllocStatus for swapping in/out the given \n+                sequence_group on to the 'device'.\n+        \"\"\"\n+        # First determine the number of blocks that will be touched by this\n+        # swap. Then verify if there are available blocks in the device\n+        # to perform the swap.\n+        num_blocks_touched = 0\n+        blocks: List[Block] = []\n+        for seq in seq_group.get_seqs(status=status):\n+            block_table = self.block_tables[seq.seq_id]\n+            if block_table.blocks is not None:\n+                # Compute the number blocks to touch for the tokens to be\n+                # appended. This does NOT include the full blocks that need\n+                # to be touched for the swap.\n+                num_blocks_touched += \\\n+                    block_table.get_num_blocks_touched_by_append_slots(\n+                        block_table.get_unseen_token_ids(seq.get_token_ids()),\n+                        num_lookahead_slots=num_lookahead_slots)\n+                blocks.extend(block_table.blocks)\n+        # Compute the number of full blocks to touch and add it to the\n+        # existing count of blocks to touch.\n+        num_blocks_touched += self.block_allocator.get_num_full_blocks_touched(\n+            blocks, device=device)\n+\n+        watermark_blocks = 0\n+        if device == Device.GPU:\n+            watermark_blocks = self.watermark_blocks\n+\n+        if self.block_allocator.get_num_total_blocks(\n+                device) < num_blocks_touched:\n+            return AllocStatus.NEVER\n+        elif self.block_allocator.get_num_free_blocks(\n+                device) - num_blocks_touched >= watermark_blocks:\n+            return AllocStatus.OK\n+        else:\n+            return AllocStatus.LATER",
      "change_type": "renamed",
      "lines_added": 506,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/core/block_manager_v1.py",
      "old_content": "\"\"\"A block manager that manages token blocks.\"\"\"\nimport math\nfrom abc import ABC, abstractmethod\nfrom itertools import count, takewhile\nfrom os.path import commonprefix\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple\n\nfrom vllm.block import BlockTable, PhysicalTokenBlock\nfrom vllm.core.block.common import CacheMetricData\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nlogger = init_logger(__name__)\n\n\nclass BlockAllocatorBase(ABC):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n        pass\n\n    @abstractmethod\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        pass\n\n    @abstractmethod\n    def free(self, block: PhysicalTokenBlock) -> None:\n        pass\n\n    @abstractmethod\n    def get_num_free_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_total_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def contains_block(self, block_hash: int) -> bool:\n        pass\n\n    @abstractmethod\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        pass\n\n    @abstractmethod\n    def get_prefix_cache_hit_rate(self) -> float:\n        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n        pass\n\n\nclass CachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        self.current_num_blocks = 0\n        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        self.default_hash_ctr = count()\n\n        self.cache_metric_data = CacheMetricData()\n\n    def allocate_block(self, block_hash: int,\n                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n        if self.current_num_blocks == self.num_blocks:\n            block = self.evictor.evict()\n            block.block_hash = block_hash\n            block.num_hashed_tokens = num_hashed_tokens\n            return block\n        block = PhysicalTokenBlock(device=self.device,\n                                   block_number=self.current_num_blocks,\n                                   block_size=self.block_size,\n                                   block_hash=block_hash,\n                                   num_hashed_tokens=num_hashed_tokens)\n        self.current_num_blocks += 1\n        return block\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if block_hash is None:\n            block_hash = next(self.default_hash_ctr)\n\n        if block_hash in self.evictor:\n            assert block_hash not in self.cached_blocks\n            block = self.evictor.remove(block_hash)\n            assert block.ref_count == 0\n            self.cached_blocks[block_hash] = block\n\n        if block_hash in self.cached_blocks:\n            self.cache_metric_data.query(hit=True)\n        else:\n            self.cache_metric_data.query(hit=False)\n            self.cached_blocks[block_hash] = self.allocate_block(\n                block_hash, num_hashed_tokens)\n        block = self.cached_blocks[block_hash]\n        assert block.block_hash == block_hash\n        block.ref_count += 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            assert block.block_hash not in self.evictor\n            self.evictor.add(block)\n\n            # Remove the block from the cached_blocks\n            del self.cached_blocks[block.block_hash]\n\n    def get_num_free_blocks(self) -> int:\n        return (self.num_blocks - self.current_num_blocks +\n                self.evictor.num_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        return block_hash in self.cached_blocks or block_hash in self.evictor\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        # Update the hash of block and the cached_blocks dictionary.\n        assert not self.contains_block(block_hash)\n        old_hash = block.block_hash\n        block.block_hash = block_hash\n        del self.cached_blocks[old_hash]\n        self.cached_blocks[block_hash] = block\n\n    def get_prefix_cache_hit_rate(self) -> float:\n        return self.cache_metric_data.get_hit_rate()\n\n\nclass UncachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: Device,\n        block_size: int,\n        num_blocks: int,\n    ) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        # Initialize the free blocks.\n        self.free_blocks: List[PhysicalTokenBlock] = []\n        for i in range(num_blocks):\n            block = PhysicalTokenBlock(device=device,\n                                       block_number=i,\n                                       block_size=block_size,\n                                       block_hash=-1,\n                                       num_hashed_tokens=0)\n            self.free_blocks.append(block)\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if not self.free_blocks:\n            raise ValueError(\"Out of memory! No free blocks are available.\")\n        block = self.free_blocks.pop()\n        block.ref_count = 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            self.free_blocks.append(block)\n\n    def get_num_free_blocks(self) -> int:\n        return len(self.free_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def get_prefix_cache_hit_rate(self) -> float:\n        return -1\n\n\nclass BlockSpaceManagerV1(BlockSpaceManager):\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        if enable_caching and sliding_window is not None:\n            raise NotImplementedError(\n                \"Sliding window is not allowed with prefix caching enabled!\")\n\n        self.block_sliding_window = None\n        if sliding_window is not None:\n            # Round up to nearest block size to regularize sliding window\n            # allocation sizes.\n            self.block_sliding_window = math.ceil(sliding_window / block_size)\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        if self.enable_caching:\n            logger.info(\"Automatic prefix caching is enabled.\")\n            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        else:\n            self.gpu_allocator = UncachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator = UncachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        # Mapping: seq_id -> BlockTable.\n        self.block_tables: Dict[int, BlockTable] = {}\n\n        # Mapping: req_id -> BlockTable\n        # Note that each SequenceGroup has a unique\n        # request ID\n        self.cross_block_tables: Dict[str, BlockTable] = {}\n\n    def _get_seq_num_required_blocks(self, seq: Optional[Sequence]) -> int:\n        return 0 if seq is None else seq.n_blocks\n\n    def can_allocate(self,\n                     seq_group: SequenceGroup,\n                     num_lookahead_slots: int = 0) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        assert (num_lookahead_slots == 0\n                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        self_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n        cross_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_encoder_seq())\n        num_required_blocks = self_num_required_blocks + \\\n                              cross_num_required_blocks\n\n        if self.block_sliding_window is not None:\n\n            num_required_blocks = min(num_required_blocks,\n                                      self.block_sliding_window)\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, \\\n                           seq: Optional[Sequence], \\\n                           ref_count: int, \\\n                           is_encoder_decoder: bool = True) -> BlockTable:\n        # Allocate new physical token blocks that will store the prompt tokens.\n        num_prompt_blocks = self._get_seq_num_required_blocks(seq)\n\n        block_table: BlockTable = BlockTable()\n        assert seq is not None\n        for logical_idx in range(num_prompt_blocks):\n            if (self.block_sliding_window is not None\n                    and logical_idx >= self.block_sliding_window):\n                block = block_table[logical_idx % self.block_sliding_window]\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            elif not is_encoder_decoder and self.enable_caching:\n                block = self.gpu_allocator.allocate(\n                    seq.hash_of_block(logical_idx),\n                    seq.num_hashed_tokens_of_block(logical_idx))\n            else:\n                block = self.gpu_allocator.allocate()\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            block_table.append(block)\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        is_encoder_decoder = seq_group.is_encoder_decoder()\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        # Allocate decoder sequences\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # decoder prompt.\n        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        seq = wait_seqs[0]\n        block_table: BlockTable = \\\n            self._allocate_sequence(seq,\n                                    seq_group.num_seqs(),\n                                    is_encoder_decoder)\n\n        # Assign the self-attention block tables for each sequence.\n        if len(wait_seqs) == 1:\n            self.block_tables[seq.seq_id] = block_table\n        else:\n            for seq in wait_seqs:\n                self.block_tables[seq.seq_id] = block_table.copy()\n\n        # Allocate encoder sequence\n        if is_encoder_decoder:\n            # A SequenceGroup has only a single encoder sequence (at most),\n            # thus allocate with a ref count of 1\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n                                                  1, is_encoder_decoder)\n            # Assign the cross-attention block table for the SequenceGroup.\n            self.cross_block_tables[seq_group.request_id] = block_table\n\n    def can_append_slots(self,\n                         seq_group: SequenceGroup,\n                         num_lookahead_slots: int = 0) -> bool:\n        assert (num_lookahead_slots == 0\n                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n\n        # Simple heuristic: If there is at least one free block\n        # for each sequence, we can append.\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n        return num_seqs <= num_free_gpu_blocks\n\n    def _promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        assert self.enable_caching\n\n        # Compute a new hash for the block so that it can be shared by other\n        # Sequences\n        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n\n        # if new_hash is already in the cached table, then free last_block\n        # and return the cached version\n        if self.gpu_allocator.contains_block(new_hash):\n            self.gpu_allocator.free(last_block)\n            return self.gpu_allocator.allocate(new_hash)\n        else:\n            self.gpu_allocator.update_hash(new_hash, last_block)\n            return last_block\n\n    def _is_last_block_full(\n        self,\n        seq: Sequence,\n    ) -> bool:\n        token_ids_len = seq.data.get_len()\n        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n\n    def _maybe_promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        if self._is_last_block_full(seq):\n            return self._promote_last_block(seq, last_block)\n        else:\n            return last_block\n\n    def _allocate_last_physical_block(\n        self,\n        seq: Sequence,\n    ) -> PhysicalTokenBlock:\n        # Called before a new block is appended.\n        # This is in charge of allocating a new physical block (to be appended).\n\n        # None if the last block is not full. Otherwise, we set it to the\n        # content hash.\n        if not self.enable_caching:\n            return self.gpu_allocator.allocate()\n        block_hash: Optional[int] = None\n        n_blocks = seq.n_blocks\n        if (self._is_last_block_full(seq)):\n            block_hash = seq.hash_of_block(n_blocks - 1)\n        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n\n        # num_hashed_tokens is used to compute future hashes\n        # (e.g. in the hashing function, it is used to ask the sequence for\n        # prefix tokens)\n        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n\n        # If the block_hash is None, then the block is not full.\n        # If the block is not full, then we expect it to have a refcount of 1.\n        if block_hash is None:\n            assert new_block.ref_count == 1\n        return new_block\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int = 0,\n    ) -> List[Tuple[int, int]]:\n        \"\"\"Allocate a physical slot for a new token.\"\"\"\n        n_blocks = seq.n_blocks\n        block_table = self.block_tables[seq.seq_id]\n        # If we need to allocate a new physical block\n        if len(block_table) < n_blocks:\n            # Currently this code only supports adding one physical block\n            assert len(block_table) == n_blocks - 1\n\n            if (self.block_sliding_window\n                    and len(block_table) >= self.block_sliding_window):\n                # reuse a block\n                block_table.append(block_table[len(block_table) %\n                                               self.block_sliding_window])\n            else:\n                # The sequence hash a new logical block.\n                # Allocate a new physical block.\n                new_block = self._allocate_last_physical_block(seq)\n                block_table.append(new_block)\n                return []\n\n        # We want to append the token to the last physical block.\n        last_block = block_table[-1]\n        assert last_block.device == Device.GPU\n        if last_block.ref_count == 1:\n            # Not shared with other sequences. Appendable.\n            if self.enable_caching:\n                # If the last block is now complete, we may reuse an old block\n                # to save memory.\n                maybe_new_block = self._maybe_promote_last_block(\n                    seq, last_block)\n                block_table[-1] = maybe_new_block\n            return []\n        else:\n            # The last block is shared with other sequences.\n            # Copy on Write: Allocate a new block and copy the tokens.\n            new_block = self._allocate_last_physical_block(seq)\n\n            block_table[-1] = new_block\n            self.gpu_allocator.free(last_block)\n            return [(last_block.block_number, new_block.block_number)]\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        # NOTE: fork does not allocate a new physical block.\n        # Thus, it is always safe from OOM.\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n\n        # When using a sliding window, blocks will be eventually reused.\n        # In this case the block tables will contain repeated blocks.\n        # When forking, we must make sure that each block's `ref_count`\n        # is only incremented by one, so we deduplicate them by wrapping\n        # them in a set.\n        for block in set(src_block_table):\n            block.ref_count += 1\n\n    def _get_physical_blocks(\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n\n        # NOTE: Here, we assume that the physical blocks are only shared by\n        # the sequences in the same group.\n        request_id = seq_group.request_id\n        blocks: Set[PhysicalTokenBlock] = set()\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                continue\n            blocks.update(self.block_tables[seq.seq_id])\n        # Cross-attention blocks\n        if seq_group.is_encoder_decoder():\n            blocks.update(self.cross_block_tables[request_id])\n        return list(blocks)\n\n    def can_swap_in(self,\n                    seq_group: SequenceGroup,\n                    num_lookahead_slots: int = 0) -> AllocStatus:\n        assert (num_lookahead_slots == 0\n                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n\n        blocks = self._get_physical_blocks(seq_group)\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n        if seq_group.is_encoder_decoder():\n            num_swapped_seqs += 1\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n        # NOTE: Conservatively, we assume that every sequence will allocate\n        # at least one free block right after the swap-in.\n        # NOTE: This should match the logic in can_append_slot().\n        num_required_blocks = len(blocks) + num_swapped_seqs\n        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n            return AllocStatus.NEVER\n        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _swap_block_table(\n            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n            dest_allocator: BlockAllocatorBase,\n            mapping: Dict[PhysicalTokenBlock,\n                          PhysicalTokenBlock]) -> BlockTable:\n        new_block_table: BlockTable = BlockTable()\n\n        for from_block in block_table:\n            if from_block in mapping:\n                to_block = mapping[from_block]\n                to_block.ref_count += 1\n            else:\n                to_block = dest_allocator.allocate(\n                    from_block.block_hash, from_block.num_hashed_tokens)\n                mapping[from_block] = to_block\n            new_block_table.append(to_block)\n            # Free the source block swapped in to destination.\n            src_allocator.free(from_block)\n\n        return new_block_table\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n\n        request_id = seq_group.request_id\n\n        # CPU block -> GPU block.\n        # dict is efficient in lookup `if cpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.cpu_allocator, self.gpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.cpu_allocator,\n                                       self.gpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        request_id = seq_group.request_id\n\n        # GPU block -> CPU block.\n        # dict is efficient in lookup `if gpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.gpu_allocator, self.cpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.gpu_allocator,\n                                       self.cpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def _free_block_table(self, block_table: BlockTable) -> None:\n        # when using a sliding window, each seq will only use up\n        # to `self.block_sliding_window` blocks. When freeing\n        # the block table, we must make sure to not free blocks more\n        # than once. If no sliding window is used, there is no block\n        # reuse in the block table, so we must free all blocks.\n        blocks_to_free = (block_table[-self.block_sliding_window:]\n                          if self.block_sliding_window is not None else\n                          block_table)\n        for block in set(blocks_to_free):\n            if block.device == Device.GPU:\n                self.gpu_allocator.free(block)\n            else:\n                self.cpu_allocator.free(block)\n\n    def free(self, seq: Sequence) -> None:\n        if seq.seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n        block_table = self.block_tables[seq.seq_id]\n        self._free_block_table(block_table)\n        del self.block_tables[seq.seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        if seq_group.request_id not in self.cross_block_tables:\n            # Already freed or hasn't ben scheduled yet.\n            return\n        block_table = self.cross_block_tables[seq_group.request_id]\n        self._free_block_table(block_table)\n        del self.cross_block_tables[seq_group.request_id]\n\n    def reset(self) -> None:\n        # Free decoder block tables\n        for block_table in self.block_tables.values():\n            self._free_block_table(block_table)\n        self.block_tables.clear()\n        # Free cross-attention block tables\n        for block_table in self.cross_block_tables.values():\n            self._free_block_table(block_table)\n        self.cross_block_tables.clear()\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        return self.block_tables[seq.seq_id].ids()\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        block_table = self.cross_block_tables[seq_group.request_id]\n        return [block.block_number for block in block_table]\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.gpu_allocator.get_num_free_blocks()\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.cpu_allocator.get_num_free_blocks()\n\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        if self.enable_caching:\n            # Update the last accessed time of all the blocks accessed\n            # in this step.\n            block_table = self.block_tables[seq.seq_id]\n            for block in block_table:\n                block.last_accessed = access_time\n\n    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n        if seq.seq_id not in self.block_tables:\n            return\n\n        # When chunked prefill is enabled, the computed full blocks\n        # should be calculated based on the number of computed tokens.\n        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n                               token_chunk_size)\n        computed_full_blocks = max_computed_tokens // self.block_size\n\n        block_table = self.block_tables[seq.seq_id]\n        if computed_full_blocks == 0:\n            return\n        for i in reversed(range(computed_full_blocks)):\n            if block_table[i].computed:\n                break\n            block_table[i].computed = True\n\n    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n        if seq.seq_id not in self.block_tables:\n            return []\n        block_table = self.block_tables[seq.seq_id]\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n        return [\n            b.block_number\n            for b in takewhile(lambda b: b.computed, block_table[:-1])\n        ]\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Return the block ids that are common for a given sequence group.\n\n        Used in prefill (can skip prefill of some blocks).\n        \"\"\"\n        # Can return non-empty result only with prefix caching enabled.\n        if not self.enable_caching:\n            return []\n\n        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n        return commonprefix([ids for ids in ids_list if ids != []])\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n                                token_chunk_size: int):\n        if self.enable_caching:\n            for seq in seq_group.get_seqs():\n                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        if device == Device.GPU:\n            return self.gpu_allocator.get_prefix_cache_hit_rate()\n        if device == Device.CPU:\n            return self.cpu_allocator.get_prefix_cache_hit_rate()\n        raise ValueError(f\"Invalid device: {device}\")\n",
      "diff": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\ndeleted file mode 100644\nindex 8bc0ce2bc..000000000\n--- a/vllm/core/block_manager_v1.py\n+++ /dev/null\n@@ -1,743 +0,0 @@\n-\"\"\"A block manager that manages token blocks.\"\"\"\n-import math\n-from abc import ABC, abstractmethod\n-from itertools import count, takewhile\n-from os.path import commonprefix\n-from typing import Dict, List, Optional\n-from typing import Sequence as GenericSequence\n-from typing import Set, Tuple\n-\n-from vllm.block import BlockTable, PhysicalTokenBlock\n-from vllm.core.block.common import CacheMetricData\n-from vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\n-from vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\n-from vllm.core.interfaces import AllocStatus, BlockSpaceManager\n-from vllm.logger import init_logger\n-from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n-from vllm.utils import Device\n-\n-logger = init_logger(__name__)\n-\n-\n-class BlockAllocatorBase(ABC):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    @abstractmethod\n-    def __init__(self,\n-                 device: Device,\n-                 block_size: int,\n-                 num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n-        pass\n-\n-    @abstractmethod\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        pass\n-\n-    @abstractmethod\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        pass\n-\n-    @abstractmethod\n-    def get_num_free_blocks(self) -> int:\n-        pass\n-\n-    @abstractmethod\n-    def get_num_total_blocks(self) -> int:\n-        pass\n-\n-    @abstractmethod\n-    def contains_block(self, block_hash: int) -> bool:\n-        pass\n-\n-    @abstractmethod\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        pass\n-\n-    @abstractmethod\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n-        pass\n-\n-\n-class CachedBlockAllocator(BlockAllocatorBase):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    def __init__(self,\n-                 device: Device,\n-                 block_size: int,\n-                 num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n-        self.device = device\n-        self.block_size = block_size\n-        self.num_blocks = num_blocks\n-\n-        self.current_num_blocks = 0\n-        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n-\n-        self.evictor: Evictor = make_evictor(eviction_policy)\n-\n-        self.default_hash_ctr = count()\n-\n-        self.cache_metric_data = CacheMetricData()\n-\n-    def allocate_block(self, block_hash: int,\n-                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n-        if self.current_num_blocks == self.num_blocks:\n-            block = self.evictor.evict()\n-            block.block_hash = block_hash\n-            block.num_hashed_tokens = num_hashed_tokens\n-            return block\n-        block = PhysicalTokenBlock(device=self.device,\n-                                   block_number=self.current_num_blocks,\n-                                   block_size=self.block_size,\n-                                   block_hash=block_hash,\n-                                   num_hashed_tokens=num_hashed_tokens)\n-        self.current_num_blocks += 1\n-        return block\n-\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        if block_hash is None:\n-            block_hash = next(self.default_hash_ctr)\n-\n-        if block_hash in self.evictor:\n-            assert block_hash not in self.cached_blocks\n-            block = self.evictor.remove(block_hash)\n-            assert block.ref_count == 0\n-            self.cached_blocks[block_hash] = block\n-\n-        if block_hash in self.cached_blocks:\n-            self.cache_metric_data.query(hit=True)\n-        else:\n-            self.cache_metric_data.query(hit=False)\n-            self.cached_blocks[block_hash] = self.allocate_block(\n-                block_hash, num_hashed_tokens)\n-        block = self.cached_blocks[block_hash]\n-        assert block.block_hash == block_hash\n-        block.ref_count += 1\n-        return block\n-\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        if block.ref_count == 0:\n-            raise ValueError(f\"Double free! {block} is already freed.\")\n-        block.ref_count -= 1\n-        if block.ref_count == 0:\n-            assert block.block_hash not in self.evictor\n-            self.evictor.add(block)\n-\n-            # Remove the block from the cached_blocks\n-            del self.cached_blocks[block.block_hash]\n-\n-    def get_num_free_blocks(self) -> int:\n-        return (self.num_blocks - self.current_num_blocks +\n-                self.evictor.num_blocks)\n-\n-    def get_num_total_blocks(self) -> int:\n-        return self.num_blocks\n-\n-    def contains_block(self, block_hash: int) -> bool:\n-        return block_hash in self.cached_blocks or block_hash in self.evictor\n-\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        # Update the hash of block and the cached_blocks dictionary.\n-        assert not self.contains_block(block_hash)\n-        old_hash = block.block_hash\n-        block.block_hash = block_hash\n-        del self.cached_blocks[old_hash]\n-        self.cached_blocks[block_hash] = block\n-\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        return self.cache_metric_data.get_hit_rate()\n-\n-\n-class UncachedBlockAllocator(BlockAllocatorBase):\n-    \"\"\"Manages free physical token blocks for a device.\n-\n-    The allocator maintains a list of free blocks and allocates a block when\n-    requested. When a block is freed, its reference count is decremented. If\n-    the reference count becomes zero, the block is added back to the free list.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        device: Device,\n-        block_size: int,\n-        num_blocks: int,\n-    ) -> None:\n-        self.device = device\n-        self.block_size = block_size\n-        self.num_blocks = num_blocks\n-\n-        # Initialize the free blocks.\n-        self.free_blocks: List[PhysicalTokenBlock] = []\n-        for i in range(num_blocks):\n-            block = PhysicalTokenBlock(device=device,\n-                                       block_number=i,\n-                                       block_size=block_size,\n-                                       block_hash=-1,\n-                                       num_hashed_tokens=0)\n-            self.free_blocks.append(block)\n-\n-    def allocate(self,\n-                 block_hash: Optional[int] = None,\n-                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        if not self.free_blocks:\n-            raise ValueError(\"Out of memory! No free blocks are available.\")\n-        block = self.free_blocks.pop()\n-        block.ref_count = 1\n-        return block\n-\n-    def free(self, block: PhysicalTokenBlock) -> None:\n-        if block.ref_count == 0:\n-            raise ValueError(f\"Double free! {block} is already freed.\")\n-        block.ref_count -= 1\n-        if block.ref_count == 0:\n-            self.free_blocks.append(block)\n-\n-    def get_num_free_blocks(self) -> int:\n-        return len(self.free_blocks)\n-\n-    def get_num_total_blocks(self) -> int:\n-        return self.num_blocks\n-\n-    def contains_block(self, block_hash: int) -> bool:\n-        raise NotImplementedError(\n-            \"Invalid codepath for uncached block allocator.\")\n-\n-    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        raise NotImplementedError(\n-            \"Invalid codepath for uncached block allocator.\")\n-\n-    def get_prefix_cache_hit_rate(self) -> float:\n-        return -1\n-\n-\n-class BlockSpaceManagerV1(BlockSpaceManager):\n-    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n-\n-    def __init__(\n-        self,\n-        block_size: int,\n-        num_gpu_blocks: int,\n-        num_cpu_blocks: int,\n-        watermark: float = 0.01,\n-        sliding_window: Optional[int] = None,\n-        enable_caching: bool = False,\n-    ) -> None:\n-        self.block_size = block_size\n-        self.num_total_gpu_blocks = num_gpu_blocks\n-        self.num_total_cpu_blocks = num_cpu_blocks\n-\n-        if enable_caching and sliding_window is not None:\n-            raise NotImplementedError(\n-                \"Sliding window is not allowed with prefix caching enabled!\")\n-\n-        self.block_sliding_window = None\n-        if sliding_window is not None:\n-            # Round up to nearest block size to regularize sliding window\n-            # allocation sizes.\n-            self.block_sliding_window = math.ceil(sliding_window / block_size)\n-\n-        self.watermark = watermark\n-        assert watermark >= 0.0\n-\n-        self.enable_caching = enable_caching\n-\n-        self.watermark_blocks = int(watermark * num_gpu_blocks)\n-\n-        if self.enable_caching:\n-            logger.info(\"Automatic prefix caching is enabled.\")\n-            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n-                Device.GPU, block_size, num_gpu_blocks)\n-            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n-                Device.CPU, block_size, num_cpu_blocks)\n-        else:\n-            self.gpu_allocator = UncachedBlockAllocator(\n-                Device.GPU, block_size, num_gpu_blocks)\n-            self.cpu_allocator = UncachedBlockAllocator(\n-                Device.CPU, block_size, num_cpu_blocks)\n-        # Mapping: seq_id -> BlockTable.\n-        self.block_tables: Dict[int, BlockTable] = {}\n-\n-        # Mapping: req_id -> BlockTable\n-        # Note that each SequenceGroup has a unique\n-        # request ID\n-        self.cross_block_tables: Dict[str, BlockTable] = {}\n-\n-    def _get_seq_num_required_blocks(self, seq: Optional[Sequence]) -> int:\n-        return 0 if seq is None else seq.n_blocks\n-\n-    def can_allocate(self,\n-                     seq_group: SequenceGroup,\n-                     num_lookahead_slots: int = 0) -> AllocStatus:\n-        # FIXME(woosuk): Here we assume that all sequences in the group share\n-        # the same prompt. This may not be true for preempted sequences.\n-\n-        assert (num_lookahead_slots == 0\n-                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n-\n-        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n-\n-        self_num_required_blocks = self._get_seq_num_required_blocks(\n-            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n-        cross_num_required_blocks = self._get_seq_num_required_blocks(\n-            seq_group.get_encoder_seq())\n-        num_required_blocks = self_num_required_blocks + \\\n-                              cross_num_required_blocks\n-\n-        if self.block_sliding_window is not None:\n-\n-            num_required_blocks = min(num_required_blocks,\n-                                      self.block_sliding_window)\n-        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n-\n-        # Use watermark to avoid frequent cache eviction.\n-        if (self.num_total_gpu_blocks - num_required_blocks <\n-                self.watermark_blocks):\n-            return AllocStatus.NEVER\n-        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n-            return AllocStatus.OK\n-        else:\n-            return AllocStatus.LATER\n-\n-    def _allocate_sequence(self, \\\n-                           seq: Optional[Sequence], \\\n-                           ref_count: int, \\\n-                           is_encoder_decoder: bool = True) -> BlockTable:\n-        # Allocate new physical token blocks that will store the prompt tokens.\n-        num_prompt_blocks = self._get_seq_num_required_blocks(seq)\n-\n-        block_table: BlockTable = BlockTable()\n-        assert seq is not None\n-        for logical_idx in range(num_prompt_blocks):\n-            if (self.block_sliding_window is not None\n-                    and logical_idx >= self.block_sliding_window):\n-                block = block_table[logical_idx % self.block_sliding_window]\n-                # Set the reference counts of the token blocks.\n-                block.ref_count = ref_count\n-            elif not is_encoder_decoder and self.enable_caching:\n-                block = self.gpu_allocator.allocate(\n-                    seq.hash_of_block(logical_idx),\n-                    seq.num_hashed_tokens_of_block(logical_idx))\n-            else:\n-                block = self.gpu_allocator.allocate()\n-                # Set the reference counts of the token blocks.\n-                block.ref_count = ref_count\n-            block_table.append(block)\n-\n-        return block_table\n-\n-    def allocate(self, seq_group: SequenceGroup) -> None:\n-        is_encoder_decoder = seq_group.is_encoder_decoder()\n-        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n-\n-        # Allocate decoder sequences\n-        #\n-        # NOTE: Here we assume that all sequences in the group have the same\n-        # decoder prompt.\n-        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n-        seq = wait_seqs[0]\n-        block_table: BlockTable = \\\n-            self._allocate_sequence(seq,\n-                                    seq_group.num_seqs(),\n-                                    is_encoder_decoder)\n-\n-        # Assign the self-attention block tables for each sequence.\n-        if len(wait_seqs) == 1:\n-            self.block_tables[seq.seq_id] = block_table\n-        else:\n-            for seq in wait_seqs:\n-                self.block_tables[seq.seq_id] = block_table.copy()\n-\n-        # Allocate encoder sequence\n-        if is_encoder_decoder:\n-            # A SequenceGroup has only a single encoder sequence (at most),\n-            # thus allocate with a ref count of 1\n-            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n-                                                  1, is_encoder_decoder)\n-            # Assign the cross-attention block table for the SequenceGroup.\n-            self.cross_block_tables[seq_group.request_id] = block_table\n-\n-    def can_append_slots(self,\n-                         seq_group: SequenceGroup,\n-                         num_lookahead_slots: int = 0) -> bool:\n-        assert (num_lookahead_slots == 0\n-                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n-\n-        # Simple heuristic: If there is at least one free block\n-        # for each sequence, we can append.\n-        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n-        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n-        return num_seqs <= num_free_gpu_blocks\n-\n-    def _promote_last_block(\n-        self,\n-        seq: Sequence,\n-        last_block: PhysicalTokenBlock,\n-    ) -> PhysicalTokenBlock:\n-        assert self.enable_caching\n-\n-        # Compute a new hash for the block so that it can be shared by other\n-        # Sequences\n-        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n-\n-        # if new_hash is already in the cached table, then free last_block\n-        # and return the cached version\n-        if self.gpu_allocator.contains_block(new_hash):\n-            self.gpu_allocator.free(last_block)\n-            return self.gpu_allocator.allocate(new_hash)\n-        else:\n-            self.gpu_allocator.update_hash(new_hash, last_block)\n-            return last_block\n-\n-    def _is_last_block_full(\n-        self,\n-        seq: Sequence,\n-    ) -> bool:\n-        token_ids_len = seq.data.get_len()\n-        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n-\n-    def _maybe_promote_last_block(\n-        self,\n-        seq: Sequence,\n-        last_block: PhysicalTokenBlock,\n-    ) -> PhysicalTokenBlock:\n-        if self._is_last_block_full(seq):\n-            return self._promote_last_block(seq, last_block)\n-        else:\n-            return last_block\n-\n-    def _allocate_last_physical_block(\n-        self,\n-        seq: Sequence,\n-    ) -> PhysicalTokenBlock:\n-        # Called before a new block is appended.\n-        # This is in charge of allocating a new physical block (to be appended).\n-\n-        # None if the last block is not full. Otherwise, we set it to the\n-        # content hash.\n-        if not self.enable_caching:\n-            return self.gpu_allocator.allocate()\n-        block_hash: Optional[int] = None\n-        n_blocks = seq.n_blocks\n-        if (self._is_last_block_full(seq)):\n-            block_hash = seq.hash_of_block(n_blocks - 1)\n-        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n-\n-        # num_hashed_tokens is used to compute future hashes\n-        # (e.g. in the hashing function, it is used to ask the sequence for\n-        # prefix tokens)\n-        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n-\n-        # If the block_hash is None, then the block is not full.\n-        # If the block is not full, then we expect it to have a refcount of 1.\n-        if block_hash is None:\n-            assert new_block.ref_count == 1\n-        return new_block\n-\n-    def append_slots(\n-        self,\n-        seq: Sequence,\n-        num_lookahead_slots: int = 0,\n-    ) -> List[Tuple[int, int]]:\n-        \"\"\"Allocate a physical slot for a new token.\"\"\"\n-        n_blocks = seq.n_blocks\n-        block_table = self.block_tables[seq.seq_id]\n-        # If we need to allocate a new physical block\n-        if len(block_table) < n_blocks:\n-            # Currently this code only supports adding one physical block\n-            assert len(block_table) == n_blocks - 1\n-\n-            if (self.block_sliding_window\n-                    and len(block_table) >= self.block_sliding_window):\n-                # reuse a block\n-                block_table.append(block_table[len(block_table) %\n-                                               self.block_sliding_window])\n-            else:\n-                # The sequence hash a new logical block.\n-                # Allocate a new physical block.\n-                new_block = self._allocate_last_physical_block(seq)\n-                block_table.append(new_block)\n-                return []\n-\n-        # We want to append the token to the last physical block.\n-        last_block = block_table[-1]\n-        assert last_block.device == Device.GPU\n-        if last_block.ref_count == 1:\n-            # Not shared with other sequences. Appendable.\n-            if self.enable_caching:\n-                # If the last block is now complete, we may reuse an old block\n-                # to save memory.\n-                maybe_new_block = self._maybe_promote_last_block(\n-                    seq, last_block)\n-                block_table[-1] = maybe_new_block\n-            return []\n-        else:\n-            # The last block is shared with other sequences.\n-            # Copy on Write: Allocate a new block and copy the tokens.\n-            new_block = self._allocate_last_physical_block(seq)\n-\n-            block_table[-1] = new_block\n-            self.gpu_allocator.free(last_block)\n-            return [(last_block.block_number, new_block.block_number)]\n-\n-    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n-        # NOTE: fork does not allocate a new physical block.\n-        # Thus, it is always safe from OOM.\n-        if parent_seq.seq_id not in self.block_tables:\n-            # Parent sequence has either been freed or never existed.\n-            return\n-        src_block_table = self.block_tables[parent_seq.seq_id]\n-        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n-\n-        # When using a sliding window, blocks will be eventually reused.\n-        # In this case the block tables will contain repeated blocks.\n-        # When forking, we must make sure that each block's `ref_count`\n-        # is only incremented by one, so we deduplicate them by wrapping\n-        # them in a set.\n-        for block in set(src_block_table):\n-            block.ref_count += 1\n-\n-    def _get_physical_blocks(\n-            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n-\n-        # NOTE: Here, we assume that the physical blocks are only shared by\n-        # the sequences in the same group.\n-        request_id = seq_group.request_id\n-        blocks: Set[PhysicalTokenBlock] = set()\n-        for seq in seq_group.get_seqs():\n-            if seq.is_finished():\n-                continue\n-            blocks.update(self.block_tables[seq.seq_id])\n-        # Cross-attention blocks\n-        if seq_group.is_encoder_decoder():\n-            blocks.update(self.cross_block_tables[request_id])\n-        return list(blocks)\n-\n-    def can_swap_in(self,\n-                    seq_group: SequenceGroup,\n-                    num_lookahead_slots: int = 0) -> AllocStatus:\n-        assert (num_lookahead_slots == 0\n-                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n-\n-        blocks = self._get_physical_blocks(seq_group)\n-        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n-        if seq_group.is_encoder_decoder():\n-            num_swapped_seqs += 1\n-        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n-        # NOTE: Conservatively, we assume that every sequence will allocate\n-        # at least one free block right after the swap-in.\n-        # NOTE: This should match the logic in can_append_slot().\n-        num_required_blocks = len(blocks) + num_swapped_seqs\n-        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n-            return AllocStatus.NEVER\n-        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n-            return AllocStatus.OK\n-        else:\n-            return AllocStatus.LATER\n-\n-    def _swap_block_table(\n-            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n-            dest_allocator: BlockAllocatorBase,\n-            mapping: Dict[PhysicalTokenBlock,\n-                          PhysicalTokenBlock]) -> BlockTable:\n-        new_block_table: BlockTable = BlockTable()\n-\n-        for from_block in block_table:\n-            if from_block in mapping:\n-                to_block = mapping[from_block]\n-                to_block.ref_count += 1\n-            else:\n-                to_block = dest_allocator.allocate(\n-                    from_block.block_hash, from_block.num_hashed_tokens)\n-                mapping[from_block] = to_block\n-            new_block_table.append(to_block)\n-            # Free the source block swapped in to destination.\n-            src_allocator.free(from_block)\n-\n-        return new_block_table\n-\n-    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n-\n-        request_id = seq_group.request_id\n-\n-        # CPU block -> GPU block.\n-        # dict is efficient in lookup `if cpu_block in mapping`\n-        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n-        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n-            self.block_tables[seq.seq_id] = \\\n-                self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.cpu_allocator, self.gpu_allocator,\n-                                       mapping)\n-\n-        if seq_group.is_encoder_decoder():\n-            self.cross_block_tables[request_id] = \\\n-                self._swap_block_table(self.cross_block_tables[request_id],\n-                                       self.cpu_allocator,\n-                                       self.gpu_allocator,\n-                                       mapping)\n-\n-        return [(cpu_block.block_number, gpu_block.block_number)\n-                for cpu_block, gpu_block in mapping.items()]\n-\n-    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n-        blocks = self._get_physical_blocks(seq_group)\n-        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n-\n-    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n-        request_id = seq_group.request_id\n-\n-        # GPU block -> CPU block.\n-        # dict is efficient in lookup `if gpu_block in mapping`\n-        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n-        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n-            self.block_tables[seq.seq_id] = \\\n-                self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.gpu_allocator, self.cpu_allocator,\n-                                       mapping)\n-\n-        if seq_group.is_encoder_decoder():\n-            self.cross_block_tables[request_id] = \\\n-                self._swap_block_table(self.cross_block_tables[request_id],\n-                                       self.gpu_allocator,\n-                                       self.cpu_allocator,\n-                                       mapping)\n-\n-        return [(cpu_block.block_number, gpu_block.block_number)\n-                for cpu_block, gpu_block in mapping.items()]\n-\n-    def _free_block_table(self, block_table: BlockTable) -> None:\n-        # when using a sliding window, each seq will only use up\n-        # to `self.block_sliding_window` blocks. When freeing\n-        # the block table, we must make sure to not free blocks more\n-        # than once. If no sliding window is used, there is no block\n-        # reuse in the block table, so we must free all blocks.\n-        blocks_to_free = (block_table[-self.block_sliding_window:]\n-                          if self.block_sliding_window is not None else\n-                          block_table)\n-        for block in set(blocks_to_free):\n-            if block.device == Device.GPU:\n-                self.gpu_allocator.free(block)\n-            else:\n-                self.cpu_allocator.free(block)\n-\n-    def free(self, seq: Sequence) -> None:\n-        if seq.seq_id not in self.block_tables:\n-            # Already freed or haven't been scheduled yet.\n-            return\n-        block_table = self.block_tables[seq.seq_id]\n-        self._free_block_table(block_table)\n-        del self.block_tables[seq.seq_id]\n-\n-    def free_cross(self, seq_group: SequenceGroup) -> None:\n-        if seq_group.request_id not in self.cross_block_tables:\n-            # Already freed or hasn't ben scheduled yet.\n-            return\n-        block_table = self.cross_block_tables[seq_group.request_id]\n-        self._free_block_table(block_table)\n-        del self.cross_block_tables[seq_group.request_id]\n-\n-    def reset(self) -> None:\n-        # Free decoder block tables\n-        for block_table in self.block_tables.values():\n-            self._free_block_table(block_table)\n-        self.block_tables.clear()\n-        # Free cross-attention block tables\n-        for block_table in self.cross_block_tables.values():\n-            self._free_block_table(block_table)\n-        self.cross_block_tables.clear()\n-\n-    def get_block_table(self, seq: Sequence) -> List[int]:\n-        return self.block_tables[seq.seq_id].ids()\n-\n-    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n-        block_table = self.cross_block_tables[seq_group.request_id]\n-        return [block.block_number for block in block_table]\n-\n-    def get_num_free_gpu_blocks(self) -> int:\n-        return self.gpu_allocator.get_num_free_blocks()\n-\n-    def get_num_free_cpu_blocks(self) -> int:\n-        return self.cpu_allocator.get_num_free_blocks()\n-\n-    def access_all_blocks_in_seq(\n-        self,\n-        seq: Sequence,\n-        access_time: float,\n-    ) -> None:\n-        if self.enable_caching:\n-            # Update the last accessed time of all the blocks accessed\n-            # in this step.\n-            block_table = self.block_tables[seq.seq_id]\n-            for block in block_table:\n-                block.last_accessed = access_time\n-\n-    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n-        if seq.seq_id not in self.block_tables:\n-            return\n-\n-        # When chunked prefill is enabled, the computed full blocks\n-        # should be calculated based on the number of computed tokens.\n-        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n-                               token_chunk_size)\n-        computed_full_blocks = max_computed_tokens // self.block_size\n-\n-        block_table = self.block_tables[seq.seq_id]\n-        if computed_full_blocks == 0:\n-            return\n-        for i in reversed(range(computed_full_blocks)):\n-            if block_table[i].computed:\n-                break\n-            block_table[i].computed = True\n-\n-    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n-        if seq.seq_id not in self.block_tables:\n-            return []\n-        block_table = self.block_tables[seq.seq_id]\n-        # NOTE We exclude the last block to avoid the case where the entire\n-        # prompt is cached. This would cause erroneous behavior in model\n-        # runner.\n-        return [\n-            b.block_number\n-            for b in takewhile(lambda b: b.computed, block_table[:-1])\n-        ]\n-\n-    def get_common_computed_block_ids(\n-            self, seqs: List[Sequence]) -> GenericSequence[int]:\n-        \"\"\"Return the block ids that are common for a given sequence group.\n-\n-        Used in prefill (can skip prefill of some blocks).\n-        \"\"\"\n-        # Can return non-empty result only with prefix caching enabled.\n-        if not self.enable_caching:\n-            return []\n-\n-        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n-        return commonprefix([ids for ids in ids_list if ids != []])\n-\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n-                                token_chunk_size: int):\n-        if self.enable_caching:\n-            for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n-\n-    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n-        if device == Device.GPU:\n-            return self.gpu_allocator.get_prefix_cache_hit_rate()\n-        if device == Device.CPU:\n-            return self.cpu_allocator.get_prefix_cache_hit_rate()\n-        raise ValueError(f\"Invalid device: {device}\")",
      "change_type": "deleted",
      "lines_added": 1,
      "lines_removed": 744
    },
    {
      "file_path": "vllm/core/interfaces.py",
      "old_content": "import enum\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom typing import Sequence as GenericSequence\nfrom typing import Tuple\n\nfrom vllm.sequence import Sequence, SequenceGroup\nfrom vllm.utils import Device\n\n\nclass AllocStatus(enum.Enum):\n    \"\"\"Result for BlockSpaceManager.can_allocate\n\n    1. Ok: seq_group can be allocated now.\n    2. Later: seq_group cannot be allocated.\n      The capacity of allocator is larger than seq_group required.\n    3. Never: seq_group can never be allocated.\n      The seq_group is too large to allocated in GPU.\n    \"\"\"\n    OK = enum.auto()\n    LATER = enum.auto()\n    NEVER = enum.auto()\n\n\nclass BlockSpaceManager(ABC):\n\n    @staticmethod\n    def get_block_space_manager_class(version: str):\n        version = version.lower()\n\n        if version == \"v1\":\n            from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n            return BlockSpaceManagerV1\n\n        if version == \"v2\":\n            from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n            return BlockSpaceManagerV2\n\n        if version == \"placeholder\":\n            from vllm.core.placeholder_block_space_manager import (\n                PlaceholderBlockSpaceManager)\n            return PlaceholderBlockSpaceManager\n\n        raise ValueError(f\"Unknown version {version=}\")\n\n    @abstractmethod\n    def can_allocate(self,\n                     seq_group: SequenceGroup,\n                     num_lookahead_slots: int = 0) -> AllocStatus:\n        pass\n\n    @abstractmethod\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        pass\n\n    @abstractmethod\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        pass\n\n    @abstractmethod\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        pass\n\n    @abstractmethod\n    def can_swap_in(self, seq_group: SequenceGroup,\n                    num_lookahead_slots: int) -> AllocStatus:\n        pass\n\n    @abstractmethod\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        pass\n\n    @abstractmethod\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def free(self, seq: Sequence) -> None:\n        pass\n\n    @abstractmethod\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        pass\n\n    @abstractmethod\n    def get_num_free_gpu_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_free_cpu_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        pass\n\n    @abstractmethod\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n                                token_chunk_size: int):\n        pass\n\n    @abstractmethod\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n        pass\n",
      "diff": "diff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex 9e1d1b02f..9501a516b 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -28,13 +28,9 @@ class BlockSpaceManager(ABC):\n     def get_block_space_manager_class(version: str):\n         version = version.lower()\n \n-        if version == \"v1\":\n-            from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n-            return BlockSpaceManagerV1\n-\n-        if version == \"v2\":\n-            from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n-            return BlockSpaceManagerV2\n+        if version == \"selfattn\":\n+            from vllm.core.block_manager import SelfAttnBlockSpaceManager\n+            return SelfAttnBlockSpaceManager\n \n         if version == \"placeholder\":\n             from vllm.core.placeholder_block_space_manager import (",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 8
    },
    {
      "file_path": "vllm/core/scheduler.py",
      "old_content": "import enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Deque, Dict, Iterable, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import Device, PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: GenericSequence[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[ScheduledSequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(SequenceGroup(\"\", [], -1),\n                                  token_chunk_size=0)\n    # return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n        output_proc_callback: Optional[Callable] = None,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if (self.scheduler_config.embedding_mode\n                or self.cache_config.is_attention_free):\n            version = \"placeholder\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._seq_group_metadata_cache: List[PyObjectCache] = []\n        self._scheduler_running_outputs_cache: List[PyObjectCache] = []\n        self._scheduled_seq_group_cache: List[PyObjectCache] = []\n\n        # For async output processing, we need to swap cache buffers between\n        # iterations. I.e. since the output processing is lagged one step,\n        # we cannot reuse the cached objects immediately when the schedule()\n        # is called again, but only when schedule() is called the second time.\n        self.output_proc_callback = output_proc_callback\n        self.use_async_output_proc = self.output_proc_callback is not None\n        self.num_cache_iters = 2 if self.use_async_output_proc else 1\n\n        self.cache_id = 0\n        for i in range(self.num_cache_iters):\n            self._seq_group_metadata_cache.append(\n                PyObjectCache(seq_group_metadata_builder))\n            self._scheduler_running_outputs_cache.append(\n                PyObjectCache(scheduler_running_outputs_builder))\n            self._scheduled_seq_group_cache.append(\n                PyObjectCache(scheduled_seq_group_builder))\n\n        # For async postprocessor, the extra decode run cannot be done\n        # when the request reaches max_model_len. In this case, the request\n        # will be stopped during schedule() call and added to this stop list\n        # for processing and deallocation by the free_finished_seq_groups()\n        self._async_stopped: List[SequenceGroup] = []\n\n    @property\n    def next_cache_id(self):\n        return (self.cache_id + 1) % self.num_cache_iters\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\n        if isinstance(request_id, str):\n            request_id = (request_id, )\n        request_ids = set(request_id)\n        for state_queue in [self.waiting, self.running, self.swapped]:\n            aborted_groups: List[SequenceGroup] = []\n            for seq_group in state_queue:\n                if not request_ids:\n                    # Using 'break' here may add two extra iterations,\n                    # but is acceptable to reduce complexity.\n                    break\n                if seq_group.request_id in request_ids:\n                    # Appending aborted group into pending list.\n                    aborted_groups.append(seq_group)\n                    request_ids.remove(seq_group.request_id)\n            for aborted_group in aborted_groups:\n                # Remove the sequence group from the state queue.\n                state_queue.remove(aborted_group)\n                # Remove the aborted request from the Mamba cache.\n                self._finished_requests_ids.append(aborted_group.request_id)\n                for seq in aborted_group.get_seqs():\n                    if seq.is_finished():\n                        continue\n                    seq.status = SequenceStatus.FINISHED_ABORTED\n                    self.free_seq(seq)\n\n                self._free_seq_group_cross_attn_blocks(aborted_group)\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        return self.block_manager.get_prefix_cache_hit_rate(device)\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n    def _schedule_running(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerRunningOutputs:\n        \"\"\"Schedule sequence groups that are running.\n\n        Running queue should include decode and chunked prefill requests.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any decodes are preempted.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any decodes are preempted.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n    \n        Returns:\n            SchedulerRunningOutputs.\n        \"\"\"\n        ret: SchedulerRunningOutputs = \\\n            self._scheduler_running_outputs_cache[self.cache_id].get_object()\n        ret.blocks_to_swap_out.clear()\n        ret.blocks_to_copy.clear()\n        ret.decode_seq_groups.clear()\n        ret.prefill_seq_groups.clear()\n        ret.preempted.clear()\n        ret.swapped_out.clear()\n\n        ret.num_lookahead_slots = self._get_num_lookahead_slots(\n            is_prefill=False, enable_chunking=enable_chunking)\n\n        ret.decode_seq_groups_list.clear()\n        ret.prefill_seq_groups_list.clear()\n\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_out: List[Tuple[int, int]] = ret.blocks_to_swap_out\n        blocks_to_copy: List[Tuple[int, int]] = ret.blocks_to_copy\n\n        decode_seq_groups: List[ScheduledSequenceGroup] = ret.decode_seq_groups\n        prefill_seq_groups: List[\n            ScheduledSequenceGroup] = ret.prefill_seq_groups\n        preempted: List[SequenceGroup] = ret.preempted\n        swapped_out: List[SequenceGroup] = ret.swapped_out\n\n        running_queue = self.running\n        assert len(self._async_stopped) == 0\n        while running_queue:\n            seq_group = running_queue[0]\n            num_running_tokens = self._get_num_new_tokens(\n                seq_group, SequenceStatus.RUNNING, enable_chunking, budget)\n\n            if num_running_tokens == 0:\n                # No budget => Stop\n                break\n\n            running_queue.popleft()\n\n            # With async postprocessor, an extra decode run is done\n            # to process the final tokens. The check below avoids this extra\n            # decode run when the model max len is reached, in order to avoid\n            # a memory overflow.\n            if self.use_async_output_proc and seq_group.seqs[0].get_len(\n            ) > self.scheduler_config.max_model_len:\n                self._async_stopped.append(seq_group)\n                continue\n\n            # NOTE(woosuk): Preemption happens only when there is no available\n            # slot to keep all the sequence groups in the RUNNING state.\n            while not self._can_append_slots(seq_group, enable_chunking):\n                budget.subtract_num_batched_tokens(seq_group.request_id,\n                                                   num_running_tokens)\n                num_running_seqs = seq_group.get_max_num_running_seqs()\n                budget.subtract_num_seqs(seq_group.request_id,\n                                         num_running_seqs)\n\n                if (curr_loras is not None and seq_group.lora_int_id > 0\n                        and seq_group.lora_int_id in curr_loras):\n                    curr_loras.remove(seq_group.lora_int_id)\n\n                # Determine victim sequence\n                cont_loop = True\n                if running_queue:\n                    # Preempt the lowest-priority sequence group.\n                    victim_seq_group = running_queue.pop()\n                else:\n                    # No other sequence group can be preempted.\n                    # Preempt the current sequence group.\n                    # Note: This is also where we stop this loop\n                    # (since there is nothing else to preempt)\n                    victim_seq_group = seq_group\n                    cont_loop = False\n\n                # With async postprocessor, before preempting a sequence\n                # we need to ensure it has no pending async postprocessor\n                do_preempt = True\n                if self.use_async_output_proc:\n                    assert self.output_proc_callback is not None\n                    self.output_proc_callback(\n                        request_id=victim_seq_group.request_id)\n\n                    # It may be that the async pending \"victim_seq_group\"\n                    # becomes finished, in which case we simply free it.\n                    if victim_seq_group.is_finished():\n                        self._free_finished_seq_group(victim_seq_group)\n                        do_preempt = False\n\n                # Do preemption\n                if do_preempt:\n                    preempted_mode = self._preempt(victim_seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(victim_seq_group)\n                    else:\n                        swapped_out.append(victim_seq_group)\n\n                if not cont_loop:\n                    break\n            else:\n                self._append_slots(seq_group, blocks_to_copy, enable_chunking)\n                is_prefill = seq_group.is_prefill()\n\n                scheduled_seq_group: ScheduledSequenceGroup = \\\n                    self._scheduled_seq_group_cache[self.cache_id].get_object()\n                scheduled_seq_group.seq_group = seq_group\n                if is_prefill:\n                    scheduled_seq_group.token_chunk_size = num_running_tokens\n                    prefill_seq_groups.append(scheduled_seq_group)\n                    ret.prefill_seq_groups_list.append(seq_group)\n                else:\n                    scheduled_seq_group.token_chunk_size = 1\n                    decode_seq_groups.append(scheduled_seq_group)\n                    ret.decode_seq_groups_list.append(seq_group)\n\n                budget.add_num_batched_tokens(seq_group.request_id,\n                                              num_running_tokens)\n                # OPTIMIZATION:  Note that get_max_num_running_seqs is\n                # expensive. For the default scheduling chase where\n                # enable_chunking is False, num_seqs are updated before running\n                # this method, so we don't have to update it again here.\n                if enable_chunking:\n                    num_running_seqs = seq_group.get_max_num_running_seqs()\n                    budget.add_num_seqs(seq_group.request_id, num_running_seqs)\n                if curr_loras is not None and seq_group.lora_int_id > 0:\n                    curr_loras.add(seq_group.lora_int_id)\n\n        self._scheduler_running_outputs_cache[self.next_cache_id].reset()\n        self._scheduled_seq_group_cache[self.next_cache_id].reset()\n\n        return ret\n\n    def _schedule_swapped(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerSwappedInOutputs:\n        \"\"\"Schedule sequence groups that are swapped out.\n\n        It schedules swapped requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are swapped in.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are swapped in.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerSwappedInOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_in: List[Tuple[int, int]] = []\n        blocks_to_copy: List[Tuple[int, int]] = []\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        infeasible_seq_groups: List[SequenceGroup] = []\n\n        swapped_queue = self.swapped\n\n        leftover_swapped: Deque[SequenceGroup] = deque()\n        while swapped_queue:\n            seq_group = swapped_queue[0]\n\n            # If the sequence group cannot be swapped in, stop.\n            is_prefill = seq_group.is_prefill()\n            alloc_status = self.block_manager.can_swap_in(\n                seq_group,\n                self._get_num_lookahead_slots(is_prefill, enable_chunking))\n            if alloc_status == AllocStatus.LATER:\n                break\n            elif alloc_status == AllocStatus.NEVER:\n                logger.warning(\n                    \"Failing the request %s because there's not enough kv \"\n                    \"cache blocks to run the entire sequence.\",\n                    seq_group.request_id)\n                for seq in seq_group.get_seqs():\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                infeasible_seq_groups.append(seq_group)\n                swapped_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (lora_int_id > 0 and (lora_int_id not in curr_loras)\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_swapped.appendleft(seq_group)\n                    swapped_queue.popleft()\n                    continue\n\n            # The total number of sequences in the RUNNING state should not\n            # exceed the maximum number of sequences.\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.SWAPPED,\n                                                      enable_chunking, budget)\n\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            if lora_int_id > 0 and curr_loras is not None:\n                curr_loras.add(lora_int_id)\n            swapped_queue.popleft()\n            self._swap_in(seq_group, blocks_to_swap_in)\n            self._append_slots(seq_group, blocks_to_copy, enable_chunking)\n            is_prefill = seq_group.is_prefill()\n            if is_prefill:\n                prefill_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group,\n                                           token_chunk_size=num_new_tokens))\n            else:\n                decode_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        swapped_queue.extendleft(leftover_swapped)\n\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False, enable_chunking=enable_chunking),\n            infeasible_seq_groups=infeasible_seq_groups,\n        )\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled and \\\n                not self.scheduler_config.is_multi_step:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n    def _get_priority(self,\n                      seq_group: SequenceGroup) -> Tuple[Optional[int], float]:\n        \"\"\" Get the priority of the sequence group.\n        Highest preference to user-defined priority, followed by arrival time.\n        Args:\n            seq_group: The sequence group input.\n        Returns:\n            The priority of the sequence group.\n        \"\"\"\n        return seq_group.priority, seq_group.arrival_time\n\n    def _schedule_priority_preemption(\n        self,\n        budget: SchedulingBudget,\n    ) -> int:\n        \"\"\"Sorts waiting and running queue. Also, force preempt requests\n        from the running queue if their priority is lower.\n        Priority-based preemption is used with the priority policy.\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n        Returns:\n            A count of priority-based preemptions.\n        \"\"\"\n\n        waiting_queue = self.waiting\n\n        running_queue = deque(sorted(self.running, key=self._get_priority))\n\n        blocks_to_swap_out: List[Tuple[int, int]] = []\n        force_preemption_count = 0\n\n        if waiting_queue:\n            seq_group = waiting_queue.popleft()\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.WAITING,\n                                                      False, budget)\n\n            #Only preempt if priority inversion exists\n            while running_queue and self._get_priority(\n                    running_queue[-1]) > self._get_priority(seq_group):\n                #Only preempt if waiting sequence cannot be allocated\n                can_allocate = self.block_manager.can_allocate(seq_group)\n                if (num_new_tokens and can_allocate == AllocStatus.OK\n                        and budget.can_schedule(num_new_tokens=num_new_tokens,\n                                                num_new_seqs=num_new_seqs)):\n                    break\n\n                #Adjust budget to remove the victim sequence group\n                vseq_group = running_queue.pop()\n                num_running_tokens = self._get_num_new_tokens(\n                    vseq_group, SequenceStatus.RUNNING, False, budget)\n                budget.subtract_num_batched_tokens(vseq_group.request_id,\n                                                   num_running_tokens)\n                num_running_seqs = vseq_group.get_max_num_running_seqs()\n                budget.subtract_num_seqs(vseq_group.request_id,\n                                         num_running_seqs)\n\n                #Preempt out the victim sequence group\n                self._preempt(vseq_group, blocks_to_swap_out,\n                              PreemptionMode.RECOMPUTE)\n                waiting_queue.appendleft(vseq_group)\n                force_preemption_count += 1\n            #Put the sequence back into the waiting queue\n            waiting_queue.appendleft(seq_group)\n\n        waiting_queue = deque(sorted(waiting_queue, key=self._get_priority))\n\n        self.waiting = waiting_queue\n        self.running = running_queue\n        return force_preemption_count\n\n    def _schedule_prefills(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerPrefillOutputs:\n        \"\"\"Schedule sequence groups that are in prefill stage.\n\n        Note that the current scheduler treats PREEMPTED_FOR_RECOMPUTE\n        as a new prefill (that starts from beginning -> most recently generated\n        tokens).\n\n        It schedules waiting requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are scheduled.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerPrefillOutputs.\n        \"\"\"\n        ignored_seq_groups: List[SequenceGroup] = []\n        seq_groups: List[ScheduledSequenceGroup] = []\n\n        waiting_queue = self.waiting\n\n        leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n        while self._passed_delay(time.time()) and waiting_queue:\n            seq_group = waiting_queue[0]\n\n            waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n            assert len(waiting_seqs) == 1, (\n                \"Waiting sequence group should have only one prompt \"\n                \"sequence.\")\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.WAITING,\n                                                      enable_chunking, budget)\n            if not enable_chunking:\n                num_prompt_tokens = waiting_seqs[0].get_len()\n                assert num_new_tokens == num_prompt_tokens\n\n            prompt_limit = self._get_prompt_limit(seq_group)\n            if num_new_tokens > prompt_limit:\n                logger.warning(\n                    \"Input prompt (%d tokens) is too long\"\n                    \" and exceeds limit of %d\", num_new_tokens, prompt_limit)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            num_lookahead_slots: int = 0\n            if self.scheduler_config.is_multi_step and enable_chunking:\n                num_lookahead_slots = self._get_num_lookahead_slots(\n                    True, enable_chunking)\n\n            # If the sequence group cannot be allocated, stop.\n            can_allocate = self.block_manager.can_allocate(\n                seq_group, num_lookahead_slots=num_lookahead_slots)\n            if can_allocate == AllocStatus.LATER:\n                break\n            elif can_allocate == AllocStatus.NEVER:\n                logger.warning(\n                    \"Input prompt (%d tokens) + lookahead slots (%d) is \"\n                    \"too long and exceeds the capacity of block_manager\",\n                    num_new_tokens, num_lookahead_slots)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (self.lora_enabled and lora_int_id > 0\n                        and lora_int_id not in curr_loras\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_waiting_sequences.appendleft(seq_group)\n                    waiting_queue.popleft()\n                    continue\n\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            # Can schedule this request.\n            if curr_loras is not None and lora_int_id > 0:\n                curr_loras.add(lora_int_id)\n            waiting_queue.popleft()\n            self._allocate_and_set_running(seq_group)\n\n            if enable_chunking and self.scheduler_config.is_multi_step:\n                blocks_to_copy: List[Tuple[int, int]] = []\n                # init_multi_step_from_lookahead_slots happens in append_slots\n                self._append_slots(seq_group, blocks_to_copy, enable_chunking)\n                # This assert will trip when a copy-on-write happens. This is\n                # not a concern as the very first sequence-group block\n                # allocation happens above. Still, we have the assert to\n                # catch any edge-cases.\n                assert not blocks_to_copy\n            else:\n                seq_group.init_multi_step_from_lookahead_slots(\n                    num_lookahead_slots,\n                    num_scheduler_steps=self.scheduler_config.\n                    num_scheduler_steps,\n                    is_multi_step=self.scheduler_config.is_multi_step,\n                    enable_chunking=enable_chunking)\n\n            seq_groups.append(\n                ScheduledSequenceGroup(seq_group=seq_group,\n                                       token_chunk_size=num_new_tokens))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        # Queue requests that couldn't be scheduled.\n        waiting_queue.extendleft(leftover_waiting_sequences)\n        if len(seq_groups) > 0:\n            self.prev_prompt = True\n\n        return SchedulerPrefillOutputs(\n            seq_groups=seq_groups,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=True, enable_chunking=enable_chunking))\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        if len(prefills.seq_groups\n               ) == 0 and self.scheduler_config.policy == \"priority\":\n            self._schedule_priority_preemption(budget)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n\n        # Update new running requests.\n        # By default, vLLM scheduler prioritizes prefills.\n        # Once chunked prefill is enabled,\n        # the policy is changed to prioritize decode requests.\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.prefill_seq_groups +\n                                  swapped_in.prefill_seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=(len(prefills.seq_groups) +\n                                len(swapped_in.prefill_seq_groups) +\n                                len(running_scheduled.prefill_seq_groups)),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=running_scheduled.blocks_to_copy +\n            swapped_in.blocks_to_copy,\n            ignored_seq_groups=prefills.ignored_seq_groups +\n            swapped_in.infeasible_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=(len(running_scheduled.preempted) +\n                       len(running_scheduled.swapped_out)),\n        )\n\n    def _schedule(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\"\"\"\n        if self.scheduler_config.chunked_prefill_enabled:\n            return self._schedule_chunked_prefill()\n        else:\n            return self._schedule_default()\n\n    def _can_append_slots(self, seq_group: SequenceGroup,\n                          enable_chunking: bool) -> bool:\n        \"\"\"Determine whether or not we have enough space in the KV cache to\n        continue generation of the sequence group.\n        \"\"\"\n        # It is True only for testing case to trigger artificial preemption.\n        if (self.enable_artificial_preemption\n                and random.uniform(0, 1) < ARTIFICIAL_PREEMPTION_PROB\n                and self.artificial_preempt_cnt > 0):\n            self.artificial_preempt_cnt -= 1\n            return False\n\n        is_prefill = seq_group.is_prefill()\n        num_lookahead_slots = self._get_num_lookahead_slots(\n            is_prefill, enable_chunking)\n\n        if is_prefill and num_lookahead_slots > 0:\n            # Appending prefill slots only happens multi-step and\n            # chunked-prefill are enabled together.\n            assert self.scheduler_config.is_multi_step and enable_chunking\n\n        return self.block_manager.can_append_slots(\n            seq_group=seq_group, num_lookahead_slots=num_lookahead_slots)\n\n    def _allow_async_output_proc(self, seq_group: SequenceGroup) -> bool:\n        # async_output_proc is allowed only when we have a single sequence\n        # in the sequence group\n        no_single_seq = seq_group.sampling_params is None or (\n            seq_group.sampling_params.n == 1)\n        return no_single_seq\n\n    def schedule(\n            self\n    ) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs, bool]:\n        # Schedule sequence groups.\n        # This function call changes the internal states of the scheduler\n        # such as self.running, self.swapped, and self.waiting.\n        scheduler_start_time = time.perf_counter()\n\n        scheduler_outputs: SchedulerOutputs = self._schedule()\n        now = time.time()\n\n        if not self.cache_config.enable_prefix_caching:\n            common_computed_block_nums = []\n\n        allow_async_output_proc: bool = self.use_async_output_proc\n\n        # Create input data structures.\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        for i, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n            token_chunk_size = scheduled_seq_group.token_chunk_size\n            seq_group.maybe_set_first_scheduled_time(now)\n\n            seq_group_metadata = self._seq_group_metadata_cache[\n                self.cache_id].get_object()\n            seq_group_metadata.seq_data.clear()\n            seq_group_metadata.block_tables.clear()\n\n            # seq_id -> SequenceData\n            seq_data: Dict[int, SequenceData] = {}\n            # seq_id -> physical block numbers\n            block_tables: Dict[int, List[int]] = {}\n\n            if seq_group.is_encoder_decoder():\n                # Encoder associated with SequenceGroup\n                encoder_seq = seq_group.get_encoder_seq()\n                assert encoder_seq is not None\n                encoder_seq_data = encoder_seq.data\n                # Block table for cross-attention\n                # Also managed at SequenceGroup level\n                cross_block_table = self.block_manager.get_cross_block_table(\n                    seq_group)\n            else:\n                encoder_seq_data = None\n                cross_block_table = None\n\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n                seq_id = seq.seq_id\n                seq_data[seq_id] = seq.data\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                self.block_manager.access_all_blocks_in_seq(seq, now)\n\n            if self.cache_config.enable_prefix_caching:\n                common_computed_block_nums = (\n                    self.block_manager.get_common_computed_block_ids(\n                        seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n\n            do_sample = True\n            is_prompt = seq_group.is_prefill()\n            # We should send the metadata to workers when the first prefill\n            # is sent. Subsequent requests could be chunked prefill or decode.\n            is_first_prefill = False\n            if is_prompt:\n                seqs = seq_group.get_seqs()\n                # Prefill has only 1 sequence.\n                assert len(seqs) == 1\n                num_computed_tokens = seqs[0].data.get_num_computed_tokens()\n                is_first_prefill = num_computed_tokens == 0\n                # In the next iteration, all prompt tokens are not computed.\n                # It means the prefill is chunked, and we don't need sampling.\n                # NOTE: We use get_len instead of get_prompt_len because when\n                # a sequence is preempted, prefill includes previous generated\n                # output tokens.\n                if (token_chunk_size + num_computed_tokens <\n                        seqs[0].data.get_len()):\n                    do_sample = False\n\n            # It assumes the scheduled_seq_groups is ordered by\n            # prefill < decoding.\n            if is_first_prefill or not self.scheduler_config.send_delta_data:\n                seq_group_metadata = SequenceGroupMetadata(\n                    request_id=seq_group.request_id,\n                    is_prompt=is_prompt,\n                    seq_data=seq_data,\n                    sampling_params=seq_group.sampling_params,\n                    block_tables=block_tables,\n                    do_sample=do_sample,\n                    pooling_params=seq_group.pooling_params,\n                    token_chunk_size=token_chunk_size,\n                    lora_request=seq_group.lora_request,\n                    computed_block_nums=common_computed_block_nums,\n                    encoder_seq_data=encoder_seq_data,\n                    cross_block_table=cross_block_table,\n                    state=seq_group.state,\n                    # `multi_modal_data` will only be present for the 1st comm\n                    # between engine and worker.\n                    # the subsequent comms can still use delta, but\n                    # `multi_modal_data` will be None.\n                    multi_modal_data=seq_group.multi_modal_data\n                    if scheduler_outputs.num_prefill_groups > 0 else None,\n                    mm_processor_kwargs=seq_group.mm_processor_kwargs,\n                    prompt_adapter_request=seq_group.prompt_adapter_request,\n                )\n            else:\n                # When SPMD mode is enabled, we only send delta data except for\n                # the first request to reduce serialization cost.\n                seq_data_delta = {}\n                for id, data in seq_data.items():\n                    seq_data_delta[id] = data.get_delta_and_reset()\n                seq_group_metadata = SequenceGroupMetadataDelta(\n                    seq_data_delta,\n                    seq_group.request_id,\n                    block_tables,\n                    is_prompt,\n                    do_sample=do_sample,\n                    token_chunk_size=token_chunk_size,\n                    computed_block_nums=common_computed_block_nums,\n                )\n            seq_group_metadata_list.append(seq_group_metadata)\n\n            if allow_async_output_proc:\n                allow_async_output_proc = self._allow_async_output_proc(\n                    seq_group)\n\n        # Now that the batch has been created, we can assume all blocks in the\n        # batch will have been computed before the next scheduling invocation.\n        # This is because the engine assumes that a failure in model execution\n        # will crash the vLLM instance / will not retry.\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            self.block_manager.mark_blocks_as_computed(\n                scheduled_seq_group.seq_group,\n                scheduled_seq_group.token_chunk_size)\n\n        self._seq_group_metadata_cache[self.next_cache_id].reset()\n\n        scheduler_time = time.perf_counter() - scheduler_start_time\n        # Add this to scheduler time to all the sequences that are currently\n        # running. This will help estimate if the scheduler is a significant\n        # component in the e2e latency.\n        for seq_group in self.running:\n            if seq_group is not None and seq_group.metrics is not None:\n                if seq_group.metrics.scheduler_time is not None:\n                    seq_group.metrics.scheduler_time += scheduler_time\n                else:\n                    seq_group.metrics.scheduler_time = scheduler_time\n\n        # Move to next cache (if exists)\n        self.cache_id = self.next_cache_id\n\n        # Return results\n        return (seq_group_metadata_list, scheduler_outputs,\n                allow_async_output_proc)\n\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        self.block_manager.fork(parent_seq, child_seq)\n\n    def free_seq(self, seq: Sequence) -> None:\n        \"\"\"Free a sequence from a block table.\"\"\"\n        self.block_manager.free(seq)\n\n    def _free_finished_seqs(self, seq_group: SequenceGroup) -> None:\n        \"\"\"Free finished seqs in a sequence group.\"\"\"\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                self.free_seq(seq)\n\n    def _free_finished_seq_group(self, seq_group: SequenceGroup) -> None:\n        if seq_group.is_finished():\n            # Free cross-attention block table, if it exists\n            self._free_seq_group_cross_attn_blocks(seq_group)\n\n            # Add the finished requests to the finished requests list.\n            # This list will be used to update the Mamba cache in the\n            # next step.\n            self._finished_requests_ids.append(seq_group.request_id)\n\n        # Free finished seqs\n        self._free_finished_seqs(seq_group)\n\n    def free_finished_seq_groups(self) -> None:\n        remaining: Deque[SequenceGroup] = deque()\n        for seq_group in self.running:\n            self._free_finished_seq_group(seq_group)\n            if not seq_group.is_finished():\n                remaining.append(seq_group)\n\n        self.running = remaining\n\n        # Handle async stopped sequence groups\n        # (ones that reached max model len)\n        if self._async_stopped:\n            for seq_group in self._async_stopped:\n                self._free_seq_group_cross_attn_blocks(seq_group)\n                self._finished_requests_ids.append(seq_group.request_id)\n\n                # Free finished seqs\n                self._free_finished_seqs(seq_group)\n\n            self._async_stopped.clear()\n\n    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:\n        self.block_manager.allocate(seq_group)\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            seq.status = SequenceStatus.RUNNING\n\n    def _append_slots(self,\n                      seq_group: SequenceGroup,\n                      blocks_to_copy: List[Tuple[int, int]],\n                      enable_chunking: bool = False) -> None:\n        \"\"\"Appends new slots to the sequences in the given sequence group.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group containing the\n                sequences to append slots to.\n            blocks_to_copy (List[Tuple[int, int]]): A list of tuple of two\n                ints, the first int is the source block index, and the second\n                int is the destination block index. This list is updated with\n                the new source and destination block indices for the appended\n                slots.\n            enable_chunking (bool): True if chunked prefill is enabled.\n        \"\"\"\n        is_prefill: bool = seq_group.is_prefill()\n        num_lookahead_slots: int = self._get_num_lookahead_slots(\n            is_prefill, enable_chunking)\n\n        seq_group.init_multi_step_from_lookahead_slots(\n            num_lookahead_slots,\n            num_scheduler_steps=self.scheduler_config.num_scheduler_steps,\n            is_multi_step=self.scheduler_config.is_multi_step,\n            enable_chunking=enable_chunking)\n\n        seq_status: Optional[SequenceStatus] = SequenceStatus.RUNNING\n        if self.scheduler_config.is_multi_step and enable_chunking:\n            # In multi-step chunked-prefill any sequence type can have\n            # slots appended.\n            seq_status = None\n\n        for seq in seq_group.get_seqs(status=seq_status):\n            cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n            if len(cows) > 0:\n                blocks_to_copy.extend(cows)\n\n    def _preempt(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n        preemption_mode: Optional[PreemptionMode] = None,\n    ) -> PreemptionMode:\n        # If preemption mode is not specified, we determine the mode as follows:\n        # We use recomputation by default since it incurs lower overhead than\n        # swapping. However, when the sequence group has multiple sequences\n        # (e.g., beam search), recomputation is not currently supported. In\n        # such a case, we use swapping instead.\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\n        # As swapped sequences are prioritized over waiting sequences,\n        # sequence groups with multiple sequences are implicitly prioritized\n        # over sequence groups with a single sequence.\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\n        # sequences. This may require a more sophisticated CUDA kernel.\n        if self.user_specified_preemption_mode is None:\n            if seq_group.get_max_num_running_seqs() == 1:\n                preemption_mode = PreemptionMode.RECOMPUTE\n            else:\n                preemption_mode = PreemptionMode.SWAP\n\n        elif self.user_specified_preemption_mode == \"swap\":\n            preemption_mode = PreemptionMode.SWAP\n        else:\n            preemption_mode = PreemptionMode.RECOMPUTE\n\n        if self.num_cumulative_preemption % 50 == 0:\n            logger.warning(\n                \"Sequence group %s is preempted by %s mode because there is \"\n                \"not enough KV cache space. This can affect the end-to-end \"\n                \"performance. Increase gpu_memory_utilization or \"\n                \"tensor_parallel_size to provide more KV cache memory. \"\n                \"total_num_cumulative_preemption=%d\", seq_group.request_id,\n                preemption_mode, self.num_cumulative_preemption + 1)\n        self.num_cumulative_preemption += 1\n\n        if preemption_mode == PreemptionMode.RECOMPUTE:\n            self._preempt_by_recompute(seq_group)\n        elif preemption_mode == PreemptionMode.SWAP:\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\n        else:\n            raise AssertionError(\"Invalid preemption mode.\")\n        return preemption_mode\n\n    def _preempt_by_recompute(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        assert len(seqs) == 1\n        for seq in seqs:\n            seq.status = SequenceStatus.WAITING\n            self.free_seq(seq)\n            seq.reset_state_for_recompute()\n\n    def _preempt_by_swap(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        self._swap_out(seq_group, blocks_to_swap_out)\n\n    def _swap_in(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_in: List[Tuple[int, int]],\n    ) -> None:\n        mapping = self.block_manager.swap_in(seq_group)\n        blocks_to_swap_in.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            seq.status = SequenceStatus.RUNNING\n\n    def _swap_out(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        if not self.block_manager.can_swap_out(seq_group):\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\n            # entire engine.\n            raise RuntimeError(\n                \"Aborted due to the lack of CPU swap space. Please increase \"\n                \"the swap space to avoid this error.\")\n        mapping = self.block_manager.swap_out(seq_group)\n        blocks_to_swap_out.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            seq.status = SequenceStatus.SWAPPED\n\n    def _passed_delay(self, now: float) -> bool:\n        if self.prev_prompt:\n            self.last_prompt_latency = now - self.prev_time\n        self.prev_time, self.prev_prompt = now, False\n        # Delay scheduling prompts to let waiting queue fill up\n        if self.scheduler_config.delay_factor > 0 and self.waiting:\n            earliest_arrival_time = min(\n                [e.metrics.arrival_time for e in self.waiting])\n            passed_delay = (\n                (now - earliest_arrival_time) >\n                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n                or not self.running)\n        else:\n            passed_delay = True\n        return passed_delay\n\n    def _get_num_lookahead_slots(self, is_prefill: bool,\n                                 enable_chunking: bool) -> int:\n        \"\"\"The number of slots to allocate per sequence per step, beyond known\n        token ids. Speculative decoding uses these slots to store KV activations\n        of tokens which may or may not be accepted.\n\n        Speculative decoding does not yet support prefill, so we do not perform\n        lookahead allocation for prefill.\n\n        When chunking is enabled with multi-step, we allocate lookahead slots\n        for the prefills for when the prefills turn into decodes in the first\n        step.\n        \"\"\"\n        if is_prefill:\n            if self.scheduler_config.is_multi_step and enable_chunking:\n                # num_lookahead_slots was introduced in the context of decodes,\n                # in Speculative Decoding.\n                # When the num_scheduler_steps is 8, say, then the\n                # num_lookahead_slots is 7. Meaning, we are doing a 1-step of\n                # decode anyways and we wish to do 7 more.\n                #\n                # \"lookaheads\" for prefills, is introduced in support for\n                # Chunked-Prefill in Multi-Step.\n                return self.scheduler_config.num_lookahead_slots + 1\n            else:\n                return 0\n\n        return self.scheduler_config.num_lookahead_slots\n\n    def _get_num_new_tokens(self, seq_group: SequenceGroup,\n                            status: SequenceStatus, enable_chunking: bool,\n                            budget: SchedulingBudget) -> int:\n        \"\"\"Get the next new tokens to compute for a given sequence group\n            that's in a given `status`.\n\n        The API could chunk the number of tokens to compute based on `budget`\n        if `enable_chunking` is True. If a sequence group has multiple\n        sequences (e.g., running beam search), it means it is in decoding\n        phase, so chunking doesn't happen.\n\n        Returns 0 if the new token cannot be computed due to token budget.\n        \"\"\"\n        num_new_tokens = 0\n        seqs = seq_group.get_seqs(status=status)\n        for seq in seqs:\n            num_new_tokens += seq.get_num_new_tokens()\n        assert num_new_tokens > 0\n        # Chunk if a running request cannot fit in the given budget.\n        # If number of seq > 1, it means it is doing beam search\n        # in a decode phase. Do not chunk.\n        if enable_chunking and len(seqs) == 1:\n            remaining_token_budget = budget.remaining_token_budget()\n            if self.scheduler_config.is_multi_step:\n                # The current multi-step + chunked prefill capability does\n                # not actually support chunking prompts.\n                #\n                # Therefore, `num_new_tokens` is computed in the same fashion\n                # for both multi-step+chunked-prefill &\n                # multi-step+chunked-prefill+APC\n                #\n                # Prompts with more tokens than the current remaining budget\n                # are postponed to future scheduler steps\n                if num_new_tokens > self._get_prompt_limit(seq_group):\n                    # If the seq_group is in prompt-stage, pass the\n                    # num_new_tokens as-is so the caller can ignore\n                    # the sequence.\n                    pass\n                else:\n                    num_new_tokens = 0 \\\n                        if num_new_tokens > remaining_token_budget \\\n                        else num_new_tokens\n            elif self.cache_config.enable_prefix_caching:\n                # When prefix caching is enabled, we always allocate\n                # the number of new tokens that is dividable by the block\n                # size to avoid partial block matching.\n                block_size = self.cache_config.block_size\n                remainder = budget.token_budget % block_size\n                if remainder != 0:\n                    raise ValueError(\"When enabling chunked prefill and \"\n                                     \"prefix caching, max_num_batched_tokens \"\n                                     \"(chunk size) must be dividable by \"\n                                     \"block size, but got chunk_size \"\n                                     f\"({budget.token_budget}) % block_size \"\n                                     f\"({block_size}) = {remainder}\")\n                if remaining_token_budget < num_new_tokens:\n                    num_new_tokens = (remaining_token_budget //\n                                      block_size) * block_size\n            else:\n                num_new_tokens = min(num_new_tokens, remaining_token_budget)\n        return num_new_tokens\n",
      "diff": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex e7eaaf122..f0c8e6bab 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -312,9 +312,7 @@ class Scheduler:\n         # LoRAs. This should be improved in the future.\n         self.lora_config = lora_config\n \n-        version = \"v1\"\n-        if self.scheduler_config.use_v2_block_manager:\n-            version = \"v2\"\n+        version = \"selfattn\"\n         if (self.scheduler_config.embedding_mode\n                 or self.cache_config.is_attention_free):\n             version = \"placeholder\"",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/engine/arg_utils.py",
      "old_content": "import argparse\nimport dataclasses\nimport json\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Dict, List, Literal, Mapping, Optional,\n                    Tuple, Type, Union, cast)\n\nimport torch\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, ConfigFormat, DecodingConfig,\n                         DeviceConfig, EngineConfig, LoadConfig, LoadFormat,\n                         LoRAConfig, ModelConfig, ObservabilityConfig,\n                         ParallelConfig, PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig, TokenizerPoolConfig)\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.utils import FlexibleArgumentParser\n\nif TYPE_CHECKING:\n    from vllm.transformers_utils.tokenizer_group import BaseTokenizerGroup\n\nlogger = init_logger(__name__)\n\nALLOWED_DETAILED_TRACE_MODULES = [\"model\", \"worker\", \"all\"]\n\nDEVICE_OPTIONS = [\n    \"auto\",\n    \"cuda\",\n    \"neuron\",\n    \"cpu\",\n    \"openvino\",\n    \"tpu\",\n    \"xpu\",\n]\n\n\ndef nullable_str(val: str):\n    if not val or val == \"None\":\n        return None\n    return val\n\n\ndef nullable_kvs(val: str) -> Optional[Mapping[str, int]]:\n    \"\"\"Parses a string containing comma separate key [str] to value [int]\n    pairs into a dictionary.\n\n    Args:\n        val: String value to be parsed.\n\n    Returns:\n        Dictionary with parsed values.\n    \"\"\"\n    if len(val) == 0:\n        return None\n\n    out_dict: Dict[str, int] = {}\n    for item in val.split(\",\"):\n        kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n        if len(kv_parts) != 2:\n            raise argparse.ArgumentTypeError(\n                \"Each item should be in the form KEY=VALUE\")\n        key, value = kv_parts\n\n        try:\n            parsed_value = int(value)\n        except ValueError as exc:\n            msg = f\"Failed to parse value of item {key}={value}\"\n            raise argparse.ArgumentTypeError(msg) from exc\n\n        if key in out_dict and out_dict[key] != parsed_value:\n            raise argparse.ArgumentTypeError(\n                f\"Conflicting values specified for key: {key}\")\n        out_dict[key] = parsed_value\n\n    return out_dict\n\n\n@dataclass\nclass EngineArgs:\n    \"\"\"Arguments for vLLM engine.\"\"\"\n    model: str = 'facebook/opt-125m'\n    served_model_name: Optional[Union[str, List[str]]] = None\n    tokenizer: Optional[str] = None\n    skip_tokenizer_init: bool = False\n    tokenizer_mode: str = 'auto'\n    trust_remote_code: bool = False\n    download_dir: Optional[str] = None\n    load_format: str = 'auto'\n    config_format: ConfigFormat = ConfigFormat.AUTO\n    dtype: str = 'auto'\n    kv_cache_dtype: str = 'auto'\n    quantization_param_path: Optional[str] = None\n    seed: int = 0\n    max_model_len: Optional[int] = None\n    worker_use_ray: bool = False\n    # Note: Specifying a custom executor backend by passing a class\n    # is intended for expert use only. The API may change without\n    # notice.\n    distributed_executor_backend: Optional[Union[str,\n                                                 Type[ExecutorBase]]] = None\n    pipeline_parallel_size: int = 1\n    tensor_parallel_size: int = 1\n    max_parallel_loading_workers: Optional[int] = None\n    block_size: int = 16\n    enable_prefix_caching: bool = False\n    disable_sliding_window: bool = False\n    use_v2_block_manager: bool = True\n    swap_space: float = 4  # GiB\n    cpu_offload_gb: float = 0  # GiB\n    gpu_memory_utilization: float = 0.90\n    max_num_batched_tokens: Optional[int] = None\n    max_num_seqs: int = 256\n    max_logprobs: int = 20  # Default value for OpenAI Chat Completions API\n    disable_log_stats: bool = False\n    revision: Optional[str] = None\n    code_revision: Optional[str] = None\n    rope_scaling: Optional[dict] = None\n    rope_theta: Optional[float] = None\n    tokenizer_revision: Optional[str] = None\n    quantization: Optional[str] = None\n    enforce_eager: Optional[bool] = None\n    max_context_len_to_capture: Optional[int] = None\n    max_seq_len_to_capture: int = 8192\n    disable_custom_all_reduce: bool = False\n    tokenizer_pool_size: int = 0\n    # Note: Specifying a tokenizer pool by passing a class\n    # is intended for expert use only. The API may change without\n    # notice.\n    tokenizer_pool_type: Union[str, Type[\"BaseTokenizerGroup\"]] = \"ray\"\n    tokenizer_pool_extra_config: Optional[dict] = None\n    limit_mm_per_prompt: Optional[Mapping[str, int]] = None\n    enable_lora: bool = False\n    max_loras: int = 1\n    max_lora_rank: int = 16\n    enable_prompt_adapter: bool = False\n    max_prompt_adapters: int = 1\n    max_prompt_adapter_token: int = 0\n    fully_sharded_loras: bool = False\n    lora_extra_vocab_size: int = 256\n    long_lora_scaling_factors: Optional[Tuple[float]] = None\n    lora_dtype: Optional[Union[str, torch.dtype]] = 'auto'\n    max_cpu_loras: Optional[int] = None\n    device: str = 'auto'\n    num_scheduler_steps: int = 1\n    multi_step_stream_outputs: bool = True\n    ray_workers_use_nsight: bool = False\n    num_gpu_blocks_override: Optional[int] = None\n    num_lookahead_slots: int = 0\n    model_loader_extra_config: Optional[dict] = None\n    ignore_patterns: Optional[Union[str, List[str]]] = None\n    preemption_mode: Optional[str] = None\n\n    scheduler_delay_factor: float = 0.0\n    enable_chunked_prefill: Optional[bool] = None\n\n    guided_decoding_backend: str = 'outlines'\n    # Speculative decoding configuration.\n    speculative_model: Optional[str] = None\n    speculative_model_quantization: Optional[str] = None\n    speculative_draft_tensor_parallel_size: Optional[int] = None\n    num_speculative_tokens: Optional[int] = None\n    speculative_disable_mqa_scorer: Optional[bool] = False\n    speculative_max_model_len: Optional[int] = None\n    speculative_disable_by_batch_size: Optional[int] = None\n    ngram_prompt_lookup_max: Optional[int] = None\n    ngram_prompt_lookup_min: Optional[int] = None\n    spec_decoding_acceptance_method: str = 'rejection_sampler'\n    typical_acceptance_sampler_posterior_threshold: Optional[float] = None\n    typical_acceptance_sampler_posterior_alpha: Optional[float] = None\n    qlora_adapter_name_or_path: Optional[str] = None\n    disable_logprobs_during_spec_decoding: Optional[bool] = None\n\n    otlp_traces_endpoint: Optional[str] = None\n    collect_detailed_traces: Optional[str] = None\n    disable_async_output_proc: bool = False\n    override_neuron_config: Optional[Dict[str, Any]] = None\n    mm_processor_kwargs: Optional[Dict[str, Any]] = None\n    scheduling_policy: Literal[\"fcfs\", \"priority\"] = \"fcfs\"\n\n    def __post_init__(self):\n        if not self.tokenizer:\n            self.tokenizer = self.model\n\n        # Setup plugins\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n        \"\"\"Shared CLI arguments for vLLM engine.\"\"\"\n\n        # Model arguments\n        parser.add_argument(\n            '--model',\n            type=str,\n            default=EngineArgs.model,\n            help='Name or path of the huggingface model to use.')\n        parser.add_argument(\n            '--tokenizer',\n            type=nullable_str,\n            default=EngineArgs.tokenizer,\n            help='Name or path of the huggingface tokenizer to use. '\n            'If unspecified, model name or path will be used.')\n        parser.add_argument(\n            '--skip-tokenizer-init',\n            action='store_true',\n            help='Skip initialization of tokenizer and detokenizer')\n        parser.add_argument(\n            '--revision',\n            type=nullable_str,\n            default=None,\n            help='The specific model version to use. It can be a branch '\n            'name, a tag name, or a commit id. If unspecified, will use '\n            'the default version.')\n        parser.add_argument(\n            '--code-revision',\n            type=nullable_str,\n            default=None,\n            help='The specific revision to use for the model code on '\n            'Hugging Face Hub. It can be a branch name, a tag name, or a '\n            'commit id. If unspecified, will use the default version.')\n        parser.add_argument(\n            '--tokenizer-revision',\n            type=nullable_str,\n            default=None,\n            help='Revision of the huggingface tokenizer to use. '\n            'It can be a branch name, a tag name, or a commit id. '\n            'If unspecified, will use the default version.')\n        parser.add_argument(\n            '--tokenizer-mode',\n            type=str,\n            default=EngineArgs.tokenizer_mode,\n            choices=['auto', 'slow', 'mistral'],\n            help='The tokenizer mode.\\n\\n* \"auto\" will use the '\n            'fast tokenizer if available.\\n* \"slow\" will '\n            'always use the slow tokenizer. \\n* '\n            '\"mistral\" will always use the `mistral_common` tokenizer.')\n        parser.add_argument('--trust-remote-code',\n                            action='store_true',\n                            help='Trust remote code from huggingface.')\n        parser.add_argument('--download-dir',\n                            type=nullable_str,\n                            default=EngineArgs.download_dir,\n                            help='Directory to download and load the weights, '\n                            'default to the default cache dir of '\n                            'huggingface.')\n        parser.add_argument(\n            '--load-format',\n            type=str,\n            default=EngineArgs.load_format,\n            choices=[f.value for f in LoadFormat],\n            help='The format of the model weights to load.\\n\\n'\n            '* \"auto\" will try to load the weights in the safetensors format '\n            'and fall back to the pytorch bin format if safetensors format '\n            'is not available.\\n'\n            '* \"pt\" will load the weights in the pytorch bin format.\\n'\n            '* \"safetensors\" will load the weights in the safetensors format.\\n'\n            '* \"npcache\" will load the weights in pytorch format and store '\n            'a numpy cache to speed up the loading.\\n'\n            '* \"dummy\" will initialize the weights with random values, '\n            'which is mainly for profiling.\\n'\n            '* \"tensorizer\" will load the weights using tensorizer from '\n            'CoreWeave. See the Tensorize vLLM Model script in the Examples '\n            'section for more information.\\n'\n            '* \"bitsandbytes\" will load the weights using bitsandbytes '\n            'quantization.\\n')\n        parser.add_argument(\n            '--config-format',\n            default=EngineArgs.config_format,\n            choices=[f.value for f in ConfigFormat],\n            help='The format of the model config to load.\\n\\n'\n            '* \"auto\" will try to load the config in hf format '\n            'if available else it will try to load in mistral format ')\n        parser.add_argument(\n            '--dtype',\n            type=str,\n            default=EngineArgs.dtype,\n            choices=[\n                'auto', 'half', 'float16', 'bfloat16', 'float', 'float32'\n            ],\n            help='Data type for model weights and activations.\\n\\n'\n            '* \"auto\" will use FP16 precision for FP32 and FP16 models, and '\n            'BF16 precision for BF16 models.\\n'\n            '* \"half\" for FP16. Recommended for AWQ quantization.\\n'\n            '* \"float16\" is the same as \"half\".\\n'\n            '* \"bfloat16\" for a balance between precision and range.\\n'\n            '* \"float\" is shorthand for FP32 precision.\\n'\n            '* \"float32\" for FP32 precision.')\n        parser.add_argument(\n            '--kv-cache-dtype',\n            type=str,\n            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n            default=EngineArgs.kv_cache_dtype,\n            help='Data type for kv cache storage. If \"auto\", will use model '\n            'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n            'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n        parser.add_argument(\n            '--quantization-param-path',\n            type=nullable_str,\n            default=None,\n            help='Path to the JSON file containing the KV cache '\n            'scaling factors. This should generally be supplied, when '\n            'KV cache dtype is FP8. Otherwise, KV cache scaling factors '\n            'default to 1.0, which may cause accuracy issues. '\n            'FP8_E5M2 (without scaling) is only supported on cuda version'\n            'greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead '\n            'supported for common inference criteria.')\n        parser.add_argument('--max-model-len',\n                            type=int,\n                            default=EngineArgs.max_model_len,\n                            help='Model context length. If unspecified, will '\n                            'be automatically derived from the model config.')\n        parser.add_argument(\n            '--guided-decoding-backend',\n            type=str,\n            default='outlines',\n            choices=['outlines', 'lm-format-enforcer'],\n            help='Which engine will be used for guided decoding'\n            ' (JSON schema / regex etc) by default. Currently support '\n            'https://github.com/outlines-dev/outlines and '\n            'https://github.com/noamgat/lm-format-enforcer.'\n            ' Can be overridden per request via guided_decoding_backend'\n            ' parameter.')\n        # Parallel arguments\n        parser.add_argument(\n            '--distributed-executor-backend',\n            choices=['ray', 'mp'],\n            default=EngineArgs.distributed_executor_backend,\n            help='Backend to use for distributed serving. When more than 1 GPU '\n            'is used, will be automatically set to \"ray\" if installed '\n            'or \"mp\" (multiprocessing) otherwise.')\n        parser.add_argument(\n            '--worker-use-ray',\n            action='store_true',\n            help='Deprecated, use --distributed-executor-backend=ray.')\n        parser.add_argument('--pipeline-parallel-size',\n                            '-pp',\n                            type=int,\n                            default=EngineArgs.pipeline_parallel_size,\n                            help='Number of pipeline stages.')\n        parser.add_argument('--tensor-parallel-size',\n                            '-tp',\n                            type=int,\n                            default=EngineArgs.tensor_parallel_size,\n                            help='Number of tensor parallel replicas.')\n        parser.add_argument(\n            '--max-parallel-loading-workers',\n            type=int,\n            default=EngineArgs.max_parallel_loading_workers,\n            help='Load model sequentially in multiple batches, '\n            'to avoid RAM OOM when using tensor '\n            'parallel and large models.')\n        parser.add_argument(\n            '--ray-workers-use-nsight',\n            action='store_true',\n            help='If specified, use nsight to profile Ray workers.')\n        # KV cache arguments\n        parser.add_argument('--block-size',\n                            type=int,\n                            default=EngineArgs.block_size,\n                            choices=[8, 16, 32],\n                            help='Token block size for contiguous chunks of '\n                            'tokens. This is ignored on neuron devices and '\n                            'set to max-model-len')\n\n        parser.add_argument('--enable-prefix-caching',\n                            action='store_true',\n                            help='Enables automatic prefix caching.')\n        parser.add_argument('--disable-sliding-window',\n                            action='store_true',\n                            help='Disables sliding window, '\n                            'capping to sliding window size')\n        parser.add_argument(\n            '--use-v2-block-manager',\n            default=EngineArgs.use_v2_block_manager,\n            action='store_true',\n            help='Use BlockSpaceMangerV2. By default this is set to True. '\n            'Set to False to use BlockSpaceManagerV1')\n        parser.add_argument(\n            '--num-lookahead-slots',\n            type=int,\n            default=EngineArgs.num_lookahead_slots,\n            help='Experimental scheduling config necessary for '\n            'speculative decoding. This will be replaced by '\n            'speculative config in the future; it is present '\n            'to enable correctness tests until then.')\n\n        parser.add_argument('--seed',\n                            type=int,\n                            default=EngineArgs.seed,\n                            help='Random seed for operations.')\n        parser.add_argument('--swap-space',\n                            type=float,\n                            default=EngineArgs.swap_space,\n                            help='CPU swap space size (GiB) per GPU.')\n        parser.add_argument(\n            '--cpu-offload-gb',\n            type=float,\n            default=0,\n            help='The space in GiB to offload to CPU, per GPU. '\n            'Default is 0, which means no offloading. Intuitively, '\n            'this argument can be seen as a virtual way to increase '\n            'the GPU memory size. For example, if you have one 24 GB '\n            'GPU and set this to 10, virtually you can think of it as '\n            'a 34 GB GPU. Then you can load a 13B model with BF16 weight,'\n            'which requires at least 26GB GPU memory. Note that this '\n            'requires fast CPU-GPU interconnect, as part of the model is'\n            'loaded from CPU memory to GPU memory on the fly in each '\n            'model forward pass.')\n        parser.add_argument(\n            '--gpu-memory-utilization',\n            type=float,\n            default=EngineArgs.gpu_memory_utilization,\n            help='The fraction of GPU memory to be used for the model '\n            'executor, which can range from 0 to 1. For example, a value of '\n            '0.5 would imply 50%% GPU memory utilization. If unspecified, '\n            'will use the default value of 0.9.')\n        parser.add_argument(\n            '--num-gpu-blocks-override',\n            type=int,\n            default=None,\n            help='If specified, ignore GPU profiling result and use this number'\n            'of GPU blocks. Used for testing preemption.')\n        parser.add_argument('--max-num-batched-tokens',\n                            type=int,\n                            default=EngineArgs.max_num_batched_tokens,\n                            help='Maximum number of batched tokens per '\n                            'iteration.')\n        parser.add_argument('--max-num-seqs',\n                            type=int,\n                            default=EngineArgs.max_num_seqs,\n                            help='Maximum number of sequences per iteration.')\n        parser.add_argument(\n            '--max-logprobs',\n            type=int,\n            default=EngineArgs.max_logprobs,\n            help=('Max number of log probs to return logprobs is specified in'\n                  ' SamplingParams.'))\n        parser.add_argument('--disable-log-stats',\n                            action='store_true',\n                            help='Disable logging statistics.')\n        # Quantization settings.\n        parser.add_argument('--quantization',\n                            '-q',\n                            type=nullable_str,\n                            choices=[*QUANTIZATION_METHODS, None],\n                            default=EngineArgs.quantization,\n                            help='Method used to quantize the weights. If '\n                            'None, we first check the `quantization_config` '\n                            'attribute in the model config file. If that is '\n                            'None, we assume the model weights are not '\n                            'quantized and use `dtype` to determine the data '\n                            'type of the weights.')\n        parser.add_argument(\n            '--rope-scaling',\n            default=None,\n            type=json.loads,\n            help='RoPE scaling configuration in JSON format. '\n            'For example, {\"rope_type\":\"dynamic\",\"factor\":2.0}')\n        parser.add_argument('--rope-theta',\n                            default=None,\n                            type=float,\n                            help='RoPE theta. Use with `rope_scaling`. In '\n                            'some cases, changing the RoPE theta improves the '\n                            'performance of the scaled model.')\n        parser.add_argument('--enforce-eager',\n                            action='store_true',\n                            help='Always use eager-mode PyTorch. If False, '\n                            'will use eager mode and CUDA graph in hybrid '\n                            'for maximal performance and flexibility.')\n        parser.add_argument('--max-context-len-to-capture',\n                            type=int,\n                            default=EngineArgs.max_context_len_to_capture,\n                            help='Maximum context length covered by CUDA '\n                            'graphs. When a sequence has context length '\n                            'larger than this, we fall back to eager mode. '\n                            '(DEPRECATED. Use --max-seq-len-to-capture instead'\n                            ')')\n        parser.add_argument('--max-seq-len-to-capture',\n                            type=int,\n                            default=EngineArgs.max_seq_len_to_capture,\n                            help='Maximum sequence length covered by CUDA '\n                            'graphs. When a sequence has context length '\n                            'larger than this, we fall back to eager mode. '\n                            'Additionally for encoder-decoder models, if the '\n                            'sequence length of the encoder input is larger '\n                            'than this, we fall back to the eager mode.')\n        parser.add_argument('--disable-custom-all-reduce',\n                            action='store_true',\n                            default=EngineArgs.disable_custom_all_reduce,\n                            help='See ParallelConfig.')\n        parser.add_argument('--tokenizer-pool-size',\n                            type=int,\n                            default=EngineArgs.tokenizer_pool_size,\n                            help='Size of tokenizer pool to use for '\n                            'asynchronous tokenization. If 0, will '\n                            'use synchronous tokenization.')\n        parser.add_argument('--tokenizer-pool-type',\n                            type=str,\n                            default=EngineArgs.tokenizer_pool_type,\n                            help='Type of tokenizer pool to use for '\n                            'asynchronous tokenization. Ignored '\n                            'if tokenizer_pool_size is 0.')\n        parser.add_argument('--tokenizer-pool-extra-config',\n                            type=nullable_str,\n                            default=EngineArgs.tokenizer_pool_extra_config,\n                            help='Extra config for tokenizer pool. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary. Ignored if '\n                            'tokenizer_pool_size is 0.')\n\n        # Multimodal related configs\n        parser.add_argument(\n            '--limit-mm-per-prompt',\n            type=nullable_kvs,\n            default=EngineArgs.limit_mm_per_prompt,\n            # The default value is given in\n            # MultiModalRegistry.init_mm_limits_per_prompt\n            help=('For each multimodal plugin, limit how many '\n                  'input instances to allow for each prompt. '\n                  'Expects a comma-separated list of items, '\n                  'e.g.: `image=16,video=2` allows a maximum of 16 '\n                  'images and 2 videos per prompt. Defaults to 1 for '\n                  'each modality.'))\n        parser.add_argument(\n            '--mm-processor-kwargs',\n            default=None,\n            type=json.loads,\n            help=('Overrides for the multimodal input mapping/processing,'\n                  'e.g., image processor. For example: {\"num_crops\": 4}.'))\n\n        # LoRA related configs\n        parser.add_argument('--enable-lora',\n                            action='store_true',\n                            help='If True, enable handling of LoRA adapters.')\n        parser.add_argument('--max-loras',\n                            type=int,\n                            default=EngineArgs.max_loras,\n                            help='Max number of LoRAs in a single batch.')\n        parser.add_argument('--max-lora-rank',\n                            type=int,\n                            default=EngineArgs.max_lora_rank,\n                            help='Max LoRA rank.')\n        parser.add_argument(\n            '--lora-extra-vocab-size',\n            type=int,\n            default=EngineArgs.lora_extra_vocab_size,\n            help=('Maximum size of extra vocabulary that can be '\n                  'present in a LoRA adapter (added to the base '\n                  'model vocabulary).'))\n        parser.add_argument(\n            '--lora-dtype',\n            type=str,\n            default=EngineArgs.lora_dtype,\n            choices=['auto', 'float16', 'bfloat16', 'float32'],\n            help=('Data type for LoRA. If auto, will default to '\n                  'base model dtype.'))\n        parser.add_argument(\n            '--long-lora-scaling-factors',\n            type=nullable_str,\n            default=EngineArgs.long_lora_scaling_factors,\n            help=('Specify multiple scaling factors (which can '\n                  'be different from base model scaling factor '\n                  '- see eg. Long LoRA) to allow for multiple '\n                  'LoRA adapters trained with those scaling '\n                  'factors to be used at the same time. If not '\n                  'specified, only adapters trained with the '\n                  'base model scaling factor are allowed.'))\n        parser.add_argument(\n            '--max-cpu-loras',\n            type=int,\n            default=EngineArgs.max_cpu_loras,\n            help=('Maximum number of LoRAs to store in CPU memory. '\n                  'Must be >= than max_num_seqs. '\n                  'Defaults to max_num_seqs.'))\n        parser.add_argument(\n            '--fully-sharded-loras',\n            action='store_true',\n            help=('By default, only half of the LoRA computation is '\n                  'sharded with tensor parallelism. '\n                  'Enabling this will use the fully sharded layers. '\n                  'At high sequence length, max rank or '\n                  'tensor parallel size, this is likely faster.'))\n        parser.add_argument('--enable-prompt-adapter',\n                            action='store_true',\n                            help='If True, enable handling of PromptAdapters.')\n        parser.add_argument('--max-prompt-adapters',\n                            type=int,\n                            default=EngineArgs.max_prompt_adapters,\n                            help='Max number of PromptAdapters in a batch.')\n        parser.add_argument('--max-prompt-adapter-token',\n                            type=int,\n                            default=EngineArgs.max_prompt_adapter_token,\n                            help='Max number of PromptAdapters tokens')\n        parser.add_argument(\"--device\",\n                            type=str,\n                            default=EngineArgs.device,\n                            choices=DEVICE_OPTIONS,\n                            help='Device type for vLLM execution.')\n        parser.add_argument('--num-scheduler-steps',\n                            type=int,\n                            default=1,\n                            help=('Maximum number of forward steps per '\n                                  'scheduler call.'))\n\n        parser.add_argument(\n            '--multi-step-stream-outputs',\n            action=StoreBoolean,\n            default=EngineArgs.multi_step_stream_outputs,\n            nargs=\"?\",\n            const=\"True\",\n            help='If False, then multi-step will stream outputs at the end '\n            'of all steps')\n        parser.add_argument(\n            '--scheduler-delay-factor',\n            type=float,\n            default=EngineArgs.scheduler_delay_factor,\n            help='Apply a delay (of delay factor multiplied by previous '\n            'prompt latency) before scheduling next prompt.')\n        parser.add_argument(\n            '--enable-chunked-prefill',\n            action=StoreBoolean,\n            default=EngineArgs.enable_chunked_prefill,\n            nargs=\"?\",\n            const=\"True\",\n            help='If set, the prefill requests can be chunked based on the '\n            'max_num_batched_tokens.')\n\n        parser.add_argument(\n            '--speculative-model',\n            type=nullable_str,\n            default=EngineArgs.speculative_model,\n            help=\n            'The name of the draft model to be used in speculative decoding.')\n        # Quantization settings for speculative model.\n        parser.add_argument(\n            '--speculative-model-quantization',\n            type=nullable_str,\n            choices=[*QUANTIZATION_METHODS, None],\n            default=EngineArgs.speculative_model_quantization,\n            help='Method used to quantize the weights of speculative model. '\n            'If None, we first check the `quantization_config` '\n            'attribute in the model config file. If that is '\n            'None, we assume the model weights are not '\n            'quantized and use `dtype` to determine the data '\n            'type of the weights.')\n        parser.add_argument(\n            '--num-speculative-tokens',\n            type=int,\n            default=EngineArgs.num_speculative_tokens,\n            help='The number of speculative tokens to sample from '\n            'the draft model in speculative decoding.')\n        parser.add_argument(\n            '--speculative-disable-mqa-scorer',\n            action='store_true',\n            help=\n            'If set to True, the MQA scorer will be disabled in speculative '\n            ' and fall back to batch expansion')\n        parser.add_argument(\n            '--speculative-draft-tensor-parallel-size',\n            '-spec-draft-tp',\n            type=int,\n            default=EngineArgs.speculative_draft_tensor_parallel_size,\n            help='Number of tensor parallel replicas for '\n            'the draft model in speculative decoding.')\n\n        parser.add_argument(\n            '--speculative-max-model-len',\n            type=int,\n            default=EngineArgs.speculative_max_model_len,\n            help='The maximum sequence length supported by the '\n            'draft model. Sequences over this length will skip '\n            'speculation.')\n\n        parser.add_argument(\n            '--speculative-disable-by-batch-size',\n            type=int,\n            default=EngineArgs.speculative_disable_by_batch_size,\n            help='Disable speculative decoding for new incoming requests '\n            'if the number of enqueue requests is larger than this value.')\n\n        parser.add_argument(\n            '--ngram-prompt-lookup-max',\n            type=int,\n            default=EngineArgs.ngram_prompt_lookup_max,\n            help='Max size of window for ngram prompt lookup in speculative '\n            'decoding.')\n\n        parser.add_argument(\n            '--ngram-prompt-lookup-min',\n            type=int,\n            default=EngineArgs.ngram_prompt_lookup_min,\n            help='Min size of window for ngram prompt lookup in speculative '\n            'decoding.')\n\n        parser.add_argument(\n            '--spec-decoding-acceptance-method',\n            type=str,\n            default=EngineArgs.spec_decoding_acceptance_method,\n            choices=['rejection_sampler', 'typical_acceptance_sampler'],\n            help='Specify the acceptance method to use during draft token '\n            'verification in speculative decoding. Two types of acceptance '\n            'routines are supported: '\n            '1) RejectionSampler which does not allow changing the '\n            'acceptance rate of draft tokens, '\n            '2) TypicalAcceptanceSampler which is configurable, allowing for '\n            'a higher acceptance rate at the cost of lower quality, '\n            'and vice versa.')\n\n        parser.add_argument(\n            '--typical-acceptance-sampler-posterior-threshold',\n            type=float,\n            default=EngineArgs.typical_acceptance_sampler_posterior_threshold,\n            help='Set the lower bound threshold for the posterior '\n            'probability of a token to be accepted. This threshold is '\n            'used by the TypicalAcceptanceSampler to make sampling decisions '\n            'during speculative decoding. Defaults to 0.09')\n\n        parser.add_argument(\n            '--typical-acceptance-sampler-posterior-alpha',\n            type=float,\n            default=EngineArgs.typical_acceptance_sampler_posterior_alpha,\n            help='A scaling factor for the entropy-based threshold for token '\n            'acceptance in the TypicalAcceptanceSampler. Typically defaults '\n            'to sqrt of --typical-acceptance-sampler-posterior-threshold '\n            'i.e. 0.3')\n\n        parser.add_argument(\n            '--disable-logprobs-during-spec-decoding',\n            action=StoreBoolean,\n            default=EngineArgs.disable_logprobs_during_spec_decoding,\n            nargs=\"?\",\n            const=\"True\",\n            help='If set to True, token log probabilities are not returned '\n            'during speculative decoding. If set to False, log probabilities '\n            'are returned according to the settings in SamplingParams. If '\n            'not specified, it defaults to True. Disabling log probabilities '\n            'during speculative decoding reduces latency by skipping logprob '\n            'calculation in proposal sampling, target sampling, and after '\n            'accepted tokens are determined.')\n\n        parser.add_argument('--model-loader-extra-config',\n                            type=nullable_str,\n                            default=EngineArgs.model_loader_extra_config,\n                            help='Extra config for model loader. '\n                            'This will be passed to the model loader '\n                            'corresponding to the chosen load_format. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary.')\n        parser.add_argument(\n            '--ignore-patterns',\n            action=\"append\",\n            type=str,\n            default=[],\n            help=\"The pattern(s) to ignore when loading the model.\"\n            \"Default to 'original/**/*' to avoid repeated loading of llama's \"\n            \"checkpoints.\")\n        parser.add_argument(\n            '--preemption-mode',\n            type=str,\n            default=None,\n            help='If \\'recompute\\', the engine performs preemption by '\n            'recomputing; If \\'swap\\', the engine performs preemption by '\n            'block swapping.')\n\n        parser.add_argument(\n            \"--served-model-name\",\n            nargs=\"+\",\n            type=str,\n            default=None,\n            help=\"The model name(s) used in the API. If multiple \"\n            \"names are provided, the server will respond to any \"\n            \"of the provided names. The model name in the model \"\n            \"field of a response will be the first name in this \"\n            \"list. If not specified, the model name will be the \"\n            \"same as the `--model` argument. Noted that this name(s)\"\n            \"will also be used in `model_name` tag content of \"\n            \"prometheus metrics, if multiple names provided, metrics\"\n            \"tag will take the first one.\")\n        parser.add_argument('--qlora-adapter-name-or-path',\n                            type=str,\n                            default=None,\n                            help='Name or path of the QLoRA adapter.')\n\n        parser.add_argument(\n            '--otlp-traces-endpoint',\n            type=str,\n            default=None,\n            help='Target URL to which OpenTelemetry traces will be sent.')\n        parser.add_argument(\n            '--collect-detailed-traces',\n            type=str,\n            default=None,\n            help=\"Valid choices are \" +\n            \",\".join(ALLOWED_DETAILED_TRACE_MODULES) +\n            \". It makes sense to set this only if --otlp-traces-endpoint is\"\n            \" set. If set, it will collect detailed traces for the specified \"\n            \"modules. This involves use of possibly costly and or blocking \"\n            \"operations and hence might have a performance impact.\")\n\n        parser.add_argument(\n            '--disable-async-output-proc',\n            action='store_true',\n            default=EngineArgs.disable_async_output_proc,\n            help=\"Disable async output processing. This may result in \"\n            \"lower performance.\")\n        parser.add_argument(\n            '--override-neuron-config',\n            type=json.loads,\n            default=None,\n            help=\"Override or set neuron device configuration. \"\n            \"e.g. {\\\"cast_logits_dtype\\\": \\\"bloat16\\\"}.'\")\n\n        parser.add_argument(\n            '--scheduling-policy',\n            choices=['fcfs', 'priority'],\n            default=\"fcfs\",\n            help='The scheduling policy to use. \"fcfs\" (first come first served'\n            ', i.e. requests are handled in order of arrival; default) '\n            'or \"priority\" (requests are handled based on given '\n            'priority (lower value means earlier handling) and time of '\n            'arrival deciding any ties).')\n\n        return parser\n\n    @classmethod\n    def from_cli_args(cls, args: argparse.Namespace):\n        # Get the list of attributes of this dataclass.\n        attrs = [attr.name for attr in dataclasses.fields(cls)]\n        # Set the attributes from the parsed arguments.\n        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})\n        return engine_args\n\n    def create_model_config(self) -> ModelConfig:\n        return ModelConfig(\n            model=self.model,\n            # We know this is not None because we set it in __post_init__\n            tokenizer=cast(str, self.tokenizer),\n            tokenizer_mode=self.tokenizer_mode,\n            trust_remote_code=self.trust_remote_code,\n            dtype=self.dtype,\n            seed=self.seed,\n            revision=self.revision,\n            code_revision=self.code_revision,\n            rope_scaling=self.rope_scaling,\n            rope_theta=self.rope_theta,\n            tokenizer_revision=self.tokenizer_revision,\n            max_model_len=self.max_model_len,\n            quantization=self.quantization,\n            quantization_param_path=self.quantization_param_path,\n            enforce_eager=self.enforce_eager,\n            max_context_len_to_capture=self.max_context_len_to_capture,\n            max_seq_len_to_capture=self.max_seq_len_to_capture,\n            max_logprobs=self.max_logprobs,\n            disable_sliding_window=self.disable_sliding_window,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            served_model_name=self.served_model_name,\n            limit_mm_per_prompt=self.limit_mm_per_prompt,\n            use_async_output_proc=not self.disable_async_output_proc,\n            override_neuron_config=self.override_neuron_config,\n            config_format=self.config_format,\n            mm_processor_kwargs=self.mm_processor_kwargs,\n        )\n\n    def create_load_config(self) -> LoadConfig:\n        return LoadConfig(\n            load_format=self.load_format,\n            download_dir=self.download_dir,\n            model_loader_extra_config=self.model_loader_extra_config,\n            ignore_patterns=self.ignore_patterns,\n        )\n\n    def create_engine_config(self) -> EngineConfig:\n        # gguf file needs a specific model loader and doesn't use hf_repo\n        if check_gguf_file(self.model):\n            self.quantization = self.load_format = \"gguf\"\n\n        # bitsandbytes quantization needs a specific model loader\n        # so we make sure the quant method and the load format are consistent\n        if (self.quantization == \"bitsandbytes\" or\n           self.qlora_adapter_name_or_path is not None) and \\\n           self.load_format != \"bitsandbytes\":\n            raise ValueError(\n                \"BitsAndBytes quantization and QLoRA adapter only support \"\n                f\"'bitsandbytes' load format, but got {self.load_format}\")\n\n        if (self.load_format == \"bitsandbytes\" or\n            self.qlora_adapter_name_or_path is not None) and \\\n            self.quantization != \"bitsandbytes\":\n            raise ValueError(\n                \"BitsAndBytes load format and QLoRA adapter only support \"\n                f\"'bitsandbytes' quantization, but got {self.quantization}\")\n\n        assert self.cpu_offload_gb >= 0, (\n            \"CPU offload space must be non-negative\"\n            f\", but got {self.cpu_offload_gb}\")\n\n        device_config = DeviceConfig(device=self.device)\n        model_config = self.create_model_config()\n\n        if model_config.is_multimodal_model:\n            if self.enable_prefix_caching:\n                logger.warning(\n                    \"--enable-prefix-caching is currently not \"\n                    \"supported for multimodal models and has been disabled.\")\n            self.enable_prefix_caching = False\n\n        cache_config = CacheConfig(\n            # neuron needs block_size = max_model_len\n            block_size=self.block_size if self.device != \"neuron\" else\n            (self.max_model_len if self.max_model_len is not None else 0),\n            gpu_memory_utilization=self.gpu_memory_utilization,\n            swap_space=self.swap_space,\n            cache_dtype=self.kv_cache_dtype,\n            is_attention_free=model_config.is_attention_free,\n            num_gpu_blocks_override=self.num_gpu_blocks_override,\n            sliding_window=model_config.get_sliding_window(),\n            enable_prefix_caching=self.enable_prefix_caching,\n            cpu_offload_gb=self.cpu_offload_gb,\n        )\n        parallel_config = ParallelConfig(\n            pipeline_parallel_size=self.pipeline_parallel_size,\n            tensor_parallel_size=self.tensor_parallel_size,\n            worker_use_ray=self.worker_use_ray,\n            max_parallel_loading_workers=self.max_parallel_loading_workers,\n            disable_custom_all_reduce=self.disable_custom_all_reduce,\n            tokenizer_pool_config=TokenizerPoolConfig.create_config(\n                self.tokenizer_pool_size,\n                self.tokenizer_pool_type,\n                self.tokenizer_pool_extra_config,\n            ),\n            ray_workers_use_nsight=self.ray_workers_use_nsight,\n            distributed_executor_backend=self.distributed_executor_backend)\n\n        max_model_len = model_config.max_model_len\n        use_long_context = max_model_len > 32768\n        if self.enable_chunked_prefill is None:\n            # If not explicitly set, enable chunked prefill by default for\n            # long context (> 32K) models. This is to avoid OOM errors in the\n            # initial memory profiling phase.\n\n            # Chunked prefill is currently disabled for multimodal models by\n            # default.\n            if use_long_context and not model_config.is_multimodal_model:\n                is_gpu = device_config.device_type == \"cuda\"\n                use_sliding_window = (model_config.get_sliding_window()\n                                      is not None)\n                use_spec_decode = self.speculative_model is not None\n                if (is_gpu and not use_sliding_window and not use_spec_decode\n                        and not self.enable_lora\n                        and not self.enable_prompt_adapter):\n                    self.enable_chunked_prefill = True\n                    logger.warning(\n                        \"Chunked prefill is enabled by default for models with \"\n                        \"max_model_len > 32K. Currently, chunked prefill might \"\n                        \"not work with some features or models. If you \"\n                        \"encounter any issues, please disable chunked prefill \"\n                        \"by setting --enable-chunked-prefill=False.\")\n            if self.enable_chunked_prefill is None:\n                self.enable_chunked_prefill = False\n\n        if not self.enable_chunked_prefill and use_long_context:\n            logger.warning(\n                \"The model has a long context length (%s). This may cause OOM \"\n                \"errors during the initial memory profiling phase, or result \"\n                \"in low performance due to small KV cache space. Consider \"\n                \"setting --max-model-len to a smaller value.\", max_model_len)\n\n        if self.num_scheduler_steps > 1 and not self.use_v2_block_manager:\n            self.use_v2_block_manager = True\n            logger.warning(\n                \"Enabled BlockSpaceManagerV2 because it is \"\n                \"required for multi-step (--num-scheduler-steps > 1)\")\n\n        speculative_config = SpeculativeConfig.maybe_create_spec_config(\n            target_model_config=model_config,\n            target_parallel_config=parallel_config,\n            target_dtype=self.dtype,\n            speculative_model=self.speculative_model,\n            speculative_model_quantization = \\\n                self.speculative_model_quantization,\n            speculative_draft_tensor_parallel_size = \\\n                self.speculative_draft_tensor_parallel_size,\n            num_speculative_tokens=self.num_speculative_tokens,\n            speculative_disable_mqa_scorer=self.speculative_disable_mqa_scorer,\n            speculative_disable_by_batch_size=self.\n            speculative_disable_by_batch_size,\n            speculative_max_model_len=self.speculative_max_model_len,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            use_v2_block_manager=self.use_v2_block_manager,\n            disable_log_stats=self.disable_log_stats,\n            ngram_prompt_lookup_max=self.ngram_prompt_lookup_max,\n            ngram_prompt_lookup_min=self.ngram_prompt_lookup_min,\n            draft_token_acceptance_method=\\\n                self.spec_decoding_acceptance_method,\n            typical_acceptance_sampler_posterior_threshold=self.\n            typical_acceptance_sampler_posterior_threshold,\n            typical_acceptance_sampler_posterior_alpha=self.\n            typical_acceptance_sampler_posterior_alpha,\n            disable_logprobs=self.disable_logprobs_during_spec_decoding,\n        )\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if self.num_scheduler_steps > 1:\n            if speculative_config is not None:\n                raise ValueError(\"Speculative decoding is not supported with \"\n                                 \"multi-step (--num-scheduler-steps > 1)\")\n            if self.enable_chunked_prefill and self.pipeline_parallel_size > 1:\n                raise ValueError(\"Multi-Step Chunked-Prefill is not supported \"\n                                 \"for pipeline-parallel-size > 1\")\n\n        # make sure num_lookahead_slots is set the higher value depending on\n        # if we are using speculative decoding or multi-step\n        num_lookahead_slots = max(self.num_lookahead_slots,\n                                  self.num_scheduler_steps - 1)\n        num_lookahead_slots = num_lookahead_slots \\\n            if speculative_config is None \\\n            else speculative_config.num_lookahead_slots\n\n        scheduler_config = SchedulerConfig(\n            max_num_batched_tokens=self.max_num_batched_tokens,\n            max_num_seqs=self.max_num_seqs,\n            max_model_len=model_config.max_model_len,\n            use_v2_block_manager=self.use_v2_block_manager,\n            num_lookahead_slots=num_lookahead_slots,\n            delay_factor=self.scheduler_delay_factor,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            embedding_mode=model_config.embedding_mode,\n            is_multimodal_model=model_config.is_multimodal_model,\n            preemption_mode=self.preemption_mode,\n            num_scheduler_steps=self.num_scheduler_steps,\n            multi_step_stream_outputs=self.multi_step_stream_outputs,\n            send_delta_data=(envs.VLLM_USE_RAY_SPMD_WORKER\n                             and parallel_config.use_ray),\n            policy=self.scheduling_policy,\n        )\n        lora_config = LoRAConfig(\n            max_lora_rank=self.max_lora_rank,\n            max_loras=self.max_loras,\n            fully_sharded_loras=self.fully_sharded_loras,\n            lora_extra_vocab_size=self.lora_extra_vocab_size,\n            long_lora_scaling_factors=self.long_lora_scaling_factors,\n            lora_dtype=self.lora_dtype,\n            max_cpu_loras=self.max_cpu_loras if self.max_cpu_loras\n            and self.max_cpu_loras > 0 else None) if self.enable_lora else None\n\n        if self.qlora_adapter_name_or_path is not None and \\\n            self.qlora_adapter_name_or_path != \"\":\n            if self.model_loader_extra_config is None:\n                self.model_loader_extra_config = {}\n            self.model_loader_extra_config[\n                \"qlora_adapter_name_or_path\"] = self.qlora_adapter_name_or_path\n\n        load_config = self.create_load_config()\n\n        prompt_adapter_config = PromptAdapterConfig(\n            max_prompt_adapters=self.max_prompt_adapters,\n            max_prompt_adapter_token=self.max_prompt_adapter_token) \\\n                                        if self.enable_prompt_adapter else None\n\n        decoding_config = DecodingConfig(\n            guided_decoding_backend=self.guided_decoding_backend)\n\n        detailed_trace_modules = []\n        if self.collect_detailed_traces is not None:\n            detailed_trace_modules = self.collect_detailed_traces.split(\",\")\n        for m in detailed_trace_modules:\n            if m not in ALLOWED_DETAILED_TRACE_MODULES:\n                raise ValueError(\n                    f\"Invalid module {m} in collect_detailed_traces. \"\n                    f\"Valid modules are {ALLOWED_DETAILED_TRACE_MODULES}\")\n        observability_config = ObservabilityConfig(\n            otlp_traces_endpoint=self.otlp_traces_endpoint,\n            collect_model_forward_time=\"model\" in detailed_trace_modules\n            or \"all\" in detailed_trace_modules,\n            collect_model_execute_time=\"worker\" in detailed_trace_modules\n            or \"all\" in detailed_trace_modules,\n        )\n\n        if (model_config.get_sliding_window() is not None\n                and scheduler_config.chunked_prefill_enabled\n                and not scheduler_config.use_v2_block_manager):\n            raise ValueError(\n                \"Chunked prefill is not supported with sliding window. \"\n                \"Set --disable-sliding-window to disable sliding window.\")\n\n        return EngineConfig(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            decoding_config=decoding_config,\n            observability_config=observability_config,\n            prompt_adapter_config=prompt_adapter_config,\n        )\n\n\n@dataclass\nclass AsyncEngineArgs(EngineArgs):\n    \"\"\"Arguments for asynchronous vLLM engine.\"\"\"\n    disable_log_requests: bool = False\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser,\n                     async_args_only: bool = False) -> FlexibleArgumentParser:\n        if not async_args_only:\n            parser = EngineArgs.add_cli_args(parser)\n        parser.add_argument('--disable-log-requests',\n                            action='store_true',\n                            help='Disable logging requests.')\n        return parser\n\n\nclass StoreBoolean(argparse.Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if values.lower() == \"true\":\n            setattr(namespace, self.dest, True)\n        elif values.lower() == \"false\":\n            setattr(namespace, self.dest, False)\n        else:\n            raise ValueError(f\"Invalid boolean value: {values}. \"\n                             \"Expected 'true' or 'false'.\")\n\n\n# These functions are used by sphinx to build the documentation\ndef _engine_args_parser():\n    return EngineArgs.add_cli_args(FlexibleArgumentParser())\n\n\ndef _async_engine_args_parser():\n    return AsyncEngineArgs.add_cli_args(FlexibleArgumentParser(),\n                                        async_args_only=True)\n",
      "diff": "diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 1ce9e6200..41963dcb1 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -373,12 +373,13 @@ class EngineArgs:\n                             action='store_true',\n                             help='Disables sliding window, '\n                             'capping to sliding window size')\n-        parser.add_argument(\n-            '--use-v2-block-manager',\n-            default=EngineArgs.use_v2_block_manager,\n-            action='store_true',\n-            help='Use BlockSpaceMangerV2. By default this is set to True. '\n-            'Set to False to use BlockSpaceManagerV1')\n+        parser.add_argument('--use-v2-block-manager',\n+                            action='store_true',\n+                            help='[DEPRECATED] block manager v1 has been '\n+                            'removed and SelfAttnBlockSpaceManager (i.e. '\n+                            'block manager v2) is now the default. '\n+                            'Setting this flag to True or False'\n+                            ' has no effect on vLLM behavior.')\n         parser.add_argument(\n             '--num-lookahead-slots',\n             type=int,\n@@ -969,12 +970,6 @@ class EngineArgs:\n                 \"in low performance due to small KV cache space. Consider \"\n                 \"setting --max-model-len to a smaller value.\", max_model_len)\n \n-        if self.num_scheduler_steps > 1 and not self.use_v2_block_manager:\n-            self.use_v2_block_manager = True\n-            logger.warning(\n-                \"Enabled BlockSpaceManagerV2 because it is \"\n-                \"required for multi-step (--num-scheduler-steps > 1)\")\n-\n         speculative_config = SpeculativeConfig.maybe_create_spec_config(\n             target_model_config=model_config,\n             target_parallel_config=parallel_config,\n@@ -990,7 +985,6 @@ class EngineArgs:\n             speculative_disable_by_batch_size,\n             speculative_max_model_len=self.speculative_max_model_len,\n             enable_chunked_prefill=self.enable_chunked_prefill,\n-            use_v2_block_manager=self.use_v2_block_manager,\n             disable_log_stats=self.disable_log_stats,\n             ngram_prompt_lookup_max=self.ngram_prompt_lookup_max,\n             ngram_prompt_lookup_min=self.ngram_prompt_lookup_min,\n@@ -1021,11 +1015,20 @@ class EngineArgs:\n             if speculative_config is None \\\n             else speculative_config.num_lookahead_slots\n \n+        if not self.use_v2_block_manager:\n+            logger.warning(\n+                \"[DEPRECATED] Block manager v1 has been removed, \"\n+                \"and setting --use-v2-block-manager to True or False has \"\n+                \"no effect on vLLM behavior. Please remove \"\n+                \"--use-v2-block-manager in your engine argument. \"\n+                \"If your use case is not supported by \"\n+                \"SelfAttnBlockSpaceManager (i.e. block manager v2),\"\n+                \" please file an issue with detailed information.\")\n+\n         scheduler_config = SchedulerConfig(\n             max_num_batched_tokens=self.max_num_batched_tokens,\n             max_num_seqs=self.max_num_seqs,\n             max_model_len=model_config.max_model_len,\n-            use_v2_block_manager=self.use_v2_block_manager,\n             num_lookahead_slots=num_lookahead_slots,\n             delay_factor=self.scheduler_delay_factor,\n             enable_chunked_prefill=self.enable_chunked_prefill,\n@@ -1081,13 +1084,6 @@ class EngineArgs:\n             or \"all\" in detailed_trace_modules,\n         )\n \n-        if (model_config.get_sliding_window() is not None\n-                and scheduler_config.chunked_prefill_enabled\n-                and not scheduler_config.use_v2_block_manager):\n-            raise ValueError(\n-                \"Chunked prefill is not supported with sliding window. \"\n-                \"Set --disable-sliding-window to disable sliding window.\")\n-\n         return EngineConfig(\n             model_config=model_config,\n             cache_config=cache_config,",
      "change_type": "modified",
      "lines_added": 18,
      "lines_removed": 22
    },
    {
      "file_path": "vllm/engine/llm_engine.py",
      "old_content": "import time\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Deque, Dict,\n                    Iterable, List, Mapping, NamedTuple, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Type, Union, cast, overload\n\nimport torch\nfrom typing_extensions import TypeVar\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\n                         EngineConfig, LoadConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.entrypoints.openai.logits_processors import get_logits_processors\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.gpu_executor import GPUExecutor\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, DecoderOnlyInputs,\n                         EncoderDecoderInputs, InputRegistry, PromptType)\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding import (\n    get_local_guided_decoding_logits_processor)\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.outputs import (EmbeddingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import RequestOutputKind, SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           Sequence, SequenceGroup, SequenceGroupMetadata,\n                           SequenceGroupOutput, SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter, Device, deprecate_kwargs, weak_bind\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_G = TypeVar(\"_G\", bound=BaseTokenizerGroup, default=BaseTokenizerGroup)\n_O = TypeVar(\"_O\", RequestOutput, EmbeddingRequestOutput)\n\n\n@dataclass\nclass SchedulerOutputState:\n    \"\"\"Caches the scheduler outputs for a virtual engine. Used for Multi-Step\"\"\"\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    scheduler_outputs: Optional[SchedulerOutputs] = None\n    allow_async_output_proc: bool = False\n    last_output: Optional[SamplerOutput] = None\n\n\nclass OutputData(NamedTuple):\n    outputs: List[SamplerOutput]\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n    scheduler_outputs: SchedulerOutputs\n    is_async: bool\n    is_last_step: bool\n    # Indicates if this output is from the first step of the\n    # multi-step. When multi-step is disabled, this is always\n    # set to True.\n    # is_first_step_output is invalid when `outputs` has\n    # outputs from multiple steps.\n    is_first_step_output: Optional[bool]\n    skip: List[int]\n\n\nclass SchedulerContext:\n\n    def __init__(self, multi_step_stream_outputs: bool = False):\n        self.output_queue: Deque[OutputData] = deque()\n        self.request_outputs: List[Union[RequestOutput,\n                                         EmbeddingRequestOutput]] = []\n        self.seq_group_metadata_list: Optional[\n            List[SequenceGroupMetadata]] = None\n        self.scheduler_outputs: Optional[SchedulerOutputs] = None\n\n        self.multi_step_stream_outputs: bool = multi_step_stream_outputs\n\n    def append_output(self, outputs: List[SamplerOutput],\n                      seq_group_metadata_list: List[SequenceGroupMetadata],\n                      scheduler_outputs: SchedulerOutputs, is_async: bool,\n                      is_last_step: bool,\n                      is_first_step_output: Optional[bool]):\n        self.output_queue.append(\n            OutputData(outputs=outputs,\n                       seq_group_metadata_list=seq_group_metadata_list,\n                       scheduler_outputs=scheduler_outputs,\n                       is_async=is_async,\n                       is_last_step=is_last_step,\n                       is_first_step_output=is_first_step_output,\n                       skip=[]))\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving\n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return cast(_O, output)\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        speculative_config: Optional[SpeculativeConfig],\n        decoding_config: Optional[DecodingConfig],\n        observability_config: Optional[ObservabilityConfig],\n        prompt_adapter_config: Optional[PromptAdapterConfig],\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        use_cached_outputs: bool = False,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"override_neuron_config=%s, \"\n            \"rope_scaling=%r, rope_theta=%r, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n            \"num_scheduler_steps=%d, chunked_prefill_enabled=%s \"\n            \"multi_step_stream_outputs=%s, enable_prefix_caching=%s, \"\n            \"use_async_output_proc=%s, use_cached_outputs=%s, \"\n            \"mm_processor_kwargs=%s)\",\n            VLLM_VERSION,\n            model_config.model,\n            speculative_config,\n            model_config.tokenizer,\n            model_config.skip_tokenizer_init,\n            model_config.tokenizer_mode,\n            model_config.revision,\n            model_config.override_neuron_config,\n            model_config.rope_scaling,\n            model_config.rope_theta,\n            model_config.tokenizer_revision,\n            model_config.trust_remote_code,\n            model_config.dtype,\n            model_config.max_model_len,\n            load_config.download_dir,\n            load_config.load_format,\n            parallel_config.tensor_parallel_size,\n            parallel_config.pipeline_parallel_size,\n            parallel_config.disable_custom_all_reduce,\n            model_config.quantization,\n            model_config.enforce_eager,\n            cache_config.cache_dtype,\n            model_config.quantization_param_path,\n            device_config.device,\n            decoding_config,\n            observability_config,\n            model_config.seed,\n            model_config.served_model_name,\n            scheduler_config.use_v2_block_manager,\n            scheduler_config.num_scheduler_steps,\n            scheduler_config.chunked_prefill_enabled,\n            scheduler_config.multi_step_stream_outputs,\n            cache_config.enable_prefix_caching,\n            model_config.use_async_output_proc,\n            use_cached_outputs,\n            model_config.mm_processor_kwargs,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.speculative_config = speculative_config\n        self.load_config = load_config\n        self.decoding_config = decoding_config or DecodingConfig()\n        self.prompt_adapter_config = prompt_adapter_config\n        self.observability_config = observability_config or ObservabilityConfig(\n        )\n        self.log_stats = log_stats\n        self.use_cached_outputs = use_cached_outputs\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            model_config)\n\n        self.input_preprocessor = InputPreprocessor(model_config,\n                                                    self.tokenizer)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            model_config)\n\n        self.model_executor = executor_class(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            prompt_adapter_config=prompt_adapter_config,\n            observability_config=self.observability_config,\n        )\n\n        if not self.model_config.embedding_mode:\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(model_config.dtype),\n                    \"tensor_parallel_size\":\n                    parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        self.cached_scheduler_outputs = [\n            SchedulerOutputState()\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.scheduler_contexts = [\n            SchedulerContext(multi_step_stream_outputs=self.scheduler_config.\n                             multi_step_stream_outputs)\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        if model_config.use_async_output_proc:\n            process_model_outputs = weak_bind(self._process_model_outputs)\n\n            self.async_callbacks = [\n                partial(process_model_outputs,\n                        ctx=self.scheduler_contexts[v_id])\n                for v_id in range(self.parallel_config.pipeline_parallel_size)\n            ]\n        else:\n            self.async_callbacks = []\n\n        # Currently used by AsyncLLMEngine to ensure quick append\n        # of request outputs to asyncio queues\n        self.process_request_outputs_callback: Optional[Callable] = None\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(\n                scheduler_config, cache_config, lora_config,\n                parallel_config.pipeline_parallel_size,\n                self.async_callbacks[v_id]\n                if model_config.use_async_output_proc else None)\n            for v_id in range(parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(model_name=model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: EngineConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            elif distributed_executor_backend == \"mp\":\n                # FIXME(kunshang):\n                # spawn needs calling `if __name__ == '__main__':``\n                # fork is not supported for xpu start new process.\n                logger.error(\n                    \"Both start methods (spawn and fork) have issue \"\n                    \"on XPU if you use mp backend, Please try ray instead.\")\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config()\n        executor_class = cls._get_executor_cls(engine_config)\n        # Create the LLM engine.\n        engine = cls(\n            **engine_config.to_dict(),\n            executor_class=executor_class,\n            log_stats=not engine_args.disable_log_stats,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n\n        return engine\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    def get_tokenizer_group(\n        self,\n        group_type: Type[_G] = BaseTokenizerGroup,\n    ) -> _G:\n        tokenizer_group = self.tokenizer\n\n        if tokenizer_group is None:\n            raise ValueError(\"Unable to get tokenizer because \"\n                             \"skip_tokenizer_init is True\")\n        if not isinstance(tokenizer_group, group_type):\n            raise TypeError(\"Invalid type of tokenizer group. \"\n                            f\"Expected type: {group_type}, but \"\n                            f\"found type: {type(tokenizer_group)}\")\n\n        return tokenizer_group\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: Union[DecoderOnlyInputs, EncoderDecoderInputs],\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> None:\n        self._validate_model_inputs(processed_inputs)\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)\n\n        seq = Sequence(seq_id, processed_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = None\n        if 'encoder_prompt_token_ids' in processed_inputs:\n            encoder_seq = Sequence(seq_id,\n                                   processed_inputs,\n                                   block_size,\n                                   eos_token_id,\n                                   lora_request,\n                                   prompt_adapter_request,\n                                   from_decoder_prompt=False)\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    @overload  # DEPRECATED\n    def add_request(\n        self,\n        request_id: str,\n        *,\n        inputs: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @overload\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    def add_request(\n            self,\n            request_id: str,\n            prompt: Optional[PromptType] = None,\n            params: Optional[Union[SamplingParams, PoolingParams]] = None,\n            arrival_time: Optional[float] = None,\n            lora_request: Optional[LoRARequest] = None,\n            trace_headers: Optional[Mapping[str, str]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n            priority: int = 0,\n            *,\n            inputs: Optional[PromptType] = None,  # DEPRECATED\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            params: Parameters for sampling or pooling.\n                :class:`~vllm.SamplingParams` for text generation.\n                :class:`~vllm.PoolingParams` for pooling.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            trace_headers: OpenTelemetry trace headers.\n            priority: The priority of the request.\n                Only applicable with priority scheduling.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `n` number of :class:`~vllm.Sequence` objects.\n            - Create a :class:`~vllm.SequenceGroup` object\n              from the list of :class:`~vllm.Sequence`.\n            - Add the :class:`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert prompt is not None and params is not None\n\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n\n        if priority != 0 and not self.scheduler_config.policy == \"priority\":\n            raise ValueError(f\"Got priority {priority} but \"\n                             \"Priority scheduling is not enabled.\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        preprocessed_inputs = self.input_preprocessor.preprocess(\n            prompt,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n        processed_inputs = self.input_processor(preprocessed_inputs)\n\n        # This is a bit of a hack - copy the mm_processor_kwargs that were\n        # used in the input processor to the processed output, since these\n        # kwargs are presumed to be immutable and the values should be aligned\n        # between the input processor (here) and the input mapper.\n        processed_inputs[\"mm_processor_kwargs\"] = preprocessed_inputs.get(\n            \"mm_processor_kwargs\")\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n    def _create_sequence_group_with_sampling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        sampling_params: SamplingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with SamplingParams.\"\"\"\n        max_logprobs = self.get_model_config().max_logprobs\n        if (sampling_params.logprobs\n                and sampling_params.logprobs > max_logprobs) or (\n                    sampling_params.prompt_logprobs\n                    and sampling_params.prompt_logprobs > max_logprobs):\n            raise ValueError(f\"Cannot request more than \"\n                             f\"{max_logprobs} logprobs.\")\n\n        sampling_params = self._build_logits_processors(\n            sampling_params, lora_request)\n\n        # Defensive copy of SamplingParams, which are used by the sampler,\n        # this doesn't deep-copy LogitsProcessor objects\n        sampling_params = sampling_params.clone()\n\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, seq.eos_token_id)\n\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority)\n\n        return seq_group\n\n    def _create_sequence_group_with_pooling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        pooling_params: PoolingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with PoolingParams.\"\"\"\n        # Defensive copy of PoolingParams, which are used by the pooler\n        pooling_params = pooling_params.clone()\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            pooling_params=pooling_params,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority)\n        return seq_group\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n\n        Details:\n            - Refer to the\n              :meth:`~vllm.core.scheduler.Scheduler.abort_seq_group`\n              from class :class:`~vllm.core.scheduler.Scheduler`.\n\n        Example:\n            >>> # initialize engine and add a request with request_id\n            >>> request_id = str(0)\n            >>> # abort the request\n            >>> engine.abort_request(request_id)\n        \"\"\"\n        for scheduler in self.scheduler:\n            scheduler.abort_seq_group(request_id)\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Gets the parallel configuration.\"\"\"\n        return self.parallel_config\n\n    def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Gets the decoding configuration.\"\"\"\n        return self.decoding_config\n\n    def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Gets the scheduler configuration.\"\"\"\n        return self.scheduler_config\n\n    def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Gets the LoRA configuration.\"\"\"\n        return self.lora_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return sum(scheduler.get_num_unfinished_seq_groups()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return any(scheduler.has_unfinished_seqs()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests_for_virtual_engine(\n            self, virtual_engine: int) -> bool:\n        \"\"\"\n        Returns True if there are unfinished requests for the virtual engine.\n        \"\"\"\n        return self.scheduler[virtual_engine].has_unfinished_seqs()\n\n    @staticmethod\n    def _process_sequence_group_outputs(\n        seq_group: SequenceGroup,\n        outputs: List[EmbeddingSequenceGroupOutput],\n    ) -> None:\n        seq_group.embeddings = outputs[0].embeddings\n\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.FINISHED_STOPPED\n\n        return\n\n    def _update_num_computed_tokens_for_multi_step_prefill(\n            self, seq_group: SequenceGroup,\n            seq_group_meta: SequenceGroupMetadata,\n            is_first_step_output: Optional[bool]):\n        \"\"\"\n        This function updates num_computed_tokens for prompt sequences\n        when Multi-Step is enabled.\n\n        seq_group: SequenceGroup to update the num_computed_tokens for. \n        seq_group_meta: Metadata of the given SequenceGroup.\n        is_first_step_output: Optional[bool] - \n            When available, is_first_step_output indicates if the appended\n            output token is the output of the first-step in multi-step.\n            A value of None indicates that outputs from all steps in\n            in multi-step are submitted in a single burst.\n        \"\"\"\n\n        assert self.scheduler_config.is_multi_step\n\n        if not seq_group_meta.is_prompt:\n            # num_computed_token updates for multi-step decodes happen after\n            # the tokens are appended to the sequence.\n            return\n\n        do_update: bool = False\n        if self.scheduler_config.chunked_prefill_enabled:\n            # In multi-step + chunked-prefill case, the prompt sequences\n            # that are scheduled are fully processed in the first step.\n            do_update = is_first_step_output is None or is_first_step_output\n        else:\n            # Normal multi-step decoding case. In this case prompt-sequences\n            # are actually single-stepped. Always update in this case.\n            assert seq_group.state.num_steps == 1\n            do_update = True\n\n        if do_update:\n            seq_group.update_num_computed_tokens(\n                seq_group_meta.token_chunk_size)\n\n    def _process_model_outputs(self,\n                               ctx: SchedulerContext,\n                               request_id: Optional[str] = None) -> None:\n        \"\"\"Apply the model output to the sequences in the scheduled seq groups\n        and return responses.\n\n        ctx: The virtual engine context to work on\n        request_id: If provided, then only this request is going to be processed\n        \"\"\"\n\n        now = time.time()\n\n        if len(ctx.output_queue) == 0:\n            return None\n\n        # Get pending async postprocessor\n        if request_id:\n            # When we process only one request, no pop is required\n            # (since later we will process all of the rest)\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output, skip) = ctx.output_queue[0]\n        else:\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output,\n             skip) = ctx.output_queue.popleft()\n\n        # Sanity check\n        assert len(seq_group_metadata_list) == len(\n            scheduler_outputs.scheduled_seq_groups)\n\n        has_multiple_outputs: bool = len(outputs) > 1\n        outputs_by_sequence_group: List[List[SequenceGroupOutput]]\n        if has_multiple_outputs:\n            assert self.scheduler_config.is_multi_step or \\\n                     self.speculative_config\n            # Organize outputs by [step][sequence group] instead of\n            # [sequence group][step].\n            outputs_by_sequence_group = create_output_by_sequence_group(\n                outputs, num_seq_groups=len(seq_group_metadata_list))\n            # We have outputs for multiple steps submitted in a single burst,\n            # so invalidate is_first_step_output.\n            is_first_step_output = None\n        else:\n            outputs_by_sequence_group = outputs\n\n        # Determine the requests we need to operate on\n        if request_id:\n            indices = []\n            for i, seq_group_meta in enumerate(seq_group_metadata_list):\n                if seq_group_meta.request_id == request_id:\n                    assert i not in skip  # Cannot be called twice\n                    indices.append(i)\n                    break\n\n            # If the request_id was not found, then it means that\n            # this is a new request that has no pending async\n            # postprocessor\n            if not indices:\n                return\n        else:\n            indices = range(len(seq_group_metadata_list))  # type: ignore\n\n        finished_before: List[int] = []\n        finished_now: List[int] = []\n        for i in indices:\n            if i in skip:\n                continue\n\n            seq_group_meta = seq_group_metadata_list[i]\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group: SequenceGroup = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                finished_before.append(i)\n                continue\n\n            output: List[SequenceGroupOutput]\n            if has_multiple_outputs:\n                output = outputs_by_sequence_group[i]\n            else:\n                output = [outputs_by_sequence_group[0][i]]\n\n            if not is_async:\n                if self.scheduler_config.is_multi_step:\n                    # Updates happen only if the sequence is prefill\n                    self._update_num_computed_tokens_for_multi_step_prefill(\n                        seq_group, seq_group_meta, is_first_step_output)\n                else:\n                    seq_group.update_num_computed_tokens(\n                        seq_group_meta.token_chunk_size or 0)\n\n            if outputs:\n                for o in outputs:\n                    if (isinstance(o, SamplerOutput)\n                            and seq_group.metrics is not None):\n                        if seq_group.metrics.model_forward_time is not None:\n                            seq_group.metrics.model_forward_time += (\n                                o.model_forward_time or 0)\n                        else:\n                            seq_group.metrics.model_forward_time = (\n                                o.model_forward_time)\n                        if seq_group.metrics.model_execute_time is not None:\n                            seq_group.metrics.model_execute_time += (\n                                o.model_execute_time or 0)\n                        else:\n                            seq_group.metrics.model_execute_time = (\n                                o.model_execute_time)\n\n            if self.model_config.embedding_mode:\n                self._process_sequence_group_outputs(seq_group, output)\n            else:\n                self.output_processor.process_prompt_logprob(seq_group, output)\n                if seq_group_meta.do_sample:\n                    self.output_processor.process_outputs(\n                        seq_group, output, is_async)\n\n            if seq_group.is_finished():\n                finished_now.append(i)\n\n        # Generate outputs for the requests that finished this iteration\n        for i in finished_now:\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group, use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # When we process a single request, we skip it for the next time,\n        # and invoke the request output callback (if there was final output)\n        if request_id:\n            assert len(indices) == 1\n            skip.append(indices[0])\n\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Free currently finished requests\n        if finished_now:\n            for scheduler in self.scheduler:\n                scheduler.free_finished_seq_groups()\n\n        # For multi-step without streaming, don't create outputs each iteration\n        if not is_last_step and not ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Create the outputs\n        for i in indices:\n            if i in skip or i in finished_before or i in finished_now:\n                continue  # Avoids double processing\n\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group, use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # For multi-step with streaming, create outputs each iteration\n        if not is_last_step and ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if self.process_request_outputs_callback is not None:\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        for seq_group in scheduler_outputs.ignored_seq_groups:\n            params = seq_group.sampling_params\n            if params is not None and params.output_kind == (\n                    RequestOutputKind.DELTA) and not seq_group.is_finished():\n                continue\n\n            request_output = RequestOutputFactory.create(\n                seq_group, use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # Immediately process request outputs here (if callback is given)\n        if (ctx.request_outputs\n                and self.process_request_outputs_callback is not None):\n            self.process_request_outputs_callback(ctx.request_outputs)\n            ctx.request_outputs.clear()\n\n        # For async case, we need to record the stats here.\n        # For non-async case, the stats are done in the\n        # LLMEngine/AsyncLLMEngine directly\n        if is_async:\n            # Log stats.\n            self.do_log_stats(scheduler_outputs, outputs, finished_before,\n                              skip)\n\n            # Tracing\n            self.do_tracing(scheduler_outputs)\n\n        return None\n\n    def _advance_to_next_step(\n            self, output: List[SamplerOutput],\n            seq_group_metadata_list: List[SequenceGroupMetadata],\n            scheduled_seq_groups: List[ScheduledSequenceGroup]) -> None:\n        \"\"\"Given model output from a single run, append the tokens to the\n        sequences. This is normally done inside output processor, but it is\n        required if the worker is to perform async forward pass to next step.\n        \"\"\"\n        for seq_group_metadata, sequence_group_outputs, scheduled_seq_group in \\\n            zip(seq_group_metadata_list, output, scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                continue\n\n            if self.scheduler_config.is_multi_step:\n                # Updates happen only if the sequence is prefill\n                self._update_num_computed_tokens_for_multi_step_prefill(\n                    seq_group, seq_group_metadata,\n                    seq_group.state.num_steps == 1)\n            else:\n                token_chunk_size = (seq_group_metadata.token_chunk_size\n                                    if seq_group_metadata.token_chunk_size\n                                    is not None else 0)\n                seq_group.update_num_computed_tokens(token_chunk_size)\n\n            if seq_group_metadata.do_sample:\n                assert len(sequence_group_outputs.samples) == 1, (\n                    \"Async output processor expects a single sample\"\n                    \" (i.e sampling_params.n == 1)\")\n                sample = sequence_group_outputs.samples[0]\n\n                assert len(seq_group.seqs) == 1\n                seq = seq_group.seqs[0]\n\n                if self.scheduler_config.is_multi_step:\n                    is_prefill_append = seq.data.get_num_uncomputed_tokens(\n                    ) == 0\n                    seq.append_token_id(sample.output_token, sample.logprobs)\n                    if not is_prefill_append:\n                        seq_group.update_num_computed_tokens(1)\n                else:\n                    seq.append_token_id(sample.output_token, sample.logprobs)\n\n    def step(self) -> List[Union[RequestOutput, EmbeddingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        .. figure:: https://i.imgur.com/sv2HssD.png\n            :alt: Overview of the step function\n            :align: center\n\n            Overview of the step function.\n\n        Details:\n            - Step 1: Schedules the sequences to be executed in the next\n              iteration and the token blocks to be swapped in/out/copy.\n\n                - Depending on the scheduling policy,\n                  sequences may be `preempted/reordered`.\n                - A Sequence Group (SG) refer to a group of sequences\n                  that are generated from the same prompt.\n\n            - Step 2: Calls the distributed executor to execute the model.\n            - Step 3: Processes the model output. This mainly includes:\n\n                - Decodes the relevant outputs.\n                - Updates the scheduled sequence groups with model outputs\n                  based on its `sampling parameters` (`use_beam_search` or not).\n                - Frees the finished sequence groups.\n\n            - Finally, it creates and returns the newly generated results.\n\n        Example:\n            >>> # Please see the example/ folder for more detailed examples.\n            >>>\n            >>> # initialize engine and request arguments\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> example_inputs = [(0, \"What is LLM?\",\n            >>>    SamplingParams(temperature=0.0))]\n            >>>\n            >>> # Start the engine with an event loop\n            >>> while True:\n            >>>     if example_inputs:\n            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)\n            >>>         engine.add_request(str(req_id),prompt,sampling_params)\n            >>>\n            >>>     # continue the request processing\n            >>>     request_outputs = engine.step()\n            >>>     for request_output in request_outputs:\n            >>>         if request_output.finished:\n            >>>             # return or show the request output\n            >>>\n            >>>     if not (engine.has_unfinished_requests() or example_inputs):\n            >>>         break\n        \"\"\"\n        if self.parallel_config.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is only supported through AsyncLLMEngine \"\n                \"as performance will be severely degraded otherwise.\")\n\n        # For llm_engine, there is no pipeline parallel support, so the engine\n        # used is always 0.\n        virtual_engine = 0\n\n        # These are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # Clear outputs for each new scheduler iteration\n        ctx.request_outputs.clear()\n\n        # Skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            ctx.seq_group_metadata_list = seq_group_metadata_list\n            ctx.scheduler_outputs = scheduler_outputs\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                execute_model_req.async_callback = self.async_callbacks[\n                    virtual_engine]\n\n            outputs = self.model_executor.execute_model(\n                execute_model_req=execute_model_req)\n\n            # We need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, outputs)\n        else:\n            # Nothing scheduled => If there is pending async postprocessor,\n            # then finish it here.\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            # No outputs in this case\n            outputs = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # clear the cache if we have finished all the steps.\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[0] = SchedulerOutputState()\n\n            # is_first_step_output is True only when the num_steps of all\n            # the sequences are 1. When the num_steps > 1,\n            # multi_step_model_runner does the first-step output append.\n            is_first_step_output: bool = False if not seq_group_metadata_list \\\n                else seq_group_metadata_list[0].state.num_steps == 1\n\n            # Add results to the output_queue\n            ctx.append_output(outputs=outputs,\n                              seq_group_metadata_list=seq_group_metadata_list,\n                              scheduler_outputs=scheduler_outputs,\n                              is_async=allow_async_output_proc,\n                              is_last_step=True,\n                              is_first_step_output=is_first_step_output)\n\n            if outputs and allow_async_output_proc:\n                assert len(outputs) == 1, (\n                    \"Async postprocessor expects only a single output set\")\n\n                self._advance_to_next_step(\n                    outputs[0], seq_group_metadata_list,\n                    scheduler_outputs.scheduled_seq_groups)\n\n            # Check if need to run the usual non-async path\n            if not allow_async_output_proc:\n                self._process_model_outputs(ctx=ctx)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, outputs)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n        else:\n            # Multi-step case\n            return ctx.request_outputs\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            assert len(ctx.output_queue) == 0\n\n            # Stop the execute model loop in parallel workers until there are\n            # more requests to process. This avoids waiting indefinitely in\n            # torch.distributed ops which may otherwise timeout, and unblocks\n            # the RPC thread in the workers so that they can process any other\n            # queued control plane messages, such as add/remove lora adapters.\n            logger.debug(\"Stopping remote worker execution loop.\")\n            self.model_executor.stop_remote_worker_execution_loop()\n\n        return ctx.request_outputs\n\n    def _has_remaining_steps(\n        self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]\n    ) -> bool:\n        if (not self.scheduler_config.is_multi_step\n                or not seq_group_metadata_list):\n            return False\n\n        # TODO(will) this is a sanity check for nowto make sure that all the\n        # seqs are on the same steps. Eventually we will want to do some sort of\n        # dynamic scheduling when doing multi-step decoding.\n        ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps\n        if any([\n                seq_group.state.remaining_steps != ref_remaining_steps\n                for seq_group in seq_group_metadata_list[1:]\n        ]):\n            raise AssertionError((\"All running sequence groups should \"\n                                  \"have the same remaining steps.\"))\n\n        return ref_remaining_steps > 0\n\n    def _cache_scheduler_outputs_for_multi_step(\n            self, virtual_engine: int,\n            seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n            scheduler_outputs: SchedulerOutputs,\n            allow_async_output_proc: bool) -> None:\n        co = self.cached_scheduler_outputs[virtual_engine]\n\n        co.seq_group_metadata_list = seq_group_metadata_list\n        co.scheduler_outputs = scheduler_outputs\n        co.allow_async_output_proc = allow_async_output_proc\n        co.last_output = None\n\n    def _update_cached_scheduler_output(\n            self, virtual_engine: int,\n            output: List[Optional[SamplerOutput]]) -> None:\n        if (self.parallel_config.pipeline_parallel_size > 1 and len(output) > 0\n                and output[0] is not None):\n            last_output = output[-1]\n            assert last_output is not None\n            assert last_output.sampled_token_ids_cpu is not None\n            assert last_output.sampled_token_ids is None\n            assert last_output.sampled_token_probs is None\n            self.cached_scheduler_outputs[\n                virtual_engine].last_output = last_output\n\n    def _get_last_sampled_token_ids(\n            self, virtual_engine: int) -> Optional[torch.Tensor]:\n        cached_last_output = self.cached_scheduler_outputs[\n            virtual_engine].last_output\n        if (self.scheduler_config.is_multi_step\n                and self.parallel_config.pipeline_parallel_size > 1\n                and cached_last_output is not None\n                and cached_last_output.sampled_token_ids_cpu is not None):\n            return cached_last_output.sampled_token_ids_cpu\n        return None\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} already exists.\")\n        self.stat_loggers[logger_name] = logger\n\n    def remove_logger(self, logger_name: str) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name not in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} does not exist.\")\n        del self.stat_loggers[logger_name]\n\n    def do_log_stats(self,\n                     scheduler_outputs: Optional[SchedulerOutputs] = None,\n                     model_output: Optional[List[SamplerOutput]] = None,\n                     finished_before: Optional[List[int]] = None,\n                     skip: Optional[List[int]] = None) -> None:\n        \"\"\"Forced log when no requests active.\"\"\"\n        if self.log_stats:\n            stats = self._get_stats(scheduler_outputs, model_output,\n                                    finished_before, skip)\n            for logger in self.stat_loggers.values():\n                logger.log(stats)\n\n    def _get_stats(self,\n                   scheduler_outputs: Optional[SchedulerOutputs],\n                   model_output: Optional[List[SamplerOutput]] = None,\n                   finished_before: Optional[List[int]] = None,\n                   skip: Optional[List[int]] = None) -> Stats:\n        \"\"\"Get Stats to be Logged to Prometheus.\n\n        Args:\n            scheduler_outputs: Optional, used to populate metrics related to\n                the scheduled batch,\n            model_output: Optional, used to emit speculative decoding metrics\n                which are created by the workers.\n            finished_before: Optional, indices of sequences that were finished\n                before. These sequences will be ignored.\n            skip: Optional, indices of sequences that were preempted. These\n                sequences will be ignored.\n        \"\"\"\n        now = time.time()\n\n        # System State\n        #   Scheduler State\n        num_running_sys = sum(\n            len(scheduler.running) for scheduler in self.scheduler)\n        num_swapped_sys = sum(\n            len(scheduler.swapped) for scheduler in self.scheduler)\n        num_waiting_sys = sum(\n            len(scheduler.waiting) for scheduler in self.scheduler)\n\n        # KV Cache Usage in %\n        num_total_gpu = self.cache_config.num_gpu_blocks\n        gpu_cache_usage_sys = 0.\n        if num_total_gpu is not None:\n            num_free_gpu = sum(\n                scheduler.block_manager.get_num_free_gpu_blocks()\n                for scheduler in self.scheduler)\n            gpu_cache_usage_sys = 1.0 - (num_free_gpu / num_total_gpu)\n\n        num_total_cpu = self.cache_config.num_cpu_blocks\n        cpu_cache_usage_sys = 0.\n        if num_total_cpu is not None and num_total_cpu > 0:\n            num_free_cpu = sum(\n                scheduler.block_manager.get_num_free_cpu_blocks()\n                for scheduler in self.scheduler)\n            cpu_cache_usage_sys = 1.0 - (num_free_cpu / num_total_cpu)\n\n        # Prefix Cache Hit Rate. Note that we always use\n        # the cache hit rate of the first virtual engine.\n        cpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.CPU)\n        gpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.GPU)\n\n        # Iteration stats\n        num_prompt_tokens_iter = 0\n        num_generation_tokens_iter = 0\n        time_to_first_tokens_iter: List[float] = []\n        time_per_output_tokens_iter: List[float] = []\n        num_preemption_iter = (0 if scheduler_outputs is None else\n                               scheduler_outputs.preempted)\n\n        # Request stats\n        #   Latency\n        time_e2e_requests: List[float] = []\n        #   Metadata\n        num_prompt_tokens_requests: List[int] = []\n        num_generation_tokens_requests: List[int] = []\n        n_requests: List[int] = []\n        finished_reason_requests: List[str] = []\n\n        # NOTE: This loop assumes prefill seq_groups are before\n        # decode seq_groups in scheduled_seq_groups.\n        if scheduler_outputs is not None:\n            # For async postprocessor, already finished sequences need to be\n            # not counted (to avoid double counting)\n            actual_num_batched_tokens = scheduler_outputs.num_batched_tokens  # type: ignore\n\n            num_generation_tokens_from_prefill_groups = 0.\n            # NOTE: if scheduler_outputs.num_prefill_groups > 0 and\n            # the len of scheduler_outputs.scheduled_seq_groups is !=\n            # scheduler_outputs.num_prefill_groups, this means that\n            # chunked prefills have been detected.\n\n            for idx, scheduled_seq_group in enumerate(\n                    scheduler_outputs.scheduled_seq_groups):\n                # Skip double logging when using async output proc\n                if finished_before and idx in finished_before:\n                    actual_num_batched_tokens -= 1\n                    continue\n\n                # Currently, skip == preempted sequences, so we need to skip\n                # their log stats\n                if skip and idx in skip:\n                    continue\n\n                group_was_prefill = idx < scheduler_outputs.num_prefill_groups\n                seq_group = scheduled_seq_group.seq_group\n\n                # NOTE: a seq_group that completed all of its prefill tokens\n                # in the last iteration will have seq_group.is_prefill() = False\n                # with group_was_prefill = True\n                if group_was_prefill:\n                    # Number of prompt tokens.\n                    num_prompt_tokens_iter += (\n                        scheduled_seq_group.token_chunk_size)\n\n                    # If the seq_group just finished the prefill state\n                    # get TTFT.\n                    if not seq_group.is_prefill():\n                        latency = seq_group.get_last_latency(now)\n                        time_to_first_tokens_iter.append(latency)\n\n                        # One generation token per finished prefill.\n                        num_generation_tokens_from_prefill_groups += (\n                            seq_group.num_seqs())\n                else:\n                    # TPOTs.\n                    latency = seq_group.get_last_latency(now)\n                    time_per_output_tokens_iter.append(latency)\n\n                # Because of chunked prefill, we can have a single sequence\n                # group that does multiple prompt_runs. To prevent logging\n                # the same metadata more than once per request, we standardize\n                # on logging request level information for finished requests,\n                # which can only happen once.\n                if seq_group.is_finished():\n                    # Latency timings\n                    time_e2e_requests.append(now -\n                                             seq_group.metrics.arrival_time)\n                    # Metadata\n                    num_prompt_tokens_requests.append(\n                        len(seq_group.prompt_token_ids))\n                    num_generation_tokens_requests.extend([\n                        seq.get_output_len()\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n                    if seq_group.sampling_params is not None:\n                        n_requests.append(seq_group.sampling_params.n)\n                    finished_reason_requests.extend([\n                        SequenceStatus.get_finished_reason(seq.status)\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n\n            # Number of generation tokens.\n            #   num_batched_tokens equals the number of prompt_tokens plus the\n            #   number of decode_tokens in a single iteration. So,\n            #   num_generation_tokens = num_batched_tokens - num_prompt_tokens\n            #   + num_generation_tokens_from_prefill_groups (since we generate\n            #   one token on prefills on iters where the prefill finishes).\n            num_generation_tokens_iter = (\n                actual_num_batched_tokens - num_prompt_tokens_iter +\n                num_generation_tokens_from_prefill_groups)\n\n        # Spec decode, if enabled, emits specialized metrics from the worker in\n        # sampler output.\n        if model_output and (model_output[0].spec_decode_worker_metrics\n                             is not None):\n            spec_decode_metrics = model_output[0].spec_decode_worker_metrics\n        else:\n            spec_decode_metrics = None\n\n        return Stats(\n            now=now,\n            # System stats\n            #   Scheduler State\n            num_running_sys=num_running_sys,\n            num_swapped_sys=num_swapped_sys,\n            num_waiting_sys=num_waiting_sys,\n            #   KV Cache Usage in %\n            gpu_cache_usage_sys=gpu_cache_usage_sys,\n            cpu_cache_usage_sys=cpu_cache_usage_sys,\n            #   Prefix Cache Hit Rate\n            cpu_prefix_cache_hit_rate=cpu_prefix_cache_hit_rate,\n            gpu_prefix_cache_hit_rate=gpu_prefix_cache_hit_rate,\n\n            # Iteration stats\n            num_prompt_tokens_iter=num_prompt_tokens_iter,\n            num_generation_tokens_iter=num_generation_tokens_iter,\n            time_to_first_tokens_iter=time_to_first_tokens_iter,\n            time_per_output_tokens_iter=time_per_output_tokens_iter,\n            spec_decode_metrics=spec_decode_metrics,\n            num_preemption_iter=num_preemption_iter,\n\n            # Request stats\n            #   Latency\n            time_e2e_requests=time_e2e_requests,\n            #   Metadata\n            num_prompt_tokens_requests=num_prompt_tokens_requests,\n            num_generation_tokens_requests=num_generation_tokens_requests,\n            n_requests=n_requests,\n            finished_reason_requests=finished_reason_requests,\n        )\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_executor.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_executor.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_executor.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.model_executor.pin_lora(lora_id)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        return self.model_executor.add_prompt_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_executor.remove_prompt_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> List[int]:\n        return self.model_executor.list_prompt_adapters()\n\n    def check_health(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n    def start_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes (MultiprocessingGPUExecutor)\n        if type(self.model_executor) == GPUExecutor:  # noqa: E721\n            self.model_executor.start_profile()\n        else:\n            self.model_executor._run_workers(\"start_profile\")\n\n    def stop_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes (MultiprocessingGPUExecutor)\n        if type(self.model_executor) == GPUExecutor:  # noqa: E721\n            self.model_executor.stop_profile()\n        else:\n            self.model_executor._run_workers(\"stop_profile\")\n\n    def is_tracing_enabled(self) -> bool:\n        return self.tracer is not None\n\n    def do_tracing(self, scheduler_outputs: SchedulerOutputs) -> None:\n        if self.tracer is None:\n            return\n\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            seq_group = scheduled_seq_group.seq_group\n            if seq_group.is_finished():\n                self.create_trace_span(seq_group)\n\n    def create_trace_span(self, seq_group: SequenceGroup) -> None:\n        if self.tracer is None or seq_group.sampling_params is None:\n            return\n        arrival_time_nano_seconds = int(seq_group.metrics.arrival_time * 1e9)\n\n        trace_context = extract_trace_context(seq_group.trace_headers)\n\n        with self.tracer.start_as_current_span(\n                \"llm_request\",\n                kind=SpanKind.SERVER,\n                context=trace_context,\n                start_time=arrival_time_nano_seconds) as seq_span:\n            metrics = seq_group.metrics\n            ttft = metrics.first_token_time - metrics.arrival_time\n            e2e_time = metrics.finished_time - metrics.arrival_time\n            # attribute names are based on\n            # https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md\n            seq_span.set_attribute(SpanAttributes.LLM_RESPONSE_MODEL,\n                                   self.model_config.model)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_ID,\n                                   seq_group.request_id)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TEMPERATURE,\n                                   seq_group.sampling_params.temperature)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TOP_P,\n                                   seq_group.sampling_params.top_p)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_MAX_TOKENS,\n                                   seq_group.sampling_params.max_tokens)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_N,\n                                   seq_group.sampling_params.n)\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_NUM_SEQUENCES,\n                                   seq_group.num_seqs())\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_PROMPT_TOKENS,\n                                   len(seq_group.prompt_token_ids))\n            seq_span.set_attribute(\n                SpanAttributes.LLM_USAGE_COMPLETION_TOKENS,\n                sum([\n                    seq.get_output_len()\n                    for seq in seq_group.get_finished_seqs()\n                ]))\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_TIME_IN_QUEUE,\n                                   metrics.time_in_queue)\n            seq_span.set_attribute(\n                SpanAttributes.LLM_LATENCY_TIME_TO_FIRST_TOKEN, ttft)\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_E2E, e2e_time)\n            if metrics.scheduler_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_SCHEDULER,\n                    metrics.scheduler_time)\n            if metrics.model_forward_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_FORWARD,\n                    metrics.model_forward_time / 1000.0)\n            if metrics.model_execute_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_EXECUTE,\n                    metrics.model_execute_time)\n\n    def is_encoder_decoder_model(self):\n        return self.input_preprocessor.is_encoder_decoder_model()\n\n    def is_embedding_model(self):\n        return self.model_config.is_embedding_model\n\n    def _validate_model_inputs(self, inputs: Union[DecoderOnlyInputs,\n                                                   EncoderDecoderInputs]):\n        if self.model_config.is_multimodal_model:\n            # For encoder-decoder multimodal models, the max_prompt_len\n            # restricts the decoder prompt length\n            prompt_ids = inputs.get(\"prompt_token_ids\")\n        elif self.is_encoder_decoder_model():\n            prompt_ids = inputs.get(\"encoder_prompt_token_ids\")\n        else:\n            prompt_ids = inputs.get(\"prompt_token_ids\")\n\n        if prompt_ids is None or len(prompt_ids) == 0:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        if self.model_config.is_multimodal_model:\n            max_prompt_len = self.model_config.max_model_len\n\n            if len(prompt_ids) > max_prompt_len:\n                raise ValueError(\n                    f\"The prompt (total length {len(prompt_ids)}) is too long \"\n                    f\"to fit into the model (context length {max_prompt_len}). \"\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n\n    def _build_logits_processors(\n            self, sampling_params: SamplingParams,\n            lora_request: Optional[LoRARequest]) -> SamplingParams:\n        \"\"\"Constructs logits processors based on the guided_decoding,\n        logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n        those fields and adds the constructed logits processors to the\n        logits_processors field. Returns the modified sampling params.\"\"\"\n\n        logits_processors = []\n        if (guided_decoding := sampling_params.guided_decoding) is not None:\n\n            logger.debug(\n                \"Building guided decoding logits processor in \"\n                \"LLMEngine. Params: %s\", guided_decoding)\n\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n            guided_decoding.backend = guided_decoding.backend or \\\n                self.decoding_config.guided_decoding_backend\n\n            processor = get_local_guided_decoding_logits_processor(\n                guided_params=guided_decoding, tokenizer=tokenizer)\n            if processor:\n                logits_processors.append(processor)\n\n            # Unset so this doesn't get passed down to the model\n            sampling_params.guided_decoding = None\n\n        if (sampling_params.logit_bias or sampling_params.allowed_token_ids):\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n\n            processors = get_logits_processors(\n                logit_bias=sampling_params.logit_bias,\n                allowed_token_ids=sampling_params.allowed_token_ids,\n                tokenizer=tokenizer)\n            logits_processors.extend(processors)\n\n            # Unset so these don't get passed down to the model\n            sampling_params.logit_bias = None\n            sampling_params.allowed_token_ids = None\n\n        if logits_processors:\n            if sampling_params.logits_processors is None:\n                sampling_params.logits_processors = logits_processors\n            else:\n                sampling_params.logits_processors.extend(logits_processors)\n\n        return sampling_params\n",
      "diff": "diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex a570d096d..61c21887e 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -247,7 +247,7 @@ class LLMEngine:\n             \"enforce_eager=%s, kv_cache_dtype=%s, \"\n             \"quantization_param_path=%s, device_config=%s, \"\n             \"decoding_config=%r, observability_config=%r, \"\n-            \"seed=%d, served_model_name=%s, use_v2_block_manager=%s, \"\n+            \"seed=%d, served_model_name=%s, \"\n             \"num_scheduler_steps=%d, chunked_prefill_enabled=%s \"\n             \"multi_step_stream_outputs=%s, enable_prefix_caching=%s, \"\n             \"use_async_output_proc=%s, use_cached_outputs=%s, \"\n@@ -280,7 +280,6 @@ class LLMEngine:\n             observability_config,\n             model_config.seed,\n             model_config.served_model_name,\n-            scheduler_config.use_v2_block_manager,\n             scheduler_config.num_scheduler_steps,\n             scheduler_config.chunked_prefill_enabled,\n             scheduler_config.multi_step_stream_outputs,",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/envs.py",
      "old_content": "import os\nimport tempfile\nfrom typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional\n\nif TYPE_CHECKING:\n    VLLM_HOST_IP: str = \"\"\n    VLLM_PORT: Optional[int] = None\n    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()\n    VLLM_USE_MODELSCOPE: bool = False\n    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60\n    VLLM_INSTANCE_ID: Optional[str] = None\n    VLLM_NCCL_SO_PATH: Optional[str] = None\n    LD_LIBRARY_PATH: Optional[str] = None\n    VLLM_USE_TRITON_FLASH_ATTN: bool = False\n    LOCAL_RANK: int = 0\n    CUDA_VISIBLE_DEVICES: Optional[str] = None\n    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60\n    VLLM_API_KEY: Optional[str] = None\n    S3_ACCESS_KEY_ID: Optional[str] = None\n    S3_SECRET_ACCESS_KEY: Optional[str] = None\n    S3_ENDPOINT_URL: Optional[str] = None\n    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")\n    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")\n    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"\n    VLLM_NO_USAGE_STATS: bool = False\n    VLLM_DO_NOT_TRACK: bool = False\n    VLLM_USAGE_SOURCE: str = \"\"\n    VLLM_CONFIGURE_LOGGING: int = 1\n    VLLM_LOGGING_LEVEL: str = \"INFO\"\n    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None\n    VLLM_TRACE_FUNCTION: int = 0\n    VLLM_ATTENTION_BACKEND: Optional[str] = None\n    VLLM_USE_FLASHINFER_SAMPLER: bool = False\n    VLLM_USE_FLASHINFER_REJECTION_SAMPLER: bool = False\n    VLLM_PP_LAYER_PARTITION: Optional[str] = None\n    VLLM_CPU_KVCACHE_SPACE: int = 0\n    VLLM_CPU_OMP_THREADS_BIND: str = \"\"\n    VLLM_OPENVINO_DEVICE: str = \"CPU\"\n    VLLM_OPENVINO_KVCACHE_SPACE: int = 0\n    VLLM_OPENVINO_CPU_KV_CACHE_PRECISION: Optional[str] = None\n    VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS: bool = False\n    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")\n    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024\n    VLLM_USE_RAY_SPMD_WORKER: bool = False\n    VLLM_USE_RAY_COMPILED_DAG: bool = False\n    VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL: bool = True\n    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"\n    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")\n    VLLM_IMAGE_FETCH_TIMEOUT: int = 5\n    VLLM_AUDIO_FETCH_TIMEOUT: int = 5\n    VLLM_TARGET_DEVICE: str = \"cuda\"\n    MAX_JOBS: Optional[str] = None\n    NVCC_THREADS: Optional[str] = None\n    VLLM_USE_PRECOMPILED: bool = False\n    VLLM_NO_DEPRECATION_WARNING: bool = False\n    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False\n    CMAKE_BUILD_TYPE: Optional[str] = None\n    VERBOSE: bool = False\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False\n    VLLM_TEST_FORCE_FP8_MARLIN: bool = False\n    VLLM_RPC_TIMEOUT: int = 10000  # ms\n    VLLM_PLUGINS: Optional[List[str]] = None\n    VLLM_TORCH_PROFILER_DIR: Optional[str] = None\n    VLLM_USE_TRITON_AWQ: bool = False\n    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n    VLLM_SKIP_P2P_CHECK: bool = False\n    VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1: bool = False\n    VLLM_TORCH_COMPILE_LEVEL: int = 0\n    VLLM_DISABLED_KERNELS: List[str] = []\n\n\ndef get_default_cache_root():\n    return os.getenv(\n        \"XDG_CACHE_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n    )\n\n\ndef get_default_config_root():\n    return os.getenv(\n        \"XDG_CONFIG_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n    )\n\n\n# The begin-* and end* here are used by the documentation generator\n# to extract the used env vars.\n\n# begin-env-vars-definition\n\nenvironment_variables: Dict[str, Callable[[], Any]] = {\n\n    # ================== Installation Time Env Vars ==================\n\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, neuron, cpu, openvino]\n    \"VLLM_TARGET_DEVICE\":\n    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),\n\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\":\n    lambda: os.getenv(\"MAX_JOBS\", None),\n\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\":\n    lambda: os.getenv(\"NVCC_THREADS\", None),\n\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\":\n    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")),\n\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\":\n    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),\n\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\":\n    lambda: bool(int(os.getenv('VERBOSE', '0'))),\n\n    # Root directory for VLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )),\n\n    # ================== Runtime Env Vars ==================\n\n    # Root directory for VLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )),\n\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    'VLLM_HOST_IP':\n    lambda: os.getenv('VLLM_HOST_IP', \"\") or os.getenv(\"HOST_IP\", \"\"),\n\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    # '0' is used to make mypy happy\n    'VLLM_PORT':\n    lambda: int(os.getenv('VLLM_PORT', '0'))\n    if 'VLLM_PORT' in os.environ else None,\n\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    'VLLM_RPC_BASE_PATH':\n    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),\n\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\":\n    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n\n    # Instance id represents an instance of the VLLM. All processes in the same\n    # instance should have the same instance id.\n    \"VLLM_INSTANCE_ID\":\n    lambda: os.environ.get(\"VLLM_INSTANCE_ID\", None),\n\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),\n\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\":\n    lambda: os.environ.get(\"CUDA_HOME\", None),\n\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\":\n    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\":\n    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n\n    # flag to control if vllm should use triton flash attention\n    \"VLLM_USE_TRITON_FLASH_ATTN\":\n    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Internal flag to enable Dynamo fullgraph capture\n    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":\n    lambda: bool(\n        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),\n    \"VLLM_TORCH_COMPILE_LEVEL\":\n    lambda: int(os.environ.get(\"VLLM_TORCH_COMPILE_LEVEL\", \"0\")),\n\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\":\n    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\":\n    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":\n    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),\n\n    # API key for VLLM API server\n    \"VLLM_API_KEY\":\n    lambda: os.environ.get(\"VLLM_API_KEY\", None),\n\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\":\n    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\":\n    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\":\n    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\":\n    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),\n    \"VLLM_NO_USAGE_STATS\":\n    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DO_NOT_TRACK\":\n    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(\n        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",\n    \"VLLM_USAGE_SOURCE\":\n    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\":\n    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),\n    \"VLLM_LOGGING_CONFIG_PATH\":\n    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\":\n    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\"),\n\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\":\n    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n\n    # Backend for attention computation\n    # Available options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"XFORMERS\": use XFormers\n    # - \"ROCM_FLASH\": use ROCmFlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    \"VLLM_ATTENTION_BACKEND\":\n    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),\n\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_FLASHINFER_SAMPLER\", \"0\"))),\n\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\":\n    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n\n    # (CPU backend only) CPU key-value cache space.\n    # default is 4GB\n    \"VLLM_CPU_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),\n\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\":\n    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),\n\n    # OpenVINO device selection\n    # default is CPU\n    \"VLLM_OPENVINO_DEVICE\":\n    lambda: os.getenv(\"VLLM_OPENVINO_DEVICE\", \"CPU\").upper(),\n\n    # OpenVINO key-value cache space\n    # default is 4GB\n    \"VLLM_OPENVINO_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_OPENVINO_KVCACHE_SPACE\", \"0\")),\n\n    # OpenVINO KV cache precision\n    # default is bf16 if natively supported by platform, otherwise f16\n    # To enable KV cache compression, please, explicitly specify u8\n    \"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\":\n    lambda: os.getenv(\"VLLM_OPENVINO_CPU_KV_CACHE_PRECISION\", None),\n\n    # Enables weights compression during model export via HF Optimum\n    # default is False\n    \"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\":\n    lambda: bool(os.getenv(\"VLLM_OPENVINO_ENABLE_QUANTIZED_WEIGHTS\", False)),\n\n    # If the env var is set, then all workers will execute as separate\n    # processes from the engine, and we use the same mechanism to trigger\n    # execution on all workers.\n    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.\n    \"VLLM_USE_RAY_SPMD_WORKER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),\n\n    # If the env var is set, it uses the Ray's compiled DAG API\n    # which optimizes the control plane overhead.\n    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n    \"VLLM_USE_RAY_COMPILED_DAG\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),\n\n    # If the env var is set, it uses NCCL for communication in\n    # Ray's compiled DAG. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_NCCL_CHANNEL\", \"1\"))\n                 ),\n\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\":\n    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),\n\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )),\n\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"5\")),\n\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),\n\n    # If set, vllm will skip the deprecation warnings.\n    \"VLLM_NO_DEPRECATION_WARNING\":\n    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),\n\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":\n    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),\n\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\":\n    lambda:\n    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\":\n    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),\n\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\":\n    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[\n        \"VLLM_PLUGINS\"].split(\",\"),\n\n    # Enables torch profiler if set. Path to the directory where torch profiler\n    # traces are saved. Note that it must be an absolute path.\n    \"VLLM_TORCH_PROFILER_DIR\":\n    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os\n             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),\n\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # By default, vLLM will check the peer-to-peer capability itself,\n    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    # If this env var is set to 1, vLLM will skip the peer-to-peer check,\n    # and trust the driver's peer-to-peer capability report.\n    \"VLLM_SKIP_P2P_CHECK\":\n    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n\n    # If set, allowing the use of deprecated block manager V1\n    \"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\":\n    lambda: os.environ.get(\"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\", \"0\"\n                           ) == \"1\",\n\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\":\n    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[\n        \"VLLM_DISABLED_KERNELS\"].split(\",\"),\n}\n\n# end-env-vars-definition\n\n\ndef __getattr__(name: str):\n    # lazy evaluation of environment variables\n    if name in environment_variables:\n        return environment_variables[name]()\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef __dir__():\n    return list(environment_variables.keys())\n",
      "diff": "diff --git a/vllm/envs.py b/vllm/envs.py\nindex 45a999961..2d283fae2 100644\n--- a/vllm/envs.py\n+++ b/vllm/envs.py\n@@ -64,7 +64,6 @@ if TYPE_CHECKING:\n     VLLM_USE_TRITON_AWQ: bool = False\n     VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n     VLLM_SKIP_P2P_CHECK: bool = False\n-    VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1: bool = False\n     VLLM_TORCH_COMPILE_LEVEL: int = 0\n     VLLM_DISABLED_KERNELS: List[str] = []\n \n@@ -427,11 +426,6 @@ environment_variables: Dict[str, Callable[[], Any]] = {\n     \"VLLM_SKIP_P2P_CHECK\":\n     lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n \n-    # If set, allowing the use of deprecated block manager V1\n-    \"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\":\n-    lambda: os.environ.get(\"VLLM_ALLOW_DEPRECATED_BLOCK_MANAGER_V1\", \"0\"\n-                           ) == \"1\",\n-\n     # List of quantization kernels that should be disabled, used for testing\n     # and performance comparisons. Currently only affects MPLinearKernel\n     # selection",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 7
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import dataclasses\nimport gc\nimport inspect\nimport itertools\nimport time\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set,\n                    Tuple, Type, TypeVar, Union)\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.attention.backends.abstract import AttentionState\nfrom vllm.attention.backends.utils import CommonAttentionState\nfrom vllm.compilation.compile_context import set_compile_context\nfrom vllm.compilation.levels import CompilationLevel\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.distributed import get_pp_group\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.forward_context import set_forward_context\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata, SamplingMetadataCache\nfrom vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.model_executor.models import supports_lora, supports_multimodal\nfrom vllm.model_executor.models.utils import set_cpu_offload_max_bytes\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,\n                             MultiModalInputs, MultiModalRegistry)\nfrom vllm.prompt_adapter.layers import PromptAdapterMapping\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.prompt_adapter.worker_manager import (\n    LRUCacheWorkerPromptAdapterManager)\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import IntermediateTensors, SequenceGroupMetadata\nfrom vllm.transformers_utils.config import uses_mrope\nfrom vllm.utils import (DeviceMemoryProfiler, PyObjectCache, async_tensor_h2d,\n                        flatten_2d_lists, is_hip, is_pin_memory_available,\n                        supports_dynamo)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict, dump_input_when_exception)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# all the token sizes that **can** be captured by cudagraph.\n# they can be arbitrarily large.\n# currently it includes: 1, 2, 4, 8, 16, 24, 32, 40, ..., 8192.\n# the actual sizes to capture will be determined by the model,\n# depending on the model's max_num_seqs.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 1025)\n]\n_NUM_WARMUP_ITERS = 2\n\nTModelInputForGPU = TypeVar('TModelInputForGPU', bound=\"ModelInputForGPU\")\n\n# For now, bump up cache limits for recompilations during CUDA graph warmups.\ntorch._dynamo.config.cache_size_limit = 128\ntorch._dynamo.config.accumulated_cache_size_limit = 128\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPU(ModelRunnerInputBase):\n    \"\"\"\n    This base class contains metadata needed for the base model forward pass\n    but not metadata for possible additional steps, e.g., sampling. Model\n    runners that run additional steps should subclass this method to add\n    additional fields.\n    \"\"\"\n    input_tokens: Optional[torch.Tensor] = None\n    input_positions: Optional[torch.Tensor] = None\n    seq_lens: Optional[List[int]] = None\n    query_lens: Optional[List[int]] = None\n    lora_mapping: Optional[\"LoRAMapping\"] = None\n    lora_requests: Optional[Set[LoRARequest]] = None\n    attn_metadata: Optional[\"AttentionMetadata\"] = None\n    prompt_adapter_mapping: Optional[PromptAdapterMapping] = None\n    prompt_adapter_requests: Optional[Set[PromptAdapterRequest]] = None\n    multi_modal_kwargs: Optional[BatchedTensorInputs] = None\n    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None\n    finished_requests_ids: Optional[List[str]] = None\n    virtual_engine: int = 0\n    async_callback: Optional[Callable] = None\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    scheduler_outputs: Optional[SchedulerOutputs] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[TModelInputForGPU],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> TModelInputForGPU:\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):\n    \"\"\"\n    Used by the ModelRunner.\n    \"\"\"\n    sampling_metadata: Optional[\"SamplingMetadata\"] = None\n    # Used for speculative decoding. We do not broadcast it because it is only\n    # used by the driver worker.\n    is_prompt: Optional[bool] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForGPUWithSamplingMetadata\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n    \"\"\"Build ModelInputForGPU from SequenceGroupMetadata.\"\"\"\n\n    # Note: ideally we would be using a dataclass(kw_only=True)\n    # here, so that this can be subclassed easily,\n    # but kw_only is not supported in python<3.10.\n    class InterDataForSeqGroup:\n        \"\"\"Intermediate data for the current sequence group.\"\"\"\n\n        def simple_reinit(self):\n            self.input_tokens[0].clear()  # type: ignore\n            self.input_positions[0].clear()  # type: ignore\n            self.mrope_input_positions = None  # type: ignore\n            self.seq_lens[0] = 0  # type: ignore\n            self.orig_seq_lens[0] = 0  # type: ignore\n            self.query_lens[0] = 0  # type: ignore\n            self.context_lens[0] = 0  # type: ignore\n            self.curr_sliding_window_blocks[0] = 0  # type: ignore\n            self.lora_index_mapping.clear()  # type: ignore\n            self.lora_prompt_mapping.clear()  # type: ignore\n            self.lora_requests.clear()  # type: ignore\n            self.prompt_adapter_index_mapping.clear()  # type: ignore\n            self.prompt_adapter_prompt_mapping.clear()  # type: ignore\n\n        def __init__(\n            self,\n            *,\n            # From sequence group metadata.\n            request_id: str,\n            seq_ids: List[int],\n            is_prompt: bool,\n            block_tables: Optional[Dict[int, List[int]]],\n            computed_block_nums: List[int],\n            n_seqs: int = 0,\n\n            # Input tokens and positions.\n            input_tokens: Optional[List[List[int]]] = None,\n            input_positions: Optional[List[List[int]]] = None,\n            mrope_input_positions: Optional[List[List[List[int]]]] = None,\n\n            # The sequence length (may be capped to the sliding window).\n            seq_lens: Optional[List[int]] = None,\n            # The original sequence length (before applying sliding window).\n            # This is used to compute slot mapping.\n            orig_seq_lens: Optional[List[int]] = None,\n            # The query length.\n            query_lens: Optional[List[int]] = None,\n            # The number of tokens that are already computed.\n            context_lens: Optional[List[int]] = None,\n            # The current sliding window block.\n            curr_sliding_window_blocks: Optional[List[int]] = None,\n\n            # LoRA inputs.\n            lora_index_mapping: Optional[List[List[int]]] = None,\n            lora_prompt_mapping: Optional[List[List[int]]] = None,\n            lora_requests: Optional[Set[LoRARequest]] = None,\n\n            # Prompt adapter inputs.\n            prompt_adapter_index_mapping: Optional[List[int]] = None,\n            prompt_adapter_prompt_mapping: Optional[List[int]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n\n            # Multi-modal inputs.\n            multi_modal_inputs: Optional[MultiModalInputs] = None,\n\n            # Whether the prefix cache is hit (prefill only).\n            prefix_cache_hit: bool = False,\n            reinit: bool = False,\n            reinit_use_defaults: bool = False,\n            encoder_seq_len: int = 0,\n        ):\n            if reinit:\n                assert len(self.seq_ids) == len(seq_ids)  # type: ignore\n                for i, seq_id in enumerate(seq_ids):\n                    self.seq_ids[i] = seq_id  # type: ignore\n            else:\n                self.seq_ids = seq_ids\n\n            self.request_id = request_id\n            self.is_prompt = is_prompt\n            self.block_tables = block_tables\n            self.computed_block_nums = computed_block_nums\n            self.n_seqs = n_seqs\n            self.encoder_seq_len = encoder_seq_len\n\n            if reinit:\n                if len(self.seq_ids) == 1 and reinit_use_defaults:\n                    self.simple_reinit()\n                else:\n                    if input_tokens:\n                        self.input_tokens = input_tokens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_tokens[seq_id].clear()\n\n                    if input_positions:\n                        self.input_positions = input_positions\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_positions[seq_id].clear()\n\n                    self.mrope_input_positions = None\n\n                    if seq_lens:\n                        self.seq_lens = seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.seq_lens[seq_id] = 0\n\n                    if orig_seq_lens:\n                        self.orig_seq_lens = orig_seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.orig_seq_lens[seq_id] = 0\n\n                    if query_lens:\n                        self.query_lens = query_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.query_lens[seq_id] = 0\n\n                    if context_lens:\n                        self.context_lens = context_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.context_lens[seq_id] = 0\n\n                    if curr_sliding_window_blocks:\n                        self.curr_sliding_window_blocks = \\\n                            curr_sliding_window_blocks\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.curr_sliding_window_blocks[seq_id] = 0\n\n                    if lora_index_mapping:\n                        self.lora_index_mapping = lora_index_mapping\n                    else:\n                        self.lora_index_mapping.clear()\n\n                    if lora_prompt_mapping:\n                        self.lora_prompt_mapping = lora_prompt_mapping\n                    else:\n                        self.lora_prompt_mapping.clear()\n\n                    if lora_requests:\n                        self.lora_requests = lora_requests\n                    else:\n                        self.lora_requests.clear()\n\n                    if prompt_adapter_index_mapping:\n                        self.prompt_adapter_index_mapping = \\\n                            prompt_adapter_index_mapping\n                    else:\n                        self.prompt_adapter_index_mapping.clear()\n\n                    if prompt_adapter_prompt_mapping:\n                        self.prompt_adapter_prompt_mapping = \\\n                            prompt_adapter_prompt_mapping\n                    else:\n                        self.prompt_adapter_prompt_mapping.clear()\n\n            else:\n                self.input_tokens = input_tokens or []\n                self.input_positions = input_positions or []\n                self.mrope_input_positions = mrope_input_positions or None\n                self.seq_lens = seq_lens or []\n                self.orig_seq_lens = orig_seq_lens or []\n                self.query_lens = query_lens or []\n                self.context_lens = context_lens or []\n                self.curr_sliding_window_blocks = \\\n                    curr_sliding_window_blocks or []\n\n                self.lora_index_mapping = lora_index_mapping or []\n                self.lora_prompt_mapping = lora_prompt_mapping or []\n                self.lora_requests = lora_requests or set()\n\n                self.prompt_adapter_index_mapping = (\n                    prompt_adapter_index_mapping or [])\n                self.prompt_adapter_prompt_mapping = (\n                    prompt_adapter_prompt_mapping or [])\n\n            self.prompt_adapter_request = prompt_adapter_request\n            self.multi_modal_inputs = multi_modal_inputs\n            self.prefix_cache_hit = prefix_cache_hit\n\n            self.n_seqs = len(self.seq_ids)\n\n            if not reinit:\n                self.__post_init__()\n\n        def __post_init__(self):\n            self.n_seqs = len(self.seq_ids)\n\n            self.input_tokens = [[] for _ in range(self.n_seqs)]\n            self.input_positions = [[] for _ in range(self.n_seqs)]\n            self.mrope_input_positions = None\n            self.seq_lens = [0] * self.n_seqs\n            self.orig_seq_lens = [0] * self.n_seqs\n            self.query_lens = [0] * self.n_seqs\n            self.context_lens = [0] * self.n_seqs\n            self.curr_sliding_window_blocks = [0] * self.n_seqs\n\n            self.lora_index_mapping = []\n            self.lora_prompt_mapping = []\n\n    def gen_inter_data_builder(self, num_seqs: int):\n        return lambda: ModelInputForGPUBuilder.InterDataForSeqGroup(\n            request_id=\"\",\n            seq_ids=[0] * num_seqs,\n            is_prompt=True,\n            block_tables=None,\n            computed_block_nums=[])\n\n    def init_cached_inter_data(self, *args, **kwargs):\n        assert len(args) == 0\n        assert \"seq_ids\" in kwargs\n        seq_ids = kwargs[\"seq_ids\"]\n        num_seqs = len(seq_ids)\n\n        # The inter-data cache is per model_runner\n        inter_data_cache = self.runner.inter_data_cache\n        if num_seqs not in inter_data_cache:\n            inter_data_cache[num_seqs] = PyObjectCache(\n                self.gen_inter_data_builder(num_seqs))\n\n        obj = inter_data_cache[num_seqs].get_object()\n        obj.__init__(*args, **kwargs)\n        return obj\n\n    def reset_cached_inter_data(self):\n        for cache in self.runner.inter_data_cache.values():\n            cache.reset()\n\n    def __init__(self,\n                 runner: \"GPUModelRunnerBase\",\n                 finished_requests_ids: Optional[List[str]] = None):\n        super().__init__()\n        # Compute functions for each sequence in a sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_compute_fns = [\n            self._compute_lens,\n            self._compute_for_prefix_cache_hit,\n            self._compute_for_sliding_window,\n            self._compute_lora_input,\n        ]\n        # Compute functions for each sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_group_compute_fns = [\n            self._compute_prompt_adapter_input,\n            self._compute_multi_modal_input,\n        ]\n\n        self.runner = runner\n        self.model_input_cls = self.runner._model_input_cls\n        self.attn_backend = self.runner.attn_backend\n        self.scheduler_config = self.runner.scheduler_config\n        self.sliding_window = self.runner.sliding_window\n        self.block_size = self.runner.block_size\n        self.enable_lora = self.runner.lora_config is not None\n        self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                      is not None)\n        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n        self.finished_requests_ids = finished_requests_ids\n        self.decode_only = True\n\n        # Intermediate data (data in CPU before going to GPU) for\n        # the current sequence group.\n        self.inter_data_list: List[\n            ModelInputForGPUBuilder.InterDataForSeqGroup] = []\n\n        # Attention metadata inputs.\n        self.attn_metadata_builder = self.attn_backend.make_metadata_builder(\n            weakref.proxy(self))\n\n        # Engine/Model configurations.\n        self.chunked_prefill_enabled = (\n            self.scheduler_config is not None\n            and self.scheduler_config.chunked_prefill_enabled)\n        if self.sliding_window is not None:\n            self.sliding_window_blocks = (\n                self.sliding_window + self.block_size - 1) // self.block_size\n            self.block_aligned_sliding_window = \\\n                self.sliding_window_blocks * self.block_size\n\n    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,\n                      seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Compute context length, sequence length and tokens\n        for the given sequence data.\n        \"\"\"\n        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]\n        token_chunk_size = seq_group_metadata.token_chunk_size\n\n        # Compute context length (the number of tokens that are\n        # already computed) and sequence length (total number of tokens).\n\n        seq_len = seq_data.get_len()\n        if inter_data.is_prompt:\n            context_len = seq_data.get_num_computed_tokens()\n            seq_len = min(seq_len, context_len + token_chunk_size)\n        elif self.runner.scheduler_config.is_multi_step or \\\n            self.runner.model_config.is_encoder_decoder_model:\n            context_len = seq_len - 1\n        else:\n            context_len = seq_data.get_num_computed_tokens()\n\n        # Compute tokens.\n        tokens = seq_data.get_token_ids()[context_len:seq_len]\n\n        inter_data.seq_lens[seq_idx] = seq_len\n        inter_data.orig_seq_lens[seq_idx] = seq_len\n        inter_data.context_lens[seq_idx] = context_len\n        inter_data.input_tokens[seq_idx].extend(tokens)\n        inter_data.input_positions[seq_idx].extend(range(context_len, seq_len))\n        inter_data.query_lens[seq_idx] = seq_len - context_len\n\n        if seq_data.mrope_position_delta is not None:\n            if inter_data.mrope_input_positions is None:\n                inter_data.mrope_input_positions = [None] * inter_data.n_seqs\n\n            inter_data.mrope_input_positions[\n                seq_idx] = MRotaryEmbedding.get_next_input_positions(\n                    seq_data.mrope_position_delta,\n                    context_len,\n                    seq_len,\n                )\n\n    def _compute_for_prefix_cache_hit(\n            self, inter_data: InterDataForSeqGroup, seq_idx: int,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Check if hit prefix cache (i.e., some blocks are already computed).\n        If hit, update input tokens and positions to only compute the\n        remaining blocks.\n        \"\"\"\n        computed_block_nums = inter_data.computed_block_nums\n\n        # Note that prefix caching does not support sliding window.\n        prefix_cache_hit = (computed_block_nums is not None\n                            and len(computed_block_nums) > 0\n                            and self.sliding_window is None\n                            and inter_data.is_prompt)\n        inter_data.prefix_cache_hit = prefix_cache_hit\n\n        if not prefix_cache_hit:\n            return\n\n        assert computed_block_nums is not None\n        # The cache hit prompt tokens in this sequence. Note that\n        # this may be larger than the sequence length if chunked\n        # prefill is enabled.\n        prefix_cache_len = len(computed_block_nums) * self.block_size\n        # The number of so far computed prompt tokens in this sequence.\n        context_len = inter_data.context_lens[seq_idx]\n        # The total number of prompt tokens in this sequence.\n        # When chunked prefill is enabled, this is the token number of\n        # computed chunks + current chunk.\n        seq_len = inter_data.seq_lens[seq_idx]\n        if prefix_cache_len <= context_len:\n            # We already passed the cache hit region,\n            # so do normal computation.\n            pass\n        elif context_len < prefix_cache_len < seq_len:\n            # Partial hit. Compute the missing part.\n            uncomputed_start = prefix_cache_len - context_len\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][uncomputed_start:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][uncomputed_start:]\n            context_len = prefix_cache_len\n\n            inter_data.context_lens[seq_idx] = context_len\n            inter_data.query_lens[\n                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n        elif seq_len <= prefix_cache_len:\n            # Full hit. Only compute the last token to avoid\n            # erroneous behavior. FIXME: Ideally we should directly\n            # mark all tokens as computed in the scheduler and do not\n            # schedule this sequence, so this case should not happen.\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][-1:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][-1:]\n            inter_data.query_lens[seq_idx] = 1\n            inter_data.context_lens[seq_idx] = inter_data.seq_lens[seq_idx] - 1\n\n    def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                    seq_idx: int,\n                                    seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Update seq_len and curr_sliding_window_block for the given\n        sequence data (only required by decoding) if sliding window is enabled.\n        \"\"\"\n        curr_sliding_window_block = 0\n        sliding_seq_len = inter_data.seq_lens[seq_idx]\n        if not inter_data.is_prompt and self.sliding_window is not None:\n            # TODO(sang): This is a hack to make sliding window work with\n            # paged attn. We can remove it if we make paged attn kernel\n            # to properly handle slinding window attn.\n            curr_sliding_window_block = self.sliding_window_blocks\n            if self.scheduler_config.use_v2_block_manager:\n                # number of elements in last block\n                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n                sliding_seq_len = min(\n                    inter_data.seq_lens[seq_idx],\n                    self.block_aligned_sliding_window + suff_len)\n                if suff_len > 0:\n                    curr_sliding_window_block += 1\n            else:\n                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n                                      self.sliding_window)\n\n        inter_data.curr_sliding_window_blocks[\n            seq_idx] = curr_sliding_window_block\n        inter_data.seq_lens[seq_idx] = sliding_seq_len\n\n    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,\n                            seq_idx: int,\n                            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If LoRA is enabled, compute LoRA index and prompt mapping.\"\"\"\n        if not self.enable_lora:\n            return\n\n        lora_id = seq_group_metadata.lora_int_id\n        if lora_id > 0:\n            inter_data.lora_requests.add(seq_group_metadata.lora_request)\n        query_len = inter_data.query_lens[seq_idx]\n        inter_data.lora_index_mapping.append([lora_id] * query_len)\n        inter_data.lora_prompt_mapping.append(\n            [lora_id] *\n            (query_len if seq_group_metadata.sampling_params\n             and seq_group_metadata.sampling_params.prompt_logprobs is not None\n             else 1))\n\n    def _compute_prompt_adapter_input(\n            self, inter_data: InterDataForSeqGroup,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If prompt adapter is enabled, compute index and prompt mapping.\n        \"\"\"\n        # Note that when is_prompt=True, we expect only one sequence\n        # in the group.\n        if not self.enable_prompt_adapter:\n            return\n\n        prompt_adapter_id = seq_group_metadata.prompt_adapter_id\n        if prompt_adapter_id <= 0 or not inter_data.is_prompt:\n            return\n\n        # We expect only one sequence in the group when is_prompt=True.\n        assert inter_data.n_seqs == 1\n        query_len = inter_data.query_lens[0]\n        inter_data.prompt_adapter_request = (\n            seq_group_metadata.prompt_adapter_request)\n\n        num_tokens = seq_group_metadata.prompt_adapter_num_virtual_tokens\n        inter_data.prompt_adapter_index_mapping = [\n            prompt_adapter_id\n        ] * num_tokens + [0] * (query_len - num_tokens)\n        inter_data.prompt_adapter_prompt_mapping = [prompt_adapter_id] * (\n            query_len if seq_group_metadata.sampling_params\n            and seq_group_metadata.sampling_params.prompt_logprobs else 1)\n\n    def _compute_multi_modal_input(self, inter_data: InterDataForSeqGroup,\n                                   seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If multi-modal data is given, add it to the input.\"\"\"\n        mm_data = seq_group_metadata.multi_modal_data\n        if not mm_data:\n            return\n\n        mm_kwargs = self.multi_modal_input_mapper(\n            mm_data,\n            mm_processor_kwargs=seq_group_metadata.mm_processor_kwargs)\n        inter_data.multi_modal_inputs = mm_kwargs\n\n        # special processing for mrope position deltas.\n        if self.runner.model_is_mrope:\n            image_grid_thw = mm_kwargs.get(\"image_grid_thw\", None)\n            video_grid_thw = mm_kwargs.get(\"video_grid_thw\", None)\n            assert image_grid_thw is not None or video_grid_thw is not None, (\n                \"mrope embedding type requires multi-modal input mapper \"\n                \"returns 'image_grid_thw' or 'video_grid_thw'.\")\n\n            hf_config = self.runner.model_config.hf_config\n\n            inter_data.mrope_input_positions = [None] * inter_data.n_seqs\n            for seq_idx in range(inter_data.n_seqs):\n                seq_data = seq_group_metadata.seq_data[\n                    inter_data.seq_ids[seq_idx]]\n                token_ids = seq_data.get_token_ids()\n\n                mrope_input_positions, mrope_position_delta = \\\n                    MRotaryEmbedding.get_input_positions(\n                        token_ids,\n                        image_grid_thw=image_grid_thw,\n                        video_grid_thw=video_grid_thw,\n                        image_token_id=hf_config.image_token_id,\n                        video_token_id=hf_config.video_token_id,\n                        vision_start_token_id=hf_config.vision_start_token_id,\n                        vision_end_token_id=hf_config.vision_end_token_id,\n                        spatial_merge_size=hf_config.vision_config.\n                        spatial_merge_size,\n                        context_len=inter_data.context_lens[seq_idx],\n                    )\n\n                seq_data.mrope_position_delta = mrope_position_delta\n                inter_data.mrope_input_positions[\n                    seq_idx] = mrope_input_positions\n\n    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Add a sequence group to the builder.\"\"\"\n        seq_ids = seq_group_metadata.seq_data.keys()\n        n_seqs = len(seq_ids)\n        is_prompt = seq_group_metadata.is_prompt\n\n        if is_prompt:\n            assert n_seqs == 1\n            self.decode_only = False\n\n        encoder_seq_len = 0\n\n        if self.runner.model_config.is_encoder_decoder_model:\n            encoder_seq_len = seq_group_metadata.encoder_seq_data.get_len()\n\n        inter_data = self.init_cached_inter_data(\n            request_id=seq_group_metadata.request_id,\n            seq_ids=seq_ids,\n            is_prompt=is_prompt,\n            block_tables=seq_group_metadata.block_tables,\n            computed_block_nums=seq_group_metadata.computed_block_nums,\n            reinit=True,\n            reinit_use_defaults=True,\n            encoder_seq_len=encoder_seq_len)\n\n        self.inter_data_list.append(inter_data)\n\n        for seq_idx in range(n_seqs):\n            for per_seq_fn in self.per_seq_compute_fns:\n                per_seq_fn(inter_data, seq_idx, seq_group_metadata)\n        for per_seq_group_fn in self.per_seq_group_compute_fns:\n            per_seq_group_fn(inter_data, seq_group_metadata)\n\n    def _use_captured_graph(self,\n                            batch_size: int,\n                            decode_only: bool,\n                            max_decode_seq_len: int,\n                            max_encoder_seq_len: int = 0) -> bool:\n        return (decode_only and not self.runner.model_config.enforce_eager\n                and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n                and max_decode_seq_len <= self.runner.max_seq_len_to_capture\n                and max_encoder_seq_len <= self.runner.max_seq_len_to_capture\n                and batch_size <= self.runner.max_batchsize_to_capture)\n\n    def _get_cuda_graph_pad_size(self,\n                                 num_seqs: int,\n                                 max_decode_seq_len: int,\n                                 max_encoder_seq_len: int = 0) -> int:\n        \"\"\"\n        Determine the number of padding sequences required for running in\n        CUDA graph mode. Returns -1 if CUDA graphs cannot be used.\n\n        In the multi-step + chunked-prefill case, only the first step\n        has Prefills (if any). The rest of the steps are guaranteed to be all\n        decodes. In this case, we set up the padding as if all the sequences\n        are decodes so we may run all steps except the first step in CUDA graph\n        mode. The padding is accounted for in the multi-step `advance_step`\n        family of functions.\n\n        Args:\n            num_seqs (int): Number of sequences scheduled to run. \n            max_decode_seq_len (int): Greatest of all the decode sequence\n                lengths. Used only in checking the viablility of using\n                CUDA graphs.\n            max_encoder_seq_len (int, optional): Greatest of all the encode\n                sequence lengths. Defaults to 0. Used only in checking the\n                viability of using CUDA graphs. \n        Returns:\n            int: Returns the determined number of padding sequences. If\n                CUDA graphs is not viable, returns -1.\n        \"\"\"\n        is_mscp: bool = self.runner.scheduler_config.is_multi_step and \\\n                    self.runner.scheduler_config.chunked_prefill_enabled\n        decode_only = self.decode_only or is_mscp\n        if not decode_only:\n            # Early exit so we can treat num_seqs as the batch_size below.\n            return -1\n\n        # batch_size out of this function refers to the number of input\n        # tokens being scheduled. This conflation of num_seqs as batch_size\n        # is valid as this is a decode-only case.\n        batch_size = num_seqs\n        if not self._use_captured_graph(batch_size, decode_only,\n                                        max_decode_seq_len,\n                                        max_encoder_seq_len):\n            return -1\n\n        graph_batch_size = _get_graph_batch_size(batch_size)\n        assert graph_batch_size >= batch_size\n        return graph_batch_size - batch_size\n\n    def build(self) -> ModelInputForGPU:\n        \"\"\"Finalize the builder intermediate data and\n        create on-device tensors.\n        \"\"\"\n        # Combine and flatten intermediate data.\n        input_tokens = []\n        for inter_data in self.inter_data_list:\n            for cur_input_tokens in inter_data.input_tokens:\n                input_tokens.extend(cur_input_tokens)\n\n        if not input_tokens:\n            # This may happen when all prefill requests hit\n            # prefix caching and there is no decode request.\n            return self.model_input_cls()\n\n        mrope_input_positions: Optional[List[List[int]]] = None\n        if any(inter_data.mrope_input_positions is not None\n               for inter_data in self.inter_data_list):\n            mrope_input_positions = [[] for _ in range(3)]\n            for idx in range(3):\n                for inter_data in self.inter_data_list:\n                    msections = inter_data.mrope_input_positions\n                    if msections is None:\n                        for _seq_input_positions in inter_data.input_positions:\n                            mrope_input_positions[idx].extend(\n                                _seq_input_positions)\n                    else:\n                        for _seq_mrope_input_positions in msections:\n                            mrope_input_positions[idx].extend(\n                                _seq_mrope_input_positions[idx])\n            input_positions = None\n        else:\n            input_positions = []\n            for inter_data in self.inter_data_list:\n                for cur_input_positions in inter_data.input_positions:\n                    input_positions.extend(cur_input_positions)\n\n        seq_lens = []\n        query_lens = []\n        max_decode_seq_len = 0\n        max_encoder_seq_len = 0\n        for inter_data in self.inter_data_list:\n            seq_lens.extend(inter_data.seq_lens)\n            query_lens.extend(inter_data.query_lens)\n            if not inter_data.is_prompt:\n                max_decode_seq_len = max(max_decode_seq_len,\n                                         max(inter_data.seq_lens))\n                if self.runner.model_config.is_encoder_decoder_model:\n                    max_encoder_seq_len = max(max_encoder_seq_len,\n                                              inter_data.encoder_seq_len)\n\n        # Mapping from request IDs to sequence IDs. Used for Jamba models\n        # that manages the cache by itself.\n        request_ids_to_seq_ids = {\n            data.request_id: data.seq_ids\n            for data in self.inter_data_list\n        }\n\n        cuda_graph_pad_size = self._get_cuda_graph_pad_size(\n            num_seqs=len(seq_lens),\n            max_decode_seq_len=max_encoder_seq_len,\n            max_encoder_seq_len=max_encoder_seq_len)\n\n        batch_size = len(input_tokens)\n        if cuda_graph_pad_size != -1:\n            # If cuda graph can be used, pad tensors accordingly.\n            # See `capture_model` API for more details.\n            # vLLM uses cuda graph only for decoding requests.\n            batch_size += cuda_graph_pad_size\n\n        # Tokens and positions.\n        if cuda_graph_pad_size:\n            input_tokens.extend(itertools.repeat(0, cuda_graph_pad_size))\n        assert self.runner.device is not None\n        input_tokens_tensor = async_tensor_h2d(input_tokens, torch.long,\n                                               self.runner.device,\n                                               self.runner.pin_memory)\n        if mrope_input_positions is not None:\n            for idx in range(3):\n                mrope_input_positions[idx].extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            input_positions_tensor = async_tensor_h2d(mrope_input_positions,\n                                                      torch.long,\n                                                      self.runner.device,\n                                                      self.runner.pin_memory)\n        else:\n            input_positions.extend(itertools.repeat(0, cuda_graph_pad_size))\n            input_positions_tensor = async_tensor_h2d(input_positions,\n                                                      torch.long,\n                                                      self.runner.device,\n                                                      self.runner.pin_memory)\n        # Sequence and query lengths.\n        if cuda_graph_pad_size:\n            seq_lens.extend(itertools.repeat(1, cuda_graph_pad_size))\n\n        # Attention metadata.\n        attn_metadata = self.attn_metadata_builder.build(\n            seq_lens, query_lens, cuda_graph_pad_size, batch_size)\n\n        # LoRA data.\n        lora_requests = set()\n        lora_mapping = None\n        if self.enable_lora:\n            lora_requests = set(r for data in self.inter_data_list\n                                for r in data.lora_requests)\n            lora_index_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_index_mapping)\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                lora_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            lora_prompt_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_prompt_mapping)\n                for inter_data in self.inter_data_list\n            ])\n\n            lora_mapping = LoRAMapping(\n                **dict(index_mapping=lora_index_mapping,\n                       prompt_mapping=lora_prompt_mapping,\n                       is_prefill=not self.decode_only))\n\n        # Prompt adapter data.\n        prompt_adapter_requests: Set[PromptAdapterRequest] = set()\n        prompt_adapter_mapping = None\n        if self.enable_prompt_adapter:\n            prompt_adapter_requests = set(\n                data.prompt_adapter_request for data in self.inter_data_list\n                if data.prompt_adapter_request is not None)\n            prompt_adapter_index_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_index_mapping\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                prompt_adapter_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            prompt_adapter_prompt_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_prompt_mapping\n                for inter_data in self.inter_data_list\n            ])\n            prompt_adapter_mapping = PromptAdapterMapping(\n                prompt_adapter_index_mapping,\n                prompt_adapter_prompt_mapping,\n            )\n\n        # Multi-modal data.\n        multi_modal_inputs_list = [\n            data.multi_modal_inputs for data in self.inter_data_list\n            if data.multi_modal_inputs is not None\n        ]\n        multi_modal_kwargs = MultiModalInputs.batch(multi_modal_inputs_list)\n\n        return self.model_input_cls(\n            input_tokens=input_tokens_tensor,\n            input_positions=input_positions_tensor,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_mapping=lora_mapping,\n            lora_requests=lora_requests,\n            multi_modal_kwargs=multi_modal_kwargs,\n            request_ids_to_seq_ids=request_ids_to_seq_ids,\n            finished_requests_ids=self.finished_requests_ids,\n            prompt_adapter_mapping=prompt_adapter_mapping,\n            prompt_adapter_requests=prompt_adapter_requests)\n\n\nclass GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n    \"\"\"\n    Helper class for shared methods between GPU model runners.\n    \"\"\"\n    _model_input_cls: Type[TModelInputForGPU]\n    _builder_cls: Type[ModelInputForGPUBuilder]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        return_hidden_states: bool = False,\n        observability_config: Optional[ObservabilityConfig] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        self.prompt_adapter_config = prompt_adapter_config\n        self.return_hidden_states = return_hidden_states\n        self.observability_config = observability_config\n\n        self.device = self.device_config.device\n        self.pin_memory = is_pin_memory_available()\n\n        self.kv_cache_dtype = kv_cache_dtype\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_seq_len_to_capture = self.model_config.max_seq_len_to_capture\n        self.max_batchsize_to_capture = _get_max_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n\n        self.graph_runners: List[Dict[int, CUDAGraphRunner]] = [\n            {} for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n\n        self.has_inner_state = model_config.has_inner_state\n\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = np.zeros(\n            (self.max_batchsize_to_capture, self.get_max_block_per_batch()),\n            dtype=np.int32)\n\n        # Attention-free but stateful models like Mamba need a placeholder attn\n        # backend, as the attention metadata is needed to manage internal state.\n        # However we must bypass attention selection altogether for some models\n        # used for speculative decoding to avoid a divide-by-zero in\n        # model_config.get_head_size()\n        num_attn_heads = self.model_config.get_num_attention_heads(\n            self.parallel_config)\n        needs_attn_backend = (num_attn_heads != 0\n                              or self.model_config.is_attention_free)\n\n        self.attn_backend = get_attn_backend(\n            self.model_config.get_head_size(),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.kv_cache_dtype,\n            self.block_size,\n            self.model_config.is_attention_free,\n        ) if needs_attn_backend else None\n        if self.attn_backend:\n            self.attn_state = self.attn_backend.get_state_cls()(\n                weakref.proxy(self))\n        else:\n            self.attn_state = CommonAttentionState(weakref.proxy(self))\n\n        # Multi-modal data support\n        self.input_registry = input_registry\n        self.mm_registry = mm_registry\n        self.multi_modal_input_mapper = mm_registry \\\n            .create_input_mapper(model_config)\n        self.mm_registry.init_mm_limits_per_prompt(self.model_config)\n\n        # Lazy initialization\n        self.model: nn.Module  # Set after load_model\n        # Set after load_model.\n        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None\n        self.prompt_adapter_manager: LRUCacheWorkerPromptAdapterManager = None\n\n        set_cpu_offload_max_bytes(\n            int(self.cache_config.cpu_offload_gb * 1024**3))\n\n        # Used to cache python objects\n        self.inter_data_cache: Dict[int, PyObjectCache] = {}\n\n        # Using the PythonizationCache in Pipeline-Parallel clobbers the\n        # SequenceGroupToSample object. In Pipeline-Parallel, we have\n        # more than 1 Scheduler, resulting in a potential back-to-back\n        # prepare_model_inputs() call. This clobbers the cached\n        # SequenceGroupToSample objects, as we reset the cache during\n        # every prepare_model_inputs() call.\n        self.sampling_metadata_cache: SamplingMetadataCache = \\\n              SamplingMetadataCache() \\\n                if self.parallel_config.pipeline_parallel_size == 1 else None\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with DeviceMemoryProfiler() as m:\n            self.model = get_model(model_config=self.model_config,\n                                   device_config=self.device_config,\n                                   load_config=self.load_config,\n                                   lora_config=self.lora_config,\n                                   parallel_config=self.parallel_config,\n                                   scheduler_config=self.scheduler_config,\n                                   cache_config=self.cache_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert supports_lora(\n                self.model\n            ), f\"{self.model.__class__.__name__} does not support LoRA yet.\"\n\n            if supports_multimodal(self.model):\n                logger.warning(\"Regarding multimodal models, vLLM currently \"\n                               \"only supports adding LoRA to language model.\")\n            # It's necessary to distinguish between the max_position_embeddings\n            # of VLMs and LLMs.\n            if hasattr(self.model.config, \"max_position_embeddings\"):\n                max_pos_embeddings = self.model.config.max_position_embeddings\n            else:\n                max_pos_embeddings = (\n                    self.model.config.text_config.max_position_embeddings)\n\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens,\n                self.vocab_size,\n                self.lora_config,\n                self.device,\n                self.model.embedding_modules,\n                self.model.embedding_padding_modules,\n                max_position_embeddings=max_pos_embeddings,\n            )\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.prompt_adapter_config:\n            self.prompt_adapter_manager = LRUCacheWorkerPromptAdapterManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens, self.device,\n                self.prompt_adapter_config)\n            self.model = (\n                self.prompt_adapter_manager.create_prompt_adapter_manager(\n                    self.model))\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently only ROCm accepts kv-cache scaling factors\n            # via quantization_param_path and this will be deprecated\n            # in the future.\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    warnings.warn(\n                        \"Loading kv cache scaling factor from JSON is \"\n                        \"deprecated and will be removed. Please include \"\n                        \"kv cache scaling factors in the model checkpoint.\",\n                        FutureWarning,\n                        stacklevel=2)\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                    logger.info(\"Loaded KV cache scaling factors from %s\",\n                                self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n\n        if envs.VLLM_TORCH_COMPILE_LEVEL == CompilationLevel.DYNAMO_AS_IS \\\n            and supports_dynamo():\n            from vllm.plugins import get_torch_compile_backend\n            backend = get_torch_compile_backend() or \"eager\"\n            self.model = torch.compile(\n                self.model,\n                fullgraph=envs.VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE,\n                backend=backend)\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import ShardedStateLoader\n        ShardedStateLoader.save_model(\n            self.model,\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def save_tensorized_model(\n        self,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import TensorizerLoader\n        TensorizerLoader.save_model(\n            self.model,\n            tensorizer_config=tensorizer_config,\n        )\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_model_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> TModelInputForGPU:\n        \"\"\"Helper method to prepare the model input based on a given sequence\n        group. Prepares metadata needed for the base model forward pass but not\n        metadata for possible additional steps, e.g., sampling.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)\n        for seq_group_metadata in seq_group_metadata_list:\n            builder.add_seq_group(seq_group_metadata)\n\n        builder.reset_cached_inter_data()\n\n        return builder.build()  # type: ignore\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests: List[LoRARequest] = []\n        dummy_lora_requests_per_seq: List[LoRARequest] = []\n        if self.lora_config:\n            assert self.lora_manager is not None\n            with self.lora_manager.dummy_lora_cache():\n                for idx in range(self.lora_config.max_loras):\n                    lora_id = idx + 1\n                    dummy_lora_request = LoRARequest(\n                        lora_name=f\"warmup_{lora_id}\",\n                        lora_int_id=lora_id,\n                        lora_path=\"/not/a/real/path\",\n                    )\n                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                     rank=LORA_WARMUP_RANK)\n                    dummy_lora_requests.append(dummy_lora_request)\n                dummy_lora_requests_per_seq = [\n                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n                    for idx in range(max_num_seqs)\n                ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for multi-modal encoding, which\n        # needs to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n\n        max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\n            self.model_config)\n        if max_mm_tokens > 0:\n            max_num_seqs_orig = max_num_seqs\n            max_num_seqs = min(max_num_seqs,\n                               max_num_batched_tokens // max_mm_tokens)\n            if max_num_seqs < 1:\n                expr = (f\"min({max_num_seqs_orig}, \"\n                        f\"{max_num_batched_tokens} // {max_mm_tokens})\")\n                logger.warning(\n                    \"Computed max_num_seqs (%s) to be less than 1. \"\n                    \"Setting it to the minimum value of 1.\", expr)\n                max_num_seqs = 1\n\n        batch_size = 0\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            batch_size += seq_len\n\n            seq_data, dummy_multi_modal_data = self.input_registry \\\n                .dummy_data_for_profiling(self.model_config,\n                                          seq_len,\n                                          self.mm_registry)\n\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=dummy_multi_modal_data,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        # use an empty tensor instead of `None`` to force Dynamo to pass\n        # it by reference, rather by specializing on the value ``None``.\n        # the `dtype` argument does not matter, and we use `float32` as\n        # a placeholder (it has wide hardware support).\n        # it is important to create tensors inside the loop, rather than\n        # multiplying the list, to avoid Dynamo from treating them as\n        # tensor aliasing.\n        kv_caches = [\n            torch.tensor([], dtype=torch.float32, device=self.device)\n            for _ in range(num_layers)\n        ]\n        finished_requests_ids = [seq.request_id for seq in seqs]\n        model_input = self.prepare_model_input(\n            seqs, finished_requests_ids=finished_requests_ids)\n        intermediate_tensors = None\n        if not get_pp_group().is_first_rank:\n            intermediate_tensors = self.model.make_empty_intermediate_tensors(\n                batch_size=batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        graph_batch_size = self.max_batchsize_to_capture\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n        if self.model_config.enforce_eager:\n            batch_size_capture_list = []\n        with set_compile_context(batch_size_capture_list):\n            self.execute_model(model_input, kv_caches, intermediate_tensors)\n        torch.cuda.synchronize()\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_adapters()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_adapter(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_adapter(lora_id)\n\n    def pin_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.pin_adapter(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_adapters()\n\n    def remove_all_prompt_adapters(self):\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.remove_all_adapters()\n\n    def set_active_prompt_adapters(\n            self, prompt_adapter_requests: Set[PromptAdapterRequest],\n            prompt_adapter_mapping: PromptAdapterMapping) -> None:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.set_active_adapters(\n            prompt_adapter_requests, prompt_adapter_mapping)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.add_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.remove_adapter(prompt_adapter_id)\n\n    def pin_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.pin_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> Set[int]:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.list_adapters()\n\n    @property\n    def model_is_mrope(self) -> bool:\n        \"\"\"Detect if the model has \"mrope\" rope_scaling type.\n        mrope requires keep \"rope_deltas\" between prompt and decoding phases.\"\"\"\n        return uses_mrope(self.model_config.hf_config)\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[List[torch.Tensor]]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = self.max_batchsize_to_capture\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        if self.model_is_mrope:\n            input_positions = torch.tile(input_positions, (3, 1))\n        # Prepare dummy previous_hidden_states only if needed by the model.\n        # This is used by draft models such as EAGLE.\n        previous_hidden_states = None\n        if \"previous_hidden_states\" in inspect.signature(\n                self.model.forward).parameters:\n            previous_hidden_states = torch.empty(\n                [max_batch_size,\n                 self.model_config.get_hidden_size()],\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        intermediate_inputs = None\n        if not get_pp_group().is_first_rank:\n            intermediate_inputs = self.model.make_empty_intermediate_tensors(\n                batch_size=max_batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        # Prepare buffer for outputs. These will be reused for all batch sizes.\n        # It will be filled after the first graph capture.\n        hidden_or_intermediate_states: List[Optional[torch.Tensor]] = [\n            None\n        ] * self.parallel_config.pipeline_parallel_size\n\n        graph_batch_size = self.max_batchsize_to_capture\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        with self.attn_state.graph_capture(\n                max_batch_size), graph_capture() as graph_capture_context:\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for virtual_engine in range(\n                    self.parallel_config.pipeline_parallel_size):\n                for batch_size in reversed(batch_size_capture_list):\n                    attn_metadata = (\n                        self.attn_state.graph_capture_get_metadata_for_batch(\n                            batch_size,\n                            is_encoder_decoder_model=self.model_config.\n                            is_encoder_decoder_model))\n\n                    if self.lora_config:\n                        lora_mapping = LoRAMapping(\n                            **dict(index_mapping=[0] * batch_size,\n                                   prompt_mapping=[0] * batch_size,\n                                   is_prefill=False))\n                        self.set_active_loras(set(), lora_mapping)\n\n                    if self.prompt_adapter_config:\n                        prompt_adapter_mapping = PromptAdapterMapping(\n                            [-1] * batch_size,\n                            [-1] * batch_size,\n                        )\n                        self.set_active_prompt_adapters(\n                            set(), prompt_adapter_mapping)\n                    graph_runner = CUDAGraphRunner(\n                        self.model, self.attn_backend.get_name(),\n                        self.attn_state.graph_clone(batch_size),\n                        self.model_config.is_encoder_decoder_model)\n\n                    capture_inputs = {\n                        \"input_ids\":\n                        input_tokens[:batch_size],\n                        \"positions\":\n                        input_positions[..., :batch_size],\n                        \"hidden_or_intermediate_states\":\n                        hidden_or_intermediate_states[\n                            virtual_engine]  # type: ignore\n                        [:batch_size]\n                        if hidden_or_intermediate_states[virtual_engine]\n                        is not None else None,\n                        \"intermediate_inputs\":\n                        intermediate_inputs[:batch_size]\n                        if intermediate_inputs is not None else None,\n                        \"kv_caches\":\n                        kv_caches[virtual_engine],\n                        \"attn_metadata\":\n                        attn_metadata,\n                        \"memory_pool\":\n                        self.graph_memory_pool,\n                        \"stream\":\n                        graph_capture_context.stream\n                    }\n                    if previous_hidden_states is not None:\n                        capture_inputs[\n                            \"previous_hidden_states\"] = previous_hidden_states[:\n                                                                               batch_size]\n\n                    if self.has_inner_state:\n                        # Only used by Mamba-based models CUDA graph atm (Jamba)\n                        capture_inputs.update({\n                            \"seqlen_agnostic_capture_inputs\":\n                            self.model.get_seqlen_agnostic_capture_inputs(\n                                batch_size)\n                        })\n                    if self.model_config.is_encoder_decoder_model:\n                        # add the additional inputs to capture for\n                        # encoder-decoder models.\n                        self._update_inputs_to_capture_for_enc_dec_model(\n                            capture_inputs)\n\n                    with set_forward_context(attn_metadata):\n                        graph_runner.capture(**capture_inputs)\n                    self.graph_memory_pool = graph_runner.graph.pool()\n                    self.graph_runners[virtual_engine][batch_size] = (\n                        graph_runner)\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    def _update_inputs_to_capture_for_enc_dec_model(self,\n                                                    capture_inputs: Dict[str,\n                                                                         Any]):\n        \"\"\"\n        Updates the set of input tensors needed for CUDA graph capture in an\n        encoder-decoder model.\n\n        This method modifies the provided `capture_inputs` dictionary by\n        adding tensors specific to encoder-decoder specific models that\n        need to be captured for CUDA Graph replay.\n        \"\"\"\n        # During the decode phase encoder_input_ids and encoder_positions are\n        # unset. Do the same thing for graph capture.\n        capture_inputs[\"encoder_input_ids\"] = torch.tensor(\n            [], dtype=torch.long).cuda()\n        capture_inputs[\"encoder_positions\"] = torch.tensor(\n            [], dtype=torch.long).cuda()\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n    \"\"\"\n    GPU model runner with sampling step.\n    \"\"\"\n    _model_input_cls: Type[ModelInputForGPUWithSamplingMetadata] = (\n        ModelInputForGPUWithSamplingMetadata)\n    _builder_cls: Type[ModelInputForGPUBuilder] = ModelInputForGPUBuilder\n\n    def make_model_input_from_broadcasted_tensor_dict(\n        self,\n        tensor_dict: Dict[str, Any],\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        model_input = \\\n            ModelInputForGPUWithSamplingMetadata.from_broadcasted_tensor_dict(\n                tensor_dict,\n                attn_backend=self.attn_backend,\n            )\n        return model_input\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None,\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        model_input = self._prepare_model_input_tensors(\n            seq_group_metadata_list, finished_requests_ids)\n        if get_pp_group().is_last_rank:\n            # Sampling metadata is only required for the final pp group\n            generators = self.get_generators(finished_requests_ids)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, model_input.seq_lens,\n                model_input.query_lens, self.device, self.pin_memory,\n                generators, self.sampling_metadata_cache)\n        else:\n            sampling_metadata = None\n        is_prompt = (seq_group_metadata_list[0].is_prompt\n                     if seq_group_metadata_list else None)\n        return dataclasses.replace(model_input,\n                                   sampling_metadata=sampling_metadata,\n                                   is_prompt=is_prompt,\n                                   virtual_engine=virtual_engine)\n\n    @torch.inference_mode()\n    @dump_input_when_exception(exclude_args=[0], exclude_kwargs=[\"self\"])\n    def execute_model(\n        self,\n        model_input: ModelInputForGPUWithSamplingMetadata,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:\n        if num_steps > 1:\n            raise ValueError(\"num_steps > 1 is not supported in ModelRunner\")\n\n        if self.lora_config:\n            assert model_input.lora_requests is not None\n            assert model_input.lora_mapping is not None\n            self.set_active_loras(model_input.lora_requests,\n                                  model_input.lora_mapping)\n\n        if self.prompt_adapter_config:\n            assert model_input.prompt_adapter_requests is not None\n            assert model_input.prompt_adapter_mapping is not None\n            self.set_active_prompt_adapters(\n                model_input.prompt_adapter_requests,\n                model_input.prompt_adapter_mapping)\n\n        self.attn_state.begin_forward(model_input)\n\n        # Currently cuda graph is only supported by the decode phase.\n        assert model_input.attn_metadata is not None\n        prefill_meta = model_input.attn_metadata.prefill_metadata\n        decode_meta = model_input.attn_metadata.decode_metadata\n        # TODO(andoorve): We can remove this once all\n        # virtual engines share the same kv cache.\n        virtual_engine = model_input.virtual_engine\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            assert model_input.input_tokens is not None\n            graph_batch_size = model_input.input_tokens.shape[0]\n            model_executable = self.graph_runners[virtual_engine][\n                graph_batch_size]\n        else:\n            model_executable = self.model\n\n        multi_modal_kwargs = model_input.multi_modal_kwargs or {}\n        seqlen_agnostic_kwargs = {\n            \"finished_requests_ids\": model_input.finished_requests_ids,\n            \"request_ids_to_seq_ids\": model_input.request_ids_to_seq_ids,\n        } if self.has_inner_state else {}\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_start = torch.cuda.Event(enable_timing=True)\n            model_forward_end = torch.cuda.Event(enable_timing=True)\n            model_forward_start.record()\n\n        with set_forward_context(model_input.attn_metadata):\n            hidden_or_intermediate_states = model_executable(\n                input_ids=model_input.input_tokens,\n                positions=model_input.input_positions,\n                kv_caches=kv_caches,\n                attn_metadata=model_input.attn_metadata,\n                intermediate_tensors=intermediate_tensors,\n                **MultiModalInputs.as_kwargs(multi_modal_kwargs,\n                                             device=self.device),\n                **seqlen_agnostic_kwargs)\n\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_end.record()\n\n        # Compute the logits in the last pipeline stage.\n        if not get_pp_group().is_last_rank:\n            if (self.is_driver_worker\n                    and hidden_or_intermediate_states is not None\n                    and isinstance(hidden_or_intermediate_states,\n                                   IntermediateTensors)\n                    and self.observability_config is not None\n                    and self.observability_config.collect_model_forward_time):\n                model_forward_end.synchronize()\n                model_forward_time = model_forward_start.elapsed_time(\n                    model_forward_end)\n                orig_model_forward_time = 0.0\n                if intermediate_tensors is not None:\n                    orig_model_forward_time = intermediate_tensors.tensors.get(\n                        \"model_forward_time\", torch.tensor(0.0)).item()\n                hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                    torch.tensor(model_forward_time + orig_model_forward_time))\n            return hidden_or_intermediate_states\n\n        logits = self.model.compute_logits(hidden_or_intermediate_states,\n                                           model_input.sampling_metadata)\n\n        if not self.is_driver_worker:\n            return []\n\n        if model_input.async_callback is not None:\n            model_input.async_callback()\n\n        # Sample the next token.\n        output: SamplerOutput = self.model.sample(\n            logits=logits,\n            sampling_metadata=model_input.sampling_metadata,\n        )\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time\n                and output is not None):\n            model_forward_end.synchronize()\n            model_forward_time = model_forward_start.elapsed_time(\n                model_forward_end)\n            orig_model_forward_time = 0.0\n            if intermediate_tensors is not None:\n                orig_model_forward_time = intermediate_tensors.tensors.get(\n                    \"model_forward_time\", torch.tensor(0.0)).item()\n            # If there are multiple workers, we are still tracking the latency\n            # from the start time of the driver worker to the end time of the\n            # driver worker. The model forward time will then end up covering\n            # the communication time as well.\n            output.model_forward_time = (orig_model_forward_time +\n                                         model_forward_time)\n\n        if self.return_hidden_states:\n            # we only need to pass hidden states of most recent token\n            assert model_input.sampling_metadata is not None\n            indices = model_input.sampling_metadata.selected_token_indices\n            if model_input.is_prompt:\n                hidden_states = hidden_or_intermediate_states.index_select(\n                    0, indices)\n                output.prefill_hidden_states = hidden_or_intermediate_states\n            elif decode_meta.use_cuda_graph:\n                hidden_states = hidden_or_intermediate_states[:len(indices)]\n            else:\n                hidden_states = hidden_or_intermediate_states\n\n            output.hidden_states = hidden_states\n\n        return [output]\n\n\n# NOTE: this is nn.Module so the profiler can properly capture/group\n#  kernels calls made within the graph\nclass CUDAGraphRunner(nn.Module):\n\n    def __init__(self, model: nn.Module, backend_name: str,\n                 attn_state: AttentionState, is_encoder_decoder_model: bool):\n        super().__init__()\n        self.model = model\n        self.backend_name = backend_name\n        self.attn_state = attn_state\n\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n        self._is_encoder_decoder_model = is_encoder_decoder_model\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        hidden_or_intermediate_states: Optional[Union[IntermediateTensors,\n                                                      torch.Tensor]],\n        intermediate_inputs: Optional[IntermediateTensors],\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool: Optional[Tuple[int, int]],\n        stream: torch.cuda.Stream,\n        **kwargs,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        assert self._graph is None\n        # Run the model a few times without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        # Note one iteration is not enough for torch.jit.script\n        for _ in range(_NUM_WARMUP_ITERS):\n            self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n        # Wait for the warm up operations to finish before proceeding with\n        # Graph Capture.\n        torch.cuda.synchronize()\n        # Capture the graph.\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            output_hidden_or_intermediate_states = self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n            if hidden_or_intermediate_states is not None:\n                if get_pp_group().is_last_rank:\n                    hidden_or_intermediate_states.copy_(\n                        output_hidden_or_intermediate_states)\n                else:\n                    for key in hidden_or_intermediate_states.tensors:\n                        hidden_or_intermediate_states[key].copy_(\n                            output_hidden_or_intermediate_states[key])\n            else:\n                hidden_or_intermediate_states = (\n                    output_hidden_or_intermediate_states)\n\n            del output_hidden_or_intermediate_states\n            # make sure `output_hidden_states` is deleted\n            # in the graph's memory pool\n            gc.collect()\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\":\n            input_ids,\n            \"positions\":\n            positions,\n            \"kv_caches\":\n            kv_caches,\n            **self.attn_state.get_graph_input_buffers(\n                attn_metadata, self._is_encoder_decoder_model),\n            **kwargs,\n        }\n        if intermediate_inputs is not None:\n            self.input_buffers.update(intermediate_inputs.tensors)\n        if get_pp_group().is_last_rank:\n            self.output_buffers = {\n                \"hidden_states\": hidden_or_intermediate_states\n            }\n        else:\n            self.output_buffers = hidden_or_intermediate_states\n        return hidden_or_intermediate_states\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n\n        if self.backend_name != \"placeholder-attn\":\n            self.input_buffers[\"slot_mapping\"].copy_(\n                attn_metadata.slot_mapping, non_blocking=True)\n\n        self.attn_state.prepare_graph_input_buffers(\n            self.input_buffers, attn_metadata, self._is_encoder_decoder_model)\n\n        if \"seqlen_agnostic_capture_inputs\" in self.input_buffers:\n            self.model.copy_inputs_before_cuda_graphs(self.input_buffers,\n                                                      **kwargs)\n\n        if \"previous_hidden_states\" in self.input_buffers:\n            self.input_buffers[\"previous_hidden_states\"].copy_(\n                kwargs[\"previous_hidden_states\"], non_blocking=True)\n\n        if intermediate_tensors is not None:\n            for key in intermediate_tensors.tensors:\n                if key != \"model_execute_time\" and key != \"model_forward_time\":\n                    self.input_buffers[key].copy_(intermediate_tensors[key],\n                                                  non_blocking=True)\n        if self._is_encoder_decoder_model:\n            self.input_buffers[\"encoder_input_ids\"].copy_(\n                kwargs['encoder_input_ids'], non_blocking=True)\n            self.input_buffers[\"encoder_positions\"].copy_(\n                kwargs['encoder_positions'], non_blocking=True)\n\n        # Run the graph.\n        self.graph.replay()\n        # Return the output tensor.\n        if get_pp_group().is_last_rank:\n            return self.output_buffers[\"hidden_states\"]\n\n        return self.output_buffers\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n\n\ndef _get_max_graph_batch_size(max_num_seqs: int) -> int:\n    \"\"\"\n    max_num_seqs: Maximum number of sequences in a batch.\n    _BATCH_SIZES_TO_CAPTURE: all the sizes that we want to capture.\n\n    pad the max_num_seqs if necessary by calling _get_graph_batch_size,\n    which will deal with some edge cases like 1, 2, 4.\n\n    if the padded size is in _BATCH_SIZES_TO_CAPTURE, return the padded size.\n    if not, it means the padded size is larger than the largest size in\n    _BATCH_SIZES_TO_CAPTURE, return the largest size in _BATCH_SIZES_TO_CAPTURE.\n    \"\"\"\n    padded_size = _get_graph_batch_size(max_num_seqs)\n    if padded_size in _BATCH_SIZES_TO_CAPTURE:\n        return padded_size\n    assert padded_size > _BATCH_SIZES_TO_CAPTURE[-1]\n    return _BATCH_SIZES_TO_CAPTURE[-1]\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 36753b858..a82956985 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -574,17 +574,12 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             # paged attn. We can remove it if we make paged attn kernel\n             # to properly handle slinding window attn.\n             curr_sliding_window_block = self.sliding_window_blocks\n-            if self.scheduler_config.use_v2_block_manager:\n-                # number of elements in last block\n-                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n-                sliding_seq_len = min(\n-                    inter_data.seq_lens[seq_idx],\n-                    self.block_aligned_sliding_window + suff_len)\n-                if suff_len > 0:\n-                    curr_sliding_window_block += 1\n-            else:\n-                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n-                                      self.sliding_window)\n+            # number of elements in last block\n+            suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n+            sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n+                                  self.block_aligned_sliding_window + suff_len)\n+            if suff_len > 0:\n+                curr_sliding_window_block += 1\n \n         inter_data.curr_sliding_window_blocks[\n             seq_idx] = curr_sliding_window_block",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 12
    }
  ],
  "affected_apis": [
    "vllm.LLM.__init__",
    "vllm.engine.arg_utils.EngineArgs",
    "benchmarks.benchmark_throughput.run_vllm",
    "benchmarks.benchmark_throughput.run_vllm_async"
  ],
  "summary": {
    "total_files": 45,
    "files_added": 1,
    "files_deleted": 2,
    "files_modified": 40
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "TRUE",
    "is_test_actually_there": "NO (way too much BS)",
    "is_benchmark_actually_there": "",
    "sample_clues": "arg, arg_utils, benchmark_hashing"
  }
}