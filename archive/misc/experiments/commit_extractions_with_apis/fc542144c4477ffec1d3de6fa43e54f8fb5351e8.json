{
  "commit_hash": "fc542144c4477ffec1d3de6fa43e54f8fb5351e8",
  "parent_hash": "eb5741ad422f04d0bac60c9b6c07183e0431ce8c",
  "message": "[Feature] Fix guided decoding blocking bitmask memcpy (#12563)\n\n**[Guided decoding performance optimization]** Sending the guided\ndecoding bitmask in xgrammar to the GPU\n(`self.token_bitmask.to(scores.device)`) is a blocking operation that\nprevents the CPU from pre-launching the sampler kernels. The CPU waits\nuntil decode is complete, then copies the bitmask over. This PR changes\nthe operation to async via setting `non-blocking=True`.\n\n(Current) The CPU is blocked on a `cudaStreamSynchronize` and only\npre-empts the sampling kernels after bitmask application. Below is the\nNsys profile for one decode phase from Llama 3.1 8B.\n\n![image](https://github.com/user-attachments/assets/8997eae1-b822-4f52-beb8-ef19a7c6b824)\n\nWith the optimization, this is no longer the case:\n\n![image](https://github.com/user-attachments/assets/6d5ea83f-f169-4f98-a8c1-41c719b3e1e7)\n\n---------\n\nSigned-off-by: Ryan N <ryan.nguyen@centml.ai>",
  "author": "Ryan Nguyen <96593302+xpbowler@users.noreply.github.com>",
  "date": "2025-01-31 15:37:30 -0800",
  "files_changed": [
    {
      "file_path": "vllm/model_executor/guided_decoding/xgrammar_decoding.py",
      "old_content": "# noqa: UP007\nfrom __future__ import annotations\n\nimport copy\nimport json\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Any\n\nimport torch\nfrom transformers import PreTrainedTokenizerFast\n\ntry:\n    import xgrammar as xgr\n    from xgrammar.base import _core as xgr_core\nexcept ImportError:\n    pass\n\nfrom vllm.model_executor.guided_decoding.utils import (convert_lark_to_gbnf,\n                                                       grammar_is_likely_lark)\nfrom vllm.transformers_utils.tokenizers.mistral import MistralTokenizer\n\nif TYPE_CHECKING:\n    from transformers import PreTrainedTokenizer\n\n    from vllm.config import ModelConfig\n    from vllm.sampling_params import GuidedDecodingParams\n\n\n# TODO: passing batch size to max threads here\ndef get_local_xgrammar_guided_decoding_logits_processor(\n        guided_params: GuidedDecodingParams,\n        tokenizer: PreTrainedTokenizer,\n        model_config: ModelConfig,\n        max_threads: int = 8):\n    config = GrammarConfig.from_guided_params(guided_params=guided_params,\n                                              model_config=model_config,\n                                              tokenizer=tokenizer,\n                                              max_threads=max_threads)\n    return XGrammarLogitsProcessor(config)\n\n\n@dataclass(frozen=True)\nclass TokenizerData:\n    \"\"\"Immutable container for cached tokenizer data.\"\"\"\n    encoded_vocab: list[str] = field(default_factory=list)\n    stop_token_ids: list[int] | None = None\n    # These fields are mutually exclusive: `backend_str` is used to create a\n    # TokenizeInfo with `TokenizerInfo.from_huggingface` while `vocab_type` is\n    # used within the constructor of TokenizeInfo\n    backend_str: str | None = None\n    vocab_type: xgr.VocabType | None = None\n\n    def __post_init__(self):\n        # Check for mutual exclusive\n        assert not (self.backend_str and self.vocab_type), \\\n            \"backend_str and vocab_type are mutual exclusive\"\n\n\nclass TokenizerDataCache:\n    \"\"\"Cache manager for tokenizer data to avoid repeated processing.\"\"\"\n    _cache: dict[int, TokenizerData] = {}\n\n    @classmethod\n    def get_tokenizer_data(cls,\n                           tokenizer: PreTrainedTokenizer) -> TokenizerData:\n        tokenizer_hash = hash(tokenizer)\n\n        if tokenizer_hash not in cls._cache:\n            # Vendored from xgrammar logic since we cannot pickle the tokenizer\n            # https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98 # noqa: E501\n            try:\n                encoded_vocab = [\n                    token for token, _ in sorted(tokenizer.get_vocab().items(),\n                                                 key=lambda x: x[1])\n                ]\n            except AttributeError as e:\n                raise ValueError(\n                    f\"Cannot get the vocabulary of the tokenizer \"\n                    f\"{type(tokenizer)}. The tokenizer should have a \"\n                    \"get_vocab method.\") from e\n\n            stop_token_ids = None\n            backend_str = \"\"\n            vocab_type = xgr.VocabType.RAW\n\n            if stop_token_ids is None and hasattr(\n                    tokenizer,\n                    \"eos_token_id\") and tokenizer.eos_token_id is not None:\n                stop_token_ids = [tokenizer.eos_token_id]\n\n            if isinstance(tokenizer, PreTrainedTokenizerFast):\n                backend_str = tokenizer.backend_tokenizer.to_str()\n                vocab_type = None\n\n            elif isinstance(tokenizer, MistralTokenizer):\n                # REF: https://github.com/mlc-ai/xgrammar/blob/5e141f6ff1ca02bc31f9e512e68b61f2a8ae88e5/tests/python/test_tokenizer_info.py#L43 # noqa: E501\n                vocab_type = xgr.VocabType.BYTE_FALLBACK\n\n            cls._cache[tokenizer_hash] = TokenizerData(\n                encoded_vocab=encoded_vocab,\n                stop_token_ids=stop_token_ids,\n                backend_str=backend_str,\n                vocab_type=vocab_type)\n\n        return cls._cache[tokenizer_hash]\n\n\nclass GrammarCompilerCache:\n    \"\"\"\n    Cache for GrammarCompiler instances based on tokenizer.\n\n    This cache reduces the overhead of creating new compiler instances when\n    using the same tokenizer configuration.\n    \"\"\"\n    _cache: dict[str, xgr.GrammarCompiler] = {}\n\n    @classmethod\n    def get_compiler(cls, config: GrammarConfig) -> xgr.GrammarCompiler:\n        cache_key = str(config.tokenizer_hash)\n\n        if cache_key not in cls._cache:\n            assert config.tokenizer_data is not None\n            assert config.tokenizer_data.encoded_vocab is not None\n\n            config_data = config.tokenizer_data\n\n            # In TokenizerDataCache.get_tokenizer_data, a serializable\n            # tokenizer_data is created and cached. This data is used to build\n            # a tokenizer_info and create an xgrammar compiler.\n            # - If tokenizer_data has backend_str set, use\n            # xgr_core.TokenizerInfo.from_huggingface (a C++ bind).\n            # - Otherwise, use the default constructor with vocab_type.\n            # - xgr_core.TokenizerInfo.from_huggingface !=\n            #   xgr.TokenizerInfo.from_huggingface.\n            if config_data.backend_str:\n                tokenizer_info = xgr.TokenizerInfo._create_from_handle(\n                    xgr_core.TokenizerInfo.from_huggingface(\n                        config_data.encoded_vocab, config_data.backend_str,\n                        config.vocab_size, config_data.stop_token_ids))\n            else:\n                tokenizer_info = xgr.TokenizerInfo(\n                    config_data.encoded_vocab,\n                    config_data.vocab_type,\n                    vocab_size=config.vocab_size,\n                    stop_token_ids=config_data.stop_token_ids)\n            cls._cache[cache_key] = xgr.GrammarCompiler(\n                tokenizer_info, max_threads=config.max_threads)\n\n        return cls._cache[cache_key]\n\n\n@dataclass\nclass GrammarConfig:\n    \"\"\"Serializable configuration for grammar compilation\"\"\"\n    tokenizer_hash: int\n    vocab_size: int\n    json_str: str | None = None\n    grammar_str: str | None = None\n    json_object: bool | None = None\n    max_threads: int = 8\n    tokenizer_data: TokenizerData | None = None\n\n    @classmethod\n    def from_guided_params(cls,\n                           guided_params: GuidedDecodingParams,\n                           model_config: ModelConfig,\n                           tokenizer: PreTrainedTokenizer,\n                           max_threads: int = 8) -> GrammarConfig:\n\n        tokenizer_hash = hash(tokenizer)\n        tokenizer_data = TokenizerDataCache.get_tokenizer_data(tokenizer)\n\n        if guided_params.json:\n            if not isinstance(guided_params.json, str):\n                json_str = json.dumps(guided_params.json)\n            else:\n                json_str = guided_params.json\n\n            # Validate the schema and raise ValueError here if it is invalid.\n            # This is to avoid exceptions in model execution, which will crash\n            # the engine worker process.\n            try:\n                xgr.Grammar.from_json_schema(json_str)\n            except RuntimeError as err:\n                raise ValueError(str(err)) from err\n\n            return cls(json_str=json_str,\n                       vocab_size=model_config.hf_text_config.vocab_size,\n                       tokenizer_hash=tokenizer_hash,\n                       max_threads=max_threads,\n                       tokenizer_data=tokenizer_data)\n        elif guided_params.grammar:\n            # XGrammar only supports GBNF grammars, so we must convert Lark\n            if grammar_is_likely_lark(guided_params.grammar):\n                try:\n                    grammar_str = convert_lark_to_gbnf(guided_params.grammar)\n                except ValueError as e:\n                    raise ValueError(\n                        \"Failed to convert the grammar from Lark to GBNF. \"\n                        \"Please either use GBNF grammar directly or specify\"\n                        \" --guided-decoding-backend=outlines.\\n\"\n                        f\"Conversion error: {str(e)}\") from e\n            else:\n                grammar_str = guided_params.grammar\n\n            # Validate the grammar and raise ValueError here if it is invalid.\n            # This is to avoid exceptions in model execution, which will crash\n            # the engine worker process.\n            try:\n                xgr.Grammar.from_ebnf(grammar_str)\n            except RuntimeError as err:\n                raise ValueError(str(err)) from err\n\n            return cls(grammar_str=grammar_str,\n                       vocab_size=model_config.hf_text_config.vocab_size,\n                       tokenizer_hash=tokenizer_hash,\n                       max_threads=max_threads,\n                       tokenizer_data=tokenizer_data)\n        elif guided_params.json_object:\n            return cls(\n                json_object=True,\n                vocab_size=model_config.hf_text_config.vocab_size,\n                tokenizer_hash=tokenizer_hash,\n                max_threads=max_threads,\n                tokenizer_data=tokenizer_data,\n            )\n        else:\n            raise ValueError(\n                \"Currently only support JSON and EBNF grammar mode for xgrammar\"\n            )\n\n\n@dataclass\nclass XGrammarLogitsProcessor:\n    \"\"\"Wrapper class to support pickle protocol\"\"\"\n    config: GrammarConfig\n\n    ctx: xgr.CompiledGrammar | None = None\n    token_bitmask: torch.Tensor = None  # type: ignore[assignment]\n    matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n    batch_size: int = field(default=1)\n    prefilled: bool = field(default=False)\n\n    def __getstate__(self) -> dict[str, Any]:\n        return {'config': self.config}\n\n    def __setstate__(self, state: dict[str, Any]):\n        self.config = state['config']\n\n        self.ctx = None\n        self.matchers = []\n        self.batch_size = 1\n        self.token_bitmask = None  # type: ignore[assignment]\n        self.prefilled = False\n\n    def _ensure_ctx(self):\n        \"\"\"Lazily initialize the processor in the worker process\"\"\"\n        if self.ctx is None:\n            compiler = GrammarCompilerCache.get_compiler(self.config)\n            if self.config.json_str is not None:\n                self.ctx = compiler.compile_json_schema(self.config.json_str)\n            elif self.config.grammar_str is not None:\n                self.ctx = compiler.compile_grammar(self.config.grammar_str)\n            elif self.config.json_object:\n                self.ctx = compiler.compile_builtin_json_grammar()\n            else:\n                raise ValueError(\n                    \"Invalid configuration for xgrammar logits processor\")\n\n    def __call__(self, input_ids: list[int],\n                 scores: torch.Tensor) -> torch.Tensor:\n        if self.ctx is None:\n            self._ensure_ctx()\n\n        if len(self.matchers) == 0:\n            self.matchers = [\n                xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n            ]\n            self.token_bitmask = xgr.allocate_token_bitmask(\n                self.batch_size, self.config.vocab_size)\n\n        if not self.prefilled:\n            # Have not sampled a token yet\n            self.prefilled = True\n        else:\n            for i, matcher in enumerate(self.matchers):\n                if not matcher.is_terminated():\n                    sampled_token = input_ids[-1]\n                    assert self.matchers[i].accept_token(sampled_token)\n\n        for i, matcher in enumerate(self.matchers):\n            if not matcher.is_terminated():\n                # @ubospica: ideally, fill_next_token_bitmask should be\n                # parallelized with model decoding\n                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n\n        # token_bitmask is a CPU tensor for use with accept_token and\n        # fill_next_token_bitmask so we move it to the device of scores\n        device_type = scores.device.type\n        dtype = scores.dtype\n        if device_type != \"cuda\":\n            # xgrammar on cpu only supports float32 scores\n            # see: https://github.com/mlc-ai/xgrammar/blob/c1b64920cad24f44f235778c1c00bb52d57da01a/python/xgrammar/kernels/apply_token_bitmask_inplace_cpu.py#L22\n            scores = scores.to(\"cpu\").float().unsqueeze(0)\n\n        # Note: In this method, if the tensors have different dimensions\n        # on CPU device fails, but on GPU it runs without error. Hence the\n        # unsqueeze above for scores, to match the token bitmask shape\n        xgr.apply_token_bitmask_inplace(scores,\n                                        self.token_bitmask.to(scores.device))\n        if device_type != \"cuda\":\n            scores = scores.to(dtype).to(device_type).squeeze()\n\n        return scores\n\n    def clone(self) -> XGrammarLogitsProcessor:\n        \"\"\"Deepcopy due to per-sequence state in the matchers\"\"\"\n        return copy.deepcopy(self)\n",
      "diff": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nindex 2d8594cb8..ee30ce96f 100644\n--- a/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -307,8 +307,8 @@ class XGrammarLogitsProcessor:\n         # Note: In this method, if the tensors have different dimensions\n         # on CPU device fails, but on GPU it runs without error. Hence the\n         # unsqueeze above for scores, to match the token bitmask shape\n-        xgr.apply_token_bitmask_inplace(scores,\n-                                        self.token_bitmask.to(scores.device))\n+        xgr.apply_token_bitmask_inplace(\n+            scores, self.token_bitmask.to(scores.device, non_blocking=True))\n         if device_type != \"cuda\":\n             scores = scores.to(dtype).to(device_type).squeeze()",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    }
  ],
  "affected_apis": [
    "XGrammarLogitsProcessor.__call__",
    "xgr.apply_token_bitmask_inplace",
    "torch.Tensor.to"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "NO",
    "is_benchmark_actually_there": "",
    "sample_clues": "none, xgrammar_decoding"
  }
}