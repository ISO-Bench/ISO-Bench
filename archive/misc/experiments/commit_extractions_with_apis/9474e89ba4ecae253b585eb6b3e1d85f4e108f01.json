{
  "commit_hash": "9474e89ba4ecae253b585eb6b3e1d85f4e108f01",
  "parent_hash": "20478c4d3abcd0aa8a1d9ace9c76ea3a2e04cb5e",
  "message": "[PREFIX CACHING FOLLOW UP] A bunch of fixes to block allocator performance when automatic prefix caching is disabled (#3357)\n\nCo-authored-by: Zhuohan Li <zhuohan123@gmail.com>",
  "author": "ElizaWszola <eliza@neuralmagic.com>",
  "date": "2024-03-20 00:11:11 -0700",
  "files_changed": [
    {
      "file_path": "tests/core/test_block_manager.py",
      "old_content": "import pytest\nimport time\nfrom typing import List\n\nfrom vllm import SamplingParams\nfrom vllm.block import PhysicalTokenBlock\nfrom vllm.core.block_manager import (BlockAllocator, BlockSpaceManager,\n                                     AllocStatus)\nfrom vllm.utils import Device\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus, Logprob\n\nfrom .utils import create_dummy_prompt\n\n\ndef test_block_allocator_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    num_free = num_cpu_blocks\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        num_free -= 1\n\n        assert block.block_hash not in cpu_allocator.evictor\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n    with pytest.raises(ValueError):\n        cpu_allocator.allocate()\n\n\ndef test_block_allocator_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    blocks: List[PhysicalTokenBlock] = []\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        blocks.append(block)\n        assert block.block_hash not in cpu_allocator.evictor\n\n    # Free all allocated cpu blocks.\n    num_free = 0\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for block in blocks:\n        cpu_allocator.free(block)\n        num_free += 1\n        assert block.block_hash in cpu_allocator.evictor\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n        with pytest.raises(ValueError):\n            cpu_allocator.free(block)\n\n\ndef test_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    # Allocate same sequence group to all available gpu blocks.\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group)\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n    # Allocate same sequence group to all available gpu blocks.\n    # Use watermark to reserve one gpu block.\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=1 / num_gpu_blocks)\n    for i in range(num_gpu_blocks - 1):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group)\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n\ndef test_append_slot_single_seq():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    # Allocate single seq to gpu block.\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Nothing to append. Sequence has no new logical blocks.\n    assert block_manager.can_append_slot(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slot(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks == after_blocks\n\n    # Add block_size number of new tokens and append slot.\n    for i in range(block_size):\n        token_id = i + 5\n        prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    assert block_manager.can_append_slot(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slot(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_append_slot_cow():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size=block_size,\n                                      num_cpu_blocks=num_cpu_blocks,\n                                      num_gpu_blocks=num_gpu_blocks,\n                                      watermark=0)\n\n    # Allocate prompt to gpu block. There is one slot left in the block.\n    prompt = Sequence(seq_id=1,\n                      prompt=\"one two three\",\n                      prompt_token_ids=[1, 2, 3],\n                      block_size=block_size)\n\n    # Fork the sequence, such that a COW will be required when we append a new\n    # token id.\n    child = prompt.fork(new_seq_id=2)\n\n    # Allocate space for the sequence group.\n    seq_group = SequenceGroup(\"1\", [prompt, child], SamplingParams(),\n                              time.time(), time.perf_counter)\n    block_manager.allocate(seq_group)\n\n    # Fork and append a new token id. We expect a COW to be scheduled.\n    token_id = 4\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.fork(prompt, child)\n\n    assert block_manager.can_append_slot(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n\n    maybe_src_dst_block = block_manager.append_slot(child)\n    assert maybe_src_dst_block is not None\n    src_block, dst_block = maybe_src_dst_block\n    assert src_block != dst_block\n\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_fork():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\",\n                                            block_size - 1,\n                                            block_size=block_size)\n    block_manager.allocate(seq_group)\n\n    # Fork prompt and copy block tables.\n    child = prompt.fork(2)\n    block_manager.fork(prompt, child)\n    assert block_manager.get_block_table(\n        prompt) == block_manager.get_block_table(child)\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slot(child)\n    assert block_manager.get_block_table(\n        prompt) != block_manager.get_block_table(child)\n\n\ndef test_swap():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n    prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    prompt.status = SequenceStatus.RUNNING\n    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap seq group from GPU -> CPU.\n    gpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    assert list(mapping.keys()) == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    prompt.status = SequenceStatus.SWAPPED\n\n    # Swap seq group from CPU -> GPU.\n    cpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_in(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    assert list(mapping.keys()) == cpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\ndef test_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Free allocated seq.\n    prompt_blocks = len(block_manager.get_block_table(prompt))\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    block_manager.free(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert after_blocks == before_blocks + prompt_blocks\n\n    # Block table for freed seq is deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(prompt)\n\n\ndef test_reset():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      watermark=0)\n\n    # Allocate same seq group on all available gpu blocks.\n    original_blocks = block_manager.get_num_free_gpu_blocks()\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        block_manager.allocate(seq_group)\n    assert block_manager.get_num_free_gpu_blocks() == 0\n\n    # Resetting block manager frees all allocated blocks.\n    block_manager.reset()\n    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n\n\ndef test_sliding_window_multi_seq():\n    \"\"\"\n    Tests that memory allocation and deallocation is handled\n    correctly with multiple sequences that exceed the sliding\n    window's capacity.\n    \"\"\"\n    block_size = 1\n    num_cpu_blocks = 8\n    num_gpu_blocks = 8\n    sliding_window = 2\n    block_manager = BlockSpaceManager(block_size,\n                                      num_cpu_blocks,\n                                      num_gpu_blocks,\n                                      sliding_window=sliding_window,\n                                      watermark=0)\n\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n\n    parent = Sequence(1, \"one two three\", [0, 1, 2], block_size)\n    seq_group = SequenceGroup(\"1\", [parent], SamplingParams(), time.time(),\n                              None)\n    block_manager.allocate(seq_group)\n\n    # assert the number of blocks allocated is correct\n    # the parent seq has len 3, but since sliding_window is 2,\n    # we will use at most 2 blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # Fork prompt and copy block tables.\n    child = parent.fork(2)\n    block_manager.fork(parent, child)\n\n    # assert the number of blocks allocated is correct\n    # forking does not increase memory consumption\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # assert both parent and child share all blocks\n    assert block_manager.get_block_table(\n        parent) == block_manager.get_block_table(child)\n\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slot(child)\n\n    # assert the number of blocks allocated is correct\n    # we will use now one block more. Each seq will use 2 blocks,\n    # but only one can be shared\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    token_id = 5\n    parent.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slot(parent)\n\n    # assert the number of blocks allocated is correct\n    # no change, because both sequences are still just sharing one block\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    block_table_parent = block_manager.get_block_table(parent)\n    block_table_child = block_manager.get_block_table(child)\n\n    assert block_table_parent != block_table_child\n\n    # assert both blocks are sharing the second-last block\n    assert block_table_parent[-2] == block_table_child[-2]\n\n    # now let's clean up...\n    block_manager.free(parent)\n\n    # assert the number of blocks allocated is correct\n    # We have freed one seq, reducing the ref count of two blocks by one.\n    # One of the two was only used by the parent seq, so this is now free.\n    # The child seq still consumes sliding_window blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # free all blocks\n    block_manager.free(child)\n\n    # assert all blocks are free now\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n",
      "diff": "diff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex 44ac05a14..9473a33f0 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -4,7 +4,7 @@ from typing import List\n \n from vllm import SamplingParams\n from vllm.block import PhysicalTokenBlock\n-from vllm.core.block_manager import (BlockAllocator, BlockSpaceManager,\n+from vllm.core.block_manager import (UncachedBlockAllocator, BlockSpaceManager,\n                                      AllocStatus)\n from vllm.utils import Device\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus, Logprob\n@@ -15,7 +15,8 @@ from .utils import create_dummy_prompt\n def test_block_allocator_allocate():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     num_free = num_cpu_blocks\n@@ -24,7 +25,7 @@ def test_block_allocator_allocate():\n         block = cpu_allocator.allocate()\n         num_free -= 1\n \n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n     with pytest.raises(ValueError):\n@@ -34,14 +35,15 @@ def test_block_allocator_allocate():\n def test_block_allocator_free():\n     block_size = 4\n     num_cpu_blocks = 4\n-    cpu_allocator = BlockAllocator(Device.CPU, block_size, num_cpu_blocks)\n+    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n+                                           num_cpu_blocks)\n \n     # Allocate all available cpu blocks.\n     blocks: List[PhysicalTokenBlock] = []\n     for _ in range(num_cpu_blocks):\n         block = cpu_allocator.allocate()\n         blocks.append(block)\n-        assert block.block_hash not in cpu_allocator.evictor\n+        assert block not in cpu_allocator.free_blocks\n \n     # Free all allocated cpu blocks.\n     num_free = 0\n@@ -49,7 +51,7 @@ def test_block_allocator_free():\n     for block in blocks:\n         cpu_allocator.free(block)\n         num_free += 1\n-        assert block.block_hash in cpu_allocator.evictor\n+        assert block in cpu_allocator.free_blocks\n         assert cpu_allocator.get_num_free_blocks() == num_free\n \n         with pytest.raises(ValueError):",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 7
    },
    {
      "file_path": "tests/prefix_caching/test_prefix_caching.py",
      "old_content": "\"\"\"Compare the with and without prefix caching.\n\nRun `pytest tests/prefix_caching/test_prefix_caching.py`.\n\"\"\"\nimport pytest\n\nfrom vllm.core.block_manager import BlockAllocator\nfrom vllm.utils import Device\n\n\n@pytest.mark.parametrize(\"block_size\", [16])\n@pytest.mark.parametrize(\"num_blocks\", [16])\ndef test_block_allocator(\n    block_size: int,\n    num_blocks: int,\n):\n    block_hash = 1\n    block_allocator = BlockAllocator(Device.CPU,\n                                     block_size,\n                                     num_blocks,\n                                     enable_caching=True)\n\n    # Allocate two PysicalTokenBlocks with the same hash and check\n    # that they are the same PhysicalTokenBlock\n    first_block = block_allocator.allocate(block_hash, 0)\n    second_block = block_allocator.allocate(block_hash, 0)\n    assert (first_block == second_block)\n    assert (second_block.ref_count == 2)\n\n    # Free the first_block and confirm that the ref_count is correctly\n    # decremented on the second block\n    block_allocator.free(first_block)\n    assert (second_block.ref_count == 1)\n\n    # Free the second block\n    block_allocator.free(second_block)\n\n    # Reallocate the first block and confirm that, even after the block\n    # had its ref_count go to 0, we still get the same block back\n    first_block = block_allocator.allocate(block_hash, 0)\n    assert (first_block == second_block)\n    assert (first_block.block_hash == block_hash)\n\n\n@pytest.mark.parametrize(\"num_blocks\", [16])\ndef test_eviction(num_blocks: int, ):\n    block_size = 16\n    block_allocator = BlockAllocator(Device.CPU,\n                                     block_size,\n                                     num_blocks,\n                                     enable_caching=True)\n    blocks = []\n\n    for i in range(num_blocks):\n        # use i as the block_hash\n        blocks.append(block_allocator.allocate(i, 0))\n\n    #Free all blocks\n    for block in blocks:\n        block_allocator.free(block)\n\n    # Allocate a new block and confirm that it's the first block freed.\n    # I.E The Least Recently Used block\n    new_block_hash = block_size\n    new_block = block_allocator.allocate(new_block_hash, 0)\n    assert (new_block == blocks[0])\n    assert (new_block.block_hash == new_block_hash)\n\n    # Reallocate the second in blocks to remove it from the free list\n    realloc_block_hash = 1\n    realloc_block = block_allocator.allocate(realloc_block_hash, 0)\n    assert (realloc_block == blocks[realloc_block_hash])\n    assert (realloc_block.block_hash == realloc_block_hash)\n\n    # Allocate a new block and confirm that it's not the realloc_block,\n    # since the realloc_block shouldn't be in the free list\n    new_block_hash = block_size + 1\n    new_block = block_allocator.allocate(new_block_hash, 0)\n    assert (realloc_block != new_block)\n    assert (new_block.block_hash == new_block_hash)\n    assert (new_block.block_number == 2)\n",
      "diff": "diff --git a/tests/prefix_caching/test_prefix_caching.py b/tests/prefix_caching/test_prefix_caching.py\nindex c83551c36..cb61aac39 100644\n--- a/tests/prefix_caching/test_prefix_caching.py\n+++ b/tests/prefix_caching/test_prefix_caching.py\n@@ -4,7 +4,7 @@ Run `pytest tests/prefix_caching/test_prefix_caching.py`.\n \"\"\"\n import pytest\n \n-from vllm.core.block_manager import BlockAllocator\n+from vllm.core.block_manager import CachedBlockAllocator\n from vllm.utils import Device\n \n \n@@ -15,10 +15,7 @@ def test_block_allocator(\n     num_blocks: int,\n ):\n     block_hash = 1\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n \n     # Allocate two PysicalTokenBlocks with the same hash and check\n     # that they are the same PhysicalTokenBlock\n@@ -45,10 +42,7 @@ def test_block_allocator(\n @pytest.mark.parametrize(\"num_blocks\", [16])\n def test_eviction(num_blocks: int, ):\n     block_size = 16\n-    block_allocator = BlockAllocator(Device.CPU,\n-                                     block_size,\n-                                     num_blocks,\n-                                     enable_caching=True)\n+    block_allocator = CachedBlockAllocator(Device.CPU, block_size, num_blocks)\n     blocks = []\n \n     for i in range(num_blocks):",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 10
    },
    {
      "file_path": "vllm/core/block_manager.py",
      "old_content": "\"\"\"A block manager that manages token blocks.\"\"\"\nimport enum\nfrom itertools import count, takewhile\nfrom os.path import commonprefix\nfrom typing import Dict, List, Optional, Set, Tuple\n\nfrom vllm.block import BlockTable, PhysicalTokenBlock\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\nfrom vllm.core.evictor import Evictor, EvictionPolicy, make_evictor\n\n\nclass BlockAllocator:\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n                 enable_caching: bool = False) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n        self.enable_caching = enable_caching\n\n        self.current_num_blocks = 0\n        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n\n        # Switch over to FIFO eviction when caching is disabled\n        if not self.enable_caching:\n            eviction_policy = EvictionPolicy.FIFO\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        self.default_hash_ctr = count()\n\n    def allocate_block(self, block_hash: int,\n                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n        if self.current_num_blocks == self.num_blocks:\n            block = self.evictor.evict()\n            block.block_hash = block_hash\n            block.num_hashed_tokens = num_hashed_tokens\n            return block\n        block = PhysicalTokenBlock(device=self.device,\n                                   block_number=self.current_num_blocks,\n                                   block_size=self.block_size,\n                                   block_hash=block_hash,\n                                   num_hashed_tokens=num_hashed_tokens)\n        self.current_num_blocks += 1\n        return block\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        # If caching is disabled, just allocate a new block and return it\n        if not self.enable_caching:\n            block = self.allocate_block(next(self.default_hash_ctr),\n                                        num_hashed_tokens)\n            block.ref_count += 1\n            return block\n\n        if block_hash is None:\n            block_hash = next(self.default_hash_ctr)\n        if block_hash in self.evictor:\n            assert block_hash not in self.cached_blocks\n            block = self.evictor.remove(block_hash)\n            assert block.ref_count == 0\n            self.cached_blocks[block_hash] = block\n            block.ref_count += 1\n            assert block.block_hash == block_hash\n            return block\n        if block_hash not in self.cached_blocks:\n            self.cached_blocks[block_hash] = self.allocate_block(\n                block_hash, num_hashed_tokens)\n        block = self.cached_blocks[block_hash]\n        assert block.block_hash == block_hash\n        block.ref_count += 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            assert block.block_hash not in self.evictor\n            self.evictor.add(block)\n\n            # If caching is enabled, remove the block from the cached_blocks\n            if self.enable_caching:\n                del self.cached_blocks[block.block_hash]\n\n    def get_num_free_blocks(self) -> int:\n        return (self.num_blocks - self.current_num_blocks +\n                self.evictor.num_blocks)\n\n    def contains_block(self, block_hash: int) -> bool:\n        return block_hash in self.cached_blocks or block_hash in self.evictor\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        # If caching is enabled, update the hash of block and the\n        # cached_blocks dictionary.\n        if self.enable_caching:\n            assert not self.contains_block(block_hash)\n            old_hash = block.block_hash\n            block.block_hash = block_hash\n            del self.cached_blocks[old_hash]\n            self.cached_blocks[block_hash] = block\n\n\nclass AllocStatus(enum.Enum):\n    \"\"\"Result for BlockSpaceManager.can_allocate\n\n    1. Ok: seq_group can be allocated now.\n    2. Later: seq_group cannot be allocated.\n      The capacity of allocator is larger than seq_group required.\n    3. Never: seq_group can never be allocated.\n      The seq_group is too large to allocated in GPU.\n    \"\"\"\n    OK = enum.auto()\n    LATER = enum.auto()\n    NEVER = enum.auto()\n\n\nclass BlockSpaceManager:\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        self.block_sliding_window = None\n        if sliding_window is not None:\n            assert sliding_window % block_size == 0, (sliding_window,\n                                                      block_size)\n            self.block_sliding_window = sliding_window // block_size\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n        self.gpu_allocator = BlockAllocator(Device.GPU,\n                                            block_size,\n                                            num_gpu_blocks,\n                                            enable_caching=enable_caching)\n        self.cpu_allocator = BlockAllocator(Device.CPU,\n                                            block_size,\n                                            num_cpu_blocks,\n                                            enable_caching=enable_caching)\n        # Mapping: seq_id -> BlockTable.\n        self.block_tables: Dict[int, BlockTable] = {}\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n        num_required_blocks = len(seq.logical_token_blocks)\n\n        if self.block_sliding_window is not None:\n            num_required_blocks = min(num_required_blocks,\n                                      self.block_sliding_window)\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        # NOTE: Here we assume that all sequences in the group have the same\n        # prompt.\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n\n        # Allocate new physical token blocks that will store the prompt tokens.\n        num_prompt_blocks = len(seq.logical_token_blocks)\n\n        block_table: BlockTable = []\n        for logical_idx in range(num_prompt_blocks):\n            if (self.block_sliding_window is not None\n                    and logical_idx >= self.block_sliding_window):\n                block = block_table[logical_idx % self.block_sliding_window]\n            else:\n                block = self.gpu_allocator.allocate(\n                    seq.hash_of_block(logical_idx),\n                    seq.num_hashed_tokens_of_block(logical_idx))\n            block_table.append(block)\n\n        # Assign the block table for each sequence.\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            self.block_tables[seq.seq_id] = block_table.copy()\n\n    def can_append_slot(self, seq_group: SequenceGroup) -> bool:\n        # Simple heuristic: If there is at least one free block\n        # for each sequence, we can append.\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n        return num_seqs <= num_free_gpu_blocks\n\n    def _promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        # Compute a new hash for the block so that it can be shared by\n        # other Sequences\n        new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n\n        # if new_hash is already in the cached table, then free last_block\n        # and return the cached version\n        if self.gpu_allocator.contains_block(new_hash):\n            self.gpu_allocator.free(last_block)\n            return self.gpu_allocator.allocate(new_hash)\n        else:\n            self.gpu_allocator.update_hash(new_hash, last_block)\n            return last_block\n\n    def _is_last_block_full(\n        self,\n        seq: Sequence,\n    ) -> bool:\n        token_ids_len = len(seq.data.get_token_ids())\n        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n\n    def _maybe_promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        if self._is_last_block_full(seq):\n            return self._promote_last_block(seq, last_block)\n        else:\n            return last_block\n\n    def _allocate_last_physical_block(\n        self,\n        seq: Sequence,\n    ) -> PhysicalTokenBlock:\n        block_hash: Optional[int] = None\n        if (self._is_last_block_full(seq)):\n            block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n        num_hashed_tokens = seq.num_hashed_tokens_of_block(\n            len(seq.logical_token_blocks) - 1)\n        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n        if block_hash is None:\n            assert new_block.ref_count == 1\n        return new_block\n\n    def append_slot(\n        self,\n        seq: Sequence,\n    ) -> Optional[Tuple[int, int]]:\n        \"\"\"Allocate a physical slot for a new token.\"\"\"\n        logical_blocks = seq.logical_token_blocks\n        block_table = self.block_tables[seq.seq_id]\n        # If we need to allocate a new physical block\n        if len(block_table) < len(logical_blocks):\n            # Currently this code only supports adding one physical block\n            assert len(block_table) == len(logical_blocks) - 1\n\n            if (self.block_sliding_window\n                    and len(block_table) >= self.block_sliding_window):\n                # reuse a block\n                block_table.append(block_table[len(block_table) %\n                                               self.block_sliding_window])\n            else:\n                # The sequence has a new logical block.\n                # Allocate a new physical block.\n                new_block = self._allocate_last_physical_block(seq)\n                block_table.append(new_block)\n                return None\n\n        # We want to append the token to the last physical block.\n        last_block = block_table[-1]\n        assert last_block.device == Device.GPU\n        if last_block.ref_count == 1:\n            # Not shared with other sequences. Appendable.\n            # If the last block is now complete, promote it to a full block so\n            # that it can be shared\n            new_block = self._maybe_promote_last_block(seq, last_block)\n            block_table[-1] = new_block\n            return None\n        else:\n            # The last block is shared with other sequences.\n            # Copy on Write: Allocate a new block and copy the tokens.\n            new_block = self._allocate_last_physical_block(seq)\n\n            block_table[-1] = new_block\n            self.gpu_allocator.free(last_block)\n            return last_block.block_number, new_block.block_number\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        # NOTE: fork does not allocate a new physical block.\n        # Thus, it is always safe from OOM.\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n        # When using a sliding window, blocks will be eventually reused.\n        # In this case the block tables will contain repeated blocks.\n        # When forking, we must make sure that each block's `ref_count`\n        # is only incremented by one, so we deduplicate them by wrapping\n        # them in a set.\n        for block in set(src_block_table):\n            block.ref_count += 1\n\n    def _get_physical_blocks(\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n        # NOTE: Here, we assume that the physical blocks are only shared by\n        # the sequences in the same group.\n        blocks: Set[PhysicalTokenBlock] = set()\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                continue\n            blocks.update(self.block_tables[seq.seq_id])\n        return list(blocks)\n\n    def can_swap_in(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n        # NOTE: Conservatively, we assume that every sequence will allocate\n        # at least one free block right after the swap-in.\n        # NOTE: This should match the logic in can_append_slot().\n        num_required_blocks = len(blocks) + num_swapped_seqs\n        return num_free_blocks - num_required_blocks >= self.watermark_blocks\n\n    def swap_in(self, seq_group: SequenceGroup) -> Dict[int, int]:\n        # CPU block -> GPU block.\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            new_block_table: BlockTable = []\n            block_table = self.block_tables[seq.seq_id]\n\n            for cpu_block in block_table:\n                if cpu_block in mapping:\n                    gpu_block = mapping[cpu_block]\n                    gpu_block.ref_count += 1\n                else:\n                    gpu_block = self.gpu_allocator.allocate(\n                        cpu_block.block_hash, cpu_block.num_hashed_tokens)\n                    mapping[cpu_block] = gpu_block\n                new_block_table.append(gpu_block)\n                # Free the CPU block swapped in to GPU.\n                self.cpu_allocator.free(cpu_block)\n            self.block_tables[seq.seq_id] = new_block_table\n\n        block_number_mapping = {\n            cpu_block.block_number: gpu_block.block_number\n            for cpu_block, gpu_block in mapping.items()\n        }\n        return block_number_mapping\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n\n    def swap_out(self, seq_group: SequenceGroup) -> Dict[int, int]:\n        # GPU block -> CPU block.\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            new_block_table: BlockTable = []\n            block_table = self.block_tables[seq.seq_id]\n\n            for gpu_block in block_table:\n                if gpu_block in mapping:\n                    cpu_block = mapping[gpu_block]\n                    cpu_block.ref_count += 1\n                else:\n                    cpu_block = self.cpu_allocator.allocate(\n                        gpu_block.block_hash, gpu_block.num_hashed_tokens)\n                    mapping[gpu_block] = cpu_block\n                new_block_table.append(cpu_block)\n                # Free the GPU block swapped out to CPU.\n                self.gpu_allocator.free(gpu_block)\n            self.block_tables[seq.seq_id] = new_block_table\n\n        block_number_mapping = {\n            gpu_block.block_number: cpu_block.block_number\n            for gpu_block, cpu_block in mapping.items()\n        }\n        return block_number_mapping\n\n    def _free_block_table(self, block_table: BlockTable) -> None:\n        # when using a sliding window, each seq will only use up\n        # to `self.block_sliding_window` blocks. When freeing\n        # the block table, we must make sure to not free blocks more\n        # than once. If no sliding window is used, there is no block\n        # reuse in the block table, so we must free all blocks.\n        blocks_to_free = (block_table[-self.block_sliding_window:]\n                          if self.block_sliding_window is not None else\n                          block_table)\n        for block in set(blocks_to_free):\n            if block.device == Device.GPU:\n                self.gpu_allocator.free(block)\n            else:\n                self.cpu_allocator.free(block)\n\n    def free(self, seq: Sequence) -> None:\n        if seq.seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n        block_table = self.block_tables[seq.seq_id]\n        self._free_block_table(block_table)\n        del self.block_tables[seq.seq_id]\n\n    def reset(self) -> None:\n        for block_table in self.block_tables.values():\n            self._free_block_table(block_table)\n        self.block_tables.clear()\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        block_table = self.block_tables[seq.seq_id]\n        return [block.block_number for block in block_table]\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.gpu_allocator.get_num_free_blocks()\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.cpu_allocator.get_num_free_blocks()\n\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        block_table = self.block_tables[seq.seq_id]\n        for block in block_table:\n            block.last_accessed = access_time\n\n    def compute_full_blocks_in_seq(self, seq: Sequence):\n        if seq.seq_id not in self.block_tables:\n            return\n        max_full_block = seq.get_len() // self.block_size - 1\n        block_table = self.block_tables[seq.seq_id]\n        if max_full_block == -1:\n            return\n        for i in reversed(range(max_full_block)):\n            if block_table[i].computed:\n                break\n            block_table[i].computed = True\n\n    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n        if seq.seq_id not in self.block_tables:\n            return []\n        block_table = self.block_tables[seq.seq_id]\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n        return [\n            b.block_number\n            for b in takewhile(lambda b: b.computed, block_table[:-1])\n        ]\n\n    def get_common_computed_block_ids(self,\n                                      seq_group: SequenceGroup) -> List[int]:\n        # Can return non-empty result only with prefix caching enabled.\n        if not self.enable_caching:\n            return []\n\n        ids_list = [\n            self.get_all_computed_blocks(seq)\n            for seq in iter(seq_group.seqs_dict.values())\n        ]\n        return commonprefix([ids for ids in ids_list if ids != []])\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        if self.enable_caching:\n            for seq in seq_group.seqs_dict.values():\n                self.compute_full_blocks_in_seq(seq)\n",
      "diff": "diff --git a/vllm/core/block_manager.py b/vllm/core/block_manager.py\nindex 8b089a565..ad9b557fd 100644\n--- a/vllm/core/block_manager.py\n+++ b/vllm/core/block_manager.py\n@@ -3,6 +3,7 @@ import enum\n from itertools import count, takewhile\n from os.path import commonprefix\n from typing import Dict, List, Optional, Set, Tuple\n+from abc import ABC, abstractmethod\n \n from vllm.block import BlockTable, PhysicalTokenBlock\n from vllm.sequence import Sequence, SequenceGroup, SequenceStatus\n@@ -10,7 +11,7 @@ from vllm.utils import Device\n from vllm.core.evictor import Evictor, EvictionPolicy, make_evictor\n \n \n-class BlockAllocator:\n+class BlockAllocatorBase(ABC):\n     \"\"\"Manages free physical token blocks for a device.\n \n     The allocator maintains a list of free blocks and allocates a block when\n@@ -18,23 +19,57 @@ class BlockAllocator:\n     the reference count becomes zero, the block is added back to the free list.\n     \"\"\"\n \n+    @abstractmethod\n     def __init__(self,\n                  device: Device,\n                  block_size: int,\n                  num_blocks: int,\n-                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU,\n-                 enable_caching: bool = False) -> None:\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n+        pass\n+\n+    @abstractmethod\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        pass\n+\n+    @abstractmethod\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        pass\n+\n+    @abstractmethod\n+    def get_num_free_blocks(self) -> int:\n+        pass\n+\n+    @abstractmethod\n+    def contains_block(self, block_hash: int) -> bool:\n+        pass\n+\n+    @abstractmethod\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        pass\n+\n+\n+class CachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 device: Device,\n+                 block_size: int,\n+                 num_blocks: int,\n+                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n         self.device = device\n         self.block_size = block_size\n         self.num_blocks = num_blocks\n-        self.enable_caching = enable_caching\n \n         self.current_num_blocks = 0\n         self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n \n-        # Switch over to FIFO eviction when caching is disabled\n-        if not self.enable_caching:\n-            eviction_policy = EvictionPolicy.FIFO\n         self.evictor: Evictor = make_evictor(eviction_policy)\n \n         self.default_hash_ctr = count()\n@@ -57,13 +92,6 @@ class BlockAllocator:\n     def allocate(self,\n                  block_hash: Optional[int] = None,\n                  num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n-        # If caching is disabled, just allocate a new block and return it\n-        if not self.enable_caching:\n-            block = self.allocate_block(next(self.default_hash_ctr),\n-                                        num_hashed_tokens)\n-            block.ref_count += 1\n-            return block\n-\n         if block_hash is None:\n             block_hash = next(self.default_hash_ctr)\n         if block_hash in self.evictor:\n@@ -90,9 +118,8 @@ class BlockAllocator:\n             assert block.block_hash not in self.evictor\n             self.evictor.add(block)\n \n-            # If caching is enabled, remove the block from the cached_blocks\n-            if self.enable_caching:\n-                del self.cached_blocks[block.block_hash]\n+            # Remove the block from the cached_blocks\n+            del self.cached_blocks[block.block_hash]\n \n     def get_num_free_blocks(self) -> int:\n         return (self.num_blocks - self.current_num_blocks +\n@@ -102,14 +129,68 @@ class BlockAllocator:\n         return block_hash in self.cached_blocks or block_hash in self.evictor\n \n     def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n-        # If caching is enabled, update the hash of block and the\n-        # cached_blocks dictionary.\n-        if self.enable_caching:\n-            assert not self.contains_block(block_hash)\n-            old_hash = block.block_hash\n-            block.block_hash = block_hash\n-            del self.cached_blocks[old_hash]\n-            self.cached_blocks[block_hash] = block\n+        # Update the hash of block and the cached_blocks dictionary.\n+        assert not self.contains_block(block_hash)\n+        old_hash = block.block_hash\n+        block.block_hash = block_hash\n+        del self.cached_blocks[old_hash]\n+        self.cached_blocks[block_hash] = block\n+\n+\n+class UncachedBlockAllocator(BlockAllocatorBase):\n+    \"\"\"Manages free physical token blocks for a device.\n+\n+    The allocator maintains a list of free blocks and allocates a block when\n+    requested. When a block is freed, its reference count is decremented. If\n+    the reference count becomes zero, the block is added back to the free list.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        device: Device,\n+        block_size: int,\n+        num_blocks: int,\n+    ) -> None:\n+        self.device = device\n+        self.block_size = block_size\n+        self.num_blocks = num_blocks\n+\n+        # Initialize the free blocks.\n+        self.free_blocks: BlockTable = []\n+        for i in range(num_blocks):\n+            block = PhysicalTokenBlock(device=device,\n+                                       block_number=i,\n+                                       block_size=block_size,\n+                                       block_hash=-1,\n+                                       num_hashed_tokens=0)\n+            self.free_blocks.append(block)\n+\n+    def allocate(self,\n+                 block_hash: Optional[int] = None,\n+                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n+        if not self.free_blocks:\n+            raise ValueError(\"Out of memory! No free blocks are available.\")\n+        block = self.free_blocks.pop()\n+        block.ref_count = 1\n+        return block\n+\n+    def free(self, block: PhysicalTokenBlock) -> None:\n+        if block.ref_count == 0:\n+            raise ValueError(f\"Double free! {block} is already freed.\")\n+        block.ref_count -= 1\n+        if block.ref_count == 0:\n+            self.free_blocks.append(block)\n+\n+    def get_num_free_blocks(self) -> int:\n+        return len(self.free_blocks)\n+\n+    def contains_block(self, block_hash: int) -> bool:\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n+\n+    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n+        raise NotImplementedError(\n+            \"Invalid codepath for uncached block allocator.\")\n \n \n class AllocStatus(enum.Enum):\n@@ -142,6 +223,10 @@ class BlockSpaceManager:\n         self.num_total_gpu_blocks = num_gpu_blocks\n         self.num_total_cpu_blocks = num_cpu_blocks\n \n+        if enable_caching and sliding_window is not None:\n+            raise NotImplementedError(\n+                \"Sliding window is not allowed with prefix caching enabled!\")\n+\n         self.block_sliding_window = None\n         if sliding_window is not None:\n             assert sliding_window % block_size == 0, (sliding_window,\n@@ -154,14 +239,17 @@ class BlockSpaceManager:\n         self.enable_caching = enable_caching\n \n         self.watermark_blocks = int(watermark * num_gpu_blocks)\n-        self.gpu_allocator = BlockAllocator(Device.GPU,\n-                                            block_size,\n-                                            num_gpu_blocks,\n-                                            enable_caching=enable_caching)\n-        self.cpu_allocator = BlockAllocator(Device.CPU,\n-                                            block_size,\n-                                            num_cpu_blocks,\n-                                            enable_caching=enable_caching)\n+\n+        if self.enable_caching:\n+            self.gpu_allocator = CachedBlockAllocator(Device.GPU, block_size,\n+                                                      num_gpu_blocks)\n+            self.cpu_allocator = CachedBlockAllocator(Device.CPU, block_size,\n+                                                      num_cpu_blocks)\n+        else:\n+            self.gpu_allocator = UncachedBlockAllocator(\n+                Device.GPU, block_size, num_gpu_blocks)\n+            self.cpu_allocator = UncachedBlockAllocator(\n+                Device.CPU, block_size, num_cpu_blocks)\n         # Mapping: seq_id -> BlockTable.\n         self.block_tables: Dict[int, BlockTable] = {}\n \n@@ -198,10 +286,16 @@ class BlockSpaceManager:\n             if (self.block_sliding_window is not None\n                     and logical_idx >= self.block_sliding_window):\n                 block = block_table[logical_idx % self.block_sliding_window]\n-            else:\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n+            elif self.enable_caching:\n                 block = self.gpu_allocator.allocate(\n                     seq.hash_of_block(logical_idx),\n                     seq.num_hashed_tokens_of_block(logical_idx))\n+            else:\n+                block = self.gpu_allocator.allocate()\n+                # Set the reference counts of the token blocks.\n+                block.ref_count = seq_group.num_seqs()\n             block_table.append(block)\n \n         # Assign the block table for each sequence.\n@@ -220,8 +314,10 @@ class BlockSpaceManager:\n         seq: Sequence,\n         last_block: PhysicalTokenBlock,\n     ) -> PhysicalTokenBlock:\n-        # Compute a new hash for the block so that it can be shared by\n-        # other Sequences\n+        assert self.enable_caching\n+\n+        # Compute a new hash for the block so that it can be shared by other\n+        # Sequences\n         new_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n \n         # if new_hash is already in the cached table, then free last_block\n@@ -254,6 +350,8 @@ class BlockSpaceManager:\n         self,\n         seq: Sequence,\n     ) -> PhysicalTokenBlock:\n+        if not self.enable_caching:\n+            return self.gpu_allocator.allocate()\n         block_hash: Optional[int] = None\n         if (self._is_last_block_full(seq)):\n             block_hash = seq.hash_of_block(len(seq.logical_token_blocks) - 1)\n@@ -293,10 +391,12 @@ class BlockSpaceManager:\n         assert last_block.device == Device.GPU\n         if last_block.ref_count == 1:\n             # Not shared with other sequences. Appendable.\n-            # If the last block is now complete, promote it to a full block so\n-            # that it can be shared\n-            new_block = self._maybe_promote_last_block(seq, last_block)\n-            block_table[-1] = new_block\n+            if self.enable_caching:\n+                # If the last block is now complete, we may reuse an old block\n+                # to save memory.\n+                maybe_new_block = self._maybe_promote_last_block(\n+                    seq, last_block)\n+                block_table[-1] = maybe_new_block\n             return None\n         else:\n             # The last block is shared with other sequences.\n@@ -440,9 +540,12 @@ class BlockSpaceManager:\n         seq: Sequence,\n         access_time: float,\n     ) -> None:\n-        block_table = self.block_tables[seq.seq_id]\n-        for block in block_table:\n-            block.last_accessed = access_time\n+        if self.enable_caching:\n+            # Update the last accessed time of all the blocks accessed\n+            # in this step.\n+            block_table = self.block_tables[seq.seq_id]\n+            for block in block_table:\n+                block.last_accessed = access_time\n \n     def compute_full_blocks_in_seq(self, seq: Sequence):\n         if seq.seq_id not in self.block_tables:",
      "change_type": "modified",
      "lines_added": 147,
      "lines_removed": 44
    },
    {
      "file_path": "vllm/core/evictor.py",
      "old_content": "import enum\nfrom typing import Dict, List, Optional\nfrom abc import ABC, abstractmethod, abstractproperty\n\nfrom vllm.block import PhysicalTokenBlock\n\n\nclass EvictionPolicy(enum.Enum):\n    \"\"\"Enum for eviction policy used by make_evictor to instantiate the correct\n       Evictor subclass.\n    \"\"\"\n    LRU = enum.auto()\n    FIFO = enum.auto()\n\n\nclass Evictor(ABC):\n    \"\"\"The Evictor subclasses should be used by the BlockAllocator class to\n    handle eviction of freed PhysicalTokenBlocks.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self):\n        pass\n\n    @abstractmethod\n    def __contains__(self, block_hash: int) -> bool:\n        pass\n\n    @abstractmethod\n    def evict(self) -> PhysicalTokenBlock:\n        \"\"\"Runs the eviction algorithm and returns the evicted block\"\"\"\n        pass\n\n    @abstractmethod\n    def add(self, block: PhysicalTokenBlock):\n        \"\"\"Adds block to the evictor, making it a candidate for eviction\"\"\"\n        pass\n\n    @abstractmethod\n    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n        \"\"\"Simply removes the block with the hash value block_hash from the\n        evictor. Caller is responsible for making sure that block_hash is\n        contained in the evictor before calling remove. Should be used to\n        \"bring back\" blocks that have been freed but not evicted yet.\n        \"\"\"\n        pass\n\n    @abstractproperty\n    def num_blocks(self) -> int:\n        pass\n\n\nclass LRUEvictor(Evictor):\n    \"\"\"Evicts in a least-recently-used order using the last_accessed timestamp\n    that's recorded in the PhysicalTokenBlock. If there are multiple blocks with\n    the same last_accessed time, then the one with the largest num_hashed_tokens\n    will be evicted. If two blocks each have the lowest last_accessed time and\n    highest num_hashed_tokens value, then one will be chose arbitrarily\n    \"\"\"\n\n    def __init__(self):\n        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n\n    def __contains__(self, block_hash: int) -> bool:\n        return block_hash in self.free_table\n\n    # TODO: The performance of this evict function can be optimized further.\n    def evict(self) -> PhysicalTokenBlock:\n        free_blocks: List[PhysicalTokenBlock] = list(self.free_table.values())\n        if len(free_blocks) == 0:\n            raise ValueError(\"No usable cache memory left\")\n\n        # Find lowest timestamp\n        lowest_timestamp = free_blocks[0].last_accessed\n        for block in free_blocks:\n            if block.last_accessed < lowest_timestamp:\n                lowest_timestamp = block.last_accessed\n\n        # Find all blocks with the lowest timestamp\n        least_recent: List[PhysicalTokenBlock] = []\n        for block in free_blocks:\n            if block.last_accessed == lowest_timestamp:\n                least_recent.append(block)\n\n        # Find highest prefix count per block\n        highest_num_hashed_tokens = 0\n        for block in least_recent:\n            if block.num_hashed_tokens > highest_num_hashed_tokens:\n                highest_num_hashed_tokens = block.num_hashed_tokens\n\n        evicted_block: Optional[PhysicalTokenBlock] = None\n\n        # Find the first block with the lowest timestamp\n        for block in least_recent:\n            if block.num_hashed_tokens == highest_num_hashed_tokens:\n                evicted_block = block\n                break\n\n        assert evicted_block is not None\n\n        del self.free_table[evicted_block.block_hash]\n\n        evicted_block.computed = False\n        return evicted_block\n\n    def add(self, block: PhysicalTokenBlock):\n        self.free_table[block.block_hash] = block\n\n    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n        if block_hash not in self.free_table:\n            raise ValueError(\n                \"Attempting to remove block that's not in the evictor\")\n        block: PhysicalTokenBlock = self.free_table[block_hash]\n        del self.free_table[block_hash]\n        return block\n\n    @property\n    def num_blocks(self) -> int:\n        return len(self.free_table)\n\n\nclass RandomEvictor(Evictor):\n    \"\"\"Evicts in a first-in-first-out order\"\"\"\n\n    def __init__(self):\n        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n\n    def __contains__(self, block_hash: int) -> bool:\n        return block_hash in self.free_table\n\n    def evict(self) -> PhysicalTokenBlock:\n        if len(self.free_table) == 0:\n            raise ValueError(\"No usable cache memory left\")\n        evicted_block = next(iter(self.free_table.values()))\n        evicted_block.computed = False\n        del self.free_table[evicted_block.block_hash]\n        return evicted_block\n\n    def add(self, block: PhysicalTokenBlock):\n        self.free_table[block.block_hash] = block\n\n    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n        if block_hash not in self.free_table:\n            raise ValueError(\n                \"Attempting to remove block that's not in the evictor\")\n        block: PhysicalTokenBlock = self.free_table[block_hash]\n        del self.free_table[block_hash]\n        return block\n\n    @property\n    def num_blocks(self) -> int:\n        return len(self.free_table)\n\n\ndef make_evictor(eviction_policy: EvictionPolicy) -> Evictor:\n    if eviction_policy == EvictionPolicy.LRU:\n        return LRUEvictor()\n    elif eviction_policy == EvictionPolicy.FIFO:\n        return RandomEvictor()\n    else:\n        raise ValueError(f\"Unknown cache eviction policy: {eviction_policy}\")\n",
      "diff": "diff --git a/vllm/core/evictor.py b/vllm/core/evictor.py\nindex 1d81f5a97..9f401cba3 100644\n--- a/vllm/core/evictor.py\n+++ b/vllm/core/evictor.py\n@@ -1,5 +1,5 @@\n import enum\n-from typing import Dict, List, Optional\n+from typing import Dict\n from abc import ABC, abstractmethod, abstractproperty\n \n from vllm.block import PhysicalTokenBlock\n@@ -10,7 +10,6 @@ class EvictionPolicy(enum.Enum):\n        Evictor subclass.\n     \"\"\"\n     LRU = enum.auto()\n-    FIFO = enum.auto()\n \n \n class Evictor(ABC):\n@@ -66,37 +65,18 @@ class LRUEvictor(Evictor):\n \n     # TODO: The performance of this evict function can be optimized further.\n     def evict(self) -> PhysicalTokenBlock:\n-        free_blocks: List[PhysicalTokenBlock] = list(self.free_table.values())\n-        if len(free_blocks) == 0:\n+        if len(self.free_table) == 0:\n             raise ValueError(\"No usable cache memory left\")\n+        free_blocks = self.free_table.values()\n \n-        # Find lowest timestamp\n-        lowest_timestamp = free_blocks[0].last_accessed\n-        for block in free_blocks:\n-            if block.last_accessed < lowest_timestamp:\n-                lowest_timestamp = block.last_accessed\n+        # Get evicted block\n+        evicted_block: PhysicalTokenBlock = next(iter(free_blocks))\n \n-        # Find all blocks with the lowest timestamp\n-        least_recent: List[PhysicalTokenBlock] = []\n         for block in free_blocks:\n-            if block.last_accessed == lowest_timestamp:\n-                least_recent.append(block)\n-\n-        # Find highest prefix count per block\n-        highest_num_hashed_tokens = 0\n-        for block in least_recent:\n-            if block.num_hashed_tokens > highest_num_hashed_tokens:\n-                highest_num_hashed_tokens = block.num_hashed_tokens\n-\n-        evicted_block: Optional[PhysicalTokenBlock] = None\n-\n-        # Find the first block with the lowest timestamp\n-        for block in least_recent:\n-            if block.num_hashed_tokens == highest_num_hashed_tokens:\n+            if (block.last_accessed < evicted_block.last_accessed\n+                    or block.last_accessed == evicted_block.last_accessed and\n+                    block.num_hashed_tokens > evicted_block.num_hashed_tokens):\n                 evicted_block = block\n-                break\n-\n-        assert evicted_block is not None\n \n         del self.free_table[evicted_block.block_hash]\n \n@@ -119,43 +99,8 @@ class LRUEvictor(Evictor):\n         return len(self.free_table)\n \n \n-class RandomEvictor(Evictor):\n-    \"\"\"Evicts in a first-in-first-out order\"\"\"\n-\n-    def __init__(self):\n-        self.free_table: Dict[int, PhysicalTokenBlock] = {}\n-\n-    def __contains__(self, block_hash: int) -> bool:\n-        return block_hash in self.free_table\n-\n-    def evict(self) -> PhysicalTokenBlock:\n-        if len(self.free_table) == 0:\n-            raise ValueError(\"No usable cache memory left\")\n-        evicted_block = next(iter(self.free_table.values()))\n-        evicted_block.computed = False\n-        del self.free_table[evicted_block.block_hash]\n-        return evicted_block\n-\n-    def add(self, block: PhysicalTokenBlock):\n-        self.free_table[block.block_hash] = block\n-\n-    def remove(self, block_hash: int) -> PhysicalTokenBlock:\n-        if block_hash not in self.free_table:\n-            raise ValueError(\n-                \"Attempting to remove block that's not in the evictor\")\n-        block: PhysicalTokenBlock = self.free_table[block_hash]\n-        del self.free_table[block_hash]\n-        return block\n-\n-    @property\n-    def num_blocks(self) -> int:\n-        return len(self.free_table)\n-\n-\n def make_evictor(eviction_policy: EvictionPolicy) -> Evictor:\n     if eviction_policy == EvictionPolicy.LRU:\n         return LRUEvictor()\n-    elif eviction_policy == EvictionPolicy.FIFO:\n-        return RandomEvictor()\n     else:\n         raise ValueError(f\"Unknown cache eviction policy: {eviction_policy}\")",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 64
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 4,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 4
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_block_manager, test_prefix_caching)",
    "is_benchmark_actually_there": "",
    "sample_clues": "block, block_manager, blockspacemanager"
  }
}