{
  "commit_hash": "2bb0489cb3367e46e201e84ab629df535544495b",
  "parent_hash": "7508a3dc34c2b7a5c5c971b13f15208d7dade442",
  "message": "[Core] Use numpy to speed up padded token processing (#6442)",
  "author": "Peng Guanwen <pg999w@outlook.com>",
  "date": "2024-07-16 08:13:25 -0700",
  "files_changed": [
    {
      "file_path": "vllm/model_executor/sampling_metadata.py",
      "old_content": "import random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\n\nfrom vllm.model_executor.layers.ops.sample import get_num_triton_sampler_splits\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import SequenceData, SequenceGroupMetadata\nfrom vllm.utils import (async_tensor_h2d, is_pin_memory_available,\n                        maybe_expand_dim)\n\n_SAMPLING_EPS = 1e-5\n_SEED_0_REPLACEMENT = 3403598558\n\n\n@dataclass\nclass SequenceGroupToSample:\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ----------------------|\n    #                                   |-- query_len ---|\n\n    # Sequence ids for the sequence group in a previous step.\n    seq_ids: List[int]\n    sampling_params: SamplingParams\n    # seq_id -> sequence data.\n    seq_data: Dict[int, SequenceData]\n    # The length of the sequence (all tokens seen in the past + new token to\n    # compute attention) of the sequence group. None if it is in a decode\n    # stage.\n    seq_len: Optional[int]\n    # The length of new query tokens to compute in the current step. None if it\n    # is in a decode stage. The length of query_len <= seq_len if chunked\n    # prefill is enabled.\n    query_len: Optional[int]\n    # A random number generator for sampling.\n    generator: Optional[torch.Generator]\n    # True if the sequence group is in prefill stage. False if it is in a\n    # decode stage.\n    is_prompt: bool\n    # Query token indices from logits. to compute prompt logprob. Empty if\n    # prompt logprob is not required.\n    prompt_logprob_indices: List[int]\n    # Sample token indices from logits. Empty if sampling is not required.\n    sample_indices: List[int]\n\n    @property\n    def do_sample(self):\n        return len(self.sample_indices) > 0\n\n    def __post_init__(self):\n        if len(self.prompt_logprob_indices) > 0:\n            assert self.sampling_params.prompt_logprobs is not None\n        if self.is_prompt:\n            assert self.seq_len is not None\n            assert self.query_len is not None\n\n\nclass SamplingMetadata:\n    \"\"\"Metadata for input sequences. Used in sampler.\n\n    The usage is as follow;\n    ```\n    hidden_states = execute_model(...)\n    logits = hidden_states[sampling_metadata.selected_token_indices]\n    sample(logits)\n\n    def sample(logits):\n        # Use categorized_sample_indices for sampling....\n    ```\n\n    Args:\n        seq_groups: List of batched sequence groups.\n        selected_token_indices: (num_query_tokens_to_logprob). Indices to find\n            logits from the initial model output hidden states.\n        categorized_sample_indices: SamplingType -> token indices to sample.\n            Each token indices is 2D tensor of (num_indices, num_indices) where\n            the first item means the sample index within the returned logit\n            (before pruning padding), and the second item means the sample\n            index after pruning using selected_token_indices.\n            For example, if the returned logit is [1, 2, 3], and we select\n            [1, 2] for sampling, the pruned logit will be [2, 3]. In this case,\n            The first tuple is [1, 2] (sampled index within original logit),\n            and the second tuple is [0, 1] (sampled index within pruned logit).\n        num_prompts: Number of prompt sequence groups in seq_groups.\n    \"\"\"\n\n    def __init__(\n        self,\n        seq_groups: List[SequenceGroupToSample],\n        selected_token_indices: torch.Tensor,\n        categorized_sample_indices: Dict[SamplingType, torch.Tensor],\n        num_prompts: int,\n    ) -> None:\n        self.seq_groups = seq_groups\n        self.selected_token_indices = selected_token_indices\n        self.categorized_sample_indices = categorized_sample_indices\n        self.num_prompts = num_prompts\n\n    @staticmethod\n    def prepare(\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        seq_lens: List[int],\n        query_lens: Optional[List[int]],\n        device: str,\n        pin_memory: bool,\n    ) -> \"SamplingMetadata\":\n        (\n            seq_groups,\n            selected_token_indices,\n            categorized_sample_indices,\n            num_prompts,\n        ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\n                                device)\n        selected_token_indices = async_tensor_h2d(selected_token_indices,\n                                                  dtype=torch.long,\n                                                  target_device=device,\n                                                  pin_memory=pin_memory)\n        categorized_sample_indices = {\n            t: maybe_expand_dim(\n                async_tensor_h2d(seq_ids,\n                                 dtype=torch.int,\n                                 target_device=device,\n                                 pin_memory=pin_memory), 2, 2)\n            for t, seq_ids in categorized_sample_indices.items()\n        }\n\n        sampling_metadata = SamplingMetadata(\n            seq_groups=seq_groups,\n            selected_token_indices=selected_token_indices,\n            categorized_sample_indices=categorized_sample_indices,\n            num_prompts=num_prompts,\n        )\n        return sampling_metadata\n\n    def __repr__(self) -> str:\n        return (\n            \"SamplingMetadata(\"\n            f\"seq_groups={self.seq_groups}, \"\n            f\"selected_token_indices={self.selected_token_indices}, \"\n            f\"categorized_sample_indices={self.categorized_sample_indices}), \")\n\n\ndef _prepare_seq_groups(\n    seq_group_metadata_list: List[SequenceGroupMetadata],\n    seq_lens: List[int],\n    query_lens: Optional[List[int]],\n    device: str,\n) -> Tuple[List[SequenceGroupToSample], List[int], Dict[\n        SamplingType, List[Tuple[int, int]]], int]:\n    \"\"\"Prepare sequence groups and indices for sampling.\n\n    Args:\n        seq_group_metadata_list: A list of sequence group to batch.\n        seq_lens: A list of sequence lens per sequence group.\n            Index of prompt len should match with seq_group_metadata_list.\n        query_lens: A list of query lengths. Prompt lens include the length\n            of entire prompt tokens, and it could be shorter.\n        device: A device to use for random number generator,\n            `SequenceGroupToSample.generator`.\n\n    Returns:\n        seq_groups: A list of sequence group to sample.\n        selected_token_indices: See the definition from `SamplingMetadata`.\n        categorized_sample_indices: See the definition from `SamplingMetadata`.\n        num_prompts: Total number of prompts from `seq_group_metadata_list`.\n    \"\"\"\n    # Batched sequence groups for the current model forward stsep.\n    seq_groups: List[SequenceGroupToSample] = []\n    # A list of token indices to sample/compute logprob. It is used to\n    # prune the outcome logits from the model for the performance.\n    selected_token_indices: List[int] = []\n    # Used for selected_token_indices.\n    model_output_idx = 0\n\n    # Sampling type -> (\n    # indices to sample/prompt logprob within pruned output logits,\n    # indices to sample within pruned logits)\n    categorized_sample_indices: Dict[SamplingType, List[Tuple[int, int]]] = {\n        t: []\n        for t in SamplingType\n    }\n    # Index of logits to compute logprob. Logits include both prompt logprob\n    # and sample logprob indices.\n    logit_idx = 0\n    # Index to sample from a sample tensor. It is used by triton sample kernel.\n    # See `_sample_with_triton_kernel` for more details.\n    sample_idx = 0\n    # Total number of prompts from given sequence groups.\n    num_prompts = 0\n\n    for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n        seq_ids = list(seq_group_metadata.seq_data.keys())\n        sampling_params = seq_group_metadata.sampling_params\n        is_prompt = seq_group_metadata.is_prompt\n        generator: Optional[torch.Generator] = None\n        # If the current seq group is in decode stage, it is None.\n        seq_len: Optional[int] = None\n        query_len: Optional[int] = None\n        prompt_logprob_indices: List[int] = []\n        sample_indices: List[int] = []\n        do_sample = seq_group_metadata.do_sample\n\n        if seq_group_metadata.is_prompt:\n            if sampling_params.seed is not None:\n                seq_group_metadata.state.generator = torch.Generator(\n                    device=device).manual_seed(sampling_params.seed)\n\n            num_prompts += 1\n            num_prefill_sample = len(seq_ids)\n            assert num_prefill_sample == 1\n            assert query_lens is not None and seq_lens is not None\n            query_len, seq_len = query_lens[i], seq_lens[i]\n            # If we need sampling, exclude num_prefill_sample tokens from\n            # prompt logprob.\n            prompt_logprob_len = (query_len - num_prefill_sample\n                                  if do_sample else query_len)\n            sample_len = num_prefill_sample if do_sample else 0\n        else:\n            # Decode\n            prompt_logprob_len = 0\n            sample_len = len(seq_ids) if do_sample else 0\n\n        # Update indices to select from the model output.\n        \"\"\"\n        This blocks computes selected_token_indices which is used in the\n        following way.\n\n        hidden_states = model(...)\n        logits = hidden_states[selected_token_indices]\n        \"\"\"\n\n        if sampling_params.prompt_logprobs is not None:\n            selected_token_indices.extend(\n                range(model_output_idx, model_output_idx + prompt_logprob_len))\n        model_output_idx += prompt_logprob_len\n        if do_sample:\n            selected_token_indices.extend(\n                range(model_output_idx, model_output_idx + sample_len))\n        model_output_idx += sample_len\n\n        # We now find indices for logprob computation and sampling.\n        \"\"\"\n        This block computes categorized_sample_indices which is used in the\n        following way.\n\n        hidden_states = model(...)\n        logits = hidden_states[selected_token_indices]\n        def sample(logits):\n           # Use categorized_sample_indices for sampling.\n           # prompt_logprob_indices to find prompt logprob indices.\n           # sample_indices to find sample indices.\n        \"\"\"\n\n        if sampling_params.prompt_logprobs is not None:\n            prompt_logprob_indices.extend(\n                range(logit_idx, logit_idx + prompt_logprob_len))\n            logit_idx += prompt_logprob_len\n        if do_sample:\n            sample_indices.extend(range(logit_idx, logit_idx + sample_len))\n            categorized_sample_indices[sampling_params.sampling_type].extend(\n                list(\n                    zip(range(logit_idx, logit_idx + sample_len),\n                        range(sample_idx, sample_idx + sample_len))))\n            logit_idx += sample_len\n            sample_idx += sample_len\n\n        if sampling_params.seed is not None:\n            generator = seq_group_metadata.state.generator\n\n        seq_groups.append(\n            SequenceGroupToSample(\n                seq_ids=seq_ids,\n                sampling_params=sampling_params,\n                seq_data=seq_group_metadata.seq_data,\n                seq_len=seq_len,\n                query_len=query_len,\n                generator=generator,\n                is_prompt=is_prompt,\n                prompt_logprob_indices=list(prompt_logprob_indices),\n                sample_indices=list(sample_indices)))\n    return (seq_groups, selected_token_indices, categorized_sample_indices,\n            num_prompts)\n\n\n@dataclass\nclass SamplingTensors:\n    \"\"\"Tensors for sampling.\"\"\"\n\n    temperatures: torch.Tensor\n    top_ps: torch.Tensor\n    top_ks: torch.Tensor\n    min_ps: torch.Tensor\n    presence_penalties: torch.Tensor\n    frequency_penalties: torch.Tensor\n    repetition_penalties: torch.Tensor\n    sampling_seeds: torch.Tensor\n    sample_indices: torch.Tensor\n    extra_seeds: Optional[torch.Tensor]\n    prompt_tokens: torch.Tensor\n    output_tokens: torch.Tensor\n\n    @classmethod\n    def from_sampling_metadata(\n        cls,\n        sampling_metadata: \"SamplingMetadata\",\n        vocab_size: int,\n        device: torch.device,\n        dtype: torch.dtype,\n        *,\n        extra_seeds_to_generate: int = 0,\n        extra_entropy: Optional[Tuple[int, ...]] = None\n    ) -> Tuple[\"SamplingTensors\", bool, bool, bool]:\n        \"\"\"\n        extra_seeds_to_generate: extra seeds to generate using the\n            user-defined seed for each sequence.\n        extra_entropy: extra entropy to use when generating seeds.\n        \"\"\"\n        prompt_tokens: List[List[int]] = []\n        output_tokens: List[List[int]] = []\n        top_ks: List[int] = []\n        temperatures: List[float] = []\n        top_ps: List[float] = []\n        min_ps: List[float] = []\n        presence_penalties: List[float] = []\n        frequency_penalties: List[float] = []\n        repetition_penalties: List[float] = []\n        sampling_seeds: List[int] = []\n        sample_indices: List[int] = []\n        prompt_best_of: List[int] = []\n        do_penalties = False\n        do_top_p_top_k = False\n        do_min_p = False\n\n        # We need one base seed per Triton slice.\n        seeds_to_generate = (extra_seeds_to_generate +\n                             get_num_triton_sampler_splits(vocab_size))\n\n        assert sampling_metadata.seq_groups is not None\n        for seq_group in sampling_metadata.seq_groups:\n            seq_ids = seq_group.seq_ids\n            sampling_params = seq_group.sampling_params\n            temperature = sampling_params.temperature\n            p = sampling_params.presence_penalty\n            f = sampling_params.frequency_penalty\n            r = sampling_params.repetition_penalty\n            top_p = sampling_params.top_p\n            min_p = sampling_params.min_p\n            seed = sampling_params.seed\n\n            is_greedy = sampling_params.sampling_type == SamplingType.GREEDY\n\n            # k should not be greater than the vocab size.\n            top_k = min(sampling_params.top_k, vocab_size)\n            top_k = vocab_size if top_k == -1 else top_k\n            if temperature < _SAMPLING_EPS:\n                # NOTE: Zero temperature means deterministic sampling\n                # (i.e., greedy sampling or beam search).\n                # Set the temperature to 1 to avoid division by zero.\n                temperature = 1.0\n            if not do_top_p_top_k and (top_p < 1.0 - _SAMPLING_EPS\n                                       or top_k != vocab_size):\n                do_top_p_top_k = True\n            if not do_min_p and min_p > _SAMPLING_EPS:\n                do_min_p = True\n            if not do_penalties and (abs(p) >= _SAMPLING_EPS\n                                     or abs(f) >= _SAMPLING_EPS\n                                     or abs(r - 1.0) >= _SAMPLING_EPS):\n                do_penalties = True\n\n            is_prompt = seq_group.is_prompt\n            if (seq_group.is_prompt\n                    and sampling_params.prompt_logprobs is not None):\n                # For tokens in the prompt that we only need to get\n                # their logprobs\n                query_len = seq_group.query_len\n                assert query_len is not None\n                prefill_len = len(seq_group.prompt_logprob_indices)\n                temperatures += [temperature] * prefill_len\n                top_ps += [top_p] * prefill_len\n                top_ks += [top_k] * prefill_len\n                min_ps += [min_p] * prefill_len\n                presence_penalties += [0] * prefill_len\n                frequency_penalties += [0] * prefill_len\n                repetition_penalties += [1] * prefill_len\n\n            if seq_group.do_sample:\n                sample_lens = len(seq_group.sample_indices)\n                assert sample_lens == len(seq_ids)\n                temperatures += [temperature] * len(seq_ids)\n                top_ps += [top_p] * len(seq_ids)\n                top_ks += [top_k] * len(seq_ids)\n                min_ps += [min_p] * len(seq_ids)\n                presence_penalties += [p] * len(seq_ids)\n                frequency_penalties += [f] * len(seq_ids)\n                repetition_penalties += [r] * len(seq_ids)\n\n            if is_prompt:\n                prompt_best_of.append(sampling_params.best_of)\n                query_len = seq_group.query_len\n                assert query_len is not None\n\n            for seq_id in seq_ids:\n                seq_data = seq_group.seq_data[seq_id]\n                extra_entropy = extra_entropy or ()\n                seq_seeds = cls._get_sequence_seeds(\n                    seed,\n                    seq_data.get_len(),\n                    *extra_entropy,\n                    seq_id,\n                    seeds_to_generate=seeds_to_generate,\n                    is_greedy=is_greedy)\n                sampling_seeds.append(seq_seeds)\n            sample_indices.extend(seq_group.sample_indices)\n\n        if do_penalties:\n            for seq_group in sampling_metadata.seq_groups:\n                seq_ids = seq_group.seq_ids\n                if (seq_group.is_prompt\n                        and sampling_params.prompt_logprobs is not None):\n                    prefill_len = len(seq_group.prompt_logprob_indices)\n                    prompt_tokens.extend([] for _ in range(prefill_len))\n                    output_tokens.extend([] for _ in range(prefill_len))\n                if seq_group.do_sample:\n                    for seq_id in seq_ids:\n                        seq_data = seq_group.seq_data[seq_id]\n                        prompt_tokens.append(list(seq_data.prompt_token_ids))\n                        output_tokens.append(list(seq_data.output_token_ids))\n\n        sampling_tensors = SamplingTensors.from_lists(\n            temperatures, top_ps, top_ks, min_ps, presence_penalties,\n            frequency_penalties, repetition_penalties, sampling_seeds,\n            sample_indices, prompt_tokens, output_tokens, vocab_size,\n            extra_seeds_to_generate, device, dtype)\n        return (sampling_tensors, do_penalties, do_top_p_top_k, do_min_p)\n\n    @classmethod\n    def from_lists(cls, temperatures: List[float], top_ps: List[float],\n                   top_ks: List[int], min_ps: List[float],\n                   presence_penalties: List[float],\n                   frequency_penalties: List[float],\n                   repetition_penalties: List[float],\n                   sampling_seeds: List[int], sample_indices: List[int],\n                   prompt_tokens: List[List[int]],\n                   output_tokens: List[List[int]], vocab_size: int,\n                   extra_seeds_to_generate: int, device: torch.device,\n                   dtype: torch.dtype) -> \"SamplingTensors\":\n        # Note that the performance will be very bad without\n        # pinned memory.\n        pin_memory = is_pin_memory_available()\n\n        do_penalties = prompt_tokens or output_tokens\n\n        if do_penalties:\n            prompt_max_len = max([len(tokens) for tokens in prompt_tokens],\n                                 default=0)\n            prompt_padded_tokens = [\n                tokens + [vocab_size] * (prompt_max_len - len(tokens))\n                for tokens in prompt_tokens\n            ]\n            output_max_len = max([len(tokens) for tokens in output_tokens],\n                                 default=0)\n            output_padded_tokens = [\n                tokens + [vocab_size] * (output_max_len - len(tokens))\n                for tokens in output_tokens\n            ]\n\n        temperatures_t = torch.tensor(\n            temperatures,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ps_t = torch.tensor(\n            top_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        min_ps_t = torch.tensor(\n            min_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        presence_penalties_t = torch.tensor(\n            presence_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        frequency_penalties_t = torch.tensor(\n            frequency_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        repetition_penalties_t = torch.tensor(\n            repetition_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ks_t = torch.tensor(\n            top_ks,\n            device=\"cpu\",\n            dtype=torch.int,\n            pin_memory=pin_memory,\n        )\n        sample_indices_t = torch.tensor(\n            sample_indices,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        )\n        if do_penalties:\n            prompt_tensor = torch.tensor(\n                prompt_padded_tokens,\n                device=\"cpu\",\n                dtype=torch.long,\n                pin_memory=pin_memory,\n            )\n            output_tensor = torch.tensor(\n                output_padded_tokens,\n                device=\"cpu\",\n                dtype=torch.long,\n                pin_memory=pin_memory,\n            )\n        else:\n            prompt_tensor = None\n            output_tensor = None\n        # need to transpose and make contiguous to\n        # copy the tensor correctly.\n        # [batch_size, n_seeds] -> [n_seeds, batch_size]\n        sampling_seeds_t = torch.tensor(\n            sampling_seeds,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        ).T.contiguous()\n\n        # Because the memory is pinned, we can do non-blocking\n        # transfer to device.\n\n        # How many seeds the sample operation itself will need.\n        num_base_seeds = sampling_seeds_t.shape[0] - extra_seeds_to_generate\n        sampling_seeds_gpu = sampling_seeds_t.to(device=device,\n                                                 non_blocking=True)\n        extra_seeds_gpu = sampling_seeds_gpu[num_base_seeds:]\n        if not extra_seeds_gpu.numel():\n            extra_seeds_gpu = None\n        sampling_seeds_gpu = sampling_seeds_gpu[:num_base_seeds]\n\n        if do_penalties:\n            prompt_tokens_gpu = prompt_tensor.to(device=device,\n                                                 non_blocking=True)\n            output_tokens_gpu = output_tensor.to(device=device,\n                                                 non_blocking=True)\n        else:\n            empty_tensor = torch.empty(0, device=device, dtype=torch.long)\n            prompt_tokens_gpu = empty_tensor\n            output_tokens_gpu = empty_tensor\n\n        return cls(\n            temperatures=temperatures_t.to(device=device, non_blocking=True),\n            top_ps=top_ps_t.to(device=device, non_blocking=True),\n            top_ks=top_ks_t.to(device=device, non_blocking=True),\n            min_ps=min_ps_t.to(device=device, non_blocking=True),\n            presence_penalties=presence_penalties_t.to(device=device,\n                                                       non_blocking=True),\n            frequency_penalties=frequency_penalties_t.to(device=device,\n                                                         non_blocking=True),\n            repetition_penalties=repetition_penalties_t.to(device=device,\n                                                           non_blocking=True),\n            prompt_tokens=prompt_tokens_gpu,\n            output_tokens=output_tokens_gpu,\n            sampling_seeds=sampling_seeds_gpu,\n            sample_indices=sample_indices_t.to(device=device,\n                                               non_blocking=True),\n            extra_seeds=extra_seeds_gpu,\n        )\n\n    @staticmethod\n    def _get_sequence_seeds(\n        seed: int,\n        *extra_entropy: int,\n        seeds_to_generate: int,\n        is_greedy: bool,\n    ):\n        \"\"\"Get `seeds_to_generate` child seeds from `seed` and extra entropy.\"\"\"\n        if not is_greedy:\n            if seed is None:\n                randint_fn = random.randint\n            else:\n                generator = random.Random(str((seed, ) + extra_entropy))\n                randint_fn = generator.randint\n            lo, hi = torch.iinfo(torch.long).min, torch.iinfo(torch.long).max\n            # If the user/random sets seed = 0 but request should\n            # have sampling, we need to change it to something\n            # else. We use a constant in that case.\n            # This way we don't need to create and load a bool\n            # matrix in the sampling kernel, which reduces CPU\n            # overhead and latency.\n            seq_seeds = [\n                randint_fn(lo, hi) or _SEED_0_REPLACEMENT\n                for _ in range(seeds_to_generate)\n            ]\n        else:\n            # For the kernel, seed == 0 means greedy decoding.\n            seq_seeds = [0] * seeds_to_generate\n        return seq_seeds\n",
      "diff": "diff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex ad5fb1317..c346cd056 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -2,6 +2,7 @@ import random\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple\n \n+import numpy as np\n import torch\n \n from vllm.model_executor.layers.ops.sample import get_num_triton_sampler_splits\n@@ -457,16 +458,20 @@ class SamplingTensors:\n         if do_penalties:\n             prompt_max_len = max([len(tokens) for tokens in prompt_tokens],\n                                  default=0)\n-            prompt_padded_tokens = [\n-                tokens + [vocab_size] * (prompt_max_len - len(tokens))\n-                for tokens in prompt_tokens\n-            ]\n+            prompt_padded_tokens = np.full(\n+                (len(prompt_tokens), prompt_max_len),\n+                vocab_size,\n+                dtype=np.int64)\n+            for i, tokens in enumerate(prompt_tokens):\n+                prompt_padded_tokens[i, :len(tokens)] = tokens\n             output_max_len = max([len(tokens) for tokens in output_tokens],\n                                  default=0)\n-            output_padded_tokens = [\n-                tokens + [vocab_size] * (output_max_len - len(tokens))\n-                for tokens in output_tokens\n-            ]\n+            output_padded_tokens = np.full(\n+                (len(output_tokens), output_max_len),\n+                vocab_size,\n+                dtype=np.int64)\n+            for i, tokens in enumerate(output_tokens):\n+                output_padded_tokens[i, :len(tokens)] = tokens\n \n         temperatures_t = torch.tensor(\n             temperatures,\n@@ -517,18 +522,11 @@ class SamplingTensors:\n             pin_memory=pin_memory,\n         )\n         if do_penalties:\n-            prompt_tensor = torch.tensor(\n-                prompt_padded_tokens,\n-                device=\"cpu\",\n-                dtype=torch.long,\n-                pin_memory=pin_memory,\n-            )\n-            output_tensor = torch.tensor(\n-                output_padded_tokens,\n-                device=\"cpu\",\n-                dtype=torch.long,\n-                pin_memory=pin_memory,\n-            )\n+            prompt_tensor = torch.from_numpy(prompt_padded_tokens)\n+            output_tensor = torch.from_numpy(output_padded_tokens)\n+            if pin_memory:\n+                prompt_tensor = prompt_tensor.pin_memory()\n+                output_tensor = output_tensor.pin_memory()\n         else:\n             prompt_tensor = None\n             output_tensor = None",
      "change_type": "modified",
      "lines_added": 19,
      "lines_removed": 21
    }
  ],
  "affected_apis": [
    "SamplingTensors.__init__"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "",
    "is_benchmark_actually_there": "",
    "sample_clues": "from, lists, sampling_metadata"
  }
}