{
  "commit_hash": "ab7165f2c7ea358df969d68a0fb0ce9bb184a083",
  "parent_hash": "0c2fa50b84dddd4866313ac37074255d29a94055",
  "message": "[TPU] Optimize RoPE forward_native2 (#7636)",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2024-08-18 01:15:10 -0700",
  "files_changed": [
    {
      "file_path": "vllm/model_executor/layers/rotary_embedding.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.33.2/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Rotary Positional Embeddings.\"\"\"\nimport math\nfrom typing import Any, Dict, List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn as nn\n\nfrom vllm.model_executor.custom_op import CustomOp\nfrom vllm.platforms import current_platform\n\n\ndef _rotate_neox(x: torch.Tensor) -> torch.Tensor:\n    x1 = x[..., :x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2:]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef _rotate_gptj(x: torch.Tensor) -> torch.Tensor:\n    x1 = x[..., ::2]\n    x2 = x[..., 1::2]\n    x = torch.stack((-x2, x1), dim=-1)\n    return x.flatten(-2)\n\n\ndef _apply_rotary_emb(\n    x: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> torch.Tensor:\n    x_ = torch.view_as_complex(\n        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n                          -1).transpose(1, 2)\n    return x_out\n\n\nclass RotaryEmbedding(CustomOp):\n    \"\"\"Original rotary positional embedding.\"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        dtype: torch.dtype,\n    ) -> None:\n        super().__init__()\n        self.head_size = head_size\n        self.rotary_dim = rotary_dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        self.is_neox_style = is_neox_style\n        self.dtype = dtype\n\n        cache = self._compute_cos_sin_cache()\n        self.use_native2 = current_platform.is_tpu() and is_neox_style\n        if not self.use_native2:\n            cache = cache.to(dtype)\n            self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n        else:\n            cos, sin = cache.chunk(2, dim=-1)\n            freqs_cis = cos + 1j * sin\n            self.register_buffer(\"freqs_cis\", freqs_cis, persistent=False)\n\n    def _compute_inv_freq(self, base: Union[int, float]) -> torch.Tensor:\n        \"\"\"Compute the inverse frequency.\"\"\"\n        # NOTE(woosuk): The HF implementation uses `torch.arange(...).float()`.\n        # However, we use `torch.arange(..., dtype=torch.float)` instead to\n        # avoid numerical issues with large base values (e.g., 10000000).\n        # This may cause a slight numerical difference between the HF\n        # implementation and ours.\n        # NOTE(woosuk): To exactly match the HF implementation, we need to\n        # use CPU to compute the cache and then move it to GPU. However, we\n        # create the cache on GPU for faster initialization. This may cause\n        # a slight numerical difference between the HF implementation and ours.\n        inv_freq = 1.0 / (base**(torch.arange(\n            0, self.rotary_dim, 2, dtype=torch.float) / self.rotary_dim))\n        return inv_freq\n\n    def _compute_cos_sin_cache(self) -> torch.Tensor:\n        \"\"\"Compute the cos and sin cache.\"\"\"\n        inv_freq = self._compute_inv_freq(self.base)\n        t = torch.arange(self.max_position_embeddings, dtype=torch.float)\n\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n        cos = freqs.cos()\n        sin = freqs.sin()\n        cache = torch.cat((cos, sin), dim=-1)\n        return cache\n\n    def forward_native(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"A PyTorch-native implementation equivalent to forward().\n\n        This method mimics the implementation of the custom CUDA kernel\n        used in `forward_cuda()`.\n        \"\"\"\n        query = query.view(*query.shape[:-1], -1, self.head_size)\n        key = key.view(*key.shape[:-1], -1, self.head_size)\n\n        query_rot = query[..., :self.rotary_dim]\n        key_rot = key[..., :self.rotary_dim]\n        if self.rotary_dim < self.head_size:\n            query_pass = query[..., self.rotary_dim:]\n            key_pass = key[..., self.rotary_dim:]\n\n        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(\n            positions.device, dtype=query.dtype)\n        cos_sin = self.cos_sin_cache[torch.add(positions, offsets)\n                                     if offsets is not None else positions]\n        cos, sin = cos_sin.chunk(2, dim=-1)\n        if self.is_neox_style:\n            # NOTE(woosuk): Here we assume that the positions tensor has the\n            # shape [batch_size, seq_len].\n            cos = cos.repeat(1, 1, 2).unsqueeze(-2)\n            sin = sin.repeat(1, 1, 2).unsqueeze(-2)\n        else:\n            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)\n            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)\n\n        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj\n        query_rot = query_rot * cos + rotate_fn(query_rot) * sin\n        key_rot = key_rot * cos + rotate_fn(key_rot) * sin\n\n        if self.rotary_dim < self.head_size:\n            query = torch.cat((query_rot, query_pass), dim=-1)\n            key = torch.cat((key_rot, key_pass), dim=-1)\n        else:\n            query = query_rot\n            key = key_rot\n        query = query.flatten(-2)\n        key = key.flatten(-2)\n        return query, key\n\n    def forward_native2(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Another PyTorch-native implementation of forward().\n\n        This method might perform better than `forward_native()` when compiled.\n        \"\"\"\n        if positions.dim() == 1:\n            batch_size = 1\n            seq_len = positions.shape[0]\n        else:\n            batch_size, seq_len = positions.shape\n        if offsets is not None:\n            positions = positions + offsets\n        freqs_cis = self.freqs_cis.index_select(0, positions.flatten())\n        freqs_cis = freqs_cis.view(batch_size, 1, seq_len, -1)\n\n        query_shape = query.shape\n        query = query.view(batch_size, seq_len, -1, self.head_size)\n        query_rot = query[..., :self.rotary_dim]\n        query_pass = query[..., self.rotary_dim:]\n        query_rot = _apply_rotary_emb(query_rot, freqs_cis)\n        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n\n        key_shape = key.shape\n        key = key.view(batch_size, seq_len, -1, self.head_size)\n        key_rot = key[..., :self.rotary_dim]\n        key_pass = key[..., self.rotary_dim:]\n        key_rot = _apply_rotary_emb(key_rot, freqs_cis)\n        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n        return query, key\n\n    def forward_cuda(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        from vllm import _custom_ops as ops\n\n        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n                                                   dtype=query.dtype)\n        # ops.rotary_embedding()/batched_rotary_embedding()\n        # are in-place operations that update the query and key tensors.\n        if offsets is not None:\n            ops.batched_rotary_embedding(positions, query, key, self.head_size,\n                                         self.cos_sin_cache,\n                                         self.is_neox_style, self.rotary_dim,\n                                         offsets)\n        else:\n            ops.rotary_embedding(positions, query, key, self.head_size,\n                                 self.cos_sin_cache, self.is_neox_style)\n        return query, key\n\n    def forward_xpu(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        from vllm._ipex_ops import ipex_ops as ops\n\n        self.cos_sin_cache = self.cos_sin_cache.to(positions.device,\n                                                   dtype=query.dtype)\n        # ops.rotary_embedding()/batched_rotary_embedding()\n        # are in-place operations that update the query and key tensors.\n        if offsets is not None:\n            ops.batched_rotary_embedding(positions, query, key, self.head_size,\n                                         self.cos_sin_cache,\n                                         self.is_neox_style, self.rotary_dim,\n                                         offsets)\n        else:\n            ops.rotary_embedding(positions, query, key, self.head_size,\n                                 self.cos_sin_cache, self.is_neox_style)\n        return query, key\n\n    def forward_tpu(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        forward_fn = (self.forward_native2\n                      if self.use_native2 else self.forward_native)\n        return forward_fn(positions, query, key, offsets)\n\n    def extra_repr(self) -> str:\n        s = f\"head_size={self.head_size}, rotary_dim={self.rotary_dim}\"\n        s += f\", max_position_embeddings={self.max_position_embeddings}\"\n        s += f\", base={self.base}, is_neox_style={self.is_neox_style}\"\n        return s\n\n\nclass LinearScalingRotaryEmbedding(RotaryEmbedding):\n    \"\"\"RotaryEmbedding extended with linear scaling.\n\n    It supports multiple scaling factors. Since multiple LoRA adapters may have\n    different scaling factors, we need multiple cos/sin caches. In this way,\n    instead of running rotary embedding kernel per lora, we can run multiple\n    lora in a batched way.\n\n    In addition to that, we also keep the cos/sin cache for the scaling factor\n    of 1 (default) at all times.\n\n    Exemplary for two scaling factors x=1, y and z with embeddings\n    [[x11, x12, ... x1m], ..., [xn1, xn2, ..., xnm]] and\n    [[y11, y12, ... y1o], ..., [yn1, yn2, ..., yno]], and\n    [[z11, z12, ... z1p], ..., [zn1, zn2, ..., znp]],\n\n    we construct the cos/sin cache as follows:\n    [[x11, x12, ... x1m, y11, y12, ... y1o, z11, z12, ... z1p],\n        ...\n     [xn1, xn2, ... xnm, yn1, yn2, ... yno, zn1, zn2, ... znp]]\n\n    We then use offsets to index into the cos/sin cache for\n    the respective scaling factors.\n\n    The offset to cache can be accessed via `scaling_factor_to_offset` API.\n\n    Credits to the Reddit user /u/kaiokendev\n    \"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        scaling_factors: Union[List[float], float],\n        dtype: torch.dtype,\n    ) -> None:\n        if isinstance(scaling_factors, float):\n            scaling_factors = [scaling_factors]\n        self.scaling_factors: List[float] = scaling_factors  # noqa\n        super().__init__(head_size, rotary_dim, max_position_embeddings, base,\n                         is_neox_style, dtype)\n        # Lazy initialized.\n        self._scaling_factor_to_offset: Dict[float, int]\n\n    def _compute_cos_sin_cache(self) -> torch.Tensor:\n        inv_freq = self._compute_inv_freq(self.base)\n        cache_list: List[torch.Tensor] = []\n        # offsets to the next cache in a tensor.\n        # Each offset corresponds to the same index in scaling_factors.\n        offsets: List[int] = []\n        for scaling_factor in self.scaling_factors:\n            # NOTE(woosuk): self.max_position_embeddings is the original\n            # maximum length before applying the rope scaling.\n            # Thus, the maximum length after applying the rope scaling is\n            # self.max_position_embeddings * self.scaling_factor.\n            max_len = self.max_position_embeddings * scaling_factor\n            t = torch.arange(max_len, dtype=torch.float)\n            t = t / scaling_factor\n\n            freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n            cos = freqs.cos()\n            sin = freqs.sin()\n            cache = torch.cat((cos, sin), dim=-1)\n            if not cache_list:\n                offset = 0\n            else:\n                last_offset = offsets[-1]\n                next_max_len = cache_list[-1].shape[0]\n                offset = last_offset + next_max_len\n            offsets.append(offset)\n            cache_list.append(cache)\n        self._scaling_factor_to_offset = {\n            float(scaling_factor): offsets[i]\n            for i, scaling_factor in enumerate(self.scaling_factors)\n        }\n        assert len(self.scaling_factors) == len(offsets)\n        return torch.cat(cache_list, dim=0)\n\n    @property\n    def scaling_factor_to_offset(self) -> Dict[float, int]:\n        return self._scaling_factor_to_offset\n\n\nclass DynamicNTKScalingRotaryEmbedding(RotaryEmbedding):\n    \"\"\"RotaryEmbedding extended with Dynamic NTK scaling.\n\n    Credits to the Reddit users /u/bloc97 and /u/emozilla\n    \"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        scaling_factor: float,\n        dtype: torch.dtype,\n    ) -> None:\n        self.scaling_factor = scaling_factor\n        super().__init__(head_size, rotary_dim, max_position_embeddings, base,\n                         is_neox_style, dtype)\n\n    def _compute_cos_sin_cache(self) -> torch.Tensor:\n        # NOTE(woosuk): self.max_position_embeddings is the original\n        # maximum length before applying the rope scaling.\n        # Thus, the maximum length after applying the rope scaling is\n        # self.max_position_embeddings * self.scaling_factor.\n        max_len = self.max_position_embeddings * self.scaling_factor\n        base = self.base * (\n            (self.scaling_factor * max_len / self.max_position_embeddings) -\n            (self.scaling_factor - 1))**(self.rotary_dim /\n                                         (self.rotary_dim - 2))\n        inv_freq = self._compute_inv_freq(base)\n        t = torch.arange(max_len, dtype=torch.float)\n\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n        cos = freqs.cos()\n        sin = freqs.sin()\n        cache = torch.cat((cos, sin), dim=-1)\n        return cache\n\n\n# Inverse dim formula to find dim based on number of rotations\ndef _yarn_find_correction_dim(num_rotations: int,\n                              dim: int,\n                              base: float = 10000,\n                              max_position_embeddings: int = 2048) -> float:\n    return (dim * math.log(max_position_embeddings /\n                           (num_rotations * 2 * math.pi))) / (2 *\n                                                              math.log(base))\n\n\n# Find dim range bounds based on rotations\ndef _yarn_find_correction_range(\n        low_rot: int,\n        high_rot: int,\n        dim: int,\n        base: float = 10000,\n        max_position_embeddings: int = 2048) -> Tuple[int, int]:\n    low = math.floor(\n        _yarn_find_correction_dim(low_rot, dim, base, max_position_embeddings))\n    high = math.ceil(\n        _yarn_find_correction_dim(high_rot, dim, base,\n                                  max_position_embeddings))\n    return max(low, 0), min(high, dim - 1)  # Clamp values just in case\n\n\ndef _yarn_linear_ramp_mask(low: float, high: float, dim: int,\n                           dtype: torch.dtype) -> torch.Tensor:\n    if low == high:\n        high += 0.001  # Prevent singularity\n\n    linear_func = (torch.arange(dim, dtype=dtype) - low) / (high - low)\n    ramp_func = torch.clamp(linear_func, 0, 1)\n    return ramp_func\n\n\ndef _yarn_get_mscale(scale: float = 1) -> float:\n    if scale <= 1:\n        return 1.0\n    return 0.1 * math.log(scale) + 1.0\n\n\nclass YaRNScalingRotaryEmbedding(RotaryEmbedding):\n    \"\"\"RotaryEmbedding extended with YaRN method.\n\n    Credits to Peng et al. github.com/jquesnelle/yarn\n    \"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        scaling_factor: float,\n        dtype: torch.dtype,\n        *,\n        extrapolation_factor: float = 1,\n        attn_factor: float = 1,\n        beta_fast: int = 32,\n        beta_slow: int = 1,\n    ) -> None:\n        self.scaling_factor = scaling_factor\n        self.extrapolation_factor = extrapolation_factor\n        self.attn_factor = attn_factor\n        self.beta_fast = beta_fast\n        self.beta_slow = beta_slow\n        # Get n-d magnitude scaling corrected for interpolation\n        self.mscale = float(\n            _yarn_get_mscale(self.scaling_factor) * attn_factor)\n        super().__init__(head_size, rotary_dim, max_position_embeddings, base,\n                         is_neox_style, dtype)\n\n    def _compute_inv_freq(self, scaling_factor: float) -> torch.Tensor:\n        pos_freqs = self.base**(\n            torch.arange(0, self.rotary_dim, 2, dtype=torch.float) /\n            self.rotary_dim)\n        inv_freq_extrapolation = 1.0 / pos_freqs\n        inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)\n\n        low, high = _yarn_find_correction_range(self.beta_fast, self.beta_slow,\n                                                self.rotary_dim, self.base,\n                                                self.max_position_embeddings)\n        # Get n-d rotational scaling corrected for extrapolation\n        inv_freq_mask = (1 - _yarn_linear_ramp_mask(\n            low, high, self.rotary_dim // 2,\n            dtype=torch.float)) * self.extrapolation_factor\n        inv_freq = inv_freq_interpolation * (\n            1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask\n        return inv_freq\n\n    def _compute_cos_sin_cache(self) -> torch.Tensor:\n        inv_freq = self._compute_inv_freq(self.scaling_factor)\n        t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                         dtype=torch.float32)\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n        cos = (freqs.cos() * self.mscale)\n        sin = (freqs.sin() * self.mscale)\n        cache = torch.cat((cos, sin), dim=-1)\n        return cache\n\n\nclass Phi3LongRoPEScaledRotaryEmbedding(nn.Module):\n    \"\"\"Phi3 family of models scaled rotary embedding.\n\n    Based on the original RotaryEmbedding implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        original_max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        dtype: torch.dtype,\n        short_factor: List[float],\n        long_factor: List[float],\n        short_mscale: float = 1.0,\n        long_mscale: float = 1.0,\n    ):\n        super().__init__()\n\n        if rotary_dim != head_size:\n            raise ValueError(\n                f\"`Phi3LongRoPEScaledRotaryEmbedding` does not support \\\n                    rotary_dim != head_size ({rotary_dim}!={head_size}).\")\n        if is_neox_style is False:\n            raise ValueError(\n                \"`Phi3LongRoPEScaledRotaryEmbedding` only supports neox_style.\"\n            )\n\n        self.head_size = head_size\n        self.max_position_embeddings = max_position_embeddings\n        self.original_max_position_embeddings = original_max_position_embeddings\n        self.base = base\n        self.short_factor = short_factor\n        self.long_factor = long_factor\n        self.short_mscale = short_mscale\n        self.long_mscale = long_mscale\n\n        scale = (self.max_position_embeddings /\n                 self.original_max_position_embeddings)\n\n        if scale <= 1.0:\n            self.scaling_factor = 1.0\n        else:\n            self.scaling_factor = math.sqrt(\n                1 + math.log(scale) /\n                math.log(self.original_max_position_embeddings))\n\n        short_cache = self._compute_cos_sin_cache(\n            original_max_position_embeddings, short_factor, short_mscale)\n        short_cache = short_cache.to(dtype)\n        self.register_buffer(\"short_cos_sin_cache\",\n                             short_cache,\n                             persistent=False)\n\n        long_cache = self._compute_cos_sin_cache(max_position_embeddings,\n                                                 long_factor, long_mscale)\n        long_cache = long_cache.to(dtype)\n        self.register_buffer(\"long_cos_sin_cache\",\n                             long_cache,\n                             persistent=False)\n\n        long_short_cache = torch.cat(\n            [self.short_cos_sin_cache, self.long_cos_sin_cache], dim=0)\n        self.register_buffer(\"long_short_cos_sin_cache\",\n                             long_short_cache,\n                             persistent=False)\n\n    def _compute_inv_freq(self, rescale_factors: List[float]) -> torch.Tensor:\n        rescale_factors = torch.tensor(rescale_factors, dtype=torch.float32)\n        inv_freq = 1.0 / (rescale_factors * (self.base**(torch.arange(\n            0, self.head_size, 2, dtype=torch.float) / self.head_size)))\n        return inv_freq\n\n    def _compute_cos_sin_cache(\n        self,\n        max_position_embeddings: int,\n        rescale_factors: List[float],\n        mscale: float,\n    ) -> torch.Tensor:\n        inv_freq = self._compute_inv_freq(rescale_factors)\n        t = torch.arange(max_position_embeddings, dtype=torch.float)\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n        cos = freqs.cos() * mscale * self.scaling_factor\n        sin = freqs.sin() * mscale * self.scaling_factor\n        cache = torch.cat((cos, sin), dim=-1)\n        return cache\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        query = query.view(*query.shape[:-1], -1, self.head_size)\n        key = key.view(*key.shape[:-1], -1, self.head_size)\n\n        k = self.original_max_position_embeddings\n        long_prompt_offset = (torch.any(positions > k).float() *\n                              torch.full_like(positions, k)).long()\n        idx = (torch.add(positions, long_prompt_offset)\n               if long_prompt_offset is not None else positions)\n        self.long_short_cos_sin_cache: torch.Tensor = (\n            self.long_short_cos_sin_cache.to(idx.device))\n        idx = torch.add(idx, offsets) if offsets is not None else idx\n        cos_sin = torch.index_select(self.long_short_cos_sin_cache, 0, idx)\n\n        cos, sin = cos_sin.chunk(2, dim=-1)\n        cos = cos.repeat(1, 2).unsqueeze(-2)\n        sin = sin.repeat(1, 2).unsqueeze(-2)\n\n        query = query * cos + _rotate_neox(query) * sin\n        key = key * cos + _rotate_neox(key) * sin\n\n        return query.flatten(-2), key.flatten(-2)\n\n\ndef yarn_get_mscale(scale: float = 1, mscale: float = 1) -> float:\n    if scale <= 1:\n        return 1.0\n    return 0.1 * mscale * math.log(scale) + 1.0\n\n\nclass DeepseekScalingRotaryEmbedding(RotaryEmbedding):\n    \"\"\"RotaryEmbedding extended with YaRN method.\n\n    Credits to Peng et al. github.com/jquesnelle/yarn\n    \"\"\"\n\n    def __init__(\n        self,\n        head_size: int,\n        rotary_dim: int,\n        max_position_embeddings: int,\n        base: int,\n        is_neox_style: bool,\n        scaling_factor: float,\n        dtype: torch.dtype,\n        *,\n        extrapolation_factor: float = 1,\n        attn_factor: float = 1,\n        beta_fast: int = 32,\n        beta_slow: int = 1,\n        mscale: float = 1,\n        mscale_all_dim: float = 0,\n    ) -> None:\n        self.scaling_factor = scaling_factor\n        self.extrapolation_factor = extrapolation_factor\n        self.attn_factor = attn_factor\n        self.beta_fast = beta_fast\n        self.beta_slow = beta_slow\n        # Get n-d magnitude scaling corrected for interpolation.\n        self.mscale = float(\n            yarn_get_mscale(self.scaling_factor, float(mscale)) /\n            yarn_get_mscale(self.scaling_factor, float(mscale_all_dim)) *\n            attn_factor)\n        super().__init__(head_size, rotary_dim, max_position_embeddings, base,\n                         is_neox_style, dtype)\n\n    def _compute_inv_freq(self, scaling_factor: float) -> torch.Tensor:\n        pos_freqs = self.base**(torch.arange(\n            0, self.rotary_dim, 2, dtype=torch.float, device=\"cuda\") /\n                                self.rotary_dim)\n        inv_freq_extrapolation = 1.0 / pos_freqs\n        inv_freq_interpolation = 1.0 / (scaling_factor * pos_freqs)\n\n        low, high = _yarn_find_correction_range(self.beta_fast, self.beta_slow,\n                                                self.rotary_dim, self.base,\n                                                self.max_position_embeddings)\n        # Get n-d rotational scaling corrected for extrapolation\n        inv_freq_mask = (1 - _yarn_linear_ramp_mask(\n            low, high, self.rotary_dim // 2,\n            dtype=torch.float)) * self.extrapolation_factor\n        inv_freq = inv_freq_interpolation * (\n            1 - inv_freq_mask) + inv_freq_extrapolation * inv_freq_mask\n        return inv_freq\n\n    def _compute_cos_sin_cache(self) -> torch.Tensor:\n        inv_freq = self._compute_inv_freq(self.scaling_factor)\n        t = torch.arange(self.max_position_embeddings * self.scaling_factor,\n                         device=\"cuda\",\n                         dtype=torch.float32)\n        freqs = torch.einsum(\"i,j -> ij\", t, inv_freq)\n        cos = (freqs.cos() * self.mscale)\n        sin = (freqs.sin() * self.mscale)\n        cache = torch.cat((cos, sin), dim=-1)\n        print(\"Cache shape\", cache.shape)\n        return cache\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        offsets: Optional[torch.Tensor] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"PyTorch-native implementation equivalent to forward().\"\"\"\n        query_rot = query[..., :self.rotary_dim]\n        key_rot = key[..., :self.rotary_dim]\n        if self.rotary_dim < self.head_size:\n            query_pass = query[..., self.rotary_dim:]\n            key_pass = key[..., self.rotary_dim:]\n\n        self.cos_sin_cache: torch.Tensor = self.cos_sin_cache.to(\n            positions.device)\n        cos_sin = self.cos_sin_cache[torch.add(positions, offsets)\n                                     if offsets is not None else positions]\n        cos, sin = cos_sin.chunk(2, dim=-1)\n        if self.is_neox_style:\n            # NOTE(woosuk): Here we assume that the positions tensor has the\n            # shape [batch_size, seq_len].\n            cos = cos.repeat(1, 1, 2).unsqueeze(-2)\n            sin = sin.repeat(1, 1, 2).unsqueeze(-2)\n        else:\n            cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)\n            sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)\n\n        rotate_fn = _rotate_neox if self.is_neox_style else _rotate_gptj\n        query_rot = query_rot * cos + rotate_fn(query_rot) * sin\n        key_rot = key_rot * cos + rotate_fn(key_rot) * sin\n\n        if self.rotary_dim < self.head_size:\n            query = torch.cat((query_rot, query_pass), dim=-1)\n            key = torch.cat((key_rot, key_pass), dim=-1)\n        else:\n            query = query_rot\n            key = key_rot\n        return query, key\n\n\nclass GemmaRotaryEmbedding(RotaryEmbedding):\n\n    def _compute_inv_freq(self, base: Union[int, float]) -> torch.Tensor:\n        # https://github.com/huggingface/transformers/blob/v4.41.2/src/transformers/models/gemma/modeling_gemma.py#L107\n        inv_freq = 1.0 / (base**(\n            torch.arange(0, self.rotary_dim, 2, dtype=torch.int64).float() /\n            self.rotary_dim))\n        return inv_freq\n\n\nclass ExtendedRotaryEmbedding(RotaryEmbedding):\n\n    def _compute_inv_freq(self, base: Union[int, float]) -> torch.Tensor:\n        inv_freqs = super()._compute_inv_freq(base)\n        return self.apply_scaling(inv_freqs)\n\n    def apply_scaling(self, freqs: torch.Tensor):\n        scale_factor = 8\n        low_freq_factor = 1\n        high_freq_factor = 4\n        old_context_len = 8192\n\n        low_freq_wavelen = old_context_len / low_freq_factor\n        high_freq_wavelen = old_context_len / high_freq_factor\n        new_freqs = []\n        for freq in freqs:\n            wavelen = 2 * math.pi / freq\n            if wavelen < high_freq_wavelen:\n                new_freqs.append(freq)\n            elif wavelen > low_freq_wavelen:\n                new_freqs.append(freq / scale_factor)\n            else:\n                assert low_freq_wavelen != high_freq_wavelen\n                smooth = (old_context_len / wavelen - low_freq_factor) / (\n                    high_freq_factor - low_freq_factor)\n                new_freqs.append((1 - smooth) * freq / scale_factor +\n                                 smooth * freq)\n        return torch.tensor(new_freqs, dtype=freqs.dtype, device=freqs.device)\n\n\n_ROPE_DICT: Dict[Tuple, RotaryEmbedding] = {}\n\n\ndef get_rope(\n    head_size: int,\n    rotary_dim: int,\n    max_position: int,\n    base: int,\n    is_neox_style: bool = True,\n    rope_scaling: Optional[Dict[str, Any]] = None,\n    dtype: Optional[torch.dtype] = None,\n    partial_rotary_factor: float = 1.0,\n) -> RotaryEmbedding:\n    if dtype is None:\n        dtype = torch.get_default_dtype()\n    if rope_scaling is not None:\n        # Transforms every value that is a list into a tuple for caching calls\n        rope_scaling_tuple = {\n            k: tuple(v) if isinstance(v, list) else v\n            for k, v in rope_scaling.items()\n        }\n        rope_scaling_args = tuple(rope_scaling_tuple.items())\n    else:\n        rope_scaling_args = None\n    if partial_rotary_factor < 1.0:\n        rotary_dim = int(rotary_dim * partial_rotary_factor)\n    key = (head_size, rotary_dim, max_position, base, is_neox_style,\n           rope_scaling_args, dtype)\n    if key in _ROPE_DICT:\n        return _ROPE_DICT[key]\n    if rope_scaling is None:\n        rotary_emb = RotaryEmbedding(head_size, rotary_dim, max_position, base,\n                                     is_neox_style, dtype)\n    else:\n        scaling_type = rope_scaling[\n            \"type\"] if \"type\" in rope_scaling else rope_scaling[\"rope_type\"]\n        # The correct one should be \"longrope\" but keep \"su\" here\n        # for backward compatible\n        if scaling_type not in {\"su\", \"longrope\", \"llama3\"}:\n            scaling_factor = rope_scaling[\"factor\"]\n        if scaling_type == \"llama3\":\n            rotary_emb = ExtendedRotaryEmbedding(head_size, rotary_dim,\n                                                 max_position, base,\n                                                 is_neox_style, dtype)\n        elif scaling_type == \"linear\":\n            rotary_emb = LinearScalingRotaryEmbedding(head_size, rotary_dim,\n                                                      max_position, base,\n                                                      is_neox_style,\n                                                      scaling_factor, dtype)\n        elif scaling_type == \"dynamic\":\n            rotary_emb = DynamicNTKScalingRotaryEmbedding(\n                head_size, rotary_dim, max_position, base, is_neox_style,\n                scaling_factor, dtype)\n        elif scaling_type == \"yarn\":\n            original_max_position = rope_scaling[\n                \"original_max_position_embeddings\"]\n            extra_kwargs = {\n                k: v\n                for k, v in rope_scaling.items()\n                if k in (\"extrapolation_factor\", \"attn_factor\", \"beta_fast\",\n                         \"beta_slow\")\n            }\n            rotary_emb = YaRNScalingRotaryEmbedding(head_size, rotary_dim,\n                                                    original_max_position,\n                                                    base, is_neox_style,\n                                                    scaling_factor, dtype,\n                                                    **extra_kwargs)\n        elif scaling_type == \"deepseek_yarn\":\n            original_max_position = rope_scaling[\n                \"original_max_position_embeddings\"]\n            # assert max_position == original_max_position * scaling_factor\n            extra_kwargs = {\n                k: v\n                for k, v in rope_scaling.items()\n                if k in (\"extrapolation_factor\", \"attn_factor\", \"beta_fast\",\n                         \"beta_slow\", \"mscale\", \"mscale_all_dim\")\n            }\n            rotary_emb = DeepseekScalingRotaryEmbedding(\n                head_size, rotary_dim, original_max_position, base,\n                is_neox_style, scaling_factor, dtype, **extra_kwargs)\n        # The correct one should be \"longrope\" but keep \"su\" here\n        # for backward compatible\n        elif scaling_type == \"su\" or scaling_type == \"longrope\":\n            short_factor = rope_scaling[\"short_factor\"]\n            long_factor = rope_scaling[\"long_factor\"]\n            original_max_position = rope_scaling[\n                \"original_max_position_embeddings\"]\n            extra_kwargs = {\n                k: v\n                for k, v in rope_scaling.items()\n                if k in (\"short_mscale\", \"long_mscale\")\n            }\n            rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(\n                head_size, rotary_dim, max_position, original_max_position,\n                base, is_neox_style, dtype, short_factor, long_factor,\n                **extra_kwargs)\n        else:\n            raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n    _ROPE_DICT[key] = rotary_emb\n    return rotary_emb\n",
      "diff": "diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py\nindex 7b3acd7f3..fa85f72e3 100644\n--- a/vllm/model_executor/layers/rotary_embedding.py\n+++ b/vllm/model_executor/layers/rotary_embedding.py\n@@ -46,15 +46,23 @@ def _rotate_gptj(x: torch.Tensor) -> torch.Tensor:\n \n def _apply_rotary_emb(\n     x: torch.Tensor,\n-    freqs_cis: torch.Tensor,\n+    cos: torch.Tensor,\n+    sin: torch.Tensor,\n ) -> torch.Tensor:\n-    x_ = torch.view_as_complex(\n-        torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n-    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x)\n-    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n-    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2],\n-                          -1).transpose(1, 2)\n-    return x_out\n+    \"\"\"\n+    Args:\n+        x: [num_tokens, num_heads, head_size]\n+        cos: [num_tokens, head_size // 2]\n+        sin: [num_tokens, head_size // 2]\n+    \"\"\"\n+    orig_dtype = x.dtype\n+    x = x.float()\n+    x1, x2 = torch.chunk(x, 2, dim=-1)\n+    cos = cos.unsqueeze(-2)\n+    sin = sin.unsqueeze(-2)\n+    o1 = x1 * cos - x2 * sin\n+    o2 = x2 * cos + x1 * sin\n+    return torch.cat((o1, o2), dim=-1).to(orig_dtype)\n \n \n class RotaryEmbedding(CustomOp):\n@@ -78,14 +86,10 @@ class RotaryEmbedding(CustomOp):\n         self.dtype = dtype\n \n         cache = self._compute_cos_sin_cache()\n+        cache = cache.to(dtype)\n+        self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n+\n         self.use_native2 = current_platform.is_tpu() and is_neox_style\n-        if not self.use_native2:\n-            cache = cache.to(dtype)\n-            self.register_buffer(\"cos_sin_cache\", cache, persistent=False)\n-        else:\n-            cos, sin = cache.chunk(2, dim=-1)\n-            freqs_cis = cos + 1j * sin\n-            self.register_buffer(\"freqs_cis\", freqs_cis, persistent=False)\n \n     def _compute_inv_freq(self, base: Union[int, float]) -> torch.Tensor:\n         \"\"\"Compute the inverse frequency.\"\"\"\n@@ -173,28 +177,25 @@ class RotaryEmbedding(CustomOp):\n \n         This method might perform better than `forward_native()` when compiled.\n         \"\"\"\n-        if positions.dim() == 1:\n-            batch_size = 1\n-            seq_len = positions.shape[0]\n-        else:\n-            batch_size, seq_len = positions.shape\n         if offsets is not None:\n             positions = positions + offsets\n-        freqs_cis = self.freqs_cis.index_select(0, positions.flatten())\n-        freqs_cis = freqs_cis.view(batch_size, 1, seq_len, -1)\n+        positions = positions.flatten()\n+        num_tokens = positions.shape[0]\n+        cos_sin = self.cos_sin_cache.index_select(0, positions)\n+        cos, sin = cos_sin.chunk(2, dim=-1)\n \n         query_shape = query.shape\n-        query = query.view(batch_size, seq_len, -1, self.head_size)\n+        query = query.view(num_tokens, -1, self.head_size)\n         query_rot = query[..., :self.rotary_dim]\n         query_pass = query[..., self.rotary_dim:]\n-        query_rot = _apply_rotary_emb(query_rot, freqs_cis)\n+        query_rot = _apply_rotary_emb(query_rot, cos, sin)\n         query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)\n \n         key_shape = key.shape\n-        key = key.view(batch_size, seq_len, -1, self.head_size)\n+        key = key.view(num_tokens, -1, self.head_size)\n         key_rot = key[..., :self.rotary_dim]\n         key_pass = key[..., self.rotary_dim:]\n-        key_rot = _apply_rotary_emb(key_rot, freqs_cis)\n+        key_rot = _apply_rotary_emb(key_rot, cos, sin)\n         key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)\n         return query, key",
      "change_type": "modified",
      "lines_added": 28,
      "lines_removed": 27
    }
  ],
  "affected_apis": [
    "RotaryEmbedding.__init__",
    "RotaryEmbedding.forward_native2"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (this is for TPU)",
    "is_benchmark_actually_there": "",
    "sample_clues": "forward, init, native"
  }
}