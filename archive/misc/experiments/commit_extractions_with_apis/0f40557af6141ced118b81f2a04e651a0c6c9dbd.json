{
  "commit_hash": "0f40557af6141ced118b81f2a04e651a0c6c9dbd",
  "parent_hash": "a490aafa3671da1b6b2be6cff4568913fcb1732c",
  "message": "Implement block copy kernel to optimize beam search (#32)",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2023-04-07 17:45:07 -0700",
  "files_changed": [
    {
      "file_path": "benchmark/benchmark_latency.py",
      "old_content": "import argparse\nimport time\nfrom typing import List\n\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\n\nfrom cacheflow.master.simple_frontend import SimpleFrontend\nfrom cacheflow.master.server import (Server, add_server_arguments,\n                                     initialize_ray_cluster)\nfrom cacheflow.sampling_params import SamplingParams\nfrom cacheflow.utils import get_gpu_memory, get_cpu_memory\n\n\ndef main(args: argparse.Namespace):\n    # TODO(zhuohan): Support pipeline parallelism.\n    assert args.pipeline_parallel_size == 1, (\n        'Pipeline parallelism is not supported yet.')\n\n    (num_nodes, num_devices_per_node, distributed_init_method,\n    all_stage_devices) = (\n        initialize_ray_cluster(\n            address='local',\n            pipeline_parallel_size=args.pipeline_parallel_size,\n            tensor_parallel_size=args.tensor_parallel_size))\n\n    # Create a server.\n    server = Server(\n        model=args.model,\n        model_path=args.model_path,\n        pipeline_parallel_size=args.pipeline_parallel_size,\n        tensor_parallel_size=args.tensor_parallel_size,\n        block_size=args.block_size,\n        dtype=args.dtype,\n        seed=args.seed,\n        swap_space=args.swap_space,\n        max_num_batched_tokens=args.max_num_batched_tokens,\n        num_nodes=num_nodes,\n        num_devices_per_node=num_devices_per_node,\n        distributed_init_method=distributed_init_method,\n        all_stage_devices=all_stage_devices,\n        gpu_memory=get_gpu_memory(),\n        cpu_memory=get_cpu_memory(),\n    )\n\n    # Create a frontend.\n    frontend = SimpleFrontend(\n        model_name=args.model,\n        block_size=args.block_size,\n    )\n    sampling_params_dict = {\n        'n': 1,\n        'temperature': 0.0,\n        'top_p': 1.0,\n        'use_beam_search': False,\n        'stop_token_ids': set(),\n        'max_num_steps': args.output_len,\n    }\n    sampling_params = SamplingParams.from_dict(sampling_params_dict)\n    input_token_ids = [0] * args.input_len\n\n    def profile_step(profile=False):\n        if profile:\n            torch.cuda.cudart().cudaProfilerStart()\n        for _ in range(args.batch_size):\n            frontend._add_query(input_token_ids, sampling_params)\n        server.add_sequence_groups(frontend.get_inputs())\n        start_time = time.time()\n        while True:\n            server.step()\n            if not server.has_unfinished_requests():\n                break\n        end_time = time.time()\n        latency = end_time - start_time\n        if profile:\n            torch.cuda.cudart().cudaProfilerStop()\n        return latency\n\n    print(\"Warm up step\")\n    profile_step()\n\n    # Benchmark.\n    latencies = []\n    for _ in tqdm(range(3), desc=\"Profile step\"):\n        latencies.append(profile_step())\n    print(f'Avg latency: {np.mean(latencies)} seconds')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='CacheFlow simple server.')\n    parser = add_server_arguments(parser)\n    parser.add_argument('--input-len', type=int, default=32)\n    parser.add_argument('--output-len', type=int, default=128)\n    parser.add_argument('--batch-size', type=int, default=8)\n    args = parser.parse_args()\n    args.max_num_batched_tokens = max(\n        args.max_num_batched_tokens, args.batch_size * args.input_len)\n    print(args)\n    main(args)\n",
      "diff": "diff --git a/benchmark/benchmark_latency.py b/benchmark/benchmark_latency.py\nindex aa611ac0d..24727713f 100644\n--- a/benchmark/benchmark_latency.py\n+++ b/benchmark/benchmark_latency.py\n@@ -50,14 +50,15 @@ def main(args: argparse.Namespace):\n         block_size=args.block_size,\n     )\n     sampling_params_dict = {\n-        'n': 1,\n-        'temperature': 0.0,\n+        'n': args.n,\n+        'temperature': 0.0 if args.use_beam_search else 1.0,\n         'top_p': 1.0,\n-        'use_beam_search': False,\n+        'use_beam_search': args.use_beam_search,\n         'stop_token_ids': set(),\n         'max_num_steps': args.output_len,\n     }\n     sampling_params = SamplingParams.from_dict(sampling_params_dict)\n+    print(sampling_params)\n     input_token_ids = [0] * args.input_len\n \n     def profile_step(profile=False):\n@@ -93,6 +94,8 @@ if __name__ == '__main__':\n     parser.add_argument('--input-len', type=int, default=32)\n     parser.add_argument('--output-len', type=int, default=128)\n     parser.add_argument('--batch-size', type=int, default=8)\n+    parser.add_argument('--n', type=int, default=1)\n+    parser.add_argument('--use-beam-search', action='store_true')\n     args = parser.parse_args()\n     args.max_num_batched_tokens = max(\n         args.max_num_batched_tokens, args.batch_size * args.input_len)",
      "change_type": "modified",
      "lines_added": 7,
      "lines_removed": 4
    },
    {
      "file_path": "cacheflow/models/sample.py",
      "old_content": "from typing import Dict, List, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom cacheflow.models import InputMetadata\nfrom cacheflow.sampling_params import SamplingParams\nfrom cacheflow.sequence import SequenceOutputs\nfrom cacheflow.parallel_utils.tensor_parallel import gather_from_tensor_model_parallel_region\n\n\nclass Sampler(nn.Module):\n\n    def __init__(self) -> None:\n        super().__init__()\n\n    def forward(\n        self,\n        embedding: torch.Tensor,\n        hidden_states: torch.Tensor,\n        input_metadata: InputMetadata,\n    ) -> Dict[int, SequenceOutputs]:\n        # Get the hidden states that we use for sampling.\n        hidden_states = _prune_hidden_states(hidden_states, input_metadata)\n\n        # Get the logits for the next tokens.\n        logits = torch.matmul(hidden_states, embedding.t())\n        logits = gather_from_tensor_model_parallel_region(logits)\n\n        # Apply temperature scaling.\n        temperatures = _get_temperatures(input_metadata)\n        assert len(temperatures) == logits.shape[0]\n        if any(t != 1.0 for t in temperatures):\n            t = torch.tensor(\n                temperatures, dtype=logits.dtype, device=logits.device)\n            # Use in-place division to avoid creating a new tensor.\n            logits.div_(t.unsqueeze(dim=1))\n\n        # We use float32 for probabilities and log probabilities.\n        # Compute the probabilities.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n        # Compute the log probabilities (before applying top-p).\n        logprobs = torch.log(probs)\n\n        # Apply top-p truncation.\n        top_ps = _get_top_ps(input_metadata)\n        assert len(top_ps) == probs.shape[0]\n        if any(p < 1.0 for p in top_ps):\n            p = torch.tensor(top_ps, dtype=probs.dtype, device=probs.device)\n            probs = _apply_top_p(probs, p)\n\n        # Sample the next tokens.\n        return _sample(probs, logprobs, input_metadata)\n\n\ndef _prune_hidden_states(\n    hidden_states: torch.Tensor,\n    input_metadata: InputMetadata,\n) -> torch.Tensor:\n    start_idx = 0\n    last_token_indicies: List[int] = []\n    for prompt_len in input_metadata.prompt_lens:\n        last_token_indicies.append(start_idx + prompt_len - 1)\n        start_idx += prompt_len\n    last_token_indicies.extend(\n        range(start_idx, start_idx + input_metadata.num_generation_tokens))\n    return hidden_states[last_token_indicies]\n\n\ndef _get_temperatures(\n    input_metadata: InputMetadata,\n) -> List[float]:\n    # Collect the temperatures for the logits.\n    temperatures: List[float] = []\n    for i, seq_group in enumerate(input_metadata.seq_groups):\n        seq_ids, sampling_params = seq_group\n        temperature = sampling_params.temperature\n        if temperature == 0.0:\n            # NOTE: Zero temperature means deterministic sampling\n            # (i.e., greedy sampling or beam search).\n            # Set the temperature to 1 to avoid division by zero.\n            temperature = 1.0\n\n        if i < input_metadata.num_prompts:\n            # A prompt input.\n            temperatures.append(temperature)\n        else:\n            # A generation token.\n            temperatures += [temperature] * len(seq_ids)\n    return temperatures\n\n\ndef _get_top_ps(\n    input_metadata: InputMetadata,\n) -> List[float]:\n    top_ps: List[float] = []\n    for i, seq_group in enumerate(input_metadata.seq_groups):\n        seq_ids, sampling_params = seq_group\n        if i < input_metadata.num_prompts:\n            # A prompt input.\n            top_ps.append(sampling_params.top_p)\n        else:\n            # A generation token.\n            top_ps += [sampling_params.top_p] * len(seq_ids)\n    return top_ps\n\n\ndef _apply_top_p(\n    probs: torch.Tensor,\n    p: torch.Tensor,\n) -> torch.Tensor:\n    # TODO(woosuk): Optimize.\n    probs_sort, probs_idx = probs.sort(dim=-1, descending=True)\n    probs_sum = torch.cumsum(probs_sort, dim=-1)\n    mask = (probs_sum - probs_sort) > p.unsqueeze(dim=1)\n    probs_sort[mask] = 0.0\n    probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n    probs = torch.gather(\n        probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n    return probs\n\n\ndef _get_topk_logprobs(\n    logprobs: torch.Tensor,\n    num_logprobs: int,\n) -> Dict[int, float]:\n    if num_logprobs == 0:\n        return {}\n\n    topk_logprobs, topk_ids = torch.topk(logprobs, num_logprobs)\n    if num_logprobs == 1:\n        topk_logprobs = [topk_logprobs.item()]\n        topk_ids = [topk_ids.item()]\n    else:\n        topk_logprobs = topk_logprobs.tolist()\n        topk_ids = topk_ids.tolist()\n\n    token_to_logprob: Dict[int, float] = {}\n    for token_id, logprob in zip(topk_ids, topk_logprobs):\n        token_to_logprob[token_id] = logprob\n    return token_to_logprob\n\n\ndef _sample_from_prompt(\n    prob: torch.Tensor,\n    sampling_params: SamplingParams,\n) -> List[int]:\n    if sampling_params.use_beam_search:\n        # Beam search.\n        beam_width = sampling_params.n\n        _, next_token_ids = torch.topk(prob, beam_width)\n        next_token_ids = next_token_ids.tolist()\n    elif sampling_params.temperature == 0.0:\n        # Greedy sampling.\n        assert sampling_params.n == 1\n        next_token_id = torch.argmax(prob)\n        next_token_ids = [next_token_id.item()]\n    else:\n        # Neucleus sampling.\n        # Sample n tokens for the prompt.\n        n = sampling_params.n\n        next_token_ids = torch.multinomial(\n            prob, num_samples=n, replacement=True)\n        next_token_ids = next_token_ids.tolist()\n    return next_token_ids\n\n\ndef _sample_from_generation_tokens(\n    seq_ids: List[int],\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    seq_logprobs: List[float],\n    sampling_params: SamplingParams,\n) -> Tuple[List[int], List[int]]:\n    # NOTE(woosuk): sampling_params.n can be greater than\n    # len(seq_ids) because some sequences in the group might have\n    # been already terminated.\n    if sampling_params.use_beam_search:\n        # Beam search.\n        # Add cumulative logprobs for the sequences in the group.\n        seq_logprobs = torch.tensor(\n            seq_logprobs, dtype=torch.float, device=logprobs.device)\n        logprobs = logprobs + seq_logprobs.unsqueeze(dim=1)\n\n        vocab_size = logprobs.size(-1)\n        beam_width = len(seq_ids)\n        _, topk_ids = torch.topk(logprobs.flatten(), beam_width)\n        seq_idx = torch.div(topk_ids, vocab_size, rounding_mode='floor').tolist()\n        beam_seq_ids = [seq_ids[i] for i in seq_idx]\n        token_ids = (topk_ids % vocab_size).tolist()\n\n        beam_outputs: Dict[int, Tuple[int, int]] = {}\n        outstanding_beams: List[Tuple[int, int]] = []\n        # If a beam survives, continue with it.\n        for seq_id, token_id in zip(beam_seq_ids, token_ids):\n            if seq_id not in beam_outputs:\n                beam_outputs[seq_id] = (seq_id, token_id)\n            else:\n                outstanding_beams.append((seq_id, token_id))\n\n        # If a beam is discarded, fork another beam.\n        for seq_id in seq_ids:\n            if seq_id not in beam_outputs:\n                beam_outputs[seq_id] = outstanding_beams.pop()\n        assert not outstanding_beams\n\n        parent_seq_ids = [beam_outputs[seq_id][0] for seq_id in seq_ids]\n        next_token_ids = [beam_outputs[seq_id][1] for seq_id in seq_ids]\n    elif sampling_params.temperature == 0.0:\n        # Greedy sampling.\n        assert len(seq_ids) == 1\n        next_token_id = torch.argmax(probs, dim=-1)\n        next_token_ids = [next_token_id.item()]\n        parent_seq_ids = seq_ids\n    else:\n        # Neucleus sampling.\n        # Sample 1 token for each sequence in the group.\n        next_token_ids = torch.multinomial(\n            probs, num_samples=1, replacement=True)\n        next_token_ids = next_token_ids.squeeze(dim=-1).tolist()\n        parent_seq_ids = seq_ids\n    return parent_seq_ids, next_token_ids\n\n\ndef _sample(\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    input_metadata: InputMetadata,\n) -> Dict[int, SequenceOutputs]:\n    seq_outputs: Dict[int, SequenceOutputs] = {}\n\n    # TODO(woosuk): Optimize.\n    idx = 0\n    for i, seq_group in enumerate(input_metadata.seq_groups):\n        seq_ids, sampling_params = seq_group\n        if i < input_metadata.num_prompts:\n            # Generate the next tokens for a prompt input.\n            assert len(seq_ids) == sampling_params.n\n            prob = probs[idx]\n            logprob = logprobs[idx]\n            idx += 1\n\n            # Sample the next tokens.\n            next_token_ids = _sample_from_prompt(prob, sampling_params)\n            # Get top-k log probabilities for the next tokens.\n            next_logprobs = _get_topk_logprobs(\n                logprob, sampling_params.num_logprobs)\n\n            # Build the output.\n            for seq_id, next_token_id in zip(seq_ids, next_token_ids):\n                output_logprobs = next_logprobs.copy()\n                output_logprobs[next_token_id] = logprob[next_token_id].item()\n                seq_outputs[seq_id] = SequenceOutputs(\n                    seq_id, seq_id, next_token_id, output_logprobs)\n        else:\n            # Generate the next tokens for generation tokens.\n            prob = probs[idx:idx + len(seq_ids)]\n            logprob = logprobs[idx:idx + len(seq_ids)]\n            idx += len(seq_ids)\n\n            # Sample the next tokens.\n            seq_logprobs = [\n                input_metadata.seq_logprobs[seq_id] for seq_id in seq_ids]\n            parent_seq_ids, next_token_ids = _sample_from_generation_tokens(\n                seq_ids, prob, logprob, seq_logprobs, sampling_params)\n\n            # Get top-k log probabilities for the next tokens.\n            next_logprobs: Dict[int, Dict[int, float]] = {}\n            for i, seq_id in enumerate(seq_ids):\n                next_logprobs[seq_id] = _get_topk_logprobs(\n                    logprob[i], sampling_params.num_logprobs)\n\n            # Build the output.\n            for seq_id, parent_seq_id, next_token_id in zip(\n                seq_ids, parent_seq_ids, next_token_ids):\n                i = seq_ids.index(parent_seq_id)\n                output_logprobs = next_logprobs[parent_seq_id].copy()\n                output_logprobs[next_token_id] = logprob[i, next_token_id].item()\n                seq_outputs[seq_id] = SequenceOutputs(\n                    seq_id,\n                    parent_seq_id,\n                    next_token_id,\n                    output_logprobs,\n                )\n\n    return seq_outputs\n",
      "diff": "diff --git a/cacheflow/models/sample.py b/cacheflow/models/sample.py\nindex 3b53f34f4..1e358c7e5 100644\n--- a/cacheflow/models/sample.py\n+++ b/cacheflow/models/sample.py\n@@ -185,9 +185,10 @@ def _sample_from_generation_tokens(\n         vocab_size = logprobs.size(-1)\n         beam_width = len(seq_ids)\n         _, topk_ids = torch.topk(logprobs.flatten(), beam_width)\n-        seq_idx = torch.div(topk_ids, vocab_size, rounding_mode='floor').tolist()\n+        topk_ids = topk_ids.tolist()\n+        seq_idx = [i // vocab_size for i in topk_ids]\n         beam_seq_ids = [seq_ids[i] for i in seq_idx]\n-        token_ids = (topk_ids % vocab_size).tolist()\n+        token_ids = [i % vocab_size for i in topk_ids]\n \n         beam_outputs: Dict[int, Tuple[int, int]] = {}\n         outstanding_beams: List[Tuple[int, int]] = []",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    },
    {
      "file_path": "cacheflow/worker/cache_engine.py",
      "old_content": "from typing import Dict, List, Tuple\n\nimport torch\nfrom cacheflow import cache_ops\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass CacheEngine:\n\n    def __init__(\n        self,\n        worker_id: int,\n        num_layers: int,\n        num_heads: int,\n        head_size: int,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        dtype: torch.dtype,\n    ) -> None:\n        if head_size % 16 != 0:\n            raise ValueError(\n                f'head_size ({head_size}) must be a multiple of 16.')\n\n        self.worker_id = worker_id\n        self.num_layers = num_layers\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.block_size = block_size\n        self.num_gpu_blocks = num_gpu_blocks\n        self.num_cpu_blocks = num_cpu_blocks\n        self.dtype = dtype\n\n        # Initialize the cache.\n        self.gpu_cache = self.allocate_gpu_cache()\n        self.cpu_cache = self.allocate_cpu_cache()\n\n        # Initialize the stream for caching operations.\n        self.cache_stream = torch.cuda.Stream()\n        assert self.cache_stream != torch.cuda.current_stream()\n        # Initialize the events for stream synchronization.\n        self.events = [torch.cuda.Event() for _ in range(num_layers)]\n\n    def get_key_block_shape(self) -> Tuple[int, int, int, int]:\n        element_size = torch.tensor([], dtype=self.dtype).element_size()\n        x = 16 // element_size\n        return (\n            self.num_heads,\n            self.head_size // x,\n            self.block_size,\n            x,\n        )\n\n    def get_value_block_shape(self) -> Tuple[int, int, int]:\n        return (\n            self.num_heads,\n            self.head_size,\n            self.block_size,\n        )\n\n    def allocate_gpu_cache(self) -> List[KVCache]:\n        gpu_cache: List[KVCache] = []\n        key_block_shape = self.get_key_block_shape()\n        value_block_shape = self.get_value_block_shape()\n        for _ in range(self.num_layers):\n            key_blocks = torch.empty(\n                size=(self.num_gpu_blocks, *key_block_shape),\n                dtype=self.dtype,\n                device=\"cuda\",\n            )\n            value_blocks = torch.empty(\n                size=(self.num_gpu_blocks, *value_block_shape),\n                dtype=self.dtype,\n                device=\"cuda\",\n            )\n            gpu_cache.append((key_blocks, value_blocks))\n        return gpu_cache\n\n    def allocate_cpu_cache(self) -> List[KVCache]:\n        cpu_cache: List[KVCache] = []\n        key_block_shape = self.get_key_block_shape()\n        value_block_shape = self.get_value_block_shape()\n        for _ in range(self.num_layers):\n            key_blocks = torch.empty(\n                size=(self.num_cpu_blocks, *key_block_shape),\n                dtype=self.dtype,\n                pin_memory=True,\n            )\n            value_blocks = torch.empty(\n                size=(self.num_cpu_blocks, *value_block_shape),\n                dtype=self.dtype,\n                pin_memory=True,\n            )\n            cpu_cache.append((key_blocks, value_blocks))\n        return cpu_cache\n\n    def _swap(\n        self,\n        src: List[KVCache],\n        dst: List[KVCache],\n        src_to_dst: Dict[int, int],\n    ) -> None:\n        with torch.cuda.stream(self.cache_stream):\n            for i in range(self.num_layers):\n                src_key_cache, src_value_cache = src[i]\n                dst_key_cache, dst_value_cache = dst[i]\n                # Copy the key blocks.\n                cache_ops.swap_blocks(\n                    src_key_cache, dst_key_cache, src_to_dst)\n                # Copy the value blocks.\n                cache_ops.swap_blocks(\n                    src_value_cache, dst_value_cache, src_to_dst)\n                event = self.events[i]\n                event.record(stream=self.cache_stream)\n\n    def swap_in(self, src_to_dst: Dict[int, int]) -> None:\n        self._swap(self.cpu_cache, self.gpu_cache, src_to_dst)\n\n    def swap_out(self, src_to_dst: Dict[int, int]) -> None:\n        self._swap(self.gpu_cache, self.cpu_cache, src_to_dst)\n\n    def _copy(\n        self,\n        src: List[KVCache],\n        dst: List[KVCache],\n        src_to_dsts: Dict[int, List[int]],\n    ) -> None:\n        with torch.cuda.stream(self.cache_stream):\n            for i in range(self.num_layers):\n                src_key_cache, src_value_cache = src[i]\n                dst_key_cache, dst_value_cache = dst[i]\n                # Copy the key blocks.\n                cache_ops.copy_blocks(\n                    src_key_cache, dst_key_cache, src_to_dsts)\n                # Copy the value blocks.\n                cache_ops.copy_blocks(\n                    src_value_cache, dst_value_cache, src_to_dsts)\n                event = self.events[i]\n                event.record(stream=self.cache_stream)\n\n    def copy(self, src_to_dsts: Dict[int, List[int]]) -> None:\n        self._copy(self.gpu_cache, self.gpu_cache, src_to_dsts)\n",
      "diff": "diff --git a/cacheflow/worker/cache_engine.py b/cacheflow/worker/cache_engine.py\nindex 164b2a2a6..addde3883 100644\n--- a/cacheflow/worker/cache_engine.py\n+++ b/cacheflow/worker/cache_engine.py\n@@ -120,24 +120,8 @@ class CacheEngine:\n     def swap_out(self, src_to_dst: Dict[int, int]) -> None:\n         self._swap(self.gpu_cache, self.cpu_cache, src_to_dst)\n \n-    def _copy(\n-        self,\n-        src: List[KVCache],\n-        dst: List[KVCache],\n-        src_to_dsts: Dict[int, List[int]],\n-    ) -> None:\n-        with torch.cuda.stream(self.cache_stream):\n-            for i in range(self.num_layers):\n-                src_key_cache, src_value_cache = src[i]\n-                dst_key_cache, dst_value_cache = dst[i]\n-                # Copy the key blocks.\n-                cache_ops.copy_blocks(\n-                    src_key_cache, dst_key_cache, src_to_dsts)\n-                # Copy the value blocks.\n-                cache_ops.copy_blocks(\n-                    src_value_cache, dst_value_cache, src_to_dsts)\n-                event = self.events[i]\n-                event.record(stream=self.cache_stream)\n-\n     def copy(self, src_to_dsts: Dict[int, List[int]]) -> None:\n-        self._copy(self.gpu_cache, self.gpu_cache, src_to_dsts)\n+        key_caches = [key_cache for key_cache, _ in self.gpu_cache]\n+        value_caches = [value_cache for _, value_cache in self.gpu_cache]\n+        # NOTE(woosuk): This operation implicitly synchronizes the CPU and GPU.\n+        cache_ops.copy_blocks(key_caches, value_caches, src_to_dsts)",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 21
    },
    {
      "file_path": "csrc/cache.cpp",
      "old_content": "#include <torch/extension.h>\n\n#include <map>\n#include <vector>\n\nvoid swap_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, int64_t>& block_mapping);\n\nvoid copy_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, std::vector<int64_t>>& block_mapping);\n\nvoid reshape_and_cache(\n  torch::Tensor& key,\n  torch::Tensor& value,\n  torch::Tensor& key_cache,\n  torch::Tensor& value_cache,\n  torch::Tensor& slot_mapping);\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\n    \"swap_blocks\",\n    &swap_blocks,\n    \"Swap in (out) the cache blocks from src to dst\");\n  m.def(\n    \"copy_blocks\",\n    &copy_blocks,\n    \"Copy the cache blocks from src to dst\");\n  m.def(\n    \"reshape_and_cache\",\n    &reshape_and_cache,\n    \"Reshape the key and value tensors and cache them\");\n}\n",
      "diff": "diff --git a/csrc/cache.cpp b/csrc/cache.cpp\nindex fcf8b69fe..907736a98 100644\n--- a/csrc/cache.cpp\n+++ b/csrc/cache.cpp\n@@ -9,8 +9,8 @@ void swap_blocks(\n   const std::map<int64_t, int64_t>& block_mapping);\n \n void copy_blocks(\n-  torch::Tensor& src,\n-  torch::Tensor& dst,\n+  std::vector<torch::Tensor>& key_caches,\n+  std::vector<torch::Tensor>& value_caches,\n   const std::map<int64_t, std::vector<int64_t>>& block_mapping);\n \n void reshape_and_cache(",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    },
    {
      "file_path": "csrc/cache_kernels.cu",
      "old_content": "#include <torch/extension.h>\n#include <ATen/cuda/CUDAContext.h>\n\n#include <algorithm>\n#include <cassert>\n#include <map>\n#include <vector>\n\nvoid swap_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, int64_t>& block_mapping) {\n  torch::Device src_device = src.device();\n  torch::Device dst_device = dst.device();\n  cudaMemcpyKind memcpy_type;\n  if (src_device.is_cuda() && dst_device.is_cuda()) {\n    assert(src_device.index() == dst_device.index());\n    memcpy_type = cudaMemcpyDeviceToDevice;\n  } else if (src_device.is_cuda() && dst_device.is_cpu()) {\n    memcpy_type = cudaMemcpyDeviceToHost;\n  } else if (src_device.is_cpu() && dst_device.is_cuda()) {\n    memcpy_type = cudaMemcpyHostToDevice;\n  } else {\n    assert(false);\n  }\n\n  void *src_ptr = src.data_ptr();\n  void *dst_ptr = dst.data_ptr();\n\n  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  for (const auto& pair : block_mapping) {\n    int64_t src_block_number = pair.first;\n    int64_t dst_block_number = pair.second;\n    int64_t src_offset = src_block_number * block_size_in_bytes;\n    int64_t dst_offset = dst_block_number * block_size_in_bytes;\n    cudaMemcpyAsync(\n      dst_ptr + dst_offset,\n      src_ptr + src_offset,\n      block_size_in_bytes,\n      memcpy_type,\n      stream);\n  }\n}\n\nvoid copy_blocks(\n  torch::Tensor& src,\n  torch::Tensor& dst,\n  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n  torch::Device src_device = src.device();\n  torch::Device dst_device = dst.device();\n  assert(src_device.is_cuda() && dst_device.is_cuda());\n  cudaMemcpyKind memcpy_type = cudaMemcpyDeviceToDevice;\n\n  void *src_ptr = src.data_ptr();\n  void *dst_ptr = dst.data_ptr();\n\n  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  for (const auto& pair : block_mapping) {\n    int64_t src_block_number = pair.first;\n    for (int64_t dst_block_number : pair.second) {\n      int64_t src_offset = src_block_number * block_size_in_bytes;\n      int64_t dst_offset = dst_block_number * block_size_in_bytes;\n      cudaMemcpyAsync(\n        dst_ptr + dst_offset,\n        src_ptr + src_offset,\n        block_size_in_bytes,\n        memcpy_type,\n        stream);\n    }\n  }\n}\n\nnamespace cacheflow {\n\ntemplate<typename scalar_t>\n__global__ void reshape_and_cache_kernel(\n  const scalar_t* __restrict__ key,     // [num_tokens, num_heads, head_size]\n  const scalar_t* __restrict__ value,   // [num_tokens, num_heads, head_size]\n  scalar_t* __restrict__ key_cache,     // [num_blocks, num_heads, head_size/x, block_size, x]\n  scalar_t* __restrict__ value_cache,   // [num_blocks, num_heads, head_size, block_size]\n  const int* __restrict__ slot_mapping, // [num_tokens]\n  const int key_stride,\n  const int value_stride,\n  const int num_heads,\n  const int head_size,\n  const int block_size,\n  const int x) {\n  const int token_idx = blockIdx.x;\n  const int slot_idx = slot_mapping[token_idx];\n  const int block_idx = slot_idx / block_size;\n  const int block_offset = slot_idx % block_size;\n\n  const int n = num_heads * head_size;\n  for (int i = threadIdx.x; i < n; i += blockDim.x) {\n    const int src_key_idx = token_idx * key_stride + i;\n    const int src_value_idx = token_idx * value_stride + i;\n\n    const int head_idx = i / head_size;\n    const int head_offset = i % head_size;\n    const int x_idx = head_offset / x;\n    const int x_offset = head_offset % x;\n\n    const int tgt_key_idx = block_idx * num_heads * (head_size / x) * block_size * x\n                            + head_idx * (head_size / x) * block_size * x\n                            + x_idx * block_size * x\n                            + block_offset * x\n                            + x_offset;\n    const int tgt_value_idx = block_idx * num_heads * head_size * block_size\n                              + head_idx * head_size * block_size\n                              + head_offset * block_size\n                              + block_offset;\n    key_cache[tgt_key_idx] = __ldg(&key[src_key_idx]);\n    value_cache[tgt_value_idx] = __ldg(&value[src_value_idx]);\n  }\n}\n\n} // namespace cacheflow\n\nvoid reshape_and_cache(\n  torch::Tensor& key,           // [num_tokens, num_heads, head_size]\n  torch::Tensor& value,         // [num_tokens, num_heads, head_size]\n  torch::Tensor& key_cache,     // [num_blocks, num_heads, head_size/x, block_size, x]\n  torch::Tensor& value_cache,   // [num_blocks, num_heads, head_size, block_size]\n  torch::Tensor& slot_mapping)  // [num_tokens]\n{\n  int num_tokens = key.size(0);\n  int num_heads = key.size(1);\n  int head_size = key.size(2);\n  int block_size = key_cache.size(3);\n  int x = key_cache.size(4);\n\n  int key_stride = key.stride(0);\n  int value_stride = value.stride(0);\n\n  dim3 grid(num_tokens);\n  dim3 block(std::min(num_heads * head_size, 512));\n  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n    key.scalar_type(),\n    \"reshape_and_cache_kernel\",\n    [&] {\n      cacheflow::reshape_and_cache_kernel<scalar_t><<<grid, block, 0, stream>>>(\n        key.data_ptr<scalar_t>(),\n        value.data_ptr<scalar_t>(),\n        key_cache.data_ptr<scalar_t>(),\n        value_cache.data_ptr<scalar_t>(),\n        slot_mapping.data_ptr<int>(),\n        key_stride,\n        value_stride,\n        num_heads,\n        head_size,\n        block_size,\n        x);\n    });\n}\n",
      "diff": "diff --git a/csrc/cache_kernels.cu b/csrc/cache_kernels.cu\nindex 8b5537c47..3a34ba578 100644\n--- a/csrc/cache_kernels.cu\n+++ b/csrc/cache_kernels.cu\n@@ -43,33 +43,93 @@ void swap_blocks(\n   }\n }\n \n-void copy_blocks(\n-  torch::Tensor& src,\n-  torch::Tensor& dst,\n-  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n-  torch::Device src_device = src.device();\n-  torch::Device dst_device = dst.device();\n-  assert(src_device.is_cuda() && dst_device.is_cuda());\n-  cudaMemcpyKind memcpy_type = cudaMemcpyDeviceToDevice;\n+namespace cacheflow {\n \n-  void *src_ptr = src.data_ptr();\n-  void *dst_ptr = dst.data_ptr();\n+// Grid: (num_layers, num_pairs)\n+template<typename scalar_t>\n+__global__ void copy_blocks_kernel(\n+  int64_t* key_cache_ptrs,\n+  int64_t* value_cache_ptrs,\n+  const int* __restrict__ block_mapping,\n+  const int numel_per_block) {\n+  const int layer_idx = blockIdx.x;\n+  const int pair_idx = blockIdx.y;\n+\n+  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);\n+  scalar_t* value_cache = reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);\n+  int src_block_number = block_mapping[2 * pair_idx];\n+  int dst_block_number = block_mapping[2 * pair_idx + 1];\n+\n+  const int src_block_offset = src_block_number * numel_per_block;\n+  const int dst_block_offset = dst_block_number * numel_per_block;\n+  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n+    int src_offset = src_block_offset + i;\n+    int dst_offset = dst_block_offset + i;\n+    key_cache[dst_offset] = key_cache[src_offset];\n+  }\n+  for (int i = threadIdx.x; i < numel_per_block; i += blockDim.x) {\n+    int src_offset = src_block_offset + i;\n+    int dst_offset = dst_block_offset + i;\n+    value_cache[dst_offset] = value_cache[src_offset];\n+  }\n+}\n \n-  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();\n-  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+} // namespace cacheflow\n+\n+void copy_blocks(\n+  std::vector<torch::Tensor>& key_caches,\n+  std::vector<torch::Tensor>& value_caches,\n+  const std::map<int64_t, std::vector<int64_t>>& block_mapping) {\n+  int num_layers = key_caches.size();\n+  TORCH_CHECK(num_layers == value_caches.size());\n+  if (num_layers == 0) {\n+    return;\n+  }\n+  torch::Device cache_device = key_caches[0].device();\n+  TORCH_CHECK(cache_device.is_cuda());\n+\n+  // Create data structures for the kernel.\n+  // Create an array of pointers to the key and value caches.\n+  int64_t key_cache_ptrs[num_layers];\n+  int64_t value_cache_ptrs[num_layers];\n+  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {\n+    key_cache_ptrs[layer_idx] = reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());\n+    value_cache_ptrs[layer_idx] = reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());\n+  }\n+  // Create block mapping array.\n+  std::vector<int> block_mapping_vec;\n   for (const auto& pair : block_mapping) {\n-    int64_t src_block_number = pair.first;\n-    for (int64_t dst_block_number : pair.second) {\n-      int64_t src_offset = src_block_number * block_size_in_bytes;\n-      int64_t dst_offset = dst_block_number * block_size_in_bytes;\n-      cudaMemcpyAsync(\n-        dst_ptr + dst_offset,\n-        src_ptr + src_offset,\n-        block_size_in_bytes,\n-        memcpy_type,\n-        stream);\n+    int src_block_number = pair.first;\n+    for (int dst_block_number : pair.second) {\n+      block_mapping_vec.push_back(src_block_number);\n+      block_mapping_vec.push_back(dst_block_number);\n     }\n   }\n+  int* block_mapping_array = block_mapping_vec.data();\n+  int num_pairs = block_mapping_vec.size() / 2;\n+\n+  // Move the data structures to the GPU.\n+  // NOTE: This synchronizes the CPU and GPU.\n+  torch::Tensor key_cache_ptrs_tensor = torch::from_blob(\n+    key_cache_ptrs, {num_layers}, torch::kInt64).to(cache_device);\n+  torch::Tensor value_cache_ptrs_tensor = torch::from_blob(\n+    value_cache_ptrs, {num_layers}, torch::kInt64).to(cache_device);\n+  torch::Tensor block_mapping_tensor = torch::from_blob(\n+    block_mapping_array, {2 * num_pairs}, torch::kInt).to(cache_device);\n+\n+  // Launch the kernel.\n+  const int numel_per_block = key_caches[0][0].numel();\n+  dim3 grid(num_layers, num_pairs);\n+  dim3 block(std::min(1024, numel_per_block));\n+  const cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n+  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n+    key_caches[0].scalar_type(), \"copy_blocks_kernel\", ([&] {\n+      cacheflow::copy_blocks_kernel<scalar_t><<<grid, block, 0, stream>>>(\n+        key_cache_ptrs_tensor.data_ptr<int64_t>(),\n+        value_cache_ptrs_tensor.data_ptr<int64_t>(),\n+        block_mapping_tensor.data_ptr<int>(),\n+        numel_per_block);\n+    }));\n }\n \n namespace cacheflow {",
      "change_type": "modified",
      "lines_added": 83,
      "lines_removed": 23
    },
    {
      "file_path": "tests/kernels/cache.py",
      "old_content": "import random\n\nimport torch\n\nfrom cacheflow import cache_ops\n\n\ndef test_reshape_and_cache(\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n) -> None:\n    num_slots = block_size * num_blocks\n    slot_mapping = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.int, device='cuda')\n\n    qkv = torch.randn(\n        num_tokens, 3, num_heads, head_size, dtype=dtype, device='cuda')\n    _, key, value = qkv.unbind(dim=1)\n\n    x = 16 // torch.tensor([], dtype=dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_cache = torch.randn(size=key_cache_shape, dtype=dtype, device='cuda')\n    cloned_key_cache = key_cache.clone()\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_cache = torch.randn(\n        size=value_cache_shape, dtype=dtype, device='cuda')\n    cloned_value_cache = value_cache.clone()\n\n    cache_ops.reshape_and_cache(key, value, key_cache, value_cache, slot_mapping)\n\n    for i in range(num_tokens):\n        reshaped_key = key.reshape(num_tokens, num_heads, head_size // x, x)\n        block_idx = torch.div(slot_mapping[i], block_size, rounding_mode='floor')\n        block_offset = slot_mapping[i] % block_size\n        cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\n        cloned_value_cache[block_idx, :, :, block_offset] = value[i]\n\n    assert torch.allclose(key_cache, cloned_key_cache)\n    assert torch.allclose(value_cache, cloned_value_cache)\n\n\n@torch.inference_mode()\ndef test_cache() -> None:\n    test_reshape_and_cache(\n        num_tokens=3, num_heads=2, head_size=16, block_size=8, num_blocks=2,\n        dtype=torch.half)\n\n\nif __name__ == '__main__':\n    test_cache()\n",
      "diff": "diff --git a/tests/kernels/cache.py b/tests/kernels/cache.py\nindex d6b1c3d2d..89f14cca8 100644\n--- a/tests/kernels/cache.py\n+++ b/tests/kernels/cache.py\n@@ -5,6 +5,61 @@ import torch\n from cacheflow import cache_ops\n \n \n+def test_copy_blocks(\n+    num_mappings: int,\n+    num_layers: int,\n+    num_heads: int,\n+    head_size: int,\n+    block_size: int,\n+    num_blocks: int,\n+    dtype: torch.dtype,\n+) -> None:\n+    # Generate random block mappings.\n+    src_blocks = random.sample(range(num_blocks), num_mappings)\n+    remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\n+    dst_blocks = random.sample(remainig_blocks, num_mappings)\n+    block_mapping = {src: [dst] for src, dst in zip(src_blocks, dst_blocks)}\n+\n+    # Create the KV cache.\n+    x = 16 // torch.tensor([], dtype=dtype).element_size()\n+    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n+    key_caches = []\n+    for _ in range(num_layers):\n+        key_cache = torch.randn(\n+            size=key_cache_shape, dtype=dtype, device='cuda')\n+        key_caches.append(key_cache)\n+    cloned_key_caches = []\n+    for key_cache in key_caches:\n+        cloned_key_caches.append(key_cache.clone())\n+\n+    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n+    value_caches = []\n+    for _ in range(num_layers):\n+        value_cache = torch.randn(\n+            size=value_cache_shape, dtype=dtype, device='cuda')\n+        value_caches.append(value_cache)\n+    cloned_value_caches = []\n+    for value_cache in value_caches:\n+        cloned_value_caches.append(value_cache.clone())\n+\n+    # Call the copy blocks kernel.\n+    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n+\n+    # Reference implementation.\n+    for src, dsts in block_mapping.items():\n+        for dst in dsts:\n+            for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n+                cloned_key_cache[dst] = cloned_key_cache[src]\n+            for value_cache, cloned_value_cache in zip(value_caches, cloned_value_caches):\n+                cloned_value_cache[dst] = cloned_value_cache[src]\n+\n+    # Compare the results.\n+    for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n+        assert torch.allclose(key_cache, cloned_key_cache)\n+    for value_cache, cloned_value_cache in zip(value_caches, cloned_value_caches):\n+        assert torch.allclose(value_cache, cloned_value_cache)\n+\n+\n def test_reshape_and_cache(\n     num_tokens: int,\n     num_heads: int,\n@@ -46,6 +101,9 @@ def test_reshape_and_cache(\n \n @torch.inference_mode()\n def test_cache() -> None:\n+    test_copy_blocks(\n+        num_mappings=23, num_layers=7, num_heads=17, head_size=16,\n+        block_size=8, num_blocks=1024, dtype=torch.half)\n     test_reshape_and_cache(\n         num_tokens=3, num_heads=2, head_size=16, block_size=8, num_blocks=2,\n         dtype=torch.half)",
      "change_type": "modified",
      "lines_added": 59,
      "lines_removed": 1
    }
  ],
  "affected_apis": [
    "cacheflow.cache_ops.copy_blocks",
    "cacheflow.worker.cache_engine.CacheEngine.copy",
    "cacheflow.models.sample._sample_from_generation_tokens"
  ],
  "summary": {
    "total_files": 6,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 6
  },
  "csv_metadata": {
    "category": "kernel-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "TRUE",
    "is_test_actually_there": "Not really sure about this one",
    "is_benchmark_actually_there": "",
    "sample_clues": "benchmark_latency, cache, cache_engine"
  }
}