{
  "commit_hash": "9323a3153b20d4a2ca7ac04a2784609d6ce656e0",
  "parent_hash": "3257d449fa0fd3e05aa20cc8c5fff79ad101984f",
  "message": "[Core][Performance] Add XGrammar support for guided decoding and set it as default (#10785)\n\nSigned-off-by: Aaron Pham <contact@aarnphm.xyz>\nSigned-off-by: mgoin <michael@neuralmagic.com>\nCo-authored-by: mgoin <michael@neuralmagic.com>",
  "author": "Aaron Pham <contact@aarnphm.xyz>",
  "date": "2024-12-03 15:17:00 +0800",
  "files_changed": [
    {
      "file_path": "docs/source/conf.py",
      "old_content": "# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport inspect\nimport logging\nimport os\nimport sys\nfrom typing import List\n\nimport requests\nfrom sphinx.ext import autodoc\n\nlogger = logging.getLogger(__name__)\nsys.path.append(os.path.abspath(\"../..\"))\n\n# -- Project information -----------------------------------------------------\n\nproject = 'vLLM'\ncopyright = '2024, vLLM Team'\nauthor = 'the vLLM Team'\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx.ext.linkcode\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx_copybutton\",\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"myst_parser\",\n    \"sphinxarg.ext\",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns: List[str] = [\"**/*.template.rst\"]\n\n# Exclude the prompt \"$\" when copying code\ncopybutton_prompt_text = r\"\\$ \"\ncopybutton_prompt_is_regexp = True\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_title = project\nhtml_theme = 'sphinx_book_theme'\nhtml_logo = 'assets/logos/vllm-logo-text-light.png'\nhtml_theme_options = {\n    'path_to_docs': 'docs/source',\n    'repository_url': 'https://github.com/vllm-project/vllm',\n    'use_repository_button': True,\n    'use_edit_page_button': True,\n}\nhtml_static_path = [\"_static\"]\nhtml_js_files = [\"custom.js\"]\n\n# see https://docs.readthedocs.io/en/stable/reference/environment-variables.html # noqa\nREADTHEDOCS_VERSION_TYPE = os.environ.get('READTHEDOCS_VERSION_TYPE')\nif READTHEDOCS_VERSION_TYPE == \"tag\":\n    # remove the warning banner if the version is a tagged release\n    header_file = os.path.join(os.path.dirname(__file__),\n                               \"_templates/sections/header.html\")\n    # The file might be removed already if the build is triggered multiple times\n    # (readthedocs build both HTML and PDF versions separately)\n    if os.path.exists(header_file):\n        os.remove(header_file)\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n# html_static_path = ['_static']\n\n\n# Generate additional rst documentation here.\ndef setup(app):\n    from docs.source.generate_examples import generate_examples\n    generate_examples()\n\n\n_cached_base: str = \"\"\n_cached_branch: str = \"\"\n\n\ndef get_repo_base_and_branch(pr_number):\n    global _cached_base, _cached_branch\n    if _cached_base and _cached_branch:\n        return _cached_base, _cached_branch\n\n    url = f\"https://api.github.com/repos/vllm-project/vllm/pulls/{pr_number}\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        data = response.json()\n        _cached_base = data['head']['repo']['full_name']\n        _cached_branch = data['head']['ref']\n        return _cached_base, _cached_branch\n    else:\n        logger.error(\"Failed to fetch PR details: %s\", response)\n        return None, None\n\n\ndef linkcode_resolve(domain, info):\n    if domain != 'py':\n        return None\n    if not info['module']:\n        return None\n    filename = info['module'].replace('.', '/')\n    module = info['module']\n\n    # try to determine the correct file and line number to link to\n    obj = sys.modules[module]\n\n    # get as specific as we can\n    lineno: int = 0\n    filename: str = \"\"\n    try:\n        for part in info['fullname'].split('.'):\n            obj = getattr(obj, part)\n\n            if not (inspect.isclass(obj) or inspect.isfunction(obj)\n                    or inspect.ismethod(obj)):\n                obj = obj.__class__  # Get the class of the instance\n\n            lineno = inspect.getsourcelines(obj)[1]\n            filename = (inspect.getsourcefile(obj)\n                        or f\"{filename}.py\").split(\"vllm/\", 1)[1]\n    except Exception:\n        # For some things, like a class member, won't work, so\n        # we'll use the line number of the parent (the class)\n        pass\n\n    if filename.startswith(\"checkouts/\"):\n        # a PR build on readthedocs\n        pr_number = filename.split(\"/\")[1]\n        filename = filename.split(\"/\", 2)[2]\n        base, branch = get_repo_base_and_branch(pr_number)\n        if base and branch:\n            return f\"https://github.com/{base}/blob/{branch}/{filename}#L{lineno}\"\n\n    # Otherwise, link to the source file on the main branch\n    return f\"https://github.com/vllm-project/vllm/blob/main/{filename}#L{lineno}\"\n\n\n# Mock out external dependencies here, otherwise the autodoc pages may be blank.\nautodoc_mock_imports = [\n    \"compressed_tensors\",\n    \"cpuinfo\",\n    \"cv2\",\n    \"torch\",\n    \"transformers\",\n    \"psutil\",\n    \"prometheus_client\",\n    \"sentencepiece\",\n    \"vllm._C\",\n    \"PIL\",\n    \"numpy\",\n    'triton',\n    \"tqdm\",\n    \"tensorizer\",\n    \"pynvml\",\n    \"outlines\",\n    \"librosa\",\n    \"soundfile\",\n    \"gguf\",\n    \"lark\",\n    \"decord\",\n]\n\nfor mock_target in autodoc_mock_imports:\n    if mock_target in sys.modules:\n        logger.info(\n            \"Potentially problematic mock target (%s) found; \"\n            \"autodoc_mock_imports cannot mock modules that have already \"\n            \"been loaded into sys.modules when the sphinx build starts.\",\n            mock_target)\n\n\nclass MockedClassDocumenter(autodoc.ClassDocumenter):\n    \"\"\"Remove note about base class when a class is derived from object.\"\"\"\n\n    def add_line(self, line: str, source: str, *lineno: int) -> None:\n        if line == \"   Bases: :py:class:`object`\":\n            return\n        super().add_line(line, source, *lineno)\n\n\nautodoc.ClassDocumenter = MockedClassDocumenter\n\nintersphinx_mapping = {\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"typing_extensions\":\n    (\"https://typing-extensions.readthedocs.io/en/latest\", None),\n    \"aiohttp\": (\"https://docs.aiohttp.org/en/stable\", None),\n    \"pillow\": (\"https://pillow.readthedocs.io/en/stable\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable\", None),\n    \"torch\": (\"https://pytorch.org/docs/stable\", None),\n    \"psutil\": (\"https://psutil.readthedocs.io/en/stable\", None),\n}\n\nautodoc_preserve_defaults = True\nautodoc_warningiserror = True\n\nnavigation_with_keys = False\n",
      "diff": "diff --git a/docs/source/conf.py b/docs/source/conf.py\nindex 4a1a5fb45..e9d9ac68c 100644\n--- a/docs/source/conf.py\n+++ b/docs/source/conf.py\n@@ -178,6 +178,7 @@ autodoc_mock_imports = [\n     \"tensorizer\",\n     \"pynvml\",\n     \"outlines\",\n+    \"xgrammar,\"\n     \"librosa\",\n     \"soundfile\",\n     \"gguf\",",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 1
    },
    {
      "file_path": "requirements-common.txt",
      "old_content": "psutil\nsentencepiece  # Required for LLaMA tokenizer.\nnumpy < 2.0.0\nrequests >= 2.26.0\ntqdm\npy-cpuinfo\ntransformers >= 4.45.2  # Required for Llama 3.2 and Qwen2-VL.\ntokenizers >= 0.19.1  # Required for Llama 3.\nprotobuf # Required by LlamaTokenizer.\nfastapi >= 0.107.0, < 0.113.0; python_version < '3.9'\nfastapi >= 0.107.0, != 0.113.*, != 0.114.0; python_version >= '3.9'\naiohttp\nopenai >= 1.45.0 # Ensure modern openai package (ensure types module present and max_completion_tokens field support)\nuvicorn[standard]\npydantic >= 2.9  # Required for fastapi >= 0.113.0\npillow  # Required for image processing\nprometheus_client >= 0.18.0\nprometheus-fastapi-instrumentator >= 7.0.0\ntiktoken >= 0.6.0  # Required for DBRX tokenizer\nlm-format-enforcer >= 0.10.9, < 0.11\noutlines >= 0.0.43, < 0.1\ntyping_extensions >= 4.10\nfilelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\npartial-json-parser # used for parsing partial JSON outputs\npyzmq\nmsgspec\ngguf == 0.10.0\nimportlib_metadata\nmistral_common[opencv] >= 1.5.0\npyyaml\nsix>=1.16.0; python_version > '3.11' # transitive dependency of pandas that needs to be the latest version for python 3.12\nsetuptools>=74.1.1; python_version > '3.11' # Setuptools is used by triton, we need to ensure a modern version is installed for 3.12+ so that it does not try to import distutils, which was removed in 3.12\neinops # Required for Qwen2-VL.\ncompressed-tensors == 0.8.0 # required for compressed-tensors\n",
      "diff": "diff --git a/requirements-common.txt b/requirements-common.txt\nindex 02e3d65fb..818f72e14 100644\n--- a/requirements-common.txt\n+++ b/requirements-common.txt\n@@ -19,6 +19,7 @@ prometheus-fastapi-instrumentator >= 7.0.0\n tiktoken >= 0.6.0  # Required for DBRX tokenizer\n lm-format-enforcer >= 0.10.9, < 0.11\n outlines >= 0.0.43, < 0.1\n+xgrammar\n typing_extensions >= 4.10\n filelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\n partial-json-parser # used for parsing partial JSON outputs",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 1
    },
    {
      "file_path": "tests/entrypoints/llm/test_guided_generate.py",
      "old_content": "import json\nimport re\nimport weakref\n\nimport jsonschema\nimport pytest\n\nfrom vllm.distributed import cleanup_dist_env_and_memory\nfrom vllm.entrypoints.llm import LLM\nfrom vllm.outputs import RequestOutput\nfrom vllm.sampling_params import GuidedDecodingParams, SamplingParams\n\nMODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n\n\n@pytest.fixture(scope=\"module\")\ndef llm():\n    # pytest caches the fixture so we use weakref.proxy to\n    # enable garbage collection\n    llm = LLM(model=MODEL_NAME, max_model_len=1024)\n\n    with llm.deprecate_legacy_api():\n        yield weakref.proxy(llm)\n        del llm\n    cleanup_dist_env_and_memory()\n\n\n@pytest.mark.skip_global_cleanup\ndef test_guided_regex(sample_regex, llm):\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        top_p=0.95,\n        guided_decoding=GuidedDecodingParams(regex=sample_regex))\n    outputs = llm.generate(prompts=[\n        f\"Give an example IPv4 address with this regex: {sample_regex}\"\n    ] * 2,\n                           sampling_params=sampling_params,\n                           use_tqdm=True)\n\n    assert outputs is not None\n    for output in outputs:\n        assert output is not None\n        assert isinstance(output, RequestOutput)\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(generated_text)\n        assert generated_text is not None\n        assert re.fullmatch(sample_regex, generated_text) is not None\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n\n@pytest.mark.skip_global_cleanup\ndef test_guided_json_completion(sample_json_schema, llm):\n    sampling_params = SamplingParams(\n        temperature=1.0,\n        max_tokens=1000,\n        guided_decoding=GuidedDecodingParams(json=sample_json_schema))\n    outputs = llm.generate(prompts=[\n        f\"Give an example JSON for an employee profile \"\n        f\"that fits this schema: {sample_json_schema}\"\n    ] * 2,\n                           sampling_params=sampling_params,\n                           use_tqdm=True)\n\n    assert outputs is not None\n\n    for output in outputs:\n        assert output is not None\n        assert isinstance(output, RequestOutput)\n        prompt = output.prompt\n\n        generated_text = output.outputs[0].text\n        assert generated_text is not None\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n        output_json = json.loads(generated_text)\n        jsonschema.validate(instance=output_json, schema=sample_json_schema)\n\n\n@pytest.mark.skip_global_cleanup\ndef test_guided_choice_completion(sample_guided_choice, llm):\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        top_p=0.95,\n        guided_decoding=GuidedDecodingParams(choice=sample_guided_choice))\n    outputs = llm.generate(\n        prompts=\"The best language for type-safe systems programming is \",\n        sampling_params=sampling_params,\n        use_tqdm=True)\n\n    assert outputs is not None\n    for output in outputs:\n        assert output is not None\n        assert isinstance(output, RequestOutput)\n        prompt = output.prompt\n        generated_text = output.outputs[0].text\n        print(generated_text)\n        assert generated_text is not None\n        assert generated_text in sample_guided_choice\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n\n@pytest.mark.skip_global_cleanup\ndef test_guided_grammar(sample_sql_statements, llm):\n\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        top_p=0.95,\n        max_tokens=1000,\n        guided_decoding=GuidedDecodingParams(grammar=sample_sql_statements))\n    outputs = llm.generate(\n        prompts=(\"Generate a sql state that select col_1 from \"\n                 \"table_1 where it is equals to 1\"),\n        sampling_params=sampling_params,\n        use_tqdm=True,\n    )\n\n    assert outputs is not None\n    for output in outputs:\n        assert output is not None\n        assert isinstance(output, RequestOutput)\n        prompt = output.prompt\n\n        generated_text = output.outputs[0].text\n        assert generated_text is not None\n        # use Lark to parse the output, and make sure it's a valid parse tree\n        from lark import Lark\n        parser = Lark(sample_sql_statements)\n        parser.parse(generated_text)\n\n        # remove spaces for comparison b/c we removed them in the grammar\n        ground_truth = \"SELECT col_1 from table_1 where col_1 = 1\".replace(\n            \" \", \"\")\n\n        assert generated_text.strip() == ground_truth\n\n        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n\n\n@pytest.mark.skip_global_cleanup\ndef test_guided_options_request_deprecation_warning(sample_regex, llm):\n    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\n    with pytest.warns(DeprecationWarning, match=\"guided_options_request\"):\n        llm.generate(prompts=\"This should fail\",\n                     sampling_params=sampling_params,\n                     use_tqdm=True,\n                     guided_options_request=dict(guided_regex=sample_regex))\n\n\n@pytest.mark.skip_global_cleanup\ndef test_validation_against_both_guided_decoding_options(sample_regex, llm):\n    sampling_params = SamplingParams(\n        temperature=0.8,\n        top_p=0.95,\n        guided_decoding=GuidedDecodingParams(regex=sample_regex))\n\n    with pytest.raises(ValueError, match=\"Cannot set both\"):\n        llm.generate(prompts=\"This should fail\",\n                     sampling_params=sampling_params,\n                     use_tqdm=True,\n                     guided_options_request=dict(guided_regex=sample_regex))\n",
      "diff": "diff --git a/tests/entrypoints/llm/test_guided_generate.py b/tests/entrypoints/llm/test_guided_generate.py\nindex 67c79415f..c3706f696 100644\n--- a/tests/entrypoints/llm/test_guided_generate.py\n+++ b/tests/entrypoints/llm/test_guided_generate.py\n@@ -159,3 +159,30 @@ def test_validation_against_both_guided_decoding_options(sample_regex, llm):\n                      sampling_params=sampling_params,\n                      use_tqdm=True,\n                      guided_options_request=dict(guided_regex=sample_regex))\n+\n+\n+@pytest.mark.skip_global_cleanup\n+def test_guided_json_object(llm):\n+    sampling_params = SamplingParams(\n+        temperature=1.0,\n+        max_tokens=100,\n+        guided_decoding=GuidedDecodingParams(json_object=True))\n+\n+    outputs = llm.generate(\n+        prompts=(\"Generate a JSON object describing a person with name \"\n+                 \"and age for John Smith who is 31 years old.\"),\n+        sampling_params=sampling_params,\n+        use_tqdm=True)\n+\n+    assert outputs is not None\n+    for output in outputs:\n+        assert output is not None\n+        assert isinstance(output, RequestOutput)\n+\n+        generated_text = output.outputs[0].text\n+        print(generated_text)\n+        assert generated_text is not None\n+\n+        # Parse to verify it is valid JSON\n+        parsed_json = json.loads(generated_text)\n+        assert isinstance(parsed_json, dict)",
      "change_type": "modified",
      "lines_added": 28,
      "lines_removed": 1
    },
    {
      "file_path": "tests/model_executor/test_guided_processors.py",
      "old_content": "import pytest\nimport torch\nfrom transformers import AutoTokenizer\n\nfrom vllm.model_executor.guided_decoding import (\n    get_guided_decoding_logits_processor)\nfrom vllm.model_executor.guided_decoding.outlines_logits_processors import (\n    JSONLogitsProcessor, RegexLogitsProcessor)\nfrom vllm.sampling_params import GuidedDecodingParams\n\n\ndef test_guided_logits_processors(sample_regex, sample_json_schema):\n    \"\"\"Basic unit test for RegexLogitsProcessor and JSONLogitsProcessor.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\n    regex_LP = RegexLogitsProcessor(sample_regex, tokenizer)\n    json_LP = JSONLogitsProcessor(sample_json_schema,\n                                  tokenizer,\n                                  whitespace_pattern=None)\n\n    token_ids = tokenizer.encode(\n        f\"Give an example IPv4 address with this regex: {sample_regex}\")\n    tensor = torch.rand(32000)\n    original_tensor = torch.clone(tensor)\n    regex_LP(token_ids, tensor)\n    assert tensor.shape == original_tensor.shape\n    assert not torch.allclose(tensor, original_tensor)\n\n    token_ids = tokenizer.encode(\n        f\"Give an employee profile that fits this schema: {sample_json_schema}\"\n    )\n    tensor = torch.rand(32000)\n    original_tensor = torch.clone(tensor)\n    json_LP(token_ids, tensor)\n    assert tensor.shape == original_tensor.shape\n    assert not torch.allclose(tensor, original_tensor)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\nasync def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                 sample_json_schema):\n    tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')\n    token_ids = tokenizer.encode(\n        f\"Give an example IPv4 address with this regex: {sample_regex}\")\n    regex_request = GuidedDecodingParams(regex=sample_regex, backend=backend)\n    regex_lp = await get_guided_decoding_logits_processor(\n        regex_request, tokenizer)\n    assert regex_lp is not None\n    tensor = torch.rand(32000)\n    original_tensor = torch.clone(tensor)\n    tensor = regex_lp(token_ids, tensor)\n    assert tensor.shape == original_tensor.shape\n    assert not torch.allclose(tensor, original_tensor)\n\n    token_ids = tokenizer.encode(\n        f\"Give an employee profile that fits this schema: {sample_json_schema}\"\n    )\n    json_request = GuidedDecodingParams(json=sample_json_schema,\n                                        backend=backend)\n    json_lp = await get_guided_decoding_logits_processor(\n        json_request, tokenizer)\n    assert json_lp is not None\n    tensor = torch.rand(32000)\n    original_tensor = torch.clone(tensor)\n    tensor = json_lp(token_ids, tensor)\n    assert tensor.shape == original_tensor.shape\n    assert not torch.allclose(tensor, original_tensor)\n\n\ndef test_multiple_guided_options_not_allowed(sample_json_schema, sample_regex):\n    with pytest.raises(ValueError,\n                       match=\"You can only use one kind of guided\"):\n        GuidedDecodingParams(json=sample_json_schema, regex=sample_regex)\n\n    with pytest.raises(ValueError,\n                       match=\"You can only use one kind of guided\"):\n        GuidedDecodingParams(json=sample_json_schema, json_object=True)\n\n    with pytest.raises(ValueError,\n                       match=\"You can only use one kind of guided\"):\n        GuidedDecodingParams(json=sample_json_schema, choice=[\"a\", \"b\"])\n\n    with pytest.raises(ValueError,\n                       match=\"You can only use one kind of guided\"):\n        GuidedDecodingParams(json=sample_json_schema, grammar=\"test grammar\")\n",
      "diff": "diff --git a/tests/model_executor/test_guided_processors.py b/tests/model_executor/test_guided_processors.py\nindex 45fab8e96..9f4d81b58 100644\n--- a/tests/model_executor/test_guided_processors.py\n+++ b/tests/model_executor/test_guided_processors.py\n@@ -36,7 +36,8 @@ def test_guided_logits_processors(sample_regex, sample_json_schema):\n \n \n @pytest.mark.asyncio\n-@pytest.mark.parametrize(\"backend\", [\"outlines\", \"lm-format-enforcer\"])\n+@pytest.mark.parametrize(\"backend\",\n+                         [\"outlines\", \"lm-format-enforcer\", \"xgrammar\"])\n async def test_guided_logits_processor_black_box(backend: str, sample_regex,\n                                                  sample_json_schema):\n     tokenizer = AutoTokenizer.from_pretrained('HuggingFaceH4/zephyr-7b-beta')",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/config.py",
      "old_content": "import copy\nimport enum\nimport hashlib\nimport json\nimport warnings\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass, field, replace\nfrom pathlib import Path\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Counter, Dict,\n                    Final, List, Literal, Mapping, Optional, Set, Tuple, Type,\n                    Union)\n\nimport torch\nfrom pydantic import BaseModel, Field, PrivateAttr\nfrom transformers import PretrainedConfig\n\nimport vllm.envs as envs\nfrom vllm.compilation.inductor_pass import CallableInductorPass, InductorPass\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n                                                     get_quantization_config)\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import current_platform\nfrom vllm.tracing import is_otel_available, otel_import_error_traceback\nfrom vllm.transformers_utils.config import (\n    ConfigFormat, get_config, get_hf_image_processor_config,\n    get_hf_text_config, get_pooling_config,\n    get_sentence_transformer_tokenizer_config, is_encoder_decoder, uses_mrope)\nfrom vllm.utils import (GiB_bytes, cuda_device_count_stateless, get_cpu_memory,\n                        print_warning_once, resolve_obj_by_qualname)\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.layers.quantization.base_config import (\n        QuantizationConfig)\n    from vllm.model_executor.model_loader.loader import BaseModelLoader\n    from vllm.transformers_utils.tokenizer_group.base_tokenizer_group import (\n        BaseTokenizerGroup)\nelse:\n    QuantizationConfig = None\n\nlogger = init_logger(__name__)\n\n_EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n_MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 5120\n\nTaskOption = Literal[\"auto\", \"generate\", \"embedding\"]\n\n# \"draft\" is only used internally for speculative decoding\n_Task = Literal[\"generate\", \"embedding\", \"draft\"]\n\nHfOverrides = Union[Dict[str, Any], Callable[[PretrainedConfig],\n                                             PretrainedConfig]]\n\n\nclass ModelConfig:\n    \"\"\"Configuration for the model.\n\n    Args:\n        model: Name or path of the huggingface model to use.\n            It is also used as the content for `model_name` tag in metrics\n            output when `served_model_name` is not specified.\n        task: The task to use the model for. Each vLLM instance only supports\n            one task, even if the same model can be used for multiple tasks.\n            When the model only supports one task, \"auto\" can be used to select\n            it; otherwise, you must specify explicitly which task to use.\n        tokenizer: Name or path of the huggingface tokenizer to use.\n        tokenizer_mode: Tokenizer mode. \"auto\" will use the fast tokenizer if\n            available, \"slow\" will always use the slow tokenizer, and\n            \"mistral\" will always use the tokenizer from `mistral_common`.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        allowed_local_media_path: Allowing API requests to read local images or\n            videos from directories specified by the server file system.\n            This is a security risk. Should only be enabled in trusted\n            environments.\n        dtype: Data type for model weights and activations. The \"auto\" option\n            will use FP16 precision for FP32 and FP16 models, and BF16 precision\n            for BF16 models.\n        seed: Random seed for reproducibility.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id. If unspecified, will use the default\n            version.\n        code_revision: The specific revision to use for the model code on\n            Hugging Face Hub. It can be a branch name, a tag name, or a\n            commit id. If unspecified, will use the default version.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id. If unspecified, will use\n            the default version.\n        max_model_len: Maximum length of a sequence (including prompt and\n            output). If None, will be derived from the model.\n        spec_target_max_model_len: Specify the the maximum length for spec\n            decoding draft models.\n        quantization: Quantization method that was used to quantize the model\n            weights. If None, we assume the model weights are not quantized.\n        quantization_param_path: Path to JSON file containing scaling factors.\n            Used to load KV cache scaling factors into the model when KV cache\n            type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also\n            be used to load activation and weight scaling factors when the\n            model dtype is FP8_E4M3 on ROCm.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n            If None, the user did not specify, so default to False.\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode. Additionally for encoder-decoder models, if the\n            sequence length of the encoder input is larger than this, we fall\n            back to the eager mode.\n        max_logprobs: Maximum number of log probabilities. Defaults to 20.\n        disable_sliding_window: Whether to disable sliding window. If True,\n            we will disable the sliding window functionality of the model.\n            If the model does not support sliding window, this argument is\n            ignored.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer.\n        served_model_name: The model name used in metrics tag `model_name`,\n            matches the model name exposed via the APIs. If multiple model\n            names provided, the first name will be used. If not specified,\n            the model name will be the same as `model`.\n        limit_mm_per_prompt: Maximum number of data items per modality\n            per prompt. Only applicable for multimodal models.\n        use_async_output_proc: Whether to use async output processor.\n            Defaults to True.\n        config_format: The config format which shall be loaded.\n            Defaults to 'auto' which defaults to 'hf'.\n        hf_overrides: If a dictionary, contains arguments to be forwarded to the\n            HuggingFace config. If a callable, it is called to update the\n            HuggingFace config.\n        mm_processor_kwargs: Arguments to be forwarded to the model's processor\n            for multi-modal data, e.g., image processor.\n        override_neuron_config: Initialize non default neuron config or\n            override default neuron config that are specific to Neuron devices,\n            this argument will be used to configure the neuron config that\n            can not be gathered from the vllm arguments.\n        override_pooler_config: Initialize non default pooling config or\n            override default pooling config for the embedding model.\n    \"\"\"\n\n    def __init__(\n            self,\n            model: str,\n            task: Union[TaskOption, _Task],\n            tokenizer: str,\n            tokenizer_mode: str,\n            trust_remote_code: bool,\n            dtype: Union[str, torch.dtype],\n            seed: int,\n            allowed_local_media_path: str = \"\",\n            revision: Optional[str] = None,\n            code_revision: Optional[str] = None,\n            rope_scaling: Optional[Dict[str, Any]] = None,\n            rope_theta: Optional[float] = None,\n            tokenizer_revision: Optional[str] = None,\n            max_model_len: Optional[int] = None,\n            spec_target_max_model_len: Optional[int] = None,\n            quantization: Optional[str] = None,\n            quantization_param_path: Optional[str] = None,\n            enforce_eager: Optional[bool] = None,\n            max_seq_len_to_capture: Optional[int] = None,\n            max_logprobs: int = 20,\n            disable_sliding_window: bool = False,\n            skip_tokenizer_init: bool = False,\n            served_model_name: Optional[Union[str, List[str]]] = None,\n            limit_mm_per_prompt: Optional[Mapping[str, int]] = None,\n            use_async_output_proc: bool = True,\n            config_format: ConfigFormat = ConfigFormat.AUTO,\n            hf_overrides: Optional[HfOverrides] = None,\n            mm_processor_kwargs: Optional[Dict[str, Any]] = None,\n            override_neuron_config: Optional[Dict[str, Any]] = None,\n            override_pooler_config: Optional[\"PoolerConfig\"] = None) -> None:\n        self.model = model\n        self.tokenizer = tokenizer\n        self.tokenizer_mode = tokenizer_mode\n        self.trust_remote_code = trust_remote_code\n        self.allowed_local_media_path = allowed_local_media_path\n        self.seed = seed\n        self.revision = revision\n        self.code_revision = code_revision\n\n        if hf_overrides is None:\n            hf_overrides = {}\n\n        if callable(hf_overrides):\n            hf_overrides_kw = {}\n            hf_overrides_fn = hf_overrides\n        else:\n            hf_overrides_kw = hf_overrides\n            hf_overrides_fn = None\n\n        if rope_scaling is not None:\n            hf_override: Dict[str, Any] = {\"rope_scaling\": rope_scaling}\n            hf_overrides_kw.update(hf_override)\n            msg = (\"`--rope-scaling` will be removed in a future release. \"\n                   f\"'Please instead use `--hf-overrides '{hf_override!r}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n        if rope_theta is not None:\n            hf_override = {\"rope_theta\": rope_theta}\n            hf_overrides_kw.update(hf_override)\n            msg = (\"`--rope-theta` will be removed in a future release. \"\n                   f\"'Please instead use `--hf-overrides '{hf_override!r}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n\n        # The tokenizer version is consistent with the model version by default.\n        if tokenizer_revision is None:\n            self.tokenizer_revision = revision\n        else:\n            self.tokenizer_revision = tokenizer_revision\n        self.quantization = quantization\n        self.quantization_param_path = quantization_param_path\n        self.enforce_eager = enforce_eager\n        self.max_seq_len_to_capture = max_seq_len_to_capture\n        self.max_logprobs = max_logprobs\n        self.disable_sliding_window = disable_sliding_window\n        self.skip_tokenizer_init = skip_tokenizer_init\n\n        hf_config = get_config(self.model, trust_remote_code, revision,\n                               code_revision, config_format)\n\n        if hf_overrides_kw:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_kw)\n            hf_config.update(hf_overrides_kw)\n        if hf_overrides_fn:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_fn)\n            hf_config = hf_overrides_fn(hf_config)\n\n        self.hf_config = hf_config\n\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.encoder_config = self._get_encoder_config()\n        self.hf_image_processor_config = get_hf_image_processor_config(\n            self.model, revision)\n        self.dtype = _get_and_verify_dtype(self.hf_text_config, dtype)\n        self.use_async_output_proc = use_async_output_proc\n        self.mm_processor_kwargs = mm_processor_kwargs\n\n        # Set enforce_eager to False if the value is unset.\n        if self.enforce_eager is None:\n            self.enforce_eager = False\n\n        sliding_window = getattr(self.hf_text_config, \"sliding_window\", None)\n        has_interleaved_attention = (sliding_window is not None) and (\n            isinstance(sliding_window, list) or\n            (self.hf_text_config.model_type in [\"gemma2\"]))\n\n        if (not self.disable_sliding_window and has_interleaved_attention):\n            if envs.VLLM_ATTENTION_BACKEND == \"XFORMERS\":\n                sliding_window_len_min = get_min_sliding_window(\n                    self.hf_text_config.sliding_window)\n\n                print_warning_once(\n                    f\"{self.hf_text_config.model_type} has interleaved \"\n                    \"attention, which is currently not supported by the \"\n                    \"XFORMERS backend. Disabling sliding window and capping \"\n                    \"the max length to the sliding window size \"\n                    f\"({sliding_window_len_min}).\")\n                self.disable_sliding_window = True\n            else:\n                # for a model with interleaved attention,\n                # the scheduler and the model treat it as full attention\n                # (i.e., not dropping any tokens outside the window).\n                # only the attention layer itself is aware of the sliding\n                # window, and use the window size to compute the attention.\n                self.hf_text_config.interleaved_sliding_window = sliding_window\n                delattr(self.hf_text_config, \"sliding_window\")\n                sliding_window = None\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window(),\n            spec_target_max_model_len=spec_target_max_model_len,\n            encoder_config=self.encoder_config)\n        self.served_model_name = get_served_model_name(model,\n                                                       served_model_name)\n        self.multimodal_config = self._init_multimodal_config(\n            limit_mm_per_prompt)\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n\n        self.is_attention_free = self._init_attention_free()\n        self.has_inner_state = self._init_has_inner_state()\n\n        if current_platform.is_neuron():\n            self.override_neuron_config = override_neuron_config\n        else:\n            self.override_neuron_config = None\n\n        supported_tasks, task = self._resolve_task(task, self.hf_config)\n        self.supported_tasks = supported_tasks\n        self.task: Final = task\n        self.pooler_config = self._init_pooler_config(override_pooler_config)\n\n        self._verify_quantization()\n        self._verify_cuda_graph()\n        self._verify_bnb_config()\n\n    def _init_multimodal_config(\n        self, limit_mm_per_prompt: Optional[Mapping[str, int]]\n    ) -> Optional[\"MultiModalConfig\"]:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        if ModelRegistry.is_multimodal_model(architectures):\n            return MultiModalConfig(limit_per_prompt=limit_mm_per_prompt or {})\n\n        if limit_mm_per_prompt:\n            raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\n                             \"multimodal models.\")\n\n        return None\n\n    def _get_encoder_config(self):\n        return get_sentence_transformer_tokenizer_config(\n            self.model, self.revision)\n\n    def _init_pooler_config(\n        self,\n        override_pooler_config: Optional[\"PoolerConfig\"],\n    ) -> Optional[\"PoolerConfig\"]:\n\n        if self.task == \"embedding\":\n            user_config = override_pooler_config or PoolerConfig()\n\n            base_config = get_pooling_config(self.model, self.revision)\n            if base_config is not None:\n                # Only set values that are not overridden by the user\n                for k, v in base_config.items():\n                    if getattr(user_config, k) is None:\n                        setattr(user_config, k, v)\n\n            return user_config\n\n        return None\n\n    def _init_attention_free(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_attention_free_model(architectures)\n\n    def _init_has_inner_state(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.model_has_inner_state(architectures)\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = self.tokenizer_mode.lower()\n        if tokenizer_mode not in [\"auto\", \"slow\", \"mistral\"]:\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                \"either 'auto', 'slow' or 'mistral'.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _resolve_task(\n        self,\n        task_option: Union[TaskOption, _Task],\n        hf_config: PretrainedConfig,\n    ) -> Tuple[Set[_Task], _Task]:\n        if task_option == \"draft\":\n            return {\"draft\"}, \"draft\"\n\n        architectures = getattr(hf_config, \"architectures\", [])\n\n        task_support: Dict[_Task, bool] = {\n            # NOTE: Listed from highest to lowest priority,\n            # in case the model supports multiple of them\n            \"generate\": ModelRegistry.is_text_generation_model(architectures),\n            \"embedding\": ModelRegistry.is_pooling_model(architectures),\n        }\n        supported_tasks_lst: List[_Task] = [\n            task for task, is_supported in task_support.items() if is_supported\n        ]\n        supported_tasks = set(supported_tasks_lst)\n\n        if task_option == \"auto\":\n            selected_task = next(iter(supported_tasks_lst))\n\n            if len(supported_tasks) > 1:\n                suffix_to_preferred_task: List[Tuple[str, _Task]] = [\n                    # Hardcode the models that are exceptions\n                    (\"AquilaModel\", \"generate\"),\n                    (\"ChatGLMModel\", \"generate\"),\n                    # Other models follow this pattern\n                    (\"ForCausalLM\", \"generate\"),\n                    (\"ForConditionalGeneration\", \"generate\"),\n                    (\"ChatModel\", \"generate\"),\n                    (\"LMHeadModel\", \"generate\"),\n                    (\"EmbeddingModel\", \"embedding\"),\n                    (\"RewardModel\", \"embedding\"),\n                    (\"ForSequenceClassification\", \"embedding\"),\n                ]\n                info, arch = ModelRegistry.inspect_model_cls(architectures)\n\n                for suffix, pref_task in suffix_to_preferred_task:\n                    if arch.endswith(suffix) and pref_task in supported_tasks:\n                        selected_task = pref_task\n                        break\n                else:\n                    if (arch.endswith(\"Model\")\n                            and info.architecture.endswith(\"ForCausalLM\")\n                            and \"embedding\" in supported_tasks):\n                        selected_task = \"embedding\"\n\n                logger.info(\n                    \"This model supports multiple tasks: %s. \"\n                    \"Defaulting to '%s'.\", supported_tasks, selected_task)\n        else:\n            if task_option not in supported_tasks:\n                msg = (\n                    f\"This model does not support the '{task_option}' task. \"\n                    f\"Supported tasks: {supported_tasks}\")\n                raise ValueError(msg)\n\n            selected_task = task_option\n\n        return supported_tasks, selected_task\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = QUANTIZATION_METHODS\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"modelopt\", \"gptq_marlin_24\", \"gptq_marlin\",\n            \"awq_marlin\", \"fbgemm_fp8\", \"compressed_tensors\",\n            \"compressed-tensors\", \"experts_int8\"\n        ]\n        if self.quantization is not None:\n            self.quantization = self.quantization.lower()\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n\n            # Detect which checkpoint is it\n            for name in QUANTIZATION_METHODS:\n                method = get_quantization_config(name)\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override:\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            current_platform.verify_quantization(self.quantization)\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n\n    def _verify_cuda_graph(self) -> None:\n        if self.max_seq_len_to_capture is None:\n            self.max_seq_len_to_capture = self.max_model_len\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n\n    def _verify_bnb_config(self) -> None:\n        \"\"\"\n        The current version of bitsandbytes (0.44.0) with 8-bit models does not\n        yet support CUDA graph.\n        \"\"\"\n        is_bitsandbytes = self.quantization == \"bitsandbytes\"\n        has_quantization_config = (getattr(self.hf_config,\n                                           \"quantization_config\", None)\n                                   is not None)\n        is_8bit = (self.hf_config.quantization_config.get(\n            \"load_in_8bit\", False) if has_quantization_config else False)\n        if all([\n                is_bitsandbytes,\n                has_quantization_config,\n                is_8bit,\n                not self.enforce_eager,\n        ]):\n            logger.warning(\n                \"CUDA graph is not supported on BitAndBytes 8bit yet, \"\n                \"fallback to the eager mode.\")\n            self.enforce_eager = True\n\n    def verify_async_output_proc(self, parallel_config, speculative_config,\n                                 device_config) -> None:\n        if not self.use_async_output_proc:\n            # Nothing to check\n            return\n\n        if parallel_config.pipeline_parallel_size > 1:\n            logger.warning(\"Async output processing can not be enabled \"\n                           \"with pipeline parallel\")\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if device_config.device_type not in (\"cuda\", \"tpu\", \"xpu\", \"hpu\"):\n            logger.warning(\n                \"Async output processing is only supported for CUDA, TPU, XPU \"\n                \"and HPU.\"\n                \"Disabling it for other platforms.\")\n            self.use_async_output_proc = False\n            return\n\n        if envs.VLLM_USE_RAY_SPMD_WORKER:\n            logger.warning(\n                \"Async output processing can not be enabled with ray spmd\")\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if device_config.device_type == \"cuda\" and self.enforce_eager:\n            logger.warning(\n                \"To see benefits of async output processing, enable CUDA \"\n                \"graph. Since, enforce-eager is enabled, async output \"\n                \"processor cannot be used\")\n            self.use_async_output_proc = not self.enforce_eager\n            return\n\n        # Async postprocessor is not necessary with embedding mode\n        # since there is no token generation\n        if self.task == \"embedding\":\n            self.use_async_output_proc = False\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if speculative_config:\n            logger.warning(\"Async output processing is not supported with\"\n                           \" speculative decoding currently.\")\n            self.use_async_output_proc = False\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if pipeline_parallel_size > 1:\n            architectures = getattr(self.hf_config, \"architectures\", [])\n            if not ModelRegistry.is_pp_supported_model(architectures):\n                raise NotImplementedError(\n                    \"Pipeline parallelism is not supported for this model. \"\n                    \"Supported models implement the `SupportsPP` interface.\")\n\n            if self.use_async_output_proc:\n                logger.warning(\"Async output processor is not supported with \"\n                               \"pipeline parallelism currently. Disabling it.\")\n                self.use_async_output_proc = False\n\n    def get_hf_config_sliding_window(\n            self) -> Union[Optional[int], List[Optional[int]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n    def get_sliding_window(self) -> Optional[Union[int, List[Optional[int]]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n        # If user disables sliding window, return None.\n        if self.disable_sliding_window:\n            return None\n        # Otherwise get the value from the hf config.\n        return self.get_hf_config_sliding_window()\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if hasattr(self.hf_text_config, \"model_type\"\n                   ) and self.hf_text_config.model_type == 'deepseek_v2':\n            # FlashAttention supports only head_size 32, 64, 128, 256,\n            # we need to pad head_size 192 to 256\n            return 256\n\n        if self.is_attention_free:\n            return 0\n\n        if hasattr(self.hf_text_config, \"head_dim\"):\n            return self.hf_text_config.head_dim\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        if self.is_attention_free:\n            return 0\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        from vllm.distributed.utils import get_pp_indices\n        total_num_hidden_layers = getattr(self.hf_text_config,\n                                          \"num_hidden_layers\", 0)\n        pp_rank = parallel_config.rank // parallel_config.tensor_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return end - start\n\n    def get_num_attention_layers(self,\n                                 parallel_config: \"ParallelConfig\") -> int:\n        if self.is_attention_free:\n            return 0\n\n        num_layers = self.get_num_layers(parallel_config)\n\n        # Transformers supports layers_block_type @property\n        layers = getattr(self.hf_config, \"layers_block_type\",\n                         [\"attention\"] * num_layers)\n        return len([t for t in layers if t == \"attention\"])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    @property\n    def is_encoder_decoder(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return is_encoder_decoder(self.hf_config)\n\n    @property\n    def uses_mrope(self) -> bool:\n        return uses_mrope(self.hf_config)\n\n    @property\n    def is_multimodal_model(self) -> bool:\n        return self.multimodal_config is not None\n\n    @property\n    def is_cross_encoder(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_cross_encoder_model(architectures)\n\n\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\n\n    Args:\n        block_size: Size of a cache block in number of tokens.\n        gpu_memory_utilization: Fraction of GPU memory to use for the\n            vLLM execution.\n        swap_space: Size of the CPU swap space per GPU (in GiB).\n        cache_dtype: Data type for kv cache storage.\n        is_attention_free: Whether the model is attention-free.\n        num_gpu_blocks_override: Number of GPU blocks to use. This overrides the\n            profiled num_gpu_blocks if specified. Does nothing if None.\n        sliding_window: Sliding window size for the KV cache. Can not work with\n            prefix caching enabled.\n        enable_prefix_caching: Whether to enable prefix caching.\n        cpu_offload_gb: Size of the CPU offload buffer in GiB.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        swap_space: float,\n        cache_dtype: str,\n        is_attention_free: bool = False,\n        num_gpu_blocks_override: Optional[int] = None,\n        sliding_window: Optional[int] = None,\n        enable_prefix_caching: bool = False,\n        cpu_offload_gb: float = 0,\n    ) -> None:\n        self.block_size = block_size\n        self.gpu_memory_utilization = gpu_memory_utilization\n        self.swap_space_bytes = swap_space * GiB_bytes\n        self.num_gpu_blocks_override = num_gpu_blocks_override\n        self.cache_dtype = cache_dtype\n        self.is_attention_free = is_attention_free\n        self.sliding_window = sliding_window\n        self.enable_prefix_caching = enable_prefix_caching\n        self.cpu_offload_gb = cpu_offload_gb\n\n        self._verify_args()\n        self._verify_cache_dtype()\n        self._verify_prefix_caching()\n\n        # Will be set after profiling.\n        self.num_gpu_blocks: Optional[int] = None\n        self.num_cpu_blocks: Optional[int] = None\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype in (\"fp8\", \"fp8_e4m3\", \"fp8_e5m2\"):\n            logger.info(\n                \"Using fp8 data type to store kv cache. It reduces the GPU \"\n                \"memory footprint and boosts the performance. \"\n                \"Meanwhile, it may cause accuracy drop without a proper \"\n                \"scaling factor\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def _verify_prefix_caching(self) -> None:\n        if not self.enable_prefix_caching:\n            return\n\n        if self.sliding_window is not None:\n            raise NotImplementedError(\n                \"Prefix caching is not supported with sliding window. \"\n                \"Run with --disable-sliding-window to use prefix caching.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / GiB_bytes:.2f} GiB out of the \"\n               f\"{total_cpu_memory / GiB_bytes:.2f} GiB total CPU memory \"\n               \"is allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. %s\", msg)\n\n\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"Configuration for the tokenizer pool.\n\n    Args:\n        pool_size: Number of tokenizer workers in the pool.\n        pool_type: Type of the pool.\n        extra_config: Additional config for the pool.\n            The way the config will be used depends on the\n            pool type.\n    \"\"\"\n    pool_size: int\n    pool_type: Union[str, Type[\"BaseTokenizerGroup\"]]\n    extra_config: dict\n\n    def __post_init__(self):\n        if self.pool_type not in (\"ray\", ) and not isinstance(\n                self.pool_type, type):\n            raise ValueError(f\"Unknown pool type: {self.pool_type}\")\n        if not isinstance(self.extra_config, dict):\n            raise ValueError(\"extra_config must be a dictionary.\")\n\n    @classmethod\n    def create_config(\n        cls, tokenizer_pool_size: int,\n        tokenizer_pool_type: Union[str, Type[\"BaseTokenizerGroup\"]],\n        tokenizer_pool_extra_config: Optional[Union[str, dict]]\n    ) -> Optional[\"TokenizerPoolConfig\"]:\n        \"\"\"Create a TokenizerPoolConfig from the given parameters.\n\n        If tokenizer_pool_size is 0, return None.\n\n        Args:\n            tokenizer_pool_size: Number of tokenizer workers in the pool.\n            tokenizer_pool_type: Type of the pool.\n            tokenizer_pool_extra_config: Additional config for the pool.\n                The way the config will be used depends on the\n                pool type. This can be a JSON string (will be parsed).\n        \"\"\"\n        if tokenizer_pool_size:\n            if isinstance(tokenizer_pool_extra_config, str):\n                tokenizer_pool_extra_config_parsed = json.loads(\n                    tokenizer_pool_extra_config)\n            else:\n                tokenizer_pool_extra_config_parsed = (\n                    tokenizer_pool_extra_config or {})\n            tokenizer_pool_config = cls(tokenizer_pool_size,\n                                        tokenizer_pool_type,\n                                        tokenizer_pool_extra_config_parsed)\n        else:\n            tokenizer_pool_config = None\n        return tokenizer_pool_config\n\n\nclass LoadFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    PT = \"pt\"\n    SAFETENSORS = \"safetensors\"\n    NPCACHE = \"npcache\"\n    DUMMY = \"dummy\"\n    TENSORIZER = \"tensorizer\"\n    SHARDED_STATE = \"sharded_state\"\n    GGUF = \"gguf\"\n    BITSANDBYTES = \"bitsandbytes\"\n    MISTRAL = \"mistral\"\n\n\n@dataclass\nclass LoadConfig:\n    \"\"\"\n        download_dir: Directory to download and load the weights, default to the\n            default cache directory of huggingface.\n        load_format: The format of the model weights to load:\n            \"auto\" will try to load the weights in the safetensors format and\n                fall back to the pytorch bin format if safetensors format is\n                not available.\n            \"pt\" will load the weights in the pytorch bin format.\n            \"safetensors\" will load the weights in the safetensors format.\n            \"npcache\" will load the weights in pytorch format and store\n                a numpy cache to speed up the loading.\n            \"dummy\" will initialize the weights with random values, which is\n                mainly for profiling.\n            \"tensorizer\" will use CoreWeave's tensorizer library for\n                fast weight loading.\n            \"bitsandbytes\" will load nf4 type weights.\n        model_loader_extra_config: The extra config for the model loader.\n        ignore_patterns: The list of patterns to ignore when loading the model.\n            Default to \"original/**/*\" to avoid repeated loading of llama's\n            checkpoints.\n    \"\"\"\n\n    load_format: Union[str, LoadFormat, \"BaseModelLoader\"] = LoadFormat.AUTO\n    download_dir: Optional[str] = None\n    model_loader_extra_config: Optional[Union[str, dict]] = field(\n        default_factory=dict)\n    ignore_patterns: Optional[Union[List[str], str]] = None\n\n    def __post_init__(self):\n        model_loader_extra_config = self.model_loader_extra_config or {}\n        if isinstance(model_loader_extra_config, str):\n            self.model_loader_extra_config = json.loads(\n                model_loader_extra_config)\n        self._verify_load_format()\n\n        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:\n            logger.info(\n                \"Ignoring the following patterns when downloading weights: %s\",\n                self.ignore_patterns)\n        else:\n            self.ignore_patterns = [\"original/**/*\"]\n\n    def _verify_load_format(self) -> None:\n        if not isinstance(self.load_format, str):\n            return\n\n        load_format = self.load_format.lower()\n        self.load_format = LoadFormat(load_format)\n\n        rocm_not_supported_load_format: List[str] = []\n        if current_platform.is_rocm(\n        ) and load_format in rocm_not_supported_load_format:\n            rocm_supported_load_format = [\n                f for f in LoadFormat.__members__\n                if (f not in rocm_not_supported_load_format)\n            ]\n            raise ValueError(\n                f\"load format '{load_format}' is not supported in ROCm. \"\n                f\"Supported load formats are \"\n                f\"{rocm_supported_load_format}\")\n\n\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1  # Number of pipeline parallel groups.\n    tensor_parallel_size: int = 1  # Number of tensor parallel groups.\n\n    # Deprecated, use distributed_executor_backend instead.\n    worker_use_ray: Optional[bool] = None\n\n    # Maximum number of multiple batches\n    # when load model sequentially. To avoid RAM OOM when using tensor\n    # parallel and large models.\n    max_parallel_loading_workers: Optional[int] = None\n\n    # Disable the custom all-reduce kernel and fall back to NCCL.\n    disable_custom_all_reduce: bool = False\n\n    # Config for the tokenizer pool. If None, will use synchronous tokenization.\n    tokenizer_pool_config: Optional[TokenizerPoolConfig] = None\n\n    # Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\n    ray_workers_use_nsight: bool = False\n\n    # ray distributed model workers placement group.\n    placement_group: Optional[\"PlacementGroup\"] = None\n\n    # Backend to use for distributed model\n    # workers, either \"ray\" or \"mp\" (multiprocessing). If the product\n    # of pipeline_parallel_size and tensor_parallel_size is less than\n    # or equal to the number of GPUs available, \"mp\" will be used to\n    # keep processing on a single host. Otherwise, this will default\n    # to \"ray\" if Ray is installed and fail otherwise. Note that tpu\n    # and hpu only support Ray for distributed inference.\n    distributed_executor_backend: Optional[Union[str,\n                                                 Type[\"ExecutorBase\"]]] = None\n\n    # the full name of the worker class to use. If \"auto\", the worker class\n    # will be determined based on the platform.\n    worker_cls: str = \"auto\"\n    sd_worker_cls: str = \"auto\"\n\n    world_size: int = field(init=False)\n\n    rank: int = 0\n\n    def __post_init__(self) -> None:\n        self.world_size = self.pipeline_parallel_size * \\\n            self.tensor_parallel_size\n\n        if self.worker_use_ray:\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            elif not self.use_ray:\n                raise ValueError(f\"worker-use-ray can't be used with \"\n                                 f\"distributed executor backend \"\n                                 f\"'{self.distributed_executor_backend}'.\")\n        ray_only_devices = [\"tpu\", \"hpu\"]\n        if (current_platform.device_type in ray_only_devices\n                and self.world_size > 1):\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            if self.distributed_executor_backend != \"ray\":\n                raise ValueError(\n                    f\"{current_platform.device_type.upper()} backend only \"\n                    \"supports Ray for distributed inference.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.executor import ray_utils\n            backend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if (current_platform.is_cuda()\n                    and cuda_device_count_stateless() < self.world_size):\n                if not ray_found:\n                    raise ValueError(\"Unable to load Ray which is \"\n                                     \"required for multi-node inference, \"\n                                     \"please install Ray with `pip install \"\n                                     \"ray`.\") from ray_utils.ray_import_err\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.info(\"Defaulting to use %s for distributed inference\",\n                        backend)\n\n        self._verify_args()\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and self.distributed_executor_backend.uses_ray)\n\n    def _verify_args(self) -> None:\n        # Lazy import to avoid circular import\n        from vllm.executor.executor_base import ExecutorBase\n\n        if self.distributed_executor_backend not in (\n                \"ray\", \"mp\", None) and not (isinstance(\n                    self.distributed_executor_backend, type) and issubclass(\n                        self.distributed_executor_backend, ExecutorBase)):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' or custom ExecutorBase subclass.\")\n        if self.use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n        if current_platform.is_rocm():\n            self.disable_custom_all_reduce = True\n            logger.info(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on AMD GPUs.\")\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n\n@dataclass\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\"\"\"\n\n    task: str = \"generate\"  # The task to use the model for.\n\n    # Maximum number of tokens to be processed in a single iteration.\n    max_num_batched_tokens: int = field(default=None)  # type: ignore\n\n    # Maximum number of sequences to be processed in a single iteration.\n    max_num_seqs: int = 128\n\n    # Maximum length of a sequence (including prompt and generated text).\n    max_model_len: int = 8192\n\n    # The number of slots to allocate per sequence per\n    # step, beyond the known token ids. This is used in speculative\n    # decoding to store KV activations of tokens which may or may not be\n    # accepted.\n    num_lookahead_slots: int = 0\n\n    # Apply a delay (of delay factor multiplied by previous\n    # prompt latency) before scheduling next prompt.\n    delay_factor: float = 0.0\n\n    # If True, prefill requests can be chunked based\n    # on the remaining max_num_batched_tokens.\n    enable_chunked_prefill: bool = False\n\n    is_multimodal_model: bool = False\n\n    # Whether to perform preemption by swapping or\n    # recomputation. If not specified, we determine the mode as follows:\n    # We use recomputation by default since it incurs lower overhead than\n    # swapping. However, when the sequence group has multiple sequences\n    # (e.g., beam search), recomputation is not currently supported. In\n    # such a case, we use swapping instead.\n    preemption_mode: Optional[str] = None\n\n    num_scheduler_steps: int = 1\n\n    multi_step_stream_outputs: bool = False\n\n    # Private API. If used, scheduler sends delta data to\n    # workers instead of an entire data. It should be enabled only\n    # when SPMD worker architecture is enabled. I.e.,\n    # VLLM_USE_RAY_SPMD_WORKER=1\n    send_delta_data: bool = False\n\n    # The scheduling policy to use. \"fcfs\" (default) or \"priority\".\n    policy: str = \"fcfs\"\n\n    chunked_prefill_enabled: bool = field(init=False)\n\n    def __post_init__(self) -> None:\n        if self.max_num_batched_tokens is None:\n            if self.enable_chunked_prefill:\n                if self.num_scheduler_steps > 1:\n                    # Multi-step Chunked-Prefill doesn't allow prompt-chunking\n                    # for now. Have max_num_batched_tokens set to max_model_len\n                    # so we don't reject sequences on account of a short\n                    # max_num_batched_tokens.\n                    self.max_num_batched_tokens = max(self.max_model_len, 2048)\n                else:\n                    # This value is chosen to have a balance between ITL\n                    # and TTFT. Note it is not optimized for throughput.\n                    self.max_num_batched_tokens = 2048\n            else:\n                # If max_model_len is too short, use 2048 as the default value\n                # for higher throughput.\n                self.max_num_batched_tokens = max(self.max_model_len, 2048)\n\n            if self.task == \"embedding\":\n                # For embedding, choose specific value for higher throughput\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _EMBEDDING_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n            if self.is_multimodal_model:\n                # The value needs to be at least the number of multimodal tokens\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n\n        if self.enable_chunked_prefill:\n            logger.info(\n                \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                self.max_num_batched_tokens)\n\n        self.chunked_prefill_enabled = self.enable_chunked_prefill\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if (self.max_num_batched_tokens < self.max_model_len\n                and not self.chunked_prefill_enabled):\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n        if self.num_lookahead_slots < 0:\n            raise ValueError(\n                \"num_lookahead_slots \"\n                f\"({self.num_lookahead_slots}) must be greater than or \"\n                \"equal to 0.\")\n\n        if self.num_scheduler_steps < 1:\n            raise ValueError(\n                \"num_scheduler_steps \"\n                f\"({self.num_scheduler_steps}) must be greater than or \"\n                \"equal to 1.\")\n\n    @property\n    def is_multi_step(self) -> bool:\n        return self.num_scheduler_steps > 1\n\n\nclass DeviceConfig:\n    device: Optional[torch.device]\n    device_type: str\n\n    def __init__(self, device: str = \"auto\") -> None:\n        if device == \"auto\":\n            # Automated device type detection\n            self.device_type = current_platform.device_type\n            if not self.device_type:\n                raise RuntimeError(\"Failed to infer device type\")\n        else:\n            # Device type is assigned explicitly\n            self.device_type = device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\", \"openvino\"]:\n            self.device = torch.device(\"cpu\")\n        elif self.device_type in [\"tpu\"]:\n            self.device = None\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\nclass SpeculativeConfig:\n    \"\"\"Configuration for speculative decoding.\n\n    The configuration is currently specialized to draft-model speculative\n    decoding with top-1 proposals.\n    \"\"\"\n\n    @staticmethod\n    def maybe_create_spec_config(\n        target_model_config: ModelConfig,\n        target_parallel_config: ParallelConfig,\n        target_dtype: str,\n        speculative_model: Optional[str],\n        speculative_model_quantization: Optional[str],\n        speculative_draft_tensor_parallel_size: Optional[int],\n        num_speculative_tokens: Optional[int],\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_max_model_len: Optional[int],\n        enable_chunked_prefill: bool,\n        disable_log_stats: bool,\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: Optional[float],\n        typical_acceptance_sampler_posterior_alpha: Optional[float],\n        disable_logprobs: Optional[bool],\n    ) -> Optional[\"SpeculativeConfig\"]:\n        \"\"\"Create a SpeculativeConfig if possible, else return None.\n\n        This function attempts to create a SpeculativeConfig object based on the\n        provided parameters. If the necessary conditions are met, it returns an\n        instance of SpeculativeConfig. Otherwise, it returns None.\n\n        Args:\n            target_model_config (ModelConfig): The configuration of the target\n                model.\n            target_parallel_config (ParallelConfig): The parallel configuration\n                for the target model.\n            target_dtype (str): The data type used for the target model.\n            speculative_model (Optional[str]): The name of the speculative\n                model, if provided.\n            speculative_model_quantization (Optional[str]): Quantization method\n                that was used to quantize the speculative model weights. If\n                None, we assume the model weights are not quantized.\n            speculative_draft_tensor_parallel_size (Optional[int]): The degree\n                of the tensor parallelism for the draft model.\n            num_speculative_tokens (Optional[int]): The number of speculative\n                tokens, if provided. Will default to the number in the draft\n                model config if present, otherwise is required.\n            speculative_disable_mqa_scorer (Optional[bool]): Disable the MQA\n                scorer for the speculative model and fall back to batch\n                expansion for scoring.\n            speculative_max_model_len (Optional[int]): The maximum model len of\n                the speculative model. Used when testing the ability to skip\n                speculation for some sequences.\n            enable_chunked_prefill (bool): Whether vLLM is configured to use\n                chunked prefill or not. Used for raising an error since its not\n                yet compatible with spec decode.\n            speculative_disable_by_batch_size (Optional[int]): Disable\n                speculative decoding for new incoming requests when the number\n                of enqueue requests  is larger than this value, if provided.\n            ngram_prompt_lookup_max (Optional[int]): Max size of ngram token\n                window, if provided.\n            ngram_prompt_lookup_min (Optional[int]): Min size of ngram token\n                window, if provided.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the\n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs (Optional[bool]): If set to True, token log\n                probabilities are not returned during speculative decoding.\n                If set to False, token log probabilities are returned\n                according to the log probability settings in SamplingParams.\n                If not specified, it defaults to True.\n\n        Returns:\n            Optional[\"SpeculativeConfig\"]: An instance of SpeculativeConfig if\n                the necessary conditions are met, else None.\n        \"\"\"\n\n        if speculative_model is None:\n            if num_speculative_tokens is not None:\n                raise ValueError(\"num_speculative_tokens was provided without \"\n                                 \"speculative_model.\")\n            return None\n\n        if (speculative_disable_by_batch_size is not None\n                and speculative_disable_by_batch_size < 2):\n            raise ValueError(\"Expect the batch size threshold of disabling \"\n                             \"speculative decoding is > 1, but got \"\n                             f\"{speculative_disable_by_batch_size=}\")\n\n        # TODO: The user should be able to specify revision/max model len\n        # for the draft model. It is not currently supported.\n        draft_revision = None\n        draft_code_revision = None\n        draft_quantization = speculative_model_quantization\n\n        if speculative_model == \"[ngram]\":\n            if ngram_prompt_lookup_min is None:\n                ngram_prompt_lookup_min = 1\n            if ngram_prompt_lookup_max is None or ngram_prompt_lookup_max < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_max=} must be > 0\")\n            if ngram_prompt_lookup_min < 1:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} must be > 0\")\n            if ngram_prompt_lookup_min > ngram_prompt_lookup_max:\n                raise ValueError(f\"{ngram_prompt_lookup_min=} cannot be \"\n                                 f\"larger than {ngram_prompt_lookup_max=}\")\n\n            # TODO: current we still need extract vocab_size from target model\n            # config, in future, we may try refactor it out, and set\n            # draft related config as None here.\n            draft_model_config = target_model_config\n            draft_parallel_config = target_parallel_config\n        else:\n            ngram_prompt_lookup_max = 0\n            ngram_prompt_lookup_min = 0\n            draft_model_config = ModelConfig(\n                model=speculative_model,\n                task=\"draft\",\n                tokenizer=target_model_config.tokenizer,\n                tokenizer_mode=target_model_config.tokenizer_mode,\n                trust_remote_code=target_model_config.trust_remote_code,\n                allowed_local_media_path=target_model_config.\n                allowed_local_media_path,\n                dtype=target_model_config.dtype,\n                seed=target_model_config.seed,\n                revision=draft_revision,\n                code_revision=draft_code_revision,\n                tokenizer_revision=target_model_config.tokenizer_revision,\n                max_model_len=None,\n                spec_target_max_model_len=target_model_config.max_model_len,\n                quantization=draft_quantization,\n                enforce_eager=target_model_config.enforce_eager,\n                max_seq_len_to_capture=target_model_config.\n                max_seq_len_to_capture,\n                max_logprobs=target_model_config.max_logprobs,\n            )\n\n            draft_hf_config = draft_model_config.hf_config\n\n            if (num_speculative_tokens is not None\n                    and hasattr(draft_hf_config, \"num_lookahead_tokens\")):\n                draft_hf_config.num_lookahead_tokens = num_speculative_tokens\n\n            n_predict = getattr(draft_hf_config, \"n_predict\", None)\n            if n_predict is not None:\n                if num_speculative_tokens is None:\n                    # Default to max value defined in draft model config.\n                    num_speculative_tokens = n_predict\n                elif num_speculative_tokens > n_predict:\n                    # Verify provided value doesn't exceed the maximum\n                    # supported by the draft model.\n                    raise ValueError(\n                        \"This speculative model supports a maximum of \"\n                        f\"num_speculative_tokens={n_predict}, but \"\n                        f\"{num_speculative_tokens=} was provided.\")\n\n            if enable_chunked_prefill and draft_hf_config.model_type in (\n                    \"medusa\", \"mlp_speculator\", \"eagle\"):\n                raise ValueError(\n                    \"Chunked prefill and hidden-state based draft models are \"\n                    \"not compatible.\")\n\n            speculative_draft_tensor_parallel_size = \\\n                SpeculativeConfig._verify_and_get_draft_model_tensor_parallel_size(\n                    target_parallel_config,\n                    speculative_draft_tensor_parallel_size,\n                    draft_hf_config\n            )\n\n            draft_model_config.max_model_len = (\n                SpeculativeConfig._maybe_override_draft_max_model_len(\n                    speculative_max_model_len,\n                    draft_model_config.max_model_len,\n                    target_model_config.max_model_len,\n                ))\n\n            draft_parallel_config = (\n                SpeculativeConfig.create_draft_parallel_config(\n                    target_parallel_config,\n                    speculative_draft_tensor_parallel_size, draft_hf_config))\n\n        if num_speculative_tokens is None:\n            raise ValueError(\n                \"num_speculative_tokens must be provided with \"\n                \"speculative_model unless the draft model config contains an \"\n                \"n_predict parameter.\")\n\n        if typical_acceptance_sampler_posterior_threshold is None:\n            typical_acceptance_sampler_posterior_threshold = 0.09\n        if typical_acceptance_sampler_posterior_alpha is None:\n            typical_acceptance_sampler_posterior_alpha = 0.3\n        if disable_logprobs is None:\n            disable_logprobs = True\n\n        return SpeculativeConfig(\n            draft_model_config,\n            draft_parallel_config,\n            num_speculative_tokens,\n            speculative_disable_mqa_scorer,\n            speculative_disable_by_batch_size,\n            ngram_prompt_lookup_max,\n            ngram_prompt_lookup_min,\n            draft_token_acceptance_method=draft_token_acceptance_method,\n            typical_acceptance_sampler_posterior_threshold=\\\n                typical_acceptance_sampler_posterior_threshold,\n            typical_acceptance_sampler_posterior_alpha=\\\n                typical_acceptance_sampler_posterior_alpha,\n            disable_logprobs=disable_logprobs,\n            disable_log_stats=disable_log_stats,\n        )\n\n    @staticmethod\n    def _maybe_override_draft_max_model_len(\n        speculative_max_model_len: Optional[int],\n        draft_max_model_len: int,\n        target_max_model_len: int,\n    ) -> int:\n        \"\"\"Determine the max sequence len for the draft model. This is usually\n        the draft_max_model_len, but may be the target_max_model_len if it is\n        less than the draft_max_model_len, or may be speculative_max_model_len\n        if it is specified.\n\n        This is necessary so that sequences do not exceed the capacity of the\n        draft model or the target model.\n\n        speculative_max_model_len is mainly used for testing that sequences can\n        skip speculation.\n        \"\"\"\n\n        if speculative_max_model_len is not None:\n\n            if speculative_max_model_len > draft_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {draft_max_model_len=}\")\n\n            if speculative_max_model_len > target_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {target_max_model_len=}\")\n\n            return speculative_max_model_len\n\n        return min(\n            draft_max_model_len,\n            target_max_model_len,\n        )\n\n    @staticmethod\n    def _verify_and_get_draft_model_tensor_parallel_size(\n            target_parallel_config: ParallelConfig,\n            speculative_draft_tensor_parallel_size: Optional[int],\n            draft_hf_config: PretrainedConfig) -> int:\n        \"\"\"\n        Verifies and adjusts the tensor parallel size for a draft model\n        specified using speculative_draft_tensor_parallel_size.\n        \"\"\"\n        # If speculative_draft_tensor_parallel_size is unset then set it\n        # appropriately else verify that it is set correctly.\n        if speculative_draft_tensor_parallel_size is None:\n            if draft_hf_config.model_type == \"mlp_speculator\":\n                speculative_draft_tensor_parallel_size = 1\n                if target_parallel_config.tensor_parallel_size > 1:\n                    logger.warning(\n                        \"MLPSpeculator cannot currently be run with tp>1; \"\n                        \"setting speculative_draft_tensor_parallel_size=1\")\n            else:\n                speculative_draft_tensor_parallel_size = \\\n                    target_parallel_config.tensor_parallel_size\n        elif speculative_draft_tensor_parallel_size not in (\n                1, target_parallel_config.tensor_parallel_size):\n            raise ValueError(\n                f\"{speculative_draft_tensor_parallel_size=} cannot be \"\n                f\"other value than 1 or target model tensor_parallel_size\")\n        return speculative_draft_tensor_parallel_size\n\n    @staticmethod\n    def create_draft_parallel_config(\n        target_parallel_config: ParallelConfig,\n        speculative_draft_tensor_parallel_size: int,\n        draft_hf_config: PretrainedConfig,\n    ) -> ParallelConfig:\n        \"\"\"Create a parallel config for use by the draft worker.\n\n        This is mostly a copy of the target parallel config, except the tp_size.\n        \"\"\"\n        draft_parallel_config = ParallelConfig(\n            pipeline_parallel_size=target_parallel_config.\n            pipeline_parallel_size,\n            tensor_parallel_size=speculative_draft_tensor_parallel_size,\n            distributed_executor_backend=target_parallel_config.\n            distributed_executor_backend,\n            max_parallel_loading_workers=target_parallel_config.\n            max_parallel_loading_workers,\n            disable_custom_all_reduce=target_parallel_config.\n            disable_custom_all_reduce,\n            tokenizer_pool_config=target_parallel_config.tokenizer_pool_config,\n            ray_workers_use_nsight=target_parallel_config.\n            ray_workers_use_nsight,\n            placement_group=target_parallel_config.placement_group,\n        )\n\n        return draft_parallel_config\n\n    def __init__(\n        self,\n        draft_model_config: ModelConfig,\n        draft_parallel_config: ParallelConfig,\n        num_speculative_tokens: int,\n        speculative_disable_mqa_scorer: Optional[bool],\n        speculative_disable_by_batch_size: Optional[int],\n        ngram_prompt_lookup_max: Optional[int],\n        ngram_prompt_lookup_min: Optional[int],\n        draft_token_acceptance_method: str,\n        typical_acceptance_sampler_posterior_threshold: float,\n        typical_acceptance_sampler_posterior_alpha: float,\n        disable_logprobs: bool,\n        disable_log_stats: bool,\n    ):\n        \"\"\"Create a SpeculativeConfig object.\n\n        Args:\n            draft_model_config: ModelConfig for the draft model.\n            draft_parallel_config: ParallelConfig for the draft model.\n            num_speculative_tokens: The number of tokens to sample from the\n                draft model before scoring with the target model.\n            speculative_disable_by_batch_size: Disable speculative\n                decoding for new incoming requests when the number of\n                enqueue requests is larger than this value.\n            ngram_prompt_lookup_max: Max size of ngram token window.\n            ngram_prompt_lookup_min: Min size of ngram token window.\n            draft_token_acceptance_method (str): The method to use for\n                accepting draft tokens. This can take two possible\n                values 'rejection_sampler' and 'typical_acceptance_sampler'\n                for RejectionSampler and TypicalAcceptanceSampler\n                respectively.\n            typical_acceptance_sampler_posterior_threshold (Optional[float]):\n                A threshold value that sets a lower bound on the posterior\n                probability of a token in the target model for it to be\n                accepted. This threshold is used only when we use the\n                TypicalAcceptanceSampler for token acceptance.\n            typical_acceptance_sampler_posterior_alpha (Optional[float]):\n                A scaling factor for the entropy-based threshold in the\n                TypicalAcceptanceSampler.\n            disable_logprobs: If set to True, token log probabilities will not\n                be returned even if requested by sampling parameters. This\n                reduces latency by skipping logprob calculation in proposal\n                sampling, target sampling, and after accepted tokens are\n                determined. If set to False, log probabilities will be\n                returned.\n            disable_log_stats: Whether to disable periodic printing of stage\n                times in speculative decoding.\n        \"\"\"\n        self.draft_model_config = draft_model_config\n        self.draft_parallel_config = draft_parallel_config\n        self.num_speculative_tokens = num_speculative_tokens\n        self.speculative_disable_mqa_scorer = speculative_disable_mqa_scorer\n        self.speculative_disable_by_batch_size = \\\n            speculative_disable_by_batch_size\n        self.ngram_prompt_lookup_max = ngram_prompt_lookup_max or 0\n        self.ngram_prompt_lookup_min = ngram_prompt_lookup_min or 0\n        self.draft_token_acceptance_method = draft_token_acceptance_method\n        self.typical_acceptance_sampler_posterior_threshold = \\\n            typical_acceptance_sampler_posterior_threshold\n        self.typical_acceptance_sampler_posterior_alpha = \\\n            typical_acceptance_sampler_posterior_alpha\n        self.disable_logprobs = disable_logprobs\n        self.disable_log_stats = disable_log_stats\n\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if self.num_speculative_tokens <= 0:\n            raise ValueError(\"Expected num_speculative_tokens to be greater \"\n                             f\"than zero ({self.num_speculative_tokens}).\")\n\n        if self.draft_model_config:\n            self.draft_model_config.verify_with_parallel_config(\n                self.draft_parallel_config)\n            # Validate and set draft token acceptance related settings.\n\n        if (self.draft_token_acceptance_method is None):\n            raise ValueError(\"draft_token_acceptance_method is not set. \"\n                             \"Expected values are rejection_sampler or \"\n                             \"typical_acceptance_sampler.\")\n\n        if (self.draft_token_acceptance_method != 'rejection_sampler'\n                and self.draft_token_acceptance_method !=\n                'typical_acceptance_sampler'):\n            raise ValueError(\n                \"Expected draft_token_acceptance_method to be either \"\n                \"rejection_sampler or typical_acceptance_sampler. Instead it \"\n                f\"is {self.draft_token_acceptance_method}\")\n\n        if (self.typical_acceptance_sampler_posterior_threshold < 0\n                or self.typical_acceptance_sampler_posterior_alpha < 0):\n            raise ValueError(\n                \"Expected typical_acceptance_sampler_posterior_threshold \"\n                \"and typical_acceptance_sampler_posterior_alpha to be > 0. \"\n                \"Instead found \"\n                f\"typical_acceptance_sampler_posterior_threshold = \"\n                f\"{self.typical_acceptance_sampler_posterior_threshold} and \"\n                f\"typical_acceptance_sampler_posterior_alpha = \"\n                f\"{self.typical_acceptance_sampler_posterior_alpha}\")\n\n    @property\n    def num_lookahead_slots(self) -> int:\n        \"\"\"The number of additional slots the scheduler should allocate per\n        step, in addition to the slots allocated for each known token.\n\n        This is equal to the number of speculative tokens, as each speculative\n        token must be scored.\n        \"\"\"\n        return self.num_speculative_tokens\n\n    def __repr__(self) -> str:\n        if self.ngram_prompt_lookup_max > 0:\n            draft_model = \"[ngram]\"\n        else:\n            draft_model = self.draft_model_config.model\n        num_spec_tokens = self.num_speculative_tokens\n        return f\"SpeculativeConfig({draft_model=}, {num_spec_tokens=})\"\n\n\n@dataclass\nclass LoRAConfig:\n    max_lora_rank: int\n    max_loras: int\n    fully_sharded_loras: bool = False\n    max_cpu_loras: Optional[int] = None\n    lora_dtype: Optional[Union[torch.dtype, str]] = None\n    lora_extra_vocab_size: int = 256\n    # This is a constant.\n    lora_vocab_padding_size: ClassVar[int] = 256\n    long_lora_scaling_factors: Optional[Tuple[float]] = None\n    bias_enabled: bool = False\n\n    def __post_init__(self):\n        # Setting the maximum rank to 256 should be able to satisfy the vast\n        # majority of applications.\n        possible_max_ranks = (8, 16, 32, 64, 128, 256)\n        possible_lora_extra_vocab_size = (0, 256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n        if model_config.quantization and model_config.quantization not in [\n                \"awq\", \"gptq\"\n        ]:\n            # TODO support marlin\n            logger.warning(\"%s quantization is not tested with LoRA yet.\",\n                           model_config.quantization)\n\n    def verify_with_scheduler_config(self, scheduler_config: SchedulerConfig):\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if scheduler_config.chunked_prefill_enabled:\n            raise ValueError(\"LoRA is not supported with chunked prefill yet.\")\n\n\n@dataclass\nclass PromptAdapterConfig:\n    max_prompt_adapters: int\n    max_prompt_adapter_token: int\n    max_cpu_prompt_adapters: Optional[int] = None\n    prompt_adapter_dtype: Optional[torch.dtype] = None\n\n    def __post_init__(self):\n\n        if self.max_prompt_adapters < 1:\n            raise ValueError(f\"max_prompt_adapters \"\n                             f\"({self.max_prompt_adapters}) must be >= 1.\")\n        if self.max_prompt_adapter_token == 0:\n            raise ValueError(\"max_prompt_adapter_token must be set.\")\n        if self.max_cpu_prompt_adapters is None:\n            self.max_cpu_prompt_adapters = self.max_prompt_adapters\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.prompt_adapter_dtype in (None, \"auto\"):\n            self.prompt_adapter_dtype = model_config.dtype\n        elif isinstance(self.prompt_adapter_dtype, str):\n            self.prompt_adapter_dtype = getattr(torch,\n                                                self.prompt_adapter_dtype)\n\n\n@dataclass\nclass MultiModalConfig:\n    \"\"\"Controls the behavior of multimodal models.\"\"\"\n\n    limit_per_prompt: Mapping[str, int] = field(default_factory=dict)\n    \"\"\"\n    The maximum number of multi-modal input instances allowed per prompt\n    for each :class:`~vllm.multimodal.MultiModalPlugin`.\n    \"\"\"\n\n    # TODO: Add configs to init vision tower or not.\n\n\n@dataclass\nclass PoolerConfig:\n    \"\"\"Controls the behavior of output pooling in embedding models.\"\"\"\n\n    pooling_type: Optional[str] = None\n    \"\"\"\n    The pooling method of the embedding model. This should be a key in\n    :class:`vllm.model_executor.layers.pooler.PoolingType`.\n    \"\"\"\n\n    normalize: Optional[bool] = None\n    \"\"\"\n    Whether to normalize the pooled outputs. Usually, this should be set to\n    ``True`` for embedding outputs.\n    \"\"\"\n\n    softmax: Optional[bool] = None\n    \"\"\"\n    Whether to apply softmax to the pooled outputs. Usually, this should be set\n    to ``True`` for classification outputs.\n    \"\"\"\n\n    step_tag_id: Optional[int] = None\n    \"\"\"\n    If set, only the score corresponding to the ``step_tag_id`` in the \n    generated sentence should be returned. Otherwise, the scores for all tokens\n    are returned.\n    \"\"\"\n\n    returned_token_ids: Optional[List[int]] = None\n    \"\"\"\n    A list of indices for the vocabulary dimensions to be extracted, \n    such as the token IDs of ``good_token`` and ``bad_token`` in the \n    ``math-shepherd-mistral-7b-prm`` model.\n    \"\"\"\n\n    @staticmethod\n    def from_json(json_str: str) -> \"PoolerConfig\":\n        return PoolerConfig(**json.loads(json_str))\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE: List[str] = []  #\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            if config_dtype == torch.float32:\n                if config.model_type == \"gemma2\":\n                    logger.info(\n                        \"For Gemma 2, we downcast float32 to bfloat16 instead \"\n                        \"of float16 by default. Please specify `dtype` if you \"\n                        \"want to use float16.\")\n                    torch_dtype = torch.bfloat16\n                else:\n                    # Following the common practice, we use float16 for float32\n                    # models.\n                    torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n\n            if current_platform.is_hpu() and config_dtype == torch.float16:\n                logger.info(\n                    \"For HPU, we cast models to bfloat16 instead of\"\n                    \"using float16 by default. Please specify `dtype` if you \"\n                    \"want to use float16.\")\n                torch_dtype = torch.bfloat16\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            logger.info(\"Upcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            logger.info(\"Downcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(\"Casting %s to %s.\", config_dtype, torch_dtype)\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n    disable_sliding_window: bool,\n    sliding_window_len: Optional[Union[int, List[Optional[int]]]],\n    spec_target_max_model_len: Optional[int] = None,\n    encoder_config: Optional[Any] = None,\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Command-R\n        \"model_max_length\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    # Choose the smallest \"max_length\" from the possible keys.\n    max_len_key = None\n    for key in possible_keys:\n        max_len = getattr(hf_config, key, None)\n        if max_len is not None:\n            max_len_key = key if max_len < derived_max_model_len \\\n                else max_len_key\n            derived_max_model_len = min(derived_max_model_len, max_len)\n\n    # If sliding window is manually disabled, max_length should be less\n    # than the sliding window length in the model config.\n    if disable_sliding_window and sliding_window_len is not None:\n\n        sliding_window_len_min = get_min_sliding_window(sliding_window_len)\n        max_len_key = \"sliding_window\" \\\n            if sliding_window_len_min < derived_max_model_len else max_len_key\n        derived_max_model_len = min(derived_max_model_len,\n                                    sliding_window_len_min)\n\n    # If none of the keys were found in the config, use a default and\n    # log a warning.\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        if spec_target_max_model_len is not None:\n            # If this is a speculative draft model, we use the max model len\n            # from the target model.\n            return spec_target_max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            \"%s. Assuming the model's maximum length is %d.\", possible_keys,\n            default_max_len)\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        # No need to consider \"type\" key because of patch_rope_scaling when\n        # loading HF config\n        rope_type = rope_scaling[\"rope_type\"]\n\n        if rope_type not in (\"su\", \"longrope\", \"llama3\"):\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that supports rope_scaling\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"with rope_scaling. Please raise an issue so we can \"\n                    \"investigate.\")\n\n            # NOTE: rope_type == \"default\" does not define factor\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/modeling_rope_utils.py\n            scaling_factor = rope_scaling.get(\"factor\", 1.0)\n\n            if rope_type == \"yarn\":\n                derived_max_model_len = rope_scaling[\n                    \"original_max_position_embeddings\"]\n            derived_max_model_len *= scaling_factor\n\n    if encoder_config and \"max_seq_length\" in encoder_config:\n        derived_max_model_len = encoder_config[\"max_seq_length\"]\n\n    # If the user specified a max length, make sure it is smaller than the\n    # derived length from the HF model config.\n    if max_model_len is None:\n        max_model_len = int(derived_max_model_len)\n    elif max_model_len > derived_max_model_len:\n        # Some models might have a separate key for specifying model_max_length\n        # that will be bigger than derived_max_model_len. We compare user input\n        # with model_max_length and allow this override when it's smaller.\n        model_max_length = getattr(hf_config, \"model_max_length\", None)\n        if model_max_length is not None and max_model_len <= model_max_length:\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that has model_max_length\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"model_max_length in the config. Please raise an issue \"\n                    \"so we can investigate.\")\n        else:\n            msg = (\n                f\"User-specified max_model_len ({max_model_len}) is greater \"\n                f\"than the derived max_model_len ({max_len_key}=\"\n                f\"{derived_max_model_len} or model_max_length=\"\n                f\"{model_max_length} in model's config.json). This may lead \"\n                \"to incorrect model outputs or CUDA errors.\")\n            if envs.VLLM_ALLOW_LONG_MAX_MODEL_LEN:\n                logger.warning(\n                    \"%s Make sure the value is correct and within the \"\n                    \"model context size.\", msg)\n            else:\n                raise ValueError(\n                    f\"{msg} To allow overriding this maximum, set \"\n                    \"the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\")\n    return int(max_model_len)\n\n\ndef get_min_sliding_window(\n        sliding_window: Union[int, List[Optional[int]]]) -> int:\n    if isinstance(sliding_window, list):\n        return min(s for s in sliding_window if s is not None)\n\n    return sliding_window\n\n\ndef get_served_model_name(model: str,\n                          served_model_name: Optional[Union[str, List[str]]]):\n    \"\"\"\n    If the input is a non-empty list, the first model_name in\n    `served_model_name` is taken.\n    If the input is a non-empty string, it is used directly.\n    For cases where the input is either an empty string or an\n    empty list, the fallback is to use `self.model`.\n    \"\"\"\n    if not served_model_name:\n        return model\n    if isinstance(served_model_name, list):\n        return served_model_name[0]\n    return served_model_name\n\n\n@dataclass\nclass DecodingConfig:\n    \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n\n    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n    guided_decoding_backend: str = 'outlines'\n\n    def __post_init__(self):\n        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n        backend = self.guided_decoding_backend\n        if backend not in valid_guided_backends:\n            raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\n                             f\"must be one of {valid_guided_backends}\")\n\n\n@dataclass\nclass ObservabilityConfig:\n    \"\"\"Configuration for observability.\"\"\"\n    otlp_traces_endpoint: Optional[str] = None\n\n    # Collecting detailed timing information for each request can be expensive.\n\n    # If set, collects the model forward time for the request.\n    collect_model_forward_time: bool = False\n\n    # If set, collects the model execute time for the request.\n    collect_model_execute_time: bool = False\n\n    def __post_init__(self):\n        if not is_otel_available() and self.otlp_traces_endpoint is not None:\n            raise ValueError(\n                \"OpenTelemetry is not available. Unable to configure \"\n                \"'otlp_traces_endpoint'. Ensure OpenTelemetry packages are \"\n                f\"installed. Original error:\\n{otel_import_error_traceback}\")\n\n\nclass KVTransferConfig(BaseModel):\n    \"\"\"Configuration for distributed KV cache transfer.\"\"\"\n\n    # The KV connector for vLLM to transmit KV caches between vLLM instances.\n    kv_connector: Optional[str] = None\n\n    # The device used by kv connector to buffer the KV cache.\n    # Currently only support 'cuda'.\n    kv_buffer_device: Optional[str] = \"cuda\"\n\n    # The buffer size for TorchDistributedConnector. Measured in number of\n    # bytes. Recommended value: 1e9 (about 1GB).\n    kv_buffer_size: float = 1e9\n\n    # Whether this vLLM instance produces, consumes KV cache, or both. Choices\n    # are 'kv_producer', 'kv_consumer', and 'both'.\n    kv_role: Optional[str] = None\n\n    # The rank of this vLLM instance in the KV cache transfer. Typical value:\n    # 0 for prefill instance, 1 for decode instance.\n    # Currently only 1P1D is supported.\n    kv_rank: Optional[int] = None\n\n    # The number of parallel instances for KV cache transfer. For\n    # PyNcclConnector, this should be 2.\n    kv_parallel_size: int = 1\n\n    # The KV connector ip, used to build distributed connection\n    kv_ip: str = \"127.0.0.1\"\n\n    # The KV connector port, used to build distributed connection\n    kv_port: int = 14579\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"KVTransferConfig\":\n        \"\"\"Parse the CLI value for the compilation config.\"\"\"\n        return KVTransferConfig.model_validate_json(cli_value)\n\n    def model_post_init(self, __context: Any) -> None:\n        if all([\n                self.kv_connector is not None,\n                self.kv_connector != \"PyNcclConnector\"\n        ]):\n            raise ValueError(f\"Unsupported kv_connector: {self.kv_connector}. \"\n                             f\"Supported connectors are \"\n                             f\"`PyNcclConnector`.\")\n\n        if self.kv_role is not None and self.kv_role not in [\n                \"kv_producer\", \"kv_consumer\", \"kv_both\"\n        ]:\n            raise ValueError(\n                f\"Unsupported kv_role: {self.kv_role}. \"\n                f\"Supported roles are `kv_producer`, `kv_consumer`, \"\n                f\"and `kv_both`\")\n\n        if self.kv_connector is not None and self.kv_role is None:\n            raise ValueError(\"Please specify kv_disagg_role when kv_connector \"\n                             \"is set, supported roles are `kv_producer`, \"\n                             \"`kv_consumer`, and `kv_both`\")\n\n    @property\n    def is_kv_transfer_instance(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_producer\", \"kv_consumer\", \"kv_both\"]\n\n    @property\n    def need_kv_parallel_group(self) -> bool:\n        # for those database-based connector, vLLM does not need to create\n        # parallel group, and in that case the kv parallel size will be 1.\n        return self.kv_connector is not None and self.kv_parallel_size > 1\n\n    @property\n    def is_kv_producer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_producer\", \"kv_both\"]\n\n    @property\n    def is_kv_consumer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in [\"kv_consumer\", \"kv_both\"]\n\n\nclass CompilationLevel:\n    # constants for the levels of the compilation process\n    NO_COMPILATION = 0\n    DYNAMO_AS_IS = 1\n    DYNAMO_ONCE = 2\n    PIECEWISE = 3\n\n\nclass CompilationConfig(BaseModel):\n    \"\"\"\n    Configuration for compilation.\n    It has three parts:\n    - Top-level Compilation control:\n        - level: the level of compilation.\n            - 0: no compilation.\n            - 1: dynamo as is.\n            - 2: dynamo once.\n            - 3: piecewise compilation.\n        - backend: the backend for compilation. It needs to be a string.\n            - \"\" (empty string): use the default backend.\n            - \"eager\"/\"openxla\"/...: use the specified backend registered in PyTorch.\n            - \"full.module.name\": a qualified name which can be used to import the backend function.\n            We use string to avoid serialization issues when using compilation in a distributed setting.\n            When the compilation level is 1 or 2, the backend is used for the compilation directly (it sees the whole graph).\n            When the compilation level is 3, the backend is used for the piecewise compilation (it sees a part of the graph).\n        - custom_ops: fine-grained control over which custom ops to enable/disable.\n            Use 'all' to enable all, 'none' to disable all.\n            Also specify a list of custom op names to enable (prefixed with a '+'),\n            or disable (prefixed with a '-').\n            Examples:\n                - 'all,-op1' to enable all except op1\n                - 'none,+op1,+op2' to enable only op1 and op2\n            By default, all custom ops are enabled when running without Inductor\n                and disabled when running with Inductor (compile_level >= Inductor).\n        - splitting_ops: a list of ops to split the full graph into subgraphs, used in piecewise compilation.\n    - CudaGraph capture:\n        - use_cudagraph: whether to use cudagraph inside compilation.\n            - False: cudagraph inside compilation is not used.\n            - True: cudagraph inside compilation is used. It requires\n                that all input buffers have fixed addresses, and all\n                splitting ops write their outputs to input buffers.\n            Note that this is orthogonal to the cudagraph capture logic\n            outside of compilation.\n            TODO: move outside cudagraph logic into compilation.\n            torch.compile will handle cudagraph capture logic in the future.\n        - cudagraph_capture_sizes: sizes to capture cudagraph.\n            - None: capture sizes are inferred from compilation context.\n            - List[int]: capture sizes are specified.\n        - cudagraph_num_of_warmups: number of warmup runs for cudagraph.\n            It means the first several runs will be treated as warmup runs.\n            Only after that, the execution will be recorded, and the recorded\n            cudagraph will be used for subsequent runs.\n        - cudagraph_copy_inputs: whether to copy input tensors for\n            cudagraph. If the caller can guarantee that the same input buffers\n            are always used, it can set this to False. Otherwise, it should\n            set this to True, and the compiler will copy the input to an\n            internally managed buffer. Default is False.\n    - Inductor compilation:\n        - use_inductor: whether to use inductor compilation.\n            - False: inductor compilation is not used. graph runs in eager.\n            - True: inductor compilation is used. one graph for symbolic shape\n                is compiled. In addition, compile for different sizes specified\n                in inductor_compile_sizes, using configurations\n                in inductor_compile_config.\n        - inductor_compile_sizes: sizes to compile for inductor.\n        - inductor_specialize_for_cudagraph_no_more_than: an optional integer\n            to specialize inductor for cudagraph sizes no more than the\n            specified size. It is useful when we want to specialize inductor\n            with a subset of cudagraph sizes.\n        - inductor_compile_config: additional configurations for inductor.\n            - None: use default configurations.\n        - inductor_passes: additional passes for inductor. It is a dictionary\n            from pass name to pass function qualified name. We use function\n            name because the config uses json format. If we pass the config\n            from Python, functions can also be passed directly via Python object\n            constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`\n        - custom inductor passes: see PassConfig for more details\n    \n    Why we have different sizes for cudagraph and inductor:\n    - cudagraph: a cudagraph captured for a specific size can only be used\n        for the same size. We need to capture all the sizes we want to use.\n    - inductor: a graph compiled by inductor for a general shape can be used\n        for different sizes. Inductor can also compile for specific sizes,\n        where it can have more information to optimize the graph with fully\n        static shapes. However, we find the general shape compilation is\n        sufficient for most cases. It might be beneficial to compile for\n        certain small batchsizes, where inductor is good at optimizing.\n    \"\"\" # noqa\n    level: int = 0\n    backend: str = \"\"\n    custom_ops: List[str] = Field(default_factory=list)\n    splitting_ops: List[str] = Field(default_factory=lambda: [\n        \"vllm.unified_attention\",\n        \"vllm.unified_attention_with_output\",\n    ])\n\n    use_inductor: bool = True\n    inductor_specialize_for_cudagraph_no_more_than: Optional[int] = None\n    inductor_compile_sizes: Optional[List[int]] = Field(default=None)\n    inductor_compile_config: Dict = Field(default_factory=dict)\n    inductor_passes: Dict[str, str] = Field(default_factory=dict)\n\n    use_cudagraph: bool = False\n    cudagraph_num_of_warmups: int = 0\n    cudagraph_capture_sizes: Optional[List[int]] = None\n    cudagraph_copy_inputs: bool = False\n\n    class PassConfig(BaseModel):\n        \"\"\"\n        Configuration for custom Inductor passes.\n        This is separate from general CompilationConfig so that inductor passes\n        don't all have access to full configuration - that would create a cycle\n        as the PassManager is set as a property of config.\n        - dump_graph_stages: list of stages for which we want to dump the graph.\n            Each pass defines its own stages (before, after, maybe in-between).\n        - dump_graph_dir: directory to dump the graphs. Default is .\n        - enable_fusion: whether to enable the custom fusion pass.\n        - enable_reshape: whether to enable the custom reshape elimination pass.\n            TODO better pass enabling system.\n        \"\"\"\n        dump_graph_stages: List[str] = Field(default_factory=list)\n        dump_graph_dir: Path = Field(default=Path(\".\"))\n        enable_fusion: bool = True\n        enable_reshape: bool = True\n\n        def uuid(self):\n            \"\"\"\n            Produces a hash unique to the pass configuration.\n            Any new fields that affect compilation should be added to the hash.\n            Do not include dump_graph_* in the hash - they don't affect\n            compilation.\n            \"\"\"\n            dict_ = self.model_dump(\n                include={\"enable_fusion\", \"enable_reshape\"})\n            encoded = json.dumps(dict_, sort_keys=True).encode(\"utf-8\")\n            return hashlib.sha256(encoded).digest()\n\n        def model_post_init(self, __context: Any) -> None:\n            if not self.enable_reshape and self.enable_fusion:\n                print_warning_once(\n                    \"Fusion enabled but reshape elimination disabled.\"\n                    \"RMSNorm + quant (fp8) fusion might not work\")\n\n    pass_config: PassConfig = Field(default_factory=PassConfig)\n\n    # not configurable, computed after init\n    compile_sizes: List[int] = PrivateAttr\n    capture_sizes: List[int] = PrivateAttr\n\n    # keep track of enabled and disabled custom ops\n    enabled_custom_ops: Counter[str] = PrivateAttr\n    disabled_custom_ops: Counter[str] = PrivateAttr\n\n    # Per-model forward context\n    # Mainly used to store attention cls\n    # Map from layer name to the attention cls\n    static_forward_context: Dict[str, Any] = PrivateAttr\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"CompilationConfig\":\n        \"\"\"Parse the CLI value for the compilation config.\"\"\"\n        if cli_value in [\"0\", \"1\", \"2\", \"3\"]:\n            return cls(level=int(cli_value))\n        return CompilationConfig.model_validate_json(cli_value)\n\n    def model_post_init(self, __context: Any) -> None:\n\n        count_none = self.custom_ops.count(\"none\")\n        count_all = self.custom_ops.count(\"all\")\n        assert count_none + count_all <= 1, \"Can only specify 'none' or 'all'\"\n\n        for k, v in self.inductor_passes.items():\n            if not isinstance(v, str):\n                assert callable(v), (\n                    f\"pass {k} should be callable or a qualified name\")\n                self.inductor_compile_config[k] = v if isinstance(\n                    v, InductorPass) else CallableInductorPass(v)\n                continue\n\n            # resolve function from qualified name\n            names = v.split(\".\")\n            module = \".\".join(names[:-1])\n            func_name = names[-1]\n            func = __import__(module).__dict__[func_name]\n            self.inductor_compile_config[k] = func if isinstance(\n                func, InductorPass) else CallableInductorPass(func)\n\n        self.enabled_custom_ops = Counter()\n        self.disabled_custom_ops = Counter()\n        self.static_forward_context = {}\n\n    def init_backend(self) -> Union[str, Callable]:\n        if self.level == CompilationLevel.NO_COMPILATION:\n            raise ValueError(\"No compilation level is set.\")\n\n        from torch._dynamo.backends.registry import list_backends\n        torch_backends = list_backends(exclude_tags=tuple())\n        if self.level in [\n                CompilationLevel.DYNAMO_AS_IS, CompilationLevel.DYNAMO_ONCE\n        ]:\n            if self.backend == \"\":\n                return \"eager\"\n            if self.backend in torch_backends:\n                return self.backend\n            return resolve_obj_by_qualname(self.backend)\n\n        # TODO: pass user-specified backend to piecewise compilation\n        # merge with the config use_inductor\n        assert self.level == CompilationLevel.PIECEWISE\n        from vllm.compilation.backends import VllmBackend\n        return VllmBackend(self)\n\n    def init_with_cudagraph_sizes(self, sizes_to_specialize: List[int]):\n        \"\"\"To complete the initialization of config,\n        we need to know the cudagraph sizes.\"\"\"\n\n        if self.cudagraph_capture_sizes is None:\n            self.capture_sizes = sizes_to_specialize\n        else:\n            self.capture_sizes = self.cudagraph_capture_sizes\n            logger.info((\"cudagraph sizes specified by model runner\"\n                         \" %s is overridden by config %s\"),\n                        sizes_to_specialize, self.cudagraph_capture_sizes)\n        if self.inductor_specialize_for_cudagraph_no_more_than is not None:\n            assert self.inductor_compile_sizes is None, (\n                \"inductor_compile_sizes should be None when \"\n                \"inductor_specialize_for_cudagraph_no_more_than is not None\")\n            self.compile_sizes = [\n                x for x in self.capture_sizes\n                if x <= self.inductor_specialize_for_cudagraph_no_more_than\n            ]\n        else:\n            if self.inductor_compile_sizes is None:\n                self.inductor_compile_sizes = []\n            self.compile_sizes = self.inductor_compile_sizes\n\n        # sort to make sure cudagraph capture sizes are in descending order\n        self.capture_sizes.sort(reverse=True)\n\n\n_BATCH_SIZE_ALIGNMENT = 8\n# all the token sizes that **can** be captured by cudagraph.\n# they can be arbitrarily large.\n# currently it includes: 1, 2, 4, 8, 16, 24, 32, 40, ..., 8192.\n# the actual sizes to capture will be determined by the model,\n# depending on the model's max_num_seqs.\n# NOTE: get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 1025)\n]\n\n\n@dataclass\nclass VllmConfig:\n    \"\"\"Dataclass which contains all vllm-related configuration. This\n    simplifies passing around the distinct configurations in the codebase.\n    \"\"\"\n\n    model_config: ModelConfig = field(default=None, init=True)  # type: ignore\n    cache_config: CacheConfig = field(default=None, init=True)  # type: ignore\n    parallel_config: ParallelConfig = field(default_factory=ParallelConfig,\n                                            init=True)\n    scheduler_config: SchedulerConfig = field(default_factory=SchedulerConfig,\n                                              init=True)\n    device_config: DeviceConfig = field(default=None,\n                                        init=True)  # type: ignore\n    load_config: LoadConfig = field(default=None, init=True)  # type: ignore\n    lora_config: Optional[LoRAConfig] = None\n    speculative_config: Optional[SpeculativeConfig] = None\n    decoding_config: Optional[DecodingConfig] = None\n    observability_config: Optional[ObservabilityConfig] = None\n    prompt_adapter_config: Optional[PromptAdapterConfig] = None\n    quant_config: Optional[QuantizationConfig] = None\n    compilation_config: CompilationConfig = field(default=None,\n                                                  init=True)  # type: ignore\n    kv_transfer_config: KVTransferConfig = field(default=None,\n                                                 init=True)  # type: ignore\n\n    @staticmethod\n    def get_graph_batch_size(batch_size: int) -> int:\n        \"\"\"Returns the padded batch size given actual batch size.\n\n        Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n        2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n        \"\"\"\n        if batch_size <= 2:\n            return batch_size\n        elif batch_size <= 4:\n            return 4\n        else:\n            return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                    _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n\n    @staticmethod\n    def get_max_graph_batch_size(max_num_seqs: int) -> int:\n        \"\"\"\n        max_num_seqs: Maximum number of sequences in a batch.\n        _BATCH_SIZES_TO_CAPTURE: all the sizes that we want to capture.\n\n        pad the max_num_seqs if necessary by calling get_graph_batch_size,\n        which will deal with some edge cases like 1, 2, 4.\n\n        if the padded size is in _BATCH_SIZES_TO_CAPTURE, return the padded\n        size. if not, it means the padded size is larger than the largest size\n        in _BATCH_SIZES_TO_CAPTURE, return the largest size in\n        _BATCH_SIZES_TO_CAPTURE.\n        \"\"\"\n        padded_size = VllmConfig.get_graph_batch_size(max_num_seqs)\n        if padded_size in _BATCH_SIZES_TO_CAPTURE:\n            return padded_size\n        assert padded_size > _BATCH_SIZES_TO_CAPTURE[-1]\n        return _BATCH_SIZES_TO_CAPTURE[-1]\n\n    @staticmethod\n    def _get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        \"\"\"Get the quantization config.\"\"\"\n        if model_config.quantization is not None:\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_quant_config)\n            quant_config = get_quant_config(model_config, load_config)\n            capability_tuple = current_platform.get_device_capability()\n\n            if capability_tuple is not None:\n                capability = capability_tuple.to_int()\n                if capability < quant_config.get_min_capability():\n                    raise ValueError(\n                        f\"The quantization method {model_config.quantization} \"\n                        \"is not supported for the current GPU. Minimum \"\n                        f\"capability: {quant_config.get_min_capability()}. \"\n                        f\"Current capability: {capability}.\")\n            supported_dtypes = quant_config.get_supported_act_dtypes()\n            if model_config.dtype not in supported_dtypes:\n                raise ValueError(\n                    f\"{model_config.dtype} is not supported for quantization \"\n                    f\"method {model_config.quantization}. Supported dtypes: \"\n                    f\"{supported_dtypes}\")\n            return quant_config\n        return None\n\n    def with_hf_config(self, hf_config: PretrainedConfig) -> \"VllmConfig\":\n        model_config = copy.deepcopy(self.model_config)\n        model_config.hf_config = hf_config\n\n        return replace(self, model_config=model_config)\n\n    def __post_init__(self):\n        \"\"\"Verify configs are valid & consistent with each other.\n        \"\"\"\n        if self.model_config is not None:\n            self.model_config.verify_async_output_proc(self.parallel_config,\n                                                       self.speculative_config,\n                                                       self.device_config)\n            self.model_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.cache_config is not None:\n            self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n        if self.quant_config is None and \\\n            self.model_config is not None and self.load_config is not None:\n            self.quant_config = VllmConfig._get_quantization_config(\n                self.model_config, self.load_config)\n\n        if self.scheduler_config is not None and \\\n            self.model_config is not None and \\\n            self.scheduler_config.chunked_prefill_enabled and \\\n            self.model_config.dtype == torch.float32 and \\\n            current_platform.get_device_capability() == (7, 5):\n            print_warning_once(\n                \"Turing devices tensor cores do not support float32 matmul. \"\n                \"To workaround this limitation, vLLM will set 'ieee' input \"\n                \"precision for chunked prefill triton kernels.\")\n\n        if self.compilation_config is None:\n            self.compilation_config = CompilationConfig()\n        if envs.VLLM_USE_V1 and not self.model_config.enforce_eager:\n            # NOTE(woosuk): Currently, we use inductor because the piecewise\n            # CUDA graphs do not work properly with the custom CUDA kernels.\n            # FIXME(woosuk): Disable inductor to reduce the compilation time\n            # and avoid any potential issues with the inductor.\n            self.compilation_config.custom_ops = [\"none\"]\n            self.compilation_config.use_cudagraph = True\n            self.compilation_config.use_inductor = True\n            self.compilation_config.pass_config.enable_fusion = False\n            self.compilation_config.pass_config.enable_reshape = False\n            self.compilation_config.level = CompilationLevel.PIECEWISE\n\n        if not envs.VLLM_USE_V1:\n            max_batchsize_to_capture = 0\n            if self.scheduler_config is not None and \\\n                self.model_config is not None and \\\n                    not self.model_config.enforce_eager:\n                max_batchsize_to_capture = \\\n                    self.get_max_graph_batch_size(\n                    self.scheduler_config.max_num_seqs)\n            batch_size_capture_list = [\n                size for size in _BATCH_SIZES_TO_CAPTURE\n                if size <= max_batchsize_to_capture\n            ]\n        else:\n            batch_size_capture_list = []\n            if self.model_config is not None and \\\n                not self.model_config.enforce_eager:\n                batch_size_capture_list = [1, 2, 4\n                                           ] + [i for i in range(8, 513, 8)]\n\n        self.compilation_config.init_with_cudagraph_sizes(\n            batch_size_capture_list)\n\n        if self.cache_config is not None and \\\n            self.cache_config.cpu_offload_gb > 0 and \\\n            self.compilation_config.level != CompilationLevel.NO_COMPILATION:\n            logger.warning(\n                \"CPU offload is not supported with `torch.compile` yet.\"\n                \" Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if self.lora_config is not None and self.compilation_config.level !=\\\n             CompilationLevel.NO_COMPILATION:\n            logger.warning(\"LoRA is not supported with `torch.compile` yet. \"\n                           \"Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        current_platform.check_and_update_config(self)\n\n    def __str__(self):\n        return (\"model=%r, speculative_config=%r, tokenizer=%r, \"\n        \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n        \"override_neuron_config=%s, tokenizer_revision=%s, \"\n        \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n        \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n        \"pipeline_parallel_size=%d, \"\n        \"disable_custom_all_reduce=%s, quantization=%s, \"\n        \"enforce_eager=%s, kv_cache_dtype=%s, \"\n        \"quantization_param_path=%s, device_config=%s, \"\n        \"decoding_config=%r, observability_config=%r, \"\n        \"seed=%d, served_model_name=%s, \"\n        \"num_scheduler_steps=%d, enable_prefix_caching=%s, \"\n        \"use_async_output_proc=%s, mm_processor_kwargs=%s\") % \\\n        (self.model_config.model, self.speculative_config,\n        self.model_config.tokenizer,\n        self.model_config.skip_tokenizer_init,\n        self.model_config.tokenizer_mode,\n        self.model_config.revision,\n        self.model_config.override_neuron_config,\n        self.model_config.tokenizer_revision,\n        self.model_config.trust_remote_code,\n        self.model_config.dtype,\n        self.model_config.max_model_len,\n        self.load_config.download_dir,\n        self.load_config.load_format,\n        self.parallel_config.tensor_parallel_size,\n        self.parallel_config.pipeline_parallel_size,\n        self.parallel_config.disable_custom_all_reduce,\n        self.model_config.quantization,\n        self.model_config.enforce_eager,\n        self.cache_config.cache_dtype,\n        self.model_config.quantization_param_path,\n        self.device_config.device, self.decoding_config,\n        self.observability_config, self.model_config.seed,\n        self.model_config.served_model_name,\n        self.scheduler_config.num_scheduler_steps,\n        self.cache_config.enable_prefix_caching,\n        self.model_config.use_async_output_proc,\n        self.model_config.mm_processor_kwargs)\n\n\n_current_vllm_config: Optional[VllmConfig] = None\n\n\n@contextmanager\ndef set_current_vllm_config(vllm_config: VllmConfig):\n    \"\"\"\n    Temporarily set the current VLLM config.\n    Used during model initialization.\n    We save the current VLLM config in a global variable,\n    so that all modules can access it, e.g. custom ops\n    can access the VLLM config to determine how to dispatch.\n    \"\"\"\n    global _current_vllm_config\n    old_vllm_config = _current_vllm_config\n    from vllm.compilation.counter import compilation_counter\n    num_models_seen = compilation_counter.num_models_seen\n    try:\n        _current_vllm_config = vllm_config\n        yield\n    finally:\n        logger.debug(\"enabled custom ops: %s\",\n                     vllm_config.compilation_config.enabled_custom_ops)\n        logger.debug(\"disabled custom ops: %s\",\n                     vllm_config.compilation_config.disabled_custom_ops)\n        if vllm_config.compilation_config.level == CompilationLevel.PIECEWISE \\\n            and compilation_counter.num_models_seen == num_models_seen:\n            # If the model supports compilation,\n            # compilation_counter.num_models_seen should be increased\n            # by at least 1.\n            # If it is not increased, it means the model does not support\n            # compilation (does not have @support_torch_compile decorator).\n            logger.warning(\n                \"`torch.compile` is turned on, but the model %s\"\n                \" does not support it. Please open an issue on GitHub\"\n                \"if you want it to be supported.\",\n                vllm_config.model_config.model)\n        _current_vllm_config = old_vllm_config\n\n\ndef get_current_vllm_config() -> VllmConfig:\n    if _current_vllm_config is None:\n        # in ci, usually when we test custom ops/modules directly,\n        # we don't set the vllm config. In that case, we set a default\n        # config.\n        logger.warning(\"Current VLLM config is not set.\")\n        from vllm.config import VllmConfig\n        return VllmConfig()\n    return _current_vllm_config\n",
      "diff": "diff --git a/vllm/config.py b/vllm/config.py\nindex 326340d3f..971eb36d6 100644\n--- a/vllm/config.py\n+++ b/vllm/config.py\n@@ -1789,15 +1789,15 @@ class PoolerConfig:\n \n     step_tag_id: Optional[int] = None\n     \"\"\"\n-    If set, only the score corresponding to the ``step_tag_id`` in the \n+    If set, only the score corresponding to the ``step_tag_id`` in the\n     generated sentence should be returned. Otherwise, the scores for all tokens\n     are returned.\n     \"\"\"\n \n     returned_token_ids: Optional[List[int]] = None\n     \"\"\"\n-    A list of indices for the vocabulary dimensions to be extracted, \n-    such as the token IDs of ``good_token`` and ``bad_token`` in the \n+    A list of indices for the vocabulary dimensions to be extracted,\n+    such as the token IDs of ``good_token`` and ``bad_token`` in the\n     ``math-shepherd-mistral-7b-prm`` model.\n     \"\"\"\n \n@@ -2031,11 +2031,12 @@ def get_served_model_name(model: str,\n class DecodingConfig:\n     \"\"\"Dataclass which contains the decoding strategy of the engine\"\"\"\n \n-    # Which guided decoding algo to use. 'outlines' / 'lm-format-enforcer'\n-    guided_decoding_backend: str = 'outlines'\n+    # Which guided decoding algo to use.\n+    # 'outlines' / 'lm-format-enforcer' / 'xgrammar'\n+    guided_decoding_backend: str = 'xgrammar'\n \n     def __post_init__(self):\n-        valid_guided_backends = ['outlines', 'lm-format-enforcer']\n+        valid_guided_backends = ['outlines', 'lm-format-enforcer', 'xgrammar']\n         backend = self.guided_decoding_backend\n         if backend not in valid_guided_backends:\n             raise ValueError(f\"Invalid guided_decoding_backend '{backend},\"\n@@ -2222,7 +2223,7 @@ class CompilationConfig(BaseModel):\n             from Python, functions can also be passed directly via Python object\n             constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`\n         - custom inductor passes: see PassConfig for more details\n-    \n+\n     Why we have different sizes for cudagraph and inductor:\n     - cudagraph: a cudagraph captured for a specific size can only be used\n         for the same size. We need to capture all the sizes we want to use.",
      "change_type": "modified",
      "lines_added": 9,
      "lines_removed": 8
    },
    {
      "file_path": "vllm/engine/arg_utils.py",
      "old_content": "import argparse\nimport dataclasses\nimport json\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Dict, List, Literal, Mapping, Optional,\n                    Tuple, Type, Union, cast, get_args)\n\nimport torch\n\nimport vllm.envs as envs\nfrom vllm.config import (CacheConfig, CompilationConfig, ConfigFormat,\n                         DecodingConfig, DeviceConfig, HfOverrides,\n                         KVTransferConfig, LoadConfig, LoadFormat, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PoolerConfig, PromptAdapterConfig, SchedulerConfig,\n                         SpeculativeConfig, TaskOption, TokenizerPoolConfig,\n                         VllmConfig)\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\nfrom vllm.platforms import current_platform\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import FlexibleArgumentParser, StoreBoolean\n\nif TYPE_CHECKING:\n    from vllm.transformers_utils.tokenizer_group import BaseTokenizerGroup\n\nlogger = init_logger(__name__)\n\nALLOWED_DETAILED_TRACE_MODULES = [\"model\", \"worker\", \"all\"]\n\nDEVICE_OPTIONS = [\n    \"auto\",\n    \"cuda\",\n    \"neuron\",\n    \"cpu\",\n    \"openvino\",\n    \"tpu\",\n    \"xpu\",\n    \"hpu\",\n]\n\n\ndef nullable_str(val: str):\n    if not val or val == \"None\":\n        return None\n    return val\n\n\ndef nullable_kvs(val: str) -> Optional[Mapping[str, int]]:\n    \"\"\"Parses a string containing comma separate key [str] to value [int]\n    pairs into a dictionary.\n\n    Args:\n        val: String value to be parsed.\n\n    Returns:\n        Dictionary with parsed values.\n    \"\"\"\n    if len(val) == 0:\n        return None\n\n    out_dict: Dict[str, int] = {}\n    for item in val.split(\",\"):\n        kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n        if len(kv_parts) != 2:\n            raise argparse.ArgumentTypeError(\n                \"Each item should be in the form KEY=VALUE\")\n        key, value = kv_parts\n\n        try:\n            parsed_value = int(value)\n        except ValueError as exc:\n            msg = f\"Failed to parse value of item {key}={value}\"\n            raise argparse.ArgumentTypeError(msg) from exc\n\n        if key in out_dict and out_dict[key] != parsed_value:\n            raise argparse.ArgumentTypeError(\n                f\"Conflicting values specified for key: {key}\")\n        out_dict[key] = parsed_value\n\n    return out_dict\n\n\n@dataclass\nclass EngineArgs:\n    \"\"\"Arguments for vLLM engine.\"\"\"\n    model: str = 'facebook/opt-125m'\n    served_model_name: Optional[Union[str, List[str]]] = None\n    tokenizer: Optional[str] = None\n    task: TaskOption = \"auto\"\n    skip_tokenizer_init: bool = False\n    tokenizer_mode: str = 'auto'\n    trust_remote_code: bool = False\n    allowed_local_media_path: str = \"\"\n    download_dir: Optional[str] = None\n    load_format: str = 'auto'\n    config_format: ConfigFormat = ConfigFormat.AUTO\n    dtype: str = 'auto'\n    kv_cache_dtype: str = 'auto'\n    quantization_param_path: Optional[str] = None\n    seed: int = 0\n    max_model_len: Optional[int] = None\n    worker_use_ray: bool = False\n    # Note: Specifying a custom executor backend by passing a class\n    # is intended for expert use only. The API may change without\n    # notice.\n    distributed_executor_backend: Optional[Union[str,\n                                                 Type[ExecutorBase]]] = None\n    # number of P/D disaggregation (or other disaggregation) workers\n    pipeline_parallel_size: int = 1\n    tensor_parallel_size: int = 1\n    max_parallel_loading_workers: Optional[int] = None\n    # NOTE(kzawora): default block size for Gaudi should be 128\n    # smaller sizes still work, but very inefficiently\n    block_size: int = 16 if not current_platform.is_hpu() else 128\n    enable_prefix_caching: Optional[bool] = None\n    disable_sliding_window: bool = False\n    use_v2_block_manager: bool = True\n    swap_space: float = 4  # GiB\n    cpu_offload_gb: float = 0  # GiB\n    gpu_memory_utilization: float = 0.90\n    max_num_batched_tokens: Optional[int] = None\n    max_num_seqs: int = 256\n    max_logprobs: int = 20  # Default value for OpenAI Chat Completions API\n    disable_log_stats: bool = False\n    revision: Optional[str] = None\n    code_revision: Optional[str] = None\n    rope_scaling: Optional[Dict[str, Any]] = None\n    rope_theta: Optional[float] = None\n    hf_overrides: Optional[HfOverrides] = None\n    tokenizer_revision: Optional[str] = None\n    quantization: Optional[str] = None\n    enforce_eager: Optional[bool] = None\n    max_seq_len_to_capture: int = 8192\n    disable_custom_all_reduce: bool = False\n    tokenizer_pool_size: int = 0\n    # Note: Specifying a tokenizer pool by passing a class\n    # is intended for expert use only. The API may change without\n    # notice.\n    tokenizer_pool_type: Union[str, Type[\"BaseTokenizerGroup\"]] = \"ray\"\n    tokenizer_pool_extra_config: Optional[Dict[str, Any]] = None\n    limit_mm_per_prompt: Optional[Mapping[str, int]] = None\n    mm_processor_kwargs: Optional[Dict[str, Any]] = None\n    enable_lora: bool = False\n    enable_lora_bias: bool = False\n    max_loras: int = 1\n    max_lora_rank: int = 16\n    enable_prompt_adapter: bool = False\n    max_prompt_adapters: int = 1\n    max_prompt_adapter_token: int = 0\n    fully_sharded_loras: bool = False\n    lora_extra_vocab_size: int = 256\n    long_lora_scaling_factors: Optional[Tuple[float]] = None\n    lora_dtype: Optional[Union[str, torch.dtype]] = 'auto'\n    max_cpu_loras: Optional[int] = None\n    device: str = 'auto'\n    num_scheduler_steps: int = 1\n    multi_step_stream_outputs: bool = True\n    ray_workers_use_nsight: bool = False\n    num_gpu_blocks_override: Optional[int] = None\n    num_lookahead_slots: int = 0\n    model_loader_extra_config: Optional[dict] = None\n    ignore_patterns: Optional[Union[str, List[str]]] = None\n    preemption_mode: Optional[str] = None\n\n    scheduler_delay_factor: float = 0.0\n    enable_chunked_prefill: Optional[bool] = None\n\n    guided_decoding_backend: str = 'outlines'\n    # Speculative decoding configuration.\n    speculative_model: Optional[str] = None\n    speculative_model_quantization: Optional[str] = None\n    speculative_draft_tensor_parallel_size: Optional[int] = None\n    num_speculative_tokens: Optional[int] = None\n    speculative_disable_mqa_scorer: Optional[bool] = False\n    speculative_max_model_len: Optional[int] = None\n    speculative_disable_by_batch_size: Optional[int] = None\n    ngram_prompt_lookup_max: Optional[int] = None\n    ngram_prompt_lookup_min: Optional[int] = None\n    spec_decoding_acceptance_method: str = 'rejection_sampler'\n    typical_acceptance_sampler_posterior_threshold: Optional[float] = None\n    typical_acceptance_sampler_posterior_alpha: Optional[float] = None\n    qlora_adapter_name_or_path: Optional[str] = None\n    disable_logprobs_during_spec_decoding: Optional[bool] = None\n\n    otlp_traces_endpoint: Optional[str] = None\n    collect_detailed_traces: Optional[str] = None\n    disable_async_output_proc: bool = False\n    scheduling_policy: Literal[\"fcfs\", \"priority\"] = \"fcfs\"\n\n    override_neuron_config: Optional[Dict[str, Any]] = None\n    override_pooler_config: Optional[PoolerConfig] = None\n    compilation_config: Optional[CompilationConfig] = None\n    worker_cls: str = \"auto\"\n\n    kv_transfer_config: Optional[KVTransferConfig] = None\n\n    def __post_init__(self):\n        if not self.tokenizer:\n            self.tokenizer = self.model\n\n        # Override the default value of enable_prefix_caching if it's not set\n        # by user.\n        if self.enable_prefix_caching is None:\n            self.enable_prefix_caching = bool(envs.VLLM_USE_V1)\n\n        # support `EngineArgs(compilation_config={...})`\n        # without having to manually construct a\n        # CompilationConfig object\n        if isinstance(self.compilation_config, (int)):\n            self.compilation_config = CompilationConfig.from_cli(\n                str(self.compilation_config))\n        elif isinstance(self.compilation_config, (dict)):\n            self.compilation_config = CompilationConfig.from_cli(\n                json.dumps(self.compilation_config))\n\n        # Setup plugins\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n        \"\"\"Shared CLI arguments for vLLM engine.\"\"\"\n\n        # Model arguments\n        parser.add_argument(\n            '--model',\n            type=str,\n            default=EngineArgs.model,\n            help='Name or path of the huggingface model to use.')\n        parser.add_argument(\n            '--task',\n            default=EngineArgs.task,\n            choices=get_args(TaskOption),\n            help='The task to use the model for. Each vLLM instance only '\n            'supports one task, even if the same model can be used for '\n            'multiple tasks. When the model only supports one task, \"auto\" '\n            'can be used to select it; otherwise, you must specify explicitly '\n            'which task to use.')\n        parser.add_argument(\n            '--tokenizer',\n            type=nullable_str,\n            default=EngineArgs.tokenizer,\n            help='Name or path of the huggingface tokenizer to use. '\n            'If unspecified, model name or path will be used.')\n        parser.add_argument(\n            '--skip-tokenizer-init',\n            action='store_true',\n            help='Skip initialization of tokenizer and detokenizer')\n        parser.add_argument(\n            '--revision',\n            type=nullable_str,\n            default=None,\n            help='The specific model version to use. It can be a branch '\n            'name, a tag name, or a commit id. If unspecified, will use '\n            'the default version.')\n        parser.add_argument(\n            '--code-revision',\n            type=nullable_str,\n            default=None,\n            help='The specific revision to use for the model code on '\n            'Hugging Face Hub. It can be a branch name, a tag name, or a '\n            'commit id. If unspecified, will use the default version.')\n        parser.add_argument(\n            '--tokenizer-revision',\n            type=nullable_str,\n            default=None,\n            help='Revision of the huggingface tokenizer to use. '\n            'It can be a branch name, a tag name, or a commit id. '\n            'If unspecified, will use the default version.')\n        parser.add_argument(\n            '--tokenizer-mode',\n            type=str,\n            default=EngineArgs.tokenizer_mode,\n            choices=['auto', 'slow', 'mistral'],\n            help='The tokenizer mode.\\n\\n* \"auto\" will use the '\n            'fast tokenizer if available.\\n* \"slow\" will '\n            'always use the slow tokenizer. \\n* '\n            '\"mistral\" will always use the `mistral_common` tokenizer.')\n        parser.add_argument('--trust-remote-code',\n                            action='store_true',\n                            help='Trust remote code from huggingface.')\n        parser.add_argument(\n            '--allowed-local-media-path',\n            type=str,\n            help=\"Allowing API requests to read local images or videos \"\n            \"from directories specified by the server file system. \"\n            \"This is a security risk. \"\n            \"Should only be enabled in trusted environments.\")\n        parser.add_argument('--download-dir',\n                            type=nullable_str,\n                            default=EngineArgs.download_dir,\n                            help='Directory to download and load the weights, '\n                            'default to the default cache dir of '\n                            'huggingface.')\n        parser.add_argument(\n            '--load-format',\n            type=str,\n            default=EngineArgs.load_format,\n            choices=[f.value for f in LoadFormat],\n            help='The format of the model weights to load.\\n\\n'\n            '* \"auto\" will try to load the weights in the safetensors format '\n            'and fall back to the pytorch bin format if safetensors format '\n            'is not available.\\n'\n            '* \"pt\" will load the weights in the pytorch bin format.\\n'\n            '* \"safetensors\" will load the weights in the safetensors format.\\n'\n            '* \"npcache\" will load the weights in pytorch format and store '\n            'a numpy cache to speed up the loading.\\n'\n            '* \"dummy\" will initialize the weights with random values, '\n            'which is mainly for profiling.\\n'\n            '* \"tensorizer\" will load the weights using tensorizer from '\n            'CoreWeave. See the Tensorize vLLM Model script in the Examples '\n            'section for more information.\\n'\n            '* \"bitsandbytes\" will load the weights using bitsandbytes '\n            'quantization.\\n')\n        parser.add_argument(\n            '--config-format',\n            default=EngineArgs.config_format,\n            choices=[f.value for f in ConfigFormat],\n            help='The format of the model config to load.\\n\\n'\n            '* \"auto\" will try to load the config in hf format '\n            'if available else it will try to load in mistral format ')\n        parser.add_argument(\n            '--dtype',\n            type=str,\n            default=EngineArgs.dtype,\n            choices=[\n                'auto', 'half', 'float16', 'bfloat16', 'float', 'float32'\n            ],\n            help='Data type for model weights and activations.\\n\\n'\n            '* \"auto\" will use FP16 precision for FP32 and FP16 models, and '\n            'BF16 precision for BF16 models.\\n'\n            '* \"half\" for FP16. Recommended for AWQ quantization.\\n'\n            '* \"float16\" is the same as \"half\".\\n'\n            '* \"bfloat16\" for a balance between precision and range.\\n'\n            '* \"float\" is shorthand for FP32 precision.\\n'\n            '* \"float32\" for FP32 precision.')\n        parser.add_argument(\n            '--kv-cache-dtype',\n            type=str,\n            choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n            default=EngineArgs.kv_cache_dtype,\n            help='Data type for kv cache storage. If \"auto\", will use model '\n            'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n            'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n        parser.add_argument(\n            '--quantization-param-path',\n            type=nullable_str,\n            default=None,\n            help='Path to the JSON file containing the KV cache '\n            'scaling factors. This should generally be supplied, when '\n            'KV cache dtype is FP8. Otherwise, KV cache scaling factors '\n            'default to 1.0, which may cause accuracy issues. '\n            'FP8_E5M2 (without scaling) is only supported on cuda version '\n            'greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is instead '\n            'supported for common inference criteria.')\n        parser.add_argument('--max-model-len',\n                            type=int,\n                            default=EngineArgs.max_model_len,\n                            help='Model context length. If unspecified, will '\n                            'be automatically derived from the model config.')\n        parser.add_argument(\n            '--guided-decoding-backend',\n            type=str,\n            default='outlines',\n            choices=['outlines', 'lm-format-enforcer'],\n            help='Which engine will be used for guided decoding'\n            ' (JSON schema / regex etc) by default. Currently support '\n            'https://github.com/outlines-dev/outlines and '\n            'https://github.com/noamgat/lm-format-enforcer.'\n            ' Can be overridden per request via guided_decoding_backend'\n            ' parameter.')\n        # Parallel arguments\n        parser.add_argument(\n            '--distributed-executor-backend',\n            choices=['ray', 'mp'],\n            default=EngineArgs.distributed_executor_backend,\n            help='Backend to use for distributed model '\n            'workers, either \"ray\" or \"mp\" (multiprocessing). If the product '\n            'of pipeline_parallel_size and tensor_parallel_size is less than '\n            'or equal to the number of GPUs available, \"mp\" will be used to '\n            'keep processing on a single host. Otherwise, this will default '\n            'to \"ray\" if Ray is installed and fail otherwise. Note that tpu '\n            'and hpu only support Ray for distributed inference.')\n\n        parser.add_argument(\n            '--worker-use-ray',\n            action='store_true',\n            help='Deprecated, use --distributed-executor-backend=ray.')\n        parser.add_argument('--pipeline-parallel-size',\n                            '-pp',\n                            type=int,\n                            default=EngineArgs.pipeline_parallel_size,\n                            help='Number of pipeline stages.')\n        parser.add_argument('--tensor-parallel-size',\n                            '-tp',\n                            type=int,\n                            default=EngineArgs.tensor_parallel_size,\n                            help='Number of tensor parallel replicas.')\n        parser.add_argument(\n            '--max-parallel-loading-workers',\n            type=int,\n            default=EngineArgs.max_parallel_loading_workers,\n            help='Load model sequentially in multiple batches, '\n            'to avoid RAM OOM when using tensor '\n            'parallel and large models.')\n        parser.add_argument(\n            '--ray-workers-use-nsight',\n            action='store_true',\n            help='If specified, use nsight to profile Ray workers.')\n        # KV cache arguments\n        parser.add_argument('--block-size',\n                            type=int,\n                            default=EngineArgs.block_size,\n                            choices=[8, 16, 32, 64, 128],\n                            help='Token block size for contiguous chunks of '\n                            'tokens. This is ignored on neuron devices and '\n                            'set to max-model-len')\n\n        parser.add_argument(\n            \"--enable-prefix-caching\",\n            action=argparse.BooleanOptionalAction,\n            default=EngineArgs.enable_prefix_caching,\n            help=\"Enables automatic prefix caching. \"\n            \"Use --no-enable-prefix-caching to disable explicitly.\",\n        )\n        parser.add_argument('--disable-sliding-window',\n                            action='store_true',\n                            help='Disables sliding window, '\n                            'capping to sliding window size')\n        parser.add_argument('--use-v2-block-manager',\n                            action='store_true',\n                            help='[DEPRECATED] block manager v1 has been '\n                            'removed and SelfAttnBlockSpaceManager (i.e. '\n                            'block manager v2) is now the default. '\n                            'Setting this flag to True or False'\n                            ' has no effect on vLLM behavior.')\n        parser.add_argument(\n            '--num-lookahead-slots',\n            type=int,\n            default=EngineArgs.num_lookahead_slots,\n            help='Experimental scheduling config necessary for '\n            'speculative decoding. This will be replaced by '\n            'speculative config in the future; it is present '\n            'to enable correctness tests until then.')\n\n        parser.add_argument('--seed',\n                            type=int,\n                            default=EngineArgs.seed,\n                            help='Random seed for operations.')\n        parser.add_argument('--swap-space',\n                            type=float,\n                            default=EngineArgs.swap_space,\n                            help='CPU swap space size (GiB) per GPU.')\n        parser.add_argument(\n            '--cpu-offload-gb',\n            type=float,\n            default=0,\n            help='The space in GiB to offload to CPU, per GPU. '\n            'Default is 0, which means no offloading. Intuitively, '\n            'this argument can be seen as a virtual way to increase '\n            'the GPU memory size. For example, if you have one 24 GB '\n            'GPU and set this to 10, virtually you can think of it as '\n            'a 34 GB GPU. Then you can load a 13B model with BF16 weight, '\n            'which requires at least 26GB GPU memory. Note that this '\n            'requires fast CPU-GPU interconnect, as part of the model is '\n            'loaded from CPU memory to GPU memory on the fly in each '\n            'model forward pass.')\n        parser.add_argument(\n            '--gpu-memory-utilization',\n            type=float,\n            default=EngineArgs.gpu_memory_utilization,\n            help='The fraction of GPU memory to be used for the model '\n            'executor, which can range from 0 to 1. For example, a value of '\n            '0.5 would imply 50%% GPU memory utilization. If unspecified, '\n            'will use the default value of 0.9. This is a global gpu memory '\n            'utilization limit, for example if 50%% of the gpu memory is '\n            'already used before vLLM starts and --gpu-memory-utilization is '\n            'set to 0.9, then only 40%% of the gpu memory will be allocated '\n            'to the model executor.')\n        parser.add_argument(\n            '--num-gpu-blocks-override',\n            type=int,\n            default=None,\n            help='If specified, ignore GPU profiling result and use this number'\n            ' of GPU blocks. Used for testing preemption.')\n        parser.add_argument('--max-num-batched-tokens',\n                            type=int,\n                            default=EngineArgs.max_num_batched_tokens,\n                            help='Maximum number of batched tokens per '\n                            'iteration.')\n        parser.add_argument('--max-num-seqs',\n                            type=int,\n                            default=EngineArgs.max_num_seqs,\n                            help='Maximum number of sequences per iteration.')\n        parser.add_argument(\n            '--max-logprobs',\n            type=int,\n            default=EngineArgs.max_logprobs,\n            help=('Max number of log probs to return logprobs is specified in'\n                  ' SamplingParams.'))\n        parser.add_argument('--disable-log-stats',\n                            action='store_true',\n                            help='Disable logging statistics.')\n        # Quantization settings.\n        parser.add_argument('--quantization',\n                            '-q',\n                            type=nullable_str,\n                            choices=[*QUANTIZATION_METHODS, None],\n                            default=EngineArgs.quantization,\n                            help='Method used to quantize the weights. If '\n                            'None, we first check the `quantization_config` '\n                            'attribute in the model config file. If that is '\n                            'None, we assume the model weights are not '\n                            'quantized and use `dtype` to determine the data '\n                            'type of the weights.')\n        parser.add_argument(\n            '--rope-scaling',\n            default=None,\n            type=json.loads,\n            help='RoPE scaling configuration in JSON format. '\n            'For example, {\"rope_type\":\"dynamic\",\"factor\":2.0}')\n        parser.add_argument('--rope-theta',\n                            default=None,\n                            type=float,\n                            help='RoPE theta. Use with `rope_scaling`. In '\n                            'some cases, changing the RoPE theta improves the '\n                            'performance of the scaled model.')\n        parser.add_argument('--hf-overrides',\n                            type=json.loads,\n                            default=EngineArgs.hf_overrides,\n                            help='Extra arguments for the HuggingFace config. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary.')\n        parser.add_argument('--enforce-eager',\n                            action='store_true',\n                            help='Always use eager-mode PyTorch. If False, '\n                            'will use eager mode and CUDA graph in hybrid '\n                            'for maximal performance and flexibility.')\n        parser.add_argument('--max-seq-len-to-capture',\n                            type=int,\n                            default=EngineArgs.max_seq_len_to_capture,\n                            help='Maximum sequence length covered by CUDA '\n                            'graphs. When a sequence has context length '\n                            'larger than this, we fall back to eager mode. '\n                            'Additionally for encoder-decoder models, if the '\n                            'sequence length of the encoder input is larger '\n                            'than this, we fall back to the eager mode.')\n        parser.add_argument('--disable-custom-all-reduce',\n                            action='store_true',\n                            default=EngineArgs.disable_custom_all_reduce,\n                            help='See ParallelConfig.')\n        parser.add_argument('--tokenizer-pool-size',\n                            type=int,\n                            default=EngineArgs.tokenizer_pool_size,\n                            help='Size of tokenizer pool to use for '\n                            'asynchronous tokenization. If 0, will '\n                            'use synchronous tokenization.')\n        parser.add_argument('--tokenizer-pool-type',\n                            type=str,\n                            default=EngineArgs.tokenizer_pool_type,\n                            help='Type of tokenizer pool to use for '\n                            'asynchronous tokenization. Ignored '\n                            'if tokenizer_pool_size is 0.')\n        parser.add_argument('--tokenizer-pool-extra-config',\n                            type=nullable_str,\n                            default=EngineArgs.tokenizer_pool_extra_config,\n                            help='Extra config for tokenizer pool. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary. Ignored if '\n                            'tokenizer_pool_size is 0.')\n\n        # Multimodal related configs\n        parser.add_argument(\n            '--limit-mm-per-prompt',\n            type=nullable_kvs,\n            default=EngineArgs.limit_mm_per_prompt,\n            # The default value is given in\n            # MultiModalRegistry.init_mm_limits_per_prompt\n            help=('For each multimodal plugin, limit how many '\n                  'input instances to allow for each prompt. '\n                  'Expects a comma-separated list of items, '\n                  'e.g.: `image=16,video=2` allows a maximum of 16 '\n                  'images and 2 videos per prompt. Defaults to 1 for '\n                  'each modality.'))\n        parser.add_argument(\n            '--mm-processor-kwargs',\n            default=None,\n            type=json.loads,\n            help=('Overrides for the multimodal input mapping/processing, '\n                  'e.g., image processor. For example: {\"num_crops\": 4}.'))\n\n        # LoRA related configs\n        parser.add_argument('--enable-lora',\n                            action='store_true',\n                            help='If True, enable handling of LoRA adapters.')\n        parser.add_argument('--enable-lora-bias',\n                            action='store_true',\n                            help='If True, enable bias for LoRA adapters.')\n        parser.add_argument('--max-loras',\n                            type=int,\n                            default=EngineArgs.max_loras,\n                            help='Max number of LoRAs in a single batch.')\n        parser.add_argument('--max-lora-rank',\n                            type=int,\n                            default=EngineArgs.max_lora_rank,\n                            help='Max LoRA rank.')\n        parser.add_argument(\n            '--lora-extra-vocab-size',\n            type=int,\n            default=EngineArgs.lora_extra_vocab_size,\n            help=('Maximum size of extra vocabulary that can be '\n                  'present in a LoRA adapter (added to the base '\n                  'model vocabulary).'))\n        parser.add_argument(\n            '--lora-dtype',\n            type=str,\n            default=EngineArgs.lora_dtype,\n            choices=['auto', 'float16', 'bfloat16'],\n            help=('Data type for LoRA. If auto, will default to '\n                  'base model dtype.'))\n        parser.add_argument(\n            '--long-lora-scaling-factors',\n            type=nullable_str,\n            default=EngineArgs.long_lora_scaling_factors,\n            help=('Specify multiple scaling factors (which can '\n                  'be different from base model scaling factor '\n                  '- see eg. Long LoRA) to allow for multiple '\n                  'LoRA adapters trained with those scaling '\n                  'factors to be used at the same time. If not '\n                  'specified, only adapters trained with the '\n                  'base model scaling factor are allowed.'))\n        parser.add_argument(\n            '--max-cpu-loras',\n            type=int,\n            default=EngineArgs.max_cpu_loras,\n            help=('Maximum number of LoRAs to store in CPU memory. '\n                  'Must be >= than max_loras. '\n                  'Defaults to max_loras.'))\n        parser.add_argument(\n            '--fully-sharded-loras',\n            action='store_true',\n            help=('By default, only half of the LoRA computation is '\n                  'sharded with tensor parallelism. '\n                  'Enabling this will use the fully sharded layers. '\n                  'At high sequence length, max rank or '\n                  'tensor parallel size, this is likely faster.'))\n        parser.add_argument('--enable-prompt-adapter',\n                            action='store_true',\n                            help='If True, enable handling of PromptAdapters.')\n        parser.add_argument('--max-prompt-adapters',\n                            type=int,\n                            default=EngineArgs.max_prompt_adapters,\n                            help='Max number of PromptAdapters in a batch.')\n        parser.add_argument('--max-prompt-adapter-token',\n                            type=int,\n                            default=EngineArgs.max_prompt_adapter_token,\n                            help='Max number of PromptAdapters tokens')\n        parser.add_argument(\"--device\",\n                            type=str,\n                            default=EngineArgs.device,\n                            choices=DEVICE_OPTIONS,\n                            help='Device type for vLLM execution.')\n        parser.add_argument('--num-scheduler-steps',\n                            type=int,\n                            default=1,\n                            help=('Maximum number of forward steps per '\n                                  'scheduler call.'))\n\n        parser.add_argument(\n            '--multi-step-stream-outputs',\n            action=StoreBoolean,\n            default=EngineArgs.multi_step_stream_outputs,\n            nargs=\"?\",\n            const=\"True\",\n            help='If False, then multi-step will stream outputs at the end '\n            'of all steps')\n        parser.add_argument(\n            '--scheduler-delay-factor',\n            type=float,\n            default=EngineArgs.scheduler_delay_factor,\n            help='Apply a delay (of delay factor multiplied by previous '\n            'prompt latency) before scheduling next prompt.')\n        parser.add_argument(\n            '--enable-chunked-prefill',\n            action=StoreBoolean,\n            default=EngineArgs.enable_chunked_prefill,\n            nargs=\"?\",\n            const=\"True\",\n            help='If set, the prefill requests can be chunked based on the '\n            'max_num_batched_tokens.')\n\n        parser.add_argument(\n            '--speculative-model',\n            type=nullable_str,\n            default=EngineArgs.speculative_model,\n            help=\n            'The name of the draft model to be used in speculative decoding.')\n        # Quantization settings for speculative model.\n        parser.add_argument(\n            '--speculative-model-quantization',\n            type=nullable_str,\n            choices=[*QUANTIZATION_METHODS, None],\n            default=EngineArgs.speculative_model_quantization,\n            help='Method used to quantize the weights of speculative model. '\n            'If None, we first check the `quantization_config` '\n            'attribute in the model config file. If that is '\n            'None, we assume the model weights are not '\n            'quantized and use `dtype` to determine the data '\n            'type of the weights.')\n        parser.add_argument(\n            '--num-speculative-tokens',\n            type=int,\n            default=EngineArgs.num_speculative_tokens,\n            help='The number of speculative tokens to sample from '\n            'the draft model in speculative decoding.')\n        parser.add_argument(\n            '--speculative-disable-mqa-scorer',\n            action='store_true',\n            help=\n            'If set to True, the MQA scorer will be disabled in speculative '\n            ' and fall back to batch expansion')\n        parser.add_argument(\n            '--speculative-draft-tensor-parallel-size',\n            '-spec-draft-tp',\n            type=int,\n            default=EngineArgs.speculative_draft_tensor_parallel_size,\n            help='Number of tensor parallel replicas for '\n            'the draft model in speculative decoding.')\n\n        parser.add_argument(\n            '--speculative-max-model-len',\n            type=int,\n            default=EngineArgs.speculative_max_model_len,\n            help='The maximum sequence length supported by the '\n            'draft model. Sequences over this length will skip '\n            'speculation.')\n\n        parser.add_argument(\n            '--speculative-disable-by-batch-size',\n            type=int,\n            default=EngineArgs.speculative_disable_by_batch_size,\n            help='Disable speculative decoding for new incoming requests '\n            'if the number of enqueue requests is larger than this value.')\n\n        parser.add_argument(\n            '--ngram-prompt-lookup-max',\n            type=int,\n            default=EngineArgs.ngram_prompt_lookup_max,\n            help='Max size of window for ngram prompt lookup in speculative '\n            'decoding.')\n\n        parser.add_argument(\n            '--ngram-prompt-lookup-min',\n            type=int,\n            default=EngineArgs.ngram_prompt_lookup_min,\n            help='Min size of window for ngram prompt lookup in speculative '\n            'decoding.')\n\n        parser.add_argument(\n            '--spec-decoding-acceptance-method',\n            type=str,\n            default=EngineArgs.spec_decoding_acceptance_method,\n            choices=['rejection_sampler', 'typical_acceptance_sampler'],\n            help='Specify the acceptance method to use during draft token '\n            'verification in speculative decoding. Two types of acceptance '\n            'routines are supported: '\n            '1) RejectionSampler which does not allow changing the '\n            'acceptance rate of draft tokens, '\n            '2) TypicalAcceptanceSampler which is configurable, allowing for '\n            'a higher acceptance rate at the cost of lower quality, '\n            'and vice versa.')\n\n        parser.add_argument(\n            '--typical-acceptance-sampler-posterior-threshold',\n            type=float,\n            default=EngineArgs.typical_acceptance_sampler_posterior_threshold,\n            help='Set the lower bound threshold for the posterior '\n            'probability of a token to be accepted. This threshold is '\n            'used by the TypicalAcceptanceSampler to make sampling decisions '\n            'during speculative decoding. Defaults to 0.09')\n\n        parser.add_argument(\n            '--typical-acceptance-sampler-posterior-alpha',\n            type=float,\n            default=EngineArgs.typical_acceptance_sampler_posterior_alpha,\n            help='A scaling factor for the entropy-based threshold for token '\n            'acceptance in the TypicalAcceptanceSampler. Typically defaults '\n            'to sqrt of --typical-acceptance-sampler-posterior-threshold '\n            'i.e. 0.3')\n\n        parser.add_argument(\n            '--disable-logprobs-during-spec-decoding',\n            action=StoreBoolean,\n            default=EngineArgs.disable_logprobs_during_spec_decoding,\n            nargs=\"?\",\n            const=\"True\",\n            help='If set to True, token log probabilities are not returned '\n            'during speculative decoding. If set to False, log probabilities '\n            'are returned according to the settings in SamplingParams. If '\n            'not specified, it defaults to True. Disabling log probabilities '\n            'during speculative decoding reduces latency by skipping logprob '\n            'calculation in proposal sampling, target sampling, and after '\n            'accepted tokens are determined.')\n\n        parser.add_argument('--model-loader-extra-config',\n                            type=nullable_str,\n                            default=EngineArgs.model_loader_extra_config,\n                            help='Extra config for model loader. '\n                            'This will be passed to the model loader '\n                            'corresponding to the chosen load_format. '\n                            'This should be a JSON string that will be '\n                            'parsed into a dictionary.')\n        parser.add_argument(\n            '--ignore-patterns',\n            action=\"append\",\n            type=str,\n            default=[],\n            help=\"The pattern(s) to ignore when loading the model.\"\n            \"Default to `original/**/*` to avoid repeated loading of llama's \"\n            \"checkpoints.\")\n        parser.add_argument(\n            '--preemption-mode',\n            type=str,\n            default=None,\n            help='If \\'recompute\\', the engine performs preemption by '\n            'recomputing; If \\'swap\\', the engine performs preemption by '\n            'block swapping.')\n\n        parser.add_argument(\n            \"--served-model-name\",\n            nargs=\"+\",\n            type=str,\n            default=None,\n            help=\"The model name(s) used in the API. If multiple \"\n            \"names are provided, the server will respond to any \"\n            \"of the provided names. The model name in the model \"\n            \"field of a response will be the first name in this \"\n            \"list. If not specified, the model name will be the \"\n            \"same as the `--model` argument. Noted that this name(s) \"\n            \"will also be used in `model_name` tag content of \"\n            \"prometheus metrics, if multiple names provided, metrics \"\n            \"tag will take the first one.\")\n        parser.add_argument('--qlora-adapter-name-or-path',\n                            type=str,\n                            default=None,\n                            help='Name or path of the QLoRA adapter.')\n\n        parser.add_argument(\n            '--otlp-traces-endpoint',\n            type=str,\n            default=None,\n            help='Target URL to which OpenTelemetry traces will be sent.')\n        parser.add_argument(\n            '--collect-detailed-traces',\n            type=str,\n            default=None,\n            help=\"Valid choices are \" +\n            \",\".join(ALLOWED_DETAILED_TRACE_MODULES) +\n            \". It makes sense to set this only if --otlp-traces-endpoint is\"\n            \" set. If set, it will collect detailed traces for the specified \"\n            \"modules. This involves use of possibly costly and or blocking \"\n            \"operations and hence might have a performance impact.\")\n\n        parser.add_argument(\n            '--disable-async-output-proc',\n            action='store_true',\n            default=EngineArgs.disable_async_output_proc,\n            help=\"Disable async output processing. This may result in \"\n            \"lower performance.\")\n\n        parser.add_argument(\n            '--scheduling-policy',\n            choices=['fcfs', 'priority'],\n            default=\"fcfs\",\n            help='The scheduling policy to use. \"fcfs\" (first come first served'\n            ', i.e. requests are handled in order of arrival; default) '\n            'or \"priority\" (requests are handled based on given '\n            'priority (lower value means earlier handling) and time of '\n            'arrival deciding any ties).')\n\n        parser.add_argument(\n            '--override-neuron-config',\n            type=json.loads,\n            default=None,\n            help=\"Override or set neuron device configuration. \"\n            \"e.g. {\\\"cast_logits_dtype\\\": \\\"bloat16\\\"}.'\")\n        parser.add_argument(\n            '--override-pooler-config',\n            type=PoolerConfig.from_json,\n            default=None,\n            help=\"Override or set the pooling method in the embedding model. \"\n            \"e.g. {\\\"pooling_type\\\": \\\"mean\\\", \\\"normalize\\\": false}.'\")\n\n        parser.add_argument('--compilation-config',\n                            '-O',\n                            type=CompilationConfig.from_cli,\n                            default=None,\n                            help='torch.compile configuration for the model.'\n                            'When it is a number (0, 1, 2, 3), it will be '\n                            'interpreted as the optimization level.\\n'\n                            'NOTE: level 0 is the default level without '\n                            'any optimization. level 1 and 2 are for internal '\n                            'testing only. level 3 is the recommended level '\n                            'for production.\\n'\n                            'To specify the full compilation config, '\n                            'use a JSON string.\\n'\n                            'Following the convention of traditional '\n                            'compilers, using -O without space is also '\n                            'supported. -O3 is equivalent to -O 3.')\n\n        parser.add_argument('--kv-transfer-config',\n                            type=KVTransferConfig.from_cli,\n                            default=None,\n                            help='The configurations for distributed KV cache '\n                            'transfer. Should be a JSON string.')\n\n        parser.add_argument(\n            '--worker-cls',\n            type=str,\n            default=\"auto\",\n            help='The worker class to use for distributed execution.')\n\n        return parser\n\n    @classmethod\n    def from_cli_args(cls, args: argparse.Namespace):\n        # Get the list of attributes of this dataclass.\n        attrs = [attr.name for attr in dataclasses.fields(cls)]\n        # Set the attributes from the parsed arguments.\n        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})\n        return engine_args\n\n    def create_model_config(self) -> ModelConfig:\n        return ModelConfig(\n            model=self.model,\n            task=self.task,\n            # We know this is not None because we set it in __post_init__\n            tokenizer=cast(str, self.tokenizer),\n            tokenizer_mode=self.tokenizer_mode,\n            trust_remote_code=self.trust_remote_code,\n            allowed_local_media_path=self.allowed_local_media_path,\n            dtype=self.dtype,\n            seed=self.seed,\n            revision=self.revision,\n            code_revision=self.code_revision,\n            rope_scaling=self.rope_scaling,\n            rope_theta=self.rope_theta,\n            hf_overrides=self.hf_overrides,\n            tokenizer_revision=self.tokenizer_revision,\n            max_model_len=self.max_model_len,\n            quantization=self.quantization,\n            quantization_param_path=self.quantization_param_path,\n            enforce_eager=self.enforce_eager,\n            max_seq_len_to_capture=self.max_seq_len_to_capture,\n            max_logprobs=self.max_logprobs,\n            disable_sliding_window=self.disable_sliding_window,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            served_model_name=self.served_model_name,\n            limit_mm_per_prompt=self.limit_mm_per_prompt,\n            use_async_output_proc=not self.disable_async_output_proc,\n            config_format=self.config_format,\n            mm_processor_kwargs=self.mm_processor_kwargs,\n            override_neuron_config=self.override_neuron_config,\n            override_pooler_config=self.override_pooler_config,\n        )\n\n    def create_load_config(self) -> LoadConfig:\n        return LoadConfig(\n            load_format=self.load_format,\n            download_dir=self.download_dir,\n            model_loader_extra_config=self.model_loader_extra_config,\n            ignore_patterns=self.ignore_patterns,\n        )\n\n    def create_engine_config(self,\n                             usage_context: Optional[UsageContext] = None\n                             ) -> VllmConfig:\n        if envs.VLLM_USE_V1:\n            self._override_v1_engine_args(usage_context)\n\n        # gguf file needs a specific model loader and doesn't use hf_repo\n        if check_gguf_file(self.model):\n            self.quantization = self.load_format = \"gguf\"\n\n        # bitsandbytes quantization needs a specific model loader\n        # so we make sure the quant method and the load format are consistent\n        if (self.quantization == \"bitsandbytes\" or\n           self.qlora_adapter_name_or_path is not None) and \\\n           self.load_format != \"bitsandbytes\":\n            raise ValueError(\n                \"BitsAndBytes quantization and QLoRA adapter only support \"\n                f\"'bitsandbytes' load format, but got {self.load_format}\")\n\n        if (self.load_format == \"bitsandbytes\" or\n            self.qlora_adapter_name_or_path is not None) and \\\n            self.quantization != \"bitsandbytes\":\n            raise ValueError(\n                \"BitsAndBytes load format and QLoRA adapter only support \"\n                f\"'bitsandbytes' quantization, but got {self.quantization}\")\n\n        assert self.cpu_offload_gb >= 0, (\n            \"CPU offload space must be non-negative\"\n            f\", but got {self.cpu_offload_gb}\")\n\n        device_config = DeviceConfig(device=self.device)\n        model_config = self.create_model_config()\n\n        if model_config.is_multimodal_model:\n            if self.enable_prefix_caching:\n                logger.warning(\n                    \"--enable-prefix-caching is currently not \"\n                    \"supported for multimodal models and has been disabled.\")\n            self.enable_prefix_caching = False\n\n        cache_config = CacheConfig(\n            # neuron needs block_size = max_model_len\n            block_size=self.block_size if self.device != \"neuron\" else\n            (self.max_model_len if self.max_model_len is not None else 0),\n            gpu_memory_utilization=self.gpu_memory_utilization,\n            swap_space=self.swap_space,\n            cache_dtype=self.kv_cache_dtype,\n            is_attention_free=model_config.is_attention_free,\n            num_gpu_blocks_override=self.num_gpu_blocks_override,\n            sliding_window=model_config.get_sliding_window(),\n            enable_prefix_caching=self.enable_prefix_caching,\n            cpu_offload_gb=self.cpu_offload_gb,\n        )\n        parallel_config = ParallelConfig(\n            pipeline_parallel_size=self.pipeline_parallel_size,\n            tensor_parallel_size=self.tensor_parallel_size,\n            worker_use_ray=self.worker_use_ray,\n            max_parallel_loading_workers=self.max_parallel_loading_workers,\n            disable_custom_all_reduce=self.disable_custom_all_reduce,\n            tokenizer_pool_config=TokenizerPoolConfig.create_config(\n                self.tokenizer_pool_size,\n                self.tokenizer_pool_type,\n                self.tokenizer_pool_extra_config,\n            ),\n            ray_workers_use_nsight=self.ray_workers_use_nsight,\n            distributed_executor_backend=self.distributed_executor_backend,\n            worker_cls=self.worker_cls,\n        )\n\n        max_model_len = model_config.max_model_len\n        use_long_context = max_model_len > 32768\n        if self.enable_chunked_prefill is None:\n            # If not explicitly set, enable chunked prefill by default for\n            # long context (> 32K) models. This is to avoid OOM errors in the\n            # initial memory profiling phase.\n\n            # Chunked prefill is currently disabled for multimodal models by\n            # default.\n            if use_long_context and not model_config.is_multimodal_model:\n                is_gpu = device_config.device_type == \"cuda\"\n                use_sliding_window = (model_config.get_sliding_window()\n                                      is not None)\n                use_spec_decode = self.speculative_model is not None\n                if (is_gpu and not use_sliding_window and not use_spec_decode\n                        and not self.enable_lora\n                        and not self.enable_prompt_adapter\n                        and model_config.task != \"embedding\"):\n                    self.enable_chunked_prefill = True\n                    logger.warning(\n                        \"Chunked prefill is enabled by default for models with \"\n                        \"max_model_len > 32K. Currently, chunked prefill might \"\n                        \"not work with some features or models. If you \"\n                        \"encounter any issues, please disable chunked prefill \"\n                        \"by setting --enable-chunked-prefill=False.\")\n            if self.enable_chunked_prefill is None:\n                self.enable_chunked_prefill = False\n\n        if not self.enable_chunked_prefill and use_long_context:\n            logger.warning(\n                \"The model has a long context length (%s). This may cause OOM \"\n                \"errors during the initial memory profiling phase, or result \"\n                \"in low performance due to small KV cache space. Consider \"\n                \"setting --max-model-len to a smaller value.\", max_model_len)\n        elif self.enable_chunked_prefill and model_config.task == \"embedding\":\n            msg = \"Chunked prefill is not supported for embedding models\"\n            raise ValueError(msg)\n\n\n        speculative_config = SpeculativeConfig.maybe_create_spec_config(\n            target_model_config=model_config,\n            target_parallel_config=parallel_config,\n            target_dtype=self.dtype,\n            speculative_model=self.speculative_model,\n            speculative_model_quantization = \\\n                self.speculative_model_quantization,\n            speculative_draft_tensor_parallel_size = \\\n                self.speculative_draft_tensor_parallel_size,\n            num_speculative_tokens=self.num_speculative_tokens,\n            speculative_disable_mqa_scorer=self.speculative_disable_mqa_scorer,\n            speculative_disable_by_batch_size=self.\n            speculative_disable_by_batch_size,\n            speculative_max_model_len=self.speculative_max_model_len,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            disable_log_stats=self.disable_log_stats,\n            ngram_prompt_lookup_max=self.ngram_prompt_lookup_max,\n            ngram_prompt_lookup_min=self.ngram_prompt_lookup_min,\n            draft_token_acceptance_method=\\\n                self.spec_decoding_acceptance_method,\n            typical_acceptance_sampler_posterior_threshold=self.\n            typical_acceptance_sampler_posterior_threshold,\n            typical_acceptance_sampler_posterior_alpha=self.\n            typical_acceptance_sampler_posterior_alpha,\n            disable_logprobs=self.disable_logprobs_during_spec_decoding,\n        )\n\n        # Reminder: Please update docs/source/serving/compatibility_matrix.rst\n        # If the feature combo become valid\n        if self.num_scheduler_steps > 1:\n            if speculative_config is not None:\n                raise ValueError(\"Speculative decoding is not supported with \"\n                                 \"multi-step (--num-scheduler-steps > 1)\")\n            if self.enable_chunked_prefill and self.pipeline_parallel_size > 1:\n                raise ValueError(\"Multi-Step Chunked-Prefill is not supported \"\n                                 \"for pipeline-parallel-size > 1\")\n\n        # make sure num_lookahead_slots is set the higher value depending on\n        # if we are using speculative decoding or multi-step\n        num_lookahead_slots = max(self.num_lookahead_slots,\n                                  self.num_scheduler_steps - 1)\n        num_lookahead_slots = num_lookahead_slots \\\n            if speculative_config is None \\\n            else speculative_config.num_lookahead_slots\n\n        if not self.use_v2_block_manager:\n            logger.warning(\n                \"[DEPRECATED] Block manager v1 has been removed, \"\n                \"and setting --use-v2-block-manager to True or False has \"\n                \"no effect on vLLM behavior. Please remove \"\n                \"--use-v2-block-manager in your engine argument. \"\n                \"If your use case is not supported by \"\n                \"SelfAttnBlockSpaceManager (i.e. block manager v2),\"\n                \" please file an issue with detailed information.\")\n\n        scheduler_config = SchedulerConfig(\n            task=model_config.task,\n            max_num_batched_tokens=self.max_num_batched_tokens,\n            max_num_seqs=self.max_num_seqs,\n            max_model_len=model_config.max_model_len,\n            num_lookahead_slots=num_lookahead_slots,\n            delay_factor=self.scheduler_delay_factor,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            is_multimodal_model=model_config.is_multimodal_model,\n            preemption_mode=self.preemption_mode,\n            num_scheduler_steps=self.num_scheduler_steps,\n            multi_step_stream_outputs=self.multi_step_stream_outputs,\n            send_delta_data=(envs.VLLM_USE_RAY_SPMD_WORKER\n                             and parallel_config.use_ray),\n            policy=self.scheduling_policy)\n        lora_config = LoRAConfig(\n            bias_enabled=self.enable_lora_bias,\n            max_lora_rank=self.max_lora_rank,\n            max_loras=self.max_loras,\n            fully_sharded_loras=self.fully_sharded_loras,\n            lora_extra_vocab_size=self.lora_extra_vocab_size,\n            long_lora_scaling_factors=self.long_lora_scaling_factors,\n            lora_dtype=self.lora_dtype,\n            max_cpu_loras=self.max_cpu_loras if self.max_cpu_loras\n            and self.max_cpu_loras > 0 else None) if self.enable_lora else None\n\n        if self.qlora_adapter_name_or_path is not None and \\\n            self.qlora_adapter_name_or_path != \"\":\n            if self.model_loader_extra_config is None:\n                self.model_loader_extra_config = {}\n            self.model_loader_extra_config[\n                \"qlora_adapter_name_or_path\"] = self.qlora_adapter_name_or_path\n\n        load_config = self.create_load_config()\n\n        prompt_adapter_config = PromptAdapterConfig(\n            max_prompt_adapters=self.max_prompt_adapters,\n            max_prompt_adapter_token=self.max_prompt_adapter_token) \\\n                                        if self.enable_prompt_adapter else None\n\n        decoding_config = DecodingConfig(\n            guided_decoding_backend=self.guided_decoding_backend)\n\n        detailed_trace_modules = []\n        if self.collect_detailed_traces is not None:\n            detailed_trace_modules = self.collect_detailed_traces.split(\",\")\n        for m in detailed_trace_modules:\n            if m not in ALLOWED_DETAILED_TRACE_MODULES:\n                raise ValueError(\n                    f\"Invalid module {m} in collect_detailed_traces. \"\n                    f\"Valid modules are {ALLOWED_DETAILED_TRACE_MODULES}\")\n        observability_config = ObservabilityConfig(\n            otlp_traces_endpoint=self.otlp_traces_endpoint,\n            collect_model_forward_time=\"model\" in detailed_trace_modules\n            or \"all\" in detailed_trace_modules,\n            collect_model_execute_time=\"worker\" in detailed_trace_modules\n            or \"all\" in detailed_trace_modules,\n        )\n\n        config = VllmConfig(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            decoding_config=decoding_config,\n            observability_config=observability_config,\n            prompt_adapter_config=prompt_adapter_config,\n            compilation_config=self.compilation_config,\n            kv_transfer_config=self.kv_transfer_config,\n        )\n\n        if envs.VLLM_USE_V1:\n            self._override_v1_engine_config(config)\n        return config\n\n    def _override_v1_engine_args(self, usage_context: UsageContext) -> None:\n        \"\"\"\n        Override the EngineArgs's args based on the usage context for V1.\n        \"\"\"\n        assert envs.VLLM_USE_V1, \"V1 is not enabled\"\n\n        if self.max_num_batched_tokens is None:\n            # When no user override, set the default values based on the\n            # usage context.\n            if usage_context == UsageContext.LLM_CLASS:\n                logger.warning(\"Setting max_num_batched_tokens to 8192 \"\n                               \"for LLM_CLASS usage context.\")\n                self.max_num_seqs = 1024\n                self.max_num_batched_tokens = 8192\n            elif usage_context == UsageContext.OPENAI_API_SERVER:\n                logger.warning(\"Setting max_num_batched_tokens to 2048 \"\n                               \"for OPENAI_API_SERVER usage context.\")\n                self.max_num_seqs = 1024\n                self.max_num_batched_tokens = 2048\n\n    def _override_v1_engine_config(self, engine_config: VllmConfig) -> None:\n        \"\"\"\n        Override the EngineConfig's configs based on the usage context for V1.\n        \"\"\"\n        assert envs.VLLM_USE_V1, \"V1 is not enabled\"\n        # TODO (ywang96): Enable APC by default when VLM supports it.\n        if engine_config.model_config.is_multimodal_model:\n            logger.warning(\n                \"Prefix caching is currently not supported for multimodal \"\n                \"models and has been disabled.\")\n            engine_config.cache_config.enable_prefix_caching = False\n\n\n@dataclass\nclass AsyncEngineArgs(EngineArgs):\n    \"\"\"Arguments for asynchronous vLLM engine.\"\"\"\n    disable_log_requests: bool = False\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser,\n                     async_args_only: bool = False) -> FlexibleArgumentParser:\n        if not async_args_only:\n            parser = EngineArgs.add_cli_args(parser)\n        parser.add_argument('--disable-log-requests',\n                            action='store_true',\n                            help='Disable logging requests.')\n        return parser\n\n\n# These functions are used by sphinx to build the documentation\ndef _engine_args_parser():\n    return EngineArgs.add_cli_args(FlexibleArgumentParser())\n\n\ndef _async_engine_args_parser():\n    return AsyncEngineArgs.add_cli_args(FlexibleArgumentParser(),\n                                        async_args_only=True)\n",
      "diff": "diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py\nindex 4aa0eebd9..3b776c1d9 100644\n--- a/vllm/engine/arg_utils.py\n+++ b/vllm/engine/arg_utils.py\n@@ -168,7 +168,7 @@ class EngineArgs:\n     scheduler_delay_factor: float = 0.0\n     enable_chunked_prefill: Optional[bool] = None\n \n-    guided_decoding_backend: str = 'outlines'\n+    guided_decoding_backend: str = 'xgrammar'\n     # Speculative decoding configuration.\n     speculative_model: Optional[str] = None\n     speculative_model_quantization: Optional[str] = None\n@@ -364,11 +364,12 @@ class EngineArgs:\n         parser.add_argument(\n             '--guided-decoding-backend',\n             type=str,\n-            default='outlines',\n-            choices=['outlines', 'lm-format-enforcer'],\n+            default='xgrammar',\n+            choices=['outlines', 'lm-format-enforcer', 'xgrammar'],\n             help='Which engine will be used for guided decoding'\n             ' (JSON schema / regex etc) by default. Currently support '\n-            'https://github.com/outlines-dev/outlines and '\n+            'https://github.com/outlines-dev/outlines,'\n+            'https://github.com/mlc-ai/xgrammar, and '\n             'https://github.com/noamgat/lm-format-enforcer.'\n             ' Can be overridden per request via guided_decoding_backend'\n             ' parameter.')",
      "change_type": "modified",
      "lines_added": 6,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/engine/async_llm_engine.py",
      "old_content": "import asyncio\nimport time\nimport weakref\nfrom functools import partial\nfrom typing import (Any, AsyncGenerator, Callable, Coroutine, Dict, Iterable,\n                    List, Mapping, Optional, Set, Tuple, Type, Union, overload)\nfrom weakref import ReferenceType\n\nfrom typing_extensions import deprecated\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, LoRAConfig, ModelConfig,\n                         ParallelConfig, SchedulerConfig, VllmConfig)\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.async_timeout import asyncio_timeout\nfrom vllm.engine.llm_engine import LLMEngine, SchedulerOutputState\nfrom vllm.engine.metrics_types import StatLoggerBase\nfrom vllm.engine.protocol import EngineClient\nfrom vllm.executor.executor_base import ExecutorAsyncBase\nfrom vllm.executor.gpu_executor import GPUExecutorAsync\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import PromptType\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding import (\n    get_guided_decoding_logits_processor)\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.outputs import PoolingRequestOutput, RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import ExecuteModelRequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import deprecate_kwargs, weak_bind\n\nlogger = init_logger(__name__)\nENGINE_ITERATION_TIMEOUT_S = envs.VLLM_ENGINE_ITERATION_TIMEOUT_S\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _log_task_completion(task: asyncio.Task,\n                         error_callback: Callable[[Exception], None]) -> None:\n    \"\"\"This function is only intended for the `engine.run_engine_loop()` task.\n\n    In particular, that task runs a `while True` loop that can only exit if\n    there is an exception.\n    \"\"\"\n\n    exception = None\n    try:\n        return_value = task.result()\n        raise AssertionError(\n            f\"The engine background task should never finish without an \"\n            f\"exception. {return_value}\")\n    except asyncio.exceptions.CancelledError:\n        # We assume that if the task is cancelled, we are gracefully shutting\n        # down. This should only happen on program exit.\n        logger.info(\"Engine is gracefully shutting down.\")\n    except Exception as e:\n        exception = e\n        logger.error(\"Engine background task failed\", exc_info=e)\n        error_callback(exception)\n        raise AsyncEngineDeadError(\n            \"Task finished unexpectedly. This should never happen! \"\n            \"Please open an issue on Github. See stack trace above for the \"\n            \"actual cause.\") from e\n\n\nSTOP_ITERATION = Exception()  # Sentinel\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs or PoolingRequestOutputs for a request\n    that can be iterated over asynchronously via an async generator.\"\"\"\n\n    def __init__(self, request_id: str, cancel: Callable[[str], None]) -> None:\n        self.request_id = request_id\n        self._cancel = cancel\n        self._queue: asyncio.Queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: Union[RequestOutput, PoolingRequestOutput,\n                              Exception]) -> None:\n        if not self._finished:\n            self._queue.put_nowait(item)\n\n    def finish(\n        self,\n        exception: Optional[Union[BaseException, Type[BaseException]]] = None,\n    ) -> None:\n        if not self._finished:\n            self._finished = True\n            self._queue.put_nowait(\n                exception if self._is_raisable(exception) else STOP_ITERATION)\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    async def generator(\n        self\n    ) -> AsyncGenerator[Union[RequestOutput, PoolingRequestOutput], None]:\n        try:\n            while True:\n                result = await self._queue.get()\n                if self._is_raisable(result):\n                    if result == STOP_ITERATION:\n                        return\n                    raise result\n                yield result\n        except GeneratorExit:\n            self._cancel(self.request_id)\n            raise asyncio.CancelledError from None\n\n    @staticmethod\n    def _is_raisable(value: Any):\n        return isinstance(value, BaseException) or \\\n                (isinstance(value, type) and \\\n                 issubclass(value, BaseException))\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._aborted_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = asyncio.Event()\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def __len__(self) -> int:\n        return len(self._request_streams)\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self.abort_request(request_id, exception=exc)\n        else:\n            # NB: tuple() used here because self.abort_request pops the stream\n            # out of self._request_streams, so we can't iterate on it directly\n            for rid in tuple(self._request_streams.keys()):\n                self.abort_request(rid, exception=exc)\n\n    def process_request_output(self,\n                               request_output: Union[RequestOutput,\n                                                     PoolingRequestOutput],\n                               *,\n                               verbose: bool = False) -> None:\n        \"\"\"Process a request output from the engine.\"\"\"\n        request_id = request_output.request_id\n        finished = request_output.finished\n\n        if finished:\n            stream = self._request_streams.pop(request_id, None)\n        else:\n            stream = self._request_streams.get(request_id)\n        # Guard against a KeyError which can occur if the request was aborted\n        # while the output was generated\n        if stream is not None:\n            stream.put(request_output)\n            if finished:\n                stream.finish()\n\n        if verbose and finished:\n            logger.info(\"Finished request %s.\", request_id)\n\n    def process_exception(self,\n                          request_id: str,\n                          exception: BaseException,\n                          *,\n                          verbose: bool = False) -> None:\n        \"\"\"Propagate an exception from the engine.\"\"\"\n        if verbose:\n            logger.info(\"Finished request %s.\", request_id)\n        self.abort_request(request_id, exception=exception)\n\n    def add_request(self,\n                    request_id: str,\n                    *,\n                    verbose: bool = False,\n                    **engine_add_request_kwargs) -> AsyncStream:\n        \"\"\"Add a request to be sent to the engine on the next background\n        loop iteration.\"\"\"\n        if request_id in self._request_streams:\n            raise KeyError(f\"Request {request_id} already exists.\")\n\n        abort_request = partial(self.abort_request, verbose=verbose)\n        stream = AsyncStream(request_id, abort_request)\n        self._new_requests.put_nowait((stream, {\n            \"request_id\": request_id,\n            **engine_add_request_kwargs\n        }))\n\n        self.new_requests_event.set()\n\n        if verbose:\n            logger.info(\"Added request %s.\", request_id)\n\n        return stream\n\n    def abort_request(self,\n                      request_id: str,\n                      *,\n                      exception: Optional[Union[BaseException,\n                                                Type[BaseException]]] = None,\n                      verbose: bool = False) -> None:\n        \"\"\"Abort a request during next background loop iteration.\"\"\"\n        if verbose:\n            logger.info(\"Aborted request %s.\", request_id)\n\n        self._aborted_requests.put_nowait(request_id)\n\n        stream = self._request_streams.pop(request_id, None)\n        if stream is not None:\n            stream.finish(exception=exception)\n\n    def get_new_and_aborted_requests(self) -> Tuple[List[Dict], Set[str]]:\n        \"\"\"Get the new requests and finished requests to be\n        sent to the engine.\"\"\"\n        new_requests: List[Dict] = []\n        finished_requests: Set[str] = set()\n\n        while not self._aborted_requests.empty():\n            request_id = self._aborted_requests.get_nowait()\n            finished_requests.add(request_id)\n\n        while not self._new_requests.empty():\n            stream, new_request = self._new_requests.get_nowait()\n            request_id = stream.request_id\n            if request_id in finished_requests:\n                # The request has already been aborted.\n                stream.finish(asyncio.CancelledError)\n                finished_requests.discard(request_id)\n            else:\n                self._request_streams[request_id] = stream\n                new_requests.append(new_request)\n\n        return new_requests, finished_requests\n\n    async def wait_for_new_requests(self):\n        if not self.has_new_requests():\n            await self.new_requests_event.wait()\n        self.new_requests_event.clear()\n\n    def has_new_requests(self):\n        return not self._new_requests.empty()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    async def step_async(\n        self, virtual_engine: int\n    ) -> List[Union[RequestOutput, PoolingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        # these are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # Clear outputs for each new scheduler iteration\n        ctx.request_outputs.clear()\n\n        # skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        if not self._has_remaining_steps(seq_group_metadata_list):\n\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            ctx.seq_group_metadata_list = seq_group_metadata_list\n            ctx.scheduler_outputs = scheduler_outputs\n\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n        else:\n            finished_requests_ids = list()\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                virtual_engine=virtual_engine,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                execute_model_req.async_callback = self.async_callbacks[\n                    virtual_engine]\n\n            # Execute the model.\n            outputs = await self.model_executor.execute_model_async(\n                execute_model_req)\n\n            # we need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, outputs)\n        else:\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            outputs = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # Clear the cache if we have finished all the steps\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[\n                    virtual_engine] = SchedulerOutputState()\n\n            # is_first_step_output is True only when the num_steps of all\n            # the sequences are 1. When the num_steps > 1,\n            # multi_step_model_runner does the first-step output append.\n            is_first_step_output: bool = False if not seq_group_metadata_list \\\n                else seq_group_metadata_list[0].state.num_steps == 1\n\n            ctx.append_output(outputs=outputs,\n                              seq_group_metadata_list=seq_group_metadata_list,\n                              scheduler_outputs=scheduler_outputs,\n                              is_async=allow_async_output_proc,\n                              is_last_step=True,\n                              is_first_step_output=is_first_step_output)\n\n            if outputs and allow_async_output_proc:\n                assert len(\n                    outputs\n                ) == 1, \"Async postprocessor expects only a single output set\"\n                self._advance_to_next_step(\n                    outputs[0], seq_group_metadata_list,\n                    scheduler_outputs.scheduled_seq_groups)\n\n            if not allow_async_output_proc:\n                self._process_model_outputs(ctx=ctx)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, outputs)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n\n        else:\n            # Multi-step case\n            return ctx.request_outputs\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            assert len(ctx.output_queue) == 0\n\n        return ctx.request_outputs\n\n    async def stop_remote_worker_execution_loop_async(self) -> None:\n        \"\"\"Stop the remote worker execution loop.\"\"\"\n        await self.model_executor.stop_remote_worker_execution_loop_async()\n\n    async def get_tokenizer_async(self,\n                                  lora_request: Optional[LoRARequest] = None\n                                  ) -> AnyTokenizer:\n        return await (\n            self.get_tokenizer_group().get_lora_tokenizer_async(lora_request))\n\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    async def add_request_async(\n        self,\n        request_id: str,\n        *,\n        inputs: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @overload\n    async def add_request_async(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    async def add_request_async(\n            self,\n            request_id: str,\n            prompt: Optional[PromptType] = None,\n            params: Optional[Union[SamplingParams, PoolingParams]] = None,\n            arrival_time: Optional[float] = None,\n            lora_request: Optional[LoRARequest] = None,\n            trace_headers: Optional[Mapping[str, str]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n            priority: int = 0,\n            *,\n            inputs: Optional[PromptType] = None,  # DEPRECATED\n    ) -> None:\n        \"\"\"Async version of :meth:`add_request`.\"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert prompt is not None and params is not None\n\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n        if priority != 0 and not self.scheduler_config.policy == \"priority\":\n            raise ValueError(f\"Got priority {priority} but \"\n                             \"Priority scheduling is not enabled.\")\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        if self.tokenizer is not None:\n            tokenizer = await self.get_tokenizer_async(lora_request)\n            self._validate_token_prompt(prompt, tokenizer=tokenizer)\n\n        preprocessed_inputs = await self.input_preprocessor.preprocess_async(\n            prompt,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n        processed_inputs = self.input_processor(preprocessed_inputs)\n\n        if isinstance(params, SamplingParams) and \\\n            params.guided_decoding is not None:\n            # Guided decoding has an async implementation for building logits\n            # processors in a separate threadpool.\n            # We want to invoke that here instead of using the blocking\n            # implementation in the LLMEngine\n            params = await build_guided_decoding_logits_processor_async(\n                sampling_params=params,\n                tokenizer=await self.get_tokenizer_async(lora_request),\n                default_guided_backend=self.decoding_config.\n                guided_decoding_backend)\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n    async def check_health_async(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n\nasync def build_guided_decoding_logits_processor_async(\n        sampling_params: SamplingParams, tokenizer: AnyTokenizer,\n        default_guided_backend: str) -> SamplingParams:\n    \"\"\"Constructs logits processors based on the guided_decoding,\n    logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n    those fields and adds the constructed logits processors to the\n    logits_processors field. Modifies sampling params in-place and returns\n    the modified sampling params.\"\"\"\n    if (guided_decoding := sampling_params.guided_decoding) is None:\n        return sampling_params\n\n    logger.debug(\"Building guided decoding logits processor. \"\n                 \"Params: %s\", guided_decoding)\n\n    guided_decoding.backend = guided_decoding.backend or default_guided_backend\n\n    processor = await get_guided_decoding_logits_processor(\n        guided_params=guided_decoding, tokenizer=tokenizer)\n\n    if processor:\n        if sampling_params.logits_processors is None:\n            sampling_params.logits_processors = []\n        sampling_params.logits_processors.append(processor)\n\n    # Unset guided decoding params after constructing the lp from them\n    sampling_params.guided_decoding = None\n\n    return sampling_params\n\n\nclass AsyncLLMEngine(EngineClient):\n    \"\"\"An asynchronous wrapper for :class:`LLMEngine`.\n\n    This class is used to wrap the :class:`LLMEngine` class to make it\n    asynchronous. It uses asyncio to create a background loop that keeps\n    processing incoming requests. The :class:`LLMEngine` is kicked by the\n    generate method when there are requests in the waiting queue. The generate\n    method yields the outputs from the :class:`LLMEngine` to the caller.\n\n    Args:\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args: Arguments for :class:`LLMEngine`.\n        **kwargs: Arguments for :class:`LLMEngine`.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 *args,\n                 log_requests: bool = True,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.log_requests = log_requests\n        self.engine = self._engine_class(*args, **kwargs)\n\n        # This ensures quick processing of request outputs\n        # so the append to asyncio queues is not delayed,\n        # especially for multi-step.\n        self.use_process_request_outputs_callback = (\n            self.engine.model_config.use_async_output_proc)\n\n        if self.use_process_request_outputs_callback:\n            self.engine.process_request_outputs_callback = \\\n                weak_bind(self.process_request_outputs)\n\n        self.background_loop: Optional[asyncio.Future] = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded: Optional[asyncio.Task] = None\n        self.start_engine_loop = start_engine_loop\n        self._errored_with: Optional[BaseException] = None\n\n        # Lazy initialized fields\n        self._request_tracker: RequestTracker\n\n    def __del__(self):\n        if rt := getattr(self, \"request_tracker\", None):\n            # Wake up engine loop so that it will exit cleanly\n            rt.new_requests_event.set()\n\n    @classmethod\n    def _get_executor_cls(\n            cls, engine_config: VllmConfig) -> Type[ExecutorAsyncBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorAsyncBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorAsyncBase. Got {distributed_executor_backend}.\")\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutorAsync\n            executor_class = NeuronExecutorAsync\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                from vllm.executor.ray_tpu_executor import RayTPUExecutorAsync\n                executor_class = RayTPUExecutorAsync\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutorAsync\n                executor_class = TPUExecutorAsync\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutorAsync\n            executor_class = CPUExecutorAsync\n        elif engine_config.device_config.device_type == \"hpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_hpu_executor import RayHPUExecutorAsync\n                executor_class = RayHPUExecutorAsync\n            else:\n                from vllm.executor.hpu_executor import HPUExecutorAsync\n                executor_class = HPUExecutorAsync\n        elif engine_config.device_config.device_type == \"openvino\":\n            assert distributed_executor_backend is None, (\n                \"Distributed execution is not supported with \"\n                \"the OpenVINO backend.\")\n            from vllm.executor.openvino_executor import OpenVINOExecutorAsync\n            executor_class = OpenVINOExecutorAsync\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend is None:\n                from vllm.executor.xpu_executor import XPUExecutorAsync\n                executor_class = XPUExecutorAsync\n            elif distributed_executor_backend == \"ray\":\n                from vllm.executor.ray_xpu_executor import RayXPUExecutorAsync\n                executor_class = RayXPUExecutorAsync\n            elif distributed_executor_backend == \"mp\":\n                from vllm.executor.multiproc_xpu_executor import (\n                    MultiprocessingXPUExecutorAsync)\n                executor_class = MultiprocessingXPUExecutorAsync\n            else:\n                raise RuntimeError(\n                    \"Not supported distributed execution model on XPU device.\")\n        elif distributed_executor_backend == \"ray\":\n            from vllm.executor.ray_gpu_executor import RayGPUExecutorAsync\n            executor_class = RayGPUExecutorAsync\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutorAsync)\n            executor_class = MultiprocessingGPUExecutorAsync\n        else:\n            from vllm.executor.gpu_executor import GPUExecutorAsync\n            executor_class = GPUExecutorAsync\n        return executor_class\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: AsyncEngineArgs,\n        engine_config: Optional[VllmConfig] = None,\n        start_engine_loop: bool = True,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"AsyncLLMEngine\":\n        \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        if engine_config is None:\n            engine_config = engine_args.create_engine_config(usage_context)\n\n        executor_class = cls._get_executor_cls(engine_config)\n\n        if executor_class.uses_ray:\n            initialize_ray_cluster(engine_config.parallel_config)\n\n        # Create the async LLM engine.\n        engine = cls(\n            vllm_config=engine_config,\n            executor_class=executor_class,\n            log_requests=not engine_args.disable_log_requests,\n            log_stats=not engine_args.disable_log_stats,\n            start_engine_loop=start_engine_loop,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n        return engine\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and self._background_loop_unshielded is not None\n                and not self._background_loop_unshielded.done())\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored or (self.background_loop is not None and\n                                self._background_loop_unshielded is not None\n                                and self._background_loop_unshielded.done())\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    @property\n    def dead_error(self) -> BaseException:\n        return AsyncEngineDeadError(\n            \"Background loop is not running. If it was running, \"\n            \"inspect the output to find the stacktrace of the \"\n            \"error that caused the background loop to stop \"\n            \"(AsyncEngineDeadError).\")\n\n    def set_errored(self, exc: Exception) -> None:\n        self._errored_with = exc\n\n    def _error_callback(self, exc: Exception) -> None:\n        self.set_errored(exc)\n        self._request_tracker.propagate_exception(exc)\n\n    async def get_input_preprocessor(self) -> InputPreprocessor:\n        return self.engine.input_preprocessor\n\n    async def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return await self.engine.get_tokenizer_async(lora_request)\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.errored:\n            raise AsyncEngineDeadError(\n                \"Background loop has errored already.\") from self._errored_with\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        # Initialize the RequestTracker here so it uses the right event loop.\n        self._request_tracker = RequestTracker()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop(weakref.ref(self)))\n        self._background_loop_unshielded.add_done_callback(\n            partial(_log_task_completion, error_callback=self._error_callback))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def shutdown_background_loop(self) -> None:\n        \"\"\"\n        Shut down the background loop.\n\n        This method needs to be called during cleanup to remove\n        references to `self` and properly GC the resources held\n        by the async LLM engine (e.g., the executors as well as\n        their resources).\n        \"\"\"\n        if self._background_loop_unshielded is not None:\n            self._background_loop_unshielded.cancel()\n            self._background_loop_unshielded = None\n        self.background_loop = None\n\n    async def engine_step(self, virtual_engine: int) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, aborted_requests = (\n            self._request_tracker.get_new_and_aborted_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            try:\n                await self.engine.add_request_async(**new_request)\n            except ValueError as e:\n                # TODO: use a vLLM specific error for failed validation\n                self._request_tracker.process_exception(\n                    new_request[\"request_id\"],\n                    e,\n                    verbose=self.log_requests,\n                )\n\n        if aborted_requests:\n            await self._engine_abort(aborted_requests)\n\n        request_outputs = await self.engine.step_async(virtual_engine)\n\n        # Put the outputs into the corresponding streams.\n        # If used as a callback, then already invoked inside\n        # LLMEngine's _process_model_outputs\n        if not self.use_process_request_outputs_callback:\n            all_finished = self.process_request_outputs(request_outputs)\n        else:\n            # For callback case, we only need to detect when all\n            # requests are finished\n            all_finished = all(request_output.finished\n                               for request_output in request_outputs)\n\n        return not all_finished\n\n    def process_request_outputs(self, request_outputs) -> bool:\n        # Put the outputs into the corresponding streams.\n        all_finished = True\n        for request_output in request_outputs:\n            self._request_tracker.process_request_output(\n                request_output, verbose=self.log_requests)\n            all_finished = all_finished and request_output.finished\n\n        return all_finished\n\n    async def _engine_abort(self, request_ids: Iterable[str]):\n        self.engine.abort_request(request_ids)\n\n    @staticmethod\n    async def run_engine_loop(engine_ref: ReferenceType):\n        \"\"\"We use a weakref to the engine so that the running loop\n        doesn't prevent the engine being garbage collected.\"\"\"\n        engine: Optional[AsyncLLMEngine] = engine_ref()\n        if not engine:\n            return\n\n        pipeline_parallel_size = \\\n                engine.engine.parallel_config.pipeline_parallel_size\n        has_requests_in_progress = [False] * pipeline_parallel_size\n        while True:\n            if not any(has_requests_in_progress):\n                logger.debug(\"Waiting for new requests...\")\n                # Stop the execute model loop in parallel workers until there\n                # are more requests to process. This avoids waiting\n                # indefinitely in torch.distributed ops which may otherwise\n                # timeout, and unblocks the RPC thread in the workers so that\n                # they can process any other queued control plane messages,\n                # such as add/remove lora adapters.\n                await engine.engine.stop_remote_worker_execution_loop_async()\n                request_tracker = engine._request_tracker\n                # Allow engine to be garbage collected while\n                # waiting for new requests\n                del engine\n                await asyncio.sleep(0)\n                if engine_ref() is None:\n                    return\n                await request_tracker.wait_for_new_requests()\n                engine = engine_ref()\n                if not engine:\n                    return\n                logger.debug(\"Got new requests!\")\n                requests_in_progress = [\n                    asyncio.create_task(engine.engine_step(ve))\n                    for ve in range(pipeline_parallel_size)\n                ]\n                has_requests_in_progress = [True] * pipeline_parallel_size\n\n            # Abort if iteration takes too long due to unrecoverable errors\n            # (eg. NCCL timeouts).\n            try:\n                async with asyncio_timeout(ENGINE_ITERATION_TIMEOUT_S):\n                    done, _ = await asyncio.wait(\n                        requests_in_progress,\n                        return_when=asyncio.FIRST_COMPLETED)\n                    for _ in range(pipeline_parallel_size):\n                        await asyncio.sleep(0)\n                for task in done:\n                    result = task.result()\n                    virtual_engine = requests_in_progress.index(task)\n                    has_unfinished_requests = (\n                        engine.engine.\n                        has_unfinished_requests_for_virtual_engine(\n                            virtual_engine))\n                    if result or has_unfinished_requests:\n                        requests_in_progress[virtual_engine] = (\n                            asyncio.create_task(\n                                engine.engine_step(virtual_engine)))\n                        has_requests_in_progress[virtual_engine] = True\n                    else:\n                        has_requests_in_progress[virtual_engine] = False\n            except asyncio.TimeoutError as exc:\n                logger.error(\n                    \"Engine iteration timed out. This should never happen!\")\n                engine.set_errored(exc)\n                raise\n            await asyncio.sleep(0)\n\n    # This method does not need to be async, but kept that way\n    # for backwards compatibility.\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    def add_request(\n        self,\n        request_id: str,\n        *,\n        inputs: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> Coroutine[None, None, AsyncGenerator[Union[\n            RequestOutput, PoolingRequestOutput], None]]:\n        ...\n\n    @overload\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> Coroutine[None, None, AsyncGenerator[Union[\n            RequestOutput, PoolingRequestOutput], None]]:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    async def add_request(\n        self,\n        request_id: str,\n        prompt: Optional[PromptType] = None,\n        params: Optional[Union[SamplingParams, PoolingParams]] = None,\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n        *,\n        inputs: Optional[PromptType] = None,  # DEPRECATED\n    ) -> AsyncGenerator[Union[RequestOutput, PoolingRequestOutput], None]:\n        if inputs is not None:\n            prompt = inputs\n        assert prompt is not None and params is not None\n\n        if not self.is_running:\n            if self.start_engine_loop:\n                self.start_background_loop()\n            else:\n                raise AsyncEngineDeadError(\n                    \"Background loop is not running. If it was running, \"\n                    \"inspect the output to find the stacktrace of the \"\n                    \"error that caused the background loop to stop \"\n                    \"(AsyncEngineDeadError).\")\n\n        if (priority != 0\n                and not self.engine.scheduler_config.policy == \"priority\"):\n            raise ValueError(f\"Got priority {priority} but \"\n                             \"Priority scheduling is not enabled.\")\n\n        stream = self._request_tracker.add_request(\n            request_id,\n            verbose=self.log_requests,\n            prompt=prompt,\n            params=params,\n            arrival_time=arrival_time or time.time(),\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request,\n            priority=priority,\n        )\n\n        return stream.generator()\n\n    async def generate(\n        self,\n        prompt: PromptType,\n        sampling_params: SamplingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n            prompt_adapter_request: Prompt Adapter request to use\n                                            for generation, if any.\n            priority: The priority of the request.\n                Only applicable with priority scheduling.\n\n        Yields:\n            The output `RequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> # note that engine_args here is AsyncEngineArgs instance\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"prompt\": \"What is LLM?\",\n            >>>     \"stream\": False, # assume the non-streaming case\n            >>>     \"temperature\": 0.0,\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.generate(\n            >>>    example_input[\"prompt\"],\n            >>>    SamplingParams(temperature=example_input[\"temperature\"]),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in await self.add_request(\n                request_id,\n                prompt,\n                sampling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority,\n        ):\n            yield LLMEngine.validate_output(output, RequestOutput)\n\n    async def encode(\n        self,\n        prompt: PromptType,\n        pooling_params: PoolingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[PoolingRequestOutput, None]:\n        \"\"\"Generate outputs for a request from an embedding model.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            pooling_params: The pooling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n            priority: The priority of the request.\n                Only applicable with priority scheduling.\n\n        Yields:\n            The output `PoolingRequestOutput` objects from the LLMEngine\n            for the request.\n\n        Details:\n            - If the engine is not running, start the background loop,\n              which iteratively invokes\n              :meth:`~vllm.engine.async_llm_engine.AsyncLLMEngine.engine_step`\n              to process the waiting requests.\n            - Add the request to the engine's `RequestTracker`.\n              On the next background loop, this request will be sent to\n              the underlying engine.\n              Also, a corresponding `AsyncStream` will be created.\n            - Wait for the request outputs from `AsyncStream` and yield them.\n\n        Example:\n            >>> # Please refer to entrypoints/api_server.py for\n            >>> # the complete example.\n            >>>\n            >>> # initialize the engine and the example input\n            >>> # note that engine_args here is AsyncEngineArgs instance\n            >>> engine = AsyncLLMEngine.from_engine_args(engine_args)\n            >>> example_input = {\n            >>>     \"input\": \"What is LLM?\",\n            >>>     \"request_id\": 0,\n            >>> }\n            >>>\n            >>> # start the generation\n            >>> results_generator = engine.encode(\n            >>>    example_input[\"input\"],\n            >>>    PoolingParams(),\n            >>>    example_input[\"request_id\"])\n            >>>\n            >>> # get the results\n            >>> final_output = None\n            >>> async for request_output in results_generator:\n            >>>     if await request.is_disconnected():\n            >>>         # Abort the request if the client disconnects.\n            >>>         await engine.abort(request_id)\n            >>>         # Return or raise an error\n            >>>         ...\n            >>>     final_output = request_output\n            >>>\n            >>> # Process and return the final output\n            >>> ...\n        \"\"\"\n        async for output in await self.add_request(\n                request_id,\n                prompt,\n                pooling_params,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                priority=priority,\n        ):\n            yield LLMEngine.validate_output(output, PoolingRequestOutput)\n\n    async def abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        if not self.is_running:\n            raise AsyncEngineDeadError(\n                \"Background loop is not running. If it was running, \"\n                \"inspect the output to find the stacktrace of the \"\n                \"error that caused the background loop to stop \"\n                \"(AsyncEngineDeadError).\")\n\n        return self._abort(request_id)\n\n    def _abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        self._request_tracker.abort_request(request_id,\n                                            exception=asyncio.CancelledError,\n                                            verbose=self.log_requests)\n\n    async def get_model_config(self) -> ModelConfig:\n        \"\"\"Get the model configuration of the vLLM engine.\"\"\"\n        return self.engine.get_model_config()\n\n    async def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Get the parallel configuration of the vLLM engine.\"\"\"\n        return self.engine.get_parallel_config()\n\n    async def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Get the decoding configuration of the vLLM engine.\"\"\"\n        return self.engine.get_decoding_config()\n\n    async def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Get the scheduling configuration of the vLLM engine.\"\"\"\n        return self.engine.get_scheduler_config()\n\n    async def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Get the lora configuration of the vLLM engine.\"\"\"\n        return self.engine.get_lora_config()\n\n    async def do_log_stats(\n            self,\n            scheduler_outputs: Optional[SchedulerOutputs] = None,\n            model_output: Optional[List[SamplerOutput]] = None) -> None:\n        self.engine.do_log_stats()\n\n    async def check_health(self) -> None:\n        \"\"\"Raises an error if engine is unhealthy.\"\"\"\n        t = time.perf_counter()\n        logger.debug(\"Starting health check...\")\n        if self.is_stopped:\n            raise AsyncEngineDeadError(\"Background loop is stopped.\")\n\n        await self.engine.check_health_async()\n        logger.debug(\"Health check took %fs\", time.perf_counter() - t)\n\n    async def is_tracing_enabled(self) -> bool:\n        return self.engine.is_tracing_enabled()\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        self.engine.add_logger(logger_name=logger_name, logger=logger)\n\n    def remove_logger(self, logger_name: str) -> None:\n        self.engine.remove_logger(logger_name=logger_name)\n\n    async def start_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes\n        if type(self.engine.model_executor) == GPUExecutorAsync:  # noqa: E721\n            self.engine.model_executor.start_profile()\n        else:\n            self.engine.model_executor._run_workers(\"start_profile\")\n\n    async def stop_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes\n        if type(self.engine.model_executor) == GPUExecutorAsync:  # noqa: E721\n            self.engine.model_executor.stop_profile()\n        else:\n            self.engine.model_executor._run_workers(\"stop_profile\")\n",
      "diff": "diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 4395588d2..60dccd7a0 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -1,4 +1,5 @@\n import asyncio\n+import copy\n import time\n import weakref\n from functools import partial\n@@ -507,7 +508,8 @@ class _AsyncLLMEngine(LLMEngine):\n                 sampling_params=params,\n                 tokenizer=await self.get_tokenizer_async(lora_request),\n                 default_guided_backend=self.decoding_config.\n-                guided_decoding_backend)\n+                guided_decoding_backend,\n+                model_config=self.model_config)\n \n         self._add_processed_request(\n             request_id=request_id,\n@@ -528,22 +530,30 @@ class _AsyncLLMEngine(LLMEngine):\n \n async def build_guided_decoding_logits_processor_async(\n         sampling_params: SamplingParams, tokenizer: AnyTokenizer,\n-        default_guided_backend: str) -> SamplingParams:\n+        default_guided_backend: str,\n+        model_config: ModelConfig) -> SamplingParams:\n     \"\"\"Constructs logits processors based on the guided_decoding,\n     logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n     those fields and adds the constructed logits processors to the\n     logits_processors field. Modifies sampling params in-place and returns\n     the modified sampling params.\"\"\"\n-    if (guided_decoding := sampling_params.guided_decoding) is None:\n+    if sampling_params.guided_decoding is None:\n         return sampling_params\n \n+    # Defensively copy sampling params since guided decoding logits\n+    # processors can have different state for each request\n+    sampling_params = copy.copy(sampling_params)\n+    guided_decoding = sampling_params.guided_decoding\n+\n     logger.debug(\"Building guided decoding logits processor. \"\n                  \"Params: %s\", guided_decoding)\n \n     guided_decoding.backend = guided_decoding.backend or default_guided_backend\n \n     processor = await get_guided_decoding_logits_processor(\n-        guided_params=guided_decoding, tokenizer=tokenizer)\n+        guided_params=guided_decoding,\n+        tokenizer=tokenizer,\n+        model_config=model_config)\n \n     if processor:\n         if sampling_params.logits_processors is None:",
      "change_type": "modified",
      "lines_added": 15,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/engine/llm_engine.py",
      "old_content": "import time\nfrom collections import Counter as collectionsCounter\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Deque, Dict,\n                    Iterable, List, Mapping, NamedTuple, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Type, Union, cast, overload\n\nimport torch\nfrom typing_extensions import TypeVar, deprecated\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig, SchedulerConfig,\n                         VllmConfig)\nfrom vllm.core.scheduler import (ScheduledSequenceGroup, Scheduler,\n                                 SchedulerOutputs)\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.entrypoints.openai.logits_processors import (\n    get_logits_processors as get_openai_logits_processors)\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.gpu_executor import GPUExecutor\nfrom vllm.executor.ray_utils import initialize_ray_cluster\nfrom vllm.inputs import (INPUT_REGISTRY, InputRegistry, ProcessorInputs,\n                         PromptType, SingletonInputsAdapter)\nfrom vllm.inputs.parse import is_encoder_decoder_inputs, is_token_prompt\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.logits_process import get_bad_words_logits_processors\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding import (\n    get_local_guided_decoding_logits_processor)\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.outputs import (PoolingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import RequestOutputKind, SamplingParams\nfrom vllm.sequence import (EmbeddingSequenceGroupOutput, ExecuteModelRequest,\n                           ParallelSampleSequenceGroup, Sequence,\n                           SequenceGroup, SequenceGroupBase,\n                           SequenceGroupMetadata, SequenceGroupOutput,\n                           SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.config import try_get_generation_config\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    BaseTokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import Counter, Device, deprecate_kwargs, weak_bind\nfrom vllm.version import __version__ as VLLM_VERSION\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n\ndef _load_generation_config_dict(model_config: ModelConfig) -> Dict[str, Any]:\n    config = try_get_generation_config(\n        model_config.model,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.revision,\n    )\n\n    if config is None:\n        return {}\n\n    return config.to_diff_dict()\n\n\n_G = TypeVar(\"_G\", bound=BaseTokenizerGroup, default=BaseTokenizerGroup)\n_O = TypeVar(\"_O\", RequestOutput, PoolingRequestOutput)\n\n\n@dataclass\nclass SchedulerOutputState:\n    \"\"\"Caches the scheduler outputs for a virtual engine. Used for Multi-Step\"\"\"\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    scheduler_outputs: Optional[SchedulerOutputs] = None\n    allow_async_output_proc: bool = False\n    last_output: Optional[SamplerOutput] = None\n\n\nclass OutputData(NamedTuple):\n    outputs: List[SamplerOutput]\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n    scheduler_outputs: SchedulerOutputs\n    is_async: bool\n    is_last_step: bool\n    # Indicates if this output is from the first step of the\n    # multi-step. When multi-step is disabled, this is always\n    # set to True.\n    # is_first_step_output is invalid when `outputs` has\n    # outputs from multiple steps.\n    is_first_step_output: Optional[bool]\n    skip: List[int]\n\n\nclass SchedulerContext:\n\n    def __init__(self, multi_step_stream_outputs: bool = False):\n        self.output_queue: Deque[OutputData] = deque()\n        self.request_outputs: List[Union[RequestOutput,\n                                         PoolingRequestOutput]] = []\n        self.seq_group_metadata_list: Optional[\n            List[SequenceGroupMetadata]] = None\n        self.scheduler_outputs: Optional[SchedulerOutputs] = None\n\n        self.multi_step_stream_outputs: bool = multi_step_stream_outputs\n\n    def append_output(self, outputs: List[SamplerOutput],\n                      seq_group_metadata_list: List[SequenceGroupMetadata],\n                      scheduler_outputs: SchedulerOutputs, is_async: bool,\n                      is_last_step: bool,\n                      is_first_step_output: Optional[bool]):\n        self.output_queue.append(\n            OutputData(outputs=outputs,\n                       seq_group_metadata_list=seq_group_metadata_list,\n                       scheduler_outputs=scheduler_outputs,\n                       is_async=is_async,\n                       is_last_step=is_last_step,\n                       is_first_step_output=is_first_step_output,\n                       skip=[]))\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The :class:`~vllm.LLM` class wraps this class for offline batched inference\n    and the :class:`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from :class:`~vllm.EngineArgs`. (See\n    :ref:`engine_args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving\n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return cast(_O, output)\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[BaseTokenizerGroup]\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n        use_cached_outputs: bool = False,\n    ) -> None:\n\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.device_config = vllm_config.device_config\n        self.speculative_config = vllm_config.speculative_config  # noqa\n        self.load_config = vllm_config.load_config\n        self.decoding_config = vllm_config.decoding_config or DecodingConfig(  # noqa\n        )\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config  # noqa\n        self.observability_config = vllm_config.observability_config or ObservabilityConfig(  # noqa\n        )\n\n        logger.info(\n            \"Initializing an LLM engine (v%s) with config: \"\n            \"model=%r, speculative_config=%r, tokenizer=%r, \"\n            \"skip_tokenizer_init=%s, tokenizer_mode=%s, revision=%s, \"\n            \"override_neuron_config=%s, tokenizer_revision=%s, \"\n            \"trust_remote_code=%s, dtype=%s, max_seq_len=%d, \"\n            \"download_dir=%r, load_format=%s, tensor_parallel_size=%d, \"\n            \"pipeline_parallel_size=%d, \"\n            \"disable_custom_all_reduce=%s, quantization=%s, \"\n            \"enforce_eager=%s, kv_cache_dtype=%s, \"\n            \"quantization_param_path=%s, device_config=%s, \"\n            \"decoding_config=%r, observability_config=%r, \"\n            \"seed=%d, served_model_name=%s, \"\n            \"num_scheduler_steps=%d, chunked_prefill_enabled=%s \"\n            \"multi_step_stream_outputs=%s, enable_prefix_caching=%s, \"\n            \"use_async_output_proc=%s, use_cached_outputs=%s, \"\n            \"mm_processor_kwargs=%s, pooler_config=%r,\"\n            \"compilation_config=%r\",\n            VLLM_VERSION,\n            self.model_config.model,\n            self.speculative_config,\n            self.model_config.tokenizer,\n            self.model_config.skip_tokenizer_init,\n            self.model_config.tokenizer_mode,\n            self.model_config.revision,\n            self.model_config.override_neuron_config,\n            self.model_config.tokenizer_revision,\n            self.model_config.trust_remote_code,\n            self.model_config.dtype,\n            self.model_config.max_model_len,\n            self.load_config.download_dir,\n            self.load_config.load_format,\n            self.parallel_config.tensor_parallel_size,\n            self.parallel_config.pipeline_parallel_size,\n            self.parallel_config.disable_custom_all_reduce,\n            self.model_config.quantization,\n            self.model_config.enforce_eager,\n            self.cache_config.cache_dtype,\n            self.model_config.quantization_param_path,\n            self.device_config.device,\n            self.decoding_config,\n            self.observability_config,\n            self.model_config.seed,\n            self.model_config.served_model_name,\n            self.scheduler_config.num_scheduler_steps,\n            self.scheduler_config.chunked_prefill_enabled,\n            self.scheduler_config.multi_step_stream_outputs,\n            self.cache_config.enable_prefix_caching,\n            self.model_config.use_async_output_proc,\n            use_cached_outputs,\n            self.model_config.mm_processor_kwargs,\n            self.model_config.pooler_config,\n            vllm_config.compilation_config,\n        )\n        # TODO(woosuk): Print more configs in debug mode.\n\n        self.log_stats = log_stats\n        self.use_cached_outputs = use_cached_outputs\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = _load_generation_config_dict(\n            self.model_config)\n\n        self.input_preprocessor = InputPreprocessor(self.model_config,\n                                                    self.tokenizer,\n                                                    mm_registry)\n\n        self.input_registry = input_registry\n        self.input_processor = input_registry.create_input_processor(\n            self.model_config)\n\n        self.model_executor = executor_class(vllm_config=vllm_config, )\n\n        if self.model_config.task != \"embedding\":\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(self.model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(self.model_config.dtype),\n                    \"tensor_parallel_size\":\n                    self.parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    self.cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    self.cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    self.model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(self.cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(self.lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(self.prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    self.cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    self.model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    self.parallel_config.disable_custom_all_reduce,\n                })\n\n        if self.tokenizer:\n            # Ping the tokenizer to ensure liveness if it runs in a\n            # different process.\n            self.tokenizer.ping()\n\n        self.cached_scheduler_outputs = [\n            SchedulerOutputState()\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.scheduler_contexts = [\n            SchedulerContext(multi_step_stream_outputs=self.scheduler_config.\n                             multi_step_stream_outputs)\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        if self.model_config.use_async_output_proc:\n            process_model_outputs = weak_bind(self._process_model_outputs)\n\n            self.async_callbacks = [\n                partial(process_model_outputs,\n                        ctx=self.scheduler_contexts[v_id])\n                for v_id in range(self.parallel_config.pipeline_parallel_size)\n            ]\n        else:\n            self.async_callbacks = []\n\n        # Currently used by AsyncLLMEngine to ensure quick append\n        # of request outputs to asyncio queues\n        self.process_request_outputs_callback: Optional[Callable] = None\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        self.scheduler = [\n            Scheduler(\n                self.scheduler_config, self.cache_config, self.lora_config,\n                self.parallel_config.pipeline_parallel_size,\n                self.async_callbacks[v_id]\n                if self.model_config.use_async_output_proc else None)\n            for v_id in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(\n                            model_name=self.model_config.served_model_name),\n                        max_model_len=self.model_config.max_model_len),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(\n                    self.scheduler_config.max_model_len,\n                    get_tokenizer_for_seq,\n                ),\n            ))\n\n        self.seq_id_to_seq_group: Dict[str, SequenceGroupBase] = {}\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: VllmConfig) -> Type[ExecutorBase]:\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            if distributed_executor_backend.uses_ray:  # type: ignore\n                initialize_ray_cluster(engine_config.parallel_config)\n            executor_class = distributed_executor_backend\n        elif engine_config.device_config.device_type == \"neuron\":\n            from vllm.executor.neuron_executor import NeuronExecutor\n            executor_class = NeuronExecutor\n        elif engine_config.device_config.device_type == \"tpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_tpu_executor import RayTPUExecutor\n                executor_class = RayTPUExecutor\n            else:\n                assert distributed_executor_backend is None\n                from vllm.executor.tpu_executor import TPUExecutor\n                executor_class = TPUExecutor\n        elif engine_config.device_config.device_type == \"cpu\":\n            from vllm.executor.cpu_executor import CPUExecutor\n            executor_class = CPUExecutor\n        elif engine_config.device_config.device_type == \"hpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_hpu_executor import RayHPUExecutor\n                executor_class = RayHPUExecutor\n            else:\n                from vllm.executor.hpu_executor import HPUExecutor\n                executor_class = HPUExecutor\n        elif engine_config.device_config.device_type == \"openvino\":\n            from vllm.executor.openvino_executor import OpenVINOExecutor\n            executor_class = OpenVINOExecutor\n        elif engine_config.device_config.device_type == \"xpu\":\n            if distributed_executor_backend == \"ray\":\n                initialize_ray_cluster(engine_config.parallel_config)\n                from vllm.executor.ray_xpu_executor import RayXPUExecutor\n                executor_class = RayXPUExecutor\n            elif distributed_executor_backend == \"mp\":\n                # FIXME(kunshang):\n                # spawn needs calling `if __name__ == '__main__':``\n                # fork is not supported for xpu start new process.\n                logger.error(\n                    \"Both start methods (spawn and fork) have issue \"\n                    \"on XPU if you use mp backend, Please try ray instead.\")\n            else:\n                from vllm.executor.xpu_executor import XPUExecutor\n                executor_class = XPUExecutor\n        elif distributed_executor_backend == \"ray\":\n            initialize_ray_cluster(engine_config.parallel_config)\n            from vllm.executor.ray_gpu_executor import RayGPUExecutor\n            executor_class = RayGPUExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.multiproc_gpu_executor import (\n                MultiprocessingGPUExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingGPUExecutor\n        else:\n            from vllm.executor.gpu_executor import GPUExecutor\n            executor_class = GPUExecutor\n        return executor_class\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_config = engine_args.create_engine_config(usage_context)\n        executor_class = cls._get_executor_cls(engine_config)\n        # Create the LLM engine.\n        engine = cls(\n            vllm_config=engine_config,\n            executor_class=executor_class,\n            log_stats=not engine_args.disable_log_stats,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n\n        return engine\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    def get_tokenizer_group(\n        self,\n        group_type: Type[_G] = BaseTokenizerGroup,\n    ) -> _G:\n        tokenizer_group = self.tokenizer\n\n        if tokenizer_group is None:\n            raise ValueError(\"Unable to get tokenizer because \"\n                             \"skip_tokenizer_init is True\")\n        if not isinstance(tokenizer_group, group_type):\n            raise TypeError(\"Invalid type of tokenizer group. \"\n                            f\"Expected type: {group_type}, but \"\n                            f\"found type: {type(tokenizer_group)}\")\n\n        return tokenizer_group\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> BaseTokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            parallel_config=self.parallel_config,\n            enable_lora=bool(self.lora_config))\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: ProcessorInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> Optional[SequenceGroup]:\n        \"\"\"Add a processed request to the engine's request pool.\n        return the created sequence group.\n        \"\"\"\n        if isinstance(params, SamplingParams) and params.n > 1:\n            ParallelSampleSequenceGroup.add_request(\n                request_id,\n                self,\n                params,\n                processed_inputs=processed_inputs,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority,\n            )\n            return None\n\n        self._validate_model_inputs(processed_inputs, lora_request)\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)\n\n        if is_encoder_decoder_inputs(processed_inputs):\n            decoder_inputs = processed_inputs[\"decoder\"]\n            encoder_inputs = processed_inputs[\"encoder\"]\n        else:\n            decoder_inputs = processed_inputs\n            encoder_inputs = None\n\n        seq = Sequence(seq_id, decoder_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = (None if encoder_inputs is None else Sequence(\n            seq_id, encoder_inputs, block_size, eos_token_id, lora_request,\n            prompt_adapter_request))\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n        return seq_group\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    def add_request(\n        self,\n        request_id: str,\n        *,\n        inputs: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @overload\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    def add_request(\n            self,\n            request_id: str,\n            prompt: Optional[PromptType] = None,\n            params: Optional[Union[SamplingParams, PoolingParams]] = None,\n            arrival_time: Optional[float] = None,\n            lora_request: Optional[LoRARequest] = None,\n            trace_headers: Optional[Mapping[str, str]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n            priority: int = 0,\n            *,\n            inputs: Optional[PromptType] = None,  # DEPRECATED\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            params: Parameters for sampling or pooling.\n                :class:`~vllm.SamplingParams` for text generation.\n                :class:`~vllm.PoolingParams` for pooling.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            trace_headers: OpenTelemetry trace headers.\n            priority: The priority of the request.\n                Only applicable with priority scheduling.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `n` number of :class:`~vllm.Sequence` objects.\n            - Create a :class:`~vllm.SequenceGroup` object\n              from the list of :class:`~vllm.Sequence`.\n            - Add the :class:`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert prompt is not None and params is not None\n\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n\n        if priority != 0 and not self.scheduler_config.policy == \"priority\":\n            raise ValueError(f\"Got priority {priority} but \"\n                             \"Priority scheduling is not enabled.\")\n\n        if isinstance(params, SamplingParams) \\\n            and (params.guided_decoding or params.logits_processors) \\\n            and self.scheduler_config.num_scheduler_steps > 1:\n            raise ValueError(\n                \"Guided decoding and logits processors are not supported \"\n                \"in multi-step decoding\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        if self.tokenizer is not None:\n            self._validate_token_prompt(\n                prompt,\n                tokenizer=self.get_tokenizer(lora_request=lora_request))\n\n        preprocessed_inputs = self.input_preprocessor.preprocess(\n            prompt,\n            request_id=request_id,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n        processed_inputs = self.input_processor(preprocessed_inputs)\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n    def _validate_token_prompt(self, prompt: PromptType,\n                               tokenizer: AnyTokenizer):\n        # Guard against out-of-vocab tokens.\n        # For some tokenizers, tokenizer.decode will happily return empty text\n        # for token ids that are out of vocab, and we don't detect token ids\n        # that are greater than the max token id before running the model.\n        # However, these token ids will later crash a cuda kernel at runtime\n        # with an index out of bounds error. This will crash the entire engine.\n        # This needs to happen before multimodal input pre-processing, which\n        # may add dummy <image> tokens that aren't part of the tokenizer's\n        # vocabulary.\n        if is_token_prompt(prompt):\n            prompt_ids = prompt[\"prompt_token_ids\"]\n            if len(prompt_ids) == 0:\n                # Empty prompt check is handled later\n                return\n            max_input_id = max(prompt_ids)\n            if max_input_id > tokenizer.max_token_id:\n                raise ValueError(\n                    \"Token id {} is out of vocabulary\".format(max_input_id))\n\n    def _create_sequence_group_with_sampling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        sampling_params: SamplingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with SamplingParams.\"\"\"\n        max_logprobs = self.get_model_config().max_logprobs\n        if (sampling_params.logprobs\n                and sampling_params.logprobs > max_logprobs) or (\n                    sampling_params.prompt_logprobs\n                    and sampling_params.prompt_logprobs > max_logprobs):\n            raise ValueError(f\"Cannot request more than \"\n                             f\"{max_logprobs} logprobs.\")\n\n        sampling_params = self._build_logits_processors(\n            sampling_params, lora_request)\n\n        # Defensive copy of SamplingParams, which are used by the sampler,\n        # this doesn't deep-copy LogitsProcessor objects\n        sampling_params = sampling_params.clone()\n\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, seq.eos_token_id)\n\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority)\n\n        return seq_group\n\n    def _create_sequence_group_with_pooling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        pooling_params: PoolingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with PoolingParams.\"\"\"\n        # Defensive copy of PoolingParams, which are used by the pooler\n        pooling_params = pooling_params.clone()\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            pooling_params=pooling_params,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority)\n        return seq_group\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n\n        Details:\n            - Refer to the\n              :meth:`~vllm.core.scheduler.Scheduler.abort_seq_group`\n              from class :class:`~vllm.core.scheduler.Scheduler`.\n\n        Example:\n            >>> # initialize engine and add a request with request_id\n            >>> request_id = str(0)\n            >>> # abort the request\n            >>> engine.abort_request(request_id)\n        \"\"\"\n        for scheduler in self.scheduler:\n            scheduler.abort_seq_group(request_id)\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Gets the parallel configuration.\"\"\"\n        return self.parallel_config\n\n    def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Gets the decoding configuration.\"\"\"\n        return self.decoding_config\n\n    def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Gets the scheduler configuration.\"\"\"\n        return self.scheduler_config\n\n    def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Gets the LoRA configuration.\"\"\"\n        return self.lora_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return sum(scheduler.get_num_unfinished_seq_groups()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return any(scheduler.has_unfinished_seqs()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests_for_virtual_engine(\n            self, virtual_engine: int) -> bool:\n        \"\"\"\n        Returns True if there are unfinished requests for the virtual engine.\n        \"\"\"\n        return self.scheduler[virtual_engine].has_unfinished_seqs()\n\n    @staticmethod\n    def _process_sequence_group_outputs(\n        seq_group: SequenceGroup,\n        outputs: List[EmbeddingSequenceGroupOutput],\n    ) -> None:\n        seq_group.embeddings = outputs[0].embeddings\n\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.FINISHED_STOPPED\n\n        return\n\n    def _update_num_computed_tokens_for_multi_step_prefill(\n            self, seq_group: SequenceGroup,\n            seq_group_meta: SequenceGroupMetadata,\n            is_first_step_output: Optional[bool]):\n        \"\"\"\n        This function updates num_computed_tokens for prompt sequences\n        when Multi-Step is enabled.\n\n        seq_group: SequenceGroup to update the num_computed_tokens for. \n        seq_group_meta: Metadata of the given SequenceGroup.\n        is_first_step_output: Optional[bool] - \n            When available, is_first_step_output indicates if the appended\n            output token is the output of the first-step in multi-step.\n            A value of None indicates that outputs from all steps in\n            in multi-step are submitted in a single burst.\n        \"\"\"\n\n        assert self.scheduler_config.is_multi_step\n\n        if not seq_group_meta.is_prompt:\n            # num_computed_token updates for multi-step decodes happen after\n            # the tokens are appended to the sequence.\n            return\n\n        do_update: bool = False\n        if self.scheduler_config.chunked_prefill_enabled:\n            # In multi-step + chunked-prefill case, the prompt sequences\n            # that are scheduled are fully processed in the first step.\n            do_update = is_first_step_output is None or is_first_step_output\n        else:\n            # Normal multi-step decoding case. In this case prompt-sequences\n            # are actually single-stepped. Always update in this case.\n            assert seq_group.state.num_steps == 1\n            do_update = True\n\n        if do_update:\n            seq_group.update_num_computed_tokens(\n                seq_group_meta.token_chunk_size)\n\n    def _process_model_outputs(self,\n                               ctx: SchedulerContext,\n                               request_id: Optional[str] = None) -> None:\n        \"\"\"Apply the model output to the sequences in the scheduled seq groups\n        and return responses.\n\n        ctx: The virtual engine context to work on\n        request_id: If provided, then only this request is going to be processed\n        \"\"\"\n\n        now = time.time()\n\n        if len(ctx.output_queue) == 0:\n            return None\n\n        # Get pending async postprocessor\n        if request_id:\n            # When we process only one request, no pop is required\n            # (since later we will process all of the rest)\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output, skip) = ctx.output_queue[0]\n        else:\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output,\n             skip) = ctx.output_queue.popleft()\n\n        # Sanity check\n        assert len(seq_group_metadata_list) == len(\n            scheduler_outputs.scheduled_seq_groups)\n\n        has_multiple_outputs: bool = len(outputs) > 1\n        outputs_by_sequence_group: List[List[SequenceGroupOutput]]\n        if has_multiple_outputs:\n            assert self.scheduler_config.is_multi_step or \\\n                     self.speculative_config\n            # Organize outputs by [step][sequence group] instead of\n            # [sequence group][step].\n            outputs_by_sequence_group = create_output_by_sequence_group(\n                outputs, num_seq_groups=len(seq_group_metadata_list))\n            # We have outputs for multiple steps submitted in a single burst,\n            # so invalidate is_first_step_output.\n            is_first_step_output = None\n        else:\n            outputs_by_sequence_group = outputs\n\n        # Determine the requests we need to operate on\n        if request_id:\n            indices = []\n            for i, seq_group_meta in enumerate(seq_group_metadata_list):\n                if seq_group_meta.request_id == request_id:\n                    assert i not in skip  # Cannot be called twice\n                    indices.append(i)\n                    break\n\n            # If the request_id was not found, then it means that\n            # this is a new request that has no pending async\n            # postprocessor\n            if not indices:\n                return\n        else:\n            indices = range(len(seq_group_metadata_list))  # type: ignore\n\n        finished_before: List[int] = []\n        finished_now: List[int] = []\n        for i in indices:\n            if i in skip:\n                continue\n\n            seq_group_meta = seq_group_metadata_list[i]\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group: SequenceGroup = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                finished_before.append(i)\n                continue\n\n            output: List[SequenceGroupOutput]\n            if has_multiple_outputs:\n                output = outputs_by_sequence_group[i]\n            else:\n                output = [outputs_by_sequence_group[0][i]]\n\n            if not is_async:\n                if self.scheduler_config.is_multi_step:\n                    # Updates happen only if the sequence is prefill\n                    self._update_num_computed_tokens_for_multi_step_prefill(\n                        seq_group, seq_group_meta, is_first_step_output)\n                else:\n                    seq_group.update_num_computed_tokens(\n                        seq_group_meta.token_chunk_size or 0)\n\n            if outputs:\n                for o in outputs:\n                    if (isinstance(o, SamplerOutput)\n                            and seq_group.metrics is not None):\n                        if seq_group.metrics.model_forward_time is not None:\n                            seq_group.metrics.model_forward_time += (\n                                o.model_forward_time or 0)\n                        else:\n                            seq_group.metrics.model_forward_time = (\n                                o.model_forward_time)\n                        if seq_group.metrics.model_execute_time is not None:\n                            seq_group.metrics.model_execute_time += (\n                                o.model_execute_time or 0)\n                        else:\n                            seq_group.metrics.model_execute_time = (\n                                o.model_execute_time)\n\n            if self.model_config.task == \"embedding\":\n                self._process_sequence_group_outputs(seq_group, output)\n            else:\n                self.output_processor.process_prompt_logprob(seq_group, output)\n                if seq_group_meta.do_sample:\n                    self.output_processor.process_outputs(\n                        seq_group, output, is_async)\n\n            if seq_group.is_finished():\n                finished_now.append(i)\n\n        # Generate outputs for the requests that finished this iteration\n        for i in finished_now:\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # When we process a single request, we skip it for the next time,\n        # and invoke the request output callback (if there was final output)\n        if request_id:\n            assert len(indices) == 1\n            skip.append(indices[0])\n\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Free currently finished requests\n        if finished_now:\n            for scheduler in self.scheduler:\n                scheduler.free_finished_seq_groups()\n\n        # For multi-step without streaming, don't create outputs each iteration\n        if not is_last_step and not ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Create the outputs\n        for i in indices:\n            if i in skip or i in finished_before or i in finished_now:\n                continue  # Avoids double processing\n\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # For multi-step with streaming, create outputs each iteration\n        if not is_last_step and ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if self.process_request_outputs_callback is not None:\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        for seq_group in scheduler_outputs.ignored_seq_groups:\n            params = seq_group.sampling_params\n            if params is not None and params.output_kind == (\n                    RequestOutputKind.DELTA) and not seq_group.is_finished():\n                continue\n\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs,\n            )\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # Immediately process request outputs here (if callback is given)\n        if (ctx.request_outputs\n                and self.process_request_outputs_callback is not None):\n            self.process_request_outputs_callback(ctx.request_outputs)\n            ctx.request_outputs.clear()\n\n        # For async case, we need to record the stats here.\n        # For non-async case, the stats are done in the\n        # LLMEngine/AsyncLLMEngine directly\n        if is_async:\n            # Log stats.\n            self.do_log_stats(scheduler_outputs, outputs, finished_before,\n                              skip)\n\n            # Tracing\n            self.do_tracing(scheduler_outputs, finished_before)\n\n        return None\n\n    def _advance_to_next_step(\n            self, output: List[SamplerOutput],\n            seq_group_metadata_list: List[SequenceGroupMetadata],\n            scheduled_seq_groups: List[ScheduledSequenceGroup]) -> None:\n        \"\"\"Given model output from a single run, append the tokens to the\n        sequences. This is normally done inside output processor, but it is\n        required if the worker is to perform async forward pass to next step.\n        \"\"\"\n        for seq_group_metadata, sequence_group_outputs, scheduled_seq_group in \\\n            zip(seq_group_metadata_list, output, scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                continue\n\n            if self.scheduler_config.is_multi_step:\n                # Updates happen only if the sequence is prefill\n                self._update_num_computed_tokens_for_multi_step_prefill(\n                    seq_group, seq_group_metadata,\n                    seq_group.state.num_steps == 1)\n            else:\n                token_chunk_size = (seq_group_metadata.token_chunk_size\n                                    if seq_group_metadata.token_chunk_size\n                                    is not None else 0)\n                seq_group.update_num_computed_tokens(token_chunk_size)\n\n            if seq_group_metadata.do_sample:\n                assert len(sequence_group_outputs.samples) == 1, (\n                    \"Async output processor expects a single sample\"\n                    \" (i.e sampling_params.n == 1)\")\n                sample = sequence_group_outputs.samples[0]\n\n                assert len(seq_group.seqs) == 1\n                seq = seq_group.seqs[0]\n\n                if self.scheduler_config.is_multi_step:\n                    is_prefill_append = seq.data.get_num_uncomputed_tokens(\n                    ) == 0\n                    seq.append_token_id(sample.output_token, sample.logprobs)\n                    if not is_prefill_append:\n                        seq_group.update_num_computed_tokens(1)\n                else:\n                    seq.append_token_id(sample.output_token, sample.logprobs)\n\n    def step(self) -> List[Union[RequestOutput, PoolingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        .. figure:: https://i.imgur.com/sv2HssD.png\n            :alt: Overview of the step function\n            :align: center\n\n            Overview of the step function.\n\n        Details:\n            - Step 1: Schedules the sequences to be executed in the next\n              iteration and the token blocks to be swapped in/out/copy.\n\n                - Depending on the scheduling policy,\n                  sequences may be `preempted/reordered`.\n                - A Sequence Group (SG) refer to a group of sequences\n                  that are generated from the same prompt.\n\n            - Step 2: Calls the distributed executor to execute the model.\n            - Step 3: Processes the model output. This mainly includes:\n\n                - Decodes the relevant outputs.\n                - Updates the scheduled sequence groups with model outputs\n                  based on its `sampling parameters` (`use_beam_search` or not).\n                - Frees the finished sequence groups.\n\n            - Finally, it creates and returns the newly generated results.\n\n        Example:\n            >>> # Please see the example/ folder for more detailed examples.\n            >>>\n            >>> # initialize engine and request arguments\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> example_inputs = [(0, \"What is LLM?\",\n            >>>    SamplingParams(temperature=0.0))]\n            >>>\n            >>> # Start the engine with an event loop\n            >>> while True:\n            >>>     if example_inputs:\n            >>>         req_id, prompt, sampling_params = example_inputs.pop(0)\n            >>>         engine.add_request(str(req_id),prompt,sampling_params)\n            >>>\n            >>>     # continue the request processing\n            >>>     request_outputs = engine.step()\n            >>>     for request_output in request_outputs:\n            >>>         if request_output.finished:\n            >>>             # return or show the request output\n            >>>\n            >>>     if not (engine.has_unfinished_requests() or example_inputs):\n            >>>         break\n        \"\"\"\n        if self.parallel_config.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is only supported through AsyncLLMEngine \"\n                \"as performance will be severely degraded otherwise.\")\n\n        # For llm_engine, there is no pipeline parallel support, so the engine\n        # used is always 0.\n        virtual_engine = 0\n\n        # These are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # Clear outputs for each new scheduler iteration\n        ctx.request_outputs.clear()\n\n        # Skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            ctx.seq_group_metadata_list = seq_group_metadata_list\n            ctx.scheduler_outputs = scheduler_outputs\n\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n        else:\n            finished_requests_ids = list()\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                execute_model_req.async_callback = self.async_callbacks[\n                    virtual_engine]\n\n            outputs = self.model_executor.execute_model(\n                execute_model_req=execute_model_req)\n\n            # We need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, outputs)\n        else:\n            # Nothing scheduled => If there is pending async postprocessor,\n            # then finish it here.\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            # No outputs in this case\n            outputs = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # clear the cache if we have finished all the steps.\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[0] = SchedulerOutputState()\n\n            # is_first_step_output is True only when the num_steps of all\n            # the sequences are 1. When the num_steps > 1,\n            # multi_step_model_runner does the first-step output append.\n            is_first_step_output: bool = False if not seq_group_metadata_list \\\n                else seq_group_metadata_list[0].state.num_steps == 1\n\n            # Add results to the output_queue\n            ctx.append_output(outputs=outputs,\n                              seq_group_metadata_list=seq_group_metadata_list,\n                              scheduler_outputs=scheduler_outputs,\n                              is_async=allow_async_output_proc,\n                              is_last_step=True,\n                              is_first_step_output=is_first_step_output)\n\n            if outputs and allow_async_output_proc:\n                assert len(outputs) == 1, (\n                    \"Async postprocessor expects only a single output set\")\n\n                self._advance_to_next_step(\n                    outputs[0], seq_group_metadata_list,\n                    scheduler_outputs.scheduled_seq_groups)\n\n            # Check if need to run the usual non-async path\n            if not allow_async_output_proc:\n                self._process_model_outputs(ctx=ctx)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, outputs)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n        else:\n            # Multi-step case\n            return ctx.request_outputs\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            assert len(ctx.output_queue) == 0\n\n            # Stop the execute model loop in parallel workers until there are\n            # more requests to process. This avoids waiting indefinitely in\n            # torch.distributed ops which may otherwise timeout, and unblocks\n            # the RPC thread in the workers so that they can process any other\n            # queued control plane messages, such as add/remove lora adapters.\n            logger.debug(\"Stopping remote worker execution loop.\")\n            self.model_executor.stop_remote_worker_execution_loop()\n\n        return ctx.request_outputs\n\n    def _has_remaining_steps(\n        self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]\n    ) -> bool:\n        if (not self.scheduler_config.is_multi_step\n                or not seq_group_metadata_list):\n            return False\n\n        # TODO(will) this is a sanity check for nowto make sure that all the\n        # seqs are on the same steps. Eventually we will want to do some sort of\n        # dynamic scheduling when doing multi-step decoding.\n        ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps\n        if any([\n                seq_group.state.remaining_steps != ref_remaining_steps\n                for seq_group in seq_group_metadata_list[1:]\n        ]):\n            raise AssertionError(\"All running sequence groups should \"\n                                 \"have the same remaining steps.\")\n\n        return ref_remaining_steps > 0\n\n    def _cache_scheduler_outputs_for_multi_step(\n            self, virtual_engine: int,\n            seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n            scheduler_outputs: SchedulerOutputs,\n            allow_async_output_proc: bool) -> None:\n        co = self.cached_scheduler_outputs[virtual_engine]\n\n        co.seq_group_metadata_list = seq_group_metadata_list\n        co.scheduler_outputs = scheduler_outputs\n        co.allow_async_output_proc = allow_async_output_proc\n        co.last_output = None\n\n    def _update_cached_scheduler_output(\n            self, virtual_engine: int,\n            output: List[Optional[SamplerOutput]]) -> None:\n        if (self.parallel_config.pipeline_parallel_size > 1 and len(output) > 0\n                and output[0] is not None):\n            last_output = output[-1]\n            assert last_output is not None\n            assert last_output.sampled_token_ids_cpu is not None\n            assert last_output.sampled_token_ids is None\n            assert last_output.sampled_token_probs is None\n            self.cached_scheduler_outputs[\n                virtual_engine].last_output = last_output\n\n    def _get_last_sampled_token_ids(\n            self, virtual_engine: int) -> Optional[torch.Tensor]:\n        cached_last_output = self.cached_scheduler_outputs[\n            virtual_engine].last_output\n        if (self.scheduler_config.is_multi_step\n                and self.parallel_config.pipeline_parallel_size > 1\n                and cached_last_output is not None\n                and cached_last_output.sampled_token_ids_cpu is not None):\n            return cached_last_output.sampled_token_ids_cpu\n        return None\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} already exists.\")\n        self.stat_loggers[logger_name] = logger\n\n    def remove_logger(self, logger_name: str) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name not in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} does not exist.\")\n        del self.stat_loggers[logger_name]\n\n    def do_log_stats(self,\n                     scheduler_outputs: Optional[SchedulerOutputs] = None,\n                     model_output: Optional[List[SamplerOutput]] = None,\n                     finished_before: Optional[List[int]] = None,\n                     skip: Optional[List[int]] = None) -> None:\n        \"\"\"Forced log when no requests active.\"\"\"\n        if self.log_stats:\n            stats = self._get_stats(scheduler_outputs, model_output,\n                                    finished_before, skip)\n            for logger in self.stat_loggers.values():\n                logger.log(stats)\n\n    def _get_stats(self,\n                   scheduler_outputs: Optional[SchedulerOutputs],\n                   model_output: Optional[List[SamplerOutput]] = None,\n                   finished_before: Optional[List[int]] = None,\n                   skip: Optional[List[int]] = None) -> Stats:\n        \"\"\"Get Stats to be Logged to Prometheus.\n\n        Args:\n            scheduler_outputs: Optional, used to populate metrics related to\n                the scheduled batch,\n            model_output: Optional, used to emit speculative decoding metrics\n                which are created by the workers.\n            finished_before: Optional, indices of sequences that were finished\n                before. These sequences will be ignored.\n            skip: Optional, indices of sequences that were preempted. These\n                sequences will be ignored.\n        \"\"\"\n        now = time.time()\n\n        # System State\n        #   Scheduler State\n        num_running_sys = sum(\n            len(scheduler.running) for scheduler in self.scheduler)\n        num_swapped_sys = sum(\n            len(scheduler.swapped) for scheduler in self.scheduler)\n        num_waiting_sys = sum(\n            len(scheduler.waiting) for scheduler in self.scheduler)\n\n        # KV Cache Usage in %\n        num_total_gpu = self.cache_config.num_gpu_blocks\n        gpu_cache_usage_sys = 0.\n        if num_total_gpu:  # Guard against both None and 0\n            num_free_gpu = sum(\n                scheduler.block_manager.get_num_free_gpu_blocks()\n                for scheduler in self.scheduler)\n            gpu_cache_usage_sys = 1.0 - (num_free_gpu / num_total_gpu)\n\n        num_total_cpu = self.cache_config.num_cpu_blocks\n        cpu_cache_usage_sys = 0.\n        if num_total_cpu:  # Guard against both None and 0\n            num_free_cpu = sum(\n                scheduler.block_manager.get_num_free_cpu_blocks()\n                for scheduler in self.scheduler)\n            cpu_cache_usage_sys = 1.0 - (num_free_cpu / num_total_cpu)\n\n        # Prefix Cache Hit Rate. Note that we always use\n        # the cache hit rate of the first virtual engine.\n        cpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.CPU)\n        gpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.GPU)\n\n        # Iteration stats\n        num_prompt_tokens_iter = 0\n        num_generation_tokens_iter = 0\n        num_tokens_iter = 0\n        time_to_first_tokens_iter: List[float] = []\n        time_per_output_tokens_iter: List[float] = []\n        num_preemption_iter = (0 if scheduler_outputs is None else\n                               scheduler_outputs.preempted)\n\n        # Request stats\n        #   Latency\n        time_e2e_requests: List[float] = []\n        time_queue_requests: List[float] = []\n        time_inference_requests: List[float] = []\n        time_prefill_requests: List[float] = []\n        time_decode_requests: List[float] = []\n        time_in_queue_requests: List[float] = []\n        model_forward_time_requests: List[float] = []\n        model_execute_time_requests: List[float] = []\n        #   Metadata\n        num_prompt_tokens_requests: List[int] = []\n        num_generation_tokens_requests: List[int] = []\n        n_requests: List[int] = []\n        max_num_generation_tokens_requests: List[int] = []\n        max_tokens_requests: List[int] = []\n        finished_reason_requests: List[str] = []\n\n        # Lora requests\n        running_lora_adapters = dict(\n            collectionsCounter([\n                running_request.lora_request.lora_name\n                for scheduler in self.scheduler\n                for running_request in scheduler.running\n                if running_request.lora_request\n            ]))\n        waiting_lora_adapters = dict(\n            collectionsCounter([\n                waiting_request.lora_request.lora_name\n                for scheduler in self.scheduler\n                for waiting_request in scheduler.waiting\n                if waiting_request.lora_request\n            ]))\n        max_lora_stat = \"0\"\n        if self.lora_config:\n            max_lora_stat = str(self.lora_config.max_loras)\n\n        # NOTE: This loop assumes prefill seq_groups are before\n        # decode seq_groups in scheduled_seq_groups.\n        if scheduler_outputs is not None:\n            # For async postprocessor, already finished sequences need to be\n            # not counted (to avoid double counting)\n            actual_num_batched_tokens = scheduler_outputs.num_batched_tokens  # type: ignore\n\n            num_generation_tokens_from_prefill_groups = 0\n            # NOTE: if scheduler_outputs.num_prefill_groups > 0 and\n            # the len of scheduler_outputs.scheduled_seq_groups is !=\n            # scheduler_outputs.num_prefill_groups, this means that\n            # chunked prefills have been detected.\n\n            for idx, scheduled_seq_group in enumerate(\n                    scheduler_outputs.scheduled_seq_groups):\n                # Skip double logging when using async output proc\n                if finished_before and idx in finished_before:\n                    actual_num_batched_tokens -= 1\n                    continue\n\n                # Currently, skip == preempted sequences, so we need to skip\n                # their log stats\n                if skip and idx in skip:\n                    continue\n\n                group_was_prefill = idx < scheduler_outputs.num_prefill_groups\n                seq_group = scheduled_seq_group.seq_group\n\n                # NOTE: a seq_group that completed all of its prefill tokens\n                # in the last iteration will have seq_group.is_prefill() = False\n                # with group_was_prefill = True\n                if group_was_prefill:\n                    # Number of prompt tokens.\n                    num_prompt_tokens_iter += (\n                        scheduled_seq_group.token_chunk_size)\n\n                    # If the seq_group just finished the prefill state\n                    # get TTFT.\n                    if not seq_group.is_prefill():\n                        latency = seq_group.get_last_latency(now)\n                        time_to_first_tokens_iter.append(latency)\n\n                        # One generation token per finished prefill.\n                        num_generation_tokens_from_prefill_groups += (\n                            seq_group.num_seqs())\n                else:\n                    # TPOTs.\n                    latency = seq_group.get_last_latency(now)\n                    time_per_output_tokens_iter.append(latency)\n                    if seq_group.state.current_step == 0:\n                        # For async_output_proc, the do_log_stats()\n                        # is called following init_multi_step(), which\n                        # sets the current_step to zero.\n                        actual_num_batched_tokens +=\\\n                            seq_group.state.num_steps - 1\n                    else:\n                        actual_num_batched_tokens +=\\\n                            seq_group.state.current_step - 1\n\n                # Because of chunked prefill, we can have a single sequence\n                # group that does multiple prompt_runs. To prevent logging\n                # the same metadata more than once per request, we standardize\n                # on logging request level information for finished requests,\n                # which can only happen once.\n                if seq_group.is_finished():\n                    # Latency timings\n                    time_e2e_requests.append(now -\n                                             seq_group.metrics.arrival_time)\n                    if (seq_group.metrics.first_scheduled_time is not None and\n                            seq_group.metrics.first_token_time is not None):\n                        time_queue_requests.append(\n                            seq_group.metrics.first_scheduled_time -\n                            seq_group.metrics.arrival_time)\n                        time_prefill_requests.append(\n                            seq_group.metrics.first_token_time -\n                            seq_group.metrics.first_scheduled_time)\n                        time_decode_requests.append(\n                            now - seq_group.metrics.first_token_time)\n                        time_inference_requests.append(\n                            now - seq_group.metrics.first_scheduled_time)\n                    if seq_group.metrics.time_in_queue is not None:\n                        time_in_queue_requests.append(\n                            seq_group.metrics.time_in_queue)\n                    if seq_group.metrics.model_forward_time is not None:\n                        model_forward_time_requests.append(\n                            seq_group.metrics.model_forward_time)\n                    if seq_group.metrics.model_execute_time is not None:\n                        model_execute_time_requests.append(\n                            seq_group.metrics.model_execute_time * 1000)\n                    # Metadata\n                    num_prompt_tokens_requests.append(\n                        len(seq_group.prompt_token_ids))\n                    num_generation_tokens_requests.extend([\n                        seq.get_output_len()\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n                    max_num_generation_tokens_requests.append(\n                        max(seq.get_output_len()\n                            for seq in seq_group.get_seqs()))\n                    if seq_group.sampling_params is not None:\n                        n_requests.append(seq_group.sampling_params.n)\n                        max_tokens_requests.append(\n                            seq_group.sampling_params.max_tokens)\n                    finished_reason_requests.extend([\n                        SequenceStatus.get_finished_reason(seq.status)\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n\n            # Number of generation tokens.\n            #   num_batched_tokens equals the number of prompt_tokens plus the\n            #   number of decode_tokens in a single iteration. So,\n            #   num_generation_tokens = num_batched_tokens - num_prompt_tokens\n            #   + num_generation_tokens_from_prefill_groups (since we generate\n            #   one token on prefills on iters where the prefill finishes).\n            num_generation_tokens_iter = (\n                actual_num_batched_tokens - num_prompt_tokens_iter +\n                num_generation_tokens_from_prefill_groups)\n            num_tokens_iter = (num_generation_tokens_iter +\n                               num_prompt_tokens_iter)\n        # Spec decode, if enabled, emits specialized metrics from the worker in\n        # sampler output.\n        if model_output and (model_output[0].spec_decode_worker_metrics\n                             is not None):\n            spec_decode_metrics = model_output[0].spec_decode_worker_metrics\n        else:\n            spec_decode_metrics = None\n\n        return Stats(\n            now=now,\n            # System stats\n            #   Scheduler State\n            num_running_sys=num_running_sys,\n            num_swapped_sys=num_swapped_sys,\n            num_waiting_sys=num_waiting_sys,\n            #   KV Cache Usage in %\n            gpu_cache_usage_sys=gpu_cache_usage_sys,\n            cpu_cache_usage_sys=cpu_cache_usage_sys,\n            #   Prefix Cache Hit Rate\n            cpu_prefix_cache_hit_rate=cpu_prefix_cache_hit_rate,\n            gpu_prefix_cache_hit_rate=gpu_prefix_cache_hit_rate,\n\n            # Iteration stats\n            num_prompt_tokens_iter=num_prompt_tokens_iter,\n            num_generation_tokens_iter=num_generation_tokens_iter,\n            num_tokens_iter=num_tokens_iter,\n            time_to_first_tokens_iter=time_to_first_tokens_iter,\n            time_per_output_tokens_iter=time_per_output_tokens_iter,\n            spec_decode_metrics=spec_decode_metrics,\n            num_preemption_iter=num_preemption_iter,\n\n            # Request stats\n            #   Latency\n            time_e2e_requests=time_e2e_requests,\n            time_queue_requests=time_queue_requests,\n            time_inference_requests=time_inference_requests,\n            time_prefill_requests=time_prefill_requests,\n            time_decode_requests=time_decode_requests,\n            time_in_queue_requests=time_in_queue_requests,\n            model_forward_time_requests=model_forward_time_requests,\n            model_execute_time_requests=model_execute_time_requests,\n            #   Metadata\n            num_prompt_tokens_requests=num_prompt_tokens_requests,\n            num_generation_tokens_requests=num_generation_tokens_requests,\n            max_num_generation_tokens_requests=\n            max_num_generation_tokens_requests,\n            n_requests=n_requests,\n            max_tokens_requests=max_tokens_requests,\n            finished_reason_requests=finished_reason_requests,\n            max_lora=str(max_lora_stat),\n            waiting_lora_adapters=list(waiting_lora_adapters.keys()),\n            running_lora_adapters=list(running_lora_adapters.keys()))\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_executor.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_executor.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_executor.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.model_executor.pin_lora(lora_id)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        return self.model_executor.add_prompt_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_executor.remove_prompt_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> List[int]:\n        return self.model_executor.list_prompt_adapters()\n\n    def check_health(self) -> None:\n        if self.tokenizer:\n            self.tokenizer.check_health()\n        self.model_executor.check_health()\n\n    def start_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes (MultiprocessingGPUExecutor)\n        if type(self.model_executor) == GPUExecutor:  # noqa: E721\n            self.model_executor.start_profile()\n        else:\n            self.model_executor._run_workers(\"start_profile\")\n\n    def stop_profile(self) -> None:\n        # using type instead of isinstance to check to avoid capturing\n        # inherited classes (MultiprocessingGPUExecutor)\n        if type(self.model_executor) == GPUExecutor:  # noqa: E721\n            self.model_executor.stop_profile()\n        else:\n            self.model_executor._run_workers(\"stop_profile\")\n\n    def is_tracing_enabled(self) -> bool:\n        return self.tracer is not None\n\n    def do_tracing(self,\n                   scheduler_outputs: SchedulerOutputs,\n                   finished_before: Optional[List[int]] = None) -> None:\n        if self.tracer is None:\n            return\n\n        for idx, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            # Skip double tracing when using async output proc\n            if finished_before and idx in finished_before:\n                continue\n\n            seq_group = scheduled_seq_group.seq_group\n            if seq_group.is_finished():\n                self.create_trace_span(seq_group)\n\n    def create_trace_span(self, seq_group: SequenceGroup) -> None:\n        if self.tracer is None or seq_group.sampling_params is None:\n            return\n        arrival_time_nano_seconds = int(seq_group.metrics.arrival_time * 1e9)\n\n        trace_context = extract_trace_context(seq_group.trace_headers)\n\n        with self.tracer.start_as_current_span(\n                \"llm_request\",\n                kind=SpanKind.SERVER,\n                context=trace_context,\n                start_time=arrival_time_nano_seconds) as seq_span:\n            metrics = seq_group.metrics\n            ttft = metrics.first_token_time - metrics.arrival_time\n            e2e_time = metrics.finished_time - metrics.arrival_time\n            # attribute names are based on\n            # https://github.com/open-telemetry/semantic-conventions/blob/main/docs/gen-ai/llm-spans.md\n            seq_span.set_attribute(SpanAttributes.LLM_RESPONSE_MODEL,\n                                   self.model_config.model)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_ID,\n                                   seq_group.request_id)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TEMPERATURE,\n                                   seq_group.sampling_params.temperature)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_TOP_P,\n                                   seq_group.sampling_params.top_p)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_MAX_TOKENS,\n                                   seq_group.sampling_params.max_tokens)\n            seq_span.set_attribute(SpanAttributes.LLM_REQUEST_N,\n                                   seq_group.sampling_params.n)\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_NUM_SEQUENCES,\n                                   seq_group.num_seqs())\n            seq_span.set_attribute(SpanAttributes.LLM_USAGE_PROMPT_TOKENS,\n                                   len(seq_group.prompt_token_ids))\n            seq_span.set_attribute(\n                SpanAttributes.LLM_USAGE_COMPLETION_TOKENS,\n                sum([\n                    seq.get_output_len()\n                    for seq in seq_group.get_finished_seqs()\n                ]))\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_TIME_IN_QUEUE,\n                                   metrics.time_in_queue)\n            seq_span.set_attribute(\n                SpanAttributes.LLM_LATENCY_TIME_TO_FIRST_TOKEN, ttft)\n            seq_span.set_attribute(SpanAttributes.LLM_LATENCY_E2E, e2e_time)\n            if metrics.scheduler_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_SCHEDULER,\n                    metrics.scheduler_time)\n            if metrics.model_forward_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_FORWARD,\n                    metrics.model_forward_time / 1000.0)\n            if metrics.model_execute_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.LLM_LATENCY_TIME_IN_MODEL_EXECUTE,\n                    metrics.model_execute_time)\n\n    def _validate_model_inputs(self, inputs: ProcessorInputs,\n                               lora_request: Optional[LoRARequest]):\n        if is_encoder_decoder_inputs(inputs):\n            # For encoder-decoder multimodal models, the max_prompt_len\n            # restricts the decoder prompt length\n            prompt_inputs = inputs[\"decoder\" if self.model_config.\n                                   is_multimodal_model else \"encoder\"]\n        else:\n            prompt_inputs = inputs\n\n        prompt_ids = SingletonInputsAdapter(prompt_inputs).prompt_token_ids\n\n        if prompt_ids is None or len(prompt_ids) == 0:\n            raise ValueError(\"Prompt cannot be empty\")\n\n        if self.model_config.is_multimodal_model:\n            max_prompt_len = self.model_config.max_model_len\n\n            if len(prompt_ids) > max_prompt_len:\n                raise ValueError(\n                    f\"The prompt (total length {len(prompt_ids)}) is too long \"\n                    f\"to fit into the model (context length {max_prompt_len}). \"\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n\n    def _build_logits_processors(\n            self, sampling_params: SamplingParams,\n            lora_request: Optional[LoRARequest]) -> SamplingParams:\n        \"\"\"Constructs logits processors based on the guided_decoding,\n        logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n        those fields and adds the constructed logits processors to the\n        logits_processors field. Returns the modified sampling params.\"\"\"\n\n        logits_processors = []\n\n        if (guided_decoding := sampling_params.guided_decoding) is not None:\n\n            logger.debug(\n                \"Building guided decoding logits processor in \"\n                \"LLMEngine. Params: %s\", guided_decoding)\n\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n            guided_decoding.backend = guided_decoding.backend or \\\n                self.decoding_config.guided_decoding_backend\n\n            processor = get_local_guided_decoding_logits_processor(\n                guided_params=guided_decoding, tokenizer=tokenizer)\n            if processor:\n                logits_processors.append(processor)\n\n            # Unset so this doesn't get passed down to the model\n            sampling_params.guided_decoding = None\n\n        if (sampling_params.logit_bias or sampling_params.allowed_token_ids):\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n\n            processors = get_openai_logits_processors(\n                logit_bias=sampling_params.logit_bias,\n                allowed_token_ids=sampling_params.allowed_token_ids,\n                tokenizer=tokenizer)\n            logits_processors.extend(processors)\n\n            # Unset so these don't get passed down to the model\n            sampling_params.logit_bias = None\n            sampling_params.allowed_token_ids = None\n\n        if len(sampling_params.bad_words) > 0:\n            tokenizer = self.get_tokenizer(lora_request)\n            processors = get_bad_words_logits_processors(\n                bad_words=sampling_params.bad_words, tokenizer=tokenizer)\n            logits_processors.extend(processors)\n\n        if logits_processors:\n            if sampling_params.logits_processors is None:\n                sampling_params.logits_processors = logits_processors\n            else:\n                sampling_params.logits_processors.extend(logits_processors)\n\n        return sampling_params\n",
      "diff": "diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex dd55aa281..af66b3070 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1,3 +1,4 @@\n+import copy\n import time\n from collections import Counter as collectionsCounter\n from collections import deque\n@@ -1024,9 +1025,9 @@ class LLMEngine:\n         This function updates num_computed_tokens for prompt sequences\n         when Multi-Step is enabled.\n \n-        seq_group: SequenceGroup to update the num_computed_tokens for. \n+        seq_group: SequenceGroup to update the num_computed_tokens for.\n         seq_group_meta: Metadata of the given SequenceGroup.\n-        is_first_step_output: Optional[bool] - \n+        is_first_step_output: Optional[bool] -\n             When available, is_first_step_output indicates if the appended\n             output token is the output of the first-step in multi-step.\n             A value of None indicates that outputs from all steps in\n@@ -2036,7 +2037,11 @@ class LLMEngine:\n \n         logits_processors = []\n \n-        if (guided_decoding := sampling_params.guided_decoding) is not None:\n+        if sampling_params.guided_decoding is not None:\n+            # Defensively copy sampling params since guided decoding logits\n+            # processors can have different state for each request\n+            sampling_params = copy.copy(sampling_params)\n+            guided_decoding = sampling_params.guided_decoding\n \n             logger.debug(\n                 \"Building guided decoding logits processor in \"\n@@ -2047,7 +2052,9 @@ class LLMEngine:\n                 self.decoding_config.guided_decoding_backend\n \n             processor = get_local_guided_decoding_logits_processor(\n-                guided_params=guided_decoding, tokenizer=tokenizer)\n+                guided_params=guided_decoding,\n+                tokenizer=tokenizer,\n+                model_config=self.model_config)\n             if processor:\n                 logits_processors.append(processor)",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/engine/multiprocessing/client.py",
      "old_content": "import asyncio\nimport copy\nimport pickle\nfrom contextlib import contextmanager, suppress\nfrom typing import (Any, AsyncGenerator, Dict, Iterator, List, Mapping,\n                    Optional, Union, cast, overload)\n\nimport cloudpickle\nimport psutil\nimport zmq\nimport zmq.asyncio\nfrom typing_extensions import deprecated\nfrom zmq import Frame  # type: ignore[attr-defined]\nfrom zmq.asyncio import Socket\n\nfrom vllm import PoolingParams\nfrom vllm.config import DecodingConfig, ModelConfig, VllmConfig\nfrom vllm.core.scheduler import SchedulerOutputs\nfrom vllm.engine.arg_utils import AsyncEngineArgs\n# yapf conflicts with isort for this block\n# yapf: disable\nfrom vllm.engine.async_llm_engine import (\n    build_guided_decoding_logits_processor_async)\nfrom vllm.engine.multiprocessing import (ENGINE_DEAD_ERROR, IPC_DATA_EXT,\n                                         IPC_HEALTH_EXT, IPC_INPUT_EXT,\n                                         IPC_OUTPUT_EXT, RPC_REQUEST_T,\n                                         VLLM_RPC_SUCCESS_STR, RPCAbortRequest,\n                                         RPCError, RPCProcessRequest,\n                                         RPCStartupRequest, RPCStartupResponse,\n                                         RPCUProfileRequest)\nfrom vllm.engine.protocol import EngineClient\n# yapf: enable\nfrom vllm.envs import VLLM_RPC_TIMEOUT\nfrom vllm.inputs import PromptType\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.outputs import PoolingRequestOutput, RequestOutput\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer_group import init_tokenizer_from_configs\nfrom vllm.utils import deprecate_kwargs\n\nlogger = init_logger(__name__)\n\n\nclass MQClientClosedError(Exception):\n    \"\"\"Exception class raised when the client is used post-close.\n\n    The client can be closed, which closes the ZMQ context. This normally\n    happens on server shutdown. In some cases, methods like abort and\n    do_log_stats will still be called and then try to open a socket, which\n    causes a ZMQError and creates a huge stack trace.\n    So, we throw this error such that we can suppress it.\n    \"\"\"\n\n\nclass MQLLMEngineClient(EngineClient):\n    \"\"\"A client wrapper for MQLLMEngine that conforms to the\n    EngineClient protocol.\n\n    MQLLMEngine and MQLLMEngineClient are intended to run in separate\n    processes communicating via zeromq ipc sockets.\n\n    The entrypoint to MQLLMEngineClient is through the generate()\n    method. On generate() MQLLMEngine does three things:\n        - Creates an asyncio output queue\n        - Sends a RPCGenerateRequest to the MQLLMEngine via zmq\n        - Pulls RequestOutputs from its queue and yields them\n\n    MQLLMEngine runs two background loops:\n        - output_loop: the output loop pulls List[RequestOutput]\n            from the MQLLMEngine via zmq (each list is the output\n            of one engine_step in the LLMEngine). It then parses\n            the list and pushes individual request_outputs into\n            the corresponding output_queue such that they can be\n            consumed by the .generate() method.\n        - health_loop: the health loop queries the health socket\n            every N seconds, confirming the engine is healthy\n    \"\"\"\n\n    def __init__(self, ipc_path: str, engine_config: VllmConfig,\n                 engine_pid: int):\n        self.context = zmq.asyncio.Context()\n        self._errored_with: Optional[BaseException] = None\n\n        # Get the configs.\n        self.model_config = engine_config.model_config\n        self.decoding_config = engine_config.decoding_config\n\n        # Create the tokenizer group.\n        self.tokenizer = init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=engine_config.scheduler_config,\n            parallel_config=engine_config.parallel_config,\n            enable_lora=bool(engine_config.lora_config),\n        )\n        self.input_preprocessor = InputPreprocessor(self.model_config,\n                                                    self.tokenizer)\n\n        # Send RPCGenerateRequest to the MQLLMEngine.\n        self.input_socket: Socket = self.context.socket(zmq.constants.PUSH)\n        self.input_socket.connect(f\"{ipc_path}{IPC_INPUT_EXT}\")\n\n        # Receive streams of RequestOutput from the MQLLMEngine.\n        self.output_socket: Socket = self.context.socket(zmq.constants.PULL)\n        self.output_socket.connect(f\"{ipc_path}{IPC_OUTPUT_EXT}\")\n\n        # IPC path for acking heartbeats.\n        self.heartbeat_socket: Socket = self.context.socket(zmq.constants.PULL)\n        self.heartbeat_socket.connect(f\"{ipc_path}{IPC_HEALTH_EXT}\")\n\n        # IPC path for the data socket.\n        self.data_ipc_path = f\"{ipc_path}{IPC_DATA_EXT}\"\n\n        # Stream for each individual request.\n        self.output_queues: Dict[str, asyncio.Queue] = {}\n\n        # Loop to handle output of the LLMEngine periodically.\n        # Started after the MQLLMEngine is ready so that we can\n        # build the Client in an executor to enable clean shutdown.\n        self.output_loop: Optional[asyncio.Task] = None\n\n        # Loop to check health of the LLMEngine periodically.\n        # Started after the MQLLMEngine is ready.\n        self.health_loop: Optional[asyncio.Task] = None\n        self._engine_process = psutil.Process(engine_pid)\n\n    @staticmethod\n    def is_unsupported_config(engine_args: AsyncEngineArgs):\n        # Pipeline parallel not yet supported\n        return engine_args.pipeline_parallel_size > 1\n\n    @contextmanager\n    def get_data_socket(self) -> Iterator[Socket]:\n        socket = self.context.socket(zmq.constants.DEALER)\n        try:\n            socket.connect(self.data_ipc_path)\n            yield socket\n        finally:\n            socket.close(linger=0)\n\n    async def run_heartbeat_loop(self, timeout: int):\n        \"\"\"Background loop that continually checks to ensure the engine process\n        is still alive.\n        \"\"\"\n        try:\n            while True:\n                # Check if the engine process is running:\n                if not self._engine_process.is_running() or (\n                        self._engine_process.status() == psutil.STATUS_ZOMBIE):\n                    # NB: is_running() returns True for zombies\n                    self._set_errored(\n                        RuntimeError(\n                            f\"Engine process (pid {self._engine_process.pid}) \"\n                            \"died.\"))\n                    break\n\n                if await self.heartbeat_socket.poll(timeout=timeout):\n                    # Heartbeat received- check the message\n                    await self._check_success(\n                        error_message=\"Heartbeat failed.\",\n                        socket=self.heartbeat_socket)\n\n                logger.debug(\"Heartbeat successful.\")\n\n        except asyncio.CancelledError:\n            logger.debug(\"Shutting down MQLLMEngineClient check health loop.\")\n\n        except psutil.NoSuchProcess:\n            self._set_errored(\n                RuntimeError(\n                    f\"Engine process (pid {self._engine_process.pid}) died.\"))\n\n        except Exception as e:\n            self._set_errored(e)\n\n    async def run_output_handler_loop(self):\n        \"\"\"Get RequestOutputs from Engine and stream to Request Queues\"\"\"\n\n        try:\n            while True:\n                # Poll, checking for ENGINE_DEAD\n                while await self.output_socket.poll(timeout=VLLM_RPC_TIMEOUT\n                                                    ) == 0:\n                    logger.debug(\"Waiting for output from MQLLMEngine.\")\n\n                    # If errored, alert all running requests.\n                    if self.errored:\n                        for queue_j in tuple(self.output_queues.values()):\n                            queue_j.put_nowait(\n                                ENGINE_DEAD_ERROR(self._errored_with))\n                        return\n\n                message: Frame = await self.output_socket.recv(copy=False)\n                request_outputs = pickle.loads(message.buffer)\n\n                is_error = isinstance(request_outputs,\n                                      (BaseException, RPCError))\n                if is_error:\n                    if isinstance(request_outputs, RPCError):\n                        rpc_error: RPCError = request_outputs\n                        request_id = rpc_error.request_id\n                        exception = rpc_error.exception\n                        is_engine_errored = rpc_error.is_engine_errored\n                    else:\n                        # MPLLMEngine should always return an RPCError to\n                        # the output_socket when an issue arises.\n                        # If we are here, we are in a bad state and\n                        # should shut down the server.\n                        error: BaseException = request_outputs\n                        logger.error(\n                            \"Received Exception %s rather than RPCError from \"\n                            \"MPLLMEngine. This should never happen.\", error)\n                        request_id = None\n                        exception = error\n                        is_engine_errored = True\n\n                    # Set to error state only on engine critical error\n                    # (and record only the first one)\n                    if is_engine_errored and not self._errored_with:\n                        self._errored_with = exception\n                        # If engine is errored, no matter the type of exception\n                        # it will no longer be able to receive new requests,\n                        # therefore we have to inform that the current\n                        # processed requests failed as well. Send back a dead\n                        # engine error give this feedback and also give a\n                        # 'hint' to the server to shutdown next.\n                        exception = self.dead_error\n\n                    if request_id is None:\n                        # If request_id is None, then the engine raised an\n                        # exception for a batch, and we may not know the\n                        # request that caused it, neither if it was actually\n                        # caused by any of them (e.g. CUDA OOM). Therefore we\n                        # broadcast the same exception for all requests.\n                        for queue_i in tuple(self.output_queues.values()):\n                            queue_i.put_nowait(exception)\n                    else:\n                        queue = self.output_queues.get(request_id)\n                        if queue is not None:\n                            queue.put_nowait(exception)\n                else:\n                    # Put each output into the appropriate steam.\n                    for request_output in request_outputs:\n                        queue = self.output_queues.get(\n                            request_output.request_id)\n                        if queue is not None:\n                            queue.put_nowait(request_output)\n\n        except asyncio.CancelledError:\n            logger.debug(\"Shutting down MQLLMEngineClient output handler.\")\n\n    async def setup(self):\n        \"\"\"Setup the client before it starts sending server requests.\"\"\"\n\n        # Start output_loop\n        self.output_loop = asyncio.create_task(self.run_output_handler_loop())\n\n        with self.get_data_socket() as socket:\n            # Wait until server is ready.\n            response = await self._wait_for_server_rpc(socket)\n\n            self.tracing_flag = response.tracing_enabled\n\n            # Start health_loop.\n            self.health_loop = asyncio.create_task(\n                self.run_heartbeat_loop(timeout=VLLM_RPC_TIMEOUT))\n\n    def close(self):\n        \"\"\"Destroy the ZeroMQ Context.\"\"\"\n        # Close all sockets and terminate the context.\n        self.context.destroy(linger=0)\n\n        # Cancel background tasks.\n        if self.health_loop is not None:\n            self.health_loop.cancel()\n        if self.output_loop is not None:\n            self.output_loop.cancel()\n\n    def _set_errored(self, e: BaseException):\n        logger.exception(repr(e))\n        if self._errored_with is None:\n            self._errored_with = e\n\n    @staticmethod\n    async def _send_get_data_rpc_request(request: RPCStartupRequest,\n                                         expected_type: Any,\n                                         error_message: str,\n                                         socket: Socket) -> Any:\n        \"\"\"Send an RPC request that is expecting data back.\"\"\"\n\n        # Ping RPCServer with a request.\n        await socket.send_multipart((pickle.dumps(request), ), copy=False)\n\n        # Make sure the server responds in time.\n        if await socket.poll(timeout=VLLM_RPC_TIMEOUT) == 0:\n            raise TimeoutError(\"RPCServer didn't reply within \"\n                               f\"{VLLM_RPC_TIMEOUT} ms\")\n\n        # Await the data from the Server.\n        frame = await socket.recv(copy=False)\n        data = pickle.loads(frame.buffer)\n\n        if isinstance(data, BaseException):\n            raise data\n        elif not isinstance(data, expected_type):\n            raise ValueError(error_message)\n\n        return data\n\n    @staticmethod\n    async def _send_one_way_rpc_request(request: RPC_REQUEST_T,\n                                        socket: Socket):\n        \"\"\"Send one-way RPC request to trigger an action.\"\"\"\n\n        if socket.closed:\n            raise MQClientClosedError()\n\n        await socket.send_multipart((pickle.dumps(request), ))\n\n    async def _await_ack(self, error_message: str, socket: Socket):\n        \"\"\"Await acknowledgement that a request succeeded.\"\"\"\n\n        if socket.closed:\n            raise MQClientClosedError()\n\n        if await socket.poll(timeout=VLLM_RPC_TIMEOUT) == 0:\n            raise TimeoutError(\"MQLLMEngine didn't reply within \"\n                               f\"{VLLM_RPC_TIMEOUT}ms\")\n\n        await self._check_success(error_message, socket)\n\n    @staticmethod\n    async def _check_success(error_message: str, socket: Socket):\n        \"\"\"Confirm that socket has a VLLM_RPC_SUCCESS_STR message\"\"\"\n\n        if socket.closed:\n            raise MQClientClosedError()\n\n        frame = await socket.recv(copy=False)\n        response = pickle.loads(frame.buffer)\n\n        # Raise error if unsuccessful\n        if isinstance(response, BaseException):\n            raise response\n        elif (not isinstance(response, str)\n              or response != VLLM_RPC_SUCCESS_STR):\n            raise ValueError(error_message)\n\n    async def get_input_preprocessor(self) -> InputPreprocessor:\n        return self.input_preprocessor\n\n    async def get_tokenizer(self, lora_request: Optional[LoRARequest] = None):\n        return await self.tokenizer.get_lora_tokenizer_async(lora_request)\n\n    async def get_decoding_config(self) -> DecodingConfig:\n        return self.decoding_config\n\n    async def get_model_config(self) -> ModelConfig:\n        return self.model_config\n\n    async def is_tracing_enabled(self) -> bool:\n        return self.tracing_flag\n\n    async def _wait_for_server_rpc(self, socket: Socket) -> RPCStartupResponse:\n        \"\"\"Wait for the RPCServer to start up.\"\"\"\n\n        return await self._send_get_data_rpc_request(\n            request=RPCStartupRequest.IS_SERVER_READY,\n            expected_type=RPCStartupResponse,\n            error_message=\"Unable to start RPC Server\",\n            socket=socket)\n\n    async def abort(self, request_id: str):\n        \"\"\"Send an ABORT_REQUEST signal to the RPC Server\"\"\"\n\n        with suppress(MQClientClosedError):\n            await self._send_one_way_rpc_request(\n                request=RPCAbortRequest(request_id), socket=self.input_socket)\n\n    async def do_log_stats(\n        self,\n        scheduler_outputs: Optional[SchedulerOutputs] = None,\n        model_output: Optional[List[SamplerOutput]] = None,\n    ) -> None:\n        \"\"\"\n        Ignore do_log_stats (handled on MQLLMEngine polling)\n        \"\"\"\n        pass\n\n    async def check_health(self):\n        \"\"\"\n        The check health loop probes the health status of the\n        Engine's health every N seconds and sets _errored_with\n        if the engine is unhealthy.\n        \"\"\"\n        if self._errored_with is not None:\n            raise self._errored_with\n\n    @property\n    def is_running(self) -> bool:\n        return not self.errored\n\n    @property\n    def is_stopped(self) -> bool:\n        return self.errored\n\n    @property\n    def errored(self) -> bool:\n        return self._errored_with is not None\n\n    @property\n    def dead_error(self) -> BaseException:\n        return ENGINE_DEAD_ERROR(self._errored_with)\n\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    def generate(\n        self,\n        *,\n        inputs: PromptType,\n        sampling_params: SamplingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        ...\n\n    @overload\n    def generate(\n        self,\n        prompt: PromptType,\n        sampling_params: SamplingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[RequestOutput, None]:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    def generate(\n        self,\n        prompt: Optional[PromptType] = None,\n        sampling_params: Optional[SamplingParams] = None,\n        request_id: Optional[str] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n        *,\n        inputs: Optional[PromptType] = None  # DEPRECATED\n    ) -> AsyncGenerator[RequestOutput, None]:\n        \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n            prompt_adapter_request: Prompt Adapter request to use\n                                            for generation, if any.\n            priority: Priority of the request (lower means earlier handling). \n                Any priority other than 0 will lead to an error if the \n                scheduling policy is not \"priority\".\n        \"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert (prompt is not None and sampling_params is not None\n                and request_id is not None)\n\n        return self._process_request(prompt, sampling_params, request_id,\n                                     lora_request, trace_headers,\n                                     prompt_adapter_request, priority)\n\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    def encode(\n        self,\n        *,\n        inputs: PromptType,\n        pooling_params: PoolingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[PoolingRequestOutput, None]:\n        ...\n\n    @overload\n    def encode(\n        self,\n        prompt: PromptType,\n        pooling_params: PoolingParams,\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> AsyncGenerator[PoolingRequestOutput, None]:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    def encode(\n        self,\n        prompt: Optional[PromptType] = None,\n        pooling_params: Optional[PoolingParams] = None,\n        request_id: Optional[str] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n        *,\n        inputs: Optional[PromptType] = None  # DEPRECATED\n    ) -> AsyncGenerator[PoolingRequestOutput, None]:\n        \"\"\"Generate outputs for a request from an embedding model.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt to the LLM. See :class:`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            pooling_params: The pooling parameters of the request.\n            request_id: The unique id of the request.\n            lora_request: LoRA request to use for generation, if any.\n            trace_headers: OpenTelemetry trace headers.\n\n        Yields:\n            The output `PoolingRequestOutput` objects from the LLMEngine\n            for the request.\n        \"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert (prompt is not None and pooling_params is not None\n                and request_id is not None)\n\n        return cast(\n            AsyncGenerator[PoolingRequestOutput, None],\n            self._process_request(prompt,\n                                  pooling_params,\n                                  request_id,\n                                  lora_request,\n                                  trace_headers,\n                                  priority=priority))\n\n    async def _process_request(\n        self,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        request_id: str,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> Union[AsyncGenerator[RequestOutput, None], AsyncGenerator[\n            PoolingRequestOutput, None]]:\n        \"\"\"Send an RPCGenerateRequest to the RPCServer and stream responses.\"\"\"\n\n        # If already dead, error out.\n        if self._errored_with is not None:\n            raise ENGINE_DEAD_ERROR(self._errored_with)\n\n        # Constructing guided decoding logits processors is expensive, so we do\n        # it here to avoid contending with cpu resources and the GIL on the\n        # backend process.\n        if isinstance(params, SamplingParams) and \\\n            params.guided_decoding is not None:\n            params = await \\\n                build_guided_decoding_logits_processor_async(\n                    sampling_params=params,\n                    tokenizer=await self.get_tokenizer(lora_request),\n                    default_guided_backend=(self.decoding_config.guided_decoding_backend\n                        if self.decoding_config\n                        else DecodingConfig.guided_decoding_backend),\n                )\n\n        # 1) Create output queue for this requests.\n        queue: asyncio.Queue[Union[RequestOutput,\n                                   BaseException]] = asyncio.Queue()\n        self.output_queues[request_id] = queue\n\n        try:\n            # 2) Detach logits processors so that they can be pickled\n            # separately (may require cloudpickle which is slower)\n            if isinstance(params, SamplingParams) and params.logits_processors:\n                # Defensive shallow copy\n                params = copy.copy(params)\n                logits_processors = params.logits_processors\n                params.logits_processors = None\n                lp_bytes = cloudpickle.dumps(logits_processors)\n            else:\n                lp_bytes = None\n\n            request_bytes = pickle.dumps(\n                RPCProcessRequest(\n                    prompt=prompt,\n                    params=params,\n                    request_id=request_id,\n                    lora_request=lora_request,\n                    trace_headers=trace_headers,\n                    prompt_adapter_request=prompt_adapter_request,\n                    priority=priority,\n                ))\n\n            # 3) Send the RPCGenerateRequest to the MQLLMEngine.\n            parts = (request_bytes,\n                     lp_bytes) if lp_bytes else (request_bytes, )\n            await self.input_socket.send_multipart(parts, copy=False)\n\n            # 4) Stream the RequestOutputs from the output queue. Note\n            # that the output_loop pushes RequestOutput objects to this\n            # queue after pulling them from the zmq socket.\n            finished = False\n            try:\n                while not finished:\n                    request_output = await queue.get()\n\n                    if isinstance(request_output, BaseException):\n                        raise request_output\n\n                    finished = request_output.finished\n                    yield request_output\n            finally:\n                # Request was canceled by the client.\n                if not finished and not self.errored:\n                    await self.abort(request_id)\n        finally:\n            self.output_queues.pop(request_id)\n\n    async def start_profile(self) -> None:\n        \"\"\"Start profiling the engine\"\"\"\n\n        await self._send_one_way_rpc_request(\n            request=RPCUProfileRequest.START_PROFILE, socket=self.input_socket)\n\n    async def stop_profile(self) -> None:\n        \"\"\"Stop profiling the engine\"\"\"\n\n        await self._send_one_way_rpc_request(\n            request=RPCUProfileRequest.STOP_PROFILE, socket=self.input_socket)\n",
      "diff": "diff --git a/vllm/engine/multiprocessing/client.py b/vllm/engine/multiprocessing/client.py\nindex 8383e774d..d21136c03 100644\n--- a/vllm/engine/multiprocessing/client.py\n+++ b/vllm/engine/multiprocessing/client.py\n@@ -474,8 +474,8 @@ class MQLLMEngineClient(EngineClient):\n             trace_headers: OpenTelemetry trace headers.\n             prompt_adapter_request: Prompt Adapter request to use\n                                             for generation, if any.\n-            priority: Priority of the request (lower means earlier handling). \n-                Any priority other than 0 will lead to an error if the \n+            priority: Priority of the request (lower means earlier handling).\n+                Any priority other than 0 will lead to an error if the\n                 scheduling policy is not \"priority\".\n         \"\"\"\n         if inputs is not None:\n@@ -589,6 +589,7 @@ class MQLLMEngineClient(EngineClient):\n                     default_guided_backend=(self.decoding_config.guided_decoding_backend\n                         if self.decoding_config\n                         else DecodingConfig.guided_decoding_backend),\n+                    model_config=self.model_config\n                 )\n \n         # 1) Create output queue for this requests.",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/model_executor/guided_decoding/__init__.py",
      "old_content": "from typing import Optional\n\nfrom vllm.logits_process import LogitsProcessor\nfrom vllm.sampling_params import GuidedDecodingParams\n\n\nasync def get_guided_decoding_logits_processor(\n        guided_params: GuidedDecodingParams,\n        tokenizer) -> Optional[LogitsProcessor]:\n    # CFG grammar not supported by LMFE, so we use outlines instead\n    if guided_params.backend == 'outlines' or guided_params.grammar:\n        # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n        from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n            get_outlines_guided_decoding_logits_processor)\n        return await get_outlines_guided_decoding_logits_processor(\n            guided_params, tokenizer)\n    if guided_params.backend == 'lm-format-enforcer':\n        from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n            get_local_lm_format_enforcer_guided_decoding_logits_processor)\n        return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n            guided_params, tokenizer)\n\n    raise ValueError(\n        f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n\n\ndef get_local_guided_decoding_logits_processor(\n        guided_params: GuidedDecodingParams,\n        tokenizer) -> Optional[LogitsProcessor]:\n    # CFG grammar not supported by LMFE, so we use outlines instead\n    if guided_params.backend == 'outlines' or guided_params.grammar:\n        # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n        from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n            get_local_outlines_guided_decoding_logits_processor)\n        return get_local_outlines_guided_decoding_logits_processor(\n            guided_params, tokenizer)\n    if guided_params.backend == 'lm-format-enforcer':\n        from vllm.model_executor.guided_decoding.lm_format_enforcer_decoding import (  # noqa\n            get_local_lm_format_enforcer_guided_decoding_logits_processor)\n        return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n            guided_params, tokenizer)\n\n    raise ValueError(\n        f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n",
      "diff": "diff --git a/vllm/model_executor/guided_decoding/__init__.py b/vllm/model_executor/guided_decoding/__init__.py\nindex d7b67425f..23c31fcfd 100644\n--- a/vllm/model_executor/guided_decoding/__init__.py\n+++ b/vllm/model_executor/guided_decoding/__init__.py\n@@ -1,14 +1,54 @@\n-from typing import Optional\n+from __future__ import annotations\n \n-from vllm.logits_process import LogitsProcessor\n-from vllm.sampling_params import GuidedDecodingParams\n+from typing import TYPE_CHECKING\n+\n+from vllm.logger import init_logger\n+\n+if TYPE_CHECKING:\n+    from transformers import PreTrainedTokenizer\n+\n+    from vllm.config import ModelConfig\n+    from vllm.logits_process import LogitsProcessor\n+    from vllm.sampling_params import GuidedDecodingParams\n+\n+logger = init_logger(__name__)\n+\n+\n+def maybe_backend_fallback(\n+        guided_params: GuidedDecodingParams) -> GuidedDecodingParams:\n+    # lm-format-enforce doesn't support grammar, fallback to xgrammar\n+    if (guided_params.backend == \"lm-format-enforcer\"\n+            and guided_params.grammar is not None):\n+        logger.warning(\n+            \"lm-format-enforcer does not support grammar guided decoding. \"\n+            \"Falling back to use xgrammar instead.\")\n+        guided_params.backend = \"xgrammar\"\n+\n+    if guided_params.backend == \"xgrammar\":\n+        # xgrammar doesn't support regex or choice, fallback to outlines\n+        if guided_params.regex is not None or guided_params.choice is not None:\n+            logger.warning(\n+                \"xgrammar only supports json or grammar guided decoding. \"\n+                \"Falling back to use outlines instead.\")\n+            guided_params.backend = \"outlines\"\n+\n+        # xgrammar only supports EBNF grammars and uses the GBNF format\n+        # https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md\n+        elif (guided_params.grammar is not None\n+              and \"::=\" not in guided_params.grammar):\n+            logger.warning(\"xgrammar only supports EBNF grammars. \"\n+                           \"Falling back to use outlines instead.\")\n+            guided_params.backend = \"outlines\"\n+\n+    return guided_params\n \n \n async def get_guided_decoding_logits_processor(\n-        guided_params: GuidedDecodingParams,\n-        tokenizer) -> Optional[LogitsProcessor]:\n+        guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig) -> LogitsProcessor | None:\n+    guided_params = maybe_backend_fallback(guided_params)\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_outlines_guided_decoding_logits_processor)\n@@ -19,17 +59,23 @@ async def get_guided_decoding_logits_processor(\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n+    if guided_params.backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_local_xgrammar_guided_decoding_logits_processor)\n+        return get_local_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer, model_config)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'\")\n \n \n def get_local_guided_decoding_logits_processor(\n-        guided_params: GuidedDecodingParams,\n-        tokenizer) -> Optional[LogitsProcessor]:\n+        guided_params: GuidedDecodingParams, tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig) -> LogitsProcessor | None:\n+    guided_params = maybe_backend_fallback(guided_params)\n     # CFG grammar not supported by LMFE, so we use outlines instead\n-    if guided_params.backend == 'outlines' or guided_params.grammar:\n+    if guided_params.backend == 'outlines':\n         # NOTE: lazy import outlines to avoid https://github.com/vllm-project/vllm/issues/4193\n         from vllm.model_executor.guided_decoding.outlines_decoding import (  # noqa\n             get_local_outlines_guided_decoding_logits_processor)\n@@ -40,7 +86,12 @@ def get_local_guided_decoding_logits_processor(\n             get_local_lm_format_enforcer_guided_decoding_logits_processor)\n         return get_local_lm_format_enforcer_guided_decoding_logits_processor(\n             guided_params, tokenizer)\n+    if guided_params.backend == 'xgrammar':\n+        from vllm.model_executor.guided_decoding.xgrammar_decoding import (  # noqa\n+            get_local_xgrammar_guided_decoding_logits_processor)\n+        return get_local_xgrammar_guided_decoding_logits_processor(\n+            guided_params, tokenizer, model_config)\n \n     raise ValueError(\n         f\"Unknown guided decoding backend '{guided_params.backend}'. \"\n-        \"Must be one of 'outlines, 'lm-format-enforcer'\")\n+        \"Must be one of 'outlines, 'lm-format-enforcer', 'xgrammar'\")",
      "change_type": "modified",
      "lines_added": 63,
      "lines_removed": 12
    },
    {
      "file_path": "vllm/model_executor/guided_decoding/xgrammar_decoding.py",
      "old_content": "",
      "diff": "diff --git a/vllm/model_executor/guided_decoding/xgrammar_decoding.py b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\nnew file mode 100644\nindex 000000000..8287cd6cf\n--- /dev/null\n+++ b/vllm/model_executor/guided_decoding/xgrammar_decoding.py\n@@ -0,0 +1,251 @@\n+# noqa: UP007\n+from __future__ import annotations\n+\n+import json\n+from dataclasses import dataclass, field\n+from typing import TYPE_CHECKING, Any, NamedTuple\n+\n+import torch\n+from transformers import PreTrainedTokenizerFast\n+\n+try:\n+    import xgrammar as xgr\n+    from xgrammar.base import _core as xgr_core\n+except ImportError:\n+    pass\n+\n+if TYPE_CHECKING:\n+    from transformers import PreTrainedTokenizer\n+\n+    from vllm.config import ModelConfig\n+    from vllm.sampling_params import GuidedDecodingParams\n+\n+\n+# TODO: passing batch size to max threads here\n+def get_local_xgrammar_guided_decoding_logits_processor(\n+        guided_params: GuidedDecodingParams,\n+        tokenizer: PreTrainedTokenizer,\n+        model_config: ModelConfig,\n+        max_threads: int = 8):\n+    config = GrammarConfig.from_guided_params(guided_params=guided_params,\n+                                              model_config=model_config,\n+                                              tokenizer=tokenizer,\n+                                              max_threads=max_threads)\n+    return XGrammarLogitsProcessor(config)\n+\n+\n+class TokenizerData(NamedTuple):\n+    \"\"\"Immutable container for cached tokenizer data.\"\"\"\n+    encoded_vocab: list[str]\n+    stop_token_ids: list[int] | None\n+    backend_str: str\n+\n+\n+class TokenizerDataCache:\n+    \"\"\"Cache manager for tokenizer data to avoid repeated processing.\"\"\"\n+    _cache: dict[int, TokenizerData] = {}\n+\n+    @classmethod\n+    def get_tokenizer_data(cls,\n+                           tokenizer: PreTrainedTokenizer) -> TokenizerData:\n+        tokenizer_hash = hash(tokenizer)\n+\n+        if tokenizer_hash not in cls._cache:\n+            # Vendored from xgrammar logic since we cannot pickle the tokenizer\n+            # https://github.com/mlc-ai/xgrammar/blob/d77c0a0173ef14779c918e3be7966ba852f7910f/python/xgrammar/tokenizer_info.py#L98 # noqa: E501\n+            try:\n+                encoded_vocab = [\n+                    token for token, _ in sorted(tokenizer.get_vocab().items(),\n+                                                 key=lambda x: x[1])\n+                ]\n+            except AttributeError as e:\n+                raise ValueError(\n+                    f\"Cannot get the vocabulary of the tokenizer \"\n+                    f\"{type(tokenizer)}. The tokenizer should have a \"\n+                    \"get_vocab method.\") from e\n+\n+            stop_token_ids = None\n+            backend_str = xgr.VocabType.RAW\n+            if isinstance(tokenizer, PreTrainedTokenizerFast):\n+                backend_str = tokenizer.backend_tokenizer.to_str()\n+                if stop_token_ids is None and hasattr(\n+                        tokenizer,\n+                        \"eos_token_id\") and tokenizer.eos_token_id is not None:\n+                    stop_token_ids = [tokenizer.eos_token_id]\n+\n+            cls._cache[tokenizer_hash] = TokenizerData(\n+                encoded_vocab=encoded_vocab,\n+                stop_token_ids=stop_token_ids,\n+                backend_str=backend_str)\n+\n+        return cls._cache[tokenizer_hash]\n+\n+\n+class GrammarCompilerCache:\n+    \"\"\"\n+    Cache for GrammarCompiler instances based on tokenizer.\n+\n+    This cache reduces the overhead of creating new compiler instances when\n+    using the same tokenizer configuration.\n+    \"\"\"\n+    _cache: dict[str, xgr.GrammarCompiler] = {}\n+\n+    @classmethod\n+    def get_compiler(cls, config: GrammarConfig) -> xgr.GrammarCompiler:\n+        cache_key = str(config.tokenizer_hash)\n+\n+        if cache_key not in cls._cache:\n+            assert config.encoded_vocab is not None\n+            tokenizer_info = xgr.TokenizerInfo._create_from_handle(\n+                xgr_core.TokenizerInfo.from_huggingface(\n+                    config.encoded_vocab, config.backend_str,\n+                    config.vocab_size, config.stop_token_ids))\n+            cls._cache[cache_key] = xgr.GrammarCompiler(\n+                tokenizer_info, max_threads=config.max_threads)\n+\n+        return cls._cache[cache_key]\n+\n+\n+@dataclass\n+class GrammarConfig:\n+    \"\"\"Serializable configuration for grammar compilation\"\"\"\n+    tokenizer_hash: int\n+    vocab_size: int\n+    json_str: str | None = None\n+    grammar_str: str | None = None\n+    json_object: bool | None = None\n+    max_threads: int = 8\n+    # Only populated if tokenizer_hash not in cache\n+    encoded_vocab: list[str] | None = None\n+    stop_token_ids: list[int] | None = None\n+    backend_str: str | None = None\n+\n+    @classmethod\n+    def from_guided_params(cls,\n+                           guided_params: GuidedDecodingParams,\n+                           model_config: ModelConfig,\n+                           tokenizer: PreTrainedTokenizer,\n+                           max_threads: int = 8) -> GrammarConfig:\n+\n+        tokenizer_hash = hash(tokenizer)\n+        # Only get tokenizer data if not already cached\n+        if tokenizer_hash in TokenizerDataCache._cache:\n+            encoded_vocab = None\n+            stop_token_ids = None\n+            backend_str = None\n+        else:\n+            tokenizer_data = TokenizerDataCache.get_tokenizer_data(tokenizer)\n+            encoded_vocab = tokenizer_data.encoded_vocab\n+            stop_token_ids = tokenizer_data.stop_token_ids\n+            backend_str = tokenizer_data.backend_str\n+\n+        if guided_params.json:\n+            if not isinstance(guided_params.json, str):\n+                json_str = json.dumps(guided_params.json)\n+            else:\n+                json_str = guided_params.json\n+            return cls(json_str=json_str,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        elif guided_params.grammar:\n+            return cls(grammar_str=guided_params.grammar,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        elif guided_params.json_object:\n+            return cls(json_object=True,\n+                       vocab_size=model_config.hf_config.vocab_size,\n+                       encoded_vocab=encoded_vocab,\n+                       stop_token_ids=stop_token_ids,\n+                       backend_str=backend_str,\n+                       tokenizer_hash=tokenizer_hash,\n+                       max_threads=max_threads)\n+        else:\n+            raise ValueError(\n+                \"Currently only support JSON and EBNF grammar mode for xgrammar\"\n+            )\n+\n+\n+@dataclass\n+class XGrammarLogitsProcessor:\n+    \"\"\"Wrapper class to support pickle protocol\"\"\"\n+    config: GrammarConfig\n+\n+    ctx: xgr.CompiledGrammar | None = None\n+    token_bitmask: torch.Tensor = None  # type: ignore[assignment]\n+    matchers: list[xgr.GrammarMatcher] = field(default_factory=list)\n+    batch_size: int = field(default=1)\n+    prefilled: bool = field(default=False)\n+\n+    def __getstate__(self) -> dict[str, Any]:\n+        return {'config': self.config}\n+\n+    def __setstate__(self, state: dict[str, Any]):\n+        self.config = state['config']\n+\n+        self.ctx = None\n+        self.matchers = []\n+        self.batch_size = 1\n+        self.token_bitmask = None  # type: ignore[assignment]\n+        self.prefilled = False\n+\n+    def _ensure_ctx(self):\n+        \"\"\"Lazily initialize the processor in the worker process\"\"\"\n+        if self.ctx is None:\n+            compiler = GrammarCompilerCache.get_compiler(self.config)\n+            if self.config.json_str is not None:\n+                self.ctx = compiler.compile_json_schema(self.config.json_str)\n+            elif self.config.grammar_str is not None:\n+                self.ctx = compiler.compile_grammar(self.config.grammar_str)\n+            elif self.config.json_object:\n+                self.ctx = compiler.compile_builtin_json_grammar()\n+            else:\n+                raise ValueError(\n+                    \"Invalid configuration for xgrammar logits processor\")\n+\n+    def __call__(self, input_ids: list[int],\n+                 scores: torch.Tensor) -> torch.Tensor:\n+        if self.ctx is None:\n+            self._ensure_ctx()\n+\n+        if len(self.matchers) == 0:\n+            self.matchers = [\n+                xgr.GrammarMatcher(self.ctx) for _ in range(self.batch_size)\n+            ]\n+            self.token_bitmask = xgr.allocate_token_bitmask(\n+                self.batch_size, self.config.vocab_size)\n+\n+        if not self.prefilled:\n+            # Have not sampled a token yet\n+            self.prefilled = True\n+        else:\n+            for i, matcher in enumerate(self.matchers):\n+                if not matcher.is_terminated():\n+                    sampled_token = input_ids[-1]\n+                    assert self.matchers[i].accept_token(sampled_token)\n+\n+        for i, matcher in enumerate(self.matchers):\n+            if not matcher.is_terminated():\n+                # @ubospica: ideally, fill_next_token_bitmask should be\n+                # parallelized with model decoding\n+                # See https://github.com/vllm-project/vllm/pull/10785/files#r1864278303\n+                matcher.fill_next_token_bitmask(self.token_bitmask, i)\n+\n+        # token_bitmask is a CPU tensor for use with accept_token and\n+        # fill_next_token_bitmask so we move it to the device of scores\n+        device_type = scores.device.type\n+        if device_type != \"cuda\":\n+            scores = scores.to(\"cpu\")\n+        xgr.apply_token_bitmask_inplace(scores,\n+                                        self.token_bitmask.to(scores.device))\n+        if device_type != \"cuda\":\n+            scores = scores.to(device_type)\n+\n+        return scores",
      "change_type": "added",
      "lines_added": 252,
      "lines_removed": 1
    }
  ],
  "affected_apis": [
    "vllm.model_executor.guided_decoding.get_guided_decoding_logits_processor",
    "vllm.model_executor.guided_decoding.get_local_guided_decoding_logits_processor",
    "vllm.model_executor.guided_decoding.xgrammar_decoding.get_local_xgrammar_guided_decoding_logits_processor",
    "vllm.config.DecodingConfig.guided_decoding_backend",
    "vllm.engine.arg_utils.EngineArgs.guided_decoding_backend"
  ],
  "summary": {
    "total_files": 11,
    "files_added": 1,
    "files_deleted": 0,
    "files_modified": 10
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_async_llm, test_llm_engine)",
    "is_benchmark_actually_there": "",
    "sample_clues": "__init__, arg_utils, async"
  }
}