{
  "commit_hash": "e3580537a41a46b0f3cd750b86b633c1857a8c90",
  "parent_hash": "f508e03e7f2d8aed897d8843e1ed1668e5c4ad7a",
  "message": "[Performance] Enable chunked prefill and prefix caching together (#7753)",
  "author": "Cody Yu <hao.yu.cody@gmail.com>",
  "date": "2024-08-28 00:36:31 -0700",
  "files_changed": [
    {
      "file_path": "tests/basic_correctness/test_chunked_prefill.py",
      "old_content": "\"\"\"Compare the outputs of HF and vLLM when using greedy sampling.\n\nIt tests chunked prefill. Chunked prefill can be enabled by\nenable_chunked_prefill=True. If prefill size exceeds max_num_batched_tokens,\nprefill requests are chunked.\n\nRun `pytest tests/models/test_chunked_prefill.py`.\n\"\"\"\n\nimport pytest\n\nfrom ..models.utils import check_logprobs_close, check_outputs_equal\n\nMODELS = [\n    \"facebook/opt-125m\",\n    \"meta-llama/Llama-2-7b-hf\",\n]\nE5M2_KV_MODELS = [\n    \"facebook/opt-125m\",\n    \"meta-llama/Llama-2-7b-chat-hf\",\n]\nE4M3_KV_MODELS = [\n    \"meta-llama/Llama-2-7b-chat-hf\", \"nm-testing/Qwen2-1.5B-Instruct-FP8-K-V\",\n    \"nm-testing/TinyLlama-1.1B-compressed-tensors-kv-cache-scheme\"\n]\nKV_CACHE_QUANTIZATION_PATHS = {\n    \"meta-llama/Llama-2-7b-chat-hf\":\n    \"./tests/fp8_kv/llama2-7b-fp8-kv/kv_cache_scales.json\"\n}\n\n\n@pytest.mark.parametrize(\"model\", MODELS)\n@pytest.mark.parametrize(\"dtype\", [\"half\"])\n@pytest.mark.parametrize(\"max_tokens\", [32])\n@pytest.mark.parametrize(\"chunked_prefill_token_size\", [1, 4, 16])\n@pytest.mark.parametrize(\"enforce_eager\", [False, True])\n# NOTE: Increasing this in this suite will fail CI because we currently cannot\n# reset distributed env properly. Use a value > 1 just when you test.\n@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\ndef test_models(\n    hf_runner,\n    vllm_runner,\n    example_prompts,\n    model: str,\n    dtype: str,\n    max_tokens: int,\n    chunked_prefill_token_size: int,\n    enforce_eager: bool,\n    tensor_parallel_size: int,\n) -> None:\n    \"\"\"\n    Checks exact match decode between huggingface model and vllm runner with\n    chunked prefill.\n    \"\"\"\n    max_num_seqs = chunked_prefill_token_size\n    max_num_batched_tokens = chunked_prefill_token_size\n\n    with hf_runner(model, dtype=dtype) as hf_model:\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\n\n    with vllm_runner(\n            model,\n            dtype=dtype,\n            max_num_batched_tokens=max_num_batched_tokens,\n            enable_chunked_prefill=True,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n    ) as vllm_model:\n        vllm_outputs = vllm_model.generate_greedy(example_prompts, max_tokens)\n\n    check_outputs_equal(\n        outputs_0_lst=hf_outputs,\n        outputs_1_lst=vllm_outputs,\n        name_0=\"hf\",\n        name_1=\"vllm\",\n    )\n\n\n@pytest.mark.parametrize(\"kv_cache_dtype,model\",\n                         [(\"fp8_e5m2\", m)\n                          for m in E5M2_KV_MODELS] + [(\"fp8_e4m3\", m)\n                                                      for m in E4M3_KV_MODELS])\n# Due to low-precision numerical divergence, we only test logprob of 4 tokens\n@pytest.mark.parametrize(\"max_tokens\", [4])\n@pytest.mark.parametrize(\"chunked_prefill_token_size\", [4, 16])\n@pytest.mark.parametrize(\"enforce_eager\", [False, True])\n# NOTE: Increasing this in this suite will fail CI because we currently cannot\n# reset distributed env properly. Use a value > 1 just when you test.\n@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n# Due to low-precision numerical divergence, this test is too sensitive to\n# the async postprocessor\n@pytest.mark.parametrize(\"disable_async_output_proc\", [True])\ndef test_models_with_fp8_kv_cache(\n    vllm_runner,\n    example_prompts,\n    kv_cache_dtype: str,\n    model: str,\n    max_tokens: int,\n    chunked_prefill_token_size: int,\n    enforce_eager: bool,\n    tensor_parallel_size: int,\n    disable_async_output_proc: bool,\n) -> None:\n    \"\"\"\n    Only checks log probs match between chunked-prefill and\n    non-chunked-prefill version of vLLM model runner.\n    \n    This test is used when there is discrepancy in kernels\n    / numerics (e.g. when using lower-precision types like FP8).\n    \"\"\"\n    NUM_LOG_PROBS = 8\n\n    if model == \"facebook/opt-125m\":\n        pytest.skip(\n            \"#7378: CUDA illegal memory access (undiagnosed) facebook/opt-125m\"\n        )\n\n    max_num_seqs = chunked_prefill_token_size\n    max_num_batched_tokens = chunked_prefill_token_size\n\n    extra_kwargs = {}\n    if model in KV_CACHE_QUANTIZATION_PATHS:\n        extra_kwargs[\"quantization_param_path\"] = KV_CACHE_QUANTIZATION_PATHS[\n            model]\n\n    with vllm_runner(\n            model,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n            kv_cache_dtype=kv_cache_dtype,\n            disable_async_output_proc=disable_async_output_proc,\n            **extra_kwargs,\n    ) as vllm_model:\n        no_chunked_prefill_outputs = vllm_model.generate_greedy_logprobs(\n            example_prompts, max_tokens, NUM_LOG_PROBS)\n\n    with vllm_runner(\n            model,\n            max_num_batched_tokens=max_num_batched_tokens,\n            enable_chunked_prefill=True,\n            tensor_parallel_size=tensor_parallel_size,\n            enforce_eager=enforce_eager,\n            max_num_seqs=max_num_seqs,\n            kv_cache_dtype=kv_cache_dtype,\n            disable_async_output_proc=disable_async_output_proc,\n            **extra_kwargs,\n    ) as vllm_model:\n        chunked_prefill_outputs = vllm_model.generate_greedy_logprobs(\n            example_prompts, max_tokens, NUM_LOG_PROBS)\n\n    check_logprobs_close(\n        outputs_0_lst=no_chunked_prefill_outputs,\n        outputs_1_lst=chunked_prefill_outputs,\n        name_0=\"no_chunked_prefill\",\n        name_1=\"chunked_prefill\",\n    )\n",
      "diff": "diff --git a/tests/basic_correctness/test_chunked_prefill.py b/tests/basic_correctness/test_chunked_prefill.py\nindex 1211e6ba5..fc6f829c3 100644\n--- a/tests/basic_correctness/test_chunked_prefill.py\n+++ b/tests/basic_correctness/test_chunked_prefill.py\n@@ -6,6 +6,7 @@ prefill requests are chunked.\n \n Run `pytest tests/models/test_chunked_prefill.py`.\n \"\"\"\n+from contextlib import nullcontext\n \n import pytest\n \n@@ -156,3 +157,68 @@ def test_models_with_fp8_kv_cache(\n         name_0=\"no_chunked_prefill\",\n         name_1=\"chunked_prefill\",\n     )\n+\n+\n+@pytest.mark.parametrize(\"max_tokens\", [16])\n+@pytest.mark.parametrize(\"enforce_eager\", [False])\n+@pytest.mark.parametrize(\"chunk_size\", [30, 32])\n+@pytest.mark.parametrize(\"use_v2_block_manager\", [False, True])\n+# NOTE: Increasing this in this suite will fail CI because we currently cannot\n+# reset distributed env properly. Use a value > 1 just when you test.\n+@pytest.mark.parametrize(\"tensor_parallel_size\", [1])\n+def test_with_prefix_caching(\n+    vllm_runner,\n+    max_tokens: int,\n+    enforce_eager: bool,\n+    chunk_size: int,\n+    use_v2_block_manager: bool,\n+    tensor_parallel_size: int,\n+) -> None:\n+    \"\"\"\n+    Checks exact match decode with and without prefix caching\n+    with chunked prefill enabled.\n+    \"\"\"\n+    model = \"meta-llama/Llama-2-7b-chat-hf\"\n+    # The common prompt has 142 tokens with Llama-2 tokenizer.\n+    common_prompt = \"You are a helpful AI assistant \" * 20\n+    unique_prompts = [\n+        \"Question\",  # Warmup\n+        \"Question\",  # Fully cached\n+        \"Another question\",  # Partial cached\n+    ]\n+    full_prompts = [f\"{common_prompt}\\n{p}\" for p in unique_prompts]\n+\n+    max_num_batched_tokens = max_num_seqs = chunk_size\n+    outputs = {}  # type: ignore\n+    check_result = True\n+    for enable in (True, False):\n+        with vllm_runner(\n+                model,\n+                dtype=\"half\",\n+                max_num_batched_tokens=max_num_batched_tokens,\n+                enable_chunked_prefill=True,\n+                enable_prefix_caching=enable,\n+                tensor_parallel_size=tensor_parallel_size,\n+                use_v2_block_manager=use_v2_block_manager,\n+                enforce_eager=enforce_eager,\n+                max_num_seqs=max_num_seqs,\n+        ) as vllm_model:\n+            # It should fail when prefix caching is enable and chunk\n+            # size is not a multiple of block size (16).\n+            should_fail = chunk_size % 16 != 0 and enable\n+            check_result &= not should_fail\n+            outputs[enable] = []\n+            # Send the request one-by-one to ensure the cache is populated.\n+            with pytest.raises(ValueError) if should_fail else nullcontext():\n+                for prompt in full_prompts:\n+                    outputs[enable] += vllm_model.generate_greedy([prompt],\n+                                                                  max_tokens)\n+\n+    # Check results only if we did not expect a failure.\n+    if check_result:\n+        check_outputs_equal(\n+            outputs_0_lst=outputs[False],\n+            outputs_1_lst=outputs[True],\n+            name_0=\"w/o prefix caching\",\n+            name_1=\"with prefix caching\",\n+        )",
      "change_type": "modified",
      "lines_added": 67,
      "lines_removed": 1
    },
    {
      "file_path": "tests/core/test_block_manager.py",
      "old_content": "import time\nfrom collections import defaultdict\nfrom typing import List\n\nimport pytest\n\nfrom vllm import SamplingParams\nfrom vllm.block import PhysicalTokenBlock\nfrom vllm.core.block.utils import (STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n                                   STR_NOT_IMPL_ENC_DEC_SWA)\nfrom vllm.core.block_manager_v1 import (BlockSpaceManagerV1,\n                                        UncachedBlockAllocator)\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.sequence import Logprob, Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nfrom .utils import create_dummy_prompt, create_dummy_prompt_encoder_decoder\n\n\ndef test_block_allocator_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n                                           num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    num_free = num_cpu_blocks\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        num_free -= 1\n\n        assert block not in cpu_allocator.free_blocks\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n    with pytest.raises(ValueError):\n        cpu_allocator.allocate()\n\n\ndef test_block_allocator_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    cpu_allocator = UncachedBlockAllocator(Device.CPU, block_size,\n                                           num_cpu_blocks)\n\n    # Allocate all available cpu blocks.\n    blocks: List[PhysicalTokenBlock] = []\n    for _ in range(num_cpu_blocks):\n        block = cpu_allocator.allocate()\n        blocks.append(block)\n        assert block not in cpu_allocator.free_blocks\n\n    # Free all allocated cpu blocks.\n    num_free = 0\n    assert cpu_allocator.get_num_free_blocks() == num_free\n    for block in blocks:\n        cpu_allocator.free(block)\n        num_free += 1\n        assert block in cpu_allocator.free_blocks\n        assert cpu_allocator.get_num_free_blocks() == num_free\n\n        with pytest.raises(ValueError):\n            cpu_allocator.free(block)\n\n\ndef test_allocate():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same sequence group to all available gpu blocks.\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n    # Allocate same sequence group to all available gpu blocks.\n    # Use watermark to reserve one gpu block.\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=1 / num_gpu_blocks)\n    for i in range(num_gpu_blocks - 1):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n\ndef test_allocate_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_req_per_seq_group = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same sequence group to all available gpu blocks.\n    for i in range(num_gpu_blocks // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            str(i),\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n    # Allocate same sequence group to all available gpu blocks.\n    # Use watermark to reserve one gpu block.\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=1 / num_gpu_blocks)\n    for i in range((num_gpu_blocks - 1) // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            str(i),\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        assert block_manager.can_allocate(seq_group) == AllocStatus.OK\n        block_manager.allocate(seq_group)\n    assert block_manager.can_allocate(seq_group) != AllocStatus.OK\n\n\ndef test_allocate_encoder_decoder_fails_with_swa():\n    # SWA short for sliding window attention\n\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0,\n                                        sliding_window=5)  # swa\n\n    # Allocate same sequence group to all available gpu blocks.\n    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n        \"0\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n\n    # Assert that can_allocate() fails due to SWA\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.can_allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n\n    # Assert that allocate() fails due to SWA\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_SWA\n\n\ndef test_allocate_encoder_decoder_fails_with_prefix_caching():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0,\n                                        enable_caching=True)  # Prefix cache\n\n    # Allocate same sequence group to all available gpu blocks.\n    _, _, seq_group = create_dummy_prompt_encoder_decoder(\n        \"0\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n\n    # Assert that can_allocate() fails due to prefix caching\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.can_allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n\n    # Assert that allocate() fails due to prefix caching\n    with pytest.raises(NotImplementedError) as exc_info:\n        block_manager.allocate(seq_group)\n\n    assert str(exc_info.value) == STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\n\n\ndef test_append_slot_single_seq():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate single seq to gpu block.\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Nothing to append. Sequence has no new logical blocks.\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slots(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks == after_blocks\n\n    # Add block_size number of new tokens and append slot.\n    for i in range(block_size):\n        token_id = i + 5\n        prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    assert not block_manager.append_slots(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_append_slot_cow():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size=block_size,\n                                        num_cpu_blocks=num_cpu_blocks,\n                                        num_gpu_blocks=num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate prompt to gpu block. There is one slot left in the block.\n    prompt = Sequence(seq_id=1,\n                      inputs={\n                          \"prompt\": \"one two three\",\n                          \"prompt_token_ids\": [1, 2, 3],\n                      },\n                      block_size=block_size)\n\n    # Fork the sequence, such that a COW will be required when we append a new\n    # token id.\n    child = prompt.fork(new_seq_id=2)\n\n    # Allocate space for the sequence group.\n    seq_group = SequenceGroup(request_id=\"1\",\n                              seqs=[prompt, child],\n                              arrival_time=time.time(),\n                              sampling_params=SamplingParams())\n    block_manager.allocate(seq_group)\n\n    # Fork and append a new token id. We expect a COW to be scheduled.\n    token_id = 4\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.fork(prompt, child)\n\n    assert block_manager.can_append_slots(seq_group)\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n\n    cows = block_manager.append_slots(child)\n    assert cows\n    dict_cows = defaultdict(list)\n    for src_block, dst_block in cows:\n        dict_cows[src_block].append(dst_block)\n    for src_block, dst_blocks in dict_cows.items():\n        assert src_block not in dst_blocks\n\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_blocks - after_blocks == 1\n\n\ndef test_fork():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\",\n                                            block_size - 1,\n                                            block_size=block_size)\n    block_manager.allocate(seq_group)\n\n    # Fork prompt and copy block tables.\n    child = prompt.fork(2)\n    block_manager.fork(prompt, child)\n    assert block_manager.get_block_table(\n        prompt) == block_manager.get_block_table(child)\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(child)\n    assert block_manager.get_block_table(\n        prompt) != block_manager.get_block_table(child)\n\n\ndef test_swap():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", prompt_length=block_size - 1)\n    prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    prompt.status = SequenceStatus.RUNNING\n    prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap seq group from GPU -> CPU.\n    gpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    assert [x[0] for x in mapping] == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    prompt.status = SequenceStatus.SWAPPED\n\n    # Swap seq group from CPU -> GPU.\n    cpu_blocks = block_manager.get_block_table(prompt)\n    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    assert [x[0] for x in mapping] == cpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\ndef test_swap_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    decoder_prompt, encoder_prompt, seq_group = \\\n        create_dummy_prompt_encoder_decoder(\n        \"1\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n    decoder_prompt.status = SequenceStatus.WAITING\n    encoder_prompt.status = SequenceStatus.WAITING\n    block_manager.allocate(seq_group)\n\n    # Emulate a forward pass by appending a single token.\n    # The block manager then knows how many unprocessed\n    # tokens will be written in the next forward pass.\n    token_id = 0\n    decoder_prompt.status = SequenceStatus.RUNNING\n    decoder_prompt.append_token_id(token_id, {token_id: Logprob(0.0)})\n\n    # Swap encoder/decoder seq group from GPU -> CPU.\n    decoder_gpu_blocks = block_manager.get_block_table(decoder_prompt)\n    cross_gpu_blocks = block_manager.get_cross_block_table(seq_group)\n    gpu_blocks = decoder_gpu_blocks + cross_gpu_blocks\n    assert block_manager.can_swap_out(seq_group)\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_out(seq_group)\n    assert [x[0] for x in mapping] == gpu_blocks\n    #assert list(mapping.keys()) == gpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks == after_cpu_blocks + len(gpu_blocks)\n    assert before_gpu_blocks + len(gpu_blocks) == after_gpu_blocks\n    decoder_prompt.status = SequenceStatus.SWAPPED\n\n    # Swap encoder/decoder seq group from CPU -> GPU.\n    decoder_cpu_blocks = block_manager.get_block_table(decoder_prompt)\n    cross_cpu_blocks = block_manager.get_cross_block_table(seq_group)\n    cpu_blocks = decoder_cpu_blocks + cross_cpu_blocks\n    assert block_manager.can_swap_in(seq_group) == AllocStatus.OK\n    before_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    before_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    mapping = block_manager.swap_in(seq_group)\n    assert [x[0] for x in mapping] == cpu_blocks\n    after_cpu_blocks = block_manager.get_num_free_cpu_blocks()\n    after_gpu_blocks = block_manager.get_num_free_gpu_blocks()\n    assert before_cpu_blocks + len(cpu_blocks) == after_cpu_blocks\n    assert before_gpu_blocks == after_gpu_blocks + len(cpu_blocks)\n\n\ndef test_free():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    prompt, seq_group = create_dummy_prompt(\"1\", block_size)\n    block_manager.allocate(seq_group)\n\n    # Free allocated seq.\n    prompt_blocks = len(block_manager.get_block_table(prompt))\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    block_manager.free(prompt)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert after_blocks == before_blocks + prompt_blocks\n\n    # Block table for freed seq is deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(prompt)\n\n\ndef test_free_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    decoder_prompt, encoder_prompt, seq_group = \\\n        create_dummy_prompt_encoder_decoder(\n        \"1\",\n        decoder_prompt_length=block_size,\n        encoder_prompt_length=block_size)\n    block_manager.allocate(seq_group)\n\n    # Free allocated seq.\n    decoder_prompt_blocks = len(block_manager.get_block_table(decoder_prompt))\n    encoder_prompt_blocks = len(block_manager.get_cross_block_table(seq_group))\n    prompt_blocks = decoder_prompt_blocks + encoder_prompt_blocks\n    before_blocks = block_manager.get_num_free_gpu_blocks()\n    block_manager.free(decoder_prompt)\n    block_manager.free_cross(seq_group)\n    after_blocks = block_manager.get_num_free_gpu_blocks()\n    assert after_blocks == before_blocks + prompt_blocks\n\n    # Block table for freed encoder & decoder seq's are deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(decoder_prompt)\n\n    # Block table for freed encoder & decoder seq's are deleted.\n    with pytest.raises(KeyError):\n        block_manager.get_block_table(encoder_prompt)\n\n\ndef test_reset():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same seq group on all available gpu blocks.\n    original_blocks = block_manager.get_num_free_gpu_blocks()\n    for i in range(num_gpu_blocks):\n        _, seq_group = create_dummy_prompt(str(i), block_size)\n        block_manager.allocate(seq_group)\n    assert block_manager.get_num_free_gpu_blocks() == 0\n\n    # Resetting block manager frees all allocated blocks.\n    block_manager.reset()\n    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n\n\ndef test_reset_encoder_decoder():\n    block_size = 4\n    num_cpu_blocks = 4\n    num_gpu_blocks = 4\n    block_req_per_seq_group = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        watermark=0)\n\n    # Allocate same seq group on all available gpu blocks.\n    original_blocks = block_manager.get_num_free_gpu_blocks()\n    for i in range(num_gpu_blocks // block_req_per_seq_group):\n        _, _, seq_group = create_dummy_prompt_encoder_decoder(\n            f\"{i}\",\n            decoder_prompt_length=block_size,\n            encoder_prompt_length=block_size)\n        block_manager.allocate(seq_group)\n    assert block_manager.get_num_free_gpu_blocks() == 0\n\n    # Resetting block manager frees all allocated blocks.\n    block_manager.reset()\n    assert block_manager.get_num_free_gpu_blocks() == original_blocks\n\n\ndef test_sliding_window_multi_seq():\n    \"\"\"\n    Tests that memory allocation and deallocation is handled\n    correctly with multiple sequences that exceed the sliding\n    window's capacity.\n    \"\"\"\n    block_size = 1\n    num_cpu_blocks = 8\n    num_gpu_blocks = 8\n    sliding_window = 2\n    block_manager = BlockSpaceManagerV1(block_size,\n                                        num_cpu_blocks,\n                                        num_gpu_blocks,\n                                        sliding_window=sliding_window,\n                                        watermark=0)\n\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n\n    parent = Sequence(seq_id=1,\n                      inputs={\n                          \"prompt\": \"one two three\",\n                          \"prompt_token_ids\": [0, 1, 2],\n                      },\n                      block_size=block_size)\n    seq_group = SequenceGroup(request_id=\"1\",\n                              seqs=[parent],\n                              arrival_time=time.time(),\n                              sampling_params=SamplingParams(),\n                              lora_request=None)\n    block_manager.allocate(seq_group)\n\n    # assert the number of blocks allocated is correct\n    # the parent seq has len 3, but since sliding_window is 2,\n    # we will use at most 2 blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # Fork prompt and copy block tables.\n    child = parent.fork(2)\n    block_manager.fork(parent, child)\n\n    # assert the number of blocks allocated is correct\n    # forking does not increase memory consumption\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # assert both parent and child share all blocks\n    assert block_manager.get_block_table(\n        parent) == block_manager.get_block_table(child)\n\n    token_id = 4\n    # Append token to child. Block is shared so copy on write occurs.\n    child.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(child)\n\n    # assert the number of blocks allocated is correct\n    # we will use now one block more. Each seq will use 2 blocks,\n    # but only one can be shared\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    token_id = 5\n    parent.append_token_id(token_id, {token_id: Logprob(0.0)})\n    block_manager.append_slots(parent)\n\n    # assert the number of blocks allocated is correct\n    # no change, because both sequences are still just sharing one block\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window - 1\n\n    block_table_parent = block_manager.get_block_table(parent)\n    block_table_child = block_manager.get_block_table(child)\n\n    assert block_table_parent != block_table_child\n\n    # assert both blocks are sharing the second-last block\n    assert block_table_parent[-2] == block_table_child[-2]\n\n    # now let's clean up...\n    block_manager.free(parent)\n\n    # assert the number of blocks allocated is correct\n    # We have freed one seq, reducing the ref count of two blocks by one.\n    # One of the two was only used by the parent seq, so this is now free.\n    # The child seq still consumes sliding_window blocks\n    assert block_manager.get_num_free_gpu_blocks(\n    ) == num_gpu_blocks - sliding_window\n\n    # free all blocks\n    block_manager.free(child)\n\n    # assert all blocks are free now\n    assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n",
      "diff": "diff --git a/tests/core/test_block_manager.py b/tests/core/test_block_manager.py\nindex cd306b9e4..2ee9f2082 100644\n--- a/tests/core/test_block_manager.py\n+++ b/tests/core/test_block_manager.py\n@@ -595,3 +595,43 @@ def test_sliding_window_multi_seq():\n \n     # assert all blocks are free now\n     assert block_manager.get_num_free_gpu_blocks() == num_gpu_blocks\n+\n+\n+def test_mark_blocks_as_computed_with_prefix_cache_and_chunked_prefill():\n+    \"\"\"When prefix cache and chunked prefill are enabled, the block manager\n+    should only mark a chunk of blocks as computed instead of all blocks.\n+    \"\"\"\n+\n+    block_size = 4\n+    num_cpu_blocks = 0\n+    num_gpu_blocks = 16\n+    block_manager = BlockSpaceManagerV1(block_size,\n+                                        num_gpu_blocks,\n+                                        num_cpu_blocks,\n+                                        watermark=0,\n+                                        enable_caching=True)\n+\n+    # Set prompt size to have num_gpu_blocks - 1 full blocks.\n+    prompt_length = block_size * num_gpu_blocks - 1\n+\n+    # Allocate (reserve) all blocks.\n+    _, seq_group = create_dummy_prompt(\"0\",\n+                                       prompt_length,\n+                                       block_size=block_size)\n+    block_manager.allocate(seq_group)\n+    assert seq_group.seqs[0].n_blocks == num_gpu_blocks\n+\n+    # 1st chunk: Compute 2 and half blocks. Should mark 2 blocks as computed.\n+    token_chunk_size = int(block_size * 2.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 2\n+\n+    # Actual computed tokens.\n+    seq_group.seqs[0].data.update_num_computed_tokens(token_chunk_size)\n+\n+    # 2nd chunk: Complete 3rd block and additional 4 blocks.\n+    token_chunk_size = int(block_size * 4.5)\n+    block_manager.mark_blocks_as_computed(seq_group, token_chunk_size)\n+    computed_blocks = block_manager.get_all_computed_blocks(seq_group.seqs[0])\n+    assert len(computed_blocks) == 7",
      "change_type": "modified",
      "lines_added": 41,
      "lines_removed": 1
    },
    {
      "file_path": "tests/core/test_chunked_prefill_scheduler.py",
      "old_content": "from typing import List\nfrom unittest.mock import MagicMock\n\nimport pytest  # noqa\n\nfrom vllm.config import CacheConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus\nfrom vllm.core.scheduler import Scheduler\nfrom vllm.sequence import Logprob, SequenceGroup\n\nfrom .utils import create_dummy_prompt\n\n\ndef get_sequence_groups(scheduler_output):\n    return [s.seq_group for s in scheduler_output.scheduled_seq_groups]\n\n\ndef append_new_token(seq_group, token_id: int):\n    for seq in seq_group.get_seqs():\n        seq.append_token_id(token_id, {token_id: Logprob(token_id)})\n\n\ndef schedule_and_update_computed_tokens(scheduler):\n    metas, out, _ = scheduler.schedule()\n    for s, meta in zip(out.scheduled_seq_groups, metas):\n        s.seq_group.update_num_computed_tokens(meta.token_chunk_size)\n    return metas, out\n\n\ndef test_simple():\n    \"\"\"Verify basic scheduling works.\"\"\"\n    block_size = 4\n    num_seq_group = 4\n    max_model_len = 16\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       num_seq_group,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(num_seq_group):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=block_size)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Schedule seq groups prompts.\n    num_tokens = block_size * num_seq_group\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_tokens\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n    for s in running:\n        append_new_token(s, 1)\n\n    # Schedule seq groups generation.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert out.num_batched_tokens == num_seq_group\n    assert (not out.blocks_to_copy and not out.blocks_to_swap_in\n            and not out.blocks_to_swap_out)\n    assert len(seq_group_meta) == num_seq_group\n\n\ndef test_chunk():\n    \"\"\"Verify prefills are chunked properly.\"\"\"\n    block_size = 4\n    max_seqs = 60\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Verify the second request is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    assert seq_group_meta[0].token_chunk_size == 60\n    # Verify it is chunked.\n    assert seq_group_meta[1].token_chunk_size == 4\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # One chunked prefill, and one decoding.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert set(get_sequence_groups(out)) == set(running)\n    # The first one is prefill. Scheduler guarantees ordering.\n    assert seq_group_meta[0].token_chunk_size == 56\n    # The second one is a chunked prefill.\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 57\n\n\ndef test_complex():\n    block_size = 4\n    max_seqs = 60\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n        assert seq_group.is_prefill()\n\n    # Verify the second request is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n\n    assert set(get_sequence_groups(out)) == set(running)\n    assert seq_group_meta[0].token_chunk_size == 60\n    # Verify it is chunked.\n    assert seq_group_meta[1].token_chunk_size == 4\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # Add 2 more requests.\n    for i in range(2, 4):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=60)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Decoding & chunked prefill & first chunk of 3rd request is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 3\n    # The first one is the first chunked prefill.\n    assert seq_group_meta[0].token_chunk_size == 7\n    # The second one is the second new chunked prefill.\n    assert seq_group_meta[1].token_chunk_size == 56\n    # The last one is decode.\n    assert seq_group_meta[2].token_chunk_size == 1\n    # Two of them are in chunked prefill.\n    assert out.num_prefill_groups == 2\n    assert out.num_batched_tokens == 64\n    # The first 2 requests are now in decodine phase.\n    append_new_token(running[0], 1)\n    assert not running[0].is_prefill()\n    append_new_token(running[1], 1)\n    assert not running[1].is_prefill()\n    # The third request is still in prefill stage.\n    assert running[2].is_prefill()\n\n\ndef test_maximal_decoding():\n    \"\"\"Verify decoding requests are prioritized.\"\"\"\n    block_size = 4\n    max_seqs = 2\n    max_model_len = 8\n    max_num_batched_tokens = 2\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    # Add seq groups to scheduler.\n    for i in range(2):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=2)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n        assert seq_group.is_prefill()\n\n    # The first prefill is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 1\n    assert seq_group_meta[0].token_chunk_size == 2\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    # Only the first seq group has a new token appended.\n    append_new_token(running[0], 1)\n\n    # Create one more seq_group.\n    _, seq_group = create_dummy_prompt(\"3\", prompt_length=2)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n    # The first decoding + second chunk is scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    assert running[2].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n\n    # Decoding + running prefill is prioritized.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n    append_new_token(running[1], 1)\n\n    # Only decoding is prioritized.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n    assert out.num_prefill_groups == 0\n    assert out.num_batched_tokens == 2\n    append_new_token(running[0], 1)\n    append_new_token(running[1], 1)\n\n    # After aborting the decoding request, the fcfs new prefill is prioritized.\n    scheduler.abort_seq_group(running[0].request_id)\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 2\n    assert seq_group_meta[0].token_chunk_size == 1\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert not running[1].is_prefill()\n    assert running[2].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 2\n\n\ndef test_prompt_limit():\n    \"\"\"Verify max_num_batched_tokens < max_model_len is possible.\"\"\"\n    block_size = 4\n    max_seqs = 32\n    max_model_len = 64\n    max_num_batched_tokens = 32\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=48)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n\n    # The prompt length > max_num_batched_tokens should be still scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(get_sequence_groups(out)) == 1\n    assert seq_group_meta[0].token_chunk_size == 32\n    assert running[0].is_prefill()\n    assert out.num_prefill_groups == 1\n    assert out.num_batched_tokens == 32\n\n\ndef test_prompt_limit_exceed():\n    block_size = 4\n    max_seqs = 64\n    max_model_len = 32\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    _, seq_group = create_dummy_prompt(\"2\", prompt_length=48)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    assert seq_group.is_prefill()\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.ignored_seq_groups) == 1\n    assert out.ignored_seq_groups[0] == seq_group\n\n\ndef test_swap():\n    \"\"\"Verify swapping works with chunked prefill requests\"\"\"\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The last request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # The running prefill is now swapped.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out != []\n    assert out.blocks_to_swap_in == []\n\n    # Add 1 more task. Swap should be prioritized over new prefill.\n    _, seq_group = create_dummy_prompt(\"2\", prompt_length=60)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in != []\n    assert out.blocks_to_swap_out == []\n\n\ndef test_running_prefill_prioritized_over_swap():\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60, best_of=2)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The request should be swapped out.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group)\n\n    # The running prefill is now swapped.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out != []\n    assert out.blocks_to_swap_in == []\n\n    # Add 1 more task. Swap is not possible, so prefill is running.\n    scheduler.block_manager.can_swap_in = MagicMock()\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.LATER\n\n    _, seq_group2 = create_dummy_prompt(\"2\", prompt_length=60)\n    scheduler.add_seq_group(seq_group2)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n\n    # Now although swap is possible, running prefill is prioritized.\n    scheduler.block_manager.can_swap_in.return_value = AllocStatus.OK\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert not seq_group2.is_prefill()\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n    append_new_token(seq_group2, 1)\n\n    # Decoding is prioritized.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    # 3 decodes. It is swapped in.\n    assert out.num_batched_tokens == 1\n    assert out.blocks_to_swap_in == []\n    assert out.blocks_to_swap_out == []\n    assert not seq_group2.is_prefill()\n    assert out.scheduled_seq_groups[0].seq_group == seq_group2\n    append_new_token(seq_group2, 1)\n\n    # Since we abort the sequence group, we can finally swap.\n    scheduler.abort_seq_group(seq_group2.request_id)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_batched_tokens == 30\n    assert out.blocks_to_swap_in != []\n    assert out.blocks_to_swap_out == []\n\n\ndef test_chunked_prefill_preempt():\n    \"\"\"Verify preempt works with chunked prefill requests\"\"\"\n    block_size = 4\n    max_seqs = 30\n    max_model_len = 200\n    max_num_batched_tokens = 30\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=60)\n    scheduler.add_seq_group(seq_group)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    # The request is chunked.\n    # prefill scheduled now.\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n    # The request should be preempted.\n    scheduler.block_manager.can_append_slots = MagicMock()\n\n    def cannot_append_second_group1(seq_group, num_lookahead_slots):\n        return seq_group.request_id != \"1\"\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group1)\n\n    # The running prefill is now preempted.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 0\n    assert out.num_batched_tokens == 0\n    assert out.blocks_to_swap_out == []\n    assert out.blocks_to_swap_in == []\n\n    # Make sure we can reschedule preempted request.\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n    assert seq_group.get_num_uncomputed_tokens() == 30\n\n    # We should be able to run prefill twice as it is chunked.\n    def cannot_append_second_group2(seq_group, num_lookahead_slots):\n        return True\n\n    scheduler.block_manager.can_append_slots.side_effect = (\n        cannot_append_second_group2)\n    _, out = schedule_and_update_computed_tokens(scheduler)\n    assert len(out.scheduled_seq_groups) == 1\n    assert out.num_prefill_groups == 1\n    assert not seq_group.is_prefill()\n    assert out.num_batched_tokens == max_num_batched_tokens\n\n\ndef test_chunked_prefill_max_seqs():\n    block_size = 4\n    max_seqs = 2\n    max_model_len = 80\n    max_num_batched_tokens = 64\n    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n                                       max_seqs,\n                                       max_model_len,\n                                       enable_chunked_prefill=True)\n    cache_config = CacheConfig(block_size, 1.0, 1, \"auto\")\n    cache_config.num_cpu_blocks = 8\n    cache_config.num_gpu_blocks = 8\n    scheduler = Scheduler(scheduler_config, cache_config, None)\n    running: List[SequenceGroup] = []\n\n    _, seq_group = create_dummy_prompt(\"1\", prompt_length=65)\n    scheduler.add_seq_group(seq_group)\n    running.append(seq_group)\n    # The first prefill is chunked.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert seq_group_meta[0].token_chunk_size == max_num_batched_tokens\n    assert len(get_sequence_groups(out)) == 1\n\n    # Add new requests.\n    for i in range(4):\n        _, seq_group = create_dummy_prompt(str(i), prompt_length=65)\n        scheduler.add_seq_group(seq_group)\n        running.append(seq_group)\n\n    # Make sure only 2 requests are scheduled.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert out.num_batched_tokens == max_num_batched_tokens\n    assert len(get_sequence_groups(out)) == 2\n    assert not running[0].is_prefill()\n    assert running[1].is_prefill()\n    append_new_token(running[0], 1)\n\n    # Although we have enough token budget, we can only schedule max_seqs.\n    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n    assert seq_group_meta[0].token_chunk_size == 2\n    assert seq_group_meta[1].token_chunk_size == 1\n    assert out.num_batched_tokens == 3\n    assert len(get_sequence_groups(out)) == max_seqs\n    assert not running[0].is_prefill()\n    assert not running[1].is_prefill()\n",
      "diff": "diff --git a/tests/core/test_chunked_prefill_scheduler.py b/tests/core/test_chunked_prefill_scheduler.py\nindex 6d9c2f3eb..2f6ea632a 100644\n--- a/tests/core/test_chunked_prefill_scheduler.py\n+++ b/tests/core/test_chunked_prefill_scheduler.py\n@@ -562,3 +562,42 @@ def test_chunked_prefill_max_seqs():\n     assert len(get_sequence_groups(out)) == max_seqs\n     assert not running[0].is_prefill()\n     assert not running[1].is_prefill()\n+\n+\n+def test_perfix_caching():\n+    \"\"\"Verify allocating full blocks when prefix caching is enabled.\"\"\"\n+    block_size = 4\n+    max_seqs = 10\n+    max_model_len = 80\n+    max_num_batched_tokens = 64\n+    scheduler_config = SchedulerConfig(max_num_batched_tokens,\n+                                       max_seqs,\n+                                       max_model_len,\n+                                       enable_chunked_prefill=True)\n+    cache_config = CacheConfig(block_size,\n+                               1.0,\n+                               1,\n+                               \"auto\",\n+                               enable_prefix_caching=True)\n+    cache_config.num_cpu_blocks = 0\n+    cache_config.num_gpu_blocks = 32\n+    scheduler = Scheduler(scheduler_config, cache_config, None)\n+    running: List[SequenceGroup] = []\n+\n+    # Add seq groups to scheduler.\n+    for i in range(2):\n+        _, seq_group = create_dummy_prompt(str(i),\n+                                           block_size=block_size,\n+                                           prompt_length=50)\n+        scheduler.add_seq_group(seq_group)\n+        running.append(seq_group)\n+\n+    seq_group_meta, out = schedule_and_update_computed_tokens(scheduler)\n+    assert set(get_sequence_groups(out)) == set(running)\n+    assert seq_group_meta[0].token_chunk_size == 50\n+    # Verify it is chunked. Note that although the budget is 64-50=14,\n+    # we only allocate full blocks for prefix caching, so only 4*(14//4)=12\n+    # tokens are allocated.\n+    assert seq_group_meta[1].token_chunk_size == 12\n+    assert out.num_prefill_groups == 2\n+    assert out.num_batched_tokens == 62",
      "change_type": "modified",
      "lines_added": 40,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/core/block_manager_v1.py",
      "old_content": "\"\"\"A block manager that manages token blocks.\"\"\"\nimport math\nfrom abc import ABC, abstractmethod\nfrom itertools import count, takewhile\nfrom os.path import commonprefix\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple\n\nfrom vllm.block import BlockTable, PhysicalTokenBlock\nfrom vllm.core.block.common import CacheMetricData\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nlogger = init_logger(__name__)\n\n\nclass BlockAllocatorBase(ABC):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n        pass\n\n    @abstractmethod\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        pass\n\n    @abstractmethod\n    def free(self, block: PhysicalTokenBlock) -> None:\n        pass\n\n    @abstractmethod\n    def get_num_free_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_total_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def contains_block(self, block_hash: int) -> bool:\n        pass\n\n    @abstractmethod\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        pass\n\n    @abstractmethod\n    def get_prefix_cache_hit_rate(self) -> float:\n        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n        pass\n\n\nclass CachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        self.current_num_blocks = 0\n        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        self.default_hash_ctr = count()\n\n        self.cache_metric_data = CacheMetricData()\n\n    def allocate_block(self, block_hash: int,\n                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n        if self.current_num_blocks == self.num_blocks:\n            block = self.evictor.evict()\n            block.block_hash = block_hash\n            block.num_hashed_tokens = num_hashed_tokens\n            return block\n        block = PhysicalTokenBlock(device=self.device,\n                                   block_number=self.current_num_blocks,\n                                   block_size=self.block_size,\n                                   block_hash=block_hash,\n                                   num_hashed_tokens=num_hashed_tokens)\n        self.current_num_blocks += 1\n        return block\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if block_hash is None:\n            block_hash = next(self.default_hash_ctr)\n\n        if block_hash in self.evictor:\n            assert block_hash not in self.cached_blocks\n            block = self.evictor.remove(block_hash)\n            assert block.ref_count == 0\n            self.cached_blocks[block_hash] = block\n\n        if block_hash in self.cached_blocks:\n            self.cache_metric_data.query(hit=True)\n        else:\n            self.cache_metric_data.query(hit=False)\n            self.cached_blocks[block_hash] = self.allocate_block(\n                block_hash, num_hashed_tokens)\n        block = self.cached_blocks[block_hash]\n        assert block.block_hash == block_hash\n        block.ref_count += 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            assert block.block_hash not in self.evictor\n            self.evictor.add(block)\n\n            # Remove the block from the cached_blocks\n            del self.cached_blocks[block.block_hash]\n\n    def get_num_free_blocks(self) -> int:\n        return (self.num_blocks - self.current_num_blocks +\n                self.evictor.num_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        return block_hash in self.cached_blocks or block_hash in self.evictor\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        # Update the hash of block and the cached_blocks dictionary.\n        assert not self.contains_block(block_hash)\n        old_hash = block.block_hash\n        block.block_hash = block_hash\n        del self.cached_blocks[old_hash]\n        self.cached_blocks[block_hash] = block\n\n    def get_prefix_cache_hit_rate(self) -> float:\n        return self.cache_metric_data.get_hit_rate()\n\n\nclass UncachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: Device,\n        block_size: int,\n        num_blocks: int,\n    ) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        # Initialize the free blocks.\n        self.free_blocks: List[PhysicalTokenBlock] = []\n        for i in range(num_blocks):\n            block = PhysicalTokenBlock(device=device,\n                                       block_number=i,\n                                       block_size=block_size,\n                                       block_hash=-1,\n                                       num_hashed_tokens=0)\n            self.free_blocks.append(block)\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if not self.free_blocks:\n            raise ValueError(\"Out of memory! No free blocks are available.\")\n        block = self.free_blocks.pop()\n        block.ref_count = 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            self.free_blocks.append(block)\n\n    def get_num_free_blocks(self) -> int:\n        return len(self.free_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def get_prefix_cache_hit_rate(self) -> float:\n        return -1\n\n\nclass BlockSpaceManagerV1(BlockSpaceManager):\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        if enable_caching and sliding_window is not None:\n            raise NotImplementedError(\n                \"Sliding window is not allowed with prefix caching enabled!\")\n\n        self.block_sliding_window = None\n        if sliding_window is not None:\n            # Round up to nearest block size to regularize sliding window\n            # allocation sizes.\n            self.block_sliding_window = math.ceil(sliding_window / block_size)\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        if self.enable_caching:\n            logger.info(\"Automatic prefix caching is enabled.\")\n            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        else:\n            self.gpu_allocator = UncachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator = UncachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        # Mapping: seq_id -> BlockTable.\n        self.block_tables: Dict[int, BlockTable] = {}\n\n        # Mapping: req_id -> BlockTable\n        # Note that each SequenceGroup has a unique\n        # request ID\n        self.cross_block_tables: Dict[str, BlockTable] = {}\n\n    def _get_seq_num_required_blocks(self, seq: Optional[Sequence]) -> int:\n        return 0 if seq is None else seq.n_blocks\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        self_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n        cross_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_encoder_seq())\n        num_required_blocks = self_num_required_blocks + \\\n                              cross_num_required_blocks\n\n        if self.block_sliding_window is not None:\n\n            num_required_blocks = min(num_required_blocks,\n                                      self.block_sliding_window)\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, \\\n                           seq: Optional[Sequence], \\\n                           ref_count: int, \\\n                           is_encoder_decoder: bool = True) -> BlockTable:\n        # Allocate new physical token blocks that will store the prompt tokens.\n        num_prompt_blocks = self._get_seq_num_required_blocks(seq)\n\n        block_table: BlockTable = BlockTable()\n        assert seq is not None\n        for logical_idx in range(num_prompt_blocks):\n            if (self.block_sliding_window is not None\n                    and logical_idx >= self.block_sliding_window):\n                block = block_table[logical_idx % self.block_sliding_window]\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            elif not is_encoder_decoder and self.enable_caching:\n                block = self.gpu_allocator.allocate(\n                    seq.hash_of_block(logical_idx),\n                    seq.num_hashed_tokens_of_block(logical_idx))\n            else:\n                block = self.gpu_allocator.allocate()\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            block_table.append(block)\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        is_encoder_decoder = seq_group.is_encoder_decoder()\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        # Allocate decoder sequences\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # decoder prompt.\n        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        seq = wait_seqs[0]\n        block_table: BlockTable = \\\n            self._allocate_sequence(seq,\n                                    seq_group.num_seqs(),\n                                    is_encoder_decoder)\n\n        # Assign the self-attention block tables for each sequence.\n        if len(wait_seqs) == 1:\n            self.block_tables[seq.seq_id] = block_table\n        else:\n            for seq in wait_seqs:\n                self.block_tables[seq.seq_id] = block_table.copy()\n\n        # Allocate encoder sequence\n        if is_encoder_decoder:\n            # A SequenceGroup has only a single encoder sequence (at most),\n            # thus allocate with a ref count of 1\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n                                                  1, is_encoder_decoder)\n            # Assign the cross-attention block table for the SequenceGroup.\n            self.cross_block_tables[seq_group.request_id] = block_table\n\n    def can_append_slots(self,\n                         seq_group: SequenceGroup,\n                         num_lookahead_slots: int = 0) -> bool:\n        assert (num_lookahead_slots == 0\n                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n\n        # Simple heuristic: If there is at least one free block\n        # for each sequence, we can append.\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n        return num_seqs <= num_free_gpu_blocks\n\n    def _promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        assert self.enable_caching\n\n        # Compute a new hash for the block so that it can be shared by other\n        # Sequences\n        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n\n        # if new_hash is already in the cached table, then free last_block\n        # and return the cached version\n        if self.gpu_allocator.contains_block(new_hash):\n            self.gpu_allocator.free(last_block)\n            return self.gpu_allocator.allocate(new_hash)\n        else:\n            self.gpu_allocator.update_hash(new_hash, last_block)\n            return last_block\n\n    def _is_last_block_full(\n        self,\n        seq: Sequence,\n    ) -> bool:\n        token_ids_len = seq.data.get_len()\n        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n\n    def _maybe_promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        if self._is_last_block_full(seq):\n            return self._promote_last_block(seq, last_block)\n        else:\n            return last_block\n\n    def _allocate_last_physical_block(\n        self,\n        seq: Sequence,\n    ) -> PhysicalTokenBlock:\n        # Called before a new block is appended.\n        # This is in charge of allocating a new physical block (to be appended).\n\n        # None if the last block is not full. Otherwise, we set it to the\n        # content hash.\n        if not self.enable_caching:\n            return self.gpu_allocator.allocate()\n        block_hash: Optional[int] = None\n        n_blocks = seq.n_blocks\n        if (self._is_last_block_full(seq)):\n            block_hash = seq.hash_of_block(n_blocks - 1)\n        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n\n        # num_hashed_tokens is used to compute future hashes\n        # (e.g. in the hashing function, it is used to ask the sequence for\n        # prefix tokens)\n        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n\n        # If the block has is None, then the block is not full.\n        # If the block is not full, then we expect it to have a refcount of 1.\n        if block_hash is None:\n            assert new_block.ref_count == 1\n        return new_block\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int = 0,\n    ) -> List[Tuple[int, int]]:\n        \"\"\"Allocate a physical slot for a new token.\"\"\"\n        n_blocks = seq.n_blocks\n        block_table = self.block_tables[seq.seq_id]\n        # If we need to allocate a new physical block\n        if len(block_table) < n_blocks:\n            # Currently this code only supports adding one physical block\n            assert len(block_table) == n_blocks - 1\n\n            if (self.block_sliding_window\n                    and len(block_table) >= self.block_sliding_window):\n                # reuse a block\n                block_table.append(block_table[len(block_table) %\n                                               self.block_sliding_window])\n            else:\n                # The sequence hash a new logical block.\n                # Allocate a new physical block.\n                new_block = self._allocate_last_physical_block(seq)\n                block_table.append(new_block)\n                return []\n\n        # We want to append the token to the last physical block.\n        last_block = block_table[-1]\n        assert last_block.device == Device.GPU\n        if last_block.ref_count == 1:\n            # Not shared with other sequences. Appendable.\n            if self.enable_caching:\n                # If the last block is now complete, we may reuse an old block\n                # to save memory.\n                maybe_new_block = self._maybe_promote_last_block(\n                    seq, last_block)\n                block_table[-1] = maybe_new_block\n            return []\n        else:\n            # The last block is shared with other sequences.\n            # Copy on Write: Allocate a new block and copy the tokens.\n            new_block = self._allocate_last_physical_block(seq)\n\n            block_table[-1] = new_block\n            self.gpu_allocator.free(last_block)\n            return [(last_block.block_number, new_block.block_number)]\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        # NOTE: fork does not allocate a new physical block.\n        # Thus, it is always safe from OOM.\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n\n        # When using a sliding window, blocks will be eventually reused.\n        # In this case the block tables will contain repeated blocks.\n        # When forking, we must make sure that each block's `ref_count`\n        # is only incremented by one, so we deduplicate them by wrapping\n        # them in a set.\n        for block in set(src_block_table):\n            block.ref_count += 1\n\n    def _get_physical_blocks(\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n\n        # NOTE: Here, we assume that the physical blocks are only shared by\n        # the sequences in the same group.\n        request_id = seq_group.request_id\n        blocks: Set[PhysicalTokenBlock] = set()\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                continue\n            blocks.update(self.block_tables[seq.seq_id])\n        # Cross-attention blocks\n        if seq_group.is_encoder_decoder():\n            blocks.update(self.cross_block_tables[request_id])\n        return list(blocks)\n\n    def can_swap_in(self,\n                    seq_group: SequenceGroup,\n                    num_lookahead_slots: int = 0) -> AllocStatus:\n        assert (num_lookahead_slots == 0\n                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n\n        blocks = self._get_physical_blocks(seq_group)\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n        if seq_group.is_encoder_decoder():\n            num_swapped_seqs += 1\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n        # NOTE: Conservatively, we assume that every sequence will allocate\n        # at least one free block right after the swap-in.\n        # NOTE: This should match the logic in can_append_slot().\n        num_required_blocks = len(blocks) + num_swapped_seqs\n        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n            return AllocStatus.NEVER\n        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _swap_block_table(\n            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n            dest_allocator: BlockAllocatorBase,\n            mapping: Dict[PhysicalTokenBlock,\n                          PhysicalTokenBlock]) -> BlockTable:\n        new_block_table: BlockTable = BlockTable()\n\n        for from_block in block_table:\n            if from_block in mapping:\n                to_block = mapping[from_block]\n                to_block.ref_count += 1\n            else:\n                to_block = dest_allocator.allocate(\n                    from_block.block_hash, from_block.num_hashed_tokens)\n                mapping[from_block] = to_block\n            new_block_table.append(to_block)\n            # Free the source block swapped in to destination.\n            src_allocator.free(from_block)\n\n        return new_block_table\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n\n        request_id = seq_group.request_id\n\n        # CPU block -> GPU block.\n        # dict is efficient in lookup `if cpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.cpu_allocator, self.gpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.cpu_allocator,\n                                       self.gpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        request_id = seq_group.request_id\n\n        # GPU block -> CPU block.\n        # dict is efficient in lookup `if gpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.gpu_allocator, self.cpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.gpu_allocator,\n                                       self.cpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def _free_block_table(self, block_table: BlockTable) -> None:\n        # when using a sliding window, each seq will only use up\n        # to `self.block_sliding_window` blocks. When freeing\n        # the block table, we must make sure to not free blocks more\n        # than once. If no sliding window is used, there is no block\n        # reuse in the block table, so we must free all blocks.\n        blocks_to_free = (block_table[-self.block_sliding_window:]\n                          if self.block_sliding_window is not None else\n                          block_table)\n        for block in set(blocks_to_free):\n            if block.device == Device.GPU:\n                self.gpu_allocator.free(block)\n            else:\n                self.cpu_allocator.free(block)\n\n    def free(self, seq: Sequence) -> None:\n        if seq.seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n        block_table = self.block_tables[seq.seq_id]\n        self._free_block_table(block_table)\n        del self.block_tables[seq.seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        if seq_group.request_id not in self.cross_block_tables:\n            # Already freed or hasn't ben scheduled yet.\n            return\n        block_table = self.cross_block_tables[seq_group.request_id]\n        self._free_block_table(block_table)\n        del self.cross_block_tables[seq_group.request_id]\n\n    def reset(self) -> None:\n        # Free decoder block tables\n        for block_table in self.block_tables.values():\n            self._free_block_table(block_table)\n        self.block_tables.clear()\n        # Free cross-attention block tables\n        for block_table in self.cross_block_tables.values():\n            self._free_block_table(block_table)\n        self.cross_block_tables.clear()\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        return self.block_tables[seq.seq_id].ids()\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        block_table = self.cross_block_tables[seq_group.request_id]\n        return [block.block_number for block in block_table]\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.gpu_allocator.get_num_free_blocks()\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.cpu_allocator.get_num_free_blocks()\n\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        if self.enable_caching:\n            # Update the last accessed time of all the blocks accessed\n            # in this step.\n            block_table = self.block_tables[seq.seq_id]\n            for block in block_table:\n                block.last_accessed = access_time\n\n    def compute_full_blocks_in_seq(self, seq: Sequence):\n        if seq.seq_id not in self.block_tables:\n            return\n        max_full_block = seq.get_len() // self.block_size - 1\n        block_table = self.block_tables[seq.seq_id]\n        if max_full_block == -1:\n            return\n        for i in reversed(range(max_full_block)):\n            if block_table[i].computed:\n                break\n            block_table[i].computed = True\n\n    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n        if seq.seq_id not in self.block_tables:\n            return []\n        block_table = self.block_tables[seq.seq_id]\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n        return [\n            b.block_number\n            for b in takewhile(lambda b: b.computed, block_table[:-1])\n        ]\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Return the block ids that are common for a given sequence group.\n\n        Used in prefill (can skip prefill of some blocks).\n        \"\"\"\n        # Can return non-empty result only with prefix caching enabled.\n        if not self.enable_caching:\n            return []\n\n        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n        return commonprefix([ids for ids in ids_list if ids != []])\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        if self.enable_caching:\n            for seq in seq_group.get_seqs():\n                self.compute_full_blocks_in_seq(seq)\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        if device == Device.GPU:\n            return self.gpu_allocator.get_prefix_cache_hit_rate()\n        if device == Device.CPU:\n            return self.cpu_allocator.get_prefix_cache_hit_rate()\n        raise ValueError(f\"Invalid device: {device}\")\n",
      "diff": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex 666723313..24ab9eb66 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -681,14 +681,20 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n             for block in block_table:\n                 block.last_accessed = access_time\n \n-    def compute_full_blocks_in_seq(self, seq: Sequence):\n+    def compute_full_blocks_in_seq(self, seq: Sequence, token_chunk_size: int):\n         if seq.seq_id not in self.block_tables:\n             return\n-        max_full_block = seq.get_len() // self.block_size - 1\n+\n+        # When chunked prefill is enabled, the computed full blocks\n+        # should be calculated based on the number of computed tokens.\n+        max_computed_tokens = (seq.data.get_num_computed_tokens() +\n+                               token_chunk_size)\n+        computed_full_blocks = max_computed_tokens // self.block_size\n+\n         block_table = self.block_tables[seq.seq_id]\n-        if max_full_block == -1:\n+        if computed_full_blocks == 0:\n             return\n-        for i in reversed(range(max_full_block)):\n+        for i in reversed(range(computed_full_blocks)):\n             if block_table[i].computed:\n                 break\n             block_table[i].computed = True\n@@ -718,10 +724,11 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n         return commonprefix([ids for ids in ids_list if ids != []])\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         if self.enable_caching:\n             for seq in seq_group.get_seqs():\n-                self.compute_full_blocks_in_seq(seq)\n+                self.compute_full_blocks_in_seq(seq, token_chunk_size)\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:\n         if device == Device.GPU:",
      "change_type": "modified",
      "lines_added": 14,
      "lines_removed": 7
    },
    {
      "file_path": "vllm/core/block_manager_v2.py",
      "old_content": "\"\"\"A block manager that manages token blocks.\"\"\"\nfrom itertools import chain\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Tuple\n\nfrom vllm.core.block.block_table import BlockTable\nfrom vllm.core.block.cpu_gpu_block_allocator import CpuGpuBlockAllocator\nfrom vllm.core.block.interfaces import Block\nfrom vllm.core.block.prefix_caching_block import (ComputedBlocksTracker,\n                                                  LastAccessBlocksTracker)\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nSeqId = int\nEncoderSeqId = str\n\n\nclass BlockSpaceManagerV2(BlockSpaceManager):\n    \"\"\"BlockSpaceManager which manages the allocation of KV cache.\n\n    It owns responsibility for allocation, swapping, allocating memory for\n    autoregressively-generated tokens, and other advanced features such as\n    prefix caching, forking/copy-on-write, and sliding-window memory allocation.\n\n    The current implementation is partial; in particular prefix caching and\n    sliding-window are not feature complete. This class implements the design\n    described in https://github.com/vllm-project/vllm/pull/3492.\n\n    Lookahead slots\n        The block manager has the notion of a \"lookahead slot\". These are slots\n        in the KV cache that are allocated for a sequence. Unlike the other\n        allocated slots, the content of these slots is undefined -- the worker\n        may use the memory allocations in any way.\n\n        In practice, a worker could use these lookahead slots to run multiple\n        forward passes for a single scheduler invocation. Each successive\n        forward pass would write KV activations to the corresponding lookahead\n        slot. This allows low inter-token latency use-cases, where the overhead\n        of continuous batching scheduling is amortized over >1 generated tokens.\n\n        Speculative decoding uses lookahead slots to store KV activations of\n        proposal tokens.\n\n        See https://github.com/vllm-project/vllm/pull/3250 for more information\n        on lookahead scheduling.\n\n    Args:\n        block_size (int): The size of each memory block.\n        num_gpu_blocks (int): The number of memory blocks allocated on GPU.\n        num_cpu_blocks (int): The number of memory blocks allocated on CPU.\n        watermark (float, optional): The threshold used for memory swapping.\n            Defaults to 0.01.\n        sliding_window (Optional[int], optional): The size of the sliding\n            window. Defaults to None.\n        enable_caching (bool, optional): Flag indicating whether caching is\n            enabled. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        self.sliding_window = sliding_window\n        # max_block_sliding_window is the max number of blocks that need to be\n        # allocated\n        self.max_block_sliding_window = None\n        if sliding_window is not None:\n            # +1 here because // rounds down\n            num_blocks = sliding_window // block_size + 1\n            # +1 here because the last block may not be full,\n            # and so the sequence stretches one more block at the beginning\n            # For example, if sliding_window is 3 and block_size is 4,\n            # we may need 2 blocks when the second block only holds 1 token.\n            self.max_block_sliding_window = num_blocks + 1\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        self.block_allocator = CpuGpuBlockAllocator.create(\n            allocator_type=\"prefix_caching\" if enable_caching else \"naive\",\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            block_size=block_size,\n        )\n\n        self.block_tables: Dict[SeqId, BlockTable] = {}\n        self.cross_block_tables: Dict[EncoderSeqId, BlockTable] = {}\n\n        self._computed_blocks_tracker = ComputedBlocksTracker(\n            self.block_allocator)\n        self._last_access_blocks_tracker = LastAccessBlocksTracker(\n            self.block_allocator)\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n        num_required_blocks = BlockTable.get_num_required_blocks(\n            seq.get_token_ids(),\n            block_size=self.block_size,\n        )\n\n        if seq_group.is_encoder_decoder():\n            encoder_seq = seq_group.get_encoder_seq()\n            assert encoder_seq is not None\n            num_required_blocks += BlockTable.get_num_required_blocks(\n                encoder_seq.get_token_ids(),\n                block_size=self.block_size,\n            )\n\n        if self.max_block_sliding_window is not None:\n            num_required_blocks = min(num_required_blocks,\n                                      self.max_block_sliding_window)\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            device=Device.GPU)\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, seq: Sequence) -> BlockTable:\n        block_table = BlockTable(\n            block_size=self.block_size,\n            block_allocator=self.block_allocator,\n            max_block_sliding_window=self.max_block_sliding_window,\n        )\n        block_table.allocate(seq.get_token_ids())\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n\n        # Allocate self-attention block tables for decoder sequences\n        waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n        assert not (set(seq.seq_id for seq in waiting_seqs)\n                    & self.block_tables.keys()), \"block table already exists\"\n\n        # NOTE: Here we assume that all sequences in the group have the same\n        # prompt.\n        seq = waiting_seqs[0]\n        block_table: BlockTable = self._allocate_sequence(seq)\n        self.block_tables[seq.seq_id] = block_table\n\n        # Track seq\n        self._computed_blocks_tracker.add_seq(seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Assign the block table for each sequence.\n        for seq in waiting_seqs[1:]:\n            self.block_tables[seq.seq_id] = block_table.fork()\n\n            # Track seq\n            self._computed_blocks_tracker.add_seq(seq.seq_id)\n            self._last_access_blocks_tracker.add_seq(seq.seq_id)\n\n        # Allocate cross-attention block table for encoder sequence\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # encoder prompt.\n        request_id = seq_group.request_id\n\n        assert (request_id\n                not in self.cross_block_tables), \\\n                \"block table already exists\"\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        if seq_group.is_encoder_decoder():\n            encoder_seq = seq_group.get_encoder_seq()\n            assert encoder_seq is not None\n            block_table = self._allocate_sequence(encoder_seq)\n            self.cross_block_tables[request_id] = block_table\n\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        \"\"\"Determine if there is enough space in the GPU KV cache to continue\n        generation of the specified sequence group.\n\n        We use a worst-case heuristic: assume each touched block will require a\n        new allocation (either via CoW or new block). We can append slots if the\n        number of touched blocks is less than the number of free blocks.\n\n        \"Lookahead slots\" are slots that are allocated in addition to the slots\n        for known tokens. The contents of the lookahead slots are not defined.\n        This is used by speculative decoding when speculating future tokens.\n        \"\"\"\n\n        num_touched_blocks = 0\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            block_table = self.block_tables[seq.seq_id]\n\n            num_touched_blocks += (\n                block_table.get_num_blocks_touched_by_append_slots(\n                    token_ids=block_table.get_unseen_token_ids(\n                        seq.get_token_ids()),\n                    num_lookahead_slots=num_lookahead_slots,\n                ))\n\n        num_free_gpu_blocks = self.block_allocator.get_num_free_blocks(\n            Device.GPU)\n        return num_touched_blocks <= num_free_gpu_blocks\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n\n        block_table = self.block_tables[seq.seq_id]\n\n        block_table.append_token_ids(\n            token_ids=block_table.get_unseen_token_ids(seq.get_token_ids()),\n            num_lookahead_slots=num_lookahead_slots,\n            num_computed_slots=seq.data.get_num_computed_tokens(),\n        )\n        # Return any new copy-on-writes.\n        new_cows = self.block_allocator.clear_copy_on_writes()\n        return new_cows\n\n    def free(self, seq: Sequence) -> None:\n        seq_id = seq.seq_id\n\n        if seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n\n        # Update seq block ids with the latest access time\n        self._last_access_blocks_tracker.update_seq_blocks_last_access(\n            seq_id, self.block_tables[seq.seq_id].physical_block_ids)\n\n        # Untrack seq\n        self._last_access_blocks_tracker.remove_seq(seq_id)\n        self._computed_blocks_tracker.remove_seq(seq_id)\n\n        # Free table/blocks\n        self.block_tables[seq_id].free()\n        del self.block_tables[seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        request_id = seq_group.request_id\n        if request_id not in self.cross_block_tables:\n            # Already freed or hasn't been scheduled yet.\n            return\n        self.cross_block_tables[request_id].free()\n        del self.cross_block_tables[request_id]\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        block_ids = self.block_tables[seq.seq_id].physical_block_ids\n        return block_ids  # type: ignore\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        request_id = seq_group.request_id\n        assert request_id in self.cross_block_tables\n        block_ids = self.cross_block_tables[request_id].physical_block_ids\n        assert all(b is not None for b in block_ids)\n        return block_ids  # type: ignore\n\n    def access_all_blocks_in_seq(self, seq: Sequence, now: float):\n        if self.enable_caching:\n            # Record the latest access time for the sequence. The actual update\n            # of the block ids is deferred to the sequence free(..) call, since\n            # only during freeing of block ids, the blocks are actually added to\n            # the evictor (which is when the most updated time is required)\n            # (This avoids expensive calls to mark_blocks_as_accessed(..))\n            self._last_access_blocks_tracker.update_last_access(\n                seq.seq_id, now)\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        # If prefix caching is enabled, mark immutable blocks as computed\n        # right after they have been scheduled (for prefill). This assumes\n        # the scheduler is synchronous so blocks are actually computed when\n        # scheduling the next batch.\n        self.block_allocator.mark_blocks_as_computed([])\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Determine which blocks for which we skip prefill.\n\n        With prefix caching we can skip prefill for previously-generated blocks.\n        Currently, the attention implementation only supports skipping cached\n        blocks if they are a contiguous prefix of cached blocks.\n\n        This method determines which blocks can be safely skipped for all\n        sequences in the sequence group.\n        \"\"\"\n        computed_seq_block_ids = []\n        for seq in seqs:\n            computed_seq_block_ids.append(\n                self._computed_blocks_tracker.\n                get_cached_computed_blocks_and_update(\n                    seq.seq_id,\n                    self.block_tables[seq.seq_id].physical_block_ids))\n\n        # NOTE(sang): This assumes seq_block_ids doesn't contain any None.\n        return self.block_allocator.get_common_computed_block_ids(\n            computed_seq_block_ids)  # type: ignore\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.fork()\n\n        # Track child seq\n        self._computed_blocks_tracker.add_seq(child_seq.seq_id)\n        self._last_access_blocks_tracker.add_seq(child_seq.seq_id)\n\n    def can_swap_in(self, seq_group: SequenceGroup,\n                    num_lookahead_slots: int) -> AllocStatus:\n        \"\"\"Returns the AllocStatus for the given sequence_group \n        with num_lookahead_slots.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            AllocStatus: The AllocStatus for the given sequence group.\n        \"\"\"\n        return self._can_swap(seq_group, Device.GPU, SequenceStatus.SWAPPED,\n                              num_lookahead_slots)\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from CPU to GPU) generated by\n        swapping in the given seq_group with num_lookahead_slots.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from CPU \n                to GPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.CPU,\n                                                         dst_device=Device.GPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id)\n                for cpu_block_id, gpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        \"\"\"Returns whether we can swap out the given sequence_group \n        with num_lookahead_slots.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group to swap in.\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            bool: Whether it's possible to swap out current sequence group.\n        \"\"\"\n        alloc_status = self._can_swap(seq_group, Device.CPU,\n                                      SequenceStatus.RUNNING)\n        if alloc_status == AllocStatus.OK:\n            return True\n        return False\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        \"\"\"Returns the block id mapping (from GPU to CPU) generated by\n        swapping out the given sequence_group with num_lookahead_slots.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n\n        Returns:\n            List[Tuple[int, int]]: The mapping of swapping block from \n                GPU to CPU.\n        \"\"\"\n        physical_block_id_mapping = []\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            blocks = self.block_tables[seq.seq_id].blocks\n            if len(blocks) == 0:\n                continue\n\n            seq_swap_mapping = self.block_allocator.swap(blocks=blocks,\n                                                         src_device=Device.GPU,\n                                                         dst_device=Device.CPU)\n\n            # Refresh the block ids of the table (post-swap)\n            self.block_tables[seq.seq_id].update(blocks)\n\n            seq_physical_block_id_mapping = {\n                self.block_allocator.get_physical_block_id(\n                    Device.GPU, gpu_block_id):\n                self.block_allocator.get_physical_block_id(\n                    Device.CPU, cpu_block_id)\n                for gpu_block_id, cpu_block_id in seq_swap_mapping.items()\n            }\n\n            physical_block_id_mapping.extend(\n                list(seq_physical_block_id_mapping.items()))\n\n        return physical_block_id_mapping\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.GPU)\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.block_allocator.get_num_free_blocks(Device.CPU)\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        return self.block_allocator.get_prefix_cache_hit_rate(device)\n\n    def _can_swap(self,\n                  seq_group: SequenceGroup,\n                  device: Device,\n                  status: SequenceStatus,\n                  num_lookahead_slots: int = 0) -> AllocStatus:\n        \"\"\"Returns the AllocStatus for swapping in/out the given sequence_group \n        on to the 'device'.\n\n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            device (Device): device to swap the 'seq_group' on.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n            num_lookahead_slots (int): Number of lookahead slots used in \n                speculative decoding, default to 0.\n\n        Returns:\n            AllocStatus: The AllocStatus for swapping in/out the given \n                sequence_group on to the 'device'.\n        \"\"\"\n        blocks = self._get_blocks_for_swap(seq_group, status)\n        num_blocks_touched = self.block_allocator.get_num_blocks_touched(\n            blocks, device, num_lookahead_slots)\n        watermark_blocks = 0\n        if device == Device.GPU:\n            watermark_blocks = self.watermark_blocks\n        if self.block_allocator.get_num_total_blocks(\n                device) < num_blocks_touched:\n            return AllocStatus.NEVER\n        elif self.block_allocator.get_num_free_blocks(\n                device) - num_blocks_touched >= watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _get_blocks_for_swap(self, seq_group: SequenceGroup,\n                             status: SequenceStatus) -> List[Block]:\n        \"\"\"Returns the list of blocks those are touched by the seq_group\n        \n        Args:\n            sequence_group (SequenceGroup): The sequence group to swap in.\n            status (SequenceStatus): The status of sequence which is needed\n                for action. RUNNING for swap out and SWAPPED for swap in\n        \n        Returns:\n            The list of blocks those are touched by the seq_group.\n        \"\"\"\n        blocks: Dict[int, List[Block]] = {}\n        for seq in seq_group.get_seqs(status=status):\n            block_table = self.block_tables[seq.seq_id]\n            if block_table.blocks is not None:\n                blocks[seq.seq_id] = block_table.blocks\n        combined_blocks = list(chain(*blocks.values()))\n        return combined_blocks\n",
      "diff": "diff --git a/vllm/core/block_manager_v2.py b/vllm/core/block_manager_v2.py\nindex 7d2db43cb..b06385b06 100644\n--- a/vllm/core/block_manager_v2.py\n+++ b/vllm/core/block_manager_v2.py\n@@ -290,7 +290,8 @@ class BlockSpaceManagerV2(BlockSpaceManager):\n             self._last_access_blocks_tracker.update_last_access(\n                 seq.seq_id, now)\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         # If prefix caching is enabled, mark immutable blocks as computed\n         # right after they have been scheduled (for prefill). This assumes\n         # the scheduler is synchronous so blocks are actually computed when",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/core/embedding_model_block_manager.py",
      "old_content": "from typing import List, Tuple\n\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.sequence import Sequence, SequenceGroup\nfrom vllm.utils import Device\n\n\nclass EmbeddingModelBlockSpaceManager(BlockSpaceManager):\n    \"\"\"An embedding version of BlockSpaceManager for use in environments\n    with embedding models where block management is not required.\n\n    This class provides the same interface as BlockSpaceManager, but its\n    methods perform no actions or return simple values like True in specific\n    actions. It's designed to be used in scenarios where the overhead of\n    block management is unnecessary, such as in an embedding environment.\n    \"\"\"\n\n    def __init__(\n        self,\n        **kwargs,\n    ) -> None:\n        pass\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # Always return OK for dummy purposes\n        return AllocStatus.OK\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        # No actual allocation logic needed\n        pass\n\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        return True\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n        return None  # type: ignore\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        pass\n\n    def can_swap_in(self, seq_group: SequenceGroup,\n                    num_lookahead_slots: int) -> AllocStatus:\n        return AllocStatus.OK\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        return None  # type: ignore\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        return True\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        return None  # type: ignore\n\n    def free(self, seq: Sequence) -> None:\n        # No operation on free\n        return\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        return None  # type: ignore\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return 1\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return 1\n\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        pass\n\n    def get_common_computed_block_ids(self,\n                                      seq_group: List[Sequence]) -> List[int]:\n        return []\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        pass\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        return -1\n",
      "diff": "diff --git a/vllm/core/embedding_model_block_manager.py b/vllm/core/embedding_model_block_manager.py\nindex f16f66e99..c47d7d8df 100644\n--- a/vllm/core/embedding_model_block_manager.py\n+++ b/vllm/core/embedding_model_block_manager.py\n@@ -80,7 +80,8 @@ class EmbeddingModelBlockSpaceManager(BlockSpaceManager):\n                                       seq_group: List[Sequence]) -> List[int]:\n         return []\n \n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     def get_prefix_cache_hit_rate(self, device: Device) -> float:",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/core/interfaces.py",
      "old_content": "import enum\nfrom abc import ABC, abstractmethod\nfrom typing import List\nfrom typing import Sequence as GenericSequence\nfrom typing import Tuple\n\nfrom vllm.sequence import Sequence, SequenceGroup\nfrom vllm.utils import Device\n\n\nclass AllocStatus(enum.Enum):\n    \"\"\"Result for BlockSpaceManager.can_allocate\n\n    1. Ok: seq_group can be allocated now.\n    2. Later: seq_group cannot be allocated.\n      The capacity of allocator is larger than seq_group required.\n    3. Never: seq_group can never be allocated.\n      The seq_group is too large to allocated in GPU.\n    \"\"\"\n    OK = enum.auto()\n    LATER = enum.auto()\n    NEVER = enum.auto()\n\n\nclass BlockSpaceManager(ABC):\n\n    @staticmethod\n    def get_block_space_manager_class(version: str):\n        version = version.lower()\n\n        if version == \"v1\":\n            from vllm.core.block_manager_v1 import BlockSpaceManagerV1\n            return BlockSpaceManagerV1\n\n        if version == \"v2\":\n            from vllm.core.block_manager_v2 import BlockSpaceManagerV2\n            return BlockSpaceManagerV2\n\n        if version == \"embedding\":\n            from vllm.core.embedding_model_block_manager import (\n                EmbeddingModelBlockSpaceManager)\n            return EmbeddingModelBlockSpaceManager\n\n        raise ValueError(f\"Unknown version {version=}\")\n\n    @abstractmethod\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        pass\n\n    @abstractmethod\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        pass\n\n    @abstractmethod\n    def can_append_slots(self, seq_group: SequenceGroup,\n                         num_lookahead_slots: int) -> bool:\n        pass\n\n    @abstractmethod\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int,\n    ) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        pass\n\n    @abstractmethod\n    def can_swap_in(self, seq_group: SequenceGroup,\n                    num_lookahead_slots: int) -> AllocStatus:\n        pass\n\n    @abstractmethod\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        pass\n\n    @abstractmethod\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        pass\n\n    @abstractmethod\n    def free(self, seq: Sequence) -> None:\n        pass\n\n    @abstractmethod\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        pass\n\n    @abstractmethod\n    def get_num_free_gpu_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_free_cpu_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        pass\n\n    @abstractmethod\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        pass\n\n    @abstractmethod\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        pass\n\n    @abstractmethod\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        \"\"\"Prefix cache hit rate. -1 means not supported or disabled.\"\"\"\n        pass\n",
      "diff": "diff --git a/vllm/core/interfaces.py b/vllm/core/interfaces.py\nindex becd0d2e7..96f8dd851 100644\n--- a/vllm/core/interfaces.py\n+++ b/vllm/core/interfaces.py\n@@ -115,7 +115,8 @@ class BlockSpaceManager(ABC):\n         pass\n \n     @abstractmethod\n-    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n+    def mark_blocks_as_computed(self, seq_group: SequenceGroup,\n+                                token_chunk_size: int):\n         pass\n \n     @abstractmethod",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/core/scheduler.py",
      "old_content": "import enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import (Callable, Deque, Dict, Iterable, List, Optional, Set,\n                    Tuple, Union)\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceGroupMetadataDelta,\n                           SequenceStatus)\nfrom vllm.utils import Device, PyObjectCache\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    # Optimization for fast-access to seq_group lists\n    decode_seq_groups_list: List[SequenceGroup]\n    prefill_seq_groups_list: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            decode_seq_groups_list=[],\n            prefill_seq_groups_list=[],\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[ScheduledSequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[ScheduledSequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[ScheduledSequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\ndef seq_group_metadata_builder():\n    return SequenceGroupMetadata(request_id=\"\",\n                                 is_prompt=False,\n                                 seq_data={},\n                                 sampling_params=None,\n                                 block_tables={})\n\n\ndef scheduler_running_outputs_builder():\n    return SchedulerRunningOutputs(decode_seq_groups=[],\n                                   prefill_seq_groups=[],\n                                   preempted=[],\n                                   swapped_out=[],\n                                   blocks_to_swap_out=[],\n                                   blocks_to_copy=[],\n                                   num_lookahead_slots=0,\n                                   prefill_seq_groups_list=[],\n                                   decode_seq_groups_list=[])\n\n\ndef scheduled_seq_group_builder():\n    return ScheduledSequenceGroup(SequenceGroup(\"\", [], -1),\n                                  token_chunk_size=0)\n    # return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n        output_proc_callback: Optional[Callable] = None,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n        # Used to cache python objects\n        self._seq_group_metadata_cache: List[PyObjectCache] = []\n        self._scheduler_running_outputs_cache: List[PyObjectCache] = []\n        self._scheduled_seq_group_cache: List[PyObjectCache] = []\n\n        # For async output processing, we need to swap cache buffers between\n        # iterations. I.e. since the output processing is lagged one step,\n        # we cannot reuse the cached objects immediately when the schedule()\n        # is called again, but only when schedule() is called the second time.\n        self.output_proc_callback = output_proc_callback\n        self.use_async_output_proc = self.output_proc_callback is not None\n        self.num_cache_iters = 2 if self.use_async_output_proc else 1\n\n        self.cache_id = 0\n        for i in range(self.num_cache_iters):\n            self._seq_group_metadata_cache.append(\n                PyObjectCache(seq_group_metadata_builder))\n            self._scheduler_running_outputs_cache.append(\n                PyObjectCache(scheduler_running_outputs_builder))\n            self._scheduled_seq_group_cache.append(\n                PyObjectCache(scheduled_seq_group_builder))\n\n        # For async postprocessor, the extra decode run cannot be done\n        # when the request reaches max_model_len. In this case, the request\n        # will be stopped during schedule() call and added to this stop list\n        # for processing and deallocation by the free_finished_seq_groups()\n        self._async_stopped: List[SequenceGroup] = []\n\n    @property\n    def next_cache_id(self):\n        return (self.cache_id + 1) % self.num_cache_iters\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\n        if isinstance(request_id, str):\n            request_id = (request_id, )\n        request_ids = set(request_id)\n        for state_queue in [self.waiting, self.running, self.swapped]:\n            aborted_groups: List[SequenceGroup] = []\n            for seq_group in state_queue:\n                if not request_ids:\n                    # Using 'break' here may add two extra iterations,\n                    # but is acceptable to reduce complexity.\n                    break\n                if seq_group.request_id in request_ids:\n                    # Appending aborted group into pending list.\n                    aborted_groups.append(seq_group)\n                    request_ids.remove(seq_group.request_id)\n            for aborted_group in aborted_groups:\n                # Remove the sequence group from the state queue.\n                state_queue.remove(aborted_group)\n                # Remove the aborted request from the Mamba cache.\n                self._finished_requests_ids.append(aborted_group.request_id)\n                for seq in aborted_group.get_seqs():\n                    if seq.is_finished():\n                        continue\n                    seq.status = SequenceStatus.FINISHED_ABORTED\n                    self.free_seq(seq)\n\n                self._free_seq_group_cross_attn_blocks(aborted_group)\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_prefix_cache_hit_rate(self, device: Device) -> float:\n        return self.block_manager.get_prefix_cache_hit_rate(device)\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n    def _schedule_running(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerRunningOutputs:\n        \"\"\"Schedule sequence groups that are running.\n\n        Running queue should include decode and chunked prefill requests.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any decodes are preempted.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any decodes are preempted.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n    \n        Returns:\n            SchedulerRunningOutputs.\n        \"\"\"\n        ret: SchedulerRunningOutputs = \\\n            self._scheduler_running_outputs_cache[self.cache_id].get_object()\n        ret.blocks_to_swap_out.clear()\n        ret.blocks_to_copy.clear()\n        ret.decode_seq_groups.clear()\n        ret.prefill_seq_groups.clear()\n        ret.preempted.clear()\n        ret.swapped_out.clear()\n\n        ret.num_lookahead_slots = self._get_num_lookahead_slots(\n            is_prefill=False)\n\n        ret.decode_seq_groups_list.clear()\n        ret.prefill_seq_groups_list.clear()\n\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_out: List[Tuple[int, int]] = ret.blocks_to_swap_out\n        blocks_to_copy: List[Tuple[int, int]] = ret.blocks_to_copy\n\n        decode_seq_groups: List[ScheduledSequenceGroup] = ret.decode_seq_groups\n        prefill_seq_groups: List[\n            ScheduledSequenceGroup] = ret.prefill_seq_groups\n        preempted: List[SequenceGroup] = ret.preempted\n        swapped_out: List[SequenceGroup] = ret.swapped_out\n\n        # NOTE(woosuk): Preemption happens only when there is no available slot\n        # to keep all the sequence groups in the RUNNING state.\n\n        # Store original running requests for the case of async + preemption\n        if self.use_async_output_proc:\n            orig_running = self.running.copy()\n\n        running_queue = self.running\n        assert len(self._async_stopped) == 0\n        while running_queue:\n            seq_group = running_queue[0]\n            num_running_tokens = self._get_num_new_tokens(\n                seq_group, SequenceStatus.RUNNING, enable_chunking, budget)\n\n            if num_running_tokens == 0:\n                break\n\n            running_queue.popleft()\n\n            # With async postprocessor, an extra decode run is done\n            # to process the final tokens. The check below avoids this extra\n            # decode run when the model max len is reached, in order to avoid\n            # a memory overflow.\n            if self.use_async_output_proc and seq_group.seqs[0].get_len(\n            ) > self.scheduler_config.max_model_len:\n                self._async_stopped.append(seq_group)\n                continue\n\n            # With async postprocessor, when preemption kicks in, we need\n            # first to drain the async postprocessor, so that all async\n            # block_table freeing is applied before the preemption freeing\n            # is applied.\n            if self.use_async_output_proc and not self._can_append_slots(\n                    seq_group):\n                tmp = self.running\n                self.running = orig_running\n                assert self.output_proc_callback is not None\n                self.output_proc_callback()\n                self.running = tmp\n\n            while not self._can_append_slots(seq_group):\n                budget.subtract_num_batched_tokens(seq_group.request_id,\n                                                   num_running_tokens)\n                num_running_seqs = seq_group.get_max_num_running_seqs()\n                budget.subtract_num_seqs(seq_group.request_id,\n                                         num_running_seqs)\n\n                if (curr_loras is not None and seq_group.lora_int_id > 0\n                        and seq_group.lora_int_id in curr_loras):\n                    curr_loras.remove(seq_group.lora_int_id)\n\n                if running_queue:\n                    # Preempt the lowest-priority sequence groups.\n                    victim_seq_group = running_queue.pop()\n                    preempted_mode = self._preempt(victim_seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(victim_seq_group)\n                    else:\n                        swapped_out.append(victim_seq_group)\n                else:\n                    # No other sequence groups can be preempted.\n                    # Preempt the current sequence group.\n                    preempted_mode = self._preempt(seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(seq_group)\n                    else:\n                        swapped_out.append(seq_group)\n                    break\n            else:\n                self._append_slots(seq_group, blocks_to_copy)\n                is_prefill = seq_group.is_prefill()\n\n                scheduled_seq_group: ScheduledSequenceGroup = \\\n                    self._scheduled_seq_group_cache[self.cache_id].get_object()\n                scheduled_seq_group.seq_group = seq_group\n                if is_prefill:\n                    scheduled_seq_group.token_chunk_size = num_running_tokens\n                    prefill_seq_groups.append(scheduled_seq_group)\n                    ret.prefill_seq_groups_list.append(seq_group)\n                else:\n                    scheduled_seq_group.token_chunk_size = 1\n                    decode_seq_groups.append(scheduled_seq_group)\n                    ret.decode_seq_groups_list.append(seq_group)\n\n                budget.add_num_batched_tokens(seq_group.request_id,\n                                              num_running_tokens)\n                # OPTIMIZATION:  Note that get_max_num_running_seqs is\n                # expensive. For the default scheduling chase where\n                # enable_chunking is False, num_seqs are updated before running\n                # this method, so we don't have to update it again here.\n                if enable_chunking:\n                    num_running_seqs = seq_group.get_max_num_running_seqs()\n                    budget.add_num_seqs(seq_group.request_id, num_running_seqs)\n                if curr_loras is not None and seq_group.lora_int_id > 0:\n                    curr_loras.add(seq_group.lora_int_id)\n\n        self._scheduler_running_outputs_cache[self.next_cache_id].reset()\n        self._scheduled_seq_group_cache[self.next_cache_id].reset()\n\n        return ret\n\n    def _schedule_swapped(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerSwappedInOutputs:\n        \"\"\"Schedule sequence groups that are swapped out.\n\n        It schedules swapped requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are swapped in.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are swapped in.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerSwappedInOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_in: List[Tuple[int, int]] = []\n        blocks_to_copy: List[Tuple[int, int]] = []\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        infeasible_seq_groups: List[SequenceGroup] = []\n\n        swapped_queue = self.swapped\n\n        leftover_swapped: Deque[SequenceGroup] = deque()\n        while swapped_queue:\n            seq_group = swapped_queue[0]\n\n            # If the sequence group cannot be swapped in, stop.\n            is_prefill = seq_group.is_prefill()\n            alloc_status = self.block_manager.can_swap_in(\n                seq_group, self._get_num_lookahead_slots(is_prefill))\n            if alloc_status == AllocStatus.LATER:\n                break\n            elif alloc_status == AllocStatus.NEVER:\n                logger.warning(\n                    \"Failing the request %s because there's not enough kv \"\n                    \"cache blocks to run the entire sequence.\",\n                    seq_group.request_id)\n                for seq in seq_group.get_seqs():\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                infeasible_seq_groups.append(seq_group)\n                swapped_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (lora_int_id > 0 and (lora_int_id not in curr_loras)\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_swapped.appendleft(seq_group)\n                    swapped_queue.popleft()\n                    continue\n\n            # The total number of sequences in the RUNNING state should not\n            # exceed the maximum number of sequences.\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.SWAPPED,\n                                                      enable_chunking, budget)\n\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            if lora_int_id > 0 and curr_loras is not None:\n                curr_loras.add(lora_int_id)\n            swapped_queue.popleft()\n            self._swap_in(seq_group, blocks_to_swap_in)\n            self._append_slots(seq_group, blocks_to_copy)\n            is_prefill = seq_group.is_prefill()\n            if is_prefill:\n                prefill_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group,\n                                           token_chunk_size=num_new_tokens))\n            else:\n                decode_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        swapped_queue.extendleft(leftover_swapped)\n\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False),\n            infeasible_seq_groups=infeasible_seq_groups,\n        )\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n    def _schedule_prefills(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerPrefillOutputs:\n        \"\"\"Schedule sequence groups that are in prefill stage.\n\n        Note that the current scheduler treats PREEMPTED_FOR_RECOMPUTE\n        as a new prefill (that starts from beginning -> most recently generated\n        tokens).\n\n        It schedules waiting requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are scheduled.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerPrefillOutputs.\n        \"\"\"\n        ignored_seq_groups: List[SequenceGroup] = []\n        seq_groups: List[ScheduledSequenceGroup] = []\n\n        waiting_queue = self.waiting\n\n        leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n        while self._passed_delay(time.time()) and waiting_queue:\n            seq_group = waiting_queue[0]\n\n            waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n            assert len(waiting_seqs) == 1, (\n                \"Waiting sequence group should have only one prompt \"\n                \"sequence.\")\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.WAITING,\n                                                      enable_chunking, budget)\n            if not enable_chunking:\n                num_prompt_tokens = waiting_seqs[0].get_len()\n                assert num_new_tokens == num_prompt_tokens\n\n            prompt_limit = self._get_prompt_limit(seq_group)\n            if num_new_tokens > prompt_limit:\n                logger.warning(\n                    \"Input prompt (%d tokens) is too long\"\n                    \" and exceeds limit of %d\", num_new_tokens, prompt_limit)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            # If the sequence group cannot be allocated, stop.\n            can_allocate = self.block_manager.can_allocate(seq_group)\n            if can_allocate == AllocStatus.LATER:\n                break\n            elif can_allocate == AllocStatus.NEVER:\n                logger.warning(\n                    \"Input prompt (%d tokens) is too long\"\n                    \" and exceeds the capacity of block_manager\",\n                    num_new_tokens)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (self.lora_enabled and lora_int_id > 0\n                        and lora_int_id not in curr_loras\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_waiting_sequences.appendleft(seq_group)\n                    waiting_queue.popleft()\n                    continue\n\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            # Can schedule this request.\n            if curr_loras is not None and lora_int_id > 0:\n                curr_loras.add(lora_int_id)\n            waiting_queue.popleft()\n            self._allocate_and_set_running(seq_group)\n            seq_group.init_multi_step(\n                num_scheduler_steps=self._get_num_lookahead_slots(\n                    is_prefill=True) + 1)\n            seq_groups.append(\n                ScheduledSequenceGroup(seq_group=seq_group,\n                                       token_chunk_size=num_new_tokens))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        # Queue requests that couldn't be scheduled.\n        waiting_queue.extendleft(leftover_waiting_sequences)\n        if len(seq_groups) > 0:\n            self.prev_prompt = True\n\n        return SchedulerPrefillOutputs(\n            seq_groups=seq_groups,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill=True))\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        if len(prefills.seq_groups) > 0:\n            self.running.extend([s.seq_group for s in prefills.seq_groups])\n\n        self.running.extend(running_scheduled.decode_seq_groups_list)\n\n        if len(swapped_in.decode_seq_groups) > 0:\n            self.running.extend(\n                [s.seq_group for s in swapped_in.decode_seq_groups])\n\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n\n        # Merge lists\n        num_prefill_groups = len(prefills.seq_groups)\n        if num_prefill_groups > 0:\n            scheduled_seq_groups = prefills.seq_groups\n            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n        else:\n            scheduled_seq_groups = running_scheduled.decode_seq_groups\n        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n\n        blocks_to_copy = running_scheduled.blocks_to_copy\n        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n\n        ignored_seq_groups = prefills.ignored_seq_groups\n        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n\n        return SchedulerOutputs(\n            scheduled_seq_groups=scheduled_seq_groups,\n            num_prefill_groups=num_prefill_groups,\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.prefill_seq_groups +\n                                  swapped_in.prefill_seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=(len(prefills.seq_groups) +\n                                len(swapped_in.prefill_seq_groups) +\n                                len(running_scheduled.prefill_seq_groups)),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=running_scheduled.blocks_to_copy +\n            swapped_in.blocks_to_copy,\n            ignored_seq_groups=prefills.ignored_seq_groups +\n            swapped_in.infeasible_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=(len(running_scheduled.preempted) +\n                       len(running_scheduled.swapped_out)),\n        )\n\n    def _schedule(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\"\"\"\n        if self.scheduler_config.chunked_prefill_enabled:\n            return self._schedule_chunked_prefill()\n        else:\n            return self._schedule_default()\n\n    def _can_append_slots(self, seq_group: SequenceGroup) -> bool:\n        \"\"\"Determine whether or not we have enough space in the KV cache to\n        continue generation of the sequence group.\n        \"\"\"\n        # It is True only for testing case to trigger artificial preemption.\n        if (self.enable_artificial_preemption\n                and random.uniform(0, 1) < ARTIFICIAL_PREEMPTION_PROB\n                and self.artificial_preempt_cnt > 0):\n            self.artificial_preempt_cnt -= 1\n            return False\n\n        # Appending slots only occurs in decoding.\n        is_prefill = False\n\n        return self.block_manager.can_append_slots(\n            seq_group=seq_group,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill),\n        )\n\n    def _allow_async_output_proc(self, seq_group: SequenceGroup) -> bool:\n        no_beam_search = seq_group.sampling_params is None or (\n            seq_group.sampling_params.best_of == 1\n            and not seq_group.sampling_params.use_beam_search)\n        return no_beam_search\n\n    def schedule(\n            self\n    ) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs, bool]:\n        # Schedule sequence groups.\n        # This function call changes the internal states of the scheduler\n        # such as self.running, self.swapped, and self.waiting.\n        scheduler_start_time = time.perf_counter()\n\n        scheduler_outputs = self._schedule()\n        now = time.time()\n\n        if not self.cache_config.enable_prefix_caching:\n            common_computed_block_nums = []\n\n        # TODO: Combine multi-step and async postprocessor\n        allow_async_output_proc: bool = (\n            self.use_async_output_proc\n            and not self.scheduler_config.is_multi_step)\n\n        # Create input data structures.\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        for i, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n            token_chunk_size = scheduled_seq_group.token_chunk_size\n            seq_group.maybe_set_first_scheduled_time(now)\n\n            seq_group_metadata = self._seq_group_metadata_cache[\n                self.cache_id].get_object()\n            seq_group_metadata.seq_data.clear()\n            seq_group_metadata.block_tables.clear()\n\n            # seq_id -> SequenceData\n            seq_data: Dict[int, SequenceData] = {}\n            # seq_id -> physical block numbers\n            block_tables: Dict[int, List[int]] = {}\n\n            if seq_group.is_encoder_decoder():\n                # Encoder associated with SequenceGroup\n                encoder_seq = seq_group.get_encoder_seq()\n                assert encoder_seq is not None\n                encoder_seq_data = encoder_seq.data\n                # Block table for cross-attention\n                # Also managed at SequenceGroup level\n                cross_block_table = self.block_manager.get_cross_block_table(\n                    seq_group)\n            else:\n                encoder_seq_data = None\n                cross_block_table = None\n\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n                seq_id = seq.seq_id\n                seq_data[seq_id] = seq.data\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                self.block_manager.access_all_blocks_in_seq(seq, now)\n\n            if self.cache_config.enable_prefix_caching:\n                common_computed_block_nums = (\n                    self.block_manager.get_common_computed_block_ids(\n                        seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n\n            do_sample = True\n            is_prompt = seq_group.is_prefill()\n            # We should send the metadata to workers when the first prefill\n            # is sent. Subsequent requests could be chunked prefill or decode.\n            is_first_prefill = False\n            if is_prompt:\n                seqs = seq_group.get_seqs()\n                # Prefill has only 1 sequence.\n                assert len(seqs) == 1\n                num_computed_tokens = seqs[0].data.get_num_computed_tokens()\n                is_first_prefill = num_computed_tokens == 0\n                # In the next iteration, all prompt tokens are not computed.\n                # It means the prefill is chunked, and we don't need sampling.\n                # NOTE: We use get_len instead of get_prompt_len because when\n                # a sequence is preempted, prefill includes previous generated\n                # output tokens.\n                if (token_chunk_size + num_computed_tokens <\n                        seqs[0].data.get_len()):\n                    do_sample = False\n\n            # It assumes the scheduled_seq_groups is ordered by\n            # prefill < decoding.\n            if is_first_prefill or not self.scheduler_config.send_delta_data:\n                seq_group_metadata = SequenceGroupMetadata(\n                    request_id=seq_group.request_id,\n                    is_prompt=is_prompt,\n                    seq_data=seq_data,\n                    sampling_params=seq_group.sampling_params,\n                    block_tables=block_tables,\n                    do_sample=do_sample,\n                    pooling_params=seq_group.pooling_params,\n                    token_chunk_size=token_chunk_size,\n                    lora_request=seq_group.lora_request,\n                    computed_block_nums=common_computed_block_nums,\n                    encoder_seq_data=encoder_seq_data,\n                    cross_block_table=cross_block_table,\n                    state=seq_group.state,\n                    # `multi_modal_data` will only be present for the 1st comm\n                    # between engine and worker.\n                    # the subsequent comms can still use delta, but\n                    # `multi_modal_data` will be None.\n                    multi_modal_data=seq_group.multi_modal_data\n                    if scheduler_outputs.num_prefill_groups > 0 else None,\n                    prompt_adapter_request=seq_group.prompt_adapter_request,\n                )\n            else:\n                # When SPMD mode is enabled, we only send delta data except for\n                # the first request to reduce serialization cost.\n                seq_data_delta = {}\n                for id, data in seq_data.items():\n                    seq_data_delta[id] = data.get_delta_and_reset()\n                seq_group_metadata = SequenceGroupMetadataDelta(\n                    seq_data_delta,\n                    seq_group.request_id,\n                    block_tables,\n                    is_prompt,\n                    do_sample=do_sample,\n                    token_chunk_size=token_chunk_size,\n                    computed_block_nums=common_computed_block_nums,\n                )\n            seq_group_metadata_list.append(seq_group_metadata)\n\n            if allow_async_output_proc:\n                allow_async_output_proc = self._allow_async_output_proc(\n                    seq_group)\n\n        # Now that the batch has been created, we can assume all blocks in the\n        # batch will have been computed before the next scheduling invocation.\n        # This is because the engine assumes that a failure in model execution\n        # will crash the vLLM instance / will not retry.\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            self.block_manager.mark_blocks_as_computed(\n                scheduled_seq_group.seq_group)\n\n        self._seq_group_metadata_cache[self.next_cache_id].reset()\n\n        scheduler_time = time.perf_counter() - scheduler_start_time\n        # Add this to scheduler time to all the sequences that are currently\n        # running. This will help estimate if the scheduler is a significant\n        # component in the e2e latency.\n        for seq_group in self.running:\n            if seq_group is not None and seq_group.metrics is not None:\n                if seq_group.metrics.scheduler_time is not None:\n                    seq_group.metrics.scheduler_time += scheduler_time\n                else:\n                    seq_group.metrics.scheduler_time = scheduler_time\n\n        # Move to next cache (if exists)\n        self.cache_id = self.next_cache_id\n\n        # Return results\n        return (seq_group_metadata_list, scheduler_outputs,\n                allow_async_output_proc)\n\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        self.block_manager.fork(parent_seq, child_seq)\n\n    def free_seq(self, seq: Sequence) -> None:\n        \"\"\"Free a sequence from a block table.\"\"\"\n        self.block_manager.free(seq)\n\n    def _free_finished_seqs(self, seq_group: SequenceGroup) -> None:\n        \"\"\"Free finished seqs in a sequence group.\"\"\"\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                self.free_seq(seq)\n\n    def free_finished_seq_groups(self) -> None:\n        remaining: Deque[SequenceGroup] = deque()\n        for seq_group in self.running:\n            if seq_group.is_finished():\n                # Free cross-attention block table, if it exists\n                self._free_seq_group_cross_attn_blocks(seq_group)\n                # Add the finished requests to the finished requests list.\n                # This list will be used to update the Mamba cache in the\n                # next step.\n                self._finished_requests_ids.append(seq_group.request_id)\n            else:\n                remaining.append(seq_group)\n\n            # Free finished seqs\n            self._free_finished_seqs(seq_group)\n\n        self.running = remaining\n\n        # Handle async stopped sequence groups\n        # (ones that reached max model len)\n        if self._async_stopped:\n            for seq_group in self._async_stopped:\n                self._free_seq_group_cross_attn_blocks(seq_group)\n                self._finished_requests_ids.append(seq_group.request_id)\n\n                # Free finished seqs\n                self._free_finished_seqs(seq_group)\n\n            self._async_stopped.clear()\n\n    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:\n        self.block_manager.allocate(seq_group)\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            seq.status = SequenceStatus.RUNNING\n\n    def _append_slots(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_copy: List[Tuple[int, int]],\n    ) -> None:\n        \"\"\"Appends new slots to the sequences in the given sequence group.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group containing the\n                sequences to append slots to.\n            blocks_to_copy (List[Tuple[int, int]]): A list of tuple of two\n                ints, the first int is the source block index, and the second\n                int is the destination block index. This list is updated with\n                the new source and destination block indices for the appended\n                slots.\n        \"\"\"\n        num_lookahead_slots = self._get_num_lookahead_slots(is_prefill=False)\n        seq_group.init_multi_step(num_scheduler_steps=num_lookahead_slots + 1)\n\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n            if len(cows) > 0:\n                blocks_to_copy.extend(cows)\n\n    def _preempt(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n        preemption_mode: Optional[PreemptionMode] = None,\n    ) -> PreemptionMode:\n        # If preemption mode is not specified, we determine the mode as follows:\n        # We use recomputation by default since it incurs lower overhead than\n        # swapping. However, when the sequence group has multiple sequences\n        # (e.g., beam search), recomputation is not currently supported. In\n        # such a case, we use swapping instead.\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\n        # As swapped sequences are prioritized over waiting sequences,\n        # sequence groups with multiple sequences are implicitly prioritized\n        # over sequence groups with a single sequence.\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\n        # sequences. This may require a more sophisticated CUDA kernel.\n        if self.user_specified_preemption_mode is None:\n            if seq_group.get_max_num_running_seqs() == 1:\n                preemption_mode = PreemptionMode.RECOMPUTE\n            else:\n                preemption_mode = PreemptionMode.SWAP\n\n        elif self.user_specified_preemption_mode == \"swap\":\n            preemption_mode = PreemptionMode.SWAP\n        else:\n            preemption_mode = PreemptionMode.RECOMPUTE\n\n        if self.num_cumulative_preemption % 50 == 0:\n            logger.warning(\n                \"Sequence group %s is preempted by %s mode because there is \"\n                \"not enough KV cache space. This can affect the end-to-end \"\n                \"performance. Increase gpu_memory_utilization or \"\n                \"tensor_parallel_size to provide more KV cache memory. \"\n                \"total_num_cumulative_preemption=%d\", seq_group.request_id,\n                preemption_mode, self.num_cumulative_preemption + 1)\n        self.num_cumulative_preemption += 1\n\n        if preemption_mode == PreemptionMode.RECOMPUTE:\n            self._preempt_by_recompute(seq_group)\n        elif preemption_mode == PreemptionMode.SWAP:\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\n        else:\n            raise AssertionError(\"Invalid preemption mode.\")\n        return preemption_mode\n\n    def _preempt_by_recompute(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        assert len(seqs) == 1\n        for seq in seqs:\n            seq.status = SequenceStatus.WAITING\n            self.free_seq(seq)\n            seq.reset_state_for_recompute()\n\n    def _preempt_by_swap(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        self._swap_out(seq_group, blocks_to_swap_out)\n\n    def _swap_in(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_in: List[Tuple[int, int]],\n    ) -> None:\n        mapping = self.block_manager.swap_in(seq_group)\n        blocks_to_swap_in.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            seq.status = SequenceStatus.RUNNING\n\n    def _swap_out(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        if not self.block_manager.can_swap_out(seq_group):\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\n            # entire engine.\n            raise RuntimeError(\n                \"Aborted due to the lack of CPU swap space. Please increase \"\n                \"the swap space to avoid this error.\")\n        mapping = self.block_manager.swap_out(seq_group)\n        blocks_to_swap_out.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            seq.status = SequenceStatus.SWAPPED\n\n    def _passed_delay(self, now: float) -> bool:\n        if self.prev_prompt:\n            self.last_prompt_latency = now - self.prev_time\n        self.prev_time, self.prev_prompt = now, False\n        # Delay scheduling prompts to let waiting queue fill up\n        if self.scheduler_config.delay_factor > 0 and self.waiting:\n            earliest_arrival_time = min(\n                [e.metrics.arrival_time for e in self.waiting])\n            passed_delay = (\n                (now - earliest_arrival_time) >\n                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n                or not self.running)\n        else:\n            passed_delay = True\n        return passed_delay\n\n    def _get_num_lookahead_slots(self, is_prefill: bool) -> int:\n        \"\"\"The number of slots to allocate per sequence per step, beyond known\n        token ids. Speculative decoding uses these slots to store KV activations\n        of tokens which may or may not be accepted.\n\n        Speculative decoding does not yet support prefill, so we do not perform\n        lookahead allocation for prefill.\n        \"\"\"\n        if is_prefill:\n            return 0\n\n        return self.scheduler_config.num_lookahead_slots\n\n    def _get_num_new_tokens(self, seq_group: SequenceGroup,\n                            status: SequenceStatus, enable_chunking: bool,\n                            budget: SchedulingBudget) -> int:\n        \"\"\"Get the next new tokens to compute for a given sequence group\n            that's in a given `status`.\n\n        The API could chunk the number of tokens to compute based on `budget`\n        if `enable_chunking` is True. If a sequence group has multiple\n        sequences (e.g., running beam search), it means it is in decoding\n        phase, so chunking doesn't happen.\n\n        Returns 0 if the new token cannot be computed due to token budget.\n        \"\"\"\n        num_new_tokens = 0\n        seqs = seq_group.get_seqs(status=status)\n        for seq in seqs:\n            num_new_tokens += seq.get_num_new_tokens()\n        assert num_new_tokens > 0\n        # Chunk if a running request cannot fit in.\n        # If number of seq > 1, it means it is doing beam search in a\n        # decode phase. Do not chunk in that case.\n        if enable_chunking and len(seqs) == 1:\n            num_new_tokens = min(num_new_tokens,\n                                 budget.remaining_token_budget())\n        return num_new_tokens\n",
      "diff": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex fbc53afa3..51fde6e4e 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -1226,7 +1226,8 @@ class Scheduler:\n         # will crash the vLLM instance / will not retry.\n         for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n             self.block_manager.mark_blocks_as_computed(\n-                scheduled_seq_group.seq_group)\n+                scheduled_seq_group.seq_group,\n+                scheduled_seq_group.token_chunk_size)\n \n         self._seq_group_metadata_cache[self.next_cache_id].reset()\n \n@@ -1457,10 +1458,27 @@ class Scheduler:\n         for seq in seqs:\n             num_new_tokens += seq.get_num_new_tokens()\n         assert num_new_tokens > 0\n-        # Chunk if a running request cannot fit in.\n-        # If number of seq > 1, it means it is doing beam search in a\n-        # decode phase. Do not chunk in that case.\n+        # Chunk if a running request cannot fit in the given budget.\n+        # If number of seq > 1, it means it is doing beam search\n+        # in a decode phase. Do not chunk.\n         if enable_chunking and len(seqs) == 1:\n-            num_new_tokens = min(num_new_tokens,\n-                                 budget.remaining_token_budget())\n+            remaining_token_budget = budget.remaining_token_budget()\n+            if self.cache_config.enable_prefix_caching:\n+                # When prefix caching is enabled, we always allocate\n+                # the number of new tokens that is dividable by the block size\n+                # to avoid partial block matching.\n+                block_size = self.cache_config.block_size\n+                reminder = budget.token_budget % block_size\n+                if reminder != 0:\n+                    raise ValueError(\"When enabling chunked prefill and \"\n+                                     \"prefix caching, max_num_batched_tokens \"\n+                                     \"(chunk size) must be dividable by \"\n+                                     \"block size, but got chunk_size \"\n+                                     f\"({budget.token_budget}) % block_size \"\n+                                     f\"({block_size}) = {reminder}\")\n+                if remaining_token_budget < num_new_tokens:\n+                    num_new_tokens = (remaining_token_budget //\n+                                      block_size) * block_size\n+            else:\n+                num_new_tokens = min(num_new_tokens, remaining_token_budget)\n         return num_new_tokens",
      "change_type": "modified",
      "lines_added": 25,
      "lines_removed": 7
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import dataclasses\nimport gc\nimport inspect\nimport itertools\nimport time\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set,\n                    Tuple, Type, TypeVar, Union)\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.attention.backends.abstract import AttentionState\nfrom vllm.attention.backends.utils import CommonAttentionState\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, ObservabilityConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.distributed import get_pp_group\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata, SamplingMetadataCache\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.model_executor.models.interfaces import (supports_lora,\n                                                   supports_multimodal)\nfrom vllm.model_executor.models.utils import set_cpu_offload_max_bytes\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,\n                             MultiModalInputs, MultiModalRegistry)\nfrom vllm.prompt_adapter.layers import PromptAdapterMapping\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.prompt_adapter.worker_manager import (\n    LRUCacheWorkerPromptAdapterManager)\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (IntermediateTensors, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.utils import (CudaMemoryProfiler, PyObjectCache, async_tensor_h2d,\n                        flatten_2d_lists, is_hip, is_pin_memory_available,\n                        supports_dynamo)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# Capture graphs for token size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)\n]\n_NUM_WARMUP_ITERS = 2\n\nTModelInputForGPU = TypeVar('TModelInputForGPU', bound=\"ModelInputForGPU\")\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPU(ModelRunnerInputBase):\n    \"\"\"\n    This base class contains metadata needed for the base model forward pass\n    but not metadata for possible additional steps, e.g., sampling. Model\n    runners that run additional steps should subclass this method to add\n    additional fields.\n    \"\"\"\n    input_tokens: Optional[torch.Tensor] = None\n    input_positions: Optional[torch.Tensor] = None\n    seq_lens: Optional[List[int]] = None\n    query_lens: Optional[List[int]] = None\n    lora_mapping: Optional[\"LoRAMapping\"] = None\n    lora_requests: Optional[Set[LoRARequest]] = None\n    attn_metadata: Optional[\"AttentionMetadata\"] = None\n    prompt_adapter_mapping: Optional[PromptAdapterMapping] = None\n    prompt_adapter_requests: Optional[Set[PromptAdapterRequest]] = None\n    multi_modal_kwargs: Optional[BatchedTensorInputs] = None\n    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None\n    finished_requests_ids: Optional[List[str]] = None\n    virtual_engine: int = 0\n    async_callback: Optional[Callable] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[TModelInputForGPU],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> TModelInputForGPU:\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):\n    \"\"\"\n    Used by the ModelRunner.\n    \"\"\"\n    sampling_metadata: Optional[\"SamplingMetadata\"] = None\n    # Used for speculative decoding. We do not broadcast it because it is only\n    # used by the driver worker.\n    is_prompt: Optional[bool] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForGPUWithSamplingMetadata\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n    \"\"\"Build ModelInputForGPU from SequenceGroupMetadata.\"\"\"\n\n    # Note: ideally we would be using a dataclass(kw_only=True)\n    # here, so that this can be subclassed easily,\n    # but kw_only is not supported in python<3.10.\n    class InterDataForSeqGroup:\n        \"\"\"Intermediate data for the current sequence group.\"\"\"\n\n        def simple_reinit(self):\n            self.input_tokens[0].clear()  # type: ignore\n            self.input_positions[0].clear()  # type: ignore\n            self.seq_lens[0] = 0  # type: ignore\n            self.orig_seq_lens[0] = 0  # type: ignore\n            self.query_lens[0] = 0  # type: ignore\n            self.context_lens[0] = 0  # type: ignore\n            self.curr_sliding_window_blocks[0] = 0  # type: ignore\n            self.lora_index_mapping.clear()  # type: ignore\n            self.lora_prompt_mapping.clear()  # type: ignore\n            self.lora_requests.clear()  # type: ignore\n            self.prompt_adapter_index_mapping.clear()  # type: ignore\n            self.prompt_adapter_prompt_mapping.clear()  # type: ignore\n\n        def __init__(\n            self,\n            *,\n            # From sequence group metadata.\n            request_id: str,\n            seq_ids: List[int],\n            is_prompt: bool,\n            block_tables: Optional[Dict[int, List[int]]],\n            computed_block_nums: List[int],\n            n_seqs: int = 0,\n\n            # Input tokens and positions.\n            input_tokens: Optional[List[List[int]]] = None,\n            input_positions: Optional[List[List[int]]] = None,\n\n            # The sequence length (may be capped to the sliding window).\n            seq_lens: Optional[List[int]] = None,\n            # The original sequence length (before applying sliding window).\n            # This is used to compute slot mapping.\n            orig_seq_lens: Optional[List[int]] = None,\n            # The query length.\n            query_lens: Optional[List[int]] = None,\n            # The number of tokens that are already computed.\n            context_lens: Optional[List[int]] = None,\n            # The current sliding window block.\n            curr_sliding_window_blocks: Optional[List[int]] = None,\n\n            # LoRA inputs.\n            lora_index_mapping: Optional[List[List[int]]] = None,\n            lora_prompt_mapping: Optional[List[List[int]]] = None,\n            lora_requests: Optional[Set[LoRARequest]] = None,\n\n            # Prompt adapter inputs.\n            prompt_adapter_index_mapping: Optional[List[int]] = None,\n            prompt_adapter_prompt_mapping: Optional[List[int]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n\n            # Multi-modal inputs.\n            multi_modal_inputs: Optional[MultiModalInputs] = None,\n\n            # Whether the prefix cache is hit (prefill only).\n            prefix_cache_hit: bool = False,\n            reinit: bool = False,\n            reinit_use_defaults: bool = False,\n        ):\n            if reinit:\n                assert len(self.seq_ids) == len(seq_ids)  # type: ignore\n                for i, seq_id in enumerate(seq_ids):\n                    self.seq_ids[i] = seq_id  # type: ignore\n            else:\n                self.seq_ids = seq_ids\n\n            self.request_id = request_id\n            self.is_prompt = is_prompt\n            self.block_tables = block_tables\n            self.computed_block_nums = computed_block_nums\n            self.n_seqs = n_seqs\n\n            if reinit:\n                if len(self.seq_ids) == 1 and reinit_use_defaults:\n                    self.simple_reinit()\n                else:\n                    if input_tokens:\n                        self.input_tokens = input_tokens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_tokens[seq_id].clear()\n\n                    if input_positions:\n                        self.input_positions = input_positions\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.input_positions[seq_id].clear()\n\n                    if seq_lens:\n                        self.seq_lens = seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.seq_lens[seq_id] = 0\n\n                    if orig_seq_lens:\n                        self.orig_seq_lens = orig_seq_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.orig_seq_lens[seq_id] = 0\n\n                    if query_lens:\n                        self.query_lens = query_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.query_lens[seq_id] = 0\n\n                    if context_lens:\n                        self.context_lens = context_lens\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.context_lens[seq_id] = 0\n\n                    if curr_sliding_window_blocks:\n                        self.curr_sliding_window_blocks = \\\n                            curr_sliding_window_blocks\n                    else:\n                        for seq_id in range(len(self.seq_ids)):\n                            self.curr_sliding_window_blocks[seq_id] = 0\n\n                    if lora_index_mapping:\n                        self.lora_index_mapping = lora_index_mapping\n                    else:\n                        self.lora_index_mapping.clear()\n\n                    if lora_prompt_mapping:\n                        self.lora_prompt_mapping = lora_prompt_mapping\n                    else:\n                        self.lora_prompt_mapping.clear()\n\n                    if lora_requests:\n                        self.lora_requests = lora_requests\n                    else:\n                        self.lora_requests.clear()\n\n                    if prompt_adapter_index_mapping:\n                        self.prompt_adapter_index_mapping = \\\n                            prompt_adapter_index_mapping\n                    else:\n                        self.prompt_adapter_index_mapping.clear()\n\n                    if prompt_adapter_prompt_mapping:\n                        self.prompt_adapter_prompt_mapping = \\\n                            prompt_adapter_prompt_mapping\n                    else:\n                        self.prompt_adapter_prompt_mapping.clear()\n\n            else:\n                self.input_tokens = input_tokens or []\n                self.input_positions = input_positions or []\n                self.seq_lens = seq_lens or []\n                self.orig_seq_lens = orig_seq_lens or []\n                self.query_lens = query_lens or []\n                self.context_lens = context_lens or []\n                self.curr_sliding_window_blocks = \\\n                    curr_sliding_window_blocks or []\n\n                self.lora_index_mapping = lora_index_mapping or []\n                self.lora_prompt_mapping = lora_prompt_mapping or []\n                self.lora_requests = lora_requests or set()\n\n                self.prompt_adapter_index_mapping = (\n                    prompt_adapter_index_mapping or [])\n                self.prompt_adapter_prompt_mapping = (\n                    prompt_adapter_prompt_mapping or [])\n\n            self.prompt_adapter_request = prompt_adapter_request\n            self.multi_modal_inputs = multi_modal_inputs\n            self.prefix_cache_hit = prefix_cache_hit\n\n            self.n_seqs = len(self.seq_ids)\n\n            if not reinit:\n                self.__post_init__()\n\n        def __post_init__(self):\n            self.n_seqs = len(self.seq_ids)\n\n            self.input_tokens = [[] for _ in range(self.n_seqs)]\n            self.input_positions = [[] for _ in range(self.n_seqs)]\n            self.seq_lens = [0] * self.n_seqs\n            self.orig_seq_lens = [0] * self.n_seqs\n            self.query_lens = [0] * self.n_seqs\n            self.context_lens = [0] * self.n_seqs\n            self.curr_sliding_window_blocks = [0] * self.n_seqs\n\n            self.lora_index_mapping = []\n            self.lora_prompt_mapping = []\n\n    def gen_inter_data_builder(self, num_seqs: int):\n        return lambda: ModelInputForGPUBuilder.InterDataForSeqGroup(\n            request_id=\"\",\n            seq_ids=[0] * num_seqs,\n            is_prompt=True,\n            block_tables=None,\n            computed_block_nums=[])\n\n    def init_cached_inter_data(self, *args, **kwargs):\n        assert len(args) == 0\n        assert \"seq_ids\" in kwargs\n        seq_ids = kwargs[\"seq_ids\"]\n        num_seqs = len(seq_ids)\n\n        # The inter-data cache is per model_runner\n        inter_data_cache = self.runner.inter_data_cache\n        if num_seqs not in inter_data_cache:\n            inter_data_cache[num_seqs] = PyObjectCache(\n                self.gen_inter_data_builder(num_seqs))\n\n        obj = inter_data_cache[num_seqs].get_object()\n        obj.__init__(*args, **kwargs)\n        return obj\n\n    def reset_cached_inter_data(self):\n        for cache in self.runner.inter_data_cache.values():\n            cache.reset()\n\n    def __init__(self,\n                 runner: \"GPUModelRunnerBase\",\n                 finished_requests_ids: Optional[List[str]] = None):\n        super().__init__()\n        # Compute functions for each sequence in a sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_compute_fns = [\n            self._compute_lens,\n            self._compute_for_prefix_cache_hit,\n            self._compute_for_sliding_window,\n            self._compute_lora_input,\n        ]\n        # Compute functions for each sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_group_compute_fns = [\n            self._compute_prompt_adapter_input,\n            self._compute_multi_modal_input,\n        ]\n\n        self.runner = runner\n        self.model_input_cls = self.runner._model_input_cls\n        self.attn_backend = self.runner.attn_backend\n        self.scheduler_config = self.runner.scheduler_config\n        self.sliding_window = self.runner.sliding_window\n        self.block_size = self.runner.block_size\n        self.enable_lora = self.runner.lora_config is not None\n        self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                      is not None)\n        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n        self.finished_requests_ids = finished_requests_ids\n        self.decode_only = True\n\n        # Intermediate data (data in CPU before going to GPU) for\n        # the current sequence group.\n        self.inter_data_list: List[\n            ModelInputForGPUBuilder.InterDataForSeqGroup] = []\n\n        # Attention metadata inputs.\n        self.attn_metadata_builder = self.attn_backend.make_metadata_builder(\n            weakref.proxy(self))\n\n        # Engine/Model configurations.\n        self.chunked_prefill_enabled = (\n            self.scheduler_config is not None\n            and self.scheduler_config.chunked_prefill_enabled)\n        if self.sliding_window is not None:\n            self.sliding_window_blocks = (\n                self.sliding_window + self.block_size - 1) // self.block_size\n            self.block_aligned_sliding_window = \\\n                self.sliding_window_blocks * self.block_size\n\n    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,\n                      seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Compute context length, sequence length and tokens\n        for the given sequence data.\n        \"\"\"\n        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]\n        token_chunk_size = seq_group_metadata.token_chunk_size\n\n        # Compute context length (the number of tokens that are\n        # already computed) and sequence length (total number of tokens).\n        seq_len = seq_data.get_len()\n        if inter_data.is_prompt:\n            context_len = seq_data.get_num_computed_tokens()\n        else:\n            # get_num_computed_tokens is incorrect for spec decoding.\n            # So, we should have a special logic here.\n            # TODO(sang): Fix it.\n            context_len = seq_len - 1\n        seq_len = min(seq_len, context_len + token_chunk_size)\n\n        # Compute tokens.\n        if inter_data.is_prompt:\n            tokens = seq_data.get_token_ids()\n            if context_len != 0 or seq_len < len(tokens):\n                tokens = tokens[context_len:seq_len]\n        else:\n            # Optimization. get_token_ids requires the entire copy of\n            # tokens.\n            tokens = seq_data.get_last_token_id()\n\n        inter_data.seq_lens[seq_idx] = seq_len\n        inter_data.orig_seq_lens[seq_idx] = seq_len\n        inter_data.context_lens[seq_idx] = context_len\n\n        if isinstance(tokens, list):\n            inter_data.input_tokens[seq_idx].extend(tokens)\n        else:\n            inter_data.input_tokens[seq_idx].append(tokens)\n\n        if (seq_len - context_len) == 1:\n            inter_data.input_positions[seq_idx].append(seq_len - 1)\n        else:\n            inter_data.input_positions[seq_idx].extend(\n                range(context_len, seq_len))\n\n        inter_data.query_lens[\n            seq_idx] = seq_len - context_len if inter_data.is_prompt else 1\n\n    def _compute_for_prefix_cache_hit(\n            self, inter_data: InterDataForSeqGroup, seq_idx: int,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Check if hit prefix cache (i.e., some blocks are already computed).\n        If hit, update input tokens and positions to only compute the\n        remaining blocks.\n        \"\"\"\n        computed_block_nums = inter_data.computed_block_nums\n\n        # Note that prefix caching does not support sliding window.\n        prefix_cache_hit = (computed_block_nums is not None\n                            and len(computed_block_nums) > 0\n                            and self.sliding_window is None\n                            and inter_data.is_prompt)\n        inter_data.prefix_cache_hit = prefix_cache_hit\n        if self.chunked_prefill_enabled and prefix_cache_hit:\n            raise RuntimeError(\n                \"chunked prefill cannot be used with prefix caching now.\")\n\n        # If prefix cache is hit, advance context length to bypass\n        # hit blocks. Accordingly, input tokens, position and query length\n        # have to be updated.\n        if prefix_cache_hit:\n            assert computed_block_nums is not None\n            context_len = len(computed_block_nums) * self.block_size\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][context_len:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][context_len:]\n            inter_data.context_lens[seq_idx] = context_len\n            inter_data.query_lens[\n                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n\n    def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                    seq_idx: int,\n                                    seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Update seq_len and curr_sliding_window_block for the given\n        sequence data (only required by decoding) if sliding window is enabled.\n        \"\"\"\n        curr_sliding_window_block = 0\n        sliding_seq_len = inter_data.seq_lens[seq_idx]\n        if not inter_data.is_prompt and self.sliding_window is not None:\n            # TODO(sang): This is a hack to make sliding window work with\n            # paged attn. We can remove it if we make paged attn kernel\n            # to properly handle slinding window attn.\n            curr_sliding_window_block = self.sliding_window_blocks\n            if self.scheduler_config.use_v2_block_manager:\n                # number of elements in last block\n                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n                sliding_seq_len = min(\n                    inter_data.seq_lens[seq_idx],\n                    self.block_aligned_sliding_window + suff_len)\n                if suff_len > 0:\n                    curr_sliding_window_block += 1\n            else:\n                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n                                      self.sliding_window)\n\n        inter_data.curr_sliding_window_blocks[\n            seq_idx] = curr_sliding_window_block\n        inter_data.seq_lens[seq_idx] = sliding_seq_len\n\n    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,\n                            seq_idx: int,\n                            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If LoRA is enabled, compute LoRA index and prompt mapping.\"\"\"\n        if not self.enable_lora:\n            return\n\n        lora_id = seq_group_metadata.lora_int_id\n        if lora_id > 0:\n            inter_data.lora_requests.add(seq_group_metadata.lora_request)\n        query_len = inter_data.query_lens[seq_idx]\n        inter_data.lora_index_mapping.append([lora_id] * query_len)\n        inter_data.lora_prompt_mapping.append(\n            [lora_id] *\n            (query_len if seq_group_metadata.sampling_params\n             and seq_group_metadata.sampling_params.prompt_logprobs is not None\n             else 1))\n\n    def _compute_prompt_adapter_input(\n            self, inter_data: InterDataForSeqGroup,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If prompt adapter is enabled, compute index and prompt mapping.\n        \"\"\"\n        # Note that when is_prompt=True, we expect only one sequence\n        # in the group.\n        if not self.enable_prompt_adapter:\n            return\n\n        prompt_adapter_id = seq_group_metadata.prompt_adapter_id\n        if prompt_adapter_id <= 0 or not inter_data.is_prompt:\n            return\n\n        # We expect only one sequence in the group when is_prompt=True.\n        assert inter_data.n_seqs == 1\n        query_len = inter_data.query_lens[0]\n        inter_data.prompt_adapter_request = (\n            seq_group_metadata.prompt_adapter_request)\n\n        num_tokens = seq_group_metadata.prompt_adapter_num_virtual_tokens\n        inter_data.prompt_adapter_index_mapping = [\n            prompt_adapter_id\n        ] * num_tokens + [0] * (query_len - num_tokens)\n        inter_data.prompt_adapter_prompt_mapping = [prompt_adapter_id] * (\n            query_len if seq_group_metadata.sampling_params\n            and seq_group_metadata.sampling_params.prompt_logprobs else 1)\n\n    def _compute_multi_modal_input(self, inter_data: InterDataForSeqGroup,\n                                   seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If multi-modal data is given, add it to the input.\"\"\"\n        mm_data = seq_group_metadata.multi_modal_data\n        if not mm_data:\n            return\n\n        mm_kwargs = self.multi_modal_input_mapper(mm_data)\n        inter_data.multi_modal_inputs = mm_kwargs\n\n    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Add a sequence group to the builder.\"\"\"\n        seq_ids = seq_group_metadata.seq_data.keys()\n        n_seqs = len(seq_ids)\n        is_prompt = seq_group_metadata.is_prompt\n\n        if is_prompt:\n            assert n_seqs == 1\n            self.decode_only = False\n\n        inter_data = self.init_cached_inter_data(\n            request_id=seq_group_metadata.request_id,\n            seq_ids=seq_ids,\n            is_prompt=is_prompt,\n            block_tables=seq_group_metadata.block_tables,\n            computed_block_nums=seq_group_metadata.computed_block_nums,\n            reinit=True,\n            reinit_use_defaults=True)\n\n        self.inter_data_list.append(inter_data)\n\n        for seq_idx in range(n_seqs):\n            for per_seq_fn in self.per_seq_compute_fns:\n                per_seq_fn(inter_data, seq_idx, seq_group_metadata)\n        for per_seq_group_fn in self.per_seq_group_compute_fns:\n            per_seq_group_fn(inter_data, seq_group_metadata)\n\n    def _use_captured_graph(self, batch_size: int,\n                            max_decode_seq_len: int) -> bool:\n        return (self.decode_only and not self.runner.model_config.enforce_eager\n                and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n                and max_decode_seq_len <= self.runner.max_seq_len_to_capture)\n\n    def build(self) -> ModelInputForGPU:\n        \"\"\"Finalize the builder intermediate data and\n        create on-device tensors.\n        \"\"\"\n        # Combine and flatten intermediate data.\n        input_tokens = []\n        for inter_data in self.inter_data_list:\n            for cur_input_tokens in inter_data.input_tokens:\n                input_tokens.extend(cur_input_tokens)\n\n        if not input_tokens:\n            # This may happen when all prefill requests hit\n            # prefix caching and there is no decode request.\n            return self.model_input_cls()\n\n        input_positions = []\n        for inter_data in self.inter_data_list:\n            for cur_input_positions in inter_data.input_positions:\n                input_positions.extend(cur_input_positions)\n\n        seq_lens = []\n        max_decode_seq_len = 0\n        for inter_data in self.inter_data_list:\n            seq_lens.extend(inter_data.seq_lens)\n            if not inter_data.is_prompt:\n                max_decode_seq_len = max(max_decode_seq_len,\n                                         max(inter_data.seq_lens))\n        query_lens = []\n        for inter_data in self.inter_data_list:\n            query_lens.extend(inter_data.query_lens)\n\n        # Mapping from request IDs to sequence IDs. Used for Jamba models\n        # that manages the cache by itself.\n        request_ids_to_seq_ids = {\n            data.request_id: data.seq_ids\n            for data in self.inter_data_list\n        }\n\n        batch_size = len(input_tokens)\n        use_captured_graph = self._use_captured_graph(batch_size,\n                                                      max_decode_seq_len)\n\n        # If cuda graph can be used, pad tensors accordingly.\n        # See `capture_model` API for more details.\n        # vLLM uses cuda graph only for decoding requests.\n        cuda_graph_pad_size = -1\n        if use_captured_graph:\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            cuda_graph_pad_size = graph_batch_size - batch_size\n            batch_size = graph_batch_size\n\n        # Tokens and positions.\n        if cuda_graph_pad_size:\n            input_tokens.extend(itertools.repeat(0, cuda_graph_pad_size))\n            input_positions.extend(itertools.repeat(0, cuda_graph_pad_size))\n        assert self.runner.device is not None\n        input_tokens_tensor = async_tensor_h2d(input_tokens, torch.long,\n                                               self.runner.device,\n                                               self.runner.pin_memory)\n        input_positions_tensor = async_tensor_h2d(input_positions, torch.long,\n                                                  self.runner.device,\n                                                  self.runner.pin_memory)\n\n        # Sequence and query lengths.\n        if cuda_graph_pad_size:\n            seq_lens.extend(itertools.repeat(1, cuda_graph_pad_size))\n\n        # Attention metadata.\n        attn_metadata = self.attn_metadata_builder.build(\n            seq_lens, query_lens, cuda_graph_pad_size, batch_size)\n\n        # LoRA data.\n        lora_requests = set()\n        lora_mapping = None\n        if self.enable_lora:\n            lora_requests = set(r for data in self.inter_data_list\n                                for r in data.lora_requests)\n            lora_index_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_index_mapping)\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                lora_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            lora_prompt_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_prompt_mapping)\n                for inter_data in self.inter_data_list\n            ])\n\n            lora_mapping = LoRAMapping(\n                **dict(index_mapping=lora_index_mapping,\n                       prompt_mapping=lora_prompt_mapping,\n                       is_prefill=not self.decode_only))\n\n        # Prompt adapter data.\n        prompt_adapter_requests: Set[PromptAdapterRequest] = set()\n        prompt_adapter_mapping = None\n        if self.enable_prompt_adapter:\n            prompt_adapter_requests = set(\n                data.prompt_adapter_request for data in self.inter_data_list\n                if data.prompt_adapter_request is not None)\n            prompt_adapter_index_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_index_mapping\n                for inter_data in self.inter_data_list\n            ])\n            if cuda_graph_pad_size:\n                prompt_adapter_index_mapping.extend(\n                    itertools.repeat(0, cuda_graph_pad_size))\n            prompt_adapter_prompt_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_prompt_mapping\n                for inter_data in self.inter_data_list\n            ])\n            prompt_adapter_mapping = PromptAdapterMapping(\n                prompt_adapter_index_mapping,\n                prompt_adapter_prompt_mapping,\n            )\n\n        # Multi-modal data.\n        multi_modal_inputs_list = [\n            data.multi_modal_inputs for data in self.inter_data_list\n            if data.multi_modal_inputs is not None\n        ]\n        multi_modal_kwargs = MultiModalInputs.batch(multi_modal_inputs_list)\n\n        return self.model_input_cls(\n            input_tokens=input_tokens_tensor,\n            input_positions=input_positions_tensor,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_mapping=lora_mapping,\n            lora_requests=lora_requests,\n            multi_modal_kwargs=multi_modal_kwargs,\n            request_ids_to_seq_ids=request_ids_to_seq_ids,\n            finished_requests_ids=self.finished_requests_ids,\n            prompt_adapter_mapping=prompt_adapter_mapping,\n            prompt_adapter_requests=prompt_adapter_requests)\n\n\nclass GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n    \"\"\"\n    Helper class for shared methods between GPU model runners.\n    \"\"\"\n    _model_input_cls: Type[TModelInputForGPU]\n    _builder_cls: Type[ModelInputForGPUBuilder]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        return_hidden_states: bool = False,\n        observability_config: Optional[ObservabilityConfig] = None,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        self.prompt_adapter_config = prompt_adapter_config\n        self.return_hidden_states = return_hidden_states\n        self.observability_config = observability_config\n\n        self.device = self.device_config.device\n        self.pin_memory = is_pin_memory_available()\n\n        self.kv_cache_dtype = kv_cache_dtype\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_seq_len_to_capture = self.model_config.max_seq_len_to_capture\n\n        self.graph_runners: List[Dict[int, CUDAGraphRunner]] = [\n            {} for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n\n        self.has_seqlen_agnostic = model_config.contains_seqlen_agnostic_layers(\n            parallel_config)\n\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = np.zeros(\n            (max(_BATCH_SIZES_TO_CAPTURE), self.get_max_block_per_batch()),\n            dtype=np.int32)\n        num_attn_heads = self.model_config.get_num_attention_heads(\n            self.parallel_config)\n        self.attn_backend = get_attn_backend(\n            num_attn_heads,\n            self.model_config.get_head_size(),\n            self.model_config.get_num_kv_heads(self.parallel_config),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.kv_cache_dtype,\n            self.block_size,\n        ) if num_attn_heads else None\n        if self.attn_backend:\n            self.attn_state = self.attn_backend.get_state_cls()(\n                weakref.proxy(self))\n        else:\n            self.attn_state = CommonAttentionState(weakref.proxy(self))\n\n        # Multi-modal data support\n        self.input_registry = input_registry\n        self.mm_registry = mm_registry\n        self.multi_modal_input_mapper = mm_registry \\\n            .create_input_mapper(model_config)\n        self.mm_registry.init_mm_limits_per_prompt(self.model_config)\n\n        # Lazy initialization\n        self.model: nn.Module  # Set after load_model\n        # Set after load_model.\n        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None\n        self.prompt_adapter_manager: LRUCacheWorkerPromptAdapterManager = None\n\n        set_cpu_offload_max_bytes(\n            int(self.cache_config.cpu_offload_gb * 1024**3))\n\n        # Used to cache python objects\n        self.inter_data_cache: Dict[int, PyObjectCache] = {}\n        self.sampling_metadata_cache: SamplingMetadataCache = \\\n            SamplingMetadataCache()\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with CudaMemoryProfiler() as m:\n            self.model = get_model(model_config=self.model_config,\n                                   device_config=self.device_config,\n                                   load_config=self.load_config,\n                                   lora_config=self.lora_config,\n                                   parallel_config=self.parallel_config,\n                                   scheduler_config=self.scheduler_config,\n                                   cache_config=self.cache_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert supports_lora(self.model), \"Model does not support LoRA\"\n            assert not supports_multimodal(\n                self.model\n            ), \"To be tested: Multi-modal model with LoRA settings.\"\n\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens,\n                self.vocab_size,\n                self.lora_config,\n                self.device,\n                self.model.embedding_modules,\n                self.model.embedding_padding_modules,\n                max_position_embeddings=self.model.config.\n                max_position_embeddings,\n            )\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.prompt_adapter_config:\n            self.prompt_adapter_manager = LRUCacheWorkerPromptAdapterManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens, self.device,\n                self.prompt_adapter_config)\n            self.model = (\n                self.prompt_adapter_manager.create_prompt_adapter_manager(\n                    self.model))\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently only ROCm accepts kv-cache scaling factors\n            # via quantization_param_path and this will be deprecated\n            # in the future.\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    warnings.warn(\n                        \"Loading kv cache scaling factor from JSON is \"\n                        \"deprecated and will be removed. Please include \"\n                        \"kv cache scaling factors in the model checkpoint.\",\n                        FutureWarning,\n                        stacklevel=2)\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                    logger.info(\"Loaded KV cache scaling factors from %s\",\n                                self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n\n        if envs.VLLM_TEST_DYNAMO_GRAPH_CAPTURE and supports_dynamo():\n            self.model = torch.compile(self.model,\n                                       fullgraph=True,\n                                       backend=\"eager\")\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import ShardedStateLoader\n        ShardedStateLoader.save_model(\n            self.model,\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def save_tensorized_model(\n        self,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import TensorizerLoader\n        TensorizerLoader.save_model(\n            self.model,\n            tensorizer_config=tensorizer_config,\n        )\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_model_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> TModelInputForGPU:\n        \"\"\"Helper method to prepare the model input based on a given sequence\n        group. Prepares metadata needed for the base model forward pass but not\n        metadata for possible additional steps, e.g., sampling.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)\n        for seq_group_metadata in seq_group_metadata_list:\n            builder.add_seq_group(seq_group_metadata)\n\n        builder.reset_cached_inter_data()\n\n        return builder.build()  # type: ignore\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests: List[LoRARequest] = []\n        dummy_lora_requests_per_seq: List[LoRARequest] = []\n        if self.lora_config:\n            assert self.lora_manager is not None\n            with self.lora_manager.dummy_lora_cache():\n                for idx in range(self.lora_config.max_loras):\n                    lora_id = idx + 1\n                    dummy_lora_request = LoRARequest(\n                        lora_name=f\"warmup_{lora_id}\",\n                        lora_int_id=lora_id,\n                        lora_path=\"/not/a/real/path\",\n                    )\n                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                     rank=LORA_WARMUP_RANK)\n                    dummy_lora_requests.append(dummy_lora_request)\n                dummy_lora_requests_per_seq = [\n                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n                    for idx in range(max_num_seqs)\n                ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for multi-modal encoding, which\n        # needs to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n\n        max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(\n            self.model_config)\n        if max_mm_tokens > 0:\n            max_num_seqs_orig = max_num_seqs\n            max_num_seqs = min(max_num_seqs,\n                               max_num_batched_tokens // max_mm_tokens)\n            if max_num_seqs < 1:\n                expr = (f\"min({max_num_seqs_orig}, \"\n                        f\"{max_num_batched_tokens} // {max_mm_tokens})\")\n                logger.warning(\n                    \"Computed max_num_seqs (%s) to be less than 1. \"\n                    \"Setting it to the minimum value of 1.\", expr)\n                max_num_seqs = 1\n\n        batch_size = 0\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            batch_size += seq_len\n\n            seq_data, dummy_multi_modal_data = self.input_registry \\\n                .dummy_data_for_profiling(self.model_config,\n                                          seq_len,\n                                          self.mm_registry)\n\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=dummy_multi_modal_data,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        finished_requests_ids = [seq.request_id for seq in seqs]\n        model_input = self.prepare_model_input(\n            seqs, finished_requests_ids=finished_requests_ids)\n        intermediate_tensors = None\n        if not get_pp_group().is_first_rank:\n            intermediate_tensors = self.model.make_empty_intermediate_tensors(\n                batch_size=batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n        self.execute_model(model_input, kv_caches, intermediate_tensors)\n        torch.cuda.synchronize()\n\n        # reset and discard the guard and compiled bytecode for profiling runs\n        torch._dynamo.reset()\n\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_adapters()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_adapter(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_adapter(lora_id)\n\n    def pin_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.pin_adapter(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_adapters()\n\n    def remove_all_prompt_adapters(self):\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.remove_all_adapters()\n\n    def set_active_prompt_adapters(\n            self, prompt_adapter_requests: Set[PromptAdapterRequest],\n            prompt_adapter_mapping: PromptAdapterMapping) -> None:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.set_active_adapters(\n            prompt_adapter_requests, prompt_adapter_mapping)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.add_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.remove_adapter(prompt_adapter_id)\n\n    def pin_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.pin_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> Set[int]:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.list_adapters()\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[List[torch.Tensor]]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n\n        # Prepare dummy previous_hidden_states only if needed by the model.\n        # This is used by draft models such as EAGLE.\n        previous_hidden_states = None\n        if \"previous_hidden_states\" in inspect.signature(\n                self.model.forward).parameters:\n            previous_hidden_states = torch.empty(\n                [max_batch_size,\n                 self.model_config.get_hidden_size()],\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        intermediate_inputs = None\n        if not get_pp_group().is_first_rank:\n            intermediate_inputs = self.model.make_empty_intermediate_tensors(\n                batch_size=max_batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        # Prepare buffer for outputs. These will be reused for all batch sizes.\n        # It will be filled after the first graph capture.\n        hidden_or_intermediate_states: List[Optional[torch.Tensor]] = [\n            None\n        ] * self.parallel_config.pipeline_parallel_size\n\n        graph_batch_size = _get_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        with self.attn_state.graph_capture(\n                max_batch_size), graph_capture() as graph_capture_context:\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for virtual_engine in range(\n                    self.parallel_config.pipeline_parallel_size):\n                for batch_size in reversed(batch_size_capture_list):\n                    attn_metadata = (\n                        self.attn_state.graph_capture_get_metadata_for_batch(\n                            batch_size))\n\n                    if self.lora_config:\n                        lora_mapping = LoRAMapping(\n                            **dict(index_mapping=[0] * batch_size,\n                                   prompt_mapping=[0] * batch_size,\n                                   is_prefill=False))\n                        self.set_active_loras(set(), lora_mapping)\n\n                    if self.prompt_adapter_config:\n                        prompt_adapter_mapping = PromptAdapterMapping(\n                            [-1] * batch_size,\n                            [-1] * batch_size,\n                        )\n                        self.set_active_prompt_adapters(\n                            set(), prompt_adapter_mapping)\n\n                    graph_runner = CUDAGraphRunner(\n                        self.model, self.attn_backend.get_name(),\n                        self.attn_state.graph_clone(batch_size))\n\n                    capture_inputs = {\n                        \"input_ids\":\n                        input_tokens[:batch_size],\n                        \"positions\":\n                        input_positions[:batch_size],\n                        \"hidden_or_intermediate_states\":\n                        hidden_or_intermediate_states[\n                            virtual_engine]  # type: ignore\n                        [:batch_size]\n                        if hidden_or_intermediate_states[virtual_engine]\n                        is not None else None,\n                        \"intermediate_inputs\":\n                        intermediate_inputs[:batch_size]\n                        if intermediate_inputs is not None else None,\n                        \"kv_caches\":\n                        kv_caches[virtual_engine],\n                        \"attn_metadata\":\n                        attn_metadata,\n                        \"memory_pool\":\n                        self.graph_memory_pool,\n                        \"stream\":\n                        graph_capture_context.stream\n                    }\n                    if previous_hidden_states is not None:\n                        capture_inputs[\n                            \"previous_hidden_states\"] = previous_hidden_states[:\n                                                                               batch_size]\n\n                    if self.has_seqlen_agnostic:\n                        # Only used by Mamba-based models CUDA graph atm (Jamba)\n                        capture_inputs.update({\n                            \"seqlen_agnostic_capture_inputs\":\n                            self.model.get_seqlen_agnostic_capture_inputs(\n                                batch_size)\n                        })\n                    graph_runner.capture(**capture_inputs)\n                    self.graph_memory_pool = graph_runner.graph.pool()\n                    self.graph_runners[virtual_engine][batch_size] = (\n                        graph_runner)\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n    \"\"\"\n    GPU model runner with sampling step.\n    \"\"\"\n    _model_input_cls: Type[ModelInputForGPUWithSamplingMetadata] = (\n        ModelInputForGPUWithSamplingMetadata)\n    _builder_cls: Type[ModelInputForGPUBuilder] = ModelInputForGPUBuilder\n\n    def make_model_input_from_broadcasted_tensor_dict(\n        self,\n        tensor_dict: Dict[str, Any],\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        model_input = \\\n            ModelInputForGPUWithSamplingMetadata.from_broadcasted_tensor_dict(\n                tensor_dict,\n                attn_backend=self.attn_backend,\n            )\n        return model_input\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None,\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        model_input = self._prepare_model_input_tensors(\n            seq_group_metadata_list, finished_requests_ids)\n        if get_pp_group().is_last_rank:\n            # Sampling metadata is only required for the final pp group\n            generators = self.get_generators(finished_requests_ids)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, model_input.seq_lens,\n                model_input.query_lens, self.device, self.pin_memory,\n                generators, self.sampling_metadata_cache)\n        else:\n            sampling_metadata = None\n        is_prompt = (seq_group_metadata_list[0].is_prompt\n                     if seq_group_metadata_list else None)\n        return dataclasses.replace(model_input,\n                                   sampling_metadata=sampling_metadata,\n                                   is_prompt=is_prompt,\n                                   virtual_engine=virtual_engine)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        model_input: ModelInputForGPUWithSamplingMetadata,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:\n        if num_steps > 1:\n            raise ValueError(\"num_steps > 1 is not supported in ModelRunner\")\n\n        if self.lora_config:\n            assert model_input.lora_requests is not None\n            assert model_input.lora_mapping is not None\n            self.set_active_loras(model_input.lora_requests,\n                                  model_input.lora_mapping)\n\n        if self.prompt_adapter_config:\n            assert model_input.prompt_adapter_requests is not None\n            assert model_input.prompt_adapter_mapping is not None\n            self.set_active_prompt_adapters(\n                model_input.prompt_adapter_requests,\n                model_input.prompt_adapter_mapping)\n\n        self.attn_state.begin_forward(model_input)\n\n        # Currently cuda graph is only supported by the decode phase.\n        assert model_input.attn_metadata is not None\n        prefill_meta = model_input.attn_metadata.prefill_metadata\n        decode_meta = model_input.attn_metadata.decode_metadata\n        # TODO(andoorve): We can remove this once all\n        # virtual engines share the same kv cache.\n        virtual_engine = model_input.virtual_engine\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            assert model_input.input_tokens is not None\n            graph_batch_size = model_input.input_tokens.shape[0]\n            model_executable = self.graph_runners[virtual_engine][\n                graph_batch_size]\n        else:\n            model_executable = self.model\n\n        multi_modal_kwargs = model_input.multi_modal_kwargs or {}\n        seqlen_agnostic_kwargs = {\n            \"finished_requests_ids\": model_input.finished_requests_ids,\n            \"request_ids_to_seq_ids\": model_input.request_ids_to_seq_ids,\n        } if self.has_seqlen_agnostic else {}\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_start = torch.cuda.Event(enable_timing=True)\n            model_forward_end = torch.cuda.Event(enable_timing=True)\n            model_forward_start.record()\n\n        hidden_or_intermediate_states = model_executable(\n            input_ids=model_input.input_tokens,\n            positions=model_input.input_positions,\n            kv_caches=kv_caches,\n            attn_metadata=model_input.attn_metadata,\n            intermediate_tensors=intermediate_tensors,\n            **MultiModalInputs.as_kwargs(multi_modal_kwargs,\n                                         device=self.device),\n            **seqlen_agnostic_kwargs)\n\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time):\n            model_forward_end.record()\n\n        # Compute the logits in the last pipeline stage.\n        if not get_pp_group().is_last_rank:\n            if (self.is_driver_worker\n                    and hidden_or_intermediate_states is not None\n                    and isinstance(hidden_or_intermediate_states,\n                                   IntermediateTensors)\n                    and self.observability_config is not None\n                    and self.observability_config.collect_model_forward_time):\n                model_forward_end.synchronize()\n                model_forward_time = model_forward_start.elapsed_time(\n                    model_forward_end)\n                orig_model_forward_time = 0.0\n                if intermediate_tensors is not None:\n                    orig_model_forward_time = intermediate_tensors.tensors.get(\n                        \"model_forward_time\", torch.tensor(0.0)).item()\n                hidden_or_intermediate_states.tensors[\"model_forward_time\"] = (\n                    torch.tensor(model_forward_time + orig_model_forward_time))\n            return hidden_or_intermediate_states\n\n        logits = self.model.compute_logits(hidden_or_intermediate_states,\n                                           model_input.sampling_metadata)\n\n        if not self.is_driver_worker:\n            return []\n\n        if model_input.async_callback is not None:\n            model_input.async_callback()\n\n        # Sample the next token.\n        output: SamplerOutput = self.model.sample(\n            logits=logits,\n            sampling_metadata=model_input.sampling_metadata,\n        )\n        if (self.observability_config is not None\n                and self.observability_config.collect_model_forward_time\n                and output is not None):\n            model_forward_end.synchronize()\n            model_forward_time = model_forward_start.elapsed_time(\n                model_forward_end)\n            orig_model_forward_time = 0.0\n            if intermediate_tensors is not None:\n                orig_model_forward_time = intermediate_tensors.tensors.get(\n                    \"model_forward_time\", torch.tensor(0.0)).item()\n            # If there are multiple workers, we are still tracking the latency\n            # from the start time of the driver worker to the end time of the\n            # driver worker. The model forward time will then end up covering\n            # the communication time as well.\n            output.model_forward_time = (orig_model_forward_time +\n                                         model_forward_time)\n\n        if self.return_hidden_states:\n            # we only need to pass hidden states of most recent token\n            assert model_input.sampling_metadata is not None\n            indices = model_input.sampling_metadata.selected_token_indices\n            if model_input.is_prompt:\n                hidden_states = hidden_or_intermediate_states.index_select(\n                    0, indices)\n                output.prefill_hidden_states = hidden_or_intermediate_states\n            elif decode_meta.use_cuda_graph:\n                hidden_states = hidden_or_intermediate_states[:len(indices)]\n            else:\n                hidden_states = hidden_or_intermediate_states\n\n            output.hidden_states = hidden_states\n\n        return [output]\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module, backend_name: str,\n                 attn_state: AttentionState):\n        self.model = model\n        self.backend_name = backend_name\n        self.attn_state = attn_state\n\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        hidden_or_intermediate_states: Optional[Union[IntermediateTensors,\n                                                      torch.Tensor]],\n        intermediate_inputs: Optional[IntermediateTensors],\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool: Optional[Tuple[int, int]],\n        stream: torch.cuda.Stream,\n        **kwargs,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        assert self._graph is None\n        # Run the model a few times without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        # Note one iteration is not enough for torch.jit.script\n        for _ in range(_NUM_WARMUP_ITERS):\n            self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            output_hidden_or_intermediate_states = self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=attn_metadata,\n                intermediate_tensors=intermediate_inputs,\n                **kwargs,\n            )\n            if hidden_or_intermediate_states is not None:\n                if get_pp_group().is_last_rank:\n                    hidden_or_intermediate_states.copy_(\n                        output_hidden_or_intermediate_states)\n                else:\n                    for key in hidden_or_intermediate_states.tensors:\n                        hidden_or_intermediate_states[key].copy_(\n                            output_hidden_or_intermediate_states[key])\n            else:\n                hidden_or_intermediate_states = (\n                    output_hidden_or_intermediate_states)\n\n            del output_hidden_or_intermediate_states\n            # make sure `output_hidden_states` is deleted\n            # in the graph's memory pool\n            gc.collect()\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\": input_ids,\n            \"positions\": positions,\n            \"kv_caches\": kv_caches,\n            **self.attn_state.get_graph_input_buffers(attn_metadata),\n            **kwargs,\n        }\n        if intermediate_inputs is not None:\n            self.input_buffers.update(intermediate_inputs.tensors)\n        if get_pp_group().is_last_rank:\n            self.output_buffers = {\n                \"hidden_states\": hidden_or_intermediate_states\n            }\n        else:\n            self.output_buffers = hidden_or_intermediate_states\n        return hidden_or_intermediate_states\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(attn_metadata.slot_mapping,\n                                                 non_blocking=True)\n        self.attn_state.prepare_graph_input_buffers(self.input_buffers,\n                                                    attn_metadata)\n        if \"seqlen_agnostic_capture_inputs\" in self.input_buffers:\n            self.model.copy_inputs_before_cuda_graphs(self.input_buffers,\n                                                      **kwargs)\n\n        if \"previous_hidden_states\" in self.input_buffers:\n            self.input_buffers[\"previous_hidden_states\"].copy_(\n                kwargs[\"previous_hidden_states\"], non_blocking=True)\n\n        if intermediate_tensors is not None:\n            for key in intermediate_tensors.tensors:\n                if key != \"model_execute_time\" and key != \"model_forward_time\":\n                    self.input_buffers[key].copy_(intermediate_tensors[key],\n                                                  non_blocking=True)\n        # Run the graph.\n        self.graph.replay()\n        # Return the output tensor.\n        if get_pp_group().is_last_rank:\n            return self.output_buffers[\"hidden_states\"]\n\n        return self.output_buffers\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex f556e4ea1..2b287a5d2 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -501,23 +501,48 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                             and self.sliding_window is None\n                             and inter_data.is_prompt)\n         inter_data.prefix_cache_hit = prefix_cache_hit\n-        if self.chunked_prefill_enabled and prefix_cache_hit:\n-            raise RuntimeError(\n-                \"chunked prefill cannot be used with prefix caching now.\")\n-\n-        # If prefix cache is hit, advance context length to bypass\n-        # hit blocks. Accordingly, input tokens, position and query length\n-        # have to be updated.\n-        if prefix_cache_hit:\n-            assert computed_block_nums is not None\n-            context_len = len(computed_block_nums) * self.block_size\n+\n+        if not prefix_cache_hit:\n+            return\n+\n+        assert computed_block_nums is not None\n+        # The cache hit prompt tokens in this sequence. Note that\n+        # this may be larger than the sequence length if chunked\n+        # prefill is enabled.\n+        prefix_cache_len = len(computed_block_nums) * self.block_size\n+        # The number of so far computed prompt tokens in this sequence.\n+        context_len = inter_data.context_lens[seq_idx]\n+        # The total number of prompt tokens in this sequence.\n+        # When chunked prefill is enabled, this is the token number of\n+        # computed chunks + current chunk.\n+        seq_len = inter_data.seq_lens[seq_idx]\n+        if prefix_cache_len <= context_len:\n+            # We already passed the cache hit region,\n+            # so do normal computation.\n+            pass\n+        elif context_len < prefix_cache_len < seq_len:\n+            # Partial hit. Compute the missing part.\n+            uncomputed_start = prefix_cache_len - context_len\n             inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n             inter_data.input_positions[seq_idx] = inter_data.input_positions[\n-                seq_idx][context_len:]\n+                seq_idx][uncomputed_start:]\n+            context_len = prefix_cache_len\n+\n             inter_data.context_lens[seq_idx] = context_len\n             inter_data.query_lens[\n                 seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n+        elif seq_len <= prefix_cache_len:\n+            # Full hit. Only compute the last token to avoid\n+            # erroneous behavior. FIXME: Ideally we should directly\n+            # mark all tokens as computed in the scheduler and do not\n+            # schedule this sequence, so this case should not happen.\n+            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n+                seq_idx][-1:]\n+            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n+                seq_idx][-1:]\n+            inter_data.query_lens[seq_idx] = 1\n+            inter_data.context_lens[seq_idx] = inter_data.seq_lens[seq_idx] - 1\n \n     def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                     seq_idx: int,",
      "change_type": "modified",
      "lines_added": 38,
      "lines_removed": 13
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 9,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 9
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (test_chunked_prefill, test_block_manager, test_chunked_prefill_scheduler)",
    "is_benchmark_actually_there": "",
    "sample_clues": "block_manager_v1, block_manager_v2, blocks"
  }
}