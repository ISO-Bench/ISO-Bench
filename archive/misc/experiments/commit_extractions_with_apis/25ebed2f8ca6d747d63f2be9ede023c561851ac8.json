{
  "commit_hash": "25ebed2f8ca6d747d63f2be9ede023c561851ac8",
  "parent_hash": "d263bd9df7b2f5586910e5d006a11ff11ba7c310",
  "message": "[V1][Minor] Cache np arange to reduce input preparation overhead (#11214)\n\nSigned-off-by: Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2024-12-15 13:33:00 -0800",
  "files_changed": [
    {
      "file_path": "vllm/v1/worker/gpu_model_runner.py",
      "old_content": "import gc\nimport time\nfrom typing import TYPE_CHECKING, Dict, List, Tuple, cast\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nfrom vllm.config import CompilationLevel, VllmConfig\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.forward_context import set_forward_context\nfrom vllm.inputs import INPUT_REGISTRY, InputRegistry\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.sampling_params import SamplingType\nfrom vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,\n                        LayerBlockType, cdiv, is_pin_memory_available)\nfrom vllm.v1.attention.backends.flash_attn import (FlashAttentionBackend,\n                                                   FlashAttentionMetadata)\nfrom vllm.v1.outputs import ModelRunnerOutput\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch\n\nif TYPE_CHECKING:\n    from vllm.v1.core.scheduler import SchedulerOutput\n\nlogger = init_logger(__name__)\n\n\nclass GPUModelRunner:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        device: torch.device,\n        input_registry: InputRegistry = INPUT_REGISTRY,\n    ):\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.load_config = vllm_config.load_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.speculative_config = vllm_config.speculative_config\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config\n        self.observability_config = vllm_config.observability_config\n\n        model_config = self.model_config\n        cache_config = self.cache_config\n        scheduler_config = self.scheduler_config\n        parallel_config = self.parallel_config\n        self.device = device\n        self.pin_memory = is_pin_memory_available()\n        self.dtype = self.model_config.dtype\n        if cache_config.cache_dtype == \"auto\":\n            self.kv_cache_dtype = self.dtype\n        else:\n            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[\n                cache_config.cache_dtype]\n\n        self.is_multimodal_model = model_config.is_multimodal_model\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_model_len = model_config.max_model_len\n        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n        self.max_num_tokens = scheduler_config.max_num_batched_tokens\n        self.max_num_reqs = scheduler_config.max_num_seqs\n\n        # Model-related.\n        self.num_attn_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\n        self.head_size = model_config.get_head_size()\n        self.hidden_size = model_config.get_hidden_size()\n\n        # Multi-modal data support\n        self.input_registry = input_registry\n\n        # Lazy initialization\n        # self.model: nn.Module  # Set after load_model\n        self.kv_caches: List[torch.Tensor] = []\n        # req_id -> (input_id -> encoder_output)\n        self.encoder_cache: Dict[str, Dict[int, torch.Tensor]] = {}\n\n        # Request states.\n        self.requests: Dict[str, CachedRequestState] = {}\n        # Persistent batch.\n        self.input_batch = InputBatch(\n            max_num_reqs=self.max_num_reqs,\n            max_model_len=self.max_model_len,\n            max_num_blocks_per_req=self.max_num_blocks_per_req,\n            device=self.device,\n            pin_memory=self.pin_memory,\n        )\n\n        self.use_cuda_graph = (self.vllm_config.compilation_config.level\n                               == CompilationLevel.PIECEWISE\n                               and not self.model_config.enforce_eager)\n        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.\n        # The convention is different.\n        # self.cudagraph_batch_sizes sorts in ascending order.\n        # The batch sizes in the config are in descending order.\n        self.cudagraph_batch_sizes = list(\n            reversed(self.vllm_config.compilation_config.capture_sizes))\n\n        # Persistent buffers for CUDA graphs.\n        self.input_ids = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int32,\n                                     device=self.device)\n        self.positions = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int64,\n                                     device=self.device)\n        self.inputs_embeds = torch.zeros(\n            (self.max_num_tokens, self.hidden_size),\n            dtype=self.dtype,\n            device=self.device)\n\n        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.input_ids_np = self.input_ids_cpu.numpy()\n        self.positions_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.positions_np = self.positions_cpu.numpy()\n        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n                                            dtype=torch.int32,\n                                            device=\"cpu\",\n                                            pin_memory=self.pin_memory)\n        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n                                               dtype=torch.int32,\n                                               device=\"cpu\",\n                                               pin_memory=self.pin_memory)\n        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n                                             dtype=torch.int32,\n                                             device=\"cpu\",\n                                             pin_memory=self.pin_memory)\n        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()\n\n    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n        # Remove stopped requests from the cached states.\n        # Keep the states of the pre-empted requests.\n        for req_id in scheduler_output.finished_req_ids:\n            self.requests.pop(req_id, None)\n            self.encoder_cache.pop(req_id, None)\n\n        # Free the cached encoder outputs.\n        for req_id, input_id in scheduler_output.free_encoder_input_ids:\n            encoder_outputs = self.encoder_cache.get(req_id)\n            if encoder_outputs is not None:\n                encoder_outputs.pop(input_id, None)\n                if not encoder_outputs:\n                    self.encoder_cache.pop(req_id, None)\n\n        # Remove the requests from the persistent batch.\n        stopped_req_ids = set().union(\n            scheduler_output.preempted_req_ids,\n            scheduler_output.finished_req_ids,\n        )\n        removed_req_indices: List[int] = []\n        for req_id in stopped_req_ids:\n            req_index = self.input_batch.remove_request(req_id)\n            if req_index is not None:\n                removed_req_indices.append(req_index)\n\n        # Update the states of the running requests.\n        for req_data in scheduler_output.scheduled_running_reqs:\n            req_id = req_data.req_id\n            req_state = self.requests[req_id]\n            req_index = self.input_batch.req_id_to_index[req_id]\n\n            # Update the num_computed_tokens.\n            req_state.num_computed_tokens = req_data.num_computed_tokens\n            self.input_batch.num_computed_tokens_cpu[req_index] = (\n                req_data.num_computed_tokens)\n\n            # Update the block table.\n            num_new_blocks = len(req_data.new_block_ids)\n            if num_new_blocks == 0:\n                continue\n            start_index = len(req_state.block_ids)\n            end_index = start_index + num_new_blocks\n            req_state.block_ids.extend(req_data.new_block_ids)\n            self.input_batch.block_table_cpu[\n                req_index, start_index:end_index] = req_data.new_block_ids\n\n        req_ids_to_add: List[str] = []\n        # Add new requests to the cached states.\n        for new_req_data in scheduler_output.scheduled_new_reqs:\n            req_id = new_req_data.req_id\n            sampling_params = new_req_data.sampling_params\n            if sampling_params.sampling_type == SamplingType.RANDOM_SEED:\n                generator = torch.Generator(device=self.device)\n                generator.manual_seed(sampling_params.seed)\n            else:\n                generator = None\n\n            self.requests[req_id] = CachedRequestState(\n                req_id=req_id,\n                prompt_token_ids=new_req_data.prompt_token_ids,\n                prompt=new_req_data.prompt,\n                mm_inputs=new_req_data.mm_inputs,\n                mm_positions=new_req_data.mm_positions,\n                sampling_params=sampling_params,\n                generator=generator,\n                block_ids=new_req_data.block_ids,\n                num_computed_tokens=new_req_data.num_computed_tokens,\n                output_token_ids=[],\n            )\n            req_ids_to_add.append(req_id)\n\n        # Update the cached states of the resumed requests.\n        for res_req_data in scheduler_output.scheduled_resumed_reqs:\n            req_id = res_req_data.req_id\n            req_state = self.requests[req_id]\n\n            req_state.block_ids = res_req_data.block_ids\n            req_state.num_computed_tokens = res_req_data.num_computed_tokens\n            req_ids_to_add.append(req_id)\n\n        # Add the new or resumed requests to the persistent batch.\n        # The smaller empty indices are filled first.\n        removed_req_indices = sorted(removed_req_indices, reverse=True)\n        for req_id in req_ids_to_add:\n            req_state = self.requests[req_id]\n            if removed_req_indices:\n                # Fill the empty index.\n                req_index = removed_req_indices.pop()\n            else:\n                # Append to the end.\n                req_index = None\n            self.input_batch.add_request(req_state, req_index)\n\n        # Condense the batched states if there are empty indices.\n        if removed_req_indices:\n            self.input_batch.condense(removed_req_indices)\n\n    def _prepare_inputs(self, scheduler_output: \"SchedulerOutput\"):\n        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        assert total_num_scheduled_tokens > 0\n        num_reqs = self.input_batch.num_reqs\n        assert num_reqs > 0\n\n        # OPTIMIZATION: Start copying the block table first.\n        # This way, we can overlap the copy with the following CPU operations.\n        self.input_batch.block_table[:num_reqs].copy_(\n            self.input_batch.block_table_cpu_tensor[:num_reqs],\n            non_blocking=True)\n\n        # Get the number of scheduled tokens for each request.\n        # TODO: The Python loop can be slow. Optimize.\n        num_scheduled_tokens = []\n        max_num_scheduled_tokens = 0\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_scheduled_tokens.append(num_tokens)\n            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n                                           num_tokens)\n        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n        assert max_num_scheduled_tokens > 0\n\n        # Get request indices.\n        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n\n        # Get batched arange.\n        # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n\n        # Get positions.\n        positions_np = self.positions_np[:total_num_scheduled_tokens]\n        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n               arange,\n               out=positions_np)\n\n        # Get token indices.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]\n        # where M is the max_model_len.\n        token_indices = (positions_np +\n                         req_indices * self.input_batch.token_ids_cpu.shape[1])\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                           0,\n                           torch.from_numpy(token_indices),\n                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n\n        # Calculate the slot mapping.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]\n        # where K is the max_num_blocks_per_req and the block size is 2.\n        # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n        # because M (max_model_len) is not necessarily divisible by block_size.\n        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n                               positions_np // self.block_size)\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        block_numbers = (self.input_batch.block_table_cpu_tensor.flatten()\n                         [block_table_indices].numpy())\n        block_offsets = positions_np % self.block_size\n        np.add(block_numbers * self.block_size,\n               block_offsets,\n               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n\n        # Prepare the attention metadata.\n        self.query_start_loc_np[0] = 0\n        np.cumsum(num_scheduled_tokens,\n                  out=self.query_start_loc_np[1:num_reqs + 1])\n\n        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n                    num_scheduled_tokens)\n        max_seq_len = seq_lens.max()\n        self.seq_start_loc_np[0] = 0\n        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])\n\n        # Copy the tensors to the GPU.\n        self.input_ids[:total_num_scheduled_tokens].copy_(\n            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n        self.positions[:total_num_scheduled_tokens].copy_(\n            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)\n        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n            self.device, non_blocking=True)\n        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(\n            self.device, non_blocking=True)\n        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n            self.device, non_blocking=True).long()\n        attn_metadata = FlashAttentionMetadata(\n            num_actual_tokens=total_num_scheduled_tokens,\n            max_query_len=max_num_scheduled_tokens,\n            query_start_loc=query_start_loc,\n            max_seq_len=max_seq_len,\n            seq_start_loc=seq_start_loc,\n            block_table=self.input_batch.block_table[:num_reqs],\n            slot_mapping=slot_mapping,\n        )\n        # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial\n        # request in the batch. While we should not sample any token from this\n        # partial request, we do so for simplicity. We will ignore the sampled\n        # token from the partial request.\n        # TODO: Support prompt logprobs.\n        logits_indices = query_start_loc[1:] - 1\n        return attn_metadata, logits_indices\n\n    def _prepare_sampling(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> SamplingMetadata:\n        skip_copy = True\n        if (scheduler_output.finished_req_ids\n                or scheduler_output.preempted_req_ids):\n            skip_copy = False\n        if (scheduler_output.scheduled_new_reqs\n                or scheduler_output.scheduled_resumed_reqs):\n            skip_copy = False\n        # Create the sampling metadata.\n        sampling_metadata = self.input_batch.make_sampling_metadata(skip_copy)\n        return sampling_metadata\n\n    def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n        scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n        if not scheduled_encoder_inputs:\n            return\n\n        # Batch the multi-modal inputs.\n        mm_inputs: List[MultiModalKwargs] = []\n        req_input_ids: List[Tuple[str, int]] = []\n        for req_id, encoder_input_ids in scheduled_encoder_inputs.items():\n            req_state = self.requests[req_id]\n            for input_id in encoder_input_ids:\n                mm_inputs.append(req_state.mm_inputs[input_id])\n                req_input_ids.append((req_id, input_id))\n        batched_mm_inputs = MultiModalKwargs.batch(mm_inputs)\n        batched_mm_inputs = MultiModalKwargs.as_kwargs(batched_mm_inputs,\n                                                       device=self.device)\n\n        # Run the encoder.\n        # `encoder_outputs` is either of the following:\n        # 1. A tensor of shape [num_images, feature_size, hidden_size]\n        # in case when feature_size is fixed across all images.\n        # 2. A list (length: num_images) of tensors, each of shape\n        # [feature_size, hidden_size] in case when the feature size is\n        # dynamic depending on input images.\n        encoder_outputs = self.model.get_multimodal_embeddings(\n            **batched_mm_inputs)\n\n        # Cache the encoder outputs.\n        for (req_id, input_id), output in zip(req_input_ids, encoder_outputs):\n            if req_id not in self.encoder_cache:\n                self.encoder_cache[req_id] = {}\n            self.encoder_cache[req_id][input_id] = output\n\n    def _gather_encoder_outputs(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> List[torch.Tensor]:\n        encoder_outputs: List[torch.Tensor] = []\n        num_reqs = self.input_batch.num_reqs\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[\n                req_id]\n            req_state = self.requests[req_id]\n            num_computed_tokens = req_state.num_computed_tokens\n            mm_positions = req_state.mm_positions\n            for i, pos_info in enumerate(mm_positions):\n                start_pos = pos_info[\"offset\"]\n                num_encoder_tokens = pos_info[\"length\"]\n\n                # The encoder output is needed if the two ranges overlap:\n                # [num_computed_tokens,\n                #  num_computed_tokens + num_scheduled_tokens) and\n                # [start_pos, start_pos + num_encoder_tokens)\n                if start_pos >= num_computed_tokens + num_scheduled_tokens:\n                    # The encoder output is not needed in this step.\n                    break\n                if start_pos + num_encoder_tokens <= num_computed_tokens:\n                    # The encoder output is already processed and stored\n                    # in the decoder's KV cache.\n                    continue\n\n                start_idx = max(num_computed_tokens - start_pos, 0)\n                end_idx = min(\n                    num_computed_tokens - start_pos + num_scheduled_tokens,\n                    num_encoder_tokens)\n                assert start_idx < end_idx\n                assert req_id in self.encoder_cache\n                assert i in self.encoder_cache[req_id]\n                encoder_output = self.encoder_cache[req_id][i]\n                encoder_outputs.append(encoder_output[start_idx:end_idx])\n        return encoder_outputs\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> ModelRunnerOutput:\n        self._update_states(scheduler_output)\n\n        if self.is_multimodal_model:\n            # Run the multimodal encoder if any.\n            self._execute_encoder(scheduler_output)\n            encoder_outputs = self._gather_encoder_outputs(scheduler_output)\n        else:\n            encoder_outputs = []\n\n        # Prepare the decoder inputs.\n        attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\n        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        if (self.use_cuda_graph\n                and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n            # Use piecewise CUDA graphs.\n            # Add padding to the batch size.\n            num_input_tokens = self.vllm_config.pad_for_cudagraph(\n                num_scheduled_tokens)\n        else:\n            # Eager mode.\n            num_input_tokens = num_scheduled_tokens\n        attn_metadata.num_input_tokens = num_input_tokens\n\n        if self.is_multimodal_model:\n            # NOTE(woosuk): To unify token ids and soft tokens (vision\n            # embeddings), we always use embeddings (rather than token ids)\n            # as input to the multimodal model, even when the input is text.\n            input_ids = self.input_ids[:num_scheduled_tokens]\n            if encoder_outputs:\n                inputs_embeds = self.model.get_input_embeddings(\n                    input_ids, encoder_outputs)\n            else:\n                inputs_embeds = self.model.get_input_embeddings(input_ids)\n            # TODO(woosuk): Avoid the copy. Optimize.\n            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)\n            inputs_embeds = self.inputs_embeds[:num_input_tokens]\n            input_ids = None\n        else:\n            # For text-only models, we use token ids as input.\n            # While it is possible to use embeddings as input just like the\n            # multimodal models, it is not desirable for performance since\n            # then the embedding layer is not included in the CUDA graph.\n            input_ids = self.input_ids[:num_input_tokens]\n            inputs_embeds = None\n\n        # Run the decoder.\n        # Use persistent buffers for CUDA graphs.\n        with set_forward_context(attn_metadata, self.vllm_config):\n            hidden_states = self.model(\n                input_ids=input_ids,\n                positions=self.positions[:num_input_tokens],\n                kv_caches=self.kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        hidden_states = hidden_states[:num_scheduled_tokens]\n        hidden_states = hidden_states[logits_indices]\n        logits = self.model.compute_logits(hidden_states, None)\n\n        # Sample the next token and get logprobs if needed.\n        sampling_metadata = self._prepare_sampling(scheduler_output)\n        sampler_output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        sampled_token_ids = sampler_output.sampled_token_ids\n        # TODO(woosuk): The following loop can be slow since it iterates over\n        # the requests one by one. Optimize.\n        num_reqs = self.input_batch.num_reqs\n        for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n            assert req_id is not None\n            req_state = self.requests[req_id]\n            seq_len = (req_state.num_computed_tokens +\n                       scheduler_output.num_scheduled_tokens[req_id])\n            assert seq_len <= req_state.num_tokens\n            if seq_len == req_state.num_tokens:\n                # Append the sampled token to the output token ids.\n                token_id = sampled_token_ids[i]\n                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                req_state.output_token_ids.append(token_id)\n            else:\n                # Ignore the sampled token from the partial request.\n                # Rewind the generator state as if the token was not sampled.\n                generator = self.input_batch.generators.get(i)\n                if generator is not None:\n                    # This relies on cuda-specific torch-internal impl details\n                    generator.set_offset(generator.get_offset() - 4)\n\n        if sampler_output.logprob_token_ids is None:\n            logprob_token_ids = None\n        else:\n            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n        if sampler_output.logprobs is None:\n            logprobs = None\n        else:\n            logprobs = sampler_output.logprobs.cpu()\n\n        # num_reqs entries should be non-None\n        assert all(\n            req_id is not None for req_id in\n            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n\n        model_runner_output = ModelRunnerOutput(\n            req_ids=req_ids,\n            req_id_to_index=self.input_batch.req_id_to_index,\n            sampled_token_ids=sampled_token_ids,\n            logprob_token_ids_cpu=logprob_token_ids,\n            logprobs_cpu=logprobs,\n        )\n        return model_runner_output\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with DeviceMemoryProfiler() as m:  # noqa: SIM117\n            self.model = get_model(vllm_config=self.vllm_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n    @torch.inference_mode()\n    def _dummy_run(\n        self,\n        model: nn.Module,\n        num_tokens: int,\n        kv_caches: List[torch.Tensor],\n    ) -> torch.Tensor:\n        if self.is_multimodal_model:\n            input_ids = None\n            inputs_embeds = self.inputs_embeds[:num_tokens]\n        else:\n            input_ids = self.input_ids[:num_tokens]\n            inputs_embeds = None\n        with set_forward_context(None, self.vllm_config):\n            hidden_states = model(\n                input_ids=input_ids,\n                positions=self.positions[:num_tokens],\n                kv_caches=kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        return hidden_states\n\n    def profile_run(self) -> None:\n        # TODO(woosuk): Profile the max memory usage of the encoder and\n        # the encoder cache.\n        # use an empty tensor instead of `None`` to force Dynamo to pass\n        # it by reference, rather by specializing on the value `None`.\n        # the `dtype` argument does not matter, and we use `float32` as\n        # a placeholder (it has wide hardware support).\n        # it is important to create tensors inside the loop, rather than\n        # multiplying the list, to avoid Dynamo from treating them as\n        # tensor aliasing.\n        dummy_kv_caches = [\n            torch.tensor([], dtype=torch.float32, device=self.device)\n            for _ in range(self.num_attn_layers)\n        ]\n        # Trigger compilation for general shape.\n        hidden_states = self._dummy_run(self.model, self.max_num_tokens,\n                                        dummy_kv_caches)\n        logits = self.model.compute_logits(hidden_states, None)\n        logits = logits[:self.max_num_tokens]\n        # TODO(woosuk): Consider the memory usage of the sampler.\n        torch.cuda.synchronize()\n        del hidden_states, logits\n        gc.collect()\n\n    def capture_model(self) -> None:\n        if not self.use_cuda_graph:\n            logger.warning(\n                \"Skipping CUDA graph capture. Please add \"\n                \"-O %s to use CUDA graphs.\", CompilationLevel.PIECEWISE)\n            return\n\n        start_time = time.perf_counter()\n        start_free_gpu_memory = torch.cuda.mem_get_info()[0]\n\n        # Trigger CUDA graph capture for specific shapes.\n        # Capture the large shapes first so that the smaller shapes\n        # can reuse the memory pool allocated for the large shapes.\n        with graph_capture():\n            for num_tokens in reversed(self.cudagraph_batch_sizes):\n                for _ in range(self.vllm_config.compilation_config.\n                               cudagraph_num_of_warmups):\n                    self._dummy_run(self.model, num_tokens, self.kv_caches)\n                self._dummy_run(self.model, num_tokens, self.kv_caches)\n\n        end_time = time.perf_counter()\n        end_free_gpu_memory = torch.cuda.mem_get_info()[0]\n        elapsed_time = end_time - start_time\n        cuda_graph_size = start_free_gpu_memory - end_free_gpu_memory\n        # This usually takes 5~20 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs, took %.2f GiB\",\n                    elapsed_time, cuda_graph_size / (1 << 30))\n\n    def initialize_kv_cache(self, num_blocks: int) -> None:\n        assert len(self.kv_caches) == 0\n        kv_cache_shape = FlashAttentionBackend.get_kv_cache_shape(\n            num_blocks, self.block_size, self.num_kv_heads, self.head_size)\n        for _ in range(self.num_attn_layers):\n            self.kv_caches.append(\n                torch.zeros(kv_cache_shape,\n                            dtype=self.kv_cache_dtype,\n                            device=self.device))\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex abcd4b007..67166fb05 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -118,6 +118,12 @@ class GPUModelRunner:\n             dtype=self.dtype,\n             device=self.device)\n \n+        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n+        self.arange_np = np.arange(max(self.max_num_reqs, self.max_model_len),\n+                                   dtype=np.int32)\n+        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n+        # a faster version of creating a new tensor every time. Thus, we should\n+        # not make any assumptions about the values in these tensors.\n         self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                          dtype=torch.int32,\n                                          device=\"cpu\",\n@@ -269,11 +275,13 @@ class GPUModelRunner:\n \n         # Get request indices.\n         # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n-        req_indices = np.repeat(np.arange(num_reqs), num_scheduled_tokens)\n+        req_indices = np.repeat(self.arange_np[:num_reqs],\n+                                num_scheduled_tokens)\n \n         # Get batched arange.\n         # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n-        arange = np.concatenate([np.arange(n) for n in num_scheduled_tokens])\n+        arange = np.concatenate(\n+            [self.arange_np[:n] for n in num_scheduled_tokens])\n \n         # Get positions.\n         positions_np = self.positions_np[:total_num_scheduled_tokens]",
      "change_type": "modified",
      "lines_added": 11,
      "lines_removed": 3
    }
  ],
  "affected_apis": [
    "vllm.v1.worker.gpu_model_runner.GPUModelRunner"
  ],
  "summary": {
    "total_files": 1,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_gpu_model_runner)",
    "is_benchmark_actually_there": "",
    "sample_clues": "gpu_model_runner, gpumodelrunner, init"
  }
}