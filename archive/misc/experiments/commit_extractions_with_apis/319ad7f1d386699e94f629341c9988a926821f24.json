{
  "commit_hash": "319ad7f1d386699e94f629341c9988a926821f24",
  "parent_hash": "0f0d8bc065f3608e7657a9696f5d2d7c0d6722d1",
  "message": "[CI/Build][Misc] Add CI that benchmarks vllm performance on those PRs with `perf-benchmarks` label (#5073)\n\nCo-authored-by: simon-mo <simon.mo@hey.com>",
  "author": "Kuntai Du <kuntai@uchicago.edu>",
  "date": "2024-06-13 22:36:20 -0700",
  "files_changed": [
    {
      "file_path": ".buildkite/nightly-benchmarks/README.md",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/README.md b/.buildkite/nightly-benchmarks/README.md\nnew file mode 100644\nindex 000000000..6a18be947\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/README.md\n@@ -0,0 +1,98 @@\n+# vLLM benchmark suite\n+\n+## Introduction\n+\n+This directory contains the performance benchmarking CI for vllm.\n+The goal is to help developers know the impact of their PRs on the performance of vllm.\n+\n+This benchmark will be *triggered* upon:\n+- A PR being merged into vllm.\n+- Every commit for those PRs with `perf-benchmarks` label.\n+\n+**Benchmarking Coverage**: latency, throughput and fix-qps serving on A100 (the support for more GPUs is comming later), with different models.\n+\n+**Benchmarking Duration**: about 1hr.\n+\n+## Configuring the workload for the quick benchmark\n+\n+The workload of the quick benchmark contains two parts: latency tests in `latency-tests.json`, throughput tests in `throughput-tests.json` and serving tests in `serving-tests.json`.\n+\n+### Latency test\n+\n+Here is an example of one test inside `latency-tests.json`:\n+\n+```json\n+[\n+    ...\n+    {\n+        \"test_name\": \"latency_llama8B_tp1\",\n+        \"parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"tensor_parallel_size\": 1,\n+            \"load_format\": \"dummy\",\n+            \"num_iters_warmup\": 5,\n+            \"num_iters\": 15\n+        }\n+    },\n+    ...\n+]\n+```\n+\n+In this example:\n+-  The `test_name` attributes is a unique identifier for the test. In `latency-tests.json`, it must start with `latency_`.\n+-  The `parameters` attribute control the command line arguments to be used for `benchmark_latency.py`. Note that please use underline `_` instead of the dash `-` when specifying the command line arguments, and `run-benchmarks-suite.sh` will convert the underline to dash when feeding the arguments to `benchmark_latency.py`. For example, the corresponding command line arguments for `benchmark_latency.py` will be `--model meta-llama/Meta-Llama-3-8B --tensor-parallel-size 1 --load-format dummy --num-iters-warmup 5 --num-iters 15`\n+\n+Note that the performance numbers are highly sensitive to the value of the parameters. Please make sure the parameters are set correctly.\n+\n+WARNING: The benchmarking script will save json results by itself, so please do not configure `--output-json` parameter in the json file.\n+\n+\n+### Throughput test\n+The tests are specified in `throughput-tests.json`. The syntax is similar to `latency-tests.json`, except for that the parameters will be fed forward to `benchmark_throughput.py`.\n+\n+The number of this test is also stable -- a slight change on the value of this number might vary the performance numbers by a lot.\n+\n+### Serving test\n+We test the throughput by using `benchmark_serving.py` with request rate = inf to cover the online serving overhead. The corresponding parameters are in `serving-tests.json`, and here is an example:\n+\n+```\n+[\n+    ...\n+    {\n+        \"test_name\": \"serving_llama8B_tp1_sharegpt\",\n+        \"qps_list\": [1, 4, 16, \"inf\"],\n+        \"server_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"tensor_parallel_size\": 1,\n+            \"swap_space\": 16,\n+            \"disable_log_stats\": \"\",\n+            \"disable_log_requests\": \"\",\n+            \"load_format\": \"dummy\"\n+        },\n+        \"client_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"backend\": \"vllm\",\n+            \"dataset_name\": \"sharegpt\",\n+            \"dataset_path\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200\n+        }\n+    },\n+    ...\n+]\n+```\n+\n+Inside this example:\n+- The `test_name` attribute is also a unique identifier for the test. It must start with `serving_`.\n+- The `server-parameters` includes the command line arguments for vLLM server.\n+- The `client-parameters` includes the command line arguments for `benchmark_serving.py`.\n+- The `qps_list` controls the list of qps for test. It will be used to configure the `--request-rate` parameter in `benchmark_serving.py`\n+\n+The number of this test is less stable compared to the delay and latency benchmarks (due to randomized sharegpt dataset sampling inside `benchmark_serving.py`), but a large change on this number (e.g. 5% change) still vary the output greatly.\n+\n+WARNING: The benchmarking script will save json results by itself, so please do not configure `--save-results` or other results-saving-related parameters in `serving-tests.json`.\n+\n+## Visualizing the results\n+The `convert-results-json-to-markdown.py` helps you put the benchmarking results inside a markdown table.\n+You can find the result presented as a table inside the `buildkite/performance-benchmark` job page.\n+If you do not see the table, please wait till the benchmark finish running.\n+The JSON file is also attached within each buildkite job for further analysis.\n\\ No newline at end of file",
      "change_type": "added",
      "lines_added": 99,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/benchmark-pipeline.yaml",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/benchmark-pipeline.yaml b/.buildkite/nightly-benchmarks/benchmark-pipeline.yaml\nnew file mode 100644\nindex 000000000..8f12748b6\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/benchmark-pipeline.yaml\n@@ -0,0 +1,61 @@\n+steps:\n+  - label: \"Wait for container to be ready\"\n+    agents:\n+      queue: A100\n+    plugins:\n+    - kubernetes:\n+        podSpec:\n+          containers:\n+          - image: badouralix/curl-jq\n+            command:\n+            - sh\n+            - .buildkite/nightly-benchmarks/scripts/wait-for-image.sh\n+  - wait\n+  - label: \"A100 Benchmark\"\n+    agents:\n+      queue: A100\n+    plugins:\n+    - kubernetes:\n+        podSpec:\n+          containers:\n+          - image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT\n+            command:\n+            - bash .buildkite/nightly-benchmarks/run-benchmarks-suite.sh\n+            resources:\n+              limits:\n+                nvidia.com/gpu: 8\n+            volumeMounts:\n+            - name: devshm\n+              mountPath: /dev/shm\n+            env:\n+            - name: VLLM_USAGE_SOURCE\n+              value: ci-test\n+            - name: HF_TOKEN\n+              valueFrom:\n+                secretKeyRef:\n+                  name: hf-token-secret\n+                  key: token\n+          nodeSelector:\n+            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB\n+          volumes:\n+          - name: devshm\n+            emptyDir:\n+              medium: Memory\n+  # - label: \"H100: NVIDIA SMI\"\n+  #   agents:\n+  #     queue: H100\n+  #   plugins:\n+  #   - docker#v5.11.0:\n+  #       image: public.ecr.aws/q9t5s3a7/vllm-ci-test-repo:$BUILDKITE_COMMIT\n+  #       command:\n+  #       - bash\n+  #       - .buildkite/nightly-benchmarks/run-benchmarks-suite.sh\n+  #       mount-buildkite-agent: true\n+  #       propagate-environment: true\n+  #       propagate-uid-gid: false\n+  #       ipc: host\n+  #       gpus: all\n+  #       environment:\n+  #       - VLLM_USAGE_SOURCE\n+  #       - HF_TOKEN\n+",
      "change_type": "added",
      "lines_added": 62,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/kickoff-pipeline.sh",
      "old_content": "#!/usr/bin/env bash\n\nset -euo pipefail\n\n# Install system packages\napt update\napt install -y curl jq\n\n# Install minijinja for templating\ncurl -sSfL https://github.com/mitsuhiko/minijinja/releases/latest/download/minijinja-cli-installer.sh | sh\nsource $HOME/.cargo/env\n\n# If BUILDKITE_PULL_REQUEST != \"false\", then we check the PR labels using curl and jq\nif [ \"$BUILDKITE_PULL_REQUEST\" != \"false\" ]; then\n  PR_LABELS=$(curl -s \"https://api.github.com/repos/vllm-project/vllm/pulls/$BUILDKITE_PULL_REQUEST\" | jq -r '.labels[].name')\n\n  if [[ $PR_LABELS == *\"perf-benchmarks\"* ]]; then\n    echo \"This PR has the 'perf-benchmarks' label. Proceeding with the nightly benchmarks.\"\n  else\n    echo \"This PR does not have the 'perf-benchmarks' label. Skipping the nightly benchmarks.\"\n    exit 0\n  fi\nfi\n\n# Upload sample.yaml\nbuildkite-agent pipeline upload .buildkite/nightly-benchmarks/sample.yaml\n",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/kickoff-pipeline.sh b/.buildkite/nightly-benchmarks/kickoff-pipeline.sh\nindex d3bf3b729..15d411feb 100755\n--- a/.buildkite/nightly-benchmarks/kickoff-pipeline.sh\n+++ b/.buildkite/nightly-benchmarks/kickoff-pipeline.sh\n@@ -1,5 +1,6 @@\n #!/usr/bin/env bash\n \n+# NOTE(simon): this script runs inside a buildkite agent with CPU only access.\n set -euo pipefail\n \n # Install system packages\n@@ -23,4 +24,4 @@ if [ \"$BUILDKITE_PULL_REQUEST\" != \"false\" ]; then\n fi\n \n # Upload sample.yaml\n-buildkite-agent pipeline upload .buildkite/nightly-benchmarks/sample.yaml\n+buildkite-agent pipeline upload .buildkite/nightly-benchmarks/benchmark-pipeline.yaml",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 2
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/latency-tests.json",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/latency-tests.json b/.buildkite/nightly-benchmarks/latency-tests.json\nnew file mode 100644\nindex 000000000..294a8c439\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/latency-tests.json\n@@ -0,0 +1,32 @@\n+[\n+    {\n+        \"test_name\": \"latency_llama8B_tp1\",\n+        \"parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"tensor_parallel_size\": 1,\n+            \"load_format\": \"dummy\",\n+            \"num_iters_warmup\": 5,\n+            \"num_iters\": 15\n+        }\n+    },\n+    {\n+        \"test_name\": \"latency_llama70B_tp4\",\n+        \"parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            \"tensor_parallel_size\": 4,\n+            \"load_format\": \"dummy\",\n+            \"num-iters-warmup\": 5,\n+            \"num-iters\": 15\n+        }\n+    },\n+    {\n+        \"test_name\": \"latency_mixtral8x7B_tp2\",\n+        \"parameters\": {\n+            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n+            \"tensor_parallel_size\": 2,\n+            \"load_format\": \"dummy\",\n+            \"num-iters-warmup\": 5,\n+            \"num-iters\": 15\n+        }\n+    }\n+]",
      "change_type": "added",
      "lines_added": 33,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/run-benchmarks-suite.sh",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/run-benchmarks-suite.sh b/.buildkite/nightly-benchmarks/run-benchmarks-suite.sh\nnew file mode 100644\nindex 000000000..6cff6917f\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/run-benchmarks-suite.sh\n@@ -0,0 +1,358 @@\n+#!/bin/bash\n+\n+# This script should be run inside the CI process\n+# This script assumes that we are already inside the vllm/ directory\n+# Benchmarking results will be available inside vllm/benchmarks/results/\n+\n+# Do not set -e, as the mixtral 8x22B model tends to crash occasionally\n+# and we still want to see other benchmarking results even when mixtral crashes.\n+set -o pipefail\n+\n+check_gpus() {\n+  # check the number of GPUs and GPU type.\n+  declare -g gpu_count=$(nvidia-smi --list-gpus | wc -l)\n+  if [[ $gpu_count -gt 0 ]]; then\n+    echo \"GPU found.\"\n+  else\n+    echo \"Need at least 1 GPU to run benchmarking.\"\n+    exit 1\n+  fi\n+  declare -g gpu_type=$(echo $(nvidia-smi --query-gpu=name --format=csv,noheader) | awk '{print $2}')\n+  echo \"GPU type is $gpu_type\"\n+}\n+\n+check_hf_token() {\n+  # check if HF_TOKEN is available and valid\n+  if [[ -z \"$HF_TOKEN\" ]]; then\n+    echo \"Error: HF_TOKEN is not set.\"\n+    exit 1\n+  elif [[ ! \"$HF_TOKEN\" =~ ^hf_ ]]; then\n+    echo \"Error: HF_TOKEN does not start with 'hf_'.\"\n+    exit 1\n+  else\n+    echo \"HF_TOKEN is set and valid.\"\n+  fi\n+}\n+\n+json2args() {\n+  # transforms the JSON string to command line args, and '_' is replaced to '-'\n+  # example:\n+  # input: { \"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"tensor_parallel_size\": 1 }\n+  # output: --model meta-llama/Llama-2-7b-chat-hf --tensor-parallel-size 1\n+  local json_string=$1\n+  local args=$(\n+    echo \"$json_string\" | jq -r '\n+      to_entries |\n+      map(\"--\" + (.key | gsub(\"_\"; \"-\")) + \" \" + (.value | tostring)) |\n+      join(\" \")\n+    '\n+  )\n+  echo \"$args\"\n+}\n+\n+wait_for_server() {\n+  # wait for vllm server to start\n+  # return 1 if vllm server crashes\n+  timeout 1200 bash -c '\n+    until curl localhost:8000/v1/completions; do\n+      sleep 1\n+    done' && return 0 || return 1\n+}\n+\n+kill_gpu_processes() {\n+  # kill all processes on GPU.\n+  pids=$(nvidia-smi --query-compute-apps=pid --format=csv,noheader)\n+  if [ -z \"$pids\" ]; then\n+      echo \"No GPU processes found.\"\n+  else\n+      for pid in $pids; do\n+          kill -9 \"$pid\"\n+          echo \"Killed process with PID: $pid\"\n+      done\n+\n+      echo \"All GPU processes have been killed.\"\n+  fi\n+\n+  # waiting for GPU processes to be fully killed\n+  sleep 10\n+\n+  # remove vllm config file\n+  rm -rf ~/.config/vllm\n+\n+  # Print the GPU memory usage\n+  # so that we know if all GPU processes are killed.\n+  gpu_memory_usage=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits -i 0)\n+  # The memory usage should be 0 MB.\n+  echo \"GPU 0 Memory Usage: $gpu_memory_usage MB\"\n+}\n+\n+upload_to_buildkite() {\n+  # upload the benchmarking results to buildkite\n+\n+  # if the agent binary is not found, skip uploading the results, exit 0\n+  if [ ! -f /workspace/buildkite-agent ]; then\n+    echo \"buildkite-agent binary not found. Skip uploading the results.\"\n+    return 0\n+  fi\n+  /workspace/buildkite-agent annotate --style \"info\" --context \"benchmark-results\" < $RESULTS_FOLDER/benchmark_results.md\n+  /workspace/buildkite-agent artifact upload \"$RESULTS_FOLDER/*\"\n+}\n+\n+run_latency_tests() {\n+  # run latency tests using `benchmark_latency.py`\n+  # $1: a json file specifying latency test cases\n+\n+  local latency_test_file\n+  latency_test_file=$1\n+\n+  # Iterate over latency tests\n+  jq -c '.[]' \"$latency_test_file\" | while read -r params; do\n+    # get the test name, and append the GPU type back to it.\n+    test_name=$(echo \"$params\" | jq -r '.test_name')\n+    if [[ ! \"$test_name\" =~ ^latency_ ]]; then\n+      echo \"In latency-test.json, test_name must start with \\\"latency_\\\".\"\n+      exit 1\n+    fi\n+\n+    # if TEST_SELECTOR is set, only run the test cases that match the selector\n+    if [[ -n \"$TEST_SELECTOR\" ]] && [[ ! \"$test_name\" =~ $TEST_SELECTOR ]]; then\n+      echo \"Skip test case $test_name.\"\n+      continue\n+    fi\n+\n+    # get arguments\n+    latency_params=$(echo \"$params\" | jq -r '.parameters')\n+    latency_args=$(json2args \"$latency_params\")\n+\n+    # check if there is enough GPU to run the test\n+    tp=$(echo \"$latency_params\" | jq -r '.tensor_parallel_size')\n+    if [[ $gpu_count -lt $tp ]]; then\n+      echo \"Required tensor-parallel-size $tp but only $gpu_count GPU found. Skip testcase $testname.\"\n+      continue\n+    fi\n+\n+    latency_command=\"python3 benchmark_latency.py \\\n+      --output-json $RESULTS_FOLDER/${test_name}.json \\\n+      $latency_args\"\n+\n+    echo \"Running test case $test_name\"\n+    echo \"Latency command: $latency_command\"\n+\n+    # recoding benchmarking command ang GPU command\n+    jq_output=$(jq -n \\\n+      --arg latency \"$latency_command\" \\\n+      --arg gpu \"$gpu_type\" \\\n+      '{\n+        latency_command: $latency,\n+        gpu_type: $gpu\n+      }')\n+    echo \"$jq_output\" > \"$RESULTS_FOLDER/$test_name.commands\"\n+\n+    # run the benchmark\n+    eval \"$latency_command\"\n+\n+    kill_gpu_processes\n+\n+  done\n+}\n+\n+\n+run_throughput_tests() {\n+  # run throughput tests using `benchmark_throughput.py`\n+  # $1: a json file specifying throughput test cases\n+\n+  local throughput_test_file\n+  throughput_test_file=$1\n+\n+  # Iterate over throughput tests\n+  jq -c '.[]' \"$throughput_test_file\" | while read -r params; do\n+    # get the test name, and append the GPU type back to it.\n+    test_name=$(echo \"$params\" | jq -r '.test_name')\n+    if [[ ! \"$test_name\" =~ ^throughput_ ]]; then\n+      echo \"In throughput-test.json, test_name must start with \\\"throughput_\\\".\"\n+      exit 1\n+    fi\n+\n+    # if TEST_SELECTOR is set, only run the test cases that match the selector\n+    if [[ -n \"$TEST_SELECTOR\" ]] && [[ ! \"$test_name\" =~ $TEST_SELECTOR ]]; then\n+      echo \"Skip test case $test_name.\"\n+      continue\n+    fi\n+\n+    # get arguments\n+    throughput_params=$(echo \"$params\" | jq -r '.parameters')\n+    throughput_args=$(json2args \"$throughput_params\")\n+\n+    # check if there is enough GPU to run the test\n+    tp=$(echo $throughput_params | jq -r '.tensor_parallel_size')\n+    if [[ $gpu_count -lt $tp ]]; then\n+      echo \"Required tensor-parallel-size $tp but only $gpu_count GPU found. Skip testcase $testname.\"\n+      continue\n+    fi\n+\n+    throughput_command=\"python3 benchmark_throughput.py \\\n+      --output-json $RESULTS_FOLDER/${test_name}.json \\\n+      $throughput_args\"\n+\n+    echo \"Running test case $test_name\"\n+    echo \"Throughput command: $throughput_command\"\n+    # recoding benchmarking command ang GPU command\n+    jq_output=$(jq -n \\\n+      --arg command \"$throughput_command\" \\\n+      --arg gpu \"$gpu_type\" \\\n+      '{\n+        throughput_command: $command,\n+        gpu_type: $gpu\n+      }')\n+    echo \"$jq_output\" > \"$RESULTS_FOLDER/$test_name.commands\"\n+\n+    # run the benchmark\n+    eval \"$throughput_command\"\n+\n+    kill_gpu_processes\n+\n+  done\n+}\n+\n+run_serving_tests() {\n+  # run serving tests using `benchmark_serving.py`\n+  # $1: a json file specifying serving test cases\n+\n+  local serving_test_file\n+  serving_test_file=$1\n+\n+  # Iterate over serving tests\n+  jq -c '.[]' \"$serving_test_file\" | while read -r params; do\n+    # get the test name, and append the GPU type back to it.\n+    test_name=$(echo \"$params\" | jq -r '.test_name')\n+    if [[ ! \"$test_name\" =~ ^serving_ ]]; then\n+      echo \"In serving-test.json, test_name must start with \\\"serving_\\\".\"\n+      exit 1\n+    fi\n+\n+    # if TEST_SELECTOR is set, only run the test cases that match the selector\n+    if [[ -n \"$TEST_SELECTOR\" ]] && [[ ! \"$test_name\" =~ $TEST_SELECTOR ]]; then\n+      echo \"Skip test case $test_name.\"\n+      continue\n+    fi\n+\n+\n+    # get client and server arguments\n+    server_params=$(echo \"$params\" | jq -r '.server_parameters')\n+    client_params=$(echo \"$params\" | jq -r '.client_parameters')\n+    server_args=$(json2args \"$server_params\")\n+    client_args=$(json2args \"$client_params\")\n+    qps_list=$(echo \"$params\" | jq -r '.qps_list')\n+    qps_list=$(echo \"$qps_list\" | jq -r '.[] | @sh')\n+    echo \"Running over qps list $qps_list\"\n+\n+    # check if there is enough GPU to run the test\n+    tp=$(echo \"$server_params\" | jq -r '.tensor_parallel_size')\n+    if [[ $gpu_count -lt $tp ]]; then\n+      echo \"Required tensor-parallel-size $tp but only $gpu_count GPU found. Skip testcase $testname.\"\n+      continue\n+    fi\n+\n+    # check if server model and client model is aligned\n+    server_model=$(echo \"$server_params\" | jq -r '.model')\n+    client_model=$(echo \"$client_params\" | jq -r '.model')\n+    if [[ $server_model != \"$client_model\" ]]; then\n+      echo \"Server model and client model must be the same. Skip testcase $testname.\"\n+      continue\n+    fi\n+\n+    server_command=\"python3 \\\n+      -m vllm.entrypoints.openai.api_server \\\n+      $server_args\"\n+\n+    # run the server\n+    echo \"Running test case $test_name\"\n+    echo \"Server command: $server_command\"\n+    eval \"$server_command\" &\n+\n+    # wait until the server is alive\n+    wait_for_server\n+    if [ $? -eq 0 ]; then\n+      echo \"\"\n+      echo \"vllm server is up and running.\"\n+    else\n+      echo \"\"\n+      echo \"vllm failed to start within the timeout period.\"\n+    fi\n+\n+    # iterate over different QPS\n+    for qps in $qps_list; do\n+      # remove the surrounding single quote from qps\n+      if [[ \"$qps\" == *\"inf\"* ]]; then\n+        echo \"qps was $qps\"\n+        qps=\"inf\"\n+        echo \"now qps is $qps\"\n+      fi\n+\n+      new_test_name=$test_name\"_qps_\"$qps\n+\n+      client_command=\"python3 benchmark_serving.py \\\n+        --save-result \\\n+        --result-dir $RESULTS_FOLDER \\\n+        --result-filename ${new_test_name}.json \\\n+        --request-rate $qps \\\n+        $client_args\"\n+\n+      echo \"Running test case $test_name with qps $qps\"\n+      echo \"Client command: $client_command\"\n+\n+      eval \"$client_command\"\n+\n+      # record the benchmarking commands\n+      jq_output=$(jq -n \\\n+        --arg server \"$server_command\" \\\n+        --arg client \"$client_command\" \\\n+        --arg gpu \"$gpu_type\" \\\n+        '{\n+          server_command: $server,\n+          client_command: $client,\n+          gpu_type: $gpu\n+        }')\n+      echo \"$jq_output\" > \"$RESULTS_FOLDER/${new_test_name}.commands\"\n+\n+    done\n+\n+    # clean up\n+    kill_gpu_processes\n+  done\n+}\n+\n+main() {\n+  check_gpus\n+  check_hf_token\n+\n+  # dependencies\n+  (which wget && which curl) || (apt-get update && apt-get install -y wget curl)\n+  (which jq) || (apt-get update && apt-get -y install jq)\n+\n+  # get the current IP address, required by benchmark_serving.py\n+  export VLLM_HOST_IP=$(hostname -I | awk '{print $1}')\n+  # turn of the reporting of the status of each request, to clean up the terminal output\n+  export VLLM_LOG_LEVEL=\"WARNING\"\n+\n+  # prepare for benchmarking\n+  cd benchmarks || exit 1\n+  wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json\n+  declare -g RESULTS_FOLDER=results/\n+  mkdir -p $RESULTS_FOLDER\n+  QUICK_BENCHMARK_ROOT=../.buildkite/nightly-benchmarks/\n+\n+  # benchmarking\n+  run_serving_tests $QUICK_BENCHMARK_ROOT/serving-tests.json\n+  run_latency_tests $QUICK_BENCHMARK_ROOT/latency-tests.json\n+  run_throughput_tests $QUICK_BENCHMARK_ROOT/throughput-tests.json\n+\n+\n+  # postprocess benchmarking results\n+  pip install tabulate pandas\n+  python3 $QUICK_BENCHMARK_ROOT/scripts/convert-results-json-to-markdown.py\n+\n+  upload_to_buildkite\n+}\n+\n+main \"$@\"",
      "change_type": "added",
      "lines_added": 359,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/sample.yaml",
      "old_content": "steps:\n  # NOTE(simon): You can create separate blocks for different jobs\n  - label: \"A100: NVIDIA SMI\"\n    agents:\n      queue: A100\n    plugins:\n    - kubernetes:\n        podSpec:\n          containers:\n          # - image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:$BUILDKITE_COMMIT\n          # TODO(simon): check latest main branch or use the PR image.\n          - image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:45c35f0d58f4508bf43bd6af1d3d0d0ec0c915e6\n            command:\n            - bash -c 'nvidia-smi && nvidia-smi topo -m && pwd && ls'\n            resources:\n              limits:\n                nvidia.com/gpu: 8\n            volumeMounts:\n            - name: devshm\n              mountPath: /dev/shm\n          nodeSelector:\n            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB\n          volumes:\n          - name: devshm\n            emptyDir:\n              medium: Memory\n  # TODO(simon): bring H100 online\n  # - label: \"H100: NVIDIA SMI\"\n  #   agents:\n  #     queue: H100\n  #   plugins:\n  #   - docker#v5.11.0:\n  #       image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:45c35f0d58f4508bf43bd6af1d3d0d0ec0c915e6\n  #       command:\n  #       - bash -c 'nvidia-smi && nvidia-smi topo -m'\n  #       propagate-environment: true\n  #       ipc: host\n  #       gpus: all\n\n",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/sample.yaml b/.buildkite/nightly-benchmarks/sample.yaml\ndeleted file mode 100644\nindex 50e6e8207..000000000\n--- a/.buildkite/nightly-benchmarks/sample.yaml\n+++ /dev/null\n@@ -1,39 +0,0 @@\n-steps:\n-  # NOTE(simon): You can create separate blocks for different jobs\n-  - label: \"A100: NVIDIA SMI\"\n-    agents:\n-      queue: A100\n-    plugins:\n-    - kubernetes:\n-        podSpec:\n-          containers:\n-          # - image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:$BUILDKITE_COMMIT\n-          # TODO(simon): check latest main branch or use the PR image.\n-          - image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:45c35f0d58f4508bf43bd6af1d3d0d0ec0c915e6\n-            command:\n-            - bash -c 'nvidia-smi && nvidia-smi topo -m && pwd && ls'\n-            resources:\n-              limits:\n-                nvidia.com/gpu: 8\n-            volumeMounts:\n-            - name: devshm\n-              mountPath: /dev/shm\n-          nodeSelector:\n-            nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB\n-          volumes:\n-          - name: devshm\n-            emptyDir:\n-              medium: Memory\n-  # TODO(simon): bring H100 online\n-  # - label: \"H100: NVIDIA SMI\"\n-  #   agents:\n-  #     queue: H100\n-  #   plugins:\n-  #   - docker#v5.11.0:\n-  #       image: us-central1-docker.pkg.dev/vllm-405802/vllm-ci-test-repo/vllm-test:45c35f0d58f4508bf43bd6af1d3d0d0ec0c915e6\n-  #       command:\n-  #       - bash -c 'nvidia-smi && nvidia-smi topo -m'\n-  #       propagate-environment: true\n-  #       ipc: host\n-  #       gpus: all\n-",
      "change_type": "deleted",
      "lines_added": 1,
      "lines_removed": 40
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py b/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py\nnew file mode 100644\nindex 000000000..75cff8434\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/scripts/convert-results-json-to-markdown.py\n@@ -0,0 +1,155 @@\n+import json\n+from pathlib import Path\n+\n+import pandas as pd\n+from tabulate import tabulate\n+\n+results_folder = Path(\"results/\")\n+\n+# latency results and the keys that will be printed into markdown\n+latency_results = []\n+latency_column_mapping = {\n+    \"test_name\": \"Test name\",\n+    \"gpu_type\": \"GPU\",\n+    \"avg_latency\": \"Average latency (s)\",\n+    \"P10\": \"P10 (s)\",\n+    \"P25\": \"P25 (s)\",\n+    \"P50\": \"P50 (s)\",\n+    \"P75\": \"P75 (s)\",\n+    \"P90\": \"P90 (s)\",\n+}\n+\n+# thoughput tests and the keys that will be printed into markdown\n+throughput_results = []\n+throughput_results_column_mapping = {\n+    \"test_name\": \"Test name\",\n+    \"gpu_type\": \"GPU\",\n+    \"num_requests\": \"# of req.\",\n+    \"total_num_tokens\": \"Total # of tokens\",\n+    \"elapsed_time\": \"Elapsed time (s)\",\n+    \"requests_per_second\": \"Tput (req/s)\",\n+    \"tokens_per_second\": \"Tput (tok/s)\",\n+}\n+\n+# serving results and the keys that will be printed into markdown\n+serving_results = []\n+serving_column_mapping = {\n+    \"test_name\": \"Test name\",\n+    \"gpu_type\": \"GPU\",\n+    \"completed\": \"# of req.\",\n+    \"request_throughput\": \"Tput (req/s)\",\n+    \"input_throughput\": \"Input Tput (tok/s)\",\n+    \"output_throughput\": \"Output Tput (tok/s)\",\n+    \"mean_ttft_ms\": \"Mean TTFT (ms)\",\n+    # do not say TTFT again to avoid the table getting too wide\n+    \"median_ttft_ms\": \"Median\",\n+    \"p99_ttft_ms\": \"P99\",\n+    \"mean_tpot_ms\": \"Mean TPOT (ms)\",\n+    \"median_tpot_ms\": \"Median\",\n+    \"p99_tpot_ms\": \"P99\",\n+    \"mean_itl_ms\": \"Mean ITL (ms)\",\n+    \"median_itl_ms\": \"Median\",\n+    \"p99_itl_ms\": \"P99\",\n+}\n+\n+for test_file in results_folder.glob(\"*.json\"):\n+\n+    with open(test_file, \"r\") as f:\n+        raw_result = json.loads(f.read())\n+\n+    if \"serving\" in str(test_file):\n+        # this result is generated via `benchmark_serving.py`\n+\n+        # attach the benchmarking command to raw_result\n+        with open(test_file.with_suffix(\".commands\"), \"r\") as f:\n+            command = json.loads(f.read())\n+        raw_result.update(command)\n+\n+        # update the test name of this result\n+        raw_result.update({\"test_name\": test_file.stem})\n+\n+        # add the result to raw_result\n+        serving_results.append(raw_result)\n+        continue\n+\n+    elif \"latency\" in f.name:\n+        # this result is generated via `benchmark_latency.py`\n+\n+        # attach the benchmarking command to raw_result\n+        with open(test_file.with_suffix(\".commands\"), \"r\") as f:\n+            command = json.loads(f.read())\n+        raw_result.update(command)\n+\n+        # update the test name of this result\n+        raw_result.update({\"test_name\": test_file.stem})\n+\n+        # get different percentiles\n+        for perc in [10, 25, 50, 75, 90]:\n+            raw_result.update(\n+                {f\"P{perc}\": raw_result[\"percentiles\"][str(perc)]})\n+\n+        # add the result to raw_result\n+        latency_results.append(raw_result)\n+        continue\n+\n+    elif \"throughput\" in f.name:\n+        # this result is generated via `benchmark_throughput.py`\n+\n+        # attach the benchmarking command to raw_result\n+        with open(test_file.with_suffix(\".commands\"), \"r\") as f:\n+            command = json.loads(f.read())\n+        raw_result.update(command)\n+\n+        # update the test name of this result\n+        raw_result.update({\"test_name\": test_file.stem})\n+\n+        # add the result to raw_result\n+        throughput_results.append(raw_result)\n+        continue\n+\n+    print(f\"Skipping {test_file}\")\n+\n+latency_results = pd.DataFrame.from_dict(latency_results)\n+serving_results = pd.DataFrame.from_dict(serving_results)\n+throughput_results = pd.DataFrame.from_dict(throughput_results)\n+\n+# remapping the key, for visualization purpose\n+if not latency_results.empty:\n+    latency_results = latency_results[list(\n+        latency_column_mapping.keys())].rename(columns=latency_column_mapping)\n+if not serving_results.empty:\n+    serving_results = serving_results[list(\n+        serving_column_mapping.keys())].rename(columns=serving_column_mapping)\n+if not throughput_results.empty:\n+    throughput_results = throughput_results[list(\n+        throughput_results_column_mapping.keys())].rename(\n+            columns=throughput_results_column_mapping)\n+\n+# get markdown tables\n+latency_md_table = tabulate(latency_results,\n+                            headers='keys',\n+                            tablefmt='pipe',\n+                            showindex=False)\n+serving_md_table = tabulate(serving_results,\n+                            headers='keys',\n+                            tablefmt='pipe',\n+                            showindex=False)\n+throughput_md_table = tabulate(throughput_results,\n+                               headers='keys',\n+                               tablefmt='pipe',\n+                               showindex=False)\n+\n+# document the result\n+with open(results_folder / \"benchmark_results.md\", \"w\") as f:\n+    if not latency_results.empty:\n+        f.write(\"## Latency tests\\n\")\n+        f.write(latency_md_table)\n+        f.write(\"\\n\")\n+    if not throughput_results.empty:\n+        f.write(\"## Throughput tests\\n\")\n+        f.write(throughput_md_table)\n+        f.write(\"\\n\")\n+    if not serving_results.empty:\n+        f.write(\"## Serving tests\\n\")\n+        f.write(serving_md_table)\n+        f.write(\"\\n\")",
      "change_type": "added",
      "lines_added": 156,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/scripts/wait-for-image.sh",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/scripts/wait-for-image.sh b/.buildkite/nightly-benchmarks/scripts/wait-for-image.sh\nnew file mode 100644\nindex 000000000..c785e6a0d\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/scripts/wait-for-image.sh\n@@ -0,0 +1,17 @@\n+#!/bin/sh\n+TOKEN=$(curl -s -L \"https://public.ecr.aws/token?service=public.ecr.aws&scope=repository:q9t5s3a7/vllm-ci-test-repo:pull\" | jq -r .token)\n+URL=\"https://public.ecr.aws/v2/q9t5s3a7/vllm-ci-test-repo/manifests/$BUILDKITE_COMMIT\"\n+\n+retries=0\n+while [ $retries -lt 1000 ]; do\n+    if [ $(curl -s -L -H \"Authorization: Bearer $TOKEN\" -o /dev/null -w \"%{http_code}\" $URL) -eq 200 ]; then\n+        exit 0\n+    fi\n+\n+    echo \"Waiting for image to be available...\"\n+\n+    retries=$((retries + 1))\n+    sleep 5\n+done\n+\n+exit 1\n\\ No newline at end of file",
      "change_type": "added",
      "lines_added": 18,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/serving-tests.json",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/serving-tests.json b/.buildkite/nightly-benchmarks/serving-tests.json\nnew file mode 100644\nindex 000000000..bb6746612\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/serving-tests.json\n@@ -0,0 +1,59 @@\n+[\n+    {\n+        \"test_name\": \"serving_llama8B_tp1_sharegpt\",\n+        \"qps_list\": [1, 4, 16, \"inf\"],\n+        \"server_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"tensor_parallel_size\": 1,\n+            \"swap_space\": 16,\n+            \"disable_log_stats\": \"\",\n+            \"disable_log_requests\": \"\",\n+            \"load_format\": \"dummy\"\n+        },\n+        \"client_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"backend\": \"vllm\",\n+            \"dataset_name\": \"sharegpt\",\n+            \"dataset_path\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200\n+        }\n+    },\n+    {\n+        \"test_name\": \"serving_llama70B_tp4_sharegpt\",\n+        \"qps_list\": [1, 4, 16, \"inf\"],\n+        \"server_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            \"tensor_parallel_size\": 4,\n+            \"swap_space\": 16,\n+            \"disable_log_stats\": \"\",\n+            \"disable_log_requests\": \"\",\n+            \"load_format\": \"dummy\"\n+        },\n+        \"client_parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            \"backend\": \"vllm\",\n+            \"dataset_name\": \"sharegpt\",\n+            \"dataset_path\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200\n+        }\n+    },\n+    {\n+        \"test_name\": \"serving_mixtral8x7B_tp2_sharegpt\",\n+        \"qps_list\": [1, 4, 16, \"inf\"],\n+        \"server_parameters\": {\n+            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n+            \"tensor_parallel_size\": 2,\n+            \"swap_space\": 16,\n+            \"disable_log_stats\": \"\",\n+            \"disable_log_requests\": \"\",\n+            \"load_format\": \"dummy\"\n+        },\n+        \"client_parameters\": {\n+            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n+            \"backend\": \"vllm\",\n+            \"dataset_name\": \"sharegpt\",\n+            \"dataset_path\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200\n+        }\n+    }\n+]",
      "change_type": "added",
      "lines_added": 60,
      "lines_removed": 1
    },
    {
      "file_path": ".buildkite/nightly-benchmarks/throughput-tests.json",
      "old_content": "",
      "diff": "diff --git a/.buildkite/nightly-benchmarks/throughput-tests.json b/.buildkite/nightly-benchmarks/throughput-tests.json\nnew file mode 100644\nindex 000000000..db4f908d7\n--- /dev/null\n+++ b/.buildkite/nightly-benchmarks/throughput-tests.json\n@@ -0,0 +1,35 @@\n+[\n+    {\n+        \"test_name\": \"throughput_llama8B_tp1\",\n+        \"parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-8B\",\n+            \"tensor_parallel_size\": 1,\n+            \"load_format\": \"dummy\",\n+            \"dataset\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200,\n+            \"backend\": \"vllm\"\n+        }\n+    },\n+    {\n+        \"test_name\": \"throughput_llama70B_tp4\",\n+        \"parameters\": {\n+            \"model\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n+            \"tensor_parallel_size\": 4,\n+            \"load_format\": \"dummy\",\n+            \"dataset\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200,\n+            \"backend\": \"vllm\"\n+        }\n+    },\n+    {\n+        \"test_name\": \"throughput_mixtral8x7B_tp2\",\n+        \"parameters\": {\n+            \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n+            \"tensor_parallel_size\": 2,\n+            \"load_format\": \"dummy\",\n+            \"dataset\": \"./ShareGPT_V3_unfiltered_cleaned_split.json\",\n+            \"num_prompts\": 200,\n+            \"backend\": \"vllm\"\n+        }\n+    }\n+]",
      "change_type": "added",
      "lines_added": 36,
      "lines_removed": 1
    },
    {
      "file_path": "benchmarks/benchmark_latency.py",
      "old_content": "\"\"\"Benchmark the latency of processing a single batch of requests.\"\"\"\nimport argparse\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import List, Optional\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom vllm import LLM, SamplingParams\nfrom vllm.inputs import PromptStrictInputs\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\n\n\ndef main(args: argparse.Namespace):\n    print(args)\n\n    # NOTE(woosuk): If the request cannot be processed in a single batch,\n    # the engine will automatically process the request in multiple batches.\n    llm = LLM(model=args.model,\n              speculative_model=args.speculative_model,\n              num_speculative_tokens=args.num_speculative_tokens,\n              tokenizer=args.tokenizer,\n              quantization=args.quantization,\n              tensor_parallel_size=args.tensor_parallel_size,\n              trust_remote_code=args.trust_remote_code,\n              dtype=args.dtype,\n              enforce_eager=args.enforce_eager,\n              kv_cache_dtype=args.kv_cache_dtype,\n              quantization_param_path=args.quantization_param_path,\n              device=args.device,\n              ray_workers_use_nsight=args.ray_workers_use_nsight,\n              use_v2_block_manager=args.use_v2_block_manager,\n              enable_chunked_prefill=args.enable_chunked_prefill,\n              download_dir=args.download_dir,\n              block_size=args.block_size,\n              gpu_memory_utilization=args.gpu_memory_utilization,\n              distributed_executor_backend=args.distributed_executor_backend)\n\n    sampling_params = SamplingParams(\n        n=args.n,\n        temperature=0.0 if args.use_beam_search else 1.0,\n        top_p=1.0,\n        use_beam_search=args.use_beam_search,\n        ignore_eos=True,\n        max_tokens=args.output_len,\n    )\n    print(sampling_params)\n    dummy_prompt_token_ids = np.random.randint(10000,\n                                               size=(args.batch_size,\n                                                     args.input_len))\n    dummy_inputs: List[PromptStrictInputs] = [{\n        \"prompt_token_ids\": batch\n    } for batch in dummy_prompt_token_ids.tolist()]\n\n    def run_to_completion(profile_dir: Optional[str] = None):\n        if profile_dir:\n            with torch.profiler.profile(\n                    activities=[\n                        torch.profiler.ProfilerActivity.CPU,\n                        torch.profiler.ProfilerActivity.CUDA,\n                    ],\n                    on_trace_ready=torch.profiler.tensorboard_trace_handler(\n                        str(profile_dir))) as p:\n                llm.generate(dummy_inputs,\n                             sampling_params=sampling_params,\n                             use_tqdm=False)\n            print(p.key_averages())\n        else:\n            start_time = time.perf_counter()\n            llm.generate(dummy_inputs,\n                         sampling_params=sampling_params,\n                         use_tqdm=False)\n            end_time = time.perf_counter()\n            latency = end_time - start_time\n            return latency\n\n    print(\"Warming up...\")\n    for _ in tqdm(range(args.num_iters_warmup), desc=\"Warmup iterations\"):\n        run_to_completion(profile_dir=None)\n\n    if args.profile:\n        profile_dir = args.profile_result_dir\n        if not profile_dir:\n            profile_dir = Path(\n                \".\"\n            ) / \"vllm_benchmark_result\" / f\"latency_result_{time.time()}\"\n        print(f\"Profiling (results will be saved to '{profile_dir}')...\")\n        run_to_completion(profile_dir=profile_dir)\n        return\n\n    # Benchmark.\n    latencies = []\n    for _ in tqdm(range(args.num_iters), desc=\"Profiling iterations\"):\n        latencies.append(run_to_completion(profile_dir=None))\n    latencies = np.array(latencies)\n    percentages = [10, 25, 50, 75, 90]\n    percentiles = np.percentile(latencies, percentages)\n    print(f'Avg latency: {np.mean(latencies)} seconds')\n    for percentage, percentile in zip(percentages, percentiles):\n        print(f'{percentage}% percentile latency: {percentile} seconds')\n\n    # Output JSON results if specified\n    if args.output_json:\n        results = {\n            \"avg_latency\": np.mean(latencies),\n            \"latencies\": latencies.tolist(),\n            \"percentiles\": dict(zip(percentages, percentiles.tolist())),\n        }\n        with open(args.output_json, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description='Benchmark the latency of processing a single batch of '\n        'requests till completion.')\n    parser.add_argument('--model', type=str, default='facebook/opt-125m')\n    parser.add_argument('--speculative-model', type=str, default=None)\n    parser.add_argument('--num-speculative-tokens', type=int, default=None)\n    parser.add_argument('--tokenizer', type=str, default=None)\n    parser.add_argument('--quantization',\n                        '-q',\n                        choices=[*QUANTIZATION_METHODS, None],\n                        default=None)\n    parser.add_argument('--tensor-parallel-size', '-tp', type=int, default=1)\n    parser.add_argument('--input-len', type=int, default=32)\n    parser.add_argument('--output-len', type=int, default=128)\n    parser.add_argument('--batch-size', type=int, default=8)\n    parser.add_argument('--n',\n                        type=int,\n                        default=1,\n                        help='Number of generated sequences per prompt.')\n    parser.add_argument('--use-beam-search', action='store_true')\n    parser.add_argument('--num-iters-warmup',\n                        type=int,\n                        default=10,\n                        help='Number of iterations to run for warmup.')\n    parser.add_argument('--num-iters',\n                        type=int,\n                        default=30,\n                        help='Number of iterations to run.')\n    parser.add_argument('--trust-remote-code',\n                        action='store_true',\n                        help='trust remote code from huggingface')\n    parser.add_argument(\n        '--dtype',\n        type=str,\n        default='auto',\n        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],\n        help='data type for model weights and activations. '\n        'The \"auto\" option will use FP16 precision '\n        'for FP32 and FP16 models, and BF16 precision '\n        'for BF16 models.')\n    parser.add_argument('--enforce-eager',\n                        action='store_true',\n                        help='enforce eager mode and disable CUDA graph')\n    parser.add_argument(\n        '--kv-cache-dtype',\n        type=str,\n        choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n        default=\"auto\",\n        help='Data type for kv cache storage. If \"auto\", will use model '\n        'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n        'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n    parser.add_argument(\n        '--quantization-param-path',\n        type=str,\n        default=None,\n        help='Path to the JSON file containing the KV cache scaling factors. '\n        'This should generally be supplied, when KV cache dtype is FP8. '\n        'Otherwise, KV cache scaling factors default to 1.0, which may cause '\n        'accuracy issues. FP8_E5M2 (without scaling) is only supported on '\n        'cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is '\n        'instead supported for common inference criteria.')\n    parser.add_argument(\n        '--profile',\n        action='store_true',\n        help='profile the generation process of a single batch')\n    parser.add_argument(\n        '--profile-result-dir',\n        type=str,\n        default=None,\n        help=('path to save the pytorch profiler output. Can be visualized '\n              'with ui.perfetto.dev or Tensorboard.'))\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cuda\", \"cpu\", \"tpu\"],\n        help='device type for vLLM execution, supporting CUDA and CPU.')\n    parser.add_argument('--block-size',\n                        type=int,\n                        default=16,\n                        help='block size of key/value cache')\n    parser.add_argument(\n        '--enable-chunked-prefill',\n        action='store_true',\n        help='If True, the prefill requests can be chunked based on the '\n        'max_num_batched_tokens')\n    parser.add_argument('--use-v2-block-manager', action='store_true')\n    parser.add_argument(\n        \"--ray-workers-use-nsight\",\n        action='store_true',\n        help=\"If specified, use nsight to profile ray workers\",\n    )\n    parser.add_argument('--download-dir',\n                        type=str,\n                        default=None,\n                        help='directory to download and load the weights, '\n                        'default to the default cache dir of huggingface')\n    parser.add_argument(\n        '--output-json',\n        type=str,\n        default=None,\n        help='Path to save the latency results in JSON format.')\n    parser.add_argument('--gpu-memory-utilization',\n                        type=float,\n                        default=0.9,\n                        help='the fraction of GPU memory to be used for '\n                        'the model executor, which can range from 0 to 1.'\n                        'If unspecified, will use the default value of 0.9.')\n    parser.add_argument(\n        '--distributed-executor-backend',\n        choices=['ray', 'mp'],\n        default=None,\n        help='Backend to use for distributed serving. When more than 1 GPU '\n        'is used, will be automatically set to \"ray\" if installed '\n        'or \"mp\" (multiprocessing) otherwise.')\n    args = parser.parse_args()\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_latency.py b/benchmarks/benchmark_latency.py\nindex 17edb7515..9937f8333 100644\n--- a/benchmarks/benchmark_latency.py\n+++ b/benchmarks/benchmark_latency.py\n@@ -10,6 +10,7 @@ import torch\n from tqdm import tqdm\n \n from vllm import LLM, SamplingParams\n+from vllm.engine.arg_utils import EngineArgs\n from vllm.inputs import PromptStrictInputs\n from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\n \n@@ -37,6 +38,7 @@ def main(args: argparse.Namespace):\n               download_dir=args.download_dir,\n               block_size=args.block_size,\n               gpu_memory_utilization=args.gpu_memory_utilization,\n+              load_format=args.load_format,\n               distributed_executor_backend=args.distributed_executor_backend)\n \n     sampling_params = SamplingParams(\n@@ -222,6 +224,29 @@ if __name__ == '__main__':\n                         help='the fraction of GPU memory to be used for '\n                         'the model executor, which can range from 0 to 1.'\n                         'If unspecified, will use the default value of 0.9.')\n+    parser.add_argument(\n+        '--load-format',\n+        type=str,\n+        default=EngineArgs.load_format,\n+        choices=[\n+            'auto', 'pt', 'safetensors', 'npcache', 'dummy', 'tensorizer',\n+            'bitsandbytes'\n+        ],\n+        help='The format of the model weights to load.\\n\\n'\n+        '* \"auto\" will try to load the weights in the safetensors format '\n+        'and fall back to the pytorch bin format if safetensors format '\n+        'is not available.\\n'\n+        '* \"pt\" will load the weights in the pytorch bin format.\\n'\n+        '* \"safetensors\" will load the weights in the safetensors format.\\n'\n+        '* \"npcache\" will load the weights in pytorch format and store '\n+        'a numpy cache to speed up the loading.\\n'\n+        '* \"dummy\" will initialize the weights with random values, '\n+        'which is mainly for profiling.\\n'\n+        '* \"tensorizer\" will load the weights using tensorizer from '\n+        'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n+        'section for more information.\\n'\n+        '* \"bitsandbytes\" will load the weights using bitsandbytes '\n+        'quantization.\\n')\n     parser.add_argument(\n         '--distributed-executor-backend',\n         choices=['ray', 'mp'],",
      "change_type": "modified",
      "lines_added": 26,
      "lines_removed": 1
    },
    {
      "file_path": "benchmarks/benchmark_serving.py",
      "old_content": "\"\"\"Benchmark online serving throughput.\n\nOn the server side, run one of the following commands:\n    vLLM OpenAI API server\n    python -m vllm.entrypoints.openai.api_server \\\n        --model <your_model> --swap-space 16 \\\n        --disable-log-requests\n\n    (TGI backend)\n    ./launch_tgi_server.sh <your_model> <max_batch_total_tokens>\n\nOn the client side, run:\n    python benchmarks/benchmark_serving.py \\\n        --backend <backend> \\\n        --model <your_model> \\\n        --dataset-name sharegpt \\\n        --dataset-path <path to dataset> \\\n        --request-rate <request_rate> \\ # By default <request_rate> is inf\n        --num-prompts <num_prompts> # By default <num_prompts> is 1000\n        \n    when using tgi backend, add\n        --endpoint /generate_stream\n    to the end of the command above.\n\"\"\"\nimport argparse\nimport asyncio\nimport json\nimport os\nimport random\nimport time\nimport warnings\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom typing import AsyncGenerator, List, Optional, Tuple\n\nimport numpy as np\nfrom backend_request_func import (ASYNC_REQUEST_FUNCS, RequestFuncInput,\n                                  RequestFuncOutput)\nfrom tqdm.asyncio import tqdm\nfrom transformers import PreTrainedTokenizerBase\n\nfrom vllm.transformers_utils.tokenizer import get_tokenizer\n\n\n@dataclass\nclass BenchmarkMetrics:\n    completed: int\n    total_input: int\n    total_output: int\n    request_throughput: float\n    input_throughput: float\n    output_throughput: float\n    mean_ttft_ms: float\n    median_ttft_ms: float\n    p99_ttft_ms: float\n    mean_tpot_ms: float\n    median_tpot_ms: float\n    p99_tpot_ms: float\n    mean_itl_ms: float\n    median_itl_ms: float\n    p99_itl_ms: float\n\n\ndef sample_sharegpt_requests(\n    dataset_path: str,\n    num_requests: int,\n    tokenizer: PreTrainedTokenizerBase,\n    fixed_output_len: Optional[int] = None,\n) -> List[Tuple[str, int, int]]:\n    if fixed_output_len is not None and fixed_output_len < 4:\n        raise ValueError(\"output_len too small\")\n\n    # Load the dataset.\n    with open(dataset_path) as f:\n        dataset = json.load(f)\n    # Filter out the conversations with less than 2 turns.\n    dataset = [data for data in dataset if len(data[\"conversations\"]) >= 2]\n    # Only keep the first two turns of each conversation.\n    dataset = [(data[\"conversations\"][0][\"value\"],\n                data[\"conversations\"][1][\"value\"]) for data in dataset]\n\n    # Shuffle the dataset.\n    random.shuffle(dataset)\n\n    # Filter out sequences that are too long or too short\n    filtered_dataset: List[Tuple[str, int, int]] = []\n    for i in range(len(dataset)):\n        if len(filtered_dataset) == num_requests:\n            break\n\n        # Tokenize the prompts and completions.\n        prompt = dataset[i][0]\n        prompt_token_ids = tokenizer(prompt).input_ids\n        completion = dataset[i][1]\n        completion_token_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_token_ids)\n        output_len = len(completion_token_ids\n                         ) if fixed_output_len is None else fixed_output_len\n        if prompt_len < 4 or output_len < 4:\n            # Prune too short sequences.\n            continue\n        if prompt_len > 1024 or prompt_len + output_len > 2048:\n            # Prune too long sequences.\n            continue\n        filtered_dataset.append((prompt, prompt_len, output_len))\n\n    return filtered_dataset\n\n\ndef sample_sonnet_requests(\n    dataset_path: str,\n    num_requests: int,\n    input_len: int,\n    output_len: int,\n    prefix_len: int,\n    tokenizer: PreTrainedTokenizerBase,\n) -> List[Tuple[str, str, int, int]]:\n    assert (\n        input_len > prefix_len\n    ), \"'args.sonnet-input-len' must be greater than 'args.prefix-input-len'.\"\n\n    # Load the dataset.\n    with open(dataset_path) as f:\n        poem_lines = f.readlines()\n\n    # Tokenize the poem lines.\n    poem_token_ids = tokenizer(poem_lines).input_ids\n    average_poem_len = sum(\n        len(token_ids) for token_ids in poem_token_ids) / len(poem_token_ids)\n\n    # Base prefix for all requests.\n    base_prompt = \"Pick as many lines as you can from these poem lines:\\n\"\n    base_message = [{\n        \"role\": \"user\",\n        \"content\": base_prompt,\n    }]\n    base_prompt_formatted = tokenizer.apply_chat_template(\n        base_message, add_generation_prompt=True, tokenize=False)\n    base_prompt_offset = len(tokenizer(base_prompt_formatted).input_ids)\n\n    assert (\n        input_len > base_prompt_offset\n    ), f\"Please set 'args.sonnet-input-len' higher than {base_prompt_offset}.\"\n    num_input_lines = round(\n        (input_len - base_prompt_offset) / average_poem_len)\n\n    # First approximately `prefix_len` number of tokens in the\n    # prompt are fixed poem lines.\n    assert (\n        prefix_len > base_prompt_offset\n    ), f\"Please set 'args.sonnet-prefix-len' higher than {base_prompt_offset}.\"\n\n    num_prefix_lines = round(\n        (prefix_len - base_prompt_offset) / average_poem_len)\n    prefix_lines = poem_lines[:num_prefix_lines]\n\n    # Sample the rest of lines per request.\n    sampled_requests: List[Tuple[str, int, int]] = []\n    for _ in range(num_requests):\n        sampled_lines = \"\".join(\n            prefix_lines +\n            random.sample(poem_lines, num_input_lines - num_prefix_lines))\n\n        prompt = f\"{base_prompt}{sampled_lines}\"\n        message = [\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            },\n        ]\n        prompt_formatted = tokenizer.apply_chat_template(\n            message, add_generation_prompt=True, tokenize=False)\n        prompt_len = len(tokenizer(prompt_formatted).input_ids)\n        sampled_requests.append(\n            (prompt, prompt_formatted, prompt_len, output_len))\n\n    return sampled_requests\n\n\nasync def get_request(\n    input_requests: List[Tuple[str, int, int]],\n    request_rate: float,\n) -> AsyncGenerator[Tuple[str, int, int], None]:\n    input_requests = iter(input_requests)\n    for request in input_requests:\n        yield request\n\n        if request_rate == float(\"inf\"):\n            # If the request rate is infinity, then we don't need to wait.\n            continue\n        # Sample the request interval from the exponential distribution.\n        interval = np.random.exponential(1.0 / request_rate)\n        # The next request will be sent after the interval.\n        await asyncio.sleep(interval)\n\n\ndef calculate_metrics(\n    input_requests: List[Tuple[str, int, int]],\n    outputs: List[RequestFuncOutput],\n    dur_s: float,\n    tokenizer: PreTrainedTokenizerBase,\n) -> Tuple[BenchmarkMetrics, List[int]]:\n    actual_output_lens = []\n    total_input = 0\n    completed = 0\n    itls = []\n    tpots = []\n    ttfts = []\n    for i in range(len(outputs)):\n        if outputs[i].success:\n            # We use the tokenizer to count the number of output tokens for all\n            # serving backends instead of looking at len(outputs[i].itl) since\n            # multiple output tokens may be bundled together\n            # Note: this may inflate the output token count slightly\n            output_len = len(\n                tokenizer(outputs[i].generated_text,\n                          add_special_tokens=False).input_ids)\n            actual_output_lens.append(output_len)\n            total_input += input_requests[i][1]\n            if output_len > 1:\n                tpots.append(\n                    (outputs[i].latency - outputs[i].ttft) / (output_len - 1))\n            itls += outputs[i].itl\n            ttfts.append(outputs[i].ttft)\n            completed += 1\n        else:\n            actual_output_lens.append(0)\n\n    if completed == 0:\n        warnings.warn(\n            \"All requests failed. This is likely due to a misconfiguration \"\n            \"on the benchmark arguments.\",\n            stacklevel=2)\n    metrics = BenchmarkMetrics(\n        completed=completed,\n        total_input=total_input,\n        total_output=sum(actual_output_lens),\n        request_throughput=completed / dur_s,\n        input_throughput=total_input / dur_s,\n        output_throughput=sum(actual_output_lens) / dur_s,\n        mean_ttft_ms=np.mean(ttfts or 0) *\n        1000,  # ttfts is empty if streaming is not supported by backend\n        median_ttft_ms=np.median(ttfts or 0) * 1000,\n        p99_ttft_ms=np.percentile(ttfts or 0, 99) * 1000,\n        mean_tpot_ms=np.mean(tpots or 0) * 1000,\n        median_tpot_ms=np.median(tpots or 0) * 1000,\n        p99_tpot_ms=np.percentile(tpots or 0, 99) * 1000,\n        mean_itl_ms=np.mean(itls or 0) * 1000,\n        median_itl_ms=np.median(itls or 0) * 1000,\n        p99_itl_ms=np.percentile(itls or 0, 99) * 1000,\n    )\n\n    return metrics, actual_output_lens\n\n\nasync def benchmark(\n    backend: str,\n    api_url: str,\n    model_id: str,\n    tokenizer: PreTrainedTokenizerBase,\n    input_requests: List[Tuple[str, int, int]],\n    best_of: int,\n    use_beam_search: bool,\n    request_rate: float,\n    disable_tqdm: bool,\n):\n    if backend in ASYNC_REQUEST_FUNCS:\n        request_func = ASYNC_REQUEST_FUNCS.get(backend)\n    else:\n        raise ValueError(f\"Unknown backend: {backend}\")\n\n    print(\"Starting initial single prompt test run...\")\n    test_prompt, test_prompt_len, test_output_len = input_requests[0]\n    test_input = RequestFuncInput(\n        model=model_id,\n        prompt=test_prompt,\n        api_url=api_url,\n        prompt_len=test_prompt_len,\n        output_len=test_output_len,\n        best_of=best_of,\n        use_beam_search=use_beam_search,\n    )\n    test_output = await request_func(request_func_input=test_input)\n    if not test_output.success:\n        raise ValueError(\n            \"Initial test run failed - Please make sure benchmark arguments \"\n            f\"are correctly specified. Error: {test_output.error}\")\n    else:\n        print(\"Initial test run completed. Starting main benchmark run...\")\n    print(f\"Traffic request rate: {request_rate}\")\n\n    pbar = None if disable_tqdm else tqdm(total=len(input_requests))\n\n    benchmark_start_time = time.perf_counter()\n    tasks = []\n    async for request in get_request(input_requests, request_rate):\n        prompt, prompt_len, output_len = request\n        request_func_input = RequestFuncInput(\n            model=model_id,\n            prompt=prompt,\n            api_url=api_url,\n            prompt_len=prompt_len,\n            output_len=output_len,\n            best_of=best_of,\n            use_beam_search=use_beam_search,\n        )\n        tasks.append(\n            asyncio.create_task(\n                request_func(request_func_input=request_func_input,\n                             pbar=pbar)))\n    outputs: List[RequestFuncOutput] = await asyncio.gather(*tasks)\n\n    if not disable_tqdm:\n        pbar.close()\n\n    benchmark_duration = time.perf_counter() - benchmark_start_time\n\n    metrics, actual_output_lens = calculate_metrics(\n        input_requests=input_requests,\n        outputs=outputs,\n        dur_s=benchmark_duration,\n        tokenizer=tokenizer,\n    )\n\n    print(\"{s:{c}^{n}}\".format(s=' Serving Benchmark Result ', n=50, c='='))\n    print(\"{:<40} {:<10}\".format(\"Successful requests:\", metrics.completed))\n    print(\"{:<40} {:<10.2f}\".format(\"Benchmark duration (s):\",\n                                    benchmark_duration))\n    print(\"{:<40} {:<10}\".format(\"Total input tokens:\", metrics.total_input))\n    print(\"{:<40} {:<10}\".format(\"Total generated tokens:\",\n                                 metrics.total_output))\n    print(\"{:<40} {:<10.2f}\".format(\"Request throughput (req/s):\",\n                                    metrics.request_throughput))\n    print(\"{:<40} {:<10.2f}\".format(\"Input token throughput (tok/s):\",\n                                    metrics.input_throughput))\n    print(\"{:<40} {:<10.2f}\".format(\"Output token throughput (tok/s):\",\n                                    metrics.output_throughput))\n    print(\"{s:{c}^{n}}\".format(s='Time to First Token', n=50, c='-'))\n    print(\"{:<40} {:<10.2f}\".format(\"Mean TTFT (ms):\", metrics.mean_ttft_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"Median TTFT (ms):\",\n                                    metrics.median_ttft_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"P99 TTFT (ms):\", metrics.p99_ttft_ms))\n    print(\"{s:{c}^{n}}\".format(s='Time per Output Token (excl. 1st token)',\n                               n=50,\n                               c='-'))\n    print(\"{:<40} {:<10.2f}\".format(\"Mean TPOT (ms):\", metrics.mean_tpot_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"Median TPOT (ms):\",\n                                    metrics.median_tpot_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"P99 TPOT (ms):\", metrics.p99_tpot_ms))\n    print(\"{s:{c}^{n}}\".format(s='Inter-token Latency', n=50, c='-'))\n    print(\"{:<40} {:<10.2f}\".format(\"Mean ITL (ms):\", metrics.mean_itl_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"Median ITL (ms):\", metrics.median_itl_ms))\n    print(\"{:<40} {:<10.2f}\".format(\"P99 ITL (ms):\", metrics.p99_itl_ms))\n    print(\"=\" * 50)\n\n    result = {\n        \"duration\": benchmark_duration,\n        \"completed\": metrics.completed,\n        \"total_input_tokens\": metrics.total_input,\n        \"total_output_tokens\": metrics.total_output,\n        \"request_throughput\": metrics.request_throughput,\n        \"input_throughput\": metrics.input_throughput,\n        \"output_throughput\": metrics.output_throughput,\n        \"mean_ttft_ms\": metrics.mean_ttft_ms,\n        \"median_ttft_ms\": metrics.median_ttft_ms,\n        \"p99_ttft_ms\": metrics.p99_ttft_ms,\n        \"mean_tpot_ms\": metrics.mean_tpot_ms,\n        \"median_tpot_ms\": metrics.median_tpot_ms,\n        \"p99_tpot_ms\": metrics.p99_tpot_ms,\n        \"mean_itl_ms\": metrics.mean_itl_ms,\n        \"median_itl_ms\": metrics.median_itl_ms,\n        \"p99_itl_ms\": metrics.p99_itl_ms,\n        \"input_lens\": [output.prompt_len for output in outputs],\n        \"output_lens\": actual_output_lens,\n        \"ttfts\": [output.ttft for output in outputs],\n        \"itls\": [output.itl for output in outputs],\n        \"generated_texts\": [output.generated_text for output in outputs],\n        \"errors\": [output.error for output in outputs],\n    }\n    return result\n\n\ndef main(args: argparse.Namespace):\n    print(args)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    backend = args.backend\n    model_id = args.model\n    tokenizer_id = args.tokenizer if args.tokenizer is not None else args.model\n\n    if args.base_url is not None:\n        api_url = f\"{args.base_url}{args.endpoint}\"\n    else:\n        api_url = f\"http://{args.host}:{args.port}{args.endpoint}\"\n\n    tokenizer = get_tokenizer(tokenizer_id,\n                              trust_remote_code=args.trust_remote_code)\n\n    if args.dataset is not None:\n        warnings.warn(\n            \"The '--dataset' argument will be deprecated in the next \"\n            \"release. Please use '--dataset-name' and \"\n            \"'--dataset-path' in the future runs.\",\n            stacklevel=2)\n        input_requests = sample_sharegpt_requests(\n            dataset_path=args.dataset,\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            fixed_output_len=args.sharegpt_output_len,\n        )\n\n    elif args.dataset_name == \"sharegpt\":\n        input_requests = sample_sharegpt_requests(\n            dataset_path=args.dataset_path,\n            num_requests=args.num_prompts,\n            tokenizer=tokenizer,\n            fixed_output_len=args.sharegpt_output_len,\n        )\n\n    elif args.dataset_name == \"sonnet\":\n        # Do not format the prompt, pass to message directly\n        if args.backend == \"openai-chat\":\n            input_requests = sample_sonnet_requests(\n                dataset_path=args.dataset_path,\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n            )\n            input_requests = [(prompt, prompt_len, output_len)\n                              for prompt, prompt_formatted, prompt_len,\n                              output_len in input_requests]\n        else:\n            assert (\n                tokenizer.chat_template or tokenizer.default_chat_template\n            ), \"Tokenizer/model must have chat template for sonnet dataset.\"\n            input_requests = sample_sonnet_requests(\n                dataset_path=args.dataset_path,\n                num_requests=args.num_prompts,\n                input_len=args.sonnet_input_len,\n                output_len=args.sonnet_output_len,\n                prefix_len=args.sonnet_prefix_len,\n                tokenizer=tokenizer,\n            )\n            input_requests = [(prompt_formatted, prompt_len, output_len)\n                              for prompt, prompt_formatted, prompt_len,\n                              output_len in input_requests]\n\n    else:\n        raise ValueError(f\"Unknown dataset: {args.dataset_name}\")\n\n    benchmark_result = asyncio.run(\n        benchmark(\n            backend=backend,\n            api_url=api_url,\n            model_id=model_id,\n            tokenizer=tokenizer,\n            input_requests=input_requests,\n            best_of=args.best_of,\n            use_beam_search=args.use_beam_search,\n            request_rate=args.request_rate,\n            disable_tqdm=args.disable_tqdm,\n        ))\n\n    # Save config and results to json\n    if args.save_result:\n        result_json = {}\n\n        # Setup\n        current_dt = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n        result_json[\"date\"] = current_dt\n        result_json[\"backend\"] = backend\n        result_json[\"model_id\"] = model_id\n        result_json[\"tokenizer_id\"] = tokenizer_id\n        result_json[\"best_of\"] = args.best_of\n        result_json[\"use_beam_search\"] = args.use_beam_search\n        result_json[\"num_prompts\"] = args.num_prompts\n\n        # Metadata\n        if args.metadata:\n            for item in args.metadata:\n                if \"=\" in item:\n                    kvstring = item.split(\"=\")\n                    result_json[kvstring[0].strip()] = kvstring[1].strip()\n                else:\n                    raise ValueError(\n                        \"Invalid metadata format. Please use KEY=VALUE format.\"\n                    )\n\n        # Traffic\n        result_json[\"request_rate\"] = (\n            args.request_rate if args.request_rate < float(\"inf\") else \"inf\")\n\n        # Merge with benchmark result\n        result_json = {**result_json, **benchmark_result}\n\n        # Save to file\n        base_model_id = model_id.split(\"/\")[-1]\n        file_name = f\"{backend}-{args.request_rate}qps-{base_model_id}-{current_dt}.json\"  #noqa\n        if args.result_dir:\n            file_name = os.path.join(args.result_dir, file_name)\n        with open(file_name, \"w\") as outfile:\n            json.dump(result_json, outfile)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Benchmark the online serving throughput.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"vllm\",\n        choices=list(ASYNC_REQUEST_FUNCS.keys()),\n    )\n    parser.add_argument(\n        \"--base-url\",\n        type=str,\n        default=None,\n        help=\"Server or API base url if not using http host and port.\",\n    )\n    parser.add_argument(\"--host\", type=str, default=\"localhost\")\n    parser.add_argument(\"--port\", type=int, default=8000)\n    parser.add_argument(\n        \"--endpoint\",\n        type=str,\n        default=\"/v1/completions\",\n        help=\"API endpoint.\",\n    )\n    parser.add_argument(\n        \"--dataset\",\n        type=str,\n        default=None,\n        help=\"Path to the ShareGPT dataset, will be deprecated in the \"\n        \"next release.\",\n    )\n    parser.add_argument(\n        \"--dataset-name\",\n        type=str,\n        default=\"sharegpt\",\n        choices=[\"sharegpt\", \"sonnet\"],\n        help=\"Name of the dataset to benchmark on.\",\n    )\n    parser.add_argument(\"--dataset-path\",\n                        type=str,\n                        default=None,\n                        help=\"Path to the dataset.\")\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        required=True,\n        help=\"Name of the model.\",\n    )\n    parser.add_argument(\n        \"--tokenizer\",\n        type=str,\n        help=\n        \"Name or path of the tokenizer, if not using the default tokenizer.\",\n    )\n    parser.add_argument(\n        \"--best-of\",\n        type=int,\n        default=1,\n        help=\"Generates `best_of` sequences per prompt and \"\n        \"returns the best one.\",\n    )\n    parser.add_argument(\"--use-beam-search\", action=\"store_true\")\n    parser.add_argument(\n        \"--num-prompts\",\n        type=int,\n        default=1000,\n        help=\"Number of prompts to process.\",\n    )\n    parser.add_argument(\n        \"--sharegpt-output-len\",\n        type=int,\n        default=None,\n        help=\"Output length for each request. Overrides the output length \"\n        \"from the ShareGPT dataset.\")\n    parser.add_argument(\n        \"--sonnet-input-len\",\n        type=int,\n        default=550,\n        help=\n        \"Number of input tokens per request, used only for sonnet dataset.\",\n    )\n    parser.add_argument(\n        \"--sonnet-output-len\",\n        type=int,\n        default=150,\n        help=\n        \"Number of output tokens per request, used only for sonnet dataset.\",\n    )\n    parser.add_argument(\n        \"--sonnet-prefix-len\",\n        type=int,\n        default=200,\n        help=\n        \"Number of prefix tokens per request, used only for sonnet dataset.\",\n    )\n    parser.add_argument(\n        \"--request-rate\",\n        type=float,\n        default=float(\"inf\"),\n        help=\"Number of requests per second. If this is inf, \"\n        \"then all the requests are sent at time 0. \"\n        \"Otherwise, we use Poisson process to synthesize \"\n        \"the request arrival times.\",\n    )\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\n        \"--trust-remote-code\",\n        action=\"store_true\",\n        help=\"Trust remote code from huggingface\",\n    )\n    parser.add_argument(\n        \"--disable-tqdm\",\n        action=\"store_true\",\n        help=\"Specify to disable tqdm progress bar.\",\n    )\n    parser.add_argument(\n        \"--save-result\",\n        action=\"store_true\",\n        help=\"Specify to save benchmark results to a json file\",\n    )\n    parser.add_argument(\n        \"--metadata\",\n        metavar=\"KEY=VALUE\",\n        nargs=\"*\",\n        help=\"Key-value pairs (e.g, --metadata version=0.3.3 tp=1) \"\n        \"for metadata of this run to be saved in the result JSON file \"\n        \"for record keeping purposes.\",\n    )\n    parser.add_argument(\n        \"--result-dir\",\n        type=str,\n        default=None,\n        help=\"Specify directory to save benchmark json results.\"\n        \"If not specified, results are saved in the current directory.\",\n    )\n\n    args = parser.parse_args()\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_serving.py b/benchmarks/benchmark_serving.py\nindex 4112a3272..df32b366c 100644\n--- a/benchmarks/benchmark_serving.py\n+++ b/benchmarks/benchmark_serving.py\n@@ -499,6 +499,8 @@ def main(args: argparse.Namespace):\n         # Save to file\n         base_model_id = model_id.split(\"/\")[-1]\n         file_name = f\"{backend}-{args.request_rate}qps-{base_model_id}-{current_dt}.json\"  #noqa\n+        if args.result_filename:\n+            file_name = args.result_filename\n         if args.result_dir:\n             file_name = os.path.join(args.result_dir, file_name)\n         with open(file_name, \"w\") as outfile:\n@@ -639,6 +641,15 @@ if __name__ == \"__main__\":\n         help=\"Specify directory to save benchmark json results.\"\n         \"If not specified, results are saved in the current directory.\",\n     )\n+    parser.add_argument(\n+        \"--result-filename\",\n+        type=str,\n+        default=None,\n+        help=\"Specify the filename to save benchmark json results.\"\n+        \"If not specified, results will be saved in \"\n+        \"{backend}-{args.request_rate}qps-{base_model_id}-{current_dt}.json\"\n+        \" format.\",\n+    )\n \n     args = parser.parse_args()\n     main(args)",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 1
    },
    {
      "file_path": "benchmarks/benchmark_throughput.py",
      "old_content": "\"\"\"Benchmark offline inference throughput.\"\"\"\nimport argparse\nimport json\nimport random\nimport time\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom tqdm import tqdm\nfrom transformers import (AutoModelForCausalLM, AutoTokenizer,\n                          PreTrainedTokenizerBase)\n\nfrom vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\n\n\ndef sample_requests(\n    dataset_path: str,\n    num_requests: int,\n    tokenizer: PreTrainedTokenizerBase,\n    fixed_output_len: Optional[int],\n) -> List[Tuple[str, int, int]]:\n    if fixed_output_len is not None and fixed_output_len < 4:\n        raise ValueError(\"output_len too small\")\n\n    # Load the dataset.\n    with open(dataset_path) as f:\n        dataset = json.load(f)\n    # Filter out the conversations with less than 2 turns.\n    dataset = [data for data in dataset if len(data[\"conversations\"]) >= 2]\n    # Only keep the first two turns of each conversation.\n    dataset = [(data[\"conversations\"][0][\"value\"],\n                data[\"conversations\"][1][\"value\"]) for data in dataset]\n\n    # Shuffle the dataset.\n    random.shuffle(dataset)\n\n    # Filter out sequences that are too long or too short\n    filtered_dataset: List[Tuple[str, int, int]] = []\n    for i in range(len(dataset)):\n        if len(filtered_dataset) == num_requests:\n            break\n\n        # Tokenize the prompts and completions.\n        prompt = dataset[i][0]\n        prompt_token_ids = tokenizer(prompt).input_ids\n        completion = dataset[i][1]\n        completion_token_ids = tokenizer(completion).input_ids\n        prompt_len = len(prompt_token_ids)\n        output_len = len(completion_token_ids\n                         ) if fixed_output_len is None else fixed_output_len\n        if prompt_len < 4 or output_len < 4:\n            # Prune too short sequences.\n            continue\n        if prompt_len > 1024 or prompt_len + output_len > 2048:\n            # Prune too long sequences.\n            continue\n        filtered_dataset.append((prompt, prompt_len, output_len))\n\n    return filtered_dataset\n\n\ndef run_vllm(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tokenizer: str,\n    quantization: Optional[str],\n    tensor_parallel_size: int,\n    seed: int,\n    n: int,\n    use_beam_search: bool,\n    trust_remote_code: bool,\n    dtype: str,\n    max_model_len: Optional[int],\n    enforce_eager: bool,\n    kv_cache_dtype: str,\n    quantization_param_path: Optional[str],\n    device: str,\n    enable_prefix_caching: bool,\n    enable_chunked_prefill: bool,\n    max_num_batched_tokens: int,\n    distributed_executor_backend: Optional[str],\n    gpu_memory_utilization: float = 0.9,\n    download_dir: Optional[str] = None,\n) -> float:\n    from vllm import LLM, SamplingParams\n    llm = LLM(\n        model=model,\n        tokenizer=tokenizer,\n        quantization=quantization,\n        tensor_parallel_size=tensor_parallel_size,\n        seed=seed,\n        trust_remote_code=trust_remote_code,\n        dtype=dtype,\n        max_model_len=max_model_len,\n        gpu_memory_utilization=gpu_memory_utilization,\n        enforce_eager=enforce_eager,\n        kv_cache_dtype=kv_cache_dtype,\n        quantization_param_path=quantization_param_path,\n        device=device,\n        enable_prefix_caching=enable_prefix_caching,\n        download_dir=download_dir,\n        enable_chunked_prefill=enable_chunked_prefill,\n        max_num_batched_tokens=max_num_batched_tokens,\n        distributed_executor_backend=distributed_executor_backend,\n    )\n\n    # Add the requests to the engine.\n    prompts = []\n    sampling_params = []\n    for prompt, _, output_len in requests:\n        prompts.append(prompt)\n        sampling_params.append(\n            SamplingParams(\n                n=n,\n                temperature=0.0 if use_beam_search else 1.0,\n                top_p=1.0,\n                use_beam_search=use_beam_search,\n                ignore_eos=True,\n                max_tokens=output_len,\n            ))\n\n    start = time.perf_counter()\n    llm.generate(prompts, sampling_params, use_tqdm=True)\n    end = time.perf_counter()\n    return end - start\n\n\ndef run_hf(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tokenizer: PreTrainedTokenizerBase,\n    n: int,\n    use_beam_search: bool,\n    max_batch_size: int,\n    trust_remote_code: bool,\n) -> float:\n    assert not use_beam_search\n    llm = AutoModelForCausalLM.from_pretrained(\n        model, torch_dtype=torch.float16, trust_remote_code=trust_remote_code)\n    if llm.config.model_type == \"llama\":\n        # To enable padding in the HF backend.\n        tokenizer.pad_token = tokenizer.eos_token\n    llm = llm.cuda()\n\n    pbar = tqdm(total=len(requests))\n    start = time.perf_counter()\n    batch: List[str] = []\n    max_prompt_len = 0\n    max_output_len = 0\n    for i in range(len(requests)):\n        prompt, prompt_len, output_len = requests[i]\n        # Add the prompt to the batch.\n        batch.append(prompt)\n        max_prompt_len = max(max_prompt_len, prompt_len)\n        max_output_len = max(max_output_len, output_len)\n        if len(batch) < max_batch_size and i != len(requests) - 1:\n            # Check if we can add more requests to the batch.\n            _, next_prompt_len, next_output_len = requests[i + 1]\n            if (max(max_prompt_len, next_prompt_len) +\n                    max(max_output_len, next_output_len)) <= 2048:\n                # We can add more requests to the batch.\n                continue\n\n        # Generate the sequences.\n        input_ids = tokenizer(batch, return_tensors=\"pt\",\n                              padding=True).input_ids\n        llm_outputs = llm.generate(\n            input_ids=input_ids.cuda(),\n            do_sample=not use_beam_search,\n            num_return_sequences=n,\n            temperature=1.0,\n            top_p=1.0,\n            use_cache=True,\n            max_new_tokens=max_output_len,\n        )\n        # Include the decoding time.\n        tokenizer.batch_decode(llm_outputs, skip_special_tokens=True)\n        pbar.update(len(batch))\n\n        # Clear the batch.\n        batch = []\n        max_prompt_len = 0\n        max_output_len = 0\n    end = time.perf_counter()\n    return end - start\n\n\ndef run_mii(\n    requests: List[Tuple[str, int, int]],\n    model: str,\n    tensor_parallel_size: int,\n    output_len: int,\n) -> float:\n    from mii import client, serve\n    llm = serve(model, tensor_parallel=tensor_parallel_size)\n    prompts = [prompt for prompt, _, _ in requests]\n\n    start = time.perf_counter()\n    llm.generate(prompts, max_new_tokens=output_len)\n    end = time.perf_counter()\n    client = client(model)\n    client.terminate_server()\n    return end - start\n\n\ndef main(args: argparse.Namespace):\n    print(args)\n    random.seed(args.seed)\n\n    # Sample the requests.\n    tokenizer = AutoTokenizer.from_pretrained(\n        args.tokenizer, trust_remote_code=args.trust_remote_code)\n    if args.dataset is None:\n        # Synthesize a prompt with the given input length.\n        prompt = \"hi\" * (args.input_len - 1)\n        requests = [(prompt, args.input_len, args.output_len)\n                    for _ in range(args.num_prompts)]\n    else:\n        requests = sample_requests(args.dataset, args.num_prompts, tokenizer,\n                                   args.output_len)\n\n    if args.backend == \"vllm\":\n        elapsed_time = run_vllm(\n            requests, args.model, args.tokenizer, args.quantization,\n            args.tensor_parallel_size, args.seed, args.n, args.use_beam_search,\n            args.trust_remote_code, args.dtype, args.max_model_len,\n            args.enforce_eager, args.kv_cache_dtype,\n            args.quantization_param_path, args.device,\n            args.enable_prefix_caching, args.enable_chunked_prefill,\n            args.max_num_batched_tokens, args.distributed_executor_backend,\n            args.gpu_memory_utilization, args.download_dir)\n    elif args.backend == \"hf\":\n        assert args.tensor_parallel_size == 1\n        elapsed_time = run_hf(requests, args.model, tokenizer, args.n,\n                              args.use_beam_search, args.hf_max_batch_size,\n                              args.trust_remote_code)\n    elif args.backend == \"mii\":\n        elapsed_time = run_mii(requests, args.model, args.tensor_parallel_size,\n                               args.output_len)\n    else:\n        raise ValueError(f\"Unknown backend: {args.backend}\")\n    total_num_tokens = sum(prompt_len + output_len\n                           for _, prompt_len, output_len in requests)\n    print(f\"Throughput: {len(requests) / elapsed_time:.2f} requests/s, \"\n          f\"{total_num_tokens / elapsed_time:.2f} tokens/s\")\n\n    # Output JSON results if specified\n    if args.output_json:\n        results = {\n            \"elapsed_time\": elapsed_time,\n            \"num_requests\": len(requests),\n            \"total_num_tokens\": total_num_tokens,\n            \"requests_per_second\": len(requests) / elapsed_time,\n            \"tokens_per_second\": total_num_tokens / elapsed_time,\n        }\n        with open(args.output_json, \"w\") as f:\n            json.dump(results, f, indent=4)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Benchmark the throughput.\")\n    parser.add_argument(\"--backend\",\n                        type=str,\n                        choices=[\"vllm\", \"hf\", \"mii\"],\n                        default=\"vllm\")\n    parser.add_argument(\"--dataset\",\n                        type=str,\n                        default=None,\n                        help=\"Path to the dataset.\")\n    parser.add_argument(\"--input-len\",\n                        type=int,\n                        default=None,\n                        help=\"Input prompt length for each request\")\n    parser.add_argument(\"--output-len\",\n                        type=int,\n                        default=None,\n                        help=\"Output length for each request. Overrides the \"\n                        \"output length from the dataset.\")\n    parser.add_argument(\"--model\", type=str, default=\"facebook/opt-125m\")\n    parser.add_argument(\"--tokenizer\", type=str, default=None)\n    parser.add_argument('--quantization',\n                        '-q',\n                        choices=[*QUANTIZATION_METHODS, None],\n                        default=None)\n    parser.add_argument(\"--tensor-parallel-size\", \"-tp\", type=int, default=1)\n    parser.add_argument(\"--n\",\n                        type=int,\n                        default=1,\n                        help=\"Number of generated sequences per prompt.\")\n    parser.add_argument(\"--use-beam-search\", action=\"store_true\")\n    parser.add_argument(\"--num-prompts\",\n                        type=int,\n                        default=1000,\n                        help=\"Number of prompts to process.\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--hf-max-batch-size\",\n                        type=int,\n                        default=None,\n                        help=\"Maximum batch size for HF backend.\")\n    parser.add_argument('--trust-remote-code',\n                        action='store_true',\n                        help='trust remote code from huggingface')\n    parser.add_argument(\n        '--max-model-len',\n        type=int,\n        default=None,\n        help='Maximum length of a sequence (including prompt and output). '\n        'If None, will be derived from the model.')\n    parser.add_argument(\n        '--dtype',\n        type=str,\n        default='auto',\n        choices=['auto', 'half', 'float16', 'bfloat16', 'float', 'float32'],\n        help='data type for model weights and activations. '\n        'The \"auto\" option will use FP16 precision '\n        'for FP32 and FP16 models, and BF16 precision '\n        'for BF16 models.')\n    parser.add_argument('--gpu-memory-utilization',\n                        type=float,\n                        default=0.9,\n                        help='the fraction of GPU memory to be used for '\n                        'the model executor, which can range from 0 to 1.'\n                        'If unspecified, will use the default value of 0.9.')\n    parser.add_argument(\"--enforce-eager\",\n                        action=\"store_true\",\n                        help=\"enforce eager execution\")\n    parser.add_argument(\n        '--kv-cache-dtype',\n        type=str,\n        choices=['auto', 'fp8', 'fp8_e5m2', 'fp8_e4m3'],\n        default=\"auto\",\n        help='Data type for kv cache storage. If \"auto\", will use model '\n        'data type. CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. '\n        'ROCm (AMD GPU) supports fp8 (=fp8_e4m3)')\n    parser.add_argument(\n        '--quantization-param-path',\n        type=str,\n        default=None,\n        help='Path to the JSON file containing the KV cache scaling factors. '\n        'This should generally be supplied, when KV cache dtype is FP8. '\n        'Otherwise, KV cache scaling factors default to 1.0, which may cause '\n        'accuracy issues. FP8_E5M2 (without scaling) is only supported on '\n        'cuda version greater than 11.8. On ROCm (AMD GPU), FP8_E4M3 is '\n        'instead supported for common inference criteria.')\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"cuda\",\n        choices=[\"cuda\", \"cpu\", \"tpu\"],\n        help='device type for vLLM execution, supporting CUDA and CPU.')\n    parser.add_argument(\n        \"--enable-prefix-caching\",\n        action='store_true',\n        help=\"enable automatic prefix caching for vLLM backend.\")\n    parser.add_argument(\"--enable-chunked-prefill\",\n                        action='store_true',\n                        help=\"enable chunked prefill for vLLM backend.\")\n    parser.add_argument('--max-num-batched-tokens',\n                        type=int,\n                        default=None,\n                        help='maximum number of batched tokens per '\n                        'iteration')\n    parser.add_argument('--download-dir',\n                        type=str,\n                        default=None,\n                        help='directory to download and load the weights, '\n                        'default to the default cache dir of huggingface')\n    parser.add_argument(\n        '--output-json',\n        type=str,\n        default=None,\n        help='Path to save the throughput results in JSON format.')\n    parser.add_argument(\n        '--distributed-executor-backend',\n        choices=['ray', 'mp'],\n        default=None,\n        help='Backend to use for distributed serving. When more than 1 GPU '\n        'is used, will be automatically set to \"ray\" if installed '\n        'or \"mp\" (multiprocessing) otherwise.')\n    args = parser.parse_args()\n    if args.tokenizer is None:\n        args.tokenizer = args.model\n    if args.dataset is None:\n        assert args.input_len is not None\n        assert args.output_len is not None\n    else:\n        assert args.input_len is None\n\n    if args.backend == \"vllm\":\n        if args.hf_max_batch_size is not None:\n            raise ValueError(\"HF max batch size is only for HF backend.\")\n    elif args.backend == \"hf\":\n        if args.hf_max_batch_size is None:\n            raise ValueError(\"HF max batch size is required for HF backend.\")\n        if args.quantization is not None:\n            raise ValueError(\"Quantization is only for vLLM backend.\")\n    elif args.backend == \"mii\":\n        if args.dtype != \"auto\":\n            raise ValueError(\"dtype must be auto for MII backend.\")\n        if args.n != 1:\n            raise ValueError(\"n must be 1 for MII backend.\")\n        if args.use_beam_search:\n            raise ValueError(\"Beam search is not supported for MII backend.\")\n        if args.quantization is not None:\n            raise ValueError(\"Quantization is only for vLLM backend.\")\n        if args.hf_max_batch_size is not None:\n            raise ValueError(\"HF max batch size is only for HF backend.\")\n        if args.tokenizer != args.model:\n            raise ValueError(\"Tokenizer must be the same as the model for MII \"\n                             \"backend.\")\n    main(args)\n",
      "diff": "diff --git a/benchmarks/benchmark_throughput.py b/benchmarks/benchmark_throughput.py\nindex 07b2f8541..463d9973d 100644\n--- a/benchmarks/benchmark_throughput.py\n+++ b/benchmarks/benchmark_throughput.py\n@@ -10,6 +10,7 @@ from tqdm import tqdm\n from transformers import (AutoModelForCausalLM, AutoTokenizer,\n                           PreTrainedTokenizerBase)\n \n+from vllm.engine.arg_utils import EngineArgs\n from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\n \n \n@@ -81,6 +82,7 @@ def run_vllm(\n     distributed_executor_backend: Optional[str],\n     gpu_memory_utilization: float = 0.9,\n     download_dir: Optional[str] = None,\n+    load_format: str = EngineArgs.load_format,\n ) -> float:\n     from vllm import LLM, SamplingParams\n     llm = LLM(\n@@ -102,6 +104,7 @@ def run_vllm(\n         enable_chunked_prefill=enable_chunked_prefill,\n         max_num_batched_tokens=max_num_batched_tokens,\n         distributed_executor_backend=distributed_executor_backend,\n+        load_format=load_format,\n     )\n \n     # Add the requests to the engine.\n@@ -228,7 +231,7 @@ def main(args: argparse.Namespace):\n             args.quantization_param_path, args.device,\n             args.enable_prefix_caching, args.enable_chunked_prefill,\n             args.max_num_batched_tokens, args.distributed_executor_backend,\n-            args.gpu_memory_utilization, args.download_dir)\n+            args.gpu_memory_utilization, args.download_dir, args.load_format)\n     elif args.backend == \"hf\":\n         assert args.tensor_parallel_size == 1\n         elapsed_time = run_hf(requests, args.model, tokenizer, args.n,\n@@ -377,6 +380,29 @@ if __name__ == \"__main__\":\n         help='Backend to use for distributed serving. When more than 1 GPU '\n         'is used, will be automatically set to \"ray\" if installed '\n         'or \"mp\" (multiprocessing) otherwise.')\n+    parser.add_argument(\n+        '--load-format',\n+        type=str,\n+        default=EngineArgs.load_format,\n+        choices=[\n+            'auto', 'pt', 'safetensors', 'npcache', 'dummy', 'tensorizer',\n+            'bitsandbytes'\n+        ],\n+        help='The format of the model weights to load.\\n\\n'\n+        '* \"auto\" will try to load the weights in the safetensors format '\n+        'and fall back to the pytorch bin format if safetensors format '\n+        'is not available.\\n'\n+        '* \"pt\" will load the weights in the pytorch bin format.\\n'\n+        '* \"safetensors\" will load the weights in the safetensors format.\\n'\n+        '* \"npcache\" will load the weights in pytorch format and store '\n+        'a numpy cache to speed up the loading.\\n'\n+        '* \"dummy\" will initialize the weights with random values, '\n+        'which is mainly for profiling.\\n'\n+        '* \"tensorizer\" will load the weights using tensorizer from '\n+        'CoreWeave. See the Tensorize vLLM Model script in the Examples'\n+        'section for more information.\\n'\n+        '* \"bitsandbytes\" will load the weights using bitsandbytes '\n+        'quantization.\\n')\n     args = parser.parse_args()\n     if args.tokenizer is None:\n         args.tokenizer = args.model",
      "change_type": "modified",
      "lines_added": 28,
      "lines_removed": 2
    }
  ],
  "affected_apis": [
    "benchmark_latency",
    "benchmark_throughput",
    "benchmark_serving",
    "vllm.entrypoints.openai.api_server",
    "convert_results_json_to_markdown"
  ],
  "summary": {
    "total_files": 13,
    "files_added": 8,
    "files_deleted": 1,
    "files_modified": 4
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "TRUE",
    "is_test_actually_there": "",
    "is_benchmark_actually_there": "",
    "sample_clues": "arg, benchmark-pipeline, benchmark_latency"
  }
}