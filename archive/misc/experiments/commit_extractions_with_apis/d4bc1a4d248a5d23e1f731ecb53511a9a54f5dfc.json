{
  "commit_hash": "d4bc1a4d248a5d23e1f731ecb53511a9a54f5dfc",
  "parent_hash": "b56b6ca0d650c653c80ec113e27d6a8e640a4b2f",
  "message": "Add unoptimized OPT Attention",
  "author": "Woosuk Kwon <woosuk.kwon@berkeley.edu>",
  "date": "2023-02-23 09:31:55 +0000",
  "files_changed": [
    {
      "file_path": "cacheflow/models/attention.py",
      "old_content": "",
      "diff": "diff --git a/cacheflow/models/attention.py b/cacheflow/models/attention.py\nnew file mode 100644\nindex 000000000..c36f06c79\n--- /dev/null\n+++ b/cacheflow/models/attention.py\n@@ -0,0 +1,118 @@\n+from typing import Optional, Tuple\n+\n+import torch\n+import torch.nn as nn\n+import xformers.ops as xops\n+\n+from cacheflow import ops\n+from cacheflow.models import InputMetadata\n+\n+\n+class OPTCacheFlowAttention(nn.Module):\n+\n+    def __init__(self, scale: float) -> None:\n+        super().__init__()\n+        self.scale = scale\n+\n+        # Shape-agnostic attention mask.\n+        self.attention_mask = xops.LowerTriangularMask()\n+\n+    def multi_query_kv_attention(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+    ) -> None:\n+        out = xops.memory_efficient_attention(\n+            query, key, value, attn_bias=self.attention_mask, scale=self.scale)\n+        # FIXME(woosuk): Directly write the attention output.\n+        output.copy_(out, non_blocking=True)\n+\n+    def single_query_cached_kv_attention(\n+        self,\n+        output: torch.Tensor,\n+        query: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        input_metadata: InputMetadata,\n+    ) -> None:\n+        num_heads = value_cache.shape[1]\n+        head_size = value_cache.shape[3]\n+        block_size = value_cache.shape[2]\n+        block_tables = input_metadata.block_tables\n+\n+        # FIXME(woosuk): Replace the following with a custom op.\n+        for i in range(input_metadata.num_generation_tokens):\n+            q = query[i]\n+            block_table = block_tables[i]\n+            context_len = int(input_metadata.context_lens[i])\n+            keys = []\n+            for j in range(context_len):\n+                block_number = block_table[j // block_size]\n+                block_offset = j % block_size\n+                k = key_cache[block_number, :, :, block_offset, :]\n+                k = k.view(num_heads, head_size)\n+                keys.append(k)\n+            keys = torch.stack(keys, dim=-1)\n+            logits = q @ keys\n+            attention_weights = torch.softmax(logits, dim=-1)\n+\n+            values = []\n+            for j in range(context_len):\n+                block_number = block_table[j // block_size]\n+                block_offset = j % block_size\n+                v = value_cache[block_number, :, block_offset, :]\n+                values.append(v)\n+            values = torch.stack(values, dim=-1)\n+            out = attention_weights @ values\n+            output[i].copy_(out, non_blocking=True)\n+\n+    def forward(\n+        self,\n+        query: torch.Tensor,\n+        key: torch.Tensor,\n+        value: torch.Tensor,\n+        key_cache: torch.Tensor,\n+        value_cache: torch.Tensor,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n+        # Reshape the input tensors.\n+        num_heads = value_cache.shape[1]\n+        head_size = value_cache.shape[3]\n+        query = query.view(-1, num_heads, head_size)\n+        key = key.view(-1, num_heads, head_size)\n+        value = value.view(-1, num_heads, head_size)\n+\n+        # Compute the attention op for prompts.\n+        output = torch.empty_like(query)\n+        start_idx = 0\n+        for i in range(input_metadata.num_prompts):\n+            prompt_len = input_metadata.prompt_lens[i]\n+            out = output[start_idx:start_idx + prompt_len]\n+            q = query[start_idx:start_idx + prompt_len]\n+            k = key[start_idx:start_idx + prompt_len]\n+            v = value[start_idx:start_idx + prompt_len]\n+            self.multi_query_kv_attention(out, q, k, v)\n+            start_idx += prompt_len\n+\n+        # Wait until the cache op is done.\n+        if cache_event is not None:\n+            cache_event.wait()\n+\n+        # Reshape the keys and values and store them in the cache.\n+        ops.reshape_and_cache(\n+            key, value, key_cache, value_cache, input_metadata.slot_mapping)\n+\n+        if input_metadata.num_generation_tokens > 0:\n+            # Compute the attention op for generation tokens.\n+            self.single_query_cached_kv_attention(\n+                output[start_idx:],\n+                query[start_idx:],\n+                key_cache,\n+                value_cache,\n+                input_metadata)\n+\n+        # Reshape the output tensor.\n+        return output.view(-1, num_heads * head_size)",
      "change_type": "added",
      "lines_added": 119,
      "lines_removed": 1
    },
    {
      "file_path": "cacheflow/models/opt.py",
      "old_content": "\"\"\"1D OPT model compatible with HuggingFace weights.\"\"\"\nimport torch\nfrom torch import nn\nfrom transformers import OPTConfig\nfrom transformers import PreTrainedModel\n\n\nclass OPTLearnedPositionalEmbedding(nn.Embedding):\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # OPT is set up so that if padding_idx is specified then offset the embedding ids by 2\n        # and adjust num_embeddings appropriately. Other models don't have this hack\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n\n    def forward(self, positions: torch.LongTensor):\n        return super().forward(positions + self.offset)\n\n\nclass OPTAttention(nn.Module):\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        bias: bool = True,\n    ) -> None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scaling = self.head_dim**-0.5\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        q = self.q_proj(hidden_states) * self.scaling\n        k = self.k_proj(hidden_states)\n        v = self.v_proj(hidden_states)\n        # TODO\n        attn_output = None\n        output = self.out_proj(attn_output)\n        return output\n\n\nclass OPTDecoderLayer(nn.Module):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        self.self_attn = OPTAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.num_attention_heads,\n            bias=config.enable_bias,\n        )\n        self.do_layer_norm_before = config.do_layer_norm_before\n        assert config.activation_function == 'relu'\n        self.activation_fn = nn.ReLU()\n\n        self.self_attn_layer_norm = nn.LayerNorm(\n            self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n        self.fc1 = nn.Linear(self.embed_dim, config.ffn_dim, bias=config.enable_bias)\n        self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n        self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states = self.self_attn(hidden_states=hidden_states)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Fully Connected\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        return hidden_states\n\n\nclass OPTPreTrainedModel(PreTrainedModel):\n    config_class = OPTConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"OPTDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module) -> None:\n        del module  # unused\n        return\n\n\nclass OPTDecoder(OPTPreTrainedModel):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.word_embed_proj_dim, self.padding_idx)\n        self.embed_positions = OPTLearnedPositionalEmbedding(config.max_position_embeddings, config.hidden_size)\n\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_out = nn.Linear(config.hidden_size, config.word_embed_proj_dim, bias=False)\n        else:\n            self.project_out = None\n\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_in = nn.Linear(config.word_embed_proj_dim, config.hidden_size, bias=False)\n        else:\n            self.project_in = None\n\n        # Note that the only purpose of `config._remove_final_layer_norm` is to keep backward compatibility\n        # with checkpoints that have been fine-tuned before transformers v4.20.1\n        # see https://github.com/facebookresearch/metaseq/pull/164\n        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n            self.final_layer_norm = nn.LayerNorm(\n                config.hidden_size, elementwise_affine=config.layer_norm_elementwise_affine\n            )\n        else:\n            self.final_layer_norm = None\n\n        self.layers = nn.ModuleList([OPTDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n    ) -> torch.Tensor:\n        inputs_embeds = self.embed_tokens(input_ids)\n        pos_embeds = self.embed_positions(positions)\n        pos_embeds = None\n        if self.project_in is not None:\n            inputs_embeds = self.project_in(inputs_embeds)\n        hidden_states = inputs_embeds + pos_embeds\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states)\n\n        if self.final_layer_norm is not None:\n            hidden_states = self.final_layer_norm(hidden_states)\n        if self.project_out is not None:\n            hidden_states = self.project_out(hidden_states)\n        return hidden_states\n\n\nclass OPTModel(OPTPreTrainedModel):\n\n    def __init__(self, config: OPTConfig):\n        super().__init__(config)\n        self.decoder = OPTDecoder(config)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n    ) -> torch.Tensor:\n        return self.decoder(input_ids, positions)\n\n\nclass OPTForCausalLM(OPTPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = OPTModel(config)\n\n        # the lm_head weight is automatically tied to the embed tokens weight\n        self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.LongTensor,\n    ) -> torch.Tensor:\n        hidden_states = self.model.decoder(input_ids, positions)\n        logits = self.lm_head(hidden_states).contiguous()\n        return logits\n",
      "diff": "diff --git a/cacheflow/models/opt.py b/cacheflow/models/opt.py\nindex 3a340317a..234ab2635 100644\n--- a/cacheflow/models/opt.py\n+++ b/cacheflow/models/opt.py\n@@ -1,9 +1,17 @@\n \"\"\"1D OPT model compatible with HuggingFace weights.\"\"\"\n+from typing import Dict, List, Optional, Tuple\n+\n import torch\n from torch import nn\n from transformers import OPTConfig\n from transformers import PreTrainedModel\n \n+from cacheflow.models import InputMetadata\n+from cacheflow.models.attention import OPTCacheFlowAttention\n+from cacheflow.models.sample import Sampler\n+\n+KVCache = Tuple[torch.Tensor, torch.Tensor]\n+\n \n class OPTLearnedPositionalEmbedding(nn.Embedding):\n \n@@ -31,17 +39,27 @@ class OPTAttention(nn.Module):\n         self.head_dim = embed_dim // num_heads\n         self.scaling = self.head_dim**-0.5\n \n+        # TODO(woosuk): Fuse the three linear layers into one QKV linear layer.\n         self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        q = self.q_proj(hidden_states) * self.scaling\n+        self.attn = OPTCacheFlowAttention(scale=self.scaling)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        kv_cache: KVCache,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n+        q = self.q_proj(hidden_states)\n         k = self.k_proj(hidden_states)\n         v = self.v_proj(hidden_states)\n-        # TODO\n-        attn_output = None\n+        key_cache, value_cache = kv_cache\n+        attn_output = self.attn(\n+            q, k, v, key_cache, value_cache, input_metadata, cache_event)\n         output = self.out_proj(attn_output)\n         return output\n \n@@ -66,13 +84,23 @@ class OPTDecoderLayer(nn.Module):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        kv_cache: KVCache,\n+        input_metadata: InputMetadata,\n+        cache_event: Optional[torch.cuda.Event],\n+    ) -> torch.Tensor:\n         # Self Attention\n         residual = hidden_states\n         # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n         if self.do_layer_norm_before:\n             hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states = self.self_attn(hidden_states=hidden_states)\n+        hidden_states = self.self_attn(\n+            hidden_states=hidden_states,\n+            kv_cache=kv_cache,\n+            input_metadata=input_metadata,\n+            cache_event=cache_event)\n         hidden_states = residual + hidden_states\n         # 350m applies layer norm AFTER attention\n         if not self.do_layer_norm_before:\n@@ -145,6 +173,9 @@ class OPTDecoder(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n     ) -> torch.Tensor:\n         inputs_embeds = self.embed_tokens(input_ids)\n         pos_embeds = self.embed_positions(positions)\n@@ -153,8 +184,14 @@ class OPTDecoder(OPTPreTrainedModel):\n             inputs_embeds = self.project_in(inputs_embeds)\n         hidden_states = inputs_embeds + pos_embeds\n \n-        for layer in self.layers:\n-            hidden_states = layer(hidden_states)\n+        for i in range(len(self.layers)):\n+            if cache_events is None:\n+                cache_event = None\n+            else:\n+                cache_event = cache_events[i]\n+            layer = self.layers[i]\n+            hidden_states = layer(\n+                hidden_states, kv_caches[i], input_metadata, cache_event)\n \n         if self.final_layer_norm is not None:\n             hidden_states = self.final_layer_norm(hidden_states)\n@@ -175,8 +212,12 @@ class OPTModel(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n     ) -> torch.Tensor:\n-        return self.decoder(input_ids, positions)\n+        return self.decoder(\n+            input_ids, positions, kv_caches, input_metadata, cache_events)\n \n \n class OPTForCausalLM(OPTPreTrainedModel):\n@@ -185,9 +226,9 @@ class OPTForCausalLM(OPTPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = OPTModel(config)\n-\n         # the lm_head weight is automatically tied to the embed tokens weight\n         self.lm_head = nn.Linear(config.word_embed_proj_dim, config.vocab_size, bias=False)\n+        self.sampler = Sampler(embedding=self.lm_head.weight)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -196,7 +237,11 @@ class OPTForCausalLM(OPTPreTrainedModel):\n         self,\n         input_ids: torch.LongTensor,\n         positions: torch.LongTensor,\n-    ) -> torch.Tensor:\n-        hidden_states = self.model.decoder(input_ids, positions)\n-        logits = self.lm_head(hidden_states).contiguous()\n-        return logits\n+        kv_caches: List[KVCache],\n+        input_metadata: InputMetadata,\n+        cache_events: Optional[List[torch.cuda.Event]],\n+    ) -> Dict[int, Tuple[int, int]]:\n+        hidden_states = self.model(\n+            input_ids, positions, kv_caches, input_metadata, cache_events)\n+        next_tokens = self.sampler(hidden_states, input_metadata)\n+        return next_tokens",
      "change_type": "modified",
      "lines_added": 60,
      "lines_removed": 15
    }
  ],
  "affected_apis": [
    "OPTAttention.forward",
    "OPTDecoderLayer.forward",
    "OPTDecoder.forward",
    "OPTModel.forward",
    "OPTForCausalLM.forward"
  ],
  "summary": {
    "total_files": 2,
    "files_added": 1,
    "files_deleted": 0,
    "files_modified": 1
  },
  "csv_metadata": {
    "category": "model-based",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "No (too old that the PR doesn't even exist anymore)",
    "is_benchmark_actually_there": "",
    "sample_clues": "attention, forward, opt"
  }
}