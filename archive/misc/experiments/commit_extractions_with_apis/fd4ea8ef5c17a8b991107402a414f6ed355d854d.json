{
  "commit_hash": "fd4ea8ef5c17a8b991107402a414f6ed355d854d",
  "parent_hash": "1066cbd152fb7c9a096e914a10ce675a14796b92",
  "message": "Use NCCL instead of ray for control-plane communication to remove serialization overhead (#2221)",
  "author": "Zhuohan Li <zhuohan123@gmail.com>",
  "date": "2024-01-03 11:30:22 -0800",
  "files_changed": [
    {
      "file_path": "docs/source/models/adding_model.rst",
      "old_content": ".. _adding_a_new_model:\n\nAdding a New Model\n==================\n\nThis document provides a high-level guide on integrating a `HuggingFace Transformers <https://github.com/huggingface/transformers>`_ model into vLLM.\n\n.. note::\n    The complexity of adding a new model depends heavily on the model's architecture.\n    The process is considerably straightforward if the model shares a similar architecture with an existing model in vLLM.\n    However, for models that include new operators (e.g., a new attention mechanism), the process can be a bit more complex.\n\n.. tip::\n    If you are encountering issues while integrating your model into vLLM, feel free to open an issue on our `GitHub <https://github.com/vllm-project/vllm/issues>`_ repository.\n    We will be happy to help you out!\n\n\n0. Fork the vLLM repository\n--------------------------------\n\nStart by forking our `GitHub`_ repository and then :ref:`build it from source <build_from_source>`.\nThis gives you the ability to modify the codebase and test your model.\n\n\n1. Bring your model code\n------------------------\n\nClone the PyTorch model code from the HuggingFace Transformers repository and put it into the `vllm/model_executor/models <https://github.com/vllm-project/vllm/tree/main/vllm/model_executor/models>`_ directory.\nFor instance, vLLM's `OPT model <https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/opt.py>`_ was adapted from the HuggingFace's `modeling_opt.py <https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py>`_ file.\n\n.. warning::\n    When copying the model code, make sure to review and adhere to the code's copyright and licensing terms.\n\n\n2. Rewrite the :code:`forward` methods\n--------------------------------------\n\nNext, you need to rewrite the :code:`forward` methods of your model by following these steps:\n\n1. Remove any unnecessary code, such as the code only used for training.\n2. Change the input parameters:\n\n.. code-block:: diff\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n    -    attention_mask: Optional[torch.Tensor] = None,\n    -    position_ids: Optional[torch.LongTensor] = None,\n    -    past_key_values: Optional[List[torch.FloatTensor]] = None,\n    -    inputs_embeds: Optional[torch.FloatTensor] = None,\n    -    labels: Optional[torch.LongTensor] = None,\n    -    use_cache: Optional[bool] = None,\n    -    output_attentions: Optional[bool] = None,\n    -    output_hidden_states: Optional[bool] = None,\n    -    return_dict: Optional[bool] = None,\n    -) -> Union[Tuple, CausalLMOutputWithPast]:\n    +    positions: torch.Tensor,\n    +    kv_caches: List[KVCache],\n    +    input_metadata: InputMetadata,\n    +    cache_events: Optional[List[torch.cuda.Event]],\n    +) -> SamplerOutput:\n\n3. Update the code by considering that :code:`input_ids` and :code:`positions` are now flattened tensors.\n4. Replace the attention operation with either :code:`PagedAttention`, :code:`PagedAttentionWithRoPE`, or :code:`PagedAttentionWithALiBi` depending on the model's architecture.\n\n.. note::\n    Currently, vLLM supports the basic multi-head attention mechanism and its variant with rotary positional embeddings.\n    If your model employs a different attention mechanism, you will need to implement a new attention layer in vLLM.\n\n\n3. (Optional) Implement tensor parallelism and quantization support\n-------------------------------------------------------------------\n\nIf your model is too large to fit into a single GPU, you can use tensor parallelism to manage it.\nTo do this, substitute your model's linear and embedding layers with their tensor-parallel versions.\nFor the embedding layer, you can simply replace :code:`nn.Embedding` with :code:`VocabParallelEmbedding`. For the output LM head, you can use :code:`ParallelLMHead`.\nWhen it comes to the linear layers, we provide the following options to parallelize them:\n\n* :code:`ReplicatedLinear`: Replicates the inputs and weights across multiple GPUs. No memory saving.\n* :code:`RowParallelLinear`: The input tensor is partitioned along the hidden dimension. The weight matrix is partitioned along the rows (input dimension). An *all-reduce* operation is performed after the matrix multiplication to reduce the results. Typically used for the second FFN layer and the output linear transformation of the attention layer.\n* :code:`ColumnParallelLinear`: The input tensor is replicated. The weight matrix is partitioned along the columns (output dimension). The result is partitioned along the column dimension. Typically used for the first FFN layer and the separated QKV transformation of the attention layer in the original Transformer.\n* :code:`MergedColumnParallelLinear`: Column-parallel linear that merges multiple `ColumnParallelLinear` operators. Typically used for the first FFN layer with weighted activation functions (e.g., SiLU). This class handles the sharded weight loading logic of multiple weight matrices.\n* :code:`QKVParallelLinear`: Parallel linear layer for the query, key, and value projections of the multi-head and grouped-query attention mechanisms. When number of key/value heads are less than the world size, this class replicates the key/value heads properly. This class handles the weight loading and replication of the weight matrices.\n\nNote that all the linear layers above take `linear_method` as an input. vLLM will set this parameter according to different quantization schemes to support weight quantization.\n\n4. Implement the weight loading logic\n-------------------------------------\n\nYou now need to implement the :code:`load_weights` method in your :code:`*ForCausalLM` class.\nThis method should load the weights from the HuggingFace's checkpoint file and assign them to the corresponding layers in your model. Specifically, for `MergedColumnParallelLinear` and `QKVParallelLinear` layers, if the original model has separated weight matrices, you need to load the different parts separately.\n\n5. Register your model\n----------------------\n\nFinally, include your :code:`*ForCausalLM` class in `vllm/model_executor/models/__init__.py <https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/__init__.py>`_ and register it to the :code:`_MODEL_REGISTRY` in `vllm/model_executor/model_loader.py <https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/model_loader.py>`_.\n",
      "diff": "diff --git a/docs/source/models/adding_model.rst b/docs/source/models/adding_model.rst\nindex 1184a2322..bf243a044 100644\n--- a/docs/source/models/adding_model.rst\n+++ b/docs/source/models/adding_model.rst\n@@ -58,11 +58,10 @@ Next, you need to rewrite the :code:`forward` methods of your model by following\n     +    positions: torch.Tensor,\n     +    kv_caches: List[KVCache],\n     +    input_metadata: InputMetadata,\n-    +    cache_events: Optional[List[torch.cuda.Event]],\n-    +) -> SamplerOutput:\n+    +) -> Optional[SamplerOutput]:\n \n-3. Update the code by considering that :code:`input_ids` and :code:`positions` are now flattened tensors.\n-4. Replace the attention operation with either :code:`PagedAttention`, :code:`PagedAttentionWithRoPE`, or :code:`PagedAttentionWithALiBi` depending on the model's architecture.\n+1. Update the code by considering that :code:`input_ids` and :code:`positions` are now flattened tensors.\n+2. Replace the attention operation with either :code:`PagedAttention`, :code:`PagedAttentionWithRoPE`, or :code:`PagedAttentionWithALiBi` depending on the model's architecture.\n \n .. note::\n     Currently, vLLM supports the basic multi-head attention mechanism and its variant with rotary positional embeddings.",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 5
    },
    {
      "file_path": "requirements-rocm.txt",
      "old_content": "ninja  # For faster builds.\ntyping-extensions>=4.8.0\nstarlette\npsutil\nray >= 2.5.1\npandas  # Required for Ray data.\npyarrow  # Required for Ray data.\nsentencepiece  # Required for LLaMA tokenizer.\nnumpy\ntokenizers>=0.15.0\ntransformers >= 4.36.0  # Required for Mixtral.\nfastapi\nuvicorn[standard]\npydantic == 1.10.13  # Required for OpenAI server.\naioprometheus[starlette]\n",
      "diff": "diff --git a/requirements-rocm.txt b/requirements-rocm.txt\nindex 81bc19580..fd537f9cd 100644\n--- a/requirements-rocm.txt\n+++ b/requirements-rocm.txt\n@@ -3,8 +3,6 @@ typing-extensions>=4.8.0\n starlette\n psutil\n ray >= 2.5.1\n-pandas  # Required for Ray data.\n-pyarrow  # Required for Ray data.\n sentencepiece  # Required for LLaMA tokenizer.\n numpy\n tokenizers>=0.15.0",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 3
    },
    {
      "file_path": "requirements.txt",
      "old_content": "ninja  # For faster builds.\npsutil\nray >= 2.5.1\npandas  # Required for Ray data.\npyarrow  # Required for Ray data.\nsentencepiece  # Required for LLaMA tokenizer.\nnumpy\ntorch == 2.1.2\ntransformers >= 4.36.0  # Required for Mixtral.\nxformers == 0.0.23.post1  # Required for CUDA 12.1.\nfastapi\nuvicorn[standard]\npydantic == 1.10.13  # Required for OpenAI server.\naioprometheus[starlette]\n",
      "diff": "diff --git a/requirements.txt b/requirements.txt\nindex 92ba0a716..cee7f190d 100644\n--- a/requirements.txt\n+++ b/requirements.txt\n@@ -1,8 +1,6 @@\n ninja  # For faster builds.\n psutil\n ray >= 2.5.1\n-pandas  # Required for Ray data.\n-pyarrow  # Required for Ray data.\n sentencepiece  # Required for LLaMA tokenizer.\n numpy\n torch == 2.1.2",
      "change_type": "modified",
      "lines_added": 1,
      "lines_removed": 3
    },
    {
      "file_path": "tests/async_engine/test_api_server.py",
      "old_content": "import subprocess\nimport sys\nimport time\nfrom multiprocessing import Pool\nfrom pathlib import Path\n\nimport pytest\nimport requests\n\n\ndef _query_server(prompt: str) -> dict:\n    response = requests.post(\"http://localhost:8000/generate\",\n                             json={\n                                 \"prompt\": prompt,\n                                 \"max_tokens\": 100,\n                                 \"temperature\": 0,\n                                 \"ignore_eos\": True\n                             })\n    response.raise_for_status()\n    return response.json()\n\n\n@pytest.fixture\ndef api_server():\n    script_path = Path(__file__).parent.joinpath(\n        \"api_server_async_engine.py\").absolute()\n    uvicorn_process = subprocess.Popen([\n        sys.executable, \"-u\",\n        str(script_path), \"--model\", \"facebook/opt-125m\"\n    ])\n    yield\n    uvicorn_process.terminate()\n\n\ndef test_api_server(api_server):\n    \"\"\"\n    Run the API server and test it.\n\n    We run both the server and requests in separate processes.\n\n    We test that the server can handle incoming requests, including\n    multiple requests at the same time, and that it can handle requests\n    being cancelled without crashing.\n    \"\"\"\n    with Pool(32) as pool:\n        # Wait until the server is ready\n        prompts = [\"warm up\"] * 1\n        result = None\n        while not result:\n            try:\n                for r in pool.map(_query_server, prompts):\n                    result = r\n                    break\n            except requests.exceptions.ConnectionError:\n                time.sleep(1)\n\n        # Actual tests start here\n        # Try with 1 prompt\n        for result in pool.map(_query_server, prompts):\n            assert result\n\n        num_aborted_requests = requests.get(\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\n        assert num_aborted_requests == 0\n\n        # Try with 100 prompts\n        prompts = [\"test prompt\"] * 100\n        for result in pool.map(_query_server, prompts):\n            assert result\n\n        # Cancel requests\n        prompts = [\"canceled requests\"] * 100\n        pool.map_async(_query_server, prompts)\n        time.sleep(0.001)\n        pool.terminate()\n        pool.join()\n\n        # check cancellation stats\n        num_aborted_requests = requests.get(\n            \"http://localhost:8000/stats\").json()[\"num_aborted_requests\"]\n        assert num_aborted_requests > 0\n\n    # check that server still runs after cancellations\n    with Pool(32) as pool:\n        # Try with 100 prompts\n        prompts = [\"test prompt after canceled\"] * 100\n        for result in pool.map(_query_server, prompts):\n            assert result\n",
      "diff": "diff --git a/tests/async_engine/test_api_server.py b/tests/async_engine/test_api_server.py\nindex f1891f784..0b45e10dc 100644\n--- a/tests/async_engine/test_api_server.py\n+++ b/tests/async_engine/test_api_server.py\n@@ -8,11 +8,11 @@ import pytest\n import requests\n \n \n-def _query_server(prompt: str) -> dict:\n+def _query_server(prompt: str, max_tokens: int = 5) -> dict:\n     response = requests.post(\"http://localhost:8000/generate\",\n                              json={\n                                  \"prompt\": prompt,\n-                                 \"max_tokens\": 100,\n+                                 \"max_tokens\": max_tokens,\n                                  \"temperature\": 0,\n                                  \"ignore_eos\": True\n                              })\n@@ -20,6 +20,10 @@ def _query_server(prompt: str) -> dict:\n     return response.json()\n \n \n+def _query_server_long(prompt: str) -> dict:\n+    return _query_server(prompt, max_tokens=500)\n+\n+\n @pytest.fixture\n def api_server():\n     script_path = Path(__file__).parent.joinpath(\n@@ -68,10 +72,11 @@ def test_api_server(api_server):\n         for result in pool.map(_query_server, prompts):\n             assert result\n \n+    with Pool(32) as pool:\n         # Cancel requests\n         prompts = [\"canceled requests\"] * 100\n-        pool.map_async(_query_server, prompts)\n-        time.sleep(0.001)\n+        pool.map_async(_query_server_long, prompts)\n+        time.sleep(0.01)\n         pool.terminate()\n         pool.join()",
      "change_type": "modified",
      "lines_added": 10,
      "lines_removed": 5
    },
    {
      "file_path": "tests/kernels/test_cache.py",
      "old_content": "import random\n\nimport pytest\nimport torch\n\nfrom vllm._C import cache_ops\n\nDTYPES = [torch.half, torch.bfloat16, torch.float]\nNUM_TOKENS = [83]  # Arbitrary values for testing\nNUM_LAYERS = [1]  # Arbitrary values for testing\nNUM_HEADS = [8]  # Arbitrary values for testing\nHEAD_SIZES = [64, 80, 96, 112, 128, 256]\nBLOCK_SIZES = [8, 16, 32]\nNUM_BLOCKS = [1024, 36000]  # Arbitrary values for testing\nNUM_MAPPINGS = [256]  # Arbitrary values for testing\nSEEDS = [0]\nDEVICES = [i for i in range(1 if torch.cuda.device_count() == 1 else 2)]\n\n\n@pytest.mark.parametrize(\"num_mappings\", NUM_MAPPINGS)\n@pytest.mark.parametrize(\"num_layers\", NUM_LAYERS)\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\n@pytest.mark.parametrize(\"block_size\", BLOCK_SIZES)\n@pytest.mark.parametrize(\"num_blocks\", NUM_BLOCKS)\n@pytest.mark.parametrize(\"dtype\", DTYPES)\n@pytest.mark.parametrize(\"seed\", SEEDS)\n@pytest.mark.parametrize(\"device\", DEVICES)\n@torch.inference_mode()\ndef test_copy_blocks(\n    kv_cache_factory,\n    num_mappings: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n    seed: int,\n    device: int,\n) -> None:\n    random.seed(seed)\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gpu_id = f\"cuda:{device}\"\n    # Generate random block mappings where each source block is mapped to two\n    # destination blocks.\n    assert 2 * num_mappings <= num_blocks\n    src_blocks = random.sample(range(num_blocks), num_mappings)\n    remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\n    dst_blocks = random.sample(remainig_blocks, 2 * num_mappings)\n    block_mapping = {}\n    for i in range(num_mappings):\n        src = src_blocks[i]\n        dst1 = dst_blocks[2 * i]\n        dst2 = dst_blocks[2 * i + 1]\n        block_mapping[src] = [dst1, dst2]\n\n    # Create the KV caches.\n    key_caches, value_caches = kv_cache_factory(num_blocks, block_size,\n                                                num_layers, num_heads,\n                                                head_size, dtype, seed, gpu_id)\n\n    # Clone the KV caches.\n    cloned_key_caches = [key_cache.clone() for key_cache in key_caches]\n    cloned_value_caches = [value_cache.clone() for value_cache in value_caches]\n\n    # Call the copy blocks kernel.\n    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n\n    # Run the reference implementation.\n    for src, dsts in block_mapping.items():\n        for dst in dsts:\n            for cloned_key_cache in cloned_key_caches:\n                cloned_key_cache[dst].copy_(cloned_key_cache[src])\n            for cloned_value_cache in cloned_value_caches:\n                cloned_value_cache[dst].copy_(cloned_value_cache[src])\n\n    # Compare the results.\n    for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):\n        assert torch.allclose(key_cache, cloned_key_cache)\n    for value_cache, cloned_value_cache in zip(value_caches,\n                                               cloned_value_caches):\n        assert torch.allclose(value_cache, cloned_value_cache)\n\n\n@pytest.mark.parametrize(\"num_tokens\", NUM_TOKENS)\n@pytest.mark.parametrize(\"num_heads\", NUM_HEADS)\n@pytest.mark.parametrize(\"head_size\", HEAD_SIZES)\n@pytest.mark.parametrize(\"block_size\", BLOCK_SIZES)\n@pytest.mark.parametrize(\"num_blocks\", NUM_BLOCKS)\n@pytest.mark.parametrize(\"dtype\", DTYPES)\n@pytest.mark.parametrize(\"seed\", SEEDS)\n@pytest.mark.parametrize(\"device\", DEVICES)\n@torch.inference_mode()\ndef test_reshape_and_cache(\n    kv_cache_factory,\n    num_tokens: int,\n    num_heads: int,\n    head_size: int,\n    block_size: int,\n    num_blocks: int,\n    dtype: torch.dtype,\n    seed: int,\n    device: int,\n) -> None:\n    random.seed(seed)\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    gpu_id = f\"cuda:{device}\"\n    # Create a random slot mapping.\n    num_slots = block_size * num_blocks\n    slot_mapping = random.sample(range(num_slots), num_tokens)\n    slot_mapping = torch.tensor(slot_mapping, dtype=torch.long, device=gpu_id)\n\n    qkv = torch.randn(num_tokens,\n                      3,\n                      num_heads,\n                      head_size,\n                      dtype=dtype,\n                      device=gpu_id)\n    _, key, value = qkv.unbind(dim=1)\n\n    # Create the KV caches.\n    key_caches, value_caches = kv_cache_factory(num_blocks, block_size, 1,\n                                                num_heads, head_size, dtype,\n                                                seed, gpu_id)\n    key_cache, value_cache = key_caches[0], value_caches[0]\n\n    # Clone the KV caches.\n    cloned_key_cache = key_cache.clone()\n    cloned_value_cache = value_cache.clone()\n\n    # Call the reshape_and_cache kernel.\n    cache_ops.reshape_and_cache(key, value, key_cache, value_cache,\n                                slot_mapping)\n\n    # Run the reference implementation.\n    reshaped_key = key.reshape(num_tokens, *key_cache[0, :, :, 0, :].shape)\n    block_indicies = torch.div(slot_mapping, block_size, rounding_mode=\"floor\")\n    block_indicies = block_indicies.cpu().tolist()\n    block_offsets = slot_mapping % block_size\n    block_offsets = block_offsets.cpu().tolist()\n    for i in range(num_tokens):\n        block_idx = block_indicies[i]\n        block_offset = block_offsets[i]\n        cloned_key_cache[block_idx, :, :, block_offset, :] = reshaped_key[i]\n        cloned_value_cache[block_idx, :, :, block_offset] = value[i]\n\n    assert torch.allclose(key_cache, cloned_key_cache)\n    assert torch.allclose(value_cache, cloned_value_cache)\n",
      "diff": "diff --git a/tests/kernels/test_cache.py b/tests/kernels/test_cache.py\nindex 1d8d41e01..3749592a0 100644\n--- a/tests/kernels/test_cache.py\n+++ b/tests/kernels/test_cache.py\n@@ -49,12 +49,13 @@ def test_copy_blocks(\n     src_blocks = random.sample(range(num_blocks), num_mappings)\n     remainig_blocks = list(set(range(num_blocks)) - set(src_blocks))\n     dst_blocks = random.sample(remainig_blocks, 2 * num_mappings)\n-    block_mapping = {}\n+    copy_src = []\n+    copy_dst = []\n     for i in range(num_mappings):\n-        src = src_blocks[i]\n-        dst1 = dst_blocks[2 * i]\n-        dst2 = dst_blocks[2 * i + 1]\n-        block_mapping[src] = [dst1, dst2]\n+        copy_src.append(src_blocks[i])\n+        copy_dst.append(dst_blocks[2 * i])\n+        copy_src.append(src_blocks[i])\n+        copy_dst.append(dst_blocks[2 * i + 1])\n \n     # Create the KV caches.\n     key_caches, value_caches = kv_cache_factory(num_blocks, block_size,\n@@ -66,15 +67,14 @@ def test_copy_blocks(\n     cloned_value_caches = [value_cache.clone() for value_cache in value_caches]\n \n     # Call the copy blocks kernel.\n-    cache_ops.copy_blocks(key_caches, value_caches, block_mapping)\n+    cache_ops.copy_blocks(key_caches, value_caches, copy_src, copy_dst)\n \n     # Run the reference implementation.\n-    for src, dsts in block_mapping.items():\n-        for dst in dsts:\n-            for cloned_key_cache in cloned_key_caches:\n-                cloned_key_cache[dst].copy_(cloned_key_cache[src])\n-            for cloned_value_cache in cloned_value_caches:\n-                cloned_value_cache[dst].copy_(cloned_value_cache[src])\n+    for src, dst in zip(copy_src, copy_dst):\n+        for cloned_key_cache in cloned_key_caches:\n+            cloned_key_cache[dst].copy_(cloned_key_cache[src])\n+        for cloned_value_cache in cloned_value_caches:\n+            cloned_value_cache[dst].copy_(cloned_value_cache[src])\n \n     # Compare the results.\n     for key_cache, cloned_key_cache in zip(key_caches, cloned_key_caches):",
      "change_type": "modified",
      "lines_added": 13,
      "lines_removed": 13
    },
    {
      "file_path": "tests/worker/test_model_runner.py",
      "old_content": "import random\nimport torch\n\nfrom vllm.sequence import SamplingParams, SequenceData, SequenceGroupMetadata\nfrom vllm.worker.model_runner import ModelRunner\n\n\ndef test_prepare_prompt():\n    model_runner = ModelRunner(None, None, None)\n    model_runner.set_block_size(16)\n\n    batch_size = random.randint(1, 256)\n    prompt_lens = []\n    seq_group_metadata_list = []\n    for i in range(batch_size):\n        # make sure all tokens fit into one block\n        prompt_len = i % (model_runner.block_size - 1) + 1\n        prompt_lens.append(prompt_len)\n        seq_data = list(range(prompt_len))\n        seq_group_metadata_list.append(\n            SequenceGroupMetadata(\n                request_id=f\"test_{i}\",\n                is_prompt=True,\n                seq_data={0: SequenceData(seq_data)},\n                sampling_params=SamplingParams(temperature=0),\n                block_tables={0: [1]},\n            ))\n\n    expected_selected_token_indices = []\n    selected_token_start_idx = 0\n    max_seq_len = max(prompt_lens)\n    for prompt_len in prompt_lens:\n        expected_selected_token_indices.append(selected_token_start_idx +\n                                               prompt_len - 1)\n        selected_token_start_idx += max_seq_len\n    input_tokens, input_positions, _ = model_runner._prepare_prompt(\n        seq_group_metadata_list)\n    sampling_metadata = model_runner._prepare_sample(seq_group_metadata_list,\n                                                     prompt_lens)\n    assert input_tokens.shape == (batch_size, max_seq_len)\n    assert input_positions.shape == (batch_size, max_seq_len)\n    torch.testing.assert_close(input_tokens, input_positions)\n\n    actual = sampling_metadata.selected_token_indices\n    expected = torch.tensor(expected_selected_token_indices,\n                            device=actual.device,\n                            dtype=actual.dtype)\n    torch.testing.assert_close(actual, expected)\n",
      "diff": "diff --git a/tests/worker/test_model_runner.py b/tests/worker/test_model_runner.py\nindex 949a7e229..250d84caf 100644\n--- a/tests/worker/test_model_runner.py\n+++ b/tests/worker/test_model_runner.py\n@@ -33,8 +33,9 @@ def test_prepare_prompt():\n         expected_selected_token_indices.append(selected_token_start_idx +\n                                                prompt_len - 1)\n         selected_token_start_idx += max_seq_len\n-    input_tokens, input_positions, _ = model_runner._prepare_prompt(\n-        seq_group_metadata_list)\n+    input_tokens, input_positions, _, return_prompt_lens = (\n+        model_runner._prepare_prompt(seq_group_metadata_list))\n+    assert return_prompt_lens == prompt_lens\n     sampling_metadata = model_runner._prepare_sample(seq_group_metadata_list,\n                                                      prompt_lens)\n     assert input_tokens.shape == (batch_size, max_seq_len)",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/engine/async_llm_engine.py",
      "old_content": "import asyncio\nimport time\nfrom functools import partial\nfrom typing import (Any, Dict, Iterable, List, Optional, Set, Tuple, Type,\n                    Union, AsyncIterator)\n\nfrom vllm.config import ModelConfig\nfrom vllm.engine.arg_utils import AsyncEngineArgs\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.engine.ray_utils import initialize_cluster, ray\nfrom vllm.logger import init_logger\nfrom vllm.outputs import RequestOutput\nfrom vllm.sampling_params import SamplingParams\n\nlogger = init_logger(__name__)\n\n\nclass AsyncEngineDeadError(RuntimeError):\n    pass\n\n\ndef _raise_exception_on_finish(task: asyncio.Task,\n                               request_tracker: \"RequestTracker\") -> None:\n    msg = (\"Task finished unexpectedly. This should never happen! \"\n           \"Please open an issue on Github.\")\n    try:\n        try:\n            task.result()\n        except asyncio.CancelledError:\n            return\n        except Exception as exc:\n            raise AsyncEngineDeadError(\n                msg + \" See stack trace above for the actual cause.\") from exc\n        raise AsyncEngineDeadError(msg)\n    except Exception as exc:\n        request_tracker.propagate_exception(exc)\n        raise exc\n\n\nclass AsyncStream:\n    \"\"\"A stream of RequestOutputs for a request that can be\n    iterated over asynchronously.\"\"\"\n\n    def __init__(self, request_id: str) -> None:\n        self.request_id = request_id\n        self._queue = asyncio.Queue()\n        self._finished = False\n\n    def put(self, item: RequestOutput) -> None:\n        if self._finished:\n            return\n        self._queue.put_nowait(item)\n\n    def finish(self) -> None:\n        self._queue.put_nowait(StopIteration)\n        self._finished = True\n\n    @property\n    def finished(self) -> bool:\n        return self._finished\n\n    def __aiter__(self):\n        return self\n\n    async def __anext__(self) -> RequestOutput:\n        result = await self._queue.get()\n        if result is StopIteration:\n            raise StopAsyncIteration\n        elif isinstance(result, Exception):\n            raise result\n        return result\n\n\nclass RequestTracker:\n    \"\"\"Synchronous abstraction for tracking requests.\"\"\"\n\n    def __init__(self) -> None:\n        self._request_streams: Dict[str, AsyncStream] = {}\n        self._finished_requests: asyncio.Queue[str] = asyncio.Queue()\n        self._new_requests: asyncio.Queue[Tuple[AsyncStream,\n                                                dict]] = asyncio.Queue()\n        self.new_requests_event = None\n\n    def __contains__(self, item):\n        return item in self._request_streams\n\n    def init_event(self):\n        self.new_requests_event = asyncio.Event()\n\n    def propagate_exception(self,\n                            exc: Exception,\n                            request_id: Optional[str] = None) -> None:\n        \"\"\"Propagate an exception to request streams\n        (all if request_id is None).\"\"\"\n        if request_id is not None:\n            self._request_streams[request_id].put(exc)\n        else:\n            for stream in self._request_streams.values():\n                stream.put(exc)\n\n    def process_request_output(self,\n                               request_output: RequestOutput,\n                               *,\n                               verbose: bool = False) -> None:\n        \"\"\"Process a request output from the engine.\"\"\"\n        request_id = request_output.request_id\n\n        self._request_streams[request_id].put(request_output)\n        if request_output.finished:\n            if verbose:\n                logger.info(f\"Finished request {request_id}.\")\n            self.abort_request(request_id)\n\n    def add_request(self, request_id: str,\n                    **engine_add_request_kwargs) -> AsyncStream:\n        \"\"\"Add a request to be sent to the engine on the next background\n        loop iteration.\"\"\"\n        if request_id in self._request_streams:\n            raise KeyError(f\"Request {request_id} already exists.\")\n\n        stream = AsyncStream(request_id)\n        self._new_requests.put_nowait((stream, {\n            \"request_id\": request_id,\n            **engine_add_request_kwargs\n        }))\n\n        self.new_requests_event.set()\n\n        return stream\n\n    def abort_request(self, request_id: str, *, verbose: bool = False) -> None:\n        \"\"\"Abort a request during next background loop iteration.\"\"\"\n        if verbose:\n            logger.info(f\"Aborted request {request_id}.\")\n\n        self._finished_requests.put_nowait(request_id)\n\n        if request_id not in self._request_streams or self._request_streams[\n                request_id].finished:\n            # The request has already finished or been aborted.\n            return\n\n        self._request_streams[request_id].finish()\n\n    def get_new_and_finished_requests(self) -> Tuple[List[Dict], Set[str]]:\n        \"\"\"Get the new requests and finished requests to be\n        sent to the engine.\"\"\"\n        new_requests: List[Dict] = []\n        finished_requests: Set[str] = set()\n\n        while not self._finished_requests.empty():\n            request_id = self._finished_requests.get_nowait()\n            finished_requests.add(request_id)\n            self._request_streams.pop(request_id, None)\n\n        while not self._new_requests.empty():\n            stream, new_request = self._new_requests.get_nowait()\n            if stream.request_id in finished_requests:\n                # The request has already been aborted.\n                stream.finish()\n                continue\n            self._request_streams[stream.request_id] = stream\n            new_requests.append(new_request)\n\n        self.new_requests_event.clear()\n\n        return new_requests, finished_requests\n\n    async def wait_for_new_requests(self):\n        await self.new_requests_event.wait()\n\n\nclass _AsyncLLMEngine(LLMEngine):\n    \"\"\"Extension of LLMEngine to add async methods.\"\"\"\n\n    async def step_async(self) -> List[RequestOutput]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n        The workers are ran asynchronously if possible.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n\n        # Execute the model.\n        output = (await self._run_workers_async(\n            \"execute_model\",\n            seq_group_metadata_list=seq_group_metadata_list,\n            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n            blocks_to_copy=scheduler_outputs.blocks_to_copy,\n        )) if not scheduler_outputs.is_empty() else []\n\n        return self._process_model_outputs(output, scheduler_outputs)\n\n    async def _run_workers_async(\n        self,\n        method: str,\n        *args,\n        get_all_outputs: bool = False,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        coros = []\n        for worker in self.workers:\n            if self.parallel_config.worker_use_ray:\n                coros.append(\n                    worker.execute_method.remote(method, *args, **kwargs))\n            else:\n                executor = getattr(worker, method)\n                coros.append(asyncio.get_event_loop().run_in_executor(\n                    None, partial(executor, *args, **kwargs)))\n\n        all_outputs = await asyncio.gather(*coros)\n\n        if get_all_outputs:\n            return all_outputs\n\n        # Make sure all workers have the same results.\n        output = all_outputs[0]\n        for other_output in all_outputs[1:]:\n            assert output == other_output\n        return output\n\n\nclass AsyncLLMEngine:\n    \"\"\"An asynchronous wrapper for LLMEngine.\n\n    This class is used to wrap the LLMEngine class to make it asynchronous. It\n    uses asyncio to create a background loop that keeps processing incoming\n    requests. The LLMEngine is kicked by the generate method when there\n    are requests in the waiting queue. The generate method yields the outputs\n    from the LLMEngine to the caller.\n\n    NOTE: For the comprehensive list of arguments, see `LLMEngine`.\n\n    Args:\n        worker_use_ray: Whether to use Ray for model workers. Required for\n            distributed execution. Should be the same as\n            `parallel_config.worker_use_ray`.\n        engine_use_ray: Whether to make LLMEngine a Ray actor. If so, the\n            async frontend will be executed in a separate process as the\n            model workers.\n        log_requests: Whether to log the requests.\n        start_engine_loop: If True, the background task to run the engine\n            will be automatically started in the generate call.\n        *args, *kwargs: Arguments for LLMEngine.\n    \"\"\"\n\n    _engine_class: Type[_AsyncLLMEngine] = _AsyncLLMEngine\n\n    def __init__(self,\n                 worker_use_ray: bool,\n                 engine_use_ray: bool,\n                 *args,\n                 log_requests: bool = True,\n                 max_log_len: Optional[int] = None,\n                 start_engine_loop: bool = True,\n                 **kwargs) -> None:\n        self.worker_use_ray = worker_use_ray\n        self.engine_use_ray = engine_use_ray\n        self.log_requests = log_requests\n        self.max_log_len = max_log_len\n        self.engine = self._init_engine(*args, **kwargs)\n\n        self.background_loop = None\n        # We need to keep a reference to unshielded\n        # task as well to prevent it from being garbage\n        # collected\n        self._background_loop_unshielded = None\n        self.start_engine_loop = start_engine_loop\n        self._request_tracker = RequestTracker()\n\n    @property\n    def is_running(self) -> bool:\n        return (self.background_loop is not None\n                and not self.background_loop.done())\n\n    def start_background_loop(self) -> None:\n        \"\"\"Start the background loop.\"\"\"\n        if self.is_running:\n            raise RuntimeError(\"Background loop is already running.\")\n        self._request_tracker.init_event()\n\n        self._background_loop_unshielded = asyncio.get_event_loop(\n        ).create_task(self.run_engine_loop())\n        self._background_loop_unshielded.add_done_callback(\n            partial(_raise_exception_on_finish,\n                    request_tracker=self._request_tracker))\n        self.background_loop = asyncio.shield(self._background_loop_unshielded)\n\n    def _init_engine(self, *args,\n                     **kwargs) -> Union[_AsyncLLMEngine, \"ray.ObjectRef\"]:\n        if not self.engine_use_ray:\n            engine_class = self._engine_class\n        elif self.worker_use_ray:\n            engine_class = ray.remote(num_cpus=0)(self._engine_class).remote\n        else:\n            # FIXME(woosuk): This is a bit hacky. Be careful when changing the\n            # order of the arguments.\n            cache_config = args[1]\n            parallel_config = args[2]\n            if parallel_config.tensor_parallel_size == 1:\n                num_gpus = cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            engine_class = ray.remote(num_gpus=num_gpus)(\n                self._engine_class).remote\n        return engine_class(*args, **kwargs)\n\n    async def engine_step(self) -> bool:\n        \"\"\"Kick the engine to process the waiting requests.\n\n        Returns True if there are in-progress requests.\"\"\"\n\n        new_requests, finished_requests = (\n            self._request_tracker.get_new_and_finished_requests())\n\n        for new_request in new_requests:\n            # Add the request into the vLLM engine's waiting queue.\n            # TODO: Maybe add add_request_batch to reduce Ray overhead\n            if self.engine_use_ray:\n                await self.engine.add_request.remote(**new_request)\n            else:\n                self.engine.add_request(**new_request)\n\n        if finished_requests:\n            await self._engine_abort(finished_requests)\n\n        if self.engine_use_ray:\n            request_outputs = await self.engine.step.remote()\n        else:\n            request_outputs = await self.engine.step_async()\n\n        # Put the outputs into the corresponding streams.\n        for request_output in request_outputs:\n            self._request_tracker.process_request_output(\n                request_output, verbose=self.log_requests)\n\n        return len(request_outputs) > 0\n\n    async def _engine_abort(self, request_ids: Iterable[str]):\n        if self.engine_use_ray:\n            await self.engine.abort_request.remote(request_ids)\n        else:\n            self.engine.abort_request(request_ids)\n\n    async def run_engine_loop(self):\n        # Initialize the RequestTracker here so it uses the right event loop.\n        has_requests_in_progress = False\n        while True:\n            if not has_requests_in_progress:\n                await self._request_tracker.wait_for_new_requests()\n            has_requests_in_progress = await self.engine_step()\n            await asyncio.sleep(0)\n\n    async def add_request(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        sampling_params: SamplingParams,\n        prompt_token_ids: Optional[List[int]] = None,\n        arrival_time: Optional[float] = None,\n    ) -> AsyncStream:\n        if self.log_requests:\n            shortened_prompt = prompt\n            shortened_token_ids = prompt_token_ids\n            if self.max_log_len is not None:\n                if shortened_prompt is not None:\n                    shortened_prompt = shortened_prompt[:self.max_log_len]\n                if shortened_token_ids is not None:\n                    shortened_token_ids = shortened_token_ids[:self.\n                                                              max_log_len]\n            logger.info(f\"Received request {request_id}: \"\n                        f\"prompt: {shortened_prompt!r}, \"\n                        f\"sampling params: {sampling_params}, \"\n                        f\"prompt token ids: {shortened_token_ids}.\")\n\n        if not self.is_running:\n            if self.start_engine_loop:\n                self.start_background_loop()\n            else:\n                raise AsyncEngineDeadError(\n                    \"Background loop is not running. If it was running, \"\n                    \"inspect the output to find the stacktrace of the \"\n                    \"error that caused the background loop to stop \"\n                    \"(AsyncEngineDeadError).\")\n\n        stream = self._request_tracker.add_request(\n            request_id,\n            prompt=prompt,\n            sampling_params=sampling_params,\n            prompt_token_ids=prompt_token_ids,\n            arrival_time=arrival_time)\n\n        return stream\n\n    async def generate(\n        self,\n        prompt: Optional[str],\n        sampling_params: SamplingParams,\n        request_id: str,\n        prompt_token_ids: Optional[List[int]] = None\n    ) -> AsyncIterator[RequestOutput]:\n        \"\"\"Generate outputs for a request.\n\n        Generate outputs for a request. This method is a coroutine. It adds the\n        request into the waiting queue of the LLMEngine and streams the outputs\n        from the LLMEngine to the caller.\n\n        Args:\n            prompt: The prompt string. Can be None if prompt_token_ids is\n                provided.\n            sampling_params: The sampling parameters of the request.\n            request_id: The unique id of the request.\n            prompt_token_ids: The token IDs of the prompt. If None, we\n                use the tokenizer to convert the prompts to token IDs.\n\n        Yields:\n            The output `RequestOutput` objects from the LLMEngine for the\n            request.\n        \"\"\"\n        # Preprocess the request.\n        # This should not be used for logging, as it is monotonic time.\n        arrival_time = time.monotonic()\n\n        try:\n            stream = await self.add_request(request_id,\n                                            prompt,\n                                            sampling_params,\n                                            prompt_token_ids=prompt_token_ids,\n                                            arrival_time=arrival_time)\n\n            async for request_output in stream:\n                yield request_output\n        except (Exception, asyncio.CancelledError) as e:\n            # If there is an exception or coroutine is cancelled, abort the\n            # request.\n            self._abort(request_id)\n            raise e\n\n    async def abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        if not self.is_running:\n            raise AsyncEngineDeadError(\n                \"Background loop is not running. If it was running, \"\n                \"inspect the output to find the stacktrace of the \"\n                \"error that caused the background loop to stop \"\n                \"(AsyncEngineDeadError).\")\n\n        return self._abort(request_id)\n\n    def _abort(self, request_id: str) -> None:\n        \"\"\"Abort a request.\n\n        Abort a submitted request. If the request is finished or not found,\n        this method will be a no-op.\n\n        Args:\n            request_id: The unique id of the request.\n        \"\"\"\n        self._request_tracker.abort_request(request_id,\n                                            verbose=self.log_requests)\n\n    async def get_model_config(self) -> ModelConfig:\n        \"\"\"Get the model configuration of the vLLM engine.\"\"\"\n        if self.engine_use_ray:\n            return await self.engine.get_model_config.remote()\n        else:\n            return self.engine.get_model_config()\n\n    @classmethod\n    def from_engine_args(cls,\n                         engine_args: AsyncEngineArgs,\n                         start_engine_loop: bool = True) -> \"AsyncLLMEngine\":\n        \"\"\"Creates an async LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_configs = engine_args.create_engine_configs()\n        parallel_config = engine_configs[2]\n        # Initialize the cluster.\n        distributed_init_method, placement_group = initialize_cluster(\n            parallel_config, engine_args.engine_use_ray)\n        # Create the async LLM engine.\n        engine = cls(parallel_config.worker_use_ray,\n                     engine_args.engine_use_ray,\n                     *engine_configs,\n                     distributed_init_method,\n                     placement_group,\n                     log_requests=not engine_args.disable_log_requests,\n                     log_stats=not engine_args.disable_log_stats,\n                     max_log_len=engine_args.max_log_len,\n                     start_engine_loop=start_engine_loop)\n        return engine\n",
      "diff": "diff --git a/vllm/engine/async_llm_engine.py b/vllm/engine/async_llm_engine.py\nindex 611da51f6..fbe4a4e5d 100644\n--- a/vllm/engine/async_llm_engine.py\n+++ b/vllm/engine/async_llm_engine.py\n@@ -185,14 +185,21 @@ class _AsyncLLMEngine(LLMEngine):\n         \"\"\"\n         seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n \n-        # Execute the model.\n-        output = (await self._run_workers_async(\n-            \"execute_model\",\n-            seq_group_metadata_list=seq_group_metadata_list,\n-            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n-            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n-            blocks_to_copy=scheduler_outputs.blocks_to_copy,\n-        )) if not scheduler_outputs.is_empty() else []\n+        if not scheduler_outputs.is_empty():\n+            # Execute the model.\n+            all_outputs = await self._run_workers_async(\n+                \"execute_model\",\n+                driver_kwargs={\n+                    \"seq_group_metadata_list\": seq_group_metadata_list,\n+                    \"blocks_to_swap_in\": scheduler_outputs.blocks_to_swap_in,\n+                    \"blocks_to_swap_out\": scheduler_outputs.blocks_to_swap_out,\n+                    \"blocks_to_copy\": scheduler_outputs.blocks_to_copy,\n+                })\n+\n+            # Only the driver worker returns the sampling results.\n+            output = all_outputs[0]\n+        else:\n+            output = []\n \n         return self._process_model_outputs(output, scheduler_outputs)\n \n@@ -200,30 +207,29 @@ class _AsyncLLMEngine(LLMEngine):\n         self,\n         method: str,\n         *args,\n-        get_all_outputs: bool = False,\n+        driver_args: Optional[List[Any]] = None,\n+        driver_kwargs: Optional[Dict[str, Any]] = None,\n         **kwargs,\n     ) -> Any:\n         \"\"\"Runs the given method on all workers.\"\"\"\n         coros = []\n-        for worker in self.workers:\n-            if self.parallel_config.worker_use_ray:\n-                coros.append(\n-                    worker.execute_method.remote(method, *args, **kwargs))\n-            else:\n-                executor = getattr(worker, method)\n-                coros.append(asyncio.get_event_loop().run_in_executor(\n-                    None, partial(executor, *args, **kwargs)))\n \n-        all_outputs = await asyncio.gather(*coros)\n+        if driver_args is None:\n+            driver_args = args\n+        if driver_kwargs is None:\n+            driver_kwargs = kwargs\n \n-        if get_all_outputs:\n-            return all_outputs\n+        # Run the driver worker asynchronously.\n+        driver_executor = getattr(self.driver_worker, method)\n+        coros.append(asyncio.get_event_loop().run_in_executor(\n+            None, partial(driver_executor, *driver_args, **driver_kwargs)))\n \n-        # Make sure all workers have the same results.\n-        output = all_outputs[0]\n-        for other_output in all_outputs[1:]:\n-            assert output == other_output\n-        return output\n+        # Run the ray workers asynchronously.\n+        for worker in self.workers:\n+            coros.append(worker.execute_method.remote(method, *args, **kwargs))\n+\n+        all_outputs = await asyncio.gather(*coros)\n+        return all_outputs\n \n \n class AsyncLLMEngine:\n@@ -488,13 +494,12 @@ class AsyncLLMEngine:\n         engine_configs = engine_args.create_engine_configs()\n         parallel_config = engine_configs[2]\n         # Initialize the cluster.\n-        distributed_init_method, placement_group = initialize_cluster(\n-            parallel_config, engine_args.engine_use_ray)\n+        placement_group = initialize_cluster(parallel_config,\n+                                             engine_args.engine_use_ray)\n         # Create the async LLM engine.\n         engine = cls(parallel_config.worker_use_ray,\n                      engine_args.engine_use_ray,\n                      *engine_configs,\n-                     distributed_init_method,\n                      placement_group,\n                      log_requests=not engine_args.disable_log_requests,\n                      log_stats=not engine_args.disable_log_stats,",
      "change_type": "modified",
      "lines_added": 34,
      "lines_removed": 29
    },
    {
      "file_path": "vllm/engine/llm_engine.py",
      "old_content": "import copy\nimport os\nimport time\nfrom functools import partial\nfrom typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union\n\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                         SchedulerConfig)\nfrom vllm.core.scheduler import Scheduler, SchedulerOutputs\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics import record_metrics\nfrom vllm.engine.ray_utils import RayWorkerVllm, initialize_cluster, ray\nfrom vllm.logger import init_logger\nfrom vllm.outputs import RequestOutput\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (SamplerOutput, Sequence, SequenceGroup,\n                           SequenceGroupOutput, SequenceOutput, SequenceStatus)\nfrom vllm.transformers_utils.tokenizer import (detokenize_incrementally,\n                                               get_tokenizer)\nfrom vllm.utils import Counter\n\nif ray:\n    from ray.air.util.torch_dist import init_torch_dist_process_group\n    from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\nlogger = init_logger(__name__)\n\n_LOGGING_INTERVAL_SEC = 5\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The `LLM` class wraps this class for offline batched inference and the\n    `AsyncLLMEngine` class wraps this class for online serving.\n\n    NOTE: The config arguments are derived from the `EngineArgs` class. For the\n    comprehensive list of arguments, see `EngineArgs`.\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        distributed_init_method: The initialization method for distributed\n            execution. See `torch.distributed.init_process_group` for details.\n        placement_group: Ray placement group for distributed execution.\n            Required for distributed execution.\n        log_stats: Whether to log statistics.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        cache_config: CacheConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        distributed_init_method: str,\n        placement_group: Optional[\"PlacementGroup\"],\n        log_stats: bool,\n    ) -> None:\n        logger.info(\n            \"Initializing an LLM engine with config: \"\n            f\"model={model_config.model!r}, \"\n            f\"tokenizer={model_config.tokenizer!r}, \"\n            f\"tokenizer_mode={model_config.tokenizer_mode}, \"\n            f\"revision={model_config.revision}, \"\n            f\"tokenizer_revision={model_config.tokenizer_revision}, \"\n            f\"trust_remote_code={model_config.trust_remote_code}, \"\n            f\"dtype={model_config.dtype}, \"\n            f\"max_seq_len={model_config.max_model_len}, \"\n            f\"download_dir={model_config.download_dir!r}, \"\n            f\"load_format={model_config.load_format}, \"\n            f\"tensor_parallel_size={parallel_config.tensor_parallel_size}, \"\n            f\"quantization={model_config.quantization}, \"\n            f\"enforce_eager={model_config.enforce_eager}, \"\n            f\"seed={model_config.seed})\")\n        # TODO(woosuk): Print more configs in debug mode.\n\n        self.model_config = model_config\n        self.cache_config = cache_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.log_stats = log_stats\n        self._verify_args()\n\n        self.tokenizer = get_tokenizer(\n            model_config.tokenizer,\n            tokenizer_mode=model_config.tokenizer_mode,\n            trust_remote_code=model_config.trust_remote_code,\n            tokenizer_revision=model_config.tokenizer_revision,\n            revision=model_config.revision)\n        self.seq_counter = Counter()\n\n        # Create the parallel GPU workers.\n        if self.parallel_config.worker_use_ray:\n            # Disable Ray usage stats collection.\n            ray_usage = os.environ.get(\"RAY_USAGE_STATS_ENABLED\", \"0\")\n            if ray_usage != \"1\":\n                os.environ[\"RAY_USAGE_STATS_ENABLED\"] = \"0\"\n            self._init_workers_ray(placement_group)\n        else:\n            self._init_workers(distributed_init_method)\n\n        # Profile the memory usage and initialize the cache.\n        self._init_cache()\n\n        # Create the scheduler.\n        self.scheduler = Scheduler(scheduler_config, cache_config)\n\n        # Logging.\n        self.last_logging_time = 0.0\n        # List of (timestamp, num_tokens)\n        self.num_prompt_tokens: List[Tuple[float, int]] = []\n        # List of (timestamp, num_tokens)\n        self.num_generation_tokens: List[Tuple[float, int]] = []\n\n    def _init_workers(self, distributed_init_method: str):\n        # Lazy import the Worker to avoid importing torch.cuda/xformers\n        # before CUDA_VISIBLE_DEVICES is set in the Worker\n        from vllm.worker.worker import Worker\n\n        assert self.parallel_config.world_size == 1, (\n            \"Ray is required if parallel_config.world_size > 1.\")\n\n        self.workers: List[Worker] = []\n        worker = Worker(\n            self.model_config,\n            self.parallel_config,\n            self.scheduler_config,\n            0,\n            distributed_init_method,\n        )\n        self.workers.append(worker)\n        self._run_workers(\n            \"init_model\",\n            get_all_outputs=True,\n        )\n        self._run_workers(\n            \"load_model\",\n            get_all_outputs=True,\n            max_concurrent_workers=self.parallel_config.\n            max_parallel_loading_workers,\n        )\n\n    def _init_workers_ray(self, placement_group: \"PlacementGroup\",\n                          **ray_remote_kwargs):\n        # Lazy import the Worker to avoid importing torch.cuda/xformers\n        # before CUDA_VISIBLE_DEVICES is set in the Worker\n        from vllm.worker.worker import Worker\n\n        self.workers: List[Worker] = []\n        for bundle in placement_group.bundle_specs:\n            if not bundle.get(\"GPU\", 0):\n                continue\n            if self.parallel_config.tensor_parallel_size == 1:\n                num_gpus = self.cache_config.gpu_memory_utilization\n            else:\n                num_gpus = 1\n            worker = ray.remote(\n                num_cpus=0,\n                num_gpus=num_gpus,\n                scheduling_strategy=PlacementGroupSchedulingStrategy(\n                    placement_group=placement_group,\n                    placement_group_capture_child_tasks=True),\n                **ray_remote_kwargs,\n            )(RayWorkerVllm).remote(self.model_config.trust_remote_code)\n            self.workers.append(worker)\n\n        # Initialize torch distributed process group for the workers.\n        init_torch_dist_process_group(self.workers, backend=\"nccl\")\n        model_config = copy.deepcopy(self.model_config)\n        parallel_config = copy.deepcopy(self.parallel_config)\n        scheduler_config = copy.deepcopy(self.scheduler_config)\n        self._run_workers(\"init_worker\",\n                          get_all_outputs=True,\n                          worker_init_fn=lambda: Worker(\n                              model_config,\n                              parallel_config,\n                              scheduler_config,\n                              None,\n                              None,\n                          ))\n        self._run_workers(\n            \"init_model\",\n            get_all_outputs=True,\n        )\n        self._run_workers(\n            \"load_model\",\n            get_all_outputs=True,\n            max_concurrent_workers=self.parallel_config.\n            max_parallel_loading_workers,\n        )\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n    def _init_cache(self) -> None:\n        \"\"\"Profiles the memory usage and initializes the KV cache.\"\"\"\n        # Get the maximum number of blocks that can be allocated on GPU and CPU.\n        num_blocks = self._run_workers(\n            \"profile_num_available_blocks\",\n            get_all_outputs=True,\n            block_size=self.cache_config.block_size,\n            gpu_memory_utilization=self.cache_config.gpu_memory_utilization,\n            cpu_swap_space=self.cache_config.swap_space_bytes,\n        )\n\n        # Since we use a shared centralized controller, we take the minimum\n        # number of blocks across all workers to make sure all the memory\n        # operators can be applied to all workers.\n        num_gpu_blocks = min(b[0] for b in num_blocks)\n        num_cpu_blocks = min(b[1] for b in num_blocks)\n        # FIXME(woosuk): Change to debug log.\n        logger.info(f\"# GPU blocks: {num_gpu_blocks}, \"\n                    f\"# CPU blocks: {num_cpu_blocks}\")\n\n        if num_gpu_blocks <= 0:\n            raise ValueError(\"No available memory for the cache blocks. \"\n                             \"Try increasing `gpu_memory_utilization` when \"\n                             \"initializing the engine.\")\n        max_seq_len = self.cache_config.block_size * num_gpu_blocks\n        if self.model_config.max_model_len > max_seq_len:\n            raise ValueError(\n                f\"The model's max seq len ({self.model_config.max_model_len}) \"\n                \"is larger than the maximum number of tokens that can be \"\n                f\"stored in KV cache ({max_seq_len}). Try increasing \"\n                \"`gpu_memory_utilization` or decreasing `max_model_len` when \"\n                \"initializing the engine.\")\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        # Initialize the cache.\n        self._run_workers(\"init_cache_engine\", cache_config=self.cache_config)\n        # Warm up the model. This includes capturing the model into CUDA graph\n        # if enforce_eager is False.\n        self._run_workers(\"warm_up_model\")\n\n    @classmethod\n    def from_engine_args(cls, engine_args: EngineArgs) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        engine_configs = engine_args.create_engine_configs()\n        parallel_config = engine_configs[2]\n        # Initialize the cluster.\n        distributed_init_method, placement_group = initialize_cluster(\n            parallel_config)\n        # Create the LLM engine.\n        engine = cls(*engine_configs,\n                     distributed_init_method,\n                     placement_group,\n                     log_stats=not engine_args.disable_log_stats)\n        return engine\n\n    def add_request(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        sampling_params: SamplingParams,\n        prompt_token_ids: Optional[List[int]] = None,\n        arrival_time: Optional[float] = None,\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            prompt: The prompt string. Can be None if prompt_token_ids is\n                provided.\n            sampling_params: The sampling parameters for text generation.\n            prompt_token_ids: The token IDs of the prompt. If None, we\n                use the tokenizer to convert the prompts to token IDs.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n        \"\"\"\n        if arrival_time is None:\n            arrival_time = time.monotonic()\n        if prompt_token_ids is None:\n            assert prompt is not None\n            prompt_token_ids = self.tokenizer.encode(prompt)\n\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        seq = Sequence(seq_id, prompt, prompt_token_ids, block_size)\n\n        # Create the sequence group.\n        seq_group = SequenceGroup(request_id, [seq], sampling_params,\n                                  arrival_time)\n\n        # Add the sequence group to the scheduler.\n        self.scheduler.add_seq_group(seq_group)\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n        \"\"\"\n        self.scheduler.abort_seq_group(request_id)\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return self.scheduler.get_num_unfinished_seq_groups()\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return self.scheduler.has_unfinished_seqs()\n\n    def _check_beam_search_early_stopping(\n        self,\n        early_stopping: Union[bool, str],\n        sampling_params: SamplingParams,\n        best_running_seq: Sequence,\n        current_worst_seq: Sequence,\n    ) -> bool:\n        assert sampling_params.use_beam_search\n        length_penalty = sampling_params.length_penalty\n        if early_stopping is True:\n            return True\n\n        current_worst_score = (current_worst_seq.get_beam_search_score(\n            length_penalty=length_penalty,\n            eos_token_id=self.tokenizer.eos_token_id))\n        if early_stopping is False:\n            highest_attainable_score = (best_running_seq.get_beam_search_score(\n                length_penalty=length_penalty,\n                eos_token_id=self.tokenizer.eos_token_id))\n        else:\n            assert early_stopping == \"never\"\n            if length_penalty > 0.0:\n                # If length_penalty > 0.0, beam search will prefer longer\n                # sequences. The highest attainable score calculation is\n                # based on the longest possible sequence length in this case.\n                max_possible_length = max(\n                    best_running_seq.get_prompt_len() +\n                    sampling_params.max_tokens,\n                    self.scheduler_config.max_model_len)\n                highest_attainable_score = (\n                    best_running_seq.get_beam_search_score(\n                        length_penalty=length_penalty,\n                        eos_token_id=self.tokenizer.eos_token_id,\n                        seq_len=max_possible_length))\n            else:\n                # Otherwise, beam search will prefer shorter sequences. The\n                # highest attainable score calculation is based on the current\n                # sequence length.\n                highest_attainable_score = (\n                    best_running_seq.get_beam_search_score(\n                        length_penalty=length_penalty,\n                        eos_token_id=self.tokenizer.eos_token_id))\n        return current_worst_score >= highest_attainable_score\n\n    def _process_sequence_group_outputs(self, seq_group: SequenceGroup,\n                                        outputs: SequenceGroupOutput) -> None:\n        # Process prompt logprobs\n        prompt_logprobs = outputs.prompt_logprobs\n        if prompt_logprobs is not None:\n            seq_group.prompt_logprobs = prompt_logprobs\n\n        # Process samples\n        samples = outputs.samples\n        parent_seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        existing_finished_seqs = seq_group.get_finished_seqs()\n        parent_child_dict = {\n            parent_seq.seq_id: []\n            for parent_seq in parent_seqs\n        }\n        for sample in samples:\n            parent_child_dict[sample.parent_seq_id].append(sample)\n        # List of (child, parent)\n        child_seqs: List[Tuple[Sequence, Sequence]] = []\n\n        # Process the child samples for each parent sequence\n        for parent in parent_seqs:\n            child_samples: List[SequenceOutput] = parent_child_dict[\n                parent.seq_id]\n            if len(child_samples) == 0:\n                # This parent sequence has no children samples. Remove\n                # the parent sequence from the sequence group since it will\n                # not be used in the future iterations.\n                parent.status = SequenceStatus.FINISHED_ABORTED\n                seq_group.remove(parent.seq_id)\n                self.scheduler.free_seq(parent)\n                continue\n            # Fork the parent sequence if there are multiple child samples.\n            for child_sample in child_samples[:-1]:\n                new_child_seq_id = next(self.seq_counter)\n                child = parent.fork(new_child_seq_id)\n                child.append_token_id(child_sample.output_token,\n                                      child_sample.logprobs)\n                child_seqs.append((child, parent))\n            # Continue the parent sequence for the last child sample.\n            # We reuse the parent sequence here to reduce redundant memory\n            # copies, especially when using non-beam search sampling methods.\n            last_child_sample = child_samples[-1]\n            parent.append_token_id(last_child_sample.output_token,\n                                   last_child_sample.logprobs)\n            child_seqs.append((parent, parent))\n\n        for seq, _ in child_seqs:\n            self._decode_sequence(seq, seq_group.sampling_params)\n            self._check_stop(seq, seq_group.sampling_params)\n\n        # Non-beam search case\n        if not seq_group.sampling_params.use_beam_search:\n            # For newly created child sequences, add them to the sequence group\n            # and fork them in block manager if they are not finished.\n            for seq, parent in child_seqs:\n                if seq is not parent:\n                    seq_group.add(seq)\n                    if not seq.is_finished():\n                        self.scheduler.fork_seq(parent, seq)\n\n            # Free the finished and selected parent sequences' memory in block\n            # manager. Keep them in the sequence group as candidate output.\n            # NOTE: we need to fork the new sequences before freeing the\n            # old sequences.\n            for seq, parent in child_seqs:\n                if seq is parent and seq.is_finished():\n                    self.scheduler.free_seq(seq)\n            return\n\n        # Beam search case\n        # Select the child sequences to keep in the sequence group.\n        selected_child_seqs = []\n        unselected_child_seqs = []\n        beam_width = seq_group.sampling_params.best_of\n        length_penalty = seq_group.sampling_params.length_penalty\n\n        # Select the newly finished sequences with the highest scores\n        # to replace existing finished sequences.\n        # Tuple of (seq, parent, is_new)\n        existing_finished_seqs = [(seq, None, False)\n                                  for seq in existing_finished_seqs]\n        new_finished_seqs = [(seq, parent, True) for seq, parent in child_seqs\n                             if seq.is_finished()]\n        all_finished_seqs = existing_finished_seqs + new_finished_seqs\n        # Sort the finished sequences by their scores.\n        all_finished_seqs.sort(key=lambda x: x[0].get_beam_search_score(\n            length_penalty=length_penalty,\n            eos_token_id=self.tokenizer.eos_token_id),\n                               reverse=True)\n        for seq, parent, is_new in all_finished_seqs[:beam_width]:\n            if is_new:\n                # A newly generated child sequence finishes and has a high\n                # score, so we will add it into the sequence group.\n                selected_child_seqs.append((seq, parent))\n        for seq, parent, is_new in all_finished_seqs[beam_width:]:\n            if is_new:\n                # A newly generated child sequence finishes but has a low\n                # score, so we will not add it into the sequence group.\n                # Additionally, if this sequence is a continuation of a\n                # parent sequence, we will need remove the parent sequence\n                # from the sequence group.\n                unselected_child_seqs.append((seq, parent))\n            else:\n                # An existing finished sequence has a low score, so we will\n                # remove it from the sequence group.\n                seq_group.remove(seq.seq_id)\n\n        # select the top beam_width sequences from the running\n        # sequences for the next iteration to continue the beam\n        # search.\n        running_child_seqs = [(seq, parent) for seq, parent in child_seqs\n                              if not seq.is_finished()]\n        # Sort the running sequences by their scores.\n        running_child_seqs.sort(key=lambda x: x[0].get_beam_search_score(\n            length_penalty=length_penalty,\n            eos_token_id=self.tokenizer.eos_token_id),\n                                reverse=True)\n\n        # Check if we can stop the beam search.\n        if len(running_child_seqs) == 0:\n            # No running sequences, stop the beam search.\n            stop_beam_search = True\n        elif len(all_finished_seqs) < beam_width:\n            # Not enough finished sequences, continue the beam search.\n            stop_beam_search = False\n        else:\n            # Check the early stopping criteria\n            best_running_seq = running_child_seqs[0][0]\n            current_worst_seq = all_finished_seqs[beam_width - 1][0]\n            stop_beam_search = self._check_beam_search_early_stopping(\n                seq_group.sampling_params.early_stopping,\n                seq_group.sampling_params, best_running_seq, current_worst_seq)\n\n        if stop_beam_search:\n            # Stop the beam search and remove all the running sequences from\n            # the sequence group.\n            unselected_child_seqs.extend(running_child_seqs)\n        else:\n            # Continue the beam search and select the top beam_width sequences\n            # to continue the beam search.\n            selected_child_seqs.extend(running_child_seqs[:beam_width])\n            # The remaining running sequences will not be used in the next\n            # iteration. Again, if these sequences are continuations of\n            # parent sequences, we will need to remove the parent sequences\n            # from the sequence group.\n            unselected_child_seqs.extend(running_child_seqs[beam_width:])\n\n        # For newly created child sequences, add them to the sequence group\n        # and fork them in block manager if they are not finished.\n        for seq, parent in selected_child_seqs:\n            if seq is not parent:\n                seq_group.add(seq)\n                if not seq.is_finished():\n                    self.scheduler.fork_seq(parent, seq)\n\n        # Free the finished and selected parent sequences' memory in block\n        # manager. Keep them in the sequence group as candidate output.\n        for seq, parent in selected_child_seqs:\n            if seq is parent and seq.is_finished():\n                self.scheduler.free_seq(seq)\n\n        # Remove the unselected parent sequences from the sequence group and\n        # free their memory in block manager.\n        for seq, parent in unselected_child_seqs:\n            if seq is parent:\n                # Remove the parent sequence if it is not selected for next\n                # iteration\n                seq_group.remove(seq.seq_id)\n                self.scheduler.free_seq(seq)\n\n    def _process_model_outputs(\n            self, output: SamplerOutput,\n            scheduler_outputs: SchedulerOutputs) -> List[RequestOutput]:\n        # Update the scheduled sequence groups with the model outputs.\n        scheduled_seq_groups = scheduler_outputs.scheduled_seq_groups\n        for seq_group, outputs in zip(scheduled_seq_groups, output):\n            self._process_sequence_group_outputs(seq_group, outputs)\n\n        # Free the finished sequence groups.\n        self.scheduler.free_finished_seq_groups()\n\n        # Create the outputs.\n        request_outputs: List[RequestOutput] = []\n        for seq_group in (scheduled_seq_groups +\n                          scheduler_outputs.ignored_seq_groups):\n            request_output = RequestOutput.from_seq_group(seq_group)\n            request_outputs.append(request_output)\n\n        if self.log_stats:\n            # Log the system stats.\n            self._log_system_stats(scheduler_outputs.prompt_run,\n                                   scheduler_outputs.num_batched_tokens)\n        return request_outputs\n\n    def step(self) -> List[RequestOutput]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        This function performs one decoding iteration of the engine. It first\n        schedules the sequences to be executed in the next iteration and the\n        token blocks to be swapped in/out/copy. Then, it executes the model\n        and updates the scheduler with the model outputs. Finally, it decodes\n        the sequences and returns the newly generated results.\n        \"\"\"\n        seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n\n        # Execute the model.\n        output = self._run_workers(\n            \"execute_model\",\n            seq_group_metadata_list=seq_group_metadata_list,\n            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n            blocks_to_copy=scheduler_outputs.blocks_to_copy,\n        ) if not scheduler_outputs.is_empty() else []\n\n        return self._process_model_outputs(output, scheduler_outputs)\n\n    def _log_system_stats(\n        self,\n        prompt_run: bool,\n        num_batched_tokens: int,\n    ) -> None:\n        now = time.monotonic()\n        # Log the number of batched input tokens.\n        if prompt_run:\n            self.num_prompt_tokens.append((now, num_batched_tokens))\n        else:\n            self.num_generation_tokens.append((now, num_batched_tokens))\n\n        should_log = now - self.last_logging_time >= _LOGGING_INTERVAL_SEC\n        if not should_log:\n            return\n\n        # Discard the old stats.\n        self.num_prompt_tokens = [(t, n) for t, n in self.num_prompt_tokens\n                                  if now - t < _LOGGING_INTERVAL_SEC]\n        self.num_generation_tokens = [(t, n)\n                                      for t, n in self.num_generation_tokens\n                                      if now - t < _LOGGING_INTERVAL_SEC]\n\n        if len(self.num_prompt_tokens) > 1:\n            total_num_tokens = sum(n for _, n in self.num_prompt_tokens[:-1])\n            window = now - self.num_prompt_tokens[0][0]\n            avg_prompt_throughput = total_num_tokens / window\n        else:\n            avg_prompt_throughput = 0.0\n        if len(self.num_generation_tokens) > 1:\n            total_num_tokens = sum(n\n                                   for _, n in self.num_generation_tokens[:-1])\n            window = now - self.num_generation_tokens[0][0]\n            avg_generation_throughput = total_num_tokens / window\n        else:\n            avg_generation_throughput = 0.0\n\n        total_num_gpu_blocks = self.cache_config.num_gpu_blocks\n        num_free_gpu_blocks = (\n            self.scheduler.block_manager.get_num_free_gpu_blocks())\n        num_used_gpu_blocks = total_num_gpu_blocks - num_free_gpu_blocks\n        gpu_cache_usage = num_used_gpu_blocks / total_num_gpu_blocks\n\n        total_num_cpu_blocks = self.cache_config.num_cpu_blocks\n        if total_num_cpu_blocks > 0:\n            num_free_cpu_blocks = (\n                self.scheduler.block_manager.get_num_free_cpu_blocks())\n            num_used_cpu_blocks = total_num_cpu_blocks - num_free_cpu_blocks\n            cpu_cache_usage = num_used_cpu_blocks / total_num_cpu_blocks\n        else:\n            cpu_cache_usage = 0.0\n\n        record_metrics(\n            avg_prompt_throughput=avg_prompt_throughput,\n            avg_generation_throughput=avg_generation_throughput,\n            scheduler_running=len(self.scheduler.running),\n            scheduler_swapped=len(self.scheduler.swapped),\n            scheduler_waiting=len(self.scheduler.waiting),\n            gpu_cache_usage=gpu_cache_usage,\n            cpu_cache_usage=cpu_cache_usage,\n        )\n\n        logger.info(\"Avg prompt throughput: \"\n                    f\"{avg_prompt_throughput:.1f} tokens/s, \"\n                    \"Avg generation throughput: \"\n                    f\"{avg_generation_throughput:.1f} tokens/s, \"\n                    f\"Running: {len(self.scheduler.running)} reqs, \"\n                    f\"Swapped: {len(self.scheduler.swapped)} reqs, \"\n                    f\"Pending: {len(self.scheduler.waiting)} reqs, \"\n                    f\"GPU KV cache usage: {gpu_cache_usage * 100:.1f}%, \"\n                    f\"CPU KV cache usage: {cpu_cache_usage * 100:.1f}%\")\n        self.last_logging_time = now\n\n    def _decode_sequence(self, seq: Sequence, prms: SamplingParams) -> None:\n        \"\"\"Decodes the new token for a sequence.\"\"\"\n        (new_tokens, new_output_text, prefix_offset,\n         read_offset) = detokenize_incrementally(\n             self.tokenizer,\n             all_input_ids=seq.get_token_ids(),\n             prev_tokens=seq.tokens,\n             prefix_offset=seq.prefix_offset,\n             read_offset=seq.read_offset,\n             skip_special_tokens=prms.skip_special_tokens,\n             spaces_between_special_tokens=prms.spaces_between_special_tokens,\n         )\n        if seq.tokens is None:\n            seq.tokens = new_tokens\n        else:\n            seq.tokens.extend(new_tokens)\n        seq.prefix_offset = prefix_offset\n        seq.read_offset = read_offset\n        seq.output_text += new_output_text\n\n    def _check_stop(self, seq: Sequence,\n                    sampling_params: SamplingParams) -> None:\n        \"\"\"Stop the finished sequences.\"\"\"\n        for stop_str in sampling_params.stop:\n            if seq.output_text.endswith(stop_str):\n                if not sampling_params.include_stop_str_in_output:\n                    # Truncate the output text so that the stop string is\n                    # not included in the output.\n                    seq.output_text = seq.output_text[:-len(stop_str)]\n                seq.status = SequenceStatus.FINISHED_STOPPED\n                return\n        if seq.get_last_token_id() in sampling_params.stop_token_ids:\n            seq.status = SequenceStatus.FINISHED_STOPPED\n            return\n\n        # Check if the sequence has reached max_model_len.\n        if seq.get_len() > self.scheduler_config.max_model_len:\n            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED\n            return\n\n        # Check if the sequence has reached max_tokens.\n        if seq.get_output_len() == sampling_params.max_tokens:\n            seq.status = SequenceStatus.FINISHED_LENGTH_CAPPED\n            return\n\n        # Check if the sequence has generated the EOS token.\n        if ((not sampling_params.ignore_eos)\n                and seq.get_last_token_id() == self.tokenizer.eos_token_id):\n            seq.status = SequenceStatus.FINISHED_STOPPED\n            return\n\n    def _run_workers_in_batch(\n        self,\n        workers,\n        method: str,\n        *args,\n        **kwargs,\n    ):\n        all_outputs = []\n        for worker in workers:\n            if self.parallel_config.worker_use_ray:\n                executor = partial(worker.execute_method.remote, method)\n            else:\n                executor = getattr(worker, method)\n\n            output = executor(*args, **kwargs)\n            all_outputs.append(output)\n        if self.parallel_config.worker_use_ray:\n            all_outputs = ray.get(all_outputs)\n        return all_outputs\n\n    def _run_workers(\n        self,\n        method: str,\n        *args,\n        get_all_outputs: bool = False,\n        max_concurrent_workers: Optional[int] = None,\n        **kwargs,\n    ) -> Any:\n        \"\"\"Runs the given method on all workers.\"\"\"\n        all_outputs = []\n        if max_concurrent_workers:\n            work_groups = [\n                self.workers[i:i + max_concurrent_workers]\n                for i in range(0, len(self.workers), max_concurrent_workers)\n            ]\n        else:\n            work_groups = [self.workers]\n\n        for workers in work_groups:\n            all_outputs.extend(\n                self._run_workers_in_batch(workers, method, *args, **kwargs))\n\n        if get_all_outputs:\n            return all_outputs\n\n        # Make sure all workers have the same results.\n        output = all_outputs[0]\n        for other_output in all_outputs[1:]:\n            assert output == other_output\n        return output\n",
      "diff": "diff --git a/vllm/engine/llm_engine.py b/vllm/engine/llm_engine.py\nindex 43bf9747e..0e36a50a5 100644\n--- a/vllm/engine/llm_engine.py\n+++ b/vllm/engine/llm_engine.py\n@@ -1,8 +1,9 @@\n import copy\n+from collections import defaultdict\n import os\n import time\n-from functools import partial\n-from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union\n+from typing import (TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Tuple,\n+                    Union)\n \n from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                          SchedulerConfig)\n@@ -17,10 +18,9 @@ from vllm.sequence import (SamplerOutput, Sequence, SequenceGroup,\n                            SequenceGroupOutput, SequenceOutput, SequenceStatus)\n from vllm.transformers_utils.tokenizer import (detokenize_incrementally,\n                                                get_tokenizer)\n-from vllm.utils import Counter\n+from vllm.utils import Counter, set_cuda_visible_devices, get_ip, get_open_port\n \n if ray:\n-    from ray.air.util.torch_dist import init_torch_dist_process_group\n     from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n \n if TYPE_CHECKING:\n@@ -53,8 +53,6 @@ class LLMEngine:\n             management.\n         parallel_config: The configuration related to distributed execution.\n         scheduler_config: The configuration related to the request scheduler.\n-        distributed_init_method: The initialization method for distributed\n-            execution. See `torch.distributed.init_process_group` for details.\n         placement_group: Ray placement group for distributed execution.\n             Required for distributed execution.\n         log_stats: Whether to log statistics.\n@@ -66,7 +64,6 @@ class LLMEngine:\n         cache_config: CacheConfig,\n         parallel_config: ParallelConfig,\n         scheduler_config: SchedulerConfig,\n-        distributed_init_method: str,\n         placement_group: Optional[\"PlacementGroup\"],\n         log_stats: bool,\n     ) -> None:\n@@ -111,7 +108,7 @@ class LLMEngine:\n                 os.environ[\"RAY_USAGE_STATS_ENABLED\"] = \"0\"\n             self._init_workers_ray(placement_group)\n         else:\n-            self._init_workers(distributed_init_method)\n+            self._init_workers()\n \n         # Profile the memory usage and initialize the cache.\n         self._init_cache()\n@@ -126,7 +123,7 @@ class LLMEngine:\n         # List of (timestamp, num_tokens)\n         self.num_generation_tokens: List[Tuple[float, int]] = []\n \n-    def _init_workers(self, distributed_init_method: str):\n+    def _init_workers(self):\n         # Lazy import the Worker to avoid importing torch.cuda/xformers\n         # before CUDA_VISIBLE_DEVICES is set in the Worker\n         from vllm.worker.worker import Worker\n@@ -135,70 +132,122 @@ class LLMEngine:\n             \"Ray is required if parallel_config.world_size > 1.\")\n \n         self.workers: List[Worker] = []\n-        worker = Worker(\n+        distributed_init_method = f\"tcp://{get_ip()}:{get_open_port()}\"\n+        self.driver_worker = Worker(\n             self.model_config,\n             self.parallel_config,\n             self.scheduler_config,\n-            0,\n-            distributed_init_method,\n-        )\n-        self.workers.append(worker)\n-        self._run_workers(\n-            \"init_model\",\n-            get_all_outputs=True,\n-        )\n-        self._run_workers(\n-            \"load_model\",\n-            get_all_outputs=True,\n-            max_concurrent_workers=self.parallel_config.\n-            max_parallel_loading_workers,\n+            local_rank=0,\n+            rank=0,\n+            distributed_init_method=distributed_init_method,\n+            is_driver_worker=True,\n         )\n+        self._run_workers(\"init_model\")\n+        self._run_workers(\"load_model\")\n \n     def _init_workers_ray(self, placement_group: \"PlacementGroup\",\n                           **ray_remote_kwargs):\n-        # Lazy import the Worker to avoid importing torch.cuda/xformers\n-        # before CUDA_VISIBLE_DEVICES is set in the Worker\n-        from vllm.worker.worker import Worker\n+        if self.parallel_config.tensor_parallel_size == 1:\n+            num_gpus = self.cache_config.gpu_memory_utilization\n+        else:\n+            num_gpus = 1\n \n-        self.workers: List[Worker] = []\n-        for bundle in placement_group.bundle_specs:\n+        self.driver_dummy_worker: RayWorkerVllm = None\n+        self.workers: List[RayWorkerVllm] = []\n+\n+        driver_ip = get_ip()\n+        for bundle_id, bundle in enumerate(placement_group.bundle_specs):\n             if not bundle.get(\"GPU\", 0):\n                 continue\n-            if self.parallel_config.tensor_parallel_size == 1:\n-                num_gpus = self.cache_config.gpu_memory_utilization\n-            else:\n-                num_gpus = 1\n+            scheduling_strategy = PlacementGroupSchedulingStrategy(\n+                placement_group=placement_group,\n+                placement_group_capture_child_tasks=True,\n+                placement_group_bundle_index=bundle_id,\n+            )\n             worker = ray.remote(\n                 num_cpus=0,\n                 num_gpus=num_gpus,\n-                scheduling_strategy=PlacementGroupSchedulingStrategy(\n-                    placement_group=placement_group,\n-                    placement_group_capture_child_tasks=True),\n+                scheduling_strategy=scheduling_strategy,\n                 **ray_remote_kwargs,\n             )(RayWorkerVllm).remote(self.model_config.trust_remote_code)\n-            self.workers.append(worker)\n+\n+            worker_ip = ray.get(worker.get_node_ip.remote())\n+            if worker_ip == driver_ip and self.driver_dummy_worker is None:\n+                # If the worker is on the same node as the driver, we use it\n+                # as the resource holder for the driver process.\n+                self.driver_dummy_worker = worker\n+            else:\n+                self.workers.append(worker)\n+\n+        if self.driver_dummy_worker is None:\n+            raise ValueError(\n+                \"Ray does not allocate any GPUs on the driver node. Consider \"\n+                \"adjusting the Ray placement group or running the driver on a \"\n+                \"GPU node.\")\n+\n+        driver_node_id, driver_gpu_ids = ray.get(\n+            self.driver_dummy_worker.get_node_and_gpu_ids.remote())\n+        worker_node_and_gpu_ids = ray.get(\n+            [worker.get_node_and_gpu_ids.remote() for worker in self.workers])\n+\n+        node_workers = defaultdict(list)\n+        node_gpus = defaultdict(list)\n+\n+        node_workers[driver_node_id].append(0)\n+        node_gpus[driver_node_id].extend(driver_gpu_ids)\n+        for i, (node_id, gpu_ids) in enumerate(worker_node_and_gpu_ids,\n+                                               start=1):\n+            node_workers[node_id].append(i)\n+            node_gpus[node_id].extend(gpu_ids)\n+        for node_id, gpu_ids in node_gpus.items():\n+            node_gpus[node_id] = sorted(gpu_ids)\n+\n+        # Set CUDA_VISIBLE_DEVICES for the driver.\n+        set_cuda_visible_devices(node_gpus[driver_node_id])\n+        for worker, (node_id, _) in zip(self.workers, worker_node_and_gpu_ids):\n+            worker.set_cuda_visible_devices.remote(node_gpus[node_id])\n+\n+        distributed_init_method = f\"tcp://{driver_ip}:{get_open_port()}\"\n+\n+        # Lazy import the Worker to avoid importing torch.cuda/xformers\n+        # before CUDA_VISIBLE_DEVICES is set in the Worker\n+        from vllm.worker.worker import Worker\n \n         # Initialize torch distributed process group for the workers.\n-        init_torch_dist_process_group(self.workers, backend=\"nccl\")\n         model_config = copy.deepcopy(self.model_config)\n         parallel_config = copy.deepcopy(self.parallel_config)\n         scheduler_config = copy.deepcopy(self.scheduler_config)\n-        self._run_workers(\"init_worker\",\n-                          get_all_outputs=True,\n-                          worker_init_fn=lambda: Worker(\n-                              model_config,\n-                              parallel_config,\n-                              scheduler_config,\n-                              None,\n-                              None,\n-                          ))\n-        self._run_workers(\n-            \"init_model\",\n-            get_all_outputs=True,\n+\n+        for rank, (worker, (node_id,\n+                            _)) in enumerate(zip(self.workers,\n+                                                 worker_node_and_gpu_ids),\n+                                             start=1):\n+            local_rank = node_workers[node_id].index(rank)\n+            worker.init_worker.remote(\n+                lambda rank=rank, local_rank=local_rank: Worker(\n+                    model_config,\n+                    parallel_config,\n+                    scheduler_config,\n+                    local_rank,\n+                    rank,\n+                    distributed_init_method,\n+                ))\n+\n+        driver_rank = 0\n+        driver_local_rank = node_workers[driver_node_id].index(driver_rank)\n+        self.driver_worker = Worker(\n+            model_config,\n+            parallel_config,\n+            scheduler_config,\n+            driver_local_rank,\n+            driver_rank,\n+            distributed_init_method,\n+            is_driver_worker=True,\n         )\n+\n+        self._run_workers(\"init_model\")\n         self._run_workers(\n             \"load_model\",\n-            get_all_outputs=True,\n             max_concurrent_workers=self.parallel_config.\n             max_parallel_loading_workers,\n         )\n@@ -212,7 +261,6 @@ class LLMEngine:\n         # Get the maximum number of blocks that can be allocated on GPU and CPU.\n         num_blocks = self._run_workers(\n             \"profile_num_available_blocks\",\n-            get_all_outputs=True,\n             block_size=self.cache_config.block_size,\n             gpu_memory_utilization=self.cache_config.gpu_memory_utilization,\n             cpu_swap_space=self.cache_config.swap_space_bytes,\n@@ -256,11 +304,9 @@ class LLMEngine:\n         engine_configs = engine_args.create_engine_configs()\n         parallel_config = engine_configs[2]\n         # Initialize the cluster.\n-        distributed_init_method, placement_group = initialize_cluster(\n-            parallel_config)\n+        placement_group = initialize_cluster(parallel_config)\n         # Create the LLM engine.\n         engine = cls(*engine_configs,\n-                     distributed_init_method,\n                      placement_group,\n                      log_stats=not engine_args.disable_log_stats)\n         return engine\n@@ -577,14 +623,21 @@ class LLMEngine:\n         \"\"\"\n         seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\n \n-        # Execute the model.\n-        output = self._run_workers(\n-            \"execute_model\",\n-            seq_group_metadata_list=seq_group_metadata_list,\n-            blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n-            blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n-            blocks_to_copy=scheduler_outputs.blocks_to_copy,\n-        ) if not scheduler_outputs.is_empty() else []\n+        if not scheduler_outputs.is_empty():\n+            # Execute the model.\n+            all_outputs = self._run_workers(\n+                \"execute_model\",\n+                driver_kwargs={\n+                    \"seq_group_metadata_list\": seq_group_metadata_list,\n+                    \"blocks_to_swap_in\": scheduler_outputs.blocks_to_swap_in,\n+                    \"blocks_to_swap_out\": scheduler_outputs.blocks_to_swap_out,\n+                    \"blocks_to_copy\": scheduler_outputs.blocks_to_copy,\n+                })\n+\n+            # Only the driver worker returns the sampling results.\n+            output = all_outputs[0]\n+        else:\n+            output = []\n \n         return self._process_model_outputs(output, scheduler_outputs)\n \n@@ -712,53 +765,38 @@ class LLMEngine:\n             seq.status = SequenceStatus.FINISHED_STOPPED\n             return\n \n-    def _run_workers_in_batch(\n-        self,\n-        workers,\n-        method: str,\n-        *args,\n-        **kwargs,\n-    ):\n-        all_outputs = []\n-        for worker in workers:\n-            if self.parallel_config.worker_use_ray:\n-                executor = partial(worker.execute_method.remote, method)\n-            else:\n-                executor = getattr(worker, method)\n-\n-            output = executor(*args, **kwargs)\n-            all_outputs.append(output)\n-        if self.parallel_config.worker_use_ray:\n-            all_outputs = ray.get(all_outputs)\n-        return all_outputs\n-\n     def _run_workers(\n         self,\n         method: str,\n         *args,\n-        get_all_outputs: bool = False,\n+        driver_args: Optional[List[Any]] = None,\n+        driver_kwargs: Optional[Dict[str, Any]] = None,\n         max_concurrent_workers: Optional[int] = None,\n         **kwargs,\n     ) -> Any:\n         \"\"\"Runs the given method on all workers.\"\"\"\n-        all_outputs = []\n+\n         if max_concurrent_workers:\n-            work_groups = [\n-                self.workers[i:i + max_concurrent_workers]\n-                for i in range(0, len(self.workers), max_concurrent_workers)\n-            ]\n-        else:\n-            work_groups = [self.workers]\n+            raise NotImplementedError(\n+                \"max_concurrent_workers is not supported yet.\")\n+\n+        # Start the ray workers first.\n+        ray_worker_outputs = [\n+            worker.execute_method.remote(method, *args, **kwargs)\n+            for worker in self.workers\n+        ]\n+\n+        if driver_args is None:\n+            driver_args = args\n+        if driver_kwargs is None:\n+            driver_kwargs = kwargs\n \n-        for workers in work_groups:\n-            all_outputs.extend(\n-                self._run_workers_in_batch(workers, method, *args, **kwargs))\n+        # Start the driver worker after all the ray workers.\n+        driver_worker_output = getattr(self.driver_worker,\n+                                       method)(*driver_args, **driver_kwargs)\n \n-        if get_all_outputs:\n-            return all_outputs\n+        # Get the results of the ray workers.\n+        if self.workers:\n+            ray_worker_outputs = ray.get(ray_worker_outputs)\n \n-        # Make sure all workers have the same results.\n-        output = all_outputs[0]\n-        for other_output in all_outputs[1:]:\n-            assert output == other_output\n-        return output\n+        return [driver_worker_output] + ray_worker_outputs",
      "change_type": "modified",
      "lines_added": 139,
      "lines_removed": 101
    },
    {
      "file_path": "vllm/engine/ray_utils.py",
      "old_content": "from typing import Optional, Tuple, TYPE_CHECKING\n\nfrom vllm.config import ParallelConfig\nfrom vllm.logger import init_logger\nfrom vllm.utils import get_open_port, is_hip\n\nlogger = init_logger(__name__)\n\ntry:\n    import ray\n    from ray.air.util.torch_dist import TorchDistributedWorker\n\n    class RayWorkerVllm(TorchDistributedWorker):\n        \"\"\"Ray wrapper for vllm.worker.Worker, allowing Worker to be\n        lazliy initialized after Ray sets CUDA_VISIBLE_DEVICES.\"\"\"\n\n        def __init__(self, init_cached_hf_modules=False) -> None:\n            if init_cached_hf_modules:\n                from transformers.dynamic_module_utils import init_hf_modules\n                init_hf_modules()\n            self.worker = None\n\n        def init_worker(self, worker_init_fn):\n            self.worker = worker_init_fn()\n\n        def __getattr__(self, name):\n            return getattr(self.worker, name)\n\n        def execute_method(self, method, *args, **kwargs):\n            executor = getattr(self, method)\n            return executor(*args, **kwargs)\n\nexcept ImportError as e:\n    logger.warning(f\"Failed to import Ray with {e!r}. \"\n                   \"For distributed inference, please install Ray with \"\n                   \"`pip install ray pandas pyarrow`.\")\n    ray = None\n    TorchDistributedWorker = None\n    RayWorkerVllm = None\n\nif TYPE_CHECKING:\n    from ray.util.placement_group import PlacementGroup\n\n\ndef initialize_cluster(\n    parallel_config: ParallelConfig,\n    engine_use_ray: bool = False,\n    ray_address: Optional[str] = None,\n) -> Tuple[str, Optional[\"PlacementGroup\"]]:\n    \"\"\"Initialize the distributed cluster probably with Ray.\n\n    Args:\n        parallel_config: The configurations for parallel execution.\n        engine_use_ray: Whether to use Ray for async engine.\n        ray_address: The address of the Ray cluster. If None, uses\n            the default Ray cluster address.\n\n    Returns:\n        A tuple of (`distributed_init_method`, `placement_group`). The\n        `distributed_init_method` is the address for initializing the\n        distributed backend. `placement_group` includes the specification\n        of the resources for each distributed worker.\n    \"\"\"\n    if parallel_config.worker_use_ray or engine_use_ray:\n        if ray is None:\n            raise ImportError(\n                \"Ray is not installed. Please install Ray to use distributed \"\n                \"serving.\")\n        # Connect to a ray cluster.\n        if is_hip():\n            ray.init(address=ray_address,\n                     ignore_reinit_error=True,\n                     num_gpus=parallel_config.world_size)\n        else:\n            ray.init(address=ray_address, ignore_reinit_error=True)\n\n    if not parallel_config.worker_use_ray:\n        # Initialize cluster locally.\n        port = get_open_port()\n        # We need to setup the distributed init method to make sure\n        # the distributed megatron code (e.g., get world size) works correctly.\n        distributed_init_method = f\"tcp://localhost:{port}\"\n        return distributed_init_method, None\n\n    current_placement_group = ray.util.get_current_placement_group()\n    if current_placement_group:\n        # We are in a placement group\n        bundles = current_placement_group.bundle_specs\n        # Verify that we can use the placement group.\n        gpu_bundles = 0\n        for bundle in bundles:\n            bundle_gpus = bundle.get(\"GPU\", 0)\n            if bundle_gpus > 1:\n                raise ValueError(\n                    \"Placement group bundle cannot have more than 1 GPU.\")\n            if bundle_gpus:\n                gpu_bundles += 1\n        if parallel_config.world_size > gpu_bundles:\n            raise ValueError(\n                \"The number of required GPUs exceeds the total number of \"\n                \"available GPUs in the placement group.\")\n    else:\n        num_gpus_in_cluster = ray.cluster_resources().get(\"GPU\", 0)\n        if parallel_config.world_size > num_gpus_in_cluster:\n            raise ValueError(\n                \"The number of required GPUs exceeds the total number of \"\n                \"available GPUs in the cluster.\")\n        # Create a new placement group\n        current_placement_group = ray.util.placement_group([{\n            \"GPU\": 1\n        }] * parallel_config.world_size)\n        # Wait until PG is ready - this will block until all\n        # requested resources are available, and will timeout\n        # if they cannot be provisioned.\n        ray.get(current_placement_group.ready(), timeout=1800)\n\n    return None, current_placement_group\n",
      "diff": "diff --git a/vllm/engine/ray_utils.py b/vllm/engine/ray_utils.py\nindex f402da4c6..52e5be022 100644\n--- a/vllm/engine/ray_utils.py\n+++ b/vllm/engine/ray_utils.py\n@@ -1,16 +1,15 @@\n-from typing import Optional, Tuple, TYPE_CHECKING\n+from typing import Optional, List, Tuple, TYPE_CHECKING\n \n from vllm.config import ParallelConfig\n from vllm.logger import init_logger\n-from vllm.utils import get_open_port, is_hip\n+from vllm.utils import is_hip, set_cuda_visible_devices, get_ip\n \n logger = init_logger(__name__)\n \n try:\n     import ray\n-    from ray.air.util.torch_dist import TorchDistributedWorker\n \n-    class RayWorkerVllm(TorchDistributedWorker):\n+    class RayWorkerVllm:\n         \"\"\"Ray wrapper for vllm.worker.Worker, allowing Worker to be\n         lazliy initialized after Ray sets CUDA_VISIBLE_DEVICES.\"\"\"\n \n@@ -30,12 +29,22 @@ try:\n             executor = getattr(self, method)\n             return executor(*args, **kwargs)\n \n+        def get_node_ip(self) -> str:\n+            return get_ip()\n+\n+        def get_node_and_gpu_ids(self) -> Tuple[str, List[int]]:\n+            node_id = ray.get_runtime_context().get_node_id()\n+            gpu_ids = ray.get_gpu_ids()\n+            return node_id, gpu_ids\n+\n+        def set_cuda_visible_devices(self, device_ids) -> None:\n+            set_cuda_visible_devices(device_ids)\n+\n except ImportError as e:\n     logger.warning(f\"Failed to import Ray with {e!r}. \"\n                    \"For distributed inference, please install Ray with \"\n                    \"`pip install ray pandas pyarrow`.\")\n     ray = None\n-    TorchDistributedWorker = None\n     RayWorkerVllm = None\n \n if TYPE_CHECKING:\n@@ -75,13 +84,11 @@ def initialize_cluster(\n             ray.init(address=ray_address, ignore_reinit_error=True)\n \n     if not parallel_config.worker_use_ray:\n-        # Initialize cluster locally.\n-        port = get_open_port()\n-        # We need to setup the distributed init method to make sure\n-        # the distributed megatron code (e.g., get world size) works correctly.\n-        distributed_init_method = f\"tcp://localhost:{port}\"\n-        return distributed_init_method, None\n+        assert parallel_config.world_size == 1, (\n+            \"Ray is required if parallel_config.world_size > 1.\")\n+        return None\n \n+    # Create placement group for worker processes\n     current_placement_group = ray.util.get_current_placement_group()\n     if current_placement_group:\n         # We are in a placement group\n@@ -106,12 +113,12 @@ def initialize_cluster(\n                 \"The number of required GPUs exceeds the total number of \"\n                 \"available GPUs in the cluster.\")\n         # Create a new placement group\n-        current_placement_group = ray.util.placement_group([{\n-            \"GPU\": 1\n-        }] * parallel_config.world_size)\n+        placement_group_specs = ([{\"GPU\": 1}] * parallel_config.world_size)\n+        current_placement_group = ray.util.placement_group(\n+            placement_group_specs)\n         # Wait until PG is ready - this will block until all\n         # requested resources are available, and will timeout\n         # if they cannot be provisioned.\n         ray.get(current_placement_group.ready(), timeout=1800)\n \n-    return None, current_placement_group\n+    return current_placement_group",
      "change_type": "modified",
      "lines_added": 23,
      "lines_removed": 16
    },
    {
      "file_path": "vllm/model_executor/input_metadata.py",
      "old_content": "from typing import List, Optional\n\nimport torch\n\n\nclass InputMetadata:\n    \"\"\"Metadata for input sequences. Used in PagedAttention.\n\n    Args:\n        prompt_lens: Lengths of prompts.\n        slot_mapping: The address to write the new KV to of each token.\n        max_context_len: The maximum context length.\n        context_lens: the length of attention context for each sequence.\n        block_tables: The block tables. (Seq id -> list of physical block)\n    \"\"\"\n\n    def __init__(\n        self,\n        prompt_lens: List[int],\n        slot_mapping: torch.Tensor,\n        max_context_len: Optional[int],\n        context_lens: Optional[torch.Tensor],\n        block_tables: Optional[torch.Tensor],\n        use_cuda_graph: bool,\n    ) -> None:\n        self.prompt_lens = prompt_lens\n        self.max_context_len = max_context_len\n        self.slot_mapping = slot_mapping\n        self.context_lens = context_lens\n        self.block_tables = block_tables\n        self.use_cuda_graph = use_cuda_graph\n\n        self.is_prompt = len(prompt_lens) > 0\n        # Set during the execution of the first attention op.\n        # FIXME(woosuk): This is a hack.\n        self.attn_bias = None\n\n    def __repr__(self) -> str:\n        return (\"InputMetadata(\"\n                f\"prompt_lens={self.prompt_lens}, \"\n                f\"max_context_len={self.max_context_len}, \"\n                f\"slot_mapping={self.slot_mapping}, \"\n                f\"context_lens={self.context_lens}, \"\n                f\"block_tables={self.block_tables}, \"\n                f\"use_cuda_graph={self.use_cuda_graph})\")\n",
      "diff": "diff --git a/vllm/model_executor/input_metadata.py b/vllm/model_executor/input_metadata.py\nindex af6f49218..da615eccc 100644\n--- a/vllm/model_executor/input_metadata.py\n+++ b/vllm/model_executor/input_metadata.py\n@@ -1,4 +1,4 @@\n-from typing import List, Optional\n+from typing import Optional\n \n import torch\n \n@@ -16,28 +16,27 @@ class InputMetadata:\n \n     def __init__(\n         self,\n-        prompt_lens: List[int],\n+        is_prompt: bool,\n         slot_mapping: torch.Tensor,\n         max_context_len: Optional[int],\n         context_lens: Optional[torch.Tensor],\n         block_tables: Optional[torch.Tensor],\n         use_cuda_graph: bool,\n     ) -> None:\n-        self.prompt_lens = prompt_lens\n+        self.is_prompt = is_prompt\n         self.max_context_len = max_context_len\n         self.slot_mapping = slot_mapping\n         self.context_lens = context_lens\n         self.block_tables = block_tables\n         self.use_cuda_graph = use_cuda_graph\n \n-        self.is_prompt = len(prompt_lens) > 0\n         # Set during the execution of the first attention op.\n         # FIXME(woosuk): This is a hack.\n         self.attn_bias = None\n \n     def __repr__(self) -> str:\n         return (\"InputMetadata(\"\n-                f\"prompt_lens={self.prompt_lens}, \"\n+                f\"is_prompt={self.is_prompt}, \"\n                 f\"max_context_len={self.max_context_len}, \"\n                 f\"slot_mapping={self.slot_mapping}, \"\n                 f\"context_lens={self.context_lens}, \"",
      "change_type": "modified",
      "lines_added": 5,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/model_executor/layers/sampler.py",
      "old_content": "\"\"\"A layer that samples the next tokens from the model's outputs.\"\"\"\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom vllm.model_executor.parallel_utils.communication_op import (\n    tensor_model_parallel_all_gather)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata, SamplingTensors\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import (PromptLogprobs, SampleLogprobs, SamplerOutput,\n                           SequenceData, SequenceGroupOutput, SequenceOutput)\n\n\nclass Sampler(nn.Module):\n    \"\"\"Samples the next tokens from the model's outputs.\n\n    This layer does the following:\n    1. Discard the hidden states that are not used for sampling (i.e., all\n        tokens except the final one in each prompt).\n    2. Compute the logits for the next tokens.\n    3. Apply presence, frequency and repetition penalties.\n    4. Apply temperature scaling.\n    5. Apply top-p and top-k truncation.\n    6. Sample the next tokens.\n    Here, each sequence group within the batch can have different sampling\n    parameters (e.g., sampling method, temperature, top-p, top-k, etc.).\n    \"\"\"\n\n    def __init__(self, vocab_size: int) -> None:\n        super().__init__()\n        self.vocab_size = vocab_size\n\n    def forward(\n        self,\n        embedding: torch.Tensor,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n        embedding_bias: Optional[torch.Tensor] = None,\n    ) -> SamplerOutput:\n        # Get the hidden states that we use for sampling.\n        hidden_states = _prune_hidden_states(hidden_states, sampling_metadata)\n\n        # Get the logits for the next tokens.\n        logits = _get_logits(hidden_states, embedding, embedding_bias,\n                             self.vocab_size)\n\n        _, vocab_size = logits.shape\n\n        # Apply logits processors (if any).\n        logits = _apply_logits_processors(logits, sampling_metadata)\n\n        # Prepare sampling tensors with pinned memory to avoid blocking.\n        (sampling_tensors, do_penalties, do_top_p_top_k,\n         do_min_p) = SamplingTensors.from_sampling_metadata(\n             sampling_metadata, vocab_size, logits.device, logits.dtype)\n\n        # Apply presence and frequency penalties.\n        if do_penalties:\n            logits = _apply_penalties(logits, sampling_tensors.prompt_tokens,\n                                      sampling_tensors.output_tokens,\n                                      sampling_tensors.presence_penalties,\n                                      sampling_tensors.frequency_penalties,\n                                      sampling_tensors.repetition_penalties)\n\n        # Apply temperature scaling.\n        # Use in-place division to avoid creating a new tensor.\n        logits.div_(sampling_tensors.temperatures.unsqueeze_(dim=1))\n\n        if do_top_p_top_k:\n            logits = _apply_top_p_top_k(logits, sampling_tensors.top_ps,\n                                        sampling_tensors.top_ks)\n\n        if do_min_p:\n            logits = _apply_min_p(logits, sampling_tensors.min_ps)\n\n        # We use float32 for probabilities and log probabilities.\n        # Compute the probabilities.\n        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n        # Compute the log probabilities.\n        # Use log_softmax to ensure numerical stability.\n        logprobs = torch.log_softmax(logits, dim=-1, dtype=torch.float)\n\n        # Sample the next tokens.\n        sample_results = _sample(probs, logprobs, sampling_metadata)\n        # Get the logprobs query results.\n        prompt_logprobs, sample_logprobs = _get_logprobs(\n            logprobs, sampling_metadata, sample_results)\n        return _build_sampler_output(sample_results, sampling_metadata,\n                                     prompt_logprobs, sample_logprobs)\n\n\ndef _get_logits(hidden_states: torch.Tensor, embedding: torch.Tensor,\n                embedding_bias: Optional[torch.Tensor],\n                vocab_size: int) -> torch.Tensor:\n    # Get the logits for the next tokens.\n    logits = torch.matmul(hidden_states, embedding.t())\n    if embedding_bias is not None:\n        logits += embedding_bias\n    logits = tensor_model_parallel_all_gather(logits)\n    # Remove paddings in vocab (if any).\n    logits = logits[:, :vocab_size]\n    return logits\n\n\ndef _prune_hidden_states(\n    hidden_states: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n) -> torch.Tensor:\n    hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n    return hidden_states.index_select(0,\n                                      sampling_metadata.selected_token_indices)\n\n\ndef _get_bin_counts_and_mask(\n    tokens: torch.Tensor,\n    vocab_size: int,\n    num_seqs: int,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    # Compute the bin counts for the tokens.\n    # vocab_size + 1 for padding.\n    bin_counts = torch.zeros((num_seqs, vocab_size + 1),\n                             dtype=torch.long,\n                             device=tokens.device)\n    bin_counts.scatter_add_(1, tokens, torch.ones_like(tokens))\n    bin_counts = bin_counts[:, :vocab_size]\n    mask = bin_counts > 0\n\n    return bin_counts, mask\n\n\ndef _apply_logits_processors(\n    logits: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n) -> torch.Tensor:\n    logits_row_idx = 0\n    found_logits_processors = False\n    for seq_ids, sampling_params in sampling_metadata.seq_groups:\n        logits_processors = sampling_params.logits_processors\n        if logits_processors:\n            found_logits_processors = True\n            for seq_id in seq_ids:\n                logits_row = logits[logits_row_idx]\n                token_ids = sampling_metadata.seq_data[seq_id].output_token_ids\n                for logits_processor in logits_processors:\n                    logits_row = logits_processor(token_ids, logits_row)\n                logits[logits_row_idx] = logits_row\n                logits_row_idx += 1\n        else:\n            logits_row_idx += len(seq_ids)\n    if found_logits_processors:\n        assert logits_row_idx == logits.shape[0]\n    return logits\n\n\ndef _apply_penalties(logits: torch.Tensor, prompt_tokens_tensor: torch.Tensor,\n                     output_tokens_tensor: torch.Tensor,\n                     presence_penalties: torch.Tensor,\n                     frequency_penalties: torch.Tensor,\n                     repetition_penalties: torch.Tensor) -> torch.Tensor:\n    num_seqs, vocab_size = logits.shape\n    _, prompt_mask = _get_bin_counts_and_mask(prompt_tokens_tensor, vocab_size,\n                                              num_seqs)\n    output_bin_counts, output_mask = _get_bin_counts_and_mask(\n        output_tokens_tensor, vocab_size, num_seqs)\n\n    repetition_penalties = repetition_penalties[:, None].repeat(1, vocab_size)\n    repetition_penalties[~(prompt_mask | output_mask)] = 1.0\n    logits = torch.where(logits > 0, logits / repetition_penalties,\n                         logits * repetition_penalties)\n\n    # We follow the definition in OpenAI API.\n    # Refer to https://platform.openai.com/docs/api-reference/parameter-details\n    logits -= frequency_penalties.unsqueeze_(dim=1) * output_bin_counts\n    logits -= presence_penalties.unsqueeze_(dim=1) * output_mask\n    return logits\n\n\ndef _apply_top_p_top_k(\n    logits: torch.Tensor,\n    p: torch.Tensor,\n    k: torch.Tensor,\n) -> torch.Tensor:\n    logits_sort, logits_idx = logits.sort(dim=-1, descending=True)\n\n    # Apply top-p.\n    probs_sort = logits_sort.softmax(dim=-1)\n    probs_sum = probs_sort.cumsum(dim=-1).sub_(probs_sort)\n    top_p_mask = probs_sum > p.unsqueeze_(dim=1)\n\n    # Apply top-k.\n    # Create a mask for the top-k elements.\n    top_k_mask = torch.arange(logits_idx.shape[-1], device=logits_idx.device)\n    top_k_mask = top_k_mask.expand(logits_idx.shape[0], -1)\n    top_k_mask = top_k_mask >= k.unsqueeze_(dim=1)\n\n    # Final mask.\n    mask = (top_p_mask | top_k_mask)\n    logits_sort.masked_fill_(mask, -float(\"inf\"))\n\n    # Re-sort the probabilities.\n    src = torch.arange(logits_idx.shape[-1],\n                       device=logits_idx.device).expand_as(logits_idx)\n    logits_idx_inv = torch.empty_like(logits_idx).scatter_(dim=-1,\n                                                           index=logits_idx,\n                                                           src=src)\n    logits = torch.gather(logits_sort, dim=-1, index=logits_idx_inv)\n    return logits\n\n\ndef _apply_min_p(\n    logits: torch.Tensor,\n    min_p: torch.Tensor,\n) -> torch.Tensor:\n    \"\"\"\n    Adapted from\n    https://github.com/oobabooga/text-generation-webui/blob/3146124ec01f02c8fb1650a6517cf1b60b537aaf/modules/sampler_hijack.py#L16C17-L16C17\n    \"\"\"\n    probs = torch.softmax(logits, dim=-1)\n    top_probs, _ = probs.max(dim=-1, keepdim=True)\n    scaled_min_p = min_p.unsqueeze_(dim=1) * top_probs\n    tokens_to_remove = probs < scaled_min_p\n    logits = logits.masked_fill_(tokens_to_remove, -float(\"inf\"))\n\n    return logits\n\n\ndef _greedy_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    samples: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    samples = samples.tolist()\n    sample_idx = 0\n    results = []\n    for seq_group in selected_seq_groups:\n        seq_ids, _ = seq_group\n        num_parent_seqs = len(seq_ids)\n        assert num_parent_seqs == 1, (\n            \"Greedy sampling should have only one seq.\")\n        parent_ids = list(range(num_parent_seqs))\n        next_token_ids = [samples[sample_idx]]\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    return results\n\n\ndef _random_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    is_prompts: List[bool],\n    random_samples: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    # Find the maximum best_of value of the prompt phase requests.\n    random_samples = random_samples.cpu()\n    sample_idx = 0\n    results = []\n    for seq_group, is_prompt in zip(selected_seq_groups, is_prompts):\n        seq_ids, sampling_params = seq_group\n        num_parent_seqs = len(seq_ids)\n        if is_prompt:\n            # Prompt phase.\n            parent_ids = [0] * sampling_params.best_of\n            next_token_ids = random_samples[\n                sample_idx, :sampling_params.best_of].tolist()\n        else:\n            # Generation phase.\n            parent_ids = list(range(num_parent_seqs))\n            next_token_ids = random_samples[sample_idx:sample_idx +\n                                            num_parent_seqs, 0].tolist()\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    return results\n\n\ndef _beam_search_sample(\n    selected_seq_groups: List[Tuple[List[int], SamplingParams]],\n    is_prompts: List[bool],\n    seq_data: Dict[int, SequenceData],\n    logprobs: torch.Tensor,\n) -> List[Tuple[List[int], List[int]]]:\n    # We sample 2 * beam_width candidates to make sure that with high\n    # probability we can get `beam_width` candidates in addition to\n    # the finished sequences for the next iteration. See\n    # https://github.com/tensorflow/tensor2tensor/blob/bafdc1b67730430d38d6ab802cbd51f9d053ba2e/tensor2tensor/utils/beam_search.py#L557-L563\n    # for details. See also HF reference:\n    # https://github.com/huggingface/transformers/blob/a4dd53d88e4852f023332d284ff07a01afcd5681/src/transformers/generation/utils.py#L3063-L3065\n    #\n    # NOTE: Beam search is not vectorized, so its speed can be slower than\n    # other sampling methods.\n    sample_idx = 0\n    results = []\n    for seq_group, is_prompt in zip(selected_seq_groups, is_prompts):\n        seq_ids, sampling_params = seq_group\n        num_parent_seqs = len(seq_ids)\n        beam_width = sampling_params.best_of\n        seq_group_logprobs = logprobs[sample_idx:sample_idx + num_parent_seqs]\n        if is_prompt:\n            # Prompt phase.\n            assert num_parent_seqs == 1, (\n                \"Prompt input should have only one seq.\")\n            parent_ids = [0] * (2 * beam_width)\n            _, next_token_ids = torch.topk(seq_group_logprobs[0],\n                                           2 * beam_width)\n            next_token_ids = next_token_ids.tolist()\n        else:\n            # Generation phase.\n            cumulative_logprobs = [\n                seq_data[seq_id].cumulative_logprob for seq_id in seq_ids\n            ]\n            cumulative_logprobs = torch.tensor(\n                cumulative_logprobs,\n                dtype=torch.float,\n                device=seq_group_logprobs.device)\n            seq_group_logprobs = (seq_group_logprobs +\n                                  cumulative_logprobs.unsqueeze(dim=1))\n            _, topk_ids = torch.topk(seq_group_logprobs.flatten(),\n                                     2 * beam_width)\n            topk_ids = topk_ids.tolist()\n            vocab_size = seq_group_logprobs.size(-1)\n            parent_ids = [i // vocab_size for i in topk_ids]\n            next_token_ids = [i % vocab_size for i in topk_ids]\n        results.append((next_token_ids, parent_ids))\n        sample_idx += num_parent_seqs\n    assert sample_idx == logprobs.size(0)\n    return results\n\n\n# torch.multinomial forces a GPU<->CPU sync.\n# Therefore, we use an optimized implementation instead.\n# Note that we always sample with replacement.\n# probs will be modified in place, but this is fine, as we pass\n# in a copy already.\ndef _multinomial(\n    probs: torch.Tensor,\n    num_samples: int,\n):\n    if num_samples > 1:\n        # This is equivalent to torch.repeat_interleaved (which also\n        # forces a GPU<->CPU sync).\n        # This allows us to do sampling with replacement by creating\n        # num_samples copies of each row in the tensor, and then\n        # batch sampling the resulting tensor.\n        probs = probs[:, None, :].expand(probs.shape[0], num_samples,\n                                         probs.shape[1]).contiguous().view(\n                                             -1, probs.shape[1])\n    q = torch.empty_like(probs).exponential_(1)\n    return probs.div_(q).argmax(dim=1).view(-1, num_samples)\n\n\ndef _sample(\n    probs: torch.Tensor,\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n) -> List[Tuple[List[int], List[int]]]:\n    categorized_seq_group_ids = {t: [] for t in SamplingType}\n    categorized_sample_indices = sampling_metadata.categorized_sample_indices\n    for i, seq_group in enumerate(sampling_metadata.seq_groups):\n        _, sampling_params = seq_group\n        sampling_type = sampling_params.sampling_type\n        categorized_seq_group_ids[sampling_type].append(i)\n\n    sample_results_dict: Dict[int, Tuple[List[int], List[int]]] = {}\n    sample_metadata = {}\n\n    # Counterintiutively, having two loops here is actually faster.\n    # The first loop can run without waiting on GPU<->CPU sync.\n    for sampling_type in SamplingType:\n        sample_indices = categorized_sample_indices[sampling_type]\n        num_tokens = len(sample_indices)\n        if num_tokens == 0:\n            continue\n        seq_group_ids = categorized_seq_group_ids[sampling_type]\n        seq_groups = [sampling_metadata.seq_groups[i] for i in seq_group_ids]\n        is_prompts = [i < sampling_metadata.num_prompts for i in seq_group_ids]\n        sample_metadata[sampling_type] = (seq_group_ids, seq_groups,\n                                          is_prompts, sample_indices)\n        if sampling_type == SamplingType.GREEDY:\n            greedy_samples = torch.argmax(logprobs[sample_indices], dim=-1)\n        elif sampling_type == SamplingType.RANDOM:\n            max_best_of = 1\n            for seq_group, is_prompt in zip(seq_groups, is_prompts):\n                if is_prompt:\n                    _, sampling_params = seq_group\n                    max_best_of = max(max_best_of, sampling_params.best_of)\n            multinomial_samples = _multinomial(probs[sample_indices],\n                                               max_best_of)\n        elif sampling_type == SamplingType.BEAM:\n            beam_search_logprobs = logprobs[sample_indices]\n        else:\n            raise ValueError(f\"Unsupported sampling type: {sampling_type}\")\n\n    # GPU<->CPU sync happens in the loop below.\n\n    for sampling_type in SamplingType:\n        if sampling_type not in sample_metadata:\n            continue\n        seq_group_ids, seq_groups, is_prompts, sample_indices = sample_metadata[\n            sampling_type]\n        if sampling_type == SamplingType.GREEDY:\n            sample_results = _greedy_sample(seq_groups, greedy_samples)\n        elif sampling_type == SamplingType.RANDOM:\n            sample_results = _random_sample(seq_groups, is_prompts,\n                                            multinomial_samples)\n        elif sampling_type == SamplingType.BEAM:\n            sample_results = _beam_search_sample(seq_groups, is_prompts,\n                                                 sampling_metadata.seq_data,\n                                                 beam_search_logprobs)\n        sample_results_dict.update(zip(seq_group_ids, sample_results))\n\n    sample_results = [\n        sample_results_dict[i]\n        for i in range(len(sampling_metadata.seq_groups))\n    ]\n    return sample_results\n\n\ndef _get_logprobs(\n    logprobs: torch.Tensor,\n    sampling_metadata: SamplingMetadata,\n    sample_results: List[Tuple[List[int], List[int]]],\n) -> Tuple[List[Optional[List[Optional[Dict[int, float]]]]], List[List[Dict[\n        int, float]]]]:\n    # Prepare query indices\n    batched_logprobs_query_seq_indices: List[int] = []\n    batched_logprobs_query_token_indices: List[int] = []\n    largest_num_logprobs = 0\n    sample_idx = 0\n    for i, (seq_group, sample_result) in enumerate(\n            zip(sampling_metadata.seq_groups, sample_results)):\n        seq_ids, sampling_params = seq_group\n        next_token_ids, parent_ids = sample_result\n        num_parent_seqs = len(seq_ids)\n        if (i < sampling_metadata.num_prompts\n                and sampling_params.prompt_logprobs is not None):\n            largest_num_logprobs = max(largest_num_logprobs,\n                                       sampling_params.prompt_logprobs)\n            prompt_len = sampling_metadata.prompt_lens[i]\n            prompt_tokens = sampling_metadata.seq_data[\n                seq_ids[0]].prompt_token_ids\n            batched_logprobs_query_seq_indices.extend(\n                sample_idx + j for j in range(prompt_len - 1))\n            batched_logprobs_query_token_indices.extend(\n                token_id for token_id in prompt_tokens[1:])\n            sample_idx += prompt_len - 1\n        batched_logprobs_query_seq_indices.extend(\n            [sample_idx + parent_id for parent_id in parent_ids])\n        batched_logprobs_query_token_indices.extend(next_token_ids)\n        if sampling_params.logprobs is not None:\n            largest_num_logprobs = max(largest_num_logprobs,\n                                       sampling_params.logprobs)\n        sample_idx += num_parent_seqs\n    assert sample_idx == logprobs.size(0)\n\n    # Batched query for logprobs of selected token\n    batched_logprobs_query_result = logprobs[[\n        batched_logprobs_query_seq_indices,\n        batched_logprobs_query_token_indices\n    ]]\n\n    # Batched query for logprobs of topk tokens\n    if largest_num_logprobs > 0:\n        top_logprobs, top_token_ids = torch.topk(logprobs,\n                                                 largest_num_logprobs,\n                                                 dim=-1)\n        top_logprobs = top_logprobs.cpu()\n        top_token_ids = top_token_ids.cpu()\n    else:\n        top_logprobs, top_token_ids = None, None\n\n    batched_logprobs_query_result = batched_logprobs_query_result.cpu()\n\n    # Gather results\n    result_prompt_logprobs: List[Optional[PromptLogprobs]] = []\n    result_sample_logprobs: List[SampleLogprobs] = []\n    sample_idx = 0\n    query_result_idx = 0\n    for i, (seq_group, sample_result) in enumerate(\n            zip(sampling_metadata.seq_groups, sample_results)):\n        seq_ids, sampling_params = seq_group\n        next_token_ids, parent_ids = sample_result\n\n        # Prompt logprobs\n        if (i < sampling_metadata.num_prompts\n                and sampling_params.prompt_logprobs is not None):\n            num_logprobs = sampling_params.prompt_logprobs\n            prompt_len = sampling_metadata.prompt_lens[i]\n            prompt_tokens = sampling_metadata.seq_data[\n                seq_ids[0]].prompt_token_ids\n            group_prompt_logprobs: PromptLogprobs = [None]\n            for token_id in prompt_tokens[1:]:\n                prompt_logprobs_dict = {\n                    token_id:\n                    batched_logprobs_query_result[query_result_idx].item()\n                }\n                if num_logprobs > 0:\n                    prompt_logprobs_dict.update(\n                        zip(top_token_ids[sample_idx, :num_logprobs].tolist(),\n                            top_logprobs[sample_idx, :num_logprobs].tolist()))\n                group_prompt_logprobs.append(prompt_logprobs_dict)\n                sample_idx += 1\n                query_result_idx += 1\n            result_prompt_logprobs.append(group_prompt_logprobs)\n        else:\n            result_prompt_logprobs.append(None)\n\n        # Sample logprobs\n        num_logprobs = sampling_params.logprobs\n        if num_logprobs is None:\n            num_logprobs = 0\n        group_sample_logprobs: SampleLogprobs = []\n        for next_token_id, parent_id in zip(next_token_ids, parent_ids):\n            sample_logprobs_dict = {\n                next_token_id:\n                batched_logprobs_query_result[query_result_idx].item()\n            }\n            query_result_idx += 1\n            if num_logprobs > 0:\n                sample_logprobs_dict.update(\n                    zip(\n                        top_token_ids[sample_idx +\n                                      parent_id, :num_logprobs].tolist(),\n                        top_logprobs[sample_idx +\n                                     parent_id, :num_logprobs].tolist()))\n            group_sample_logprobs.append(sample_logprobs_dict)\n        result_sample_logprobs.append(group_sample_logprobs)\n        sample_idx += len(seq_ids)\n\n    return result_prompt_logprobs, result_sample_logprobs\n\n\ndef _build_sampler_output(\n    sample_results: List[Tuple[List[int], List[int]]],\n    sampling_metadata: SamplingMetadata,\n    prompt_logprobs: List[Optional[PromptLogprobs]],\n    sample_logprobs: List[SampleLogprobs],\n) -> SamplerOutput:\n    sampler_output = []\n    for (seq_group, sample_result, group_prompt_logprobs,\n         group_sample_logprobs) in zip(sampling_metadata.seq_groups,\n                                       sample_results, prompt_logprobs,\n                                       sample_logprobs):\n        seq_ids, _ = seq_group\n        next_token_ids, parent_ids = sample_result\n        seq_outputs = []\n        for parent_id, next_token_id, logprobs in zip(parent_ids,\n                                                      next_token_ids,\n                                                      group_sample_logprobs):\n            seq_outputs.append(\n                SequenceOutput(seq_ids[parent_id], next_token_id, logprobs))\n        sampler_output.append(\n            SequenceGroupOutput(seq_outputs, group_prompt_logprobs))\n    return sampler_output\n",
      "diff": "diff --git a/vllm/model_executor/layers/sampler.py b/vllm/model_executor/layers/sampler.py\nindex 25ba48c2a..ebc9afc1b 100644\n--- a/vllm/model_executor/layers/sampler.py\n+++ b/vllm/model_executor/layers/sampler.py\n@@ -5,7 +5,7 @@ import torch\n import torch.nn as nn\n \n from vllm.model_executor.parallel_utils.communication_op import (\n-    tensor_model_parallel_all_gather)\n+    tensor_model_parallel_gather)\n from vllm.model_executor.sampling_metadata import SamplingMetadata, SamplingTensors\n from vllm.sampling_params import SamplingParams, SamplingType\n from vllm.sequence import (PromptLogprobs, SampleLogprobs, SamplerOutput,\n@@ -37,7 +37,7 @@ class Sampler(nn.Module):\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n         embedding_bias: Optional[torch.Tensor] = None,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         # Get the hidden states that we use for sampling.\n         hidden_states = _prune_hidden_states(hidden_states, sampling_metadata)\n \n@@ -45,6 +45,14 @@ class Sampler(nn.Module):\n         logits = _get_logits(hidden_states, embedding, embedding_bias,\n                              self.vocab_size)\n \n+        # Only perform sampling in the driver worker.\n+        # Note: `_get_logits` is still distributed across TP workers because\n+        # the `embedding` weight is distributed across TP workers.\n+        # TODO(zhuohan): Change the get_logits part to a separate stage.\n+        if not sampling_metadata.perform_sampling:\n+            return None\n+\n+        assert logits is not None\n         _, vocab_size = logits.shape\n \n         # Apply logits processors (if any).\n@@ -92,14 +100,15 @@ class Sampler(nn.Module):\n \n def _get_logits(hidden_states: torch.Tensor, embedding: torch.Tensor,\n                 embedding_bias: Optional[torch.Tensor],\n-                vocab_size: int) -> torch.Tensor:\n+                vocab_size: int) -> Optional[torch.Tensor]:\n     # Get the logits for the next tokens.\n     logits = torch.matmul(hidden_states, embedding.t())\n     if embedding_bias is not None:\n         logits += embedding_bias\n-    logits = tensor_model_parallel_all_gather(logits)\n+    logits = tensor_model_parallel_gather(logits)\n     # Remove paddings in vocab (if any).\n-    logits = logits[:, :vocab_size]\n+    if logits is not None:\n+        logits = logits[:, :vocab_size]\n     return logits",
      "change_type": "modified",
      "lines_added": 15,
      "lines_removed": 6
    },
    {
      "file_path": "vllm/model_executor/models/aquila.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only LLaMA model compatible with HuggingFace weights.\"\"\"\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs.aquila import AquilaConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass AquilaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass AquilaRMSNorm(nn.Module):\n\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        AquilaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1,\n                                                               keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance +\n                                                    self.variance_epsilon)\n\n        return (self.weight * hidden_states).to(input_dtype)\n\n\nclass AquilaAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        num_kv_heads: int,\n        rope_theta: float = 10000,\n        max_position_embeddings: int = 8192,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        assert self.total_num_kv_heads % tp_size == 0\n        self.num_kv_heads = self.total_num_kv_heads // tp_size\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=self.max_position_embeddings,\n            base=self.rope_theta,\n            rope_scaling=rope_scaling,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   self.scaling,\n                                   num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass AquilaDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: AquilaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.self_attn = AquilaAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            max_position_embeddings=max_position_embeddings,\n            rope_scaling=rope_scaling,\n            linear_method=linear_method,\n        )\n        self.mlp = AquilaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.input_layernorm = AquilaRMSNorm(config.hidden_size,\n                                             eps=config.rms_norm_eps)\n        self.post_attention_layernorm = AquilaRMSNorm(config.hidden_size,\n                                                      eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        hidden_states = self.input_layernorm(hidden_states)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nclass AquilaModel(nn.Module):\n\n    def __init__(\n        self,\n        config: AquilaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            AquilaDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = AquilaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.norm(hidden_states)\n\n        return hidden_states\n\n\nclass AquilaForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = AquilaModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/aquila.py b/vllm/model_executor/models/aquila.py\nindex 2a1a0d768..2f2bd5ffb 100644\n--- a/vllm/model_executor/models/aquila.py\n+++ b/vllm/model_executor/models/aquila.py\n@@ -298,7 +298,7 @@ class AquilaForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/baichuan.py",
      "old_content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only BaiChuan model compatible with HuggingFace weights.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs.baichuan import BaiChuanConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\n    base = torch.tensor(\n        2**(-(2**-(math.log2(closest_power_of_2) - 3))),\n        dtype=torch.float32,\n    )\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\n    slopes = torch.pow(base, powers)\n\n    if closest_power_of_2 != total_num_heads:\n        extra_base = torch.tensor(\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\n            dtype=torch.float32,\n        )\n        num_remaining_heads = min(closest_power_of_2,\n                                  total_num_heads - closest_power_of_2)\n        extra_powers = torch.arange(start=1,\n                                    end=1 + 2 * num_remaining_heads,\n                                    step=2,\n                                    dtype=torch.int32)\n        slopes = torch.cat(\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\n    return slopes\n\n\nclass BaiChuanMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass BaiChuanAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        position_embedding: str,\n        rope_theta: float = 10000,\n        max_position_embeddings: int = 8192,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size(\n        )\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = (self.total_num_heads //\n                          tensor_model_parallel_world_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.postion_embedding = position_embedding\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        # pylint: disable=invalid-name\n        self.W_pack = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n        # Create the alibi slopes and slice them.\n        if self.postion_embedding == \"ALIBI\":\n            tp_rank = get_tensor_model_parallel_rank()\n            head_start = tp_rank * self.num_heads\n            head_end = (tp_rank + 1) * self.num_heads\n            alibi_slopes = _get_alibi_slopes(self.total_num_heads)\n            alibi_slopes = alibi_slopes[head_start:head_end].tolist()\n\n            scaling = self.head_dim**-0.5\n            self.attn = PagedAttention(self.num_heads,\n                                       self.head_dim,\n                                       scaling,\n                                       alibi_slopes=alibi_slopes)\n        else:\n            self.rotary_emb = get_rope(\n                self.head_dim,\n                rotary_dim=self.head_dim,\n                max_position=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n            self.scaling = self.head_dim**-0.5\n            self.attn = PagedAttention(self.num_heads, self.head_dim,\n                                       self.scaling)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.W_pack(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        if self.postion_embedding != \"ALIBI\":\n            q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass BaiChuanDecoderLayer(nn.Module):\n\n    def __init__(self,\n                 config: BaiChuanConfig,\n                 position_embedding: str,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.self_attn = BaiChuanAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            position_embedding=position_embedding,\n            rope_theta=rope_theta,\n            max_position_embeddings=max_position_embeddings,\n            linear_method=linear_method,\n        )\n        self.mlp = BaiChuanMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass BaiChuanModel(nn.Module):\n\n    def __init__(self,\n                 config: BaiChuanConfig,\n                 position_embedding: str,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            BaiChuanDecoderLayer(config, position_embedding, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass BaiChuanBaseForCausalLM(nn.Module):\n\n    def __init__(self,\n                 config,\n                 position_embedding: str,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = BaiChuanModel(config, position_embedding, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            if name == \"lm_head.weight\":\n                # Unlike Baichuan, Baichuan2 normalizes the head weights. Refer to:\n                # https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat/blob/84603cde5ebffb6084e476cfaeceaf0b8b91fe54/modeling_baichuan.py#L508\n                # Distinguish between Baichuan and Baichuan2 by checking the\n                # vocab size. This is suggested by\n                # https://github.com/vllm-project/vllm/pull/1022#discussion_r1325652704\n                is_baichuan2 = self.config.vocab_size == 125696\n                if is_baichuan2:\n                    loaded_weight = torch.nn.functional.normalize(\n                        loaded_weight)\n\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n\n\nclass BaichuanForCausalLM(BaiChuanBaseForCausalLM):\n    \"\"\"Baichuan 13B and Baichuan2 7B/13B.\"\"\"\n\n    def __init__(self,\n                 config,\n                 linear_method: Optional[LinearMethodBase] = None):\n        if config.hidden_size == 4096:  # baichuan2 7b\n            super().__init__(config, \"ROPE\", linear_method)\n        else:  # baichuan 13b, baichuan2 13b\n            super().__init__(config, \"ALIBI\", linear_method)\n\n\nclass BaiChuanForCausalLM(BaiChuanBaseForCausalLM):\n    \"\"\"Baichuan 7B.\"\"\"\n\n    def __init__(self,\n                 config,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__(config, \"ROPE\", linear_method)\n",
      "diff": "diff --git a/vllm/model_executor/models/baichuan.py b/vllm/model_executor/models/baichuan.py\nindex cd8ab4446..f08c3c8d2 100644\n--- a/vllm/model_executor/models/baichuan.py\n+++ b/vllm/model_executor/models/baichuan.py\n@@ -313,7 +313,7 @@ class BaiChuanBaseForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/bloom.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/bloom/modeling_bloom.py\n# Copyright 2023 The CacheFlow team.\n# Copyright 2022 HuggingFace Inc. team and BigScience workshop.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only BLOOM model compatible with HuggingFace weights.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import BloomConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\n    base = torch.tensor(\n        2**(-(2**-(math.log2(closest_power_of_2) - 3))),\n        dtype=torch.float32,\n    )\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\n    slopes = torch.pow(base, powers)\n\n    if closest_power_of_2 != total_num_heads:\n        extra_base = torch.tensor(\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\n            dtype=torch.float32,\n        )\n        num_remaining_heads = min(closest_power_of_2,\n                                  total_num_heads - closest_power_of_2)\n        extra_powers = torch.arange(start=1,\n                                    end=1 + 2 * num_remaining_heads,\n                                    step=2,\n                                    dtype=torch.int32)\n        slopes = torch.cat(\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\n    return slopes\n\n\nclass BloomAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: BloomConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.total_num_heads = config.n_head\n        self.head_dim = self.hidden_size // self.total_num_heads\n        assert self.head_dim * self.total_num_heads == self.hidden_size\n\n        tp_world_size = get_tensor_model_parallel_world_size()\n        assert self.total_num_heads % tp_world_size == 0\n        self.num_heads = self.total_num_heads // tp_world_size\n\n        self.query_key_value = QKVParallelLinear(\n            self.hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.dense = RowParallelLinear(\n            self.hidden_size,\n            self.hidden_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n\n        # Create the alibi slopes and slice them.\n        tp_rank = get_tensor_model_parallel_rank()\n        head_start = tp_rank * self.num_heads\n        head_end = (tp_rank + 1) * self.num_heads\n        alibi_slopes = _get_alibi_slopes(self.total_num_heads)\n        alibi_slopes = alibi_slopes[head_start:head_end].tolist()\n\n        scaling = self.head_dim**-0.5\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   scaling,\n                                   alibi_slopes=alibi_slopes)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        del position_ids  # Unused.\n        qkv, _ = self.query_key_value(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.dense(attn_output)\n        return output\n\n\nclass BloomMLP(nn.Module):\n\n    def __init__(\n        self,\n        config: BloomConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        self.dense_h_to_4h = ColumnParallelLinear(\n            hidden_size,\n            4 * hidden_size,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.gelu_impl = get_act_fn(\"gelu\", quant_config, 4 * hidden_size)\n        self.dense_4h_to_h = RowParallelLinear(\n            4 * hidden_size,\n            hidden_size,\n            linear_method=linear_method,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x, _ = self.dense_h_to_4h(x)\n        x = self.gelu_impl(x)\n        x, _ = self.dense_4h_to_h(x)\n        return x\n\n\nclass BloomBlock(nn.Module):\n\n    def __init__(\n        self,\n        config: BloomConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n\n        self.input_layernorm = nn.LayerNorm(hidden_size,\n                                            eps=config.layer_norm_epsilon)\n        self.self_attention = BloomAttention(config, linear_method)\n        self.post_attention_layernorm = nn.LayerNorm(\n            hidden_size, eps=config.layer_norm_epsilon)\n        self.mlp = BloomMLP(config, linear_method)\n        self.apply_residual_connection_post_layernorm = (\n            config.apply_residual_connection_post_layernorm)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        # Layer norm at the beginning of the transformer layer.\n        layernorm_output = self.input_layernorm(hidden_states)\n\n        # Layer norm post the self attention.\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = hidden_states\n\n        # Self attention.\n        attention_output = self.self_attention(\n            position_ids=position_ids,\n            hidden_states=layernorm_output,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        attention_output = attention_output + residual\n        layernorm_output = self.post_attention_layernorm(attention_output)\n\n        # Get residual\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = attention_output\n\n        # MLP.\n        output = self.mlp(layernorm_output) + residual\n        return output\n\n\nclass BloomModel(nn.Module):\n\n    def __init__(\n        self,\n        config: BloomConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n\n        # Embedding + LN Embedding\n        self.word_embeddings = VocabParallelEmbedding(\n            config.vocab_size,\n            self.embed_dim,\n        )\n        self.word_embeddings_layernorm = nn.LayerNorm(\n            self.embed_dim, eps=config.layer_norm_epsilon)\n\n        # Transformer blocks\n        self.h = nn.ModuleList([\n            BloomBlock(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n\n        # Final Layer Norm\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.word_embeddings(input_ids)\n        hidden_states = self.word_embeddings_layernorm(hidden_states)\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states = layer(\n                position_ids,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.ln_f(hidden_states)\n        return hidden_states\n\n\nclass BloomForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: BloomConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.transformer = BloomModel(config, linear_method)\n        self.lm_head_weight = self.transformer.word_embeddings.weight\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if name == \"lm_head.weight\":\n                continue\n            if not name.startswith(\"transformer.\"):\n                name = \"transformer.\" + name\n            param = params_dict[name]\n\n            if \"query_key_value\" in name:\n                # NOTE: BLOOM's fused QKV's output_dim has the shape of\n                # (num_heads * 3 * head_size), while the\n                # required shape is (3 * num_heads * head_size).\n                # Thus, we need weight conversion.\n                output_dim = getattr(param, \"output_dim\", None)\n                num_heads = self.config.num_attention_heads\n                if output_dim is not None:\n                    loaded_weight_shape = loaded_weight.shape\n                    loaded_weight = loaded_weight.view(\n                        loaded_weight_shape[:output_dim] + (num_heads, 3, -1) +\n                        loaded_weight_shape[output_dim + 1:])\n                    loaded_weight = loaded_weight.transpose(\n                        output_dim, output_dim + 1)\n                    loaded_weight = loaded_weight.reshape(loaded_weight_shape)\n\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/bloom.py b/vllm/model_executor/models/bloom.py\nindex 6d1aeeed7..4adfb6b78 100644\n--- a/vllm/model_executor/models/bloom.py\n+++ b/vllm/model_executor/models/bloom.py\n@@ -290,7 +290,7 @@ class BloomForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/chatglm.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/THUDM/ChatGLM2-6B\n\"\"\"Inference-only ChatGLM model compatible with THUDM weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import LayerNorm\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs import ChatGLMConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass GLMAttention(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = config.num_attention_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.multi_query_attention = config.multi_query_attention\n        self.total_num_kv_heads = (config.multi_query_group_num\n                                   if config.multi_query_attention else\n                                   config.num_attention_heads)\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = config.hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n\n        self.query_key_value = QKVParallelLinear(\n            self.hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=config.add_bias_linear or config.add_qkv_bias,\n            linear_method=linear_method,\n        )\n        self.dense = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            config.hidden_size,\n            bias=config.add_bias_linear,\n            linear_method=linear_method,\n        )\n\n        # https://huggingface.co/THUDM/chatglm3-6b-32k/blob/e210410255278dd9d74463cf396ba559c0ef801c/modeling_chatglm.py#L141\n        rope_ratio = getattr(config, \"rope_ratio\", 1.0)\n        max_positions = getattr(config, \"seq_length\", 8192)\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim // 2,\n            max_position=max_positions,\n            base=10000 * rope_ratio,\n            is_neox_style=False,\n        )\n        self.attn = PagedAttention(\n            self.num_heads,\n            self.head_dim,\n            self.scaling,\n            num_kv_heads=self.num_kv_heads,\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.query_key_value(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(position_ids, q, k)\n        key_cache, value_cache = kv_cache\n        context_layer = self.attn(\n            q,\n            k,\n            v,\n            key_cache,\n            value_cache,\n            input_metadata,\n        )\n        attn_output, _ = self.dense(context_layer)\n        return attn_output\n\n\nclass GLMMLP(nn.Module):\n    \"\"\"MLP.\n\n    MLP will take the input with h hidden state, project it to 4*h\n    hidden dimension, perform nonlinear transformation, and project the\n    state back into h hidden dimension.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n\n        self.add_bias = config.add_bias_linear\n\n        # Project to 4h.\n        self.dense_h_to_4h = MergedColumnParallelLinear(\n            config.hidden_size,\n            [config.ffn_hidden_size] * 2,\n            bias=config.add_bias_linear,\n            linear_method=linear_method,\n        )\n\n        self.activation_func = SiluAndMul()\n\n        # Project back to h.\n        self.dense_4h_to_h = RowParallelLinear(\n            config.ffn_hidden_size,\n            config.hidden_size,\n            bias=config.add_bias_linear,\n            linear_method=linear_method,\n        )\n\n    def forward(self, hidden_states):\n        # [s, b, 4hp]\n        intermediate_parallel, _ = self.dense_h_to_4h(hidden_states)\n        intermediate_parallel = self.activation_func(intermediate_parallel)\n        # [s, b, h]\n        output, _ = self.dense_4h_to_h(intermediate_parallel)\n        return output\n\n\nclass GLMBlock(nn.Module):\n    \"\"\"A single transformer layer.\n\n    Transformer layer takes input with size [s, b, h] and returns an\n    output of the same size.\n    \"\"\"\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.apply_residual_connection_post_layernorm = (\n            config.apply_residual_connection_post_layernorm)\n\n        self.fp32_residual_connection = config.fp32_residual_connection\n\n        layer_norm_func = RMSNorm if config.rmsnorm else LayerNorm\n        # Layernorm on the input data.\n        self.input_layernorm = layer_norm_func(config.hidden_size,\n                                               eps=config.layernorm_epsilon)\n\n        # Self attention.\n        self.self_attention = GLMAttention(config, linear_method)\n        self.hidden_dropout = config.hidden_dropout\n\n        # Layernorm on the attention output\n        self.post_attention_layernorm = layer_norm_func(\n            config.hidden_size, eps=config.layernorm_epsilon)\n\n        # MLP\n        self.mlp = GLMMLP(config, linear_method)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        # hidden_states: [num_tokens, h]\n        # Layer norm at the beginning of the transformer layer.\n        layernorm_output = self.input_layernorm(hidden_states)\n        # Self attention.\n        attention_output = self.self_attention(\n            hidden_states=layernorm_output,\n            position_ids=position_ids,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Residual connection.\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = hidden_states\n\n        layernorm_input = residual + attention_output\n\n        # Layer norm post the self attention.\n        layernorm_output = self.post_attention_layernorm(layernorm_input)\n\n        # Second residual connection.\n        if self.apply_residual_connection_post_layernorm:\n            residual = layernorm_output\n        else:\n            residual = layernorm_input\n\n        output = self.mlp(layernorm_output) + residual\n\n        return output\n\n\nclass GLMTransformer(nn.Module):\n    \"\"\"Transformer class.\"\"\"\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.post_layer_norm = config.post_layer_norm\n\n        # Number of layers.\n        self.num_layers = config.num_layers\n\n        # Transformer layers.\n        self.layers = nn.ModuleList(\n            [GLMBlock(config, linear_method) for i in range(self.num_layers)])\n\n        if self.post_layer_norm:\n            layer_norm_func = RMSNorm if config.rmsnorm else LayerNorm\n            # Final layer norm before output.\n            self.final_layernorm = layer_norm_func(\n                config.hidden_size, eps=config.layernorm_epsilon)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        for i in range(self.num_layers):\n            layer = self.layers[i]\n            hidden_states = layer(\n                hidden_states=hidden_states,\n                position_ids=position_ids,\n                kv_cache=kv_caches[i],\n                input_metadata=input_metadata,\n            )\n        # Final layer norm.\n        if self.post_layer_norm:\n            hidden_states = self.final_layernorm(hidden_states)\n\n        return hidden_states\n\n\nclass ChatGLMModel(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n\n        self.embedding = VocabParallelEmbedding(config.padded_vocab_size,\n                                                config.hidden_size)\n\n        self.num_layers = config.num_layers\n        self.multi_query_group_num = config.multi_query_group_num\n        self.kv_channels = config.kv_channels\n        self.encoder = GLMTransformer(config, linear_method)\n\n        self.output_layer = ParallelLMHead(config.padded_vocab_size,\n                                           config.hidden_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        inputs_embeds = self.embedding(input_ids)\n\n        # Run encoder.\n        hidden_states = self.encoder(\n            hidden_states=inputs_embeds,\n            position_ids=position_ids,\n            kv_caches=kv_caches,\n            input_metadata=input_metadata,\n        )\n        return hidden_states\n\n\nclass ChatGLMForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: ChatGLMConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config: ChatGLMConfig = config\n        self.linear_method = linear_method\n        self.transformer = ChatGLMModel(config, linear_method)\n        self.lm_head_weight = self.transformer.output_layer.weight\n        self.sampler = Sampler(config.padded_vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_pos_emb.inv_freq\" in name:\n                continue\n            if \"word_embeddings\" in name:\n                name = name.replace(\".word_embeddings\", \"\")\n            # Skip loading extra bias for GPTQ models.\n            if name.endswith(\".bias\") and name not in params_dict:\n                continue\n            param = params_dict[name]\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/chatglm.py b/vllm/model_executor/models/chatglm.py\nindex aa957b36b..dca8d724f 100644\n--- a/vllm/model_executor/models/chatglm.py\n+++ b/vllm/model_executor/models/chatglm.py\n@@ -349,7 +349,7 @@ class ChatGLMForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/falcon.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/a5cc30d72ae2dc19af534e4b35c986cc28db1275/src/transformers/models/falcon/modeling_falcon.py\n# Copyright 2023 The vLLM team.\n# Copyright 2023 the Falcon authors and HuggingFace Inc. team.  All rights\n# reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch Falcon model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import LayerNorm\nfrom transformers import FalconConfig as HF_FalconConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.communication_op import (\n    tensor_model_parallel_all_reduce)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs import RWConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\nFalconConfig = Union[HF_FalconConfig, RWConfig]\n\n\ndef _get_alibi_slopes(total_num_heads: int) -> torch.Tensor:\n    closest_power_of_2 = 2**math.floor(math.log2(total_num_heads))\n    base = torch.tensor(2**(-(2**-(math.log2(closest_power_of_2) - 3))),\n                        dtype=torch.float32)\n    powers = torch.arange(1, 1 + closest_power_of_2, dtype=torch.int32)\n    slopes = torch.pow(base, powers)\n\n    if closest_power_of_2 != total_num_heads:\n        extra_base = torch.tensor(\n            2**(-(2**-(math.log2(2 * closest_power_of_2) - 3))),\n            dtype=torch.float32)\n        num_remaining_heads = min(closest_power_of_2,\n                                  total_num_heads - closest_power_of_2)\n        extra_powers = torch.arange(1,\n                                    1 + 2 * num_remaining_heads,\n                                    2,\n                                    dtype=torch.int32)\n        slopes = torch.cat(\n            [slopes, torch.pow(extra_base, extra_powers)], dim=0)\n\n    return slopes\n\n\nclass FalconAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: FalconConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n\n        self.hidden_size = config.hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n\n        self.total_num_heads = config.num_attention_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.head_dim = self.hidden_size // self.total_num_heads\n        assert self.head_dim * self.total_num_heads == self.hidden_size\n\n        self.new_decoder_architecture = config.new_decoder_architecture\n        self.multi_query = config.multi_query\n\n        if self.new_decoder_architecture:\n            self.total_num_kv_heads = config.num_kv_heads\n        elif self.multi_query:\n            self.total_num_kv_heads = 1\n        else:\n            self.total_num_kv_heads = self.total_num_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n\n        self.query_key_value = QKVParallelLinear(\n            self.hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=config.bias,\n            skip_bias_add=True,\n            linear_method=linear_method,\n        )\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n\n        # Layer-wise attention scaling\n        self.inv_norm_factor = 1.0 / math.sqrt(self.head_dim)\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\n                                                or config.parallel_attn)\n        self.dense = RowParallelLinear(\n            self.hidden_size,\n            self.hidden_size,\n            bias=config.bias,\n            skip_bias_add=True,\n            linear_method=linear_method,\n            reduce_results=self.reduce_row_parallel_results)\n\n        self.use_rotary = config.rotary\n        self.use_alibi = config.alibi\n        assert not (self.use_rotary and self.use_alibi), (\n            \"Rotary and alibi are mutually exclusive.\")\n\n        if self.use_rotary:\n            rope_theta = getattr(config, \"rope_theta\", 10000)\n            max_position_embeddings = getattr(config,\n                                              \"max_position_embeddings\", 8192)\n            self.rotary_emb = get_rope(\n                self.head_dim,\n                rotary_dim=self.head_dim,\n                max_position=max_position_embeddings,\n                base=rope_theta,\n            )\n            self.attn = PagedAttention(self.num_heads,\n                                       self.head_dim,\n                                       self.inv_norm_factor,\n                                       num_kv_heads=self.num_kv_heads)\n        elif self.use_alibi:\n            tp_rank = get_tensor_model_parallel_rank()\n            head_start = tp_rank * self.num_heads\n            head_end = (tp_rank + 1) * self.num_heads\n            alibi_slopes = (_get_alibi_slopes(self.total_num_heads) *\n                            self.inv_norm_factor)\n            alibi_slopes = alibi_slopes[head_start:head_end].tolist()\n            self.attn = PagedAttention(self.num_heads,\n                                       self.head_dim,\n                                       self.inv_norm_factor,\n                                       num_kv_heads=self.num_kv_heads,\n                                       alibi_slopes=alibi_slopes)\n        else:\n            self.attn = PagedAttention(self.num_heads,\n                                       self.head_dim,\n                                       scale=self.inv_norm_factor,\n                                       num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, bias = self.query_key_value(hidden_states)\n        if bias is not None:\n            qkv += bias\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        if self.use_rotary:\n            q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        attn_output, bias = self.dense(attn_output)\n        return attn_output, bias\n\n\nclass FalconMLP(nn.Module):\n\n    def __init__(\n        self,\n        config: FalconConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n\n        self.dense_h_to_4h = ColumnParallelLinear(hidden_size,\n                                                  4 * hidden_size,\n                                                  bias=config.bias,\n                                                  skip_bias_add=True,\n                                                  linear_method=linear_method)\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(\"gelu\", quant_config, 4 * hidden_size)\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\n                                                or config.parallel_attn)\n        self.dense_4h_to_h = RowParallelLinear(\n            4 * hidden_size,\n            hidden_size,\n            bias=config.bias,\n            skip_bias_add=True,\n            reduce_results=self.reduce_row_parallel_results,\n            linear_method=linear_method)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # NOTE(zhuohan): Following huggingface, we do not fuse bias add here.\n        x, bias = self.dense_h_to_4h(x)\n        if bias is not None:\n            x += bias\n        x = self.act(x)\n        x, bias = self.dense_4h_to_h(x)\n        return x, bias\n\n\nclass FalconDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: FalconConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.self_attention = FalconAttention(config, linear_method)\n        self.mlp = FalconMLP(config, linear_method)\n        self.config = config\n\n        if config.new_decoder_architecture:\n            # The layer norm before self-attention\n            self.ln_attn = LayerNorm(hidden_size,\n                                     eps=config.layer_norm_epsilon)\n            # The layer norm before the MLP\n            self.ln_mlp = LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        else:\n            self.input_layernorm = LayerNorm(hidden_size,\n                                             eps=config.layer_norm_epsilon)\n            if not config.parallel_attn:\n                self.post_attention_layernorm = LayerNorm(\n                    hidden_size, eps=config.layer_norm_epsilon)\n\n        self.reduce_row_parallel_results = not (config.new_decoder_architecture\n                                                or config.parallel_attn)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        residual = hidden_states\n\n        if self.config.new_decoder_architecture:\n            attention_layernorm_out = self.ln_attn(hidden_states)\n            mlp_layernorm_out = self.ln_mlp(hidden_states)\n        else:\n            attention_layernorm_out = self.input_layernorm(hidden_states)\n\n        # Self attention.\n        attention_output, attention_bias = self.self_attention(\n            positions=positions,\n            hidden_states=attention_layernorm_out,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        if self.reduce_row_parallel_results and attention_bias is not None:\n            attention_output += attention_bias\n\n        if not self.config.new_decoder_architecture:\n            if self.config.parallel_attn:\n                mlp_layernorm_out = attention_layernorm_out\n            else:\n                residual += attention_output\n                mlp_layernorm_out = self.post_attention_layernorm(residual)\n\n        # MLP.\n        mlp_output, mlp_bias = self.mlp(mlp_layernorm_out)\n        if self.reduce_row_parallel_results and mlp_bias is not None:\n            mlp_output += mlp_bias\n\n        if not self.reduce_row_parallel_results:\n            # When MLP and Attention layers are parallel, we can use\n            # only one all-reduce operator to reduce the results from\n            # both MLP and Attention layers.\n            mlp_output += attention_output\n            mlp_output = tensor_model_parallel_all_reduce(mlp_output)\n            if attention_bias is not None:\n                mlp_output += attention_bias\n            if mlp_bias is not None:\n                mlp_output += mlp_bias\n\n        output = mlp_output + residual\n        return output\n\n\nclass FalconModel(nn.Module):\n\n    def __init__(\n        self,\n        config: FalconConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.use_alibi = config.alibi\n\n        # Embedding + LN Embedding\n        self.word_embeddings = VocabParallelEmbedding(\n            config.vocab_size,\n            self.embed_dim,\n        )\n\n        # Transformer blocks\n        self.h = nn.ModuleList([\n            FalconDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n\n        # Final Layer Norm\n        self.ln_f = LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.word_embeddings(input_ids)\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.ln_f(hidden_states)\n        return hidden_states\n\n\nclass FalconForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: FalconConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.transformer = FalconModel(config, linear_method)\n        self.lm_head = ParallelLMHead(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(\n            input_ids,\n            positions,\n            kv_caches,\n            input_metadata,\n        )\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        total_num_heads = self.config.num_attention_heads\n        if self.config.new_decoder_architecture:\n            total_num_kv_heads = self.config.num_kv_heads\n        elif self.config.multi_query:\n            total_num_kv_heads = 1\n        else:\n            total_num_kv_heads = total_num_heads\n        num_query_heads_per_kv_head = total_num_heads // total_num_kv_heads\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            # Skip loading extra bias for GPTQ models.\n            if name.endswith(\".bias\") and name not in params_dict:\n                continue\n            param = params_dict[name]\n            if \"query_key_value\" in name:\n                output_dim = getattr(param, \"output_dim\", None)\n                loaded_weight_shape = loaded_weight.shape\n                if output_dim is not None:\n                    loaded_weight = loaded_weight.view(\n                        loaded_weight_shape[:output_dim] +\n                        (total_num_kv_heads, num_query_heads_per_kv_head + 2,\n                         -1) + loaded_weight_shape[output_dim + 1:])\n                    wq = loaded_weight.narrow(\n                        output_dim + 1, 0,\n                        num_query_heads_per_kv_head).reshape(\n                            *loaded_weight_shape[:output_dim], -1,\n                            *loaded_weight_shape[output_dim + 1:])\n                    wk = loaded_weight.narrow(\n                        output_dim + 1, num_query_heads_per_kv_head,\n                        1).reshape(*loaded_weight_shape[:output_dim], -1,\n                                   *loaded_weight_shape[output_dim + 1:])\n                    wv = loaded_weight.narrow(\n                        output_dim + 1, num_query_heads_per_kv_head + 1,\n                        1).reshape(*loaded_weight_shape[:output_dim], -1,\n                                   *loaded_weight_shape[output_dim + 1:])\n                    loaded_weight = torch.cat([wq, wk, wv], dim=output_dim)\n\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/falcon.py b/vllm/model_executor/models/falcon.py\nindex 7055d0852..2b5e02231 100644\n--- a/vllm/model_executor/models/falcon.py\n+++ b/vllm/model_executor/models/falcon.py\n@@ -394,7 +394,7 @@ class FalconForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/gpt2.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt2/modeling_gpt2.py\n# Copyright 2023 The vLLM team.\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only GPT-2 model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import GPT2Config\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass GPT2Attention(nn.Module):\n\n    def __init__(\n        self,\n        config: GPT2Config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        total_num_heads = config.num_attention_heads\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        assert total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = total_num_heads // tensor_model_parallel_world_size\n        self.head_dim = self.hidden_size // total_num_heads\n        self.scale = self.head_dim**-0.5\n\n        self.c_attn = QKVParallelLinear(\n            self.hidden_size,\n            self.head_dim,\n            total_num_heads,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.c_proj = RowParallelLinear(\n            self.hidden_size,\n            self.hidden_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   scale=self.scale)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.c_attn(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        key_cache, value_cache = kv_cache\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\n                                input_metadata)\n        attn_output, _ = self.c_proj(attn_output)\n        return attn_output\n\n\nclass GPT2MLP(nn.Module):\n\n    def __init__(\n        self,\n        intermediate_size: int,\n        config: GPT2Config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        self.c_fc = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.c_proj = RowParallelLinear(\n            intermediate_size,\n            hidden_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(config.activation_function, quant_config,\n                              intermediate_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states, _ = self.c_fc(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states, _ = self.c_proj(hidden_states)\n        return hidden_states\n\n\nclass GPT2Block(nn.Module):\n\n    def __init__(\n        self,\n        config: GPT2Config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        inner_dim = (config.n_inner if config.n_inner is not None else 4 *\n                     hidden_size)\n\n        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.attn = GPT2Attention(config, linear_method)\n        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.mlp = GPT2MLP(inner_dim, config, linear_method)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_output = self.attn(\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        # residual connection\n        hidden_states = attn_output + residual\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n        return hidden_states\n\n\nclass GPT2Model(nn.Module):\n\n    def __init__(\n        self,\n        config: GPT2Config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        assert not config.add_cross_attention\n        assert not config.scale_attn_by_inverse_layer_idx\n        assert not config.reorder_and_upcast_attn\n        self.embed_dim = config.hidden_size\n        self.wte = VocabParallelEmbedding(config.vocab_size, self.embed_dim)\n        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n        self.h = nn.ModuleList([\n            GPT2Block(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n        hidden_states = inputs_embeds + position_embeds\n\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata)\n\n        hidden_states = self.ln_f(hidden_states)\n        return hidden_states\n\n\nclass GPT2LMHeadModel(nn.Module):\n\n    def __init__(\n        self,\n        config: GPT2Config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.transformer = GPT2Model(config, linear_method)\n        self.lm_head_weight = self.transformer.wte.weight\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"lm_head.weight\" in name:\n                # GPT-2 ties the weights of the embedding layer and the final\n                # linear layer.\n                continue\n            if \".attn.bias\" in name or \".attn.masked_bias\" in name:\n                # Skip attention mask.\n                # NOTE: \"c_attn.bias\" should not be skipped.\n                continue\n            if not name.startswith(\"transformer.\"):\n                name = \"transformer.\" + name\n            param = params_dict[name]\n            # The HF's GPT-2 implementation uses Conv1D instead of Linear.\n            # Because of this, we need to transpose the weights.\n            # Note(zhuohan): the logic below might break quantized models.\n            for conv1d_weight_name in [\"c_attn\", \"c_proj\", \"c_fc\"]:\n                if conv1d_weight_name not in name:\n                    continue\n                if not name.endswith(\".weight\"):\n                    continue\n                loaded_weight = loaded_weight.t()\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/gpt2.py b/vllm/model_executor/models/gpt2.py\nindex d9b561cd8..661da0fe0 100644\n--- a/vllm/model_executor/models/gpt2.py\n+++ b/vllm/model_executor/models/gpt2.py\n@@ -235,7 +235,7 @@ class GPT2LMHeadModel(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/gpt_bigcode.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt2/modeling_gpt2.py\n# Copyright 2023 The vLLM team.\n# Copyright 2023 CTranslate2, and Michael Feil\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only GPTBigCode model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import GPTBigCodeConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass GPTBigCodeAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTBigCodeConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        total_num_heads = config.num_attention_heads\n        self.tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        assert total_num_heads % self.tensor_model_parallel_world_size == 0\n        self.num_heads = (total_num_heads //\n                          self.tensor_model_parallel_world_size)\n        self.head_dim = self.hidden_size // total_num_heads\n        self.scale = self.head_dim**-0.5\n\n        self.multi_query = config.multi_query\n        if self.multi_query:\n            total_num_kv_heads = 1\n            self.num_kv_heads = 1\n        else:\n            total_num_kv_heads = total_num_heads\n            self.num_kv_heads = self.num_heads\n        self.kv_dim = self.head_dim * self.num_kv_heads\n        self.c_attn = QKVParallelLinear(\n            self.hidden_size,\n            self.head_dim,\n            total_num_heads,\n            total_num_kv_heads,\n            bias=True,\n            linear_method=linear_method,\n        )\n\n        self.c_proj = RowParallelLinear(\n            self.hidden_size,\n            self.hidden_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   scale=self.scale,\n                                   num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.c_attn(hidden_states)\n        q, k, v = qkv.split(\n            [\n                self.hidden_size // self.tensor_model_parallel_world_size,\n                self.kv_dim, self.kv_dim\n            ],\n            dim=-1,\n        )\n        key_cache, value_cache = kv_cache\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\n                                input_metadata)\n        attn_output, _ = self.c_proj(attn_output)\n        return attn_output\n\n\nclass GPTBigMLP(nn.Module):\n\n    def __init__(\n        self,\n        intermediate_size: int,\n        config: GPTBigCodeConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        self.c_fc = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.c_proj = RowParallelLinear(\n            intermediate_size,\n            hidden_size,\n            bias=True,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(config.activation_function, quant_config,\n                              intermediate_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states, _ = self.c_fc(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states, _ = self.c_proj(hidden_states)\n        return hidden_states\n\n\nclass GPTBigCodeBlock(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTBigCodeConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.hidden_size\n        inner_dim = (config.n_inner if config.n_inner is not None else 4 *\n                     hidden_size)\n\n        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.attn = GPTBigCodeAttention(config, linear_method)\n        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n        self.mlp = GPTBigMLP(inner_dim, config, linear_method)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_output = self.attn(\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        # residual connection\n        hidden_states = attn_output + residual\n\n        residual = hidden_states\n        hidden_states = self.ln_2(hidden_states)\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        # residual connection\n        hidden_states = residual + feed_forward_hidden_states\n        return hidden_states\n\n\nclass GPTBigCodeModel(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTBigCodeConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        assert not config.add_cross_attention\n\n        self.embed_dim = config.hidden_size\n\n        self.wte = VocabParallelEmbedding(config.vocab_size, self.embed_dim)\n        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n        self.h = nn.ModuleList([\n            GPTBigCodeBlock(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n        hidden_states = inputs_embeds + position_embeds\n\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata)\n\n        hidden_states = self.ln_f(hidden_states)\n        return hidden_states\n\n\nclass GPTBigCodeForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTBigCodeConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.transformer = GPTBigCodeModel(config, linear_method)\n        self.lm_head_weight = self.transformer.wte.weight\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"lm_head.weight\" in name:\n                continue\n            if \".attn.bias\" in name:\n                # Skip attention mask.\n                # NOTE: \"c_attn.bias\" should not be skipped.\n                continue\n            param = params_dict[name]\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/gpt_bigcode.py b/vllm/model_executor/models/gpt_bigcode.py\nindex 4d8144bad..ef4c1d414 100644\n--- a/vllm/model_executor/models/gpt_bigcode.py\n+++ b/vllm/model_executor/models/gpt_bigcode.py\n@@ -254,7 +254,7 @@ class GPTBigCodeForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/gpt_j.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gptj/modeling_gptj.py\n# Copyright 2023 The vLLM team.\n# Copyright 2021 The EleutherAI and HuggingFace Teams. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only GPT-J model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import GPTJConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass GPTJAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTJConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.total_num_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.head_size = self.hidden_size // self.total_num_heads\n\n        self.qkv_proj = QKVParallelLinear(\n            config.hidden_size,\n            self.head_size,\n            self.total_num_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.out_proj = RowParallelLinear(\n            config.hidden_size,\n            config.hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n\n        tp_world_size = get_tensor_model_parallel_world_size()\n        assert self.total_num_heads % tp_world_size == 0\n        self.num_heads = self.total_num_heads // tp_world_size\n\n        scaling = self.head_size**-0.5\n        assert getattr(config, \"rotary\", True)\n        assert config.rotary_dim % 2 == 0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.rotary_emb = get_rope(\n            self.head_size,\n            rotary_dim=config.rotary_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n            is_neox_style=False,\n        )\n        self.attn = PagedAttention(self.num_heads, self.head_size, scaling)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        q, k = self.rotary_emb(position_ids, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        attn_output, _ = self.out_proj(attn_output)\n        return attn_output\n\n\nclass GPTJMLP(nn.Module):\n\n    def __init__(\n        self,\n        intermediate_size: int,\n        config: GPTJConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.n_embd\n        self.fc_in = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            linear_method=linear_method,\n        )\n        self.fc_out = RowParallelLinear(\n            intermediate_size,\n            hidden_size,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(config.activation_function, quant_config,\n                              intermediate_size)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        hidden_states, _ = self.fc_in(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states, _ = self.fc_out(hidden_states)\n        return hidden_states\n\n\nclass GPTJBlock(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTJConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        inner_dim = 4 * config.n_embd if config.n_inner is None else config.n_inner\n        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n        self.attn = GPTJAttention(config, linear_method)\n        self.mlp = GPTJMLP(inner_dim, config, linear_method)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        residual = hidden_states\n        hidden_states = self.ln_1(hidden_states)\n        attn_output = self.attn(\n            position_ids=position_ids,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        mlp_output = self.mlp(hidden_states)\n        hidden_states = attn_output + mlp_output + residual\n        return hidden_states\n\n\nclass GPTJModel(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTJConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.n_embd\n        self.wte = VocabParallelEmbedding(\n            config.vocab_size,\n            self.embed_dim,\n        )\n        self.h = nn.ModuleList(\n            [GPTJBlock(config, linear_method) for _ in range(config.n_layer)])\n        self.ln_f = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.wte(input_ids)\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states = layer(\n                position_ids,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.ln_f(hidden_states)\n        return hidden_states\n\n\nclass GPTJForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTJConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        assert not config.tie_word_embeddings\n        self.transformer = GPTJModel(config, linear_method)\n        self.lm_head = ParallelLMHead(\n            config.vocab_size,\n            config.n_embd,\n            bias=True,\n        )\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata, self.lm_head.bias)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"attn.bias\" in name or \"attn.masked_bias\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/gpt_j.py b/vllm/model_executor/models/gpt_j.py\nindex ab3480a77..5bab30d9d 100644\n--- a/vllm/model_executor/models/gpt_j.py\n+++ b/vllm/model_executor/models/gpt_j.py\n@@ -240,7 +240,7 @@ class GPTJForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata, self.lm_head.bias)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/gpt_neox.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/gpt_neox/modeling_gpt_neox.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only GPT-NeoX model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import GPTNeoXConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass GPTNeoXAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTNeoXConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.total_num_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.head_size = self.hidden_size // self.total_num_heads\n        self.bias = getattr(config, \"attention_bias\", True)\n\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = (self.total_num_heads //\n                          tensor_model_parallel_world_size)\n\n        self.query_key_value = QKVParallelLinear(\n            config.hidden_size,\n            self.head_size,\n            self.total_num_heads,\n            bias=self.bias,\n            linear_method=linear_method,\n        )\n        self.dense = RowParallelLinear(\n            config.hidden_size,\n            config.hidden_size,\n            bias=self.bias,\n            linear_method=linear_method,\n        )\n        scaling = self.head_size**-0.5\n        rotary_dim = int(self.head_size * config.rotary_pct)\n        assert rotary_dim % 2 == 0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.rotary_emb = get_rope(\n            self.head_size,\n            rotary_dim=rotary_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n        )\n        self.attn = PagedAttention(self.num_heads, self.head_size, scaling)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.query_key_value(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        q, k = self.rotary_emb(position_ids, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.dense(attn_output)\n        return output\n\n\nclass GPTNeoXMLP(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTNeoXConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.dense_h_to_4h = ColumnParallelLinear(\n            config.hidden_size,\n            config.intermediate_size,\n            linear_method=linear_method,\n        )\n        self.dense_4h_to_h = RowParallelLinear(\n            config.intermediate_size,\n            config.hidden_size,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(config.hidden_act, quant_config,\n                              config.intermediate_size)\n\n    def forward(self, hidden_states):\n        hidden_states, _ = self.dense_h_to_4h(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states, _ = self.dense_4h_to_h(hidden_states)\n        return hidden_states\n\n\nclass GPTNeoXLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTNeoXConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.use_parallel_residual = config.use_parallel_residual\n        self.input_layernorm = nn.LayerNorm(config.hidden_size,\n                                            eps=config.layer_norm_eps)\n        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size,\n                                                     eps=config.layer_norm_eps)\n        self.attention = GPTNeoXAttention(config, linear_method)\n        self.mlp = GPTNeoXMLP(config, linear_method)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        attn_input = self.input_layernorm(hidden_states)\n        attn_output = self.attention(\n            position_ids=position_ids,\n            hidden_states=attn_input,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        if self.use_parallel_residual:\n            # pseudocode:\n            # x = x + attn(ln1(x)) + mlp(ln2(x))\n            mlp_input = self.post_attention_layernorm(hidden_states)\n            mlp_output = self.mlp(mlp_input)\n            hidden_states = mlp_output + attn_output + hidden_states\n        else:\n            # pseudocode:\n            # x = x + attn(ln1(x))\n            # x = x + mlp(ln2(x))\n            attn_output = attn_output + hidden_states\n            mlp_input = self.post_attention_layernorm(attn_output)\n            mlp_output = self.mlp(mlp_input)\n            hidden_states = mlp_output + attn_output\n        return hidden_states\n\n\nclass GPTNeoXModel(nn.Module):\n\n    def __init__(\n        self,\n        config: GPTNeoXConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n\n        self.embed_in = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            GPTNeoXLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.final_layer_norm = nn.LayerNorm(config.hidden_size,\n                                             eps=config.layer_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_in(input_ids)\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states = layer(\n                position_ids,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.final_layer_norm(hidden_states)\n        return hidden_states\n\n\nclass GPTNeoXForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.gpt_neox = GPTNeoXModel(config, linear_method)\n        self.embed_out = ParallelLMHead(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.gpt_neox(input_ids, positions, kv_caches,\n                                      input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.embed_out.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if (\"attention.bias\" in name or \"attention.masked_bias\" in name\n                    or \"rotary_emb.inv_freq\" in name):\n                continue\n            param = params_dict[name]\n\n            if \"query_key_value\" in name:\n                # NOTE: GPT-NeoX's fused QKV's output_dim has the shape of\n                # (num_heads * 3 * head_size), while the\n                # required shape is (3 * num_heads * head_size).\n                # Thus, we need weight conversion.\n                output_dim = getattr(param, \"output_dim\", None)\n                num_heads = self.config.num_attention_heads\n                if output_dim is not None:\n                    loaded_weight_shape = loaded_weight.shape\n                    loaded_weight = loaded_weight.view(\n                        loaded_weight_shape[:output_dim] + (num_heads, 3, -1) +\n                        loaded_weight_shape[output_dim + 1:])\n                    loaded_weight = loaded_weight.transpose(\n                        output_dim, output_dim + 1)\n                    loaded_weight = loaded_weight.reshape(loaded_weight_shape)\n\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/gpt_neox.py b/vllm/model_executor/models/gpt_neox.py\nindex 773fed36a..8f7e1063e 100644\n--- a/vllm/model_executor/models/gpt_neox.py\n+++ b/vllm/model_executor/models/gpt_neox.py\n@@ -255,7 +255,7 @@ class GPTNeoXForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.embed_out.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/internlm.py",
      "old_content": "# -*- coding: utf-8 -*-\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import LlamaConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass InternLMMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass InternLMAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        bias: bool,\n        rope_theta: float = 10000,\n        max_position_embeddings: int = 8192,\n        linear_method: Optional[LinearMethodBase] = None,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = (self.total_num_heads //\n                          tensor_model_parallel_world_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            bias=bias,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=bias,\n            linear_method=linear_method,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=self.max_position_embeddings,\n            base=self.rope_theta,\n            rope_scaling=rope_scaling,\n        )\n        self.attn = PagedAttention(self.num_heads, self.head_dim, self.scaling)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass InternLMDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.self_attn = InternLMAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            bias=config.bias,\n            rope_theta=rope_theta,\n            max_position_embeddings=max_position_embeddings,\n            linear_method=linear_method,\n            rope_scaling=getattr(config, \"rope_scaling\", None),\n        )\n        self.mlp = InternLMMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass InternLMModel(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        vocab_size = ((config.vocab_size + 63) // 64) * 64\n        self.embed_tokens = VocabParallelEmbedding(\n            vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            InternLMDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass InternLMForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = InternLMModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/internlm.py b/vllm/model_executor/models/internlm.py\nindex 00bb70fc3..5d0b93793 100644\n--- a/vllm/model_executor/models/internlm.py\n+++ b/vllm/model_executor/models/internlm.py\n@@ -255,7 +255,7 @@ class InternLMForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/llama.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only LLaMA model compatible with HuggingFace weights.\"\"\"\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import LlamaConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass LlamaMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass LlamaAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        num_kv_heads: int,\n        rope_theta: float = 10000,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        max_position_embeddings: int = 8192,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n            rope_scaling=rope_scaling,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   self.scaling,\n                                   num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass LlamaDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.self_attn = LlamaAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            rope_scaling=rope_scaling,\n            max_position_embeddings=max_position_embeddings,\n            linear_method=linear_method,\n        )\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass LlamaModel(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            LlamaDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass LlamaForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: LlamaConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = LlamaModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            if (\"rotary_emb.cos_cached\" in name\n                    or \"rotary_emb.sin_cached\" in name):\n                # Models trained using ColossalAI may include these tensors in\n                # the checkpoint. Skip them.\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/llama.py b/vllm/model_executor/models/llama.py\nindex b3b24ea6f..3791aa893 100644\n--- a/vllm/model_executor/models/llama.py\n+++ b/vllm/model_executor/models/llama.py\n@@ -291,7 +291,7 @@ class LlamaForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/mistral.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Mistral model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import MistralConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass MistralMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass MistralAttention(nn.Module):\n\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 num_kv_heads: int,\n                 max_position: int = 4096 * 32,\n                 rope_theta: float = 10000,\n                 linear_method: Optional[LinearMethodBase] = None,\n                 sliding_window: Optional[int] = None) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.sliding_window = sliding_window\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position,\n            base=self.rope_theta,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   self.scaling,\n                                   num_kv_heads=self.num_kv_heads,\n                                   sliding_window=self.sliding_window)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass MistralDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: MistralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        # Requires transformers > 4.32.0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        self.self_attn = MistralAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            max_position=config.max_position_embeddings,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            linear_method=linear_method,\n            sliding_window=config.sliding_window)\n        self.mlp = MistralMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass MistralModel(nn.Module):\n\n    def __init__(\n        self,\n        config: MistralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            MistralDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass MistralForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: MistralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = MistralModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/mistral.py b/vllm/model_executor/models/mistral.py\nindex 57230fcce..70d033fec 100644\n--- a/vllm/model_executor/models/mistral.py\n+++ b/vllm/model_executor/models/mistral.py\n@@ -287,7 +287,7 @@ class MistralForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/mixtral.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Mixtral model.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import nn\nfrom transformers import MixtralConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               ReplicatedLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.communication_op import (\n    tensor_model_parallel_all_reduce)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass MixtralMLP(nn.Module):\n\n    def __init__(\n        self,\n        num_experts: int,\n        hidden_size: int,\n        intermediate_size: int,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.num_experts = num_experts\n        self.ffn_dim = intermediate_size\n        self.hidden_dim = hidden_size\n\n        self.w1 = ReplicatedLinear(self.hidden_dim,\n                                   self.ffn_dim,\n                                   bias=False,\n                                   linear_method=linear_method)\n        self.w2 = ReplicatedLinear(self.ffn_dim,\n                                   self.hidden_dim,\n                                   bias=False,\n                                   linear_method=linear_method)\n        self.w3 = ReplicatedLinear(self.hidden_dim,\n                                   self.ffn_dim,\n                                   bias=False,\n                                   linear_method=linear_method)\n\n        # TODO: Use vllm's SiluAndMul\n        self.act_fn = nn.SiLU()\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        w1_out, _ = self.w1(hidden_states)\n        w1_out = self.act_fn(w1_out)\n        w3_out, _ = self.w3(hidden_states)\n        current_hidden_states = w1_out * w3_out\n        current_hidden_states, _ = self.w2(current_hidden_states)\n        return current_hidden_states\n\n\nclass MixtralMoE(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.rank = get_tensor_model_parallel_rank()\n        self.tp_size = get_tensor_model_parallel_world_size()\n        self.num_total_experts = config.num_local_experts\n        self.top_k = config.num_experts_per_tok\n        if self.tp_size > self.num_total_experts:\n            raise ValueError(\n                f\"Tensor parallel size {self.tp_size} is greater than \"\n                f\"the number of experts {self.num_total_experts}.\")\n        # Split experts equally between ranks\n        self.expert_indicies = np.array_split(range(\n            self.num_total_experts), self.tp_size)[self.rank].tolist()\n        if not self.expert_indicies:\n            raise ValueError(\n                f\"Rank {self.rank} has no experts assigned to it.\")\n\n        self.experts = nn.ModuleList([\n            MixtralMLP(self.num_total_experts,\n                       config.hidden_size,\n                       config.intermediate_size,\n                       linear_method=linear_method)\n            if idx in self.expert_indicies else None\n            for idx in range(self.num_total_experts)\n        ])\n        self.gate = ReplicatedLinear(config.hidden_size,\n                                     self.num_total_experts,\n                                     bias=False,\n                                     linear_method=None)\n\n    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n        batch_size, sequence_length, hidden_dim = hidden_states.shape\n        hidden_states = hidden_states.view(-1, hidden_dim)\n        # router_logits: (batch * sequence_length, n_experts)\n        router_logits, _ = self.gate(hidden_states)\n\n        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n        routing_weights, selected_experts = torch.topk(routing_weights,\n                                                       self.top_k,\n                                                       dim=-1)\n        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n\n        final_hidden_states = None\n        for expert_idx in self.expert_indicies:\n            expert_layer = self.experts[expert_idx]\n            expert_mask = (selected_experts == expert_idx)\n            expert_weights = (routing_weights * expert_mask).sum(dim=-1,\n                                                                 keepdim=True)\n\n            current_hidden_states = expert_layer(hidden_states).mul_(\n                expert_weights)\n            if final_hidden_states is None:\n                final_hidden_states = current_hidden_states\n            else:\n                final_hidden_states.add_(current_hidden_states)\n\n        return tensor_model_parallel_all_reduce(final_hidden_states).view(\n            batch_size, sequence_length, hidden_dim)\n\n\nclass MixtralAttention(nn.Module):\n\n    def __init__(self,\n                 hidden_size: int,\n                 num_heads: int,\n                 num_kv_heads: int,\n                 max_position: int = 4096 * 32,\n                 rope_theta: float = 10000,\n                 linear_method: Optional[LinearMethodBase] = None,\n                 sliding_window: Optional[int] = None) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.sliding_window = sliding_window\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position,\n            base=int(self.rope_theta),\n            is_neox_style=True,\n        )\n        self.attn = PagedAttention(\n            self.num_heads,\n            self.head_dim,\n            self.scaling,\n            num_kv_heads=self.num_kv_heads,\n            sliding_window=self.sliding_window,\n        )\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass MixtralDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        # Requires transformers > 4.32.0\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        self.self_attn = MixtralAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            max_position=config.max_position_embeddings,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            sliding_window=config.sliding_window,\n            linear_method=linear_method)\n        self.block_sparse_moe = MixtralMoE(config=config,\n                                           linear_method=linear_method)\n        self.input_layernorm = RMSNorm(config.hidden_size,\n                                       eps=config.rms_norm_eps)\n        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n                                                eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> torch.Tensor:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.input_layernorm(hidden_states)\n        else:\n            hidden_states, residual = self.input_layernorm(\n                hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.post_attention_layernorm(\n            hidden_states, residual)\n        hidden_states = self.block_sparse_moe(hidden_states)\n        return hidden_states, residual\n\n\nclass MixtralModel(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            MixtralDecoderLayer(config, linear_method=linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> SamplerOutput:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(positions, hidden_states,\n                                            kv_caches[i], input_metadata,\n                                            residual)\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass MixtralForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: MixtralConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = MixtralModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: Optional[torch.Tensor],\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n        ]\n\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path,\n                cache_dir,\n                load_format,\n                revision,\n                fall_back_to_pt=False):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                # Skip experts that are not assigned to this worker.\n                if (\"block_sparse_moe.experts.\" in name\n                        and name not in params_dict):\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/mixtral.py b/vllm/model_executor/models/mixtral.py\nindex e61b401a7..a8dadce24 100644\n--- a/vllm/model_executor/models/mixtral.py\n+++ b/vllm/model_executor/models/mixtral.py\n@@ -320,7 +320,7 @@ class MixtralModel(nn.Module):\n         positions: torch.Tensor,\n         kv_caches: List[KVCache],\n         input_metadata: InputMetadata,\n-    ) -> SamplerOutput:\n+    ) -> torch.Tensor:\n         hidden_states = self.embed_tokens(input_ids)\n         residual = None\n         for i in range(len(self.layers)):\n@@ -361,7 +361,7 @@ class MixtralForCausalLM(nn.Module):\n         self,\n         hidden_states: Optional[torch.Tensor],\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 3,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/model_executor/models/mpt.py",
      "old_content": "# coding=utf-8\n# Adapted from https://huggingface.co/mosaicml/mpt-7b/tree/main\nimport math\nfrom typing import List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_rank, get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs.mpt import MPTConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\ndef _get_alibi_slopes(\n    total_num_heads: int,\n    alibi_bias_max: int,\n) -> torch.Tensor:\n    next_power_of_2 = 2**math.ceil(math.log2(total_num_heads))\n    m = torch.arange(1, next_power_of_2 + 1, dtype=torch.float32)\n    m = m.mul(alibi_bias_max / next_power_of_2)\n    slopes = 1.0 / torch.pow(2, m)\n    if next_power_of_2 != total_num_heads:\n        slopes = torch.concat([slopes[1::2], slopes[::2]])[:total_num_heads]\n    return slopes\n\n\nclass MPTAttention(nn.Module):\n\n    def __init__(\n        self,\n        config: MPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.d_model = config.d_model\n        self.total_num_heads = config.n_heads\n        self.head_dim = self.d_model // self.total_num_heads\n        self.clip_qkv = config.attn_config[\"clip_qkv\"]\n        self.qk_ln = config.attn_config[\"qk_ln\"]\n        self.alibi_bias_max = config.attn_config[\"alibi_bias_max\"]\n        if \"kv_n_heads\" in config.attn_config:\n            self.total_num_kv_heads = config.attn_config['kv_n_heads']\n        else:\n            self.total_num_kv_heads = self.total_num_heads\n        assert not config.attn_config[\"prefix_lm\"]\n        assert config.attn_config[\"alibi\"]\n\n        # pylint: disable=invalid-name\n        self.Wqkv = QKVParallelLinear(\n            self.d_model,\n            self.d_model // self.total_num_heads,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=not config.no_bias,\n            linear_method=linear_method,\n        )\n        if self.qk_ln:\n            self.q_ln = nn.LayerNorm(self.d_model)\n            self.k_ln = nn.LayerNorm(self.d_model)\n        self.out_proj = RowParallelLinear(\n            self.d_model,\n            self.d_model,\n            bias=not config.no_bias,\n            linear_method=linear_method,\n        )\n\n        tp_world_size = get_tensor_model_parallel_world_size()\n        assert self.total_num_heads % tp_world_size == 0\n        self.num_heads = self.total_num_heads // tp_world_size\n\n        if self.total_num_kv_heads >= tp_world_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_world_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_world_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_world_size)\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        # Create the alibi slopes and slice them.\n        tp_rank = get_tensor_model_parallel_rank()\n        head_start = tp_rank * self.num_heads\n        head_end = (tp_rank + 1) * self.num_heads\n        alibi_slopes = _get_alibi_slopes(self.total_num_heads,\n                                         self.alibi_bias_max)\n        alibi_slopes = alibi_slopes[head_start:head_end].tolist()\n\n        self.head_dim = self.d_model // self.total_num_heads\n        scaling = self.head_dim**-0.5\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   scaling,\n                                   alibi_slopes=alibi_slopes,\n                                   num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        del position_ids  # unused.\n        qkv, _ = self.Wqkv(hidden_states)\n        if self.clip_qkv is not None:\n            qkv.clamp_(min=-self.clip_qkv, max=self.clip_qkv)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        if self.qk_ln:\n            q = self.q_ln(q)\n            k = self.k_ln(k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.out_proj(attn_output)\n        return output\n\n\nclass MPTMLP(nn.Module):\n\n    def __init__(\n        self,\n        config: MPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.d_model\n        expansion_ratio = config.expansion_ratio\n        intermediate_size = expansion_ratio * hidden_size\n        self.up_proj = ColumnParallelLinear(\n            hidden_size,\n            intermediate_size,\n            bias=not config.no_bias,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(\"gelu\", quant_config, intermediate_size)\n        self.down_proj = RowParallelLinear(\n            intermediate_size,\n            hidden_size,\n            bias=not config.no_bias,\n            linear_method=linear_method,\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x, _ = self.up_proj(x)\n        x = self.act(x)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass MPTBlock(nn.Module):\n\n    def __init__(\n        self,\n        config: MPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        hidden_size = config.d_model\n        self.norm_1 = nn.LayerNorm(hidden_size)\n        self.attn = MPTAttention(config, linear_method)\n        self.norm_2 = nn.LayerNorm(hidden_size)\n        self.ffn = MPTMLP(config, linear_method)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        x = self.norm_1(hidden_states)\n        x = self.attn(\n            position_ids=position_ids,\n            hidden_states=x,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        hidden_states = hidden_states + x\n        x = self.norm_2(hidden_states)\n        x = self.ffn(x)\n        hidden_states = hidden_states + x\n        return hidden_states\n\n\nclass MPTModel(nn.Module):\n\n    def __init__(\n        self,\n        config: MPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        assert config.embedding_fraction == 1.0\n        assert config.norm_type == \"low_precision_layernorm\"\n\n        self.wte = VocabParallelEmbedding(\n            config.vocab_size,\n            config.d_model,\n        )\n        self.blocks = nn.ModuleList(\n            [MPTBlock(config, linear_method) for _ in range(config.n_layers)])\n        self.norm_f = nn.LayerNorm(config.d_model)\n        if config.no_bias:\n            for module in self.modules():\n                if hasattr(module, \"bias\") and isinstance(\n                        module.bias, nn.Parameter):\n                    # Remove the bias term in Linear and LayerNorm.\n                    module.register_parameter(\"bias\", None)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        position_ids: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.wte(input_ids)\n        for i in range(len(self.blocks)):\n            block = self.blocks[i]\n            hidden_states = block(\n                position_ids,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        hidden_states = self.norm_f(hidden_states)\n        return hidden_states\n\n\nclass MPTForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: MPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        assert config.tie_word_embeddings\n        self.linear_method = linear_method\n\n        self.transformer = MPTModel(config, linear_method)\n        self.lm_head_weight = self.transformer.wte.weight\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            # Skip loading extra bias for GPTQ models.\n            if name.endswith(\".bias\") and name not in params_dict:\n                continue\n            param = params_dict[name]\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/mpt.py b/vllm/model_executor/models/mpt.py\nindex d6e9a76d2..22a876e2e 100644\n--- a/vllm/model_executor/models/mpt.py\n+++ b/vllm/model_executor/models/mpt.py\n@@ -276,7 +276,7 @@ class MPTForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/opt.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/opt/modeling_opt.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 The Fairseq Authors and The HuggingFace Inc. team. All rights\n# reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only OPT model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import OPTConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               ReplicatedLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass OPTLearnedPositionalEmbedding(nn.Embedding):\n\n    def __init__(self, num_embeddings: int, embedding_dim: int):\n        # OPT is set up so that if padding_idx is specified then offset the\n        # embedding ids by 2 and adjust num_embeddings appropriately. Other\n        # models don't have this hack\n        self.offset = 2\n        super().__init__(num_embeddings + self.offset, embedding_dim)\n\n    def forward(self, positions: torch.Tensor):\n        return super().forward(positions + self.offset)\n\n\nclass OPTAttention(nn.Module):\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        bias: bool = True,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.embed_dim = embed_dim\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        total_num_heads = num_heads\n        assert num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = total_num_heads // tensor_model_parallel_world_size\n        self.head_dim = embed_dim // total_num_heads\n        self.scaling = self.head_dim**-0.5\n\n        self.qkv_proj = QKVParallelLinear(\n            embed_dim,\n            self.head_dim,\n            total_num_heads,\n            bias=bias,\n            linear_method=linear_method,\n        )\n        self.out_proj = RowParallelLinear(\n            embed_dim,\n            embed_dim,\n            bias=bias,\n            linear_method=linear_method,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   scale=self.scaling)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        key_cache, value_cache = kv_cache\n        attn_output = self.attn(q, k, v, key_cache, value_cache,\n                                input_metadata)\n        output, _ = self.out_proj(attn_output)\n        return output\n\n\nclass OPTDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: OPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.self_attn = OPTAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.num_attention_heads,\n            bias=config.enable_bias,\n            linear_method=linear_method,\n        )\n        self.do_layer_norm_before = config.do_layer_norm_before\n\n        self.self_attn_layer_norm = nn.LayerNorm(\n            self.embed_dim,\n            elementwise_affine=config.layer_norm_elementwise_affine)\n        self.fc1 = ColumnParallelLinear(\n            self.embed_dim,\n            config.ffn_dim,\n            bias=config.enable_bias,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.activation_fn = get_act_fn(config.activation_function,\n                                        quant_config, config.ffn_dim)\n        self.fc2 = RowParallelLinear(\n            config.ffn_dim,\n            self.embed_dim,\n            bias=config.enable_bias,\n            linear_method=linear_method,\n        )\n        self.final_layer_norm = nn.LayerNorm(\n            self.embed_dim,\n            elementwise_affine=config.layer_norm_elementwise_affine)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        # Self Attention\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states = self.self_attn(hidden_states=hidden_states,\n                                       kv_cache=kv_cache,\n                                       input_metadata=input_metadata)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Fully Connected\n        residual = hidden_states\n        # 125m, 1.7B, ..., 175B applies layer norm BEFORE attention\n        if self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states, _ = self.fc1(hidden_states)\n        hidden_states = self.activation_fn(hidden_states)\n        hidden_states, _ = self.fc2(hidden_states)\n        hidden_states = residual + hidden_states\n        # 350m applies layer norm AFTER attention\n        if not self.do_layer_norm_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        return hidden_states\n\n\nclass OPTDecoder(nn.Module):\n\n    def __init__(\n        self,\n        config: OPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.word_embed_proj_dim,\n        )\n        # Positional embeddings are replicated (not sharded).\n        self.embed_positions = OPTLearnedPositionalEmbedding(\n            config.max_position_embeddings, config.hidden_size)\n\n        # Project out & in will be replicated if they exist.\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_out = ReplicatedLinear(config.hidden_size,\n                                                config.word_embed_proj_dim,\n                                                bias=False,\n                                                linear_method=linear_method)\n        else:\n            self.project_out = None\n\n        if config.word_embed_proj_dim != config.hidden_size:\n            self.project_in = ReplicatedLinear(config.word_embed_proj_dim,\n                                               config.hidden_size,\n                                               bias=False,\n                                               linear_method=linear_method)\n        else:\n            self.project_in = None\n\n        # Note that the only purpose of `config._remove_final_layer_norm` is to\n        # keep backward compatibility with checkpoints that have been fine-tuned\n        # before transformers v4.20.1\n        # see https://github.com/facebookresearch/metaseq/pull/164\n        if config.do_layer_norm_before and not config._remove_final_layer_norm:\n            self.final_layer_norm = nn.LayerNorm(\n                config.hidden_size,\n                elementwise_affine=config.layer_norm_elementwise_affine)\n        else:\n            self.final_layer_norm = None\n\n        self.layers = nn.ModuleList([\n            OPTDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        inputs_embeds = self.embed_tokens(input_ids)\n        pos_embeds = self.embed_positions(positions)\n        if self.project_in is not None:\n            inputs_embeds, _ = self.project_in(inputs_embeds)\n        hidden_states = inputs_embeds + pos_embeds\n\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states = layer(hidden_states, kv_caches[i], input_metadata)\n\n        if self.final_layer_norm is not None:\n            hidden_states = self.final_layer_norm(hidden_states)\n        if self.project_out is not None:\n            hidden_states, _ = self.project_out(hidden_states)\n        return hidden_states\n\n\nclass OPTModel(nn.Module):\n\n    def __init__(\n        self,\n        config: OPTConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.decoder = OPTDecoder(config, linear_method)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        return self.decoder(input_ids, positions, kv_caches, input_metadata)\n\n\nclass OPTForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = OPTModel(config, linear_method)\n        self.lm_head_weight = self.model.decoder.embed_tokens.weight\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n        ]\n        params_dict = dict(self.named_parameters(remove_duplicate=False))\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"lm_head.weight\" in name:\n                continue\n            if name.startswith(\"decoder.\"):\n                name = \"model.\" + name\n\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/opt.py b/vllm/model_executor/models/opt.py\nindex 22d3b5cca..393b2dcab 100644\n--- a/vllm/model_executor/models/opt.py\n+++ b/vllm/model_executor/models/opt.py\n@@ -309,7 +309,7 @@ class OPTForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head_weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/phi_1_5.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_phi.py\n# Copyright 2023 The vLLM team.\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n#\n# BSD 3-Clause License\n#\n# Copyright (c) 2022, Tri Dao, trid@cs.stanford.edu.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"Inference-only Phi-1.5 model compatible with HuggingFace weights.\"\"\"\nfrom typing import List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom transformers import PretrainedConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import get_act_fn\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.linear import (ColumnParallelLinear,\n                                               LinearMethodBase,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass PhiEmbedding(nn.Module):\n\n    def __init__(self, config: PretrainedConfig):\n        super().__init__()\n\n        self.wte = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n\n    def forward(self, input_ids: torch.LongTensor):\n        return self.wte(input_ids)\n\n\nclass PhiAttention(nn.Module):\n\n    def __init__(self,\n                 config: PretrainedConfig,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.total_num_heads = config.num_attention_heads\n        self.hidden_size = config.hidden_size\n        self.head_size = self.hidden_size // self.total_num_heads\n\n        tensor_model_parallel_world_size = (\n            get_tensor_model_parallel_world_size())\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = (self.total_num_heads //\n                          tensor_model_parallel_world_size)\n\n        # pylint: disable=C0103\n        self.Wqkv = QKVParallelLinear(\n            self.hidden_size,\n            self.head_size,\n            self.total_num_heads,\n            linear_method=linear_method,\n        )\n        self.qkv_proj = QKVParallelLinear(\n            config.hidden_size,\n            self.head_size,\n            self.total_num_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.out_proj = RowParallelLinear(\n            self.hidden_size,\n            self.hidden_size,\n            linear_method=linear_method,\n        )\n\n        scaling = self.head_size**-0.5\n        rotary_dim = config.rotary_dim\n        assert rotary_dim % 2 == 0\n\n        # pylint: disable=C0301\n        # Refer to:\n        # https://huggingface.co/microsoft/phi-1_5/blob/d212a789620c380ff32ca1d1ee9943a777360987/modeling_phi.py#L518\n        rope_theta = 10000\n        max_position_embeddings = getattr(config, \"n_positions\", 2048)\n        self.rotary_emb = get_rope(\n            self.head_size,\n            rotary_dim=rotary_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n        )\n        self.attn = PagedAttention(self.num_heads, self.head_size, scaling)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.Wqkv(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        q, k = self.rotary_emb(position_ids, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.out_proj(attn_output)\n        return output\n\n\nclass PhiMLP(nn.Module):\n\n    def __init__(self,\n                 config: PretrainedConfig,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n\n        n_inner = getattr(config, \"n_inner\", None)\n        n_inner = n_inner if n_inner is not None else 4 * config.hidden_size\n\n        self.fc1 = ColumnParallelLinear(\n            config.hidden_size,\n            n_inner,\n            linear_method=linear_method,\n        )\n        self.fc2 = RowParallelLinear(\n            n_inner,\n            config.hidden_size,\n            linear_method=linear_method,\n        )\n        quant_config = getattr(linear_method, \"quant_config\", None)\n        self.act = get_act_fn(config.activation_function, quant_config,\n                              n_inner)\n\n    def forward(self, hidden_states):\n        hidden_states, _ = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states, _ = self.fc2(hidden_states)\n        return hidden_states\n\n\nclass PhiLayer(nn.Module):\n\n    def __init__(self,\n                 config: PretrainedConfig,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.ln = nn.LayerNorm(config.hidden_size,\n                               eps=config.layer_norm_epsilon)\n        self.mixer = PhiAttention(config, linear_method)\n        self.mlp = PhiMLP(config, linear_method)\n\n    def forward(\n        self,\n        position_ids: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        residual = hidden_states\n        hidden_states = self.ln(hidden_states)\n        attn_outputs = self.mixer(\n            position_ids=position_ids,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n        feed_forward_hidden_states = self.mlp(hidden_states)\n        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n        return hidden_states\n\n\nclass PhiModel(nn.Module):\n\n    def __init__(self,\n                 config: PretrainedConfig,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.embd = PhiEmbedding(config)\n        self.h = nn.ModuleList([\n            PhiLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embd(input_ids)\n        for i in range(self.config.num_hidden_layers):\n            layer = self.h[i]\n            hidden_states = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n            )\n        return hidden_states\n\n\nclass PhiCausalLMHead(nn.Module):\n\n    def __init__(self, config: PretrainedConfig):\n        super().__init__()\n        self.ln = nn.LayerNorm(config.hidden_size,\n                               eps=config.layer_norm_epsilon)\n        self.linear = ParallelLMHead(config.vocab_size,\n                                     config.hidden_size,\n                                     bias=True)\n\n\nclass PhiForCausalLM(nn.Module):\n\n    def __init__(self,\n                 config: PretrainedConfig,\n                 linear_method: Optional[LinearMethodBase] = None):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n\n        self.transformer = PhiModel(config, linear_method)\n        self.lm_head = PhiCausalLMHead(config)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        hidden_states = self.lm_head.ln(hidden_states)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        head = self.lm_head.linear\n        next_tokens = self.sampler(head.weight, hidden_states,\n                                   sampling_metadata, head.bias)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n\n            # Skip loading extra bias for GPTQ models.\n            if name.endswith(\".bias\") and name not in params_dict:\n                continue\n            # pylint: disable=E1136\n            param = params_dict[name]\n            weight_loader = getattr(param, \"weight_loader\",\n                                    default_weight_loader)\n            weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/phi_1_5.py b/vllm/model_executor/models/phi_1_5.py\nindex 9f3c6f68d..9d4424dd0 100644\n--- a/vllm/model_executor/models/phi_1_5.py\n+++ b/vllm/model_executor/models/phi_1_5.py\n@@ -280,7 +280,7 @@ class PhiForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         head = self.lm_head.linear\n         next_tokens = self.sampler(head.weight, hidden_states,\n                                    sampling_metadata, head.bias)",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/qwen.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://huggingface.co/Qwen/Qwen-7B/blob/main/modeling_qwen.py\n# Copyright (c) Alibaba Cloud.\n# LICENSE: https://huggingface.co/Qwen/Qwen-7B/blob/main/LICENSE\n\"\"\"Inference-only QWen model compatible with HuggingFace weights.\"\"\"\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\nfrom vllm.transformers_utils.configs.qwen import QWenConfig\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass QWenMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str = \"silu\",\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.c_proj = RowParallelLinear(intermediate_size,\n                                        hidden_size,\n                                        bias=False,\n                                        linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.c_proj(x)\n        return x\n\n\nclass QWenAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        max_position_embeddings: int,\n        rope_theta: float = 10000,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        tensor_model_parallel_world_size = get_tensor_model_parallel_world_size(\n        )\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tensor_model_parallel_world_size == 0\n        self.num_heads = (self.total_num_heads //\n                          tensor_model_parallel_world_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.c_attn = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            bias=True,\n            linear_method=linear_method,\n        )\n        self.c_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.scaling = self.head_dim**-0.5\n\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position_embeddings,\n            base=rope_theta,\n            rope_scaling=rope_scaling,\n        )\n        self.attn = PagedAttention(self.num_heads, self.head_dim, self.scaling)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.c_attn(hidden_states)\n        q, k, v = qkv.chunk(chunks=3, dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n\n        output, _ = self.c_proj(attn_output)\n        return output\n\n\nclass QWenBlock(nn.Module):\n\n    def __init__(\n        self,\n        config: QWenConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.ln_1 = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        self.attn = QWenAttention(config.hidden_size,\n                                  config.num_attention_heads,\n                                  config.max_position_embeddings,\n                                  rope_theta=rope_theta,\n                                  rope_scaling=rope_scaling,\n                                  linear_method=linear_method)\n\n        self.ln_2 = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n\n        self.mlp = QWenMLP(config.hidden_size,\n                           config.intermediate_size // 2,\n                           linear_method=linear_method)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.ln_1(hidden_states)\n        else:\n            hidden_states, residual = self.ln_1(hidden_states, residual)\n        hidden_states = self.attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.ln_2(hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass QWenModel(nn.Module):\n\n    def __init__(\n        self,\n        config: QWenConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.vocab_size = config.vocab_size\n\n        self.wte = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.h = nn.ModuleList([\n            QWenBlock(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.ln_f = RMSNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.wte(input_ids)\n        residual = None\n        for i in range(len(self.h)):\n            layer = self.h[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.ln_f(hidden_states, residual)\n        return hidden_states\n\n\nclass QWenLMHeadModel(nn.Module):\n\n    def __init__(\n        self,\n        config: QWenConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ):\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.transformer = QWenModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.transformer(input_ids, positions, kv_caches,\n                                         input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"gate_up_proj\", \"w2\", 0),\n            (\"gate_up_proj\", \"w1\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/qwen.py b/vllm/model_executor/models/qwen.py\nindex 2d394a6b9..fbc7320fb 100644\n--- a/vllm/model_executor/models/qwen.py\n+++ b/vllm/model_executor/models/qwen.py\n@@ -247,7 +247,7 @@ class QWenLMHeadModel(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/models/yi.py",
      "old_content": "# coding=utf-8\n# Adapted from\n# https://github.com/huggingface/transformers/blob/v4.28.0/src/transformers/models/llama/modeling_llama.py\n# Copyright 2023 The vLLM team.\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Inference-only Yi model (https://01.ai) compatible with HuggingFace weights.\"\"\"\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom vllm.transformers_utils.configs.yi import YiConfig\n\nfrom vllm.model_executor.input_metadata import InputMetadata\nfrom vllm.model_executor.layers.activation import SiluAndMul\nfrom vllm.model_executor.layers.attention import PagedAttention\nfrom vllm.model_executor.layers.layernorm import RMSNorm\nfrom vllm.model_executor.layers.linear import (LinearMethodBase,\n                                               MergedColumnParallelLinear,\n                                               QKVParallelLinear,\n                                               RowParallelLinear)\nfrom vllm.model_executor.layers.rotary_embedding import get_rope\nfrom vllm.model_executor.layers.sampler import Sampler\nfrom vllm.model_executor.layers.vocab_parallel_embedding import (\n    VocabParallelEmbedding, ParallelLMHead)\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.weight_utils import (default_weight_loader,\n                                              hf_model_weights_iterator)\nfrom vllm.sequence import SamplerOutput\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n\n\nclass YiMLP(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.gate_up_proj = MergedColumnParallelLinear(\n            hidden_size, [intermediate_size] * 2,\n            bias=False,\n            linear_method=linear_method)\n        self.down_proj = RowParallelLinear(intermediate_size,\n                                           hidden_size,\n                                           bias=False,\n                                           linear_method=linear_method)\n        if hidden_act != \"silu\":\n            raise ValueError(f\"Unsupported activation: {hidden_act}. \"\n                             \"Only silu is supported for now.\")\n        self.act_fn = SiluAndMul()\n\n    def forward(self, x):\n        gate_up, _ = self.gate_up_proj(x)\n        x = self.act_fn(gate_up)\n        x, _ = self.down_proj(x)\n        return x\n\n\nclass YiAttention(nn.Module):\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n        num_kv_heads: int,\n        rope_theta: float = 10000,\n        rope_scaling: Optional[Dict[str, Any]] = None,\n        max_position_embeddings: int = 8192,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = hidden_size\n        tp_size = get_tensor_model_parallel_world_size()\n        self.total_num_heads = num_heads\n        assert self.total_num_heads % tp_size == 0\n        self.num_heads = self.total_num_heads // tp_size\n        self.total_num_kv_heads = num_kv_heads\n        if self.total_num_kv_heads >= tp_size:\n            # Number of KV heads is greater than TP size, so we partition\n            # the KV heads across multiple tensor parallel GPUs.\n            assert self.total_num_kv_heads % tp_size == 0\n        else:\n            # Number of KV heads is less than TP size, so we replicate\n            # the KV heads across multiple tensor parallel GPUs.\n            assert tp_size % self.total_num_kv_heads == 0\n        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)\n        self.head_dim = hidden_size // self.total_num_heads\n        self.q_size = self.num_heads * self.head_dim\n        self.kv_size = self.num_kv_heads * self.head_dim\n        self.scaling = self.head_dim**-0.5\n        self.rope_theta = rope_theta\n        self.max_position_embeddings = max_position_embeddings\n\n        self.qkv_proj = QKVParallelLinear(\n            hidden_size,\n            self.head_dim,\n            self.total_num_heads,\n            self.total_num_kv_heads,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.o_proj = RowParallelLinear(\n            self.total_num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n            linear_method=linear_method,\n        )\n        self.rotary_emb = get_rope(\n            self.head_dim,\n            rotary_dim=self.head_dim,\n            max_position=max_position_embeddings,\n            base=self.rope_theta,\n            rope_scaling=rope_scaling,\n        )\n        self.attn = PagedAttention(self.num_heads,\n                                   self.head_dim,\n                                   self.scaling,\n                                   num_kv_heads=self.num_kv_heads)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        qkv, _ = self.qkv_proj(hidden_states)\n        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)\n        q, k = self.rotary_emb(positions, q, k)\n        k_cache, v_cache = kv_cache\n        attn_output = self.attn(q, k, v, k_cache, v_cache, input_metadata)\n        output, _ = self.o_proj(attn_output)\n        return output\n\n\nclass YiDecoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        config: YiConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        rope_theta = getattr(config, \"rope_theta\", 10000)\n        rope_scaling = getattr(config, \"rope_scaling\", None)\n        max_position_embeddings = getattr(config, \"max_position_embeddings\",\n                                          8192)\n        self.self_attn = YiAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n            num_kv_heads=config.num_key_value_heads,\n            rope_theta=rope_theta,\n            rope_scaling=rope_scaling,\n            max_position_embeddings=max_position_embeddings,\n            linear_method=linear_method,\n        )\n        self.mlp = YiMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n            linear_method=linear_method,\n        )\n        self.ln1 = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.ln2 = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        positions: torch.Tensor,\n        hidden_states: torch.Tensor,\n        kv_cache: KVCache,\n        input_metadata: InputMetadata,\n        residual: Optional[torch.Tensor],\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Self Attention\n        if residual is None:\n            residual = hidden_states\n            hidden_states = self.ln1(hidden_states)\n        else:\n            hidden_states, residual = self.ln1(hidden_states, residual)\n        hidden_states = self.self_attn(\n            positions=positions,\n            hidden_states=hidden_states,\n            kv_cache=kv_cache,\n            input_metadata=input_metadata,\n        )\n\n        # Fully Connected\n        hidden_states, residual = self.ln2(hidden_states, residual)\n        hidden_states = self.mlp(hidden_states)\n        return hidden_states, residual\n\n\nclass YiModel(nn.Module):\n\n    def __init__(\n        self,\n        config: YiConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = VocabParallelEmbedding(\n            config.vocab_size,\n            config.hidden_size,\n        )\n        self.layers = nn.ModuleList([\n            YiDecoderLayer(config, linear_method)\n            for _ in range(config.num_hidden_layers)\n        ])\n        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.embed_tokens(input_ids)\n        residual = None\n        for i in range(len(self.layers)):\n            layer = self.layers[i]\n            hidden_states, residual = layer(\n                positions,\n                hidden_states,\n                kv_caches[i],\n                input_metadata,\n                residual,\n            )\n        hidden_states, _ = self.norm(hidden_states, residual)\n        return hidden_states\n\n\nclass YiForCausalLM(nn.Module):\n\n    def __init__(\n        self,\n        config: YiConfig,\n        linear_method: Optional[LinearMethodBase] = None,\n    ) -> None:\n        super().__init__()\n        self.config = config\n        self.linear_method = linear_method\n        self.model = YiModel(config, linear_method)\n        self.lm_head = ParallelLMHead(config.vocab_size, config.hidden_size)\n        self.sampler = Sampler(config.vocab_size)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        hidden_states = self.model(input_ids, positions, kv_caches,\n                                   input_metadata)\n        return hidden_states\n\n    def sample(\n        self,\n        hidden_states: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                   sampling_metadata)\n        return next_tokens\n\n    def load_weights(self,\n                     model_name_or_path: str,\n                     cache_dir: Optional[str] = None,\n                     load_format: str = \"auto\",\n                     revision: Optional[str] = None):\n        stacked_params_mapping = [\n            # (param_name, shard_name, shard_id)\n            (\"qkv_proj\", \"q_proj\", \"q\"),\n            (\"qkv_proj\", \"k_proj\", \"k\"),\n            (\"qkv_proj\", \"v_proj\", \"v\"),\n            (\"gate_up_proj\", \"gate_proj\", 0),\n            (\"gate_up_proj\", \"up_proj\", 1),\n        ]\n        params_dict = dict(self.named_parameters())\n        for name, loaded_weight in hf_model_weights_iterator(\n                model_name_or_path, cache_dir, load_format, revision):\n            if \"rotary_emb.inv_freq\" in name:\n                continue\n            for (param_name, weight_name, shard_id) in stacked_params_mapping:\n                if weight_name not in name:\n                    continue\n                name = name.replace(weight_name, param_name)\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = param.weight_loader\n                weight_loader(param, loaded_weight, shard_id)\n                break\n            else:\n                # Skip loading extra bias for GPTQ models.\n                if name.endswith(\".bias\") and name not in params_dict:\n                    continue\n                param = params_dict[name]\n                weight_loader = getattr(param, \"weight_loader\",\n                                        default_weight_loader)\n                weight_loader(param, loaded_weight)\n",
      "diff": "diff --git a/vllm/model_executor/models/yi.py b/vllm/model_executor/models/yi.py\nindex 78715a887..53daa6c4c 100644\n--- a/vllm/model_executor/models/yi.py\n+++ b/vllm/model_executor/models/yi.py\n@@ -286,7 +286,7 @@ class YiForCausalLM(nn.Module):\n         self,\n         hidden_states: torch.Tensor,\n         sampling_metadata: SamplingMetadata,\n-    ) -> SamplerOutput:\n+    ) -> Optional[SamplerOutput]:\n         next_tokens = self.sampler(self.lm_head.weight, hidden_states,\n                                    sampling_metadata)\n         return next_tokens",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/parallel_utils/communication_op.py",
      "old_content": "import torch\n\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    get_tensor_model_parallel_world_size,\n    get_tensor_model_parallel_group,\n)\n\n\ndef tensor_model_parallel_all_reduce(input_):\n    \"\"\"All-reduce the input tensor across model parallel group.\n\n    NOTE: This operation is applied in-place on the input tensor.\n    \"\"\"\n    # Bypass the function if we are using only 1 GPU.\n    if get_tensor_model_parallel_world_size() == 1:\n        return input_\n    # All-reduce.\n    torch.distributed.all_reduce(input_,\n                                 group=get_tensor_model_parallel_group())\n    return input_\n\n\ndef tensor_model_parallel_all_gather(input_, dim=-1):\n    \"\"\"All-gather the input tensor across model parallel group.\"\"\"\n    world_size = get_tensor_model_parallel_world_size()\n    # Bypass the function if we are using only 1 GPU.\n    if world_size == 1:\n        return input_\n    assert -input_.dim() <= dim < input_.dim(), (\n        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n    if dim < 0:\n        # Convert negative dim to positive.\n        dim += input_.dim()\n    input_size = input_.size()\n    # Allocate output tensor.\n    output_tensor = torch.empty((world_size, ) + input_size,\n                                dtype=input_.dtype,\n                                device=input_.device)\n    # All-gather.\n    torch.distributed.all_gather_into_tensor(\n        output_tensor, input_, group=get_tensor_model_parallel_group())\n    # Reshape\n    output_tensor = output_tensor.movedim(0, dim)\n    output_tensor = output_tensor.reshape(input_size[:dim] +\n                                          (world_size * input_size[dim], ) +\n                                          input_size[dim + 1:])\n    return output_tensor\n",
      "diff": "diff --git a/vllm/model_executor/parallel_utils/communication_op.py b/vllm/model_executor/parallel_utils/communication_op.py\nindex b1d5f5b9f..8bf04f3d1 100644\n--- a/vllm/model_executor/parallel_utils/communication_op.py\n+++ b/vllm/model_executor/parallel_utils/communication_op.py\n@@ -1,6 +1,7 @@\n import torch\n \n from vllm.model_executor.parallel_utils.parallel_state import (\n+    get_tensor_model_parallel_rank,\n     get_tensor_model_parallel_world_size,\n     get_tensor_model_parallel_group,\n )\n@@ -45,3 +46,61 @@ def tensor_model_parallel_all_gather(input_, dim=-1):\n                                           (world_size * input_size[dim], ) +\n                                           input_size[dim + 1:])\n     return output_tensor\n+\n+\n+def tensor_model_parallel_gather(input_, dst=0, dim=-1):\n+    \"\"\"Gather the input tensor across model parallel group.\n+\n+    NOTE: We assume that the input tensor is on the same device across\n+    all the ranks.\n+    \"\"\"\n+    world_size = get_tensor_model_parallel_world_size()\n+    # Bypass the function if we are using only 1 GPU.\n+    if world_size == 1:\n+        return input_\n+    assert -input_.dim() <= dim < input_.dim(), (\n+        f\"Invalid dim ({dim}) for input tensor with shape {input_.size()}\")\n+    if dim < 0:\n+        # Convert negative dim to positive.\n+        dim += input_.dim()\n+    # Allocate output tensor.\n+    if get_tensor_model_parallel_rank() == dst:\n+        gather_list = [torch.empty_like(input_) for _ in range(world_size)]\n+    else:\n+        gather_list = None\n+    # Gather.\n+    torch.distributed.gather(input_,\n+                             gather_list,\n+                             dst=dst,\n+                             group=get_tensor_model_parallel_group())\n+    if get_tensor_model_parallel_rank() == dst:\n+        output_tensor = torch.cat(gather_list, dim=dim)\n+    else:\n+        output_tensor = None\n+    return output_tensor\n+\n+\n+def broadcast(input_, src=0):\n+    \"\"\"Broadcast the input tensor.\"\"\"\n+    world_size = torch.distributed.get_world_size()\n+    assert 0 <= src < world_size, f\"Invalid src rank ({src})\"\n+\n+    # Bypass the function if we are using only 1 GPU.\n+    if world_size == 1:\n+        return input_\n+    # Broadcast.\n+    torch.distributed.broadcast(input_, src=src)\n+    return input_\n+\n+\n+def broadcast_object_list(obj_list, src=0):\n+    \"\"\"Broadcast the input object list.\"\"\"\n+    world_size = torch.distributed.get_world_size()\n+    assert 0 <= src < world_size, f\"Invalid src rank ({src})\"\n+\n+    # Bypass the function if we are using only 1 GPU.\n+    if world_size == 1:\n+        return obj_list\n+    # Broadcast.\n+    torch.distributed.broadcast_object_list(obj_list, src=src)\n+    return obj_list",
      "change_type": "modified",
      "lines_added": 60,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/model_executor/sampling_metadata.py",
      "old_content": "from dataclasses import dataclass\nfrom typing import Dict, List, Tuple\n\nimport torch\n\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import SequenceData\nfrom vllm.utils import in_wsl\n\n_SAMPLING_EPS = 1e-5\n\n\nclass SamplingMetadata:\n    \"\"\"Metadata for input sequences. Used in sampler.\n\n    Args:\n        seq_groups: List of (seq_ids, sampling_params).\n        seq_data: Seq_id -> SequenceData.\n        prompt_lens: Lengths of prompts.\n        selected_token_indices: Token indices selected for sampling.\n        categorized_sample_indices: SamplingType -> token indicies to sample.\n    \"\"\"\n\n    def __init__(\n        self,\n        seq_groups: List[Tuple[List[int], SamplingParams]],\n        seq_data: Dict[int, SequenceData],\n        prompt_lens: List[int],\n        selected_token_indices: torch.Tensor,\n        categorized_sample_indices: Dict[SamplingType, torch.Tensor],\n    ) -> None:\n        self.seq_groups = seq_groups\n        self.seq_data = seq_data\n        self.prompt_lens = prompt_lens\n        self.selected_token_indices = selected_token_indices\n        self.categorized_sample_indices = categorized_sample_indices\n\n        self.num_prompts = len(prompt_lens)\n\n    def __repr__(self) -> str:\n        return (\n            \"SamplingMetadata(\"\n            f\"seq_groups={self.seq_groups}, \"\n            f\"seq_data={self.seq_data}, \"\n            f\"prompt_lens={self.prompt_lens}, \"\n            f\"selected_token_indices={self.selected_token_indices}, \"\n            f\"categorized_sample_indices={self.categorized_sample_indices})\")\n\n\n@dataclass\nclass SamplingTensors:\n    \"\"\"Tensors for sampling.\"\"\"\n\n    temperatures: torch.Tensor\n    top_ps: torch.Tensor\n    top_ks: torch.Tensor\n    min_ps: torch.Tensor\n    presence_penalties: torch.Tensor\n    frequency_penalties: torch.Tensor\n    repetition_penalties: torch.Tensor\n    prompt_tokens: torch.Tensor\n    output_tokens: torch.Tensor\n\n    @classmethod\n    def from_sampling_metadata(\n            cls, sampling_metadata: \"SamplingMetadata\", vocab_size: int,\n            device: torch.device,\n            dtype: torch.dtype) -> Tuple[\"SamplingTensors\", bool, bool, bool]:\n        prompt_tokens: List[List[int]] = []\n        output_tokens: List[List[int]] = []\n        top_ks: List[int] = []\n        temperatures: List[float] = []\n        top_ps: List[float] = []\n        min_ps: List[float] = []\n        presence_penalties: List[float] = []\n        frequency_penalties: List[float] = []\n        repetition_penalties: List[float] = []\n        do_penalties = False\n        do_top_p_top_k = False\n        do_min_p = False\n        for i, seq_group in enumerate(sampling_metadata.seq_groups):\n            seq_ids, sampling_params = seq_group\n            temperature = sampling_params.temperature\n            p = sampling_params.presence_penalty\n            f = sampling_params.frequency_penalty\n            r = sampling_params.repetition_penalty\n            top_p = sampling_params.top_p\n            min_p = sampling_params.min_p\n            # k should not be greater than the vocab size.\n            top_k = min(sampling_params.top_k, vocab_size)\n            top_k = vocab_size if top_k == -1 else top_k\n            if temperature < _SAMPLING_EPS:\n                # NOTE: Zero temperature means deterministic sampling\n                # (i.e., greedy sampling or beam search).\n                # Set the temperature to 1 to avoid division by zero.\n                temperature = 1.0\n            if not do_top_p_top_k and (top_p < 1.0 - _SAMPLING_EPS\n                                       or top_k != vocab_size):\n                do_top_p_top_k = True\n            if not do_min_p and min_p > _SAMPLING_EPS:\n                do_min_p = True\n            if not do_penalties and (abs(p) >= _SAMPLING_EPS\n                                     or abs(f) >= _SAMPLING_EPS\n                                     or abs(r - 1.0) >= _SAMPLING_EPS):\n                do_penalties = True\n            if (i < sampling_metadata.num_prompts\n                    and sampling_params.prompt_logprobs is not None):\n                # For tokens in the prompt that we only need to get their logprobs\n                prompt_len = sampling_metadata.prompt_lens[i]\n                temperatures += [temperature] * (prompt_len - 1)\n                top_ps += [top_p] * (prompt_len - 1)\n                top_ks += [top_k] * (prompt_len - 1)\n                min_ps += [min_p] * (prompt_len - 1)\n                presence_penalties += [0] * (prompt_len - 1)\n                frequency_penalties += [0] * (prompt_len - 1)\n                repetition_penalties += [1] * (prompt_len - 1)\n                prompt_tokens.extend([] for _ in range(prompt_len - 1))\n                output_tokens.extend([] for _ in range(prompt_len - 1))\n            for seq_id in seq_ids:\n                seq_data = sampling_metadata.seq_data[seq_id]\n                prompt_tokens.append(seq_data.prompt_token_ids)\n                output_tokens.append(seq_data.output_token_ids)\n            temperatures += [temperature] * len(seq_ids)\n            top_ps += [top_p] * len(seq_ids)\n            top_ks += [top_k] * len(seq_ids)\n            min_ps += [min_p] * len(seq_ids)\n            presence_penalties += [p] * len(seq_ids)\n            frequency_penalties += [f] * len(seq_ids)\n            repetition_penalties += [r] * len(seq_ids)\n\n        sampling_tensors = SamplingTensors.from_lists(\n            temperatures, top_ps, top_ks, min_ps, presence_penalties,\n            frequency_penalties, repetition_penalties, prompt_tokens,\n            output_tokens, vocab_size, device, dtype)\n        return (sampling_tensors, do_penalties, do_top_p_top_k, do_min_p)\n\n    @classmethod\n    def from_lists(cls, temperatures: List[float], top_ps: List[float],\n                   top_ks: List[int], min_ps: List[float],\n                   presence_penalties: List[float],\n                   frequency_penalties: List[float],\n                   repetition_penalties: List[float],\n                   prompt_tokens: List[List[int]],\n                   output_tokens: List[List[int]], vocab_size: int,\n                   device: torch.device,\n                   dtype: torch.dtype) -> \"SamplingTensors\":\n        # Note that the performance will be very bad without\n        # pinned memory.\n        pin_memory = not in_wsl()\n        prompt_max_len = max(len(tokens) for tokens in prompt_tokens)\n        prompt_padded_tokens = [\n            tokens + [vocab_size] * (prompt_max_len - len(tokens))\n            for tokens in prompt_tokens\n        ]\n        output_max_len = max(len(tokens) for tokens in output_tokens)\n        output_padded_tokens = [\n            tokens + [vocab_size] * (output_max_len - len(tokens))\n            for tokens in output_tokens\n        ]\n\n        temperatures_t = torch.tensor(\n            temperatures,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ps_t = torch.tensor(\n            top_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        min_ps_t = torch.tensor(\n            min_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        presence_penalties_t = torch.tensor(\n            presence_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        frequency_penalties_t = torch.tensor(\n            frequency_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        repetition_penalties_t = torch.tensor(\n            repetition_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ks_t = torch.tensor(\n            top_ks,\n            device=\"cpu\",\n            dtype=torch.int,\n            pin_memory=pin_memory,\n        )\n        prompt_tensor = torch.tensor(\n            prompt_padded_tokens,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        )\n        output_tensor = torch.tensor(\n            output_padded_tokens,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        )\n        # Because the memory is pinned, we can do non-blocking\n        # transfer to device.\n        return cls(\n            temperatures=temperatures_t.to(device=device, non_blocking=True),\n            top_ps=top_ps_t.to(device=device, non_blocking=True),\n            top_ks=top_ks_t.to(device=device, non_blocking=True),\n            min_ps=min_ps_t.to(device=device, non_blocking=True),\n            presence_penalties=presence_penalties_t.to(device=device,\n                                                       non_blocking=True),\n            frequency_penalties=frequency_penalties_t.to(device=device,\n                                                         non_blocking=True),\n            repetition_penalties=repetition_penalties_t.to(device=device,\n                                                           non_blocking=True),\n            prompt_tokens=prompt_tensor.to(device=device, non_blocking=True),\n            output_tokens=output_tensor.to(device=device, non_blocking=True),\n        )\n",
      "diff": "diff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 49013ec27..2d41d40e0 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -1,5 +1,5 @@\n from dataclasses import dataclass\n-from typing import Dict, List, Tuple\n+from typing import Dict, List, Optional, Tuple\n \n import torch\n \n@@ -18,24 +18,29 @@ class SamplingMetadata:\n         seq_data: Seq_id -> SequenceData.\n         prompt_lens: Lengths of prompts.\n         selected_token_indices: Token indices selected for sampling.\n-        categorized_sample_indices: SamplingType -> token indicies to sample.\n+        categorized_sample_indices: SamplingType -> token indices to sample.\n+        perform_sampling: Whether to perform sampling. This option is used to\n+            make the sampling only happens in the driver worker, and disable\n+            sampling in other worker processes.\n     \"\"\"\n \n     def __init__(\n         self,\n-        seq_groups: List[Tuple[List[int], SamplingParams]],\n-        seq_data: Dict[int, SequenceData],\n-        prompt_lens: List[int],\n+        seq_groups: Optional[List[Tuple[List[int], SamplingParams]]],\n+        seq_data: Optional[Dict[int, SequenceData]],\n+        prompt_lens: Optional[List[int]],\n         selected_token_indices: torch.Tensor,\n-        categorized_sample_indices: Dict[SamplingType, torch.Tensor],\n+        categorized_sample_indices: Optional[Dict[SamplingType, torch.Tensor]],\n+        perform_sampling: bool = True,\n     ) -> None:\n         self.seq_groups = seq_groups\n         self.seq_data = seq_data\n         self.prompt_lens = prompt_lens\n         self.selected_token_indices = selected_token_indices\n         self.categorized_sample_indices = categorized_sample_indices\n+        self.perform_sampling = perform_sampling\n \n-        self.num_prompts = len(prompt_lens)\n+        self.num_prompts = len(prompt_lens) if prompt_lens is not None else 0\n \n     def __repr__(self) -> str:\n         return (\n@@ -44,7 +49,8 @@ class SamplingMetadata:\n             f\"seq_data={self.seq_data}, \"\n             f\"prompt_lens={self.prompt_lens}, \"\n             f\"selected_token_indices={self.selected_token_indices}, \"\n-            f\"categorized_sample_indices={self.categorized_sample_indices})\")\n+            f\"categorized_sample_indices={self.categorized_sample_indices}), \"\n+            f\"perform_sampling={self.perform_sampling})\")\n \n \n @dataclass",
      "change_type": "modified",
      "lines_added": 15,
      "lines_removed": 9
    },
    {
      "file_path": "vllm/utils.py",
      "old_content": "import enum\nimport socket\nimport uuid\nfrom platform import uname\n\nimport psutil\nimport torch\n\nfrom vllm._C import cuda_utils\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\n\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    # https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html\n    cudaDevAttrMaxSharedMemoryPerBlockOptin = 97 if not is_hip() else 74\n    max_shared_mem = cuda_utils.get_device_attribute(\n        cudaDevAttrMaxSharedMemoryPerBlockOptin, gpu)\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\ndef get_open_port():\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((\"\", 0))\n        return s.getsockname()[1]\n",
      "diff": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex eff5d10fd..c32047ac2 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -1,7 +1,9 @@\n import enum\n+import os\n import socket\n import uuid\n from platform import uname\n+from typing import List\n \n import psutil\n import torch\n@@ -55,7 +57,15 @@ def in_wsl() -> bool:\n     return \"microsoft\" in \" \".join(uname()).lower()\n \n \n-def get_open_port():\n+def get_ip() -> str:\n+    return socket.gethostbyname(socket.gethostname())\n+\n+\n+def get_open_port() -> int:\n     with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n         s.bind((\"\", 0))\n         return s.getsockname()[1]\n+\n+\n+def set_cuda_visible_devices(device_ids: List[int]) -> None:\n+    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, device_ids))",
      "change_type": "modified",
      "lines_added": 12,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import time\nfrom typing import Dict, List, Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom vllm.config import ModelConfig, ParallelConfig, SchedulerConfig\nfrom vllm.logger import init_logger\nfrom vllm.model_executor import get_model, InputMetadata, SamplingMetadata\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import SamplerOutput, SequenceData, SequenceGroupMetadata\nfrom vllm.utils import in_wsl\n\nlogger = init_logger(__name__)\n\nKVCache = Tuple[torch.Tensor, torch.Tensor]\n_PAD_SLOT_ID = -1\n# Capture graphs for batch size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [8 * i for i in range(1, 33)]\n\n\nclass ModelRunner:\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n\n        # model_config can be None in tests/samplers/test_sampler.py.\n        # FIXME(woosuk): This is a hack to make the tests work. Refactor this.\n        self.sliding_window = (model_config.get_sliding_window()\n                               if model_config is not None else None)\n        self.model = None\n        self.block_size = None  # Set after initial profiling.\n\n        self.graph_runners: Dict[int, CUDAGraphRunner] = {}\n        self.graph_memory_pool = None  # Set during graph capture.\n\n        self.max_context_len_to_capture = (\n            self.model_config.max_context_len_to_capture\n            if self.model_config is not None else 0)\n        # When using CUDA graph, the input block tables must be padded to\n        # max_context_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = None  # Set after initial profiling.\n        # cache in_wsl result\n        self.in_wsl = in_wsl()\n\n    def load_model(self) -> None:\n        self.model = get_model(self.model_config)\n\n    def set_block_size(self, block_size: int) -> None:\n        self.block_size = block_size\n\n        max_num_blocks = (self.max_context_len_to_capture + block_size -\n                          1) // block_size\n        self.graph_block_tables = np.zeros(\n            (max(_BATCH_SIZES_TO_CAPTURE), max_num_blocks), dtype=np.int32)\n\n    def _prepare_prompt(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:\n        assert len(seq_group_metadata_list) > 0\n        input_tokens: List[List[int]] = []\n        input_positions: List[List[int]] = []\n        slot_mapping: List[List[int]] = []\n\n        prompt_lens: List[int] = []\n        for seq_group_metadata in seq_group_metadata_list:\n            assert seq_group_metadata.is_prompt\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            assert len(seq_ids) == 1\n            seq_id = seq_ids[0]\n\n            seq_data = seq_group_metadata.seq_data[seq_id]\n            prompt_tokens = seq_data.get_token_ids()\n            prompt_len = len(prompt_tokens)\n            prompt_lens.append(prompt_len)\n\n            input_tokens.append(prompt_tokens)\n            # NOTE(woosuk): Here we assume that the first token in the prompt\n            # is always the first token in the sequence.\n            input_positions.append(list(range(prompt_len)))\n\n            if seq_group_metadata.block_tables is None:\n                # During memory profiling, the block tables are not initialized\n                # yet. In this case, we just use a dummy slot mapping.\n                slot_mapping.append([_PAD_SLOT_ID] * prompt_len)\n                continue\n\n            # Compute the slot mapping.\n            slot_mapping.append([])\n            block_table = seq_group_metadata.block_tables[seq_id]\n            # Mask the [0, start_idx) tokens of the prompt with _PAD_SLOT_ID,\n            # where start_idx is max(0, prompt_len - sliding_window).\n            # For example, if the prompt len is 10, sliding window is 8, and\n            # block size is 4, the first two tokens are masked and the slot\n            # mapping will be [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n            start_idx = 0\n            if self.sliding_window is not None:\n                start_idx = max(0, prompt_len - self.sliding_window)\n            for i in range(prompt_len):\n                if i < start_idx:\n                    slot_mapping[-1].append(_PAD_SLOT_ID)\n                    continue\n\n                block_number = block_table[i // self.block_size]\n                block_offset = i % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping[-1].append(slot)\n\n        max_prompt_len = max(prompt_lens)\n        input_tokens = _make_tensor_with_pad(input_tokens,\n                                             max_prompt_len,\n                                             pad=0,\n                                             dtype=torch.long)\n        input_positions = _make_tensor_with_pad(input_positions,\n                                                max_prompt_len,\n                                                pad=0,\n                                                dtype=torch.long)\n        slot_mapping = _make_tensor_with_pad(slot_mapping,\n                                             max_prompt_len,\n                                             pad=_PAD_SLOT_ID,\n                                             dtype=torch.long)\n\n        input_metadata = InputMetadata(\n            prompt_lens=prompt_lens,\n            slot_mapping=slot_mapping,\n            max_context_len=None,\n            context_lens=None,\n            block_tables=None,\n            use_cuda_graph=False,\n        )\n        return input_tokens, input_positions, input_metadata\n\n    def _prepare_decode(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:\n        assert len(seq_group_metadata_list) > 0\n        input_tokens: List[List[int]] = []\n        input_positions: List[List[int]] = []\n        slot_mapping: List[List[int]] = []\n        context_lens: List[int] = []\n        block_tables: List[List[int]] = []\n\n        for seq_group_metadata in seq_group_metadata_list:\n            assert not seq_group_metadata.is_prompt\n\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            for seq_id in seq_ids:\n                seq_data = seq_group_metadata.seq_data[seq_id]\n                generation_token = seq_data.get_last_token_id()\n                input_tokens.append([generation_token])\n\n                seq_len = seq_data.get_len()\n                position = seq_len - 1\n                input_positions.append([position])\n\n                context_len = seq_len if self.sliding_window is None else min(\n                    seq_len, self.sliding_window)\n                context_lens.append(context_len)\n\n                block_table = seq_group_metadata.block_tables[seq_id]\n                block_number = block_table[position // self.block_size]\n                block_offset = position % self.block_size\n                slot = block_number * self.block_size + block_offset\n                slot_mapping.append([slot])\n\n                if self.sliding_window is not None:\n                    sliding_window_blocks = (self.sliding_window //\n                                             self.block_size)\n                    block_table = block_table[-sliding_window_blocks:]\n                block_tables.append(block_table)\n\n        batch_size = len(input_tokens)\n        max_context_len = max(context_lens)\n        use_captured_graph = (\n            not self.model_config.enforce_eager\n            and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n            and max_context_len <= self.max_context_len_to_capture)\n        if use_captured_graph:\n            # Pad the input tokens, positions, and slot mapping to match the\n            # batch size of the captured graph.\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            for _ in range(graph_batch_size - batch_size):\n                input_tokens.append([])\n                input_positions.append([])\n                slot_mapping.append([])\n                context_lens.append(1)\n                block_tables.append([])\n            batch_size = graph_batch_size\n\n        # When using CUDA graph, we don't need to make the tensors on the GPU\n        # because they will be eventually copied to the designated GPU buffer.\n        device = \"cpu\" if use_captured_graph else \"cuda\"\n        pin_memory = use_captured_graph and not self.in_wsl\n        input_tokens = _make_tensor_with_pad(input_tokens,\n                                             max_len=1,\n                                             pad=0,\n                                             dtype=torch.long,\n                                             device=device,\n                                             pin_memory=pin_memory)\n        input_positions = _make_tensor_with_pad(input_positions,\n                                                max_len=1,\n                                                pad=0,\n                                                dtype=torch.long,\n                                                device=device,\n                                                pin_memory=pin_memory)\n        slot_mapping = _make_tensor_with_pad(slot_mapping,\n                                             max_len=1,\n                                             pad=_PAD_SLOT_ID,\n                                             dtype=torch.long,\n                                             device=device,\n                                             pin_memory=pin_memory)\n        context_lens = torch.tensor(context_lens,\n                                    dtype=torch.int,\n                                    device=device,\n                                    pin_memory=pin_memory)\n\n        if use_captured_graph:\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.tensor(input_block_tables, device=device)\n        else:\n            block_tables = _make_tensor_with_pad(\n                block_tables,\n                max_len=max_context_len,\n                pad=0,\n                dtype=torch.int,\n            )\n\n        input_metadata = InputMetadata(\n            prompt_lens=[],\n            slot_mapping=slot_mapping,\n            max_context_len=max_context_len,\n            context_lens=context_lens,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n        )\n        return input_tokens, input_positions, input_metadata\n\n    def _prepare_sample(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        prompt_lens: List[int],\n    ) -> SamplingMetadata:\n        seq_groups: List[Tuple[List[int], SamplingParams]] = []\n        selected_token_indices: List[int] = []\n        selected_token_start_idx = 0\n        categorized_sample_indices = {t: [] for t in SamplingType}\n        categorized_sample_indices_start_idx = 0\n\n        max_prompt_len = max(prompt_lens) if prompt_lens else 1\n        for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n            seq_ids = list(seq_group_metadata.seq_data.keys())\n            sampling_params = seq_group_metadata.sampling_params\n            seq_groups.append((seq_ids, sampling_params))\n\n            if seq_group_metadata.is_prompt:\n                assert len(seq_ids) == 1\n                prompt_len = prompt_lens[i]\n                if sampling_params.prompt_logprobs is not None:\n                    # NOTE: prompt token positions do not need sample, skip\n                    categorized_sample_indices_start_idx += prompt_len - 1\n\n                categorized_sample_indices[\n                    sampling_params.sampling_type].append(\n                        categorized_sample_indices_start_idx)\n                categorized_sample_indices_start_idx += 1\n\n                if sampling_params.prompt_logprobs is not None:\n                    selected_token_indices.extend(\n                        range(selected_token_start_idx,\n                              selected_token_start_idx + prompt_len - 1))\n                selected_token_indices.append(selected_token_start_idx +\n                                              prompt_len - 1)\n                selected_token_start_idx += max_prompt_len\n            else:\n                num_seqs = len(seq_ids)\n                selected_token_indices.extend(\n                    range(selected_token_start_idx,\n                          selected_token_start_idx + num_seqs))\n                selected_token_start_idx += num_seqs\n\n                categorized_sample_indices[\n                    sampling_params.sampling_type].extend(\n                        range(categorized_sample_indices_start_idx,\n                              categorized_sample_indices_start_idx + num_seqs))\n                categorized_sample_indices_start_idx += num_seqs\n\n        selected_token_indices = _async_h2d(selected_token_indices,\n                                            dtype=torch.long,\n                                            pin_memory=not self.in_wsl)\n        categorized_sample_indices = {\n            t: _async_h2d(seq_ids, dtype=torch.int, pin_memory=not self.in_wsl)\n            for t, seq_ids in categorized_sample_indices.items()\n        }\n\n        seq_data: Dict[int, SequenceData] = {}\n        for seq_group_metadata in seq_group_metadata_list:\n            seq_data.update(seq_group_metadata.seq_data)\n\n        sampling_metadata = SamplingMetadata(\n            seq_groups=seq_groups,\n            seq_data=seq_data,\n            prompt_lens=prompt_lens,\n            selected_token_indices=selected_token_indices,\n            categorized_sample_indices=categorized_sample_indices,\n        )\n        return sampling_metadata\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n    ) -> SamplerOutput:\n        # NOTE: We assume that all sequences in the group are all prompts or\n        # all decodes.\n        is_prompt = seq_group_metadata_list[0].is_prompt\n        # Prepare input tensors.\n        if is_prompt:\n            inputs = self._prepare_prompt(seq_group_metadata_list)\n            input_tokens, input_positions, input_metadata = inputs\n        else:\n            inputs = self._prepare_decode(seq_group_metadata_list)\n            input_tokens, input_positions, input_metadata = inputs\n\n        # Execute the model.\n        if input_metadata.use_cuda_graph:\n            graph_batch_size = input_tokens.shape[0]\n            model_executable = self.graph_runners[graph_batch_size]\n        else:\n            model_executable = self.model\n        hidden_states = model_executable(\n            input_ids=input_tokens,\n            positions=input_positions,\n            kv_caches=kv_caches,\n            input_metadata=input_metadata,\n        )\n\n        sampling_metadata = self._prepare_sample(seq_group_metadata_list,\n                                                 input_metadata.prompt_lens)\n\n        # Sample the next token.\n        output = self.model.sample(\n            hidden_states=hidden_states,\n            sampling_metadata=sampling_metadata,\n        )\n        return output\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        vocab_size = self.model_config.get_vocab_size()\n        sampling_params = SamplingParams(top_p=0.99, top_k=vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            seq_data = SequenceData([0] * seq_len)\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [(None, None)] * num_layers\n        self.execute_model(seqs, kv_caches)\n        torch.cuda.synchronize()\n        return\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[KVCache]) -> None:\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n        input_tokens = torch.zeros(max_batch_size, 1, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, 1,\n                                      dtype=torch.long).cuda()\n        slot_mapping = torch.empty(max_batch_size, 1, dtype=torch.long).cuda()\n        slot_mapping.fill_(_PAD_SLOT_ID)\n        context_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()\n        block_tables = torch.from_numpy(self.graph_block_tables).cuda()\n\n        # NOTE: Capturing the largest batch size first may help reduce the\n        # memory usage of CUDA graph.\n        for batch_size in reversed(_BATCH_SIZES_TO_CAPTURE):\n            # Create dummy input_metadata.\n            input_metadata = InputMetadata(\n                prompt_lens=[],\n                slot_mapping=slot_mapping[:batch_size],\n                max_context_len=self.max_context_len_to_capture,\n                context_lens=context_lens[:batch_size],\n                block_tables=block_tables[:batch_size],\n                use_cuda_graph=True,\n            )\n\n            graph_runner = CUDAGraphRunner(self.model)\n            graph_runner.capture(\n                input_tokens[:batch_size],\n                input_positions[:batch_size],\n                kv_caches,\n                input_metadata,\n                memory_pool=self.graph_memory_pool,\n            )\n            self.graph_memory_pool = graph_runner.graph.pool()\n            self.graph_runners[batch_size] = graph_runner\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(f\"Graph capturing finished in {elapsed_time:.0f} secs.\")\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module):\n        self.model = model\n        self.graph = None\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[KVCache],\n        input_metadata: InputMetadata,\n        memory_pool,\n    ) -> None:\n        assert self.graph is None\n        # Run the model once without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        self.model(\n            input_ids,\n            positions,\n            kv_caches,\n            input_metadata,\n        )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        self.graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self.graph, pool=memory_pool):\n            hidden_states = self.model(\n                input_ids,\n                positions,\n                kv_caches,\n                input_metadata,\n            )\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        self.input_buffers = {\n            \"input_ids\": input_ids,\n            \"positions\": positions,\n            \"kv_caches\": kv_caches,\n            \"slot_mapping\": input_metadata.slot_mapping,\n            \"context_lens\": input_metadata.context_lens,\n            \"block_tables\": input_metadata.block_tables,\n        }\n        self.output_buffers = {\"hidden_states\": hidden_states}\n        return\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n        input_metadata: InputMetadata,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(input_metadata.slot_mapping,\n                                                 non_blocking=True)\n        self.input_buffers[\"context_lens\"].copy_(input_metadata.context_lens,\n                                                 non_blocking=True)\n        self.input_buffers[\"block_tables\"].copy_(input_metadata.block_tables,\n                                                 non_blocking=True)\n\n        # Run the graph.\n        self.graph.replay()\n\n        # Return the output tensor.\n        return self.output_buffers[\"hidden_states\"]\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\ndef _pad_to_max(x: List[int], max_len: int, pad: int) -> List[int]:\n    assert len(x) <= max_len\n    return x + [pad] * (max_len - len(x))\n\n\ndef _make_tensor_with_pad(\n    x: List[List[int]],\n    max_len: int,\n    pad: int,\n    dtype: torch.dtype,\n    device: Union[str, torch.device] = \"cuda\",\n    pin_memory: bool = False,\n) -> torch.Tensor:\n    padded_x = [_pad_to_max(x_i, max_len, pad) for x_i in x]\n    return torch.tensor(padded_x,\n                        dtype=dtype,\n                        device=device,\n                        pin_memory=pin_memory and str(device) == \"cpu\")\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return (batch_size + 7) // 8 * 8\n\n\ndef _async_h2d(data: list, dtype, pin_memory):\n    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory)\n    return t.to(device=\"cuda\", non_blocking=True)\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex fb7a0c17d..be2803089 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1,5 +1,5 @@\n import time\n-from typing import Dict, List, Tuple, Union\n+from typing import Dict, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -8,6 +8,8 @@ import torch.nn as nn\n from vllm.config import ModelConfig, ParallelConfig, SchedulerConfig\n from vllm.logger import init_logger\n from vllm.model_executor import get_model, InputMetadata, SamplingMetadata\n+from vllm.model_executor.parallel_utils.communication_op import (\n+    broadcast, broadcast_object_list)\n from vllm.sampling_params import SamplingParams, SamplingType\n from vllm.sequence import SamplerOutput, SequenceData, SequenceGroupMetadata\n from vllm.utils import in_wsl\n@@ -28,10 +30,12 @@ class ModelRunner:\n         model_config: ModelConfig,\n         parallel_config: ParallelConfig,\n         scheduler_config: SchedulerConfig,\n+        is_driver_worker: bool = False,\n     ):\n         self.model_config = model_config\n         self.parallel_config = parallel_config\n         self.scheduler_config = scheduler_config\n+        self.is_driver_worker = is_driver_worker\n \n         # model_config can be None in tests/samplers/test_sampler.py.\n         # FIXME(woosuk): This is a hack to make the tests work. Refactor this.\n@@ -70,7 +74,7 @@ class ModelRunner:\n     def _prepare_prompt(\n         self,\n         seq_group_metadata_list: List[SequenceGroupMetadata],\n-    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata]:\n+    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, List[int]]:\n         assert len(seq_group_metadata_list) > 0\n         input_tokens: List[List[int]] = []\n         input_positions: List[List[int]] = []\n@@ -135,14 +139,14 @@ class ModelRunner:\n                                              dtype=torch.long)\n \n         input_metadata = InputMetadata(\n-            prompt_lens=prompt_lens,\n+            is_prompt=True,\n             slot_mapping=slot_mapping,\n             max_context_len=None,\n             context_lens=None,\n             block_tables=None,\n             use_cuda_graph=False,\n         )\n-        return input_tokens, input_positions, input_metadata\n+        return input_tokens, input_positions, input_metadata, prompt_lens\n \n     def _prepare_decode(\n         self,\n@@ -203,32 +207,24 @@ class ModelRunner:\n                 block_tables.append([])\n             batch_size = graph_batch_size\n \n-        # When using CUDA graph, we don't need to make the tensors on the GPU\n-        # because they will be eventually copied to the designated GPU buffer.\n-        device = \"cpu\" if use_captured_graph else \"cuda\"\n-        pin_memory = use_captured_graph and not self.in_wsl\n         input_tokens = _make_tensor_with_pad(input_tokens,\n                                              max_len=1,\n                                              pad=0,\n                                              dtype=torch.long,\n-                                             device=device,\n-                                             pin_memory=pin_memory)\n+                                             device=\"cuda\")\n         input_positions = _make_tensor_with_pad(input_positions,\n                                                 max_len=1,\n                                                 pad=0,\n                                                 dtype=torch.long,\n-                                                device=device,\n-                                                pin_memory=pin_memory)\n+                                                device=\"cuda\")\n         slot_mapping = _make_tensor_with_pad(slot_mapping,\n                                              max_len=1,\n                                              pad=_PAD_SLOT_ID,\n                                              dtype=torch.long,\n-                                             device=device,\n-                                             pin_memory=pin_memory)\n+                                             device=\"cuda\")\n         context_lens = torch.tensor(context_lens,\n                                     dtype=torch.int,\n-                                    device=device,\n-                                    pin_memory=pin_memory)\n+                                    device=\"cuda\")\n \n         if use_captured_graph:\n             # The shape of graph_block_tables is\n@@ -237,17 +233,18 @@ class ModelRunner:\n             for i, block_table in enumerate(block_tables):\n                 if block_table:\n                     input_block_tables[i, :len(block_table)] = block_table\n-            block_tables = torch.tensor(input_block_tables, device=device)\n+            block_tables = torch.tensor(input_block_tables, device=\"cuda\")\n         else:\n             block_tables = _make_tensor_with_pad(\n                 block_tables,\n                 max_len=max_context_len,\n                 pad=0,\n                 dtype=torch.int,\n+                device=\"cuda\",\n             )\n \n         input_metadata = InputMetadata(\n-            prompt_lens=[],\n+            is_prompt=False,\n             slot_mapping=slot_mapping,\n             max_context_len=max_context_len,\n             context_lens=context_lens,\n@@ -326,23 +323,127 @@ class ModelRunner:\n         )\n         return sampling_metadata\n \n+    def prepare_input_tensors(\n+        self,\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n+    ) -> Tuple[torch.Tensor, torch.Tensor, InputMetadata, SamplingMetadata]:\n+        if self.is_driver_worker:\n+            # NOTE: We assume that all sequences in the group are all prompts or\n+            # all decodes.\n+            is_prompt = seq_group_metadata_list[0].is_prompt\n+            # Prepare input tensors.\n+            if is_prompt:\n+                (input_tokens, input_positions, input_metadata,\n+                 prompt_lens) = self._prepare_prompt(seq_group_metadata_list)\n+            else:\n+                (input_tokens, input_positions, input_metadata\n+                 ) = self._prepare_decode(seq_group_metadata_list)\n+                prompt_lens = []\n+            sampling_metadata = self._prepare_sample(seq_group_metadata_list,\n+                                                     prompt_lens)\n+\n+            def get_size_or_none(x: Optional[torch.Tensor]):\n+                return x.size() if x is not None else None\n+\n+            # Broadcast the input data. For input tensors, we first broadcast\n+            # its shape and then broadcast the tensor to avoid high\n+            # serialization cost.\n+            py_data = {\n+                \"input_tokens_size\":\n+                input_tokens.size(),\n+                \"input_positions_size\":\n+                input_positions.size(),\n+                \"is_prompt\":\n+                input_metadata.is_prompt,\n+                \"slot_mapping_size\":\n+                get_size_or_none(input_metadata.slot_mapping),\n+                \"max_context_len\":\n+                input_metadata.max_context_len,\n+                \"context_lens_size\":\n+                get_size_or_none(input_metadata.context_lens),\n+                \"block_tables_size\":\n+                get_size_or_none(input_metadata.block_tables),\n+                \"use_cuda_graph\":\n+                input_metadata.use_cuda_graph,\n+                \"selected_token_indices_size\":\n+                sampling_metadata.selected_token_indices.size(),\n+            }\n+            broadcast_object_list([py_data], src=0)\n+            # TODO(zhuohan): Combine the broadcasts or set async_op=True.\n+            broadcast(input_tokens, src=0)\n+            broadcast(input_positions, src=0)\n+            if input_metadata.slot_mapping is not None:\n+                broadcast(input_metadata.slot_mapping, src=0)\n+            if input_metadata.context_lens is not None:\n+                broadcast(input_metadata.context_lens, src=0)\n+            if input_metadata.block_tables is not None:\n+                broadcast(input_metadata.block_tables, src=0)\n+            broadcast(sampling_metadata.selected_token_indices, src=0)\n+        else:\n+            receving_list = [None]\n+            broadcast_object_list(receving_list, src=0)\n+            py_data = receving_list[0]\n+            input_tokens = torch.empty(*py_data[\"input_tokens_size\"],\n+                                       dtype=torch.long,\n+                                       device=\"cuda\")\n+            broadcast(input_tokens, src=0)\n+            input_positions = torch.empty(*py_data[\"input_positions_size\"],\n+                                          dtype=torch.long,\n+                                          device=\"cuda\")\n+            broadcast(input_positions, src=0)\n+            if py_data[\"slot_mapping_size\"] is not None:\n+                slot_mapping = torch.empty(*py_data[\"slot_mapping_size\"],\n+                                           dtype=torch.long,\n+                                           device=\"cuda\")\n+                broadcast(slot_mapping, src=0)\n+            else:\n+                slot_mapping = None\n+            if py_data[\"context_lens_size\"] is not None:\n+                context_lens = torch.empty(*py_data[\"context_lens_size\"],\n+                                           dtype=torch.int,\n+                                           device=\"cuda\")\n+                broadcast(context_lens, src=0)\n+            else:\n+                context_lens = None\n+            if py_data[\"block_tables_size\"] is not None:\n+                block_tables = torch.empty(*py_data[\"block_tables_size\"],\n+                                           dtype=torch.int,\n+                                           device=\"cuda\")\n+                broadcast(block_tables, src=0)\n+            else:\n+                block_tables = None\n+            selected_token_indices = torch.empty(\n+                *py_data[\"selected_token_indices_size\"],\n+                dtype=torch.long,\n+                device=\"cuda\")\n+            broadcast(selected_token_indices, src=0)\n+            input_metadata = InputMetadata(\n+                is_prompt=py_data[\"is_prompt\"],\n+                slot_mapping=slot_mapping,\n+                max_context_len=py_data[\"max_context_len\"],\n+                context_lens=context_lens,\n+                block_tables=block_tables,\n+                use_cuda_graph=py_data[\"use_cuda_graph\"],\n+            )\n+            sampling_metadata = SamplingMetadata(\n+                seq_groups=None,\n+                seq_data=None,\n+                prompt_lens=None,\n+                selected_token_indices=selected_token_indices,\n+                categorized_sample_indices=None,\n+                perform_sampling=False,\n+            )\n+\n+        return input_tokens, input_positions, input_metadata, sampling_metadata\n+\n     @torch.inference_mode()\n     def execute_model(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n         kv_caches: List[Tuple[torch.Tensor, torch.Tensor]],\n-    ) -> SamplerOutput:\n-        # NOTE: We assume that all sequences in the group are all prompts or\n-        # all decodes.\n-        is_prompt = seq_group_metadata_list[0].is_prompt\n-        # Prepare input tensors.\n-        if is_prompt:\n-            inputs = self._prepare_prompt(seq_group_metadata_list)\n-            input_tokens, input_positions, input_metadata = inputs\n-        else:\n-            inputs = self._prepare_decode(seq_group_metadata_list)\n-            input_tokens, input_positions, input_metadata = inputs\n-\n+    ) -> Optional[SamplerOutput]:\n+        input_tokens, input_positions, input_metadata, sampling_metadata = (\n+            self.prepare_input_tensors(seq_group_metadata_list))\n         # Execute the model.\n         if input_metadata.use_cuda_graph:\n             graph_batch_size = input_tokens.shape[0]\n@@ -356,9 +457,6 @@ class ModelRunner:\n             input_metadata=input_metadata,\n         )\n \n-        sampling_metadata = self._prepare_sample(seq_group_metadata_list,\n-                                                 input_metadata.prompt_lens)\n-\n         # Sample the next token.\n         output = self.model.sample(\n             hidden_states=hidden_states,\n@@ -424,7 +522,7 @@ class ModelRunner:\n         for batch_size in reversed(_BATCH_SIZES_TO_CAPTURE):\n             # Create dummy input_metadata.\n             input_metadata = InputMetadata(\n-                prompt_lens=[],\n+                is_prompt=False,\n                 slot_mapping=slot_mapping[:batch_size],\n                 max_context_len=self.max_context_len_to_capture,\n                 context_lens=context_lens[:batch_size],",
      "change_type": "modified",
      "lines_added": 134,
      "lines_removed": 36
    },
    {
      "file_path": "vllm/worker/worker.py",
      "old_content": "\"\"\"A GPU worker class.\"\"\"\nimport os\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.distributed\n\nfrom vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                         SchedulerConfig)\nfrom vllm.model_executor import set_random_seed\nfrom vllm.model_executor.parallel_utils.parallel_state import (\n    initialize_model_parallel)\nfrom vllm.sequence import SamplerOutput, SequenceGroupMetadata\nfrom vllm.worker.cache_engine import CacheEngine\nfrom vllm.worker.model_runner import ModelRunner\n\n\nclass Worker:\n    \"\"\"A worker class that executes (a partition of) the model on a GPU.\n\n    Each worker is associated with a single GPU. The worker is responsible for\n    maintaining the KV cache and executing the model on the GPU. In case of\n    distributed inference, each worker is assigned a partition of the model.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        rank: Optional[int] = None,\n        distributed_init_method: Optional[str] = None,\n    ) -> None:\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.rank = rank\n        self.distributed_init_method = distributed_init_method\n\n        self.model_runner = ModelRunner(model_config, parallel_config,\n                                        scheduler_config)\n        # Uninitialized cache engine. Will be initialized by\n        # self.init_cache_engine().\n        self.cache_config = None\n        self.cache_engine = None\n        self.cache_events = None\n        self.gpu_cache = None\n\n    def init_model(self) -> None:\n        # torch.distributed.all_reduce does not free the input tensor until\n        # the synchronization point. This causes the memory usage to grow\n        # as the number of all_reduce calls increases. This env var disables\n        # this behavior.\n        # Related issue:\n        # https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573\n        os.environ[\"TORCH_NCCL_AVOID_RECORD_STREAMS\"] = \"1\"\n\n        # This env var set by Ray causes exceptions with graph building.\n        os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n        # Env vars will be set by Ray.\n        self.rank = self.rank if self.rank is not None else int(\n            os.getenv(\"RANK\", \"-1\"))\n        local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n        self.device = torch.device(f\"cuda:{local_rank}\")\n        if self.rank < 0:\n            raise ValueError(\"Invalid or unspecified rank.\")\n        torch.cuda.set_device(self.device)\n\n        _check_if_gpu_supports_dtype(self.model_config.dtype)\n\n        # Initialize the distributed environment.\n        _init_distributed_environment(self.parallel_config, self.rank,\n                                      self.distributed_init_method)\n\n        # Initialize the model.\n        set_random_seed(self.model_config.seed)\n\n    def load_model(self):\n        self.model_runner.load_model()\n\n    @torch.inference_mode()\n    def profile_num_available_blocks(\n        self,\n        block_size: int,\n        gpu_memory_utilization: float,\n        cpu_swap_space: int,\n    ) -> Tuple[int, int]:\n        # Profile the memory usage of the model and get the maximum number of\n        # cache blocks that can be allocated with the remaining free memory.\n        torch.cuda.empty_cache()\n\n        # Execute a forward pass with dummy inputs to profile the memory usage\n        # of the model.\n        self.model_runner.profile_run()\n\n        # Calculate the number of blocks that can be allocated with the\n        # profiled peak memory.\n        torch.cuda.synchronize()\n        free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n        peak_memory = total_gpu_memory - free_gpu_memory\n\n        cache_block_size = CacheEngine.get_cache_block_size(\n            block_size, self.model_config, self.parallel_config)\n        num_gpu_blocks = int(\n            (total_gpu_memory * gpu_memory_utilization - peak_memory) //\n            cache_block_size)\n        num_cpu_blocks = int(cpu_swap_space // cache_block_size)\n        num_gpu_blocks = max(num_gpu_blocks, 0)\n        num_cpu_blocks = max(num_cpu_blocks, 0)\n        torch.cuda.empty_cache()\n        return num_gpu_blocks, num_cpu_blocks\n\n    def init_cache_engine(self, cache_config: CacheConfig) -> None:\n        self.cache_config = cache_config\n        self.cache_engine = CacheEngine(self.cache_config, self.model_config,\n                                        self.parallel_config)\n        self.cache_events = self.cache_engine.events\n        self.gpu_cache = self.cache_engine.gpu_cache\n        self.model_runner.set_block_size(self.cache_engine.block_size)\n\n    def warm_up_model(self) -> None:\n        if not self.model_config.enforce_eager:\n            self.model_runner.capture_model(self.gpu_cache)\n        # Reset the seed to ensure that the random state is not affected by\n        # the model initialization and profiling.\n        set_random_seed(self.model_config.seed)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        blocks_to_swap_in: Dict[int, int],\n        blocks_to_swap_out: Dict[int, int],\n        blocks_to_copy: Dict[int, List[int]],\n    ) -> SamplerOutput:\n        # Issue cache operations.\n        issued_cache_op = False\n        if blocks_to_swap_in:\n            self.cache_engine.swap_in(blocks_to_swap_in)\n            issued_cache_op = True\n        if blocks_to_swap_out:\n            self.cache_engine.swap_out(blocks_to_swap_out)\n            issued_cache_op = True\n        if blocks_to_copy:\n            self.cache_engine.copy(blocks_to_copy)\n            issued_cache_op = True\n\n        cache_events = self.cache_events if issued_cache_op else None\n\n        # Wait for cache operations to finish.\n        # TODO(woosuk): Profile swapping overhead and optimize if needed.\n        if cache_events is not None:\n            for event in cache_events:\n                event.wait()\n        # If there is no input, we don't need to execute the model.\n        if not seq_group_metadata_list:\n            return {}\n\n        output = self.model_runner.execute_model(seq_group_metadata_list,\n                                                 self.gpu_cache)\n        return output\n\n\ndef _init_distributed_environment(\n    parallel_config: ParallelConfig,\n    rank: int,\n    distributed_init_method: Optional[str] = None,\n) -> None:\n    \"\"\"Initialize the distributed environment.\"\"\"\n    if torch.distributed.is_initialized():\n        torch_world_size = torch.distributed.get_world_size()\n        if torch_world_size != parallel_config.world_size:\n            raise RuntimeError(\n                \"torch.distributed is already initialized but the torch world \"\n                \"size does not match parallel_config.world_size \"\n                f\"({torch_world_size} vs. {parallel_config.world_size}).\")\n    elif not distributed_init_method:\n        raise ValueError(\n            \"distributed_init_method must be set if torch.distributed \"\n            \"is not already initialized\")\n    else:\n        torch.distributed.init_process_group(\n            backend=\"nccl\",\n            world_size=parallel_config.world_size,\n            rank=rank,\n            init_method=distributed_init_method,\n        )\n\n    # A small all_reduce for warmup.\n    torch.distributed.all_reduce(torch.zeros(1).cuda())\n    initialize_model_parallel(parallel_config.tensor_parallel_size,\n                              parallel_config.pipeline_parallel_size)\n\n\ndef _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):\n    # Check if the GPU supports the dtype.\n    if torch_dtype == torch.bfloat16:\n        compute_capability = torch.cuda.get_device_capability()\n        if compute_capability[0] < 8:\n            gpu_name = torch.cuda.get_device_name()\n            raise ValueError(\n                \"Bfloat16 is only supported on GPUs with compute capability \"\n                f\"of at least 8.0. Your {gpu_name} GPU has compute capability \"\n                f\"{compute_capability[0]}.{compute_capability[1]}.\")\n",
      "diff": "diff --git a/vllm/worker/worker.py b/vllm/worker/worker.py\nindex 8698b1572..6c83f708b 100644\n--- a/vllm/worker/worker.py\n+++ b/vllm/worker/worker.py\n@@ -8,6 +8,8 @@ import torch.distributed\n from vllm.config import (CacheConfig, ModelConfig, ParallelConfig,\n                          SchedulerConfig)\n from vllm.model_executor import set_random_seed\n+from vllm.model_executor.parallel_utils.communication_op import (\n+    broadcast_object_list)\n from vllm.model_executor.parallel_utils.parallel_state import (\n     initialize_model_parallel)\n from vllm.sequence import SamplerOutput, SequenceGroupMetadata\n@@ -28,17 +30,23 @@ class Worker:\n         model_config: ModelConfig,\n         parallel_config: ParallelConfig,\n         scheduler_config: SchedulerConfig,\n-        rank: Optional[int] = None,\n-        distributed_init_method: Optional[str] = None,\n+        local_rank: int,\n+        rank: int,\n+        distributed_init_method: str,\n+        is_driver_worker: bool = False,\n     ) -> None:\n         self.model_config = model_config\n         self.parallel_config = parallel_config\n         self.scheduler_config = scheduler_config\n+        self.local_rank = local_rank\n         self.rank = rank\n         self.distributed_init_method = distributed_init_method\n+        self.is_driver_worker = is_driver_worker\n+        if self.is_driver_worker:\n+            assert self.rank == 0, \"The driver worker must have rank 0.\"\n \n         self.model_runner = ModelRunner(model_config, parallel_config,\n-                                        scheduler_config)\n+                                        scheduler_config, is_driver_worker)\n         # Uninitialized cache engine. Will be initialized by\n         # self.init_cache_engine().\n         self.cache_config = None\n@@ -57,13 +65,7 @@ class Worker:\n \n         # This env var set by Ray causes exceptions with graph building.\n         os.environ.pop(\"NCCL_ASYNC_ERROR_HANDLING\", None)\n-        # Env vars will be set by Ray.\n-        self.rank = self.rank if self.rank is not None else int(\n-            os.getenv(\"RANK\", \"-1\"))\n-        local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n-        self.device = torch.device(f\"cuda:{local_rank}\")\n-        if self.rank < 0:\n-            raise ValueError(\"Invalid or unspecified rank.\")\n+        self.device = torch.device(f\"cuda:{self.local_rank}\")\n         torch.cuda.set_device(self.device)\n \n         _check_if_gpu_supports_dtype(self.model_config.dtype)\n@@ -125,14 +127,12 @@ class Worker:\n         # the model initialization and profiling.\n         set_random_seed(self.model_config.seed)\n \n-    @torch.inference_mode()\n-    def execute_model(\n+    def cache_swap(\n         self,\n-        seq_group_metadata_list: List[SequenceGroupMetadata],\n         blocks_to_swap_in: Dict[int, int],\n         blocks_to_swap_out: Dict[int, int],\n         blocks_to_copy: Dict[int, List[int]],\n-    ) -> SamplerOutput:\n+    ) -> None:\n         # Issue cache operations.\n         issued_cache_op = False\n         if blocks_to_swap_in:\n@@ -152,8 +152,38 @@ class Worker:\n         if cache_events is not None:\n             for event in cache_events:\n                 event.wait()\n+\n+    @torch.inference_mode()\n+    def execute_model(\n+        self,\n+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None,\n+        blocks_to_swap_in: Optional[Dict[int, int]] = None,\n+        blocks_to_swap_out: Optional[Dict[int, int]] = None,\n+        blocks_to_copy: Optional[Dict[int, List[int]]] = None,\n+    ) -> Optional[SamplerOutput]:\n+        if self.is_driver_worker:\n+            assert seq_group_metadata_list is not None\n+            num_seq_groups = len(seq_group_metadata_list)\n+            assert blocks_to_swap_in is not None\n+            assert blocks_to_swap_out is not None\n+            assert blocks_to_copy is not None\n+            block_swapping_info = [\n+                blocks_to_swap_in, blocks_to_swap_out, blocks_to_copy\n+            ]\n+            broadcast_object_list([num_seq_groups] + block_swapping_info,\n+                                  src=0)\n+        else:\n+            # num_seq_groups, blocks_to_swap_in, blocks_to_swap_out,\n+            # blocks_to_copy (4 elements)\n+            recv_data = [None] * 4\n+            broadcast_object_list(recv_data, src=0)\n+            num_seq_groups = recv_data[0]\n+            block_swapping_info = recv_data[1:]\n+\n+        self.cache_swap(*block_swapping_info)\n+\n         # If there is no input, we don't need to execute the model.\n-        if not seq_group_metadata_list:\n+        if num_seq_groups == 0:\n             return {}\n \n         output = self.model_runner.execute_model(seq_group_metadata_list,",
      "change_type": "modified",
      "lines_added": 46,
      "lines_removed": 16
    }
  ],
  "affected_apis": [],
  "summary": {
    "total_files": 34,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 34
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "TRUE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (but too much BS again)",
    "is_benchmark_actually_there": "",
    "sample_clues": "adding_model, aquila, async_llm_engine"
  }
}