{
  "commit_hash": "fa63e710c7fbaae3a445f669d3b5ba6b9a4ef412",
  "parent_hash": "2a0309a646b1ed83a0c40974e08c8dc628726d3c",
  "message": "[V1][Perf] Reduce scheduling overhead in model runner after cuda sync (#12094)\n\nSigned-off-by: Keyun Tong <tongkeyun@gmail.com>",
  "author": "Keyun Tong <tongkeyun@gmail.com>",
  "date": "2025-01-26 00:42:37 -0800",
  "files_changed": [
    {
      "file_path": "vllm/v1/outputs.py",
      "old_content": "from dataclasses import dataclass\nfrom typing import Dict, List, Optional\n\nimport torch\n\n\n@dataclass\nclass SamplerOutput:\n\n    # [num_reqs]\n    sampled_token_ids: List[int]\n\n    # [num_reqs, max_num_logprobs + 1]\n    logprob_token_ids: Optional[torch.Tensor]\n    # [num_reqs, max_num_logprobs + 1]\n    logprobs: Optional[torch.Tensor]\n\n    # TODO: Support prompt logprobs.\n    prompt_logprob_token_ids: Optional[torch.Tensor]\n    prompt_logprobs: Optional[torch.Tensor]\n\n\n# ModelRunnerOutput is serialized and sent to the scheduler process.\n# This is expensive for torch.Tensor so prefer to use List instead.\n@dataclass\nclass ModelRunnerOutput:\n\n    # [num_reqs]\n    req_ids: List[str]\n    # req_id -> index\n    req_id_to_index: Dict[str, int]\n\n    # [num_reqs]\n    sampled_token_ids: List[int]\n\n    # [num_reqs, max_num_logprobs + 1]\n    logprob_token_ids_cpu: Optional[torch.Tensor]\n    # [num_reqs, max_num_logprobs + 1]\n    logprobs_cpu: Optional[torch.Tensor]\n",
      "diff": "diff --git a/vllm/v1/outputs.py b/vllm/v1/outputs.py\nindex acc3a944e..32aee44e3 100644\n--- a/vllm/v1/outputs.py\n+++ b/vllm/v1/outputs.py\n@@ -8,7 +8,7 @@ import torch\n class SamplerOutput:\n \n     # [num_reqs]\n-    sampled_token_ids: List[int]\n+    sampled_token_ids: torch.Tensor\n \n     # [num_reqs, max_num_logprobs + 1]\n     logprob_token_ids: Optional[torch.Tensor]",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/v1/sample/sampler.py",
      "old_content": "\"\"\"A layer that samples the next tokens from the model's outputs.\"\"\"\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom vllm.v1.outputs import SamplerOutput\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.sample.ops.penalties import (apply_all_penalties,\n                                          apply_min_token_penalties)\nfrom vllm.v1.sample.ops.topk_topp_sampler import TopKTopPSampler\n\n_SAMPLING_EPS = 1e-5\n\n\nclass Sampler(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.topk_topp_sampler = TopKTopPSampler()\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> SamplerOutput:\n        needs_logprobs = sampling_metadata.max_num_logprobs > 0\n        if needs_logprobs:\n            # NOTE(woosuk): Use the original logits (before any penalties or\n            # temperature scaling) for the top-k logprobs.\n            # This is different from the V0 sampler, which uses the logits that\n            # is used for sampling (after penalties and temperature scaling).\n            # NOTE: We compute logprobs first because the below ops may\n            # modify the logits tensor in-place (and we don't want to clone\n            # the logits tensor for memory efficiency).\n            topk_logprobs, topk_indices = self.get_topk_logprobs(\n                logits, sampling_metadata)\n        else:\n            topk_logprobs = None\n            topk_indices = None\n\n        # Use float32 for the logits.\n        logits = logits.to(torch.float32)\n        # Apply penalties (e.g., min_tokens, freq_penalties).\n        logits = self.apply_penalties(logits, sampling_metadata)\n        # Apply temperature.\n        logits = self.apply_temperature(logits, sampling_metadata.temperature)\n        # Sample the next token.\n        sampled = self.sample(logits, sampling_metadata)\n        # Use int32 to reduce the tensor size.\n        sampled = sampled.to(torch.int32)\n\n        # NOTE: CPU-GPU synchronization happens here.\n        sampler_output = SamplerOutput(\n            sampled_token_ids=sampled.tolist(),\n            logprob_token_ids=topk_indices,\n            logprobs=topk_logprobs,\n            prompt_logprob_token_ids=None,\n            prompt_logprobs=None,\n        )\n        return sampler_output\n\n    def apply_temperature(\n        self,\n        logits: torch.Tensor,\n        temp: torch.Tensor,\n    ) -> torch.Tensor:\n        # Avoid division by zero.\n        temp = torch.where(temp < _SAMPLING_EPS, 1.0, temp)\n        # Use in-place division to avoid creating a new tensor.\n        logits.div_(temp.unsqueeze(dim=1))\n        return logits\n\n    def greedy_sample(self, logits: torch.Tensor) -> torch.Tensor:\n        return logits.argmax(dim=-1).view(-1)\n\n    def sample(\n        self,\n        logits: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> torch.Tensor:\n        assert not (sampling_metadata.all_greedy\n                    and sampling_metadata.all_random)\n        if sampling_metadata.all_greedy:\n            return self.greedy_sample(logits)\n\n        random_sampled = self.topk_topp_sampler(\n            logits,\n            sampling_metadata.generators,\n            sampling_metadata.no_top_k,\n            sampling_metadata.top_k,\n            sampling_metadata.no_top_p,\n            sampling_metadata.top_p,\n        )\n        if sampling_metadata.all_random:\n            return random_sampled\n\n        greedy_sampled = self.greedy_sample(logits)\n        sampled = torch.where(\n            sampling_metadata.temperature < _SAMPLING_EPS,\n            greedy_sampled,\n            random_sampled,\n        )\n        return sampled\n\n    def get_topk_logprobs(\n        self,\n        logits: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        logprobs = logits.log_softmax(dim=-1, dtype=torch.float32)\n        # FIXME: Mask the sampled token_id, get topk logprobs,\n        # and concatenate the topk with the sampled token_id.\n        topk_logprobs, topk_indices = torch.topk(\n            logprobs, sampling_metadata.max_num_logprobs, dim=-1)\n        # Use int32 to reduce the tensor size.\n        topk_indices = topk_indices.to(torch.int32)\n        return topk_logprobs, topk_indices\n\n    def apply_penalties(\n        self,\n        logits: torch.Tensor,\n        sampling_metadata: SamplingMetadata,\n    ) -> torch.Tensor:\n        apply_min_token_penalties(logits, sampling_metadata.output_token_ids,\n                                  sampling_metadata.stop_token_ids,\n                                  sampling_metadata.min_tokens)\n        if not sampling_metadata.no_penalties:\n            assert sampling_metadata.prompt_token_ids is not None\n            logits = apply_all_penalties(\n                logits, sampling_metadata.prompt_token_ids,\n                sampling_metadata.presence_penalties,\n                sampling_metadata.frequency_penalties,\n                sampling_metadata.repetition_penalties,\n                sampling_metadata.output_token_ids)\n        return logits\n",
      "diff": "diff --git a/vllm/v1/sample/sampler.py b/vllm/v1/sample/sampler.py\nindex 7cd42ca21..9ad665a64 100644\n--- a/vllm/v1/sample/sampler.py\n+++ b/vllm/v1/sample/sampler.py\n@@ -50,9 +50,8 @@ class Sampler(nn.Module):\n         # Use int32 to reduce the tensor size.\n         sampled = sampled.to(torch.int32)\n \n-        # NOTE: CPU-GPU synchronization happens here.\n         sampler_output = SamplerOutput(\n-            sampled_token_ids=sampled.tolist(),\n+            sampled_token_ids=sampled,\n             logprob_token_ids=topk_indices,\n             logprobs=topk_logprobs,\n             prompt_logprob_token_ids=None,",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/v1/worker/gpu_model_runner.py",
      "old_content": "import gc\nimport time\nfrom typing import TYPE_CHECKING, Dict, List, Optional, Tuple, cast\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\nfrom vllm.attention.backends.abstract import AttentionType\nfrom vllm.attention.layer import Attention\nfrom vllm.config import CompilationLevel, VllmConfig\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.forward_context import set_forward_context\nfrom vllm.inputs import INPUT_REGISTRY\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalKwargs\nfrom vllm.multimodal.utils import group_mm_inputs_by_modality\nfrom vllm.sampling_params import SamplingType\nfrom vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, DeviceMemoryProfiler,\n                        LayerBlockType, cdiv, is_pin_memory_available)\nfrom vllm.v1.attention.backends.flash_attn import (FlashAttentionBackend,\n                                                   FlashAttentionMetadata)\nfrom vllm.v1.core.encoder_cache_manager import compute_encoder_budget\nfrom vllm.v1.engine.mm_input_mapper import MMInputMapperClient\nfrom vllm.v1.kv_cache_interface import (FullAttentionSpec, KVCacheConfig,\n                                        KVCacheSpec)\nfrom vllm.v1.outputs import ModelRunnerOutput\nfrom vllm.v1.sample.metadata import SamplingMetadata\nfrom vllm.v1.utils import bind_kv_cache\nfrom vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch\n\nif TYPE_CHECKING:\n    from vllm.v1.core.scheduler import SchedulerOutput\n\nlogger = init_logger(__name__)\n\n\nclass GPUModelRunner:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        device: torch.device,\n    ):\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.load_config = vllm_config.load_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.speculative_config = vllm_config.speculative_config\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config\n        self.observability_config = vllm_config.observability_config\n\n        model_config = self.model_config\n        cache_config = self.cache_config\n        scheduler_config = self.scheduler_config\n        parallel_config = self.parallel_config\n        self.device = device\n        self.pin_memory = is_pin_memory_available()\n        self.dtype = self.model_config.dtype\n        if cache_config.cache_dtype == \"auto\":\n            self.kv_cache_dtype = self.dtype\n        else:\n            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[\n                cache_config.cache_dtype]\n\n        self.is_multimodal_model = model_config.is_multimodal_model\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_model_len = model_config.max_model_len\n        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)\n        self.max_num_tokens = scheduler_config.max_num_batched_tokens\n        self.max_num_reqs = scheduler_config.max_num_seqs\n\n        # Model-related.\n        self.num_attn_layers = model_config.get_num_layers_by_block_type(\n            parallel_config, LayerBlockType.attention)\n        self.num_query_heads = model_config.get_num_attention_heads(\n            parallel_config)\n        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)\n        self.head_size = model_config.get_head_size()\n        self.hidden_size = model_config.get_hidden_size()\n\n        # Multi-modal data support\n        self.input_registry = INPUT_REGISTRY\n        self.mm_registry = MULTIMODAL_REGISTRY\n\n        # NOTE: Initialized input mapper is only used for processing dummy\n        # multimodal data into multimodal kwargs for GPU memory profiling.\n        self.mm_input_mapper_profiling = MMInputMapperClient(self.model_config)\n        self.mm_input_mapper_profiling.use_cache = False\n\n        encoder_compute_budget, encoder_cache_size = compute_encoder_budget(\n            model_config=model_config,\n            scheduler_config=scheduler_config,\n        )\n        self.max_num_encoder_input_tokens = encoder_compute_budget\n        self.encoder_cache_size = encoder_cache_size\n\n        # Lazy initialization\n        # self.model: nn.Module  # Set after load_model\n        self.kv_caches: List[torch.Tensor] = []\n        # req_id -> (input_id -> encoder_output)\n        self.encoder_cache: Dict[str, Dict[int, torch.Tensor]] = {}\n\n        # Request states.\n        self.requests: Dict[str, CachedRequestState] = {}\n        # Persistent batch.\n        self.input_batch = InputBatch(\n            max_num_reqs=self.max_num_reqs,\n            max_model_len=self.max_model_len,\n            max_num_blocks_per_req=self.max_num_blocks_per_req,\n            device=self.device,\n            pin_memory=self.pin_memory,\n            vocab_size=model_config.get_vocab_size(),\n        )\n\n        self.use_cuda_graph = (self.vllm_config.compilation_config.level\n                               == CompilationLevel.PIECEWISE\n                               and not self.model_config.enforce_eager)\n        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.\n        # The convention is different.\n        # self.cudagraph_batch_sizes sorts in ascending order.\n        # The batch sizes in the config are in descending order.\n        self.cudagraph_batch_sizes = list(\n            reversed(\n                self.vllm_config.compilation_config.cudagraph_capture_sizes))\n\n        # Cache the device properties.\n        self.device_properties = torch.cuda.get_device_properties(self.device)\n        self.num_sms = self.device_properties.multi_processor_count\n\n        # Persistent buffers for CUDA graphs.\n        self.input_ids = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int32,\n                                     device=self.device)\n        self.positions = torch.zeros(self.max_num_tokens,\n                                     dtype=torch.int64,\n                                     device=self.device)\n\n        # Only relevant for models using M-RoPE (e.g, Qwen2-VL)\n        if self.model_config.uses_mrope:\n            # NOTE: `mrope_positions` is implemented with one additional dummy\n            # position on purpose to make it non-contiguous so that it can work\n            # with torch compile.\n            # See detailed explanation in https://github.com/vllm-project/vllm/pull/12128#discussion_r1926431923\n\n            # NOTE: When M-RoPE is enabled, position ids are 3D regardless of\n            # the modality of inputs. For text-only inputs, each dimension has\n            # identical position IDs, making M-RoPE functionally equivalent to\n            # 1D-RoPE.\n            # See page 5 of https://arxiv.org/abs/2409.12191\n            self.mrope_positions = torch.zeros((3, self.max_num_tokens + 1),\n                                               dtype=torch.int64,\n                                               device=self.device)\n            self.mrope_positions_cpu = torch.zeros(\n                (3, self.max_num_tokens + 1),\n                dtype=torch.int64,\n                device=\"cpu\",\n                pin_memory=self.pin_memory)\n\n        self.inputs_embeds = torch.zeros(\n            (self.max_num_tokens, self.hidden_size),\n            dtype=self.dtype,\n            device=self.device)\n\n        # OPTIMIZATION: Cache the tensors rather than creating them every step.\n        self.arange_np = np.arange(max(self.max_num_reqs + 1,\n                                       self.max_model_len),\n                                   dtype=np.int32)\n        # NOTE(woosuk): These tensors are \"stateless\", i.e., they are literally\n        # a faster version of creating a new tensor every time. Thus, we should\n        # not make any assumptions about the values in these tensors.\n        self.input_ids_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int32,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.input_ids_np = self.input_ids_cpu.numpy()\n        self.positions_cpu = torch.zeros(self.max_num_tokens,\n                                         dtype=torch.int64,\n                                         device=\"cpu\",\n                                         pin_memory=self.pin_memory)\n        self.positions_np = self.positions_cpu.numpy()\n        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,\n                                            dtype=torch.int32,\n                                            device=\"cpu\",\n                                            pin_memory=self.pin_memory)\n        self.slot_mapping_np = self.slot_mapping_cpu.numpy()\n        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,\n                                               dtype=torch.int32,\n                                               device=\"cpu\",\n                                               pin_memory=self.pin_memory)\n        self.query_start_loc_np = self.query_start_loc_cpu.numpy()\n        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,\n                                        dtype=torch.int32,\n                                        device=\"cpu\",\n                                        pin_memory=self.pin_memory)\n        self.seq_lens_np = self.seq_lens_cpu.numpy()\n\n    def _update_states(self, scheduler_output: \"SchedulerOutput\") -> None:\n        # Remove stopped requests from the cached states.\n        # Keep the states of the pre-empted requests.\n        for req_id in scheduler_output.finished_req_ids:\n            self.requests.pop(req_id, None)\n            self.encoder_cache.pop(req_id, None)\n\n        # Free the cached encoder outputs.\n        for req_id, input_id in scheduler_output.free_encoder_input_ids:\n            encoder_outputs = self.encoder_cache.get(req_id)\n            if encoder_outputs is not None:\n                encoder_outputs.pop(input_id, None)\n                if not encoder_outputs:\n                    self.encoder_cache.pop(req_id, None)\n\n        # Remove the requests from the persistent batch.\n        stopped_req_ids = set().union(\n            scheduler_output.preempted_req_ids,\n            scheduler_output.finished_req_ids,\n        )\n        removed_req_indices: List[int] = []\n        for req_id in stopped_req_ids:\n            req_index = self.input_batch.remove_request(req_id)\n            if req_index is not None:\n                removed_req_indices.append(req_index)\n\n        # Update the states of the running requests.\n        for req_data in scheduler_output.scheduled_running_reqs:\n            req_id = req_data.req_id\n            req_state = self.requests[req_id]\n            req_index = self.input_batch.req_id_to_index[req_id]\n\n            # Update the num_computed_tokens.\n            req_state.num_computed_tokens = req_data.num_computed_tokens\n            self.input_batch.num_computed_tokens_cpu[req_index] = (\n                req_data.num_computed_tokens)\n\n            # Update the block table.\n            num_new_blocks = len(req_data.new_block_ids)\n            if num_new_blocks == 0:\n                continue\n            start_index = len(req_state.block_ids)\n            req_state.block_ids.extend(req_data.new_block_ids)\n            self.input_batch.block_table.append_row(req_index, start_index,\n                                                    req_data.new_block_ids)\n\n        req_ids_to_add: List[str] = []\n        # Add new requests to the cached states.\n        for new_req_data in scheduler_output.scheduled_new_reqs:\n            req_id = new_req_data.req_id\n            sampling_params = new_req_data.sampling_params\n            if sampling_params.sampling_type == SamplingType.RANDOM_SEED:\n                generator = torch.Generator(device=self.device)\n                generator.manual_seed(sampling_params.seed)\n            else:\n                generator = None\n\n            self.requests[req_id] = CachedRequestState(\n                req_id=req_id,\n                prompt_token_ids=new_req_data.prompt_token_ids,\n                prompt=new_req_data.prompt,\n                mm_inputs=new_req_data.mm_inputs,\n                mm_positions=new_req_data.mm_positions,\n                sampling_params=sampling_params,\n                generator=generator,\n                block_ids=new_req_data.block_ids,\n                num_computed_tokens=new_req_data.num_computed_tokens,\n                output_token_ids=[],\n            )\n\n            # Only relevant for models using M-RoPE (e.g, Qwen2-VL)\n            if self.model_config.uses_mrope:\n                image_grid_thw = []\n                video_grid_thw = []\n                for mm_input in self.requests[req_id].mm_inputs:\n                    if mm_input.get(\"image_grid_thw\") is not None:\n                        image_grid_thw.extend(\n                            mm_input[\"image_grid_thw\"].tolist())\n                    if mm_input.get(\"video_grid_thw\") is not None:\n                        video_grid_thw.extend(\n                            mm_input[\"video_grid_thw\"].tolist())\n\n                hf_config = self.model_config.hf_config\n\n                self.requests[req_id].mrope_positions, \\\n                    self.requests[req_id].mrope_position_delta = \\\n                    MRotaryEmbedding.get_input_positions_tensor(\n                        self.requests[req_id].prompt_token_ids,\n                        image_grid_thw=image_grid_thw,\n                        video_grid_thw=video_grid_thw,\n                        image_token_id=hf_config.image_token_id,\n                        video_token_id=hf_config.video_token_id,\n                        vision_start_token_id=hf_config.vision_start_token_id,\n                        vision_end_token_id=hf_config.vision_end_token_id,\n                        spatial_merge_size=hf_config.vision_config.\n                        spatial_merge_size,\n                    )\n\n            req_ids_to_add.append(req_id)\n\n        # Update the cached states of the resumed requests.\n        for res_req_data in scheduler_output.scheduled_resumed_reqs:\n            req_id = res_req_data.req_id\n            req_state = self.requests[req_id]\n\n            req_state.block_ids = res_req_data.block_ids\n            req_state.num_computed_tokens = res_req_data.num_computed_tokens\n            req_ids_to_add.append(req_id)\n\n        # Add the new or resumed requests to the persistent batch.\n        # The smaller empty indices are filled first.\n        removed_req_indices = sorted(removed_req_indices, reverse=True)\n        for req_id in req_ids_to_add:\n            req_state = self.requests[req_id]\n            if removed_req_indices:\n                # Fill the empty index.\n                req_index = removed_req_indices.pop()\n            else:\n                # Append to the end.\n                req_index = None\n            self.input_batch.add_request(req_state, req_index)\n\n        # Condense the batched states if there are empty indices.\n        if removed_req_indices:\n            self.input_batch.condense(removed_req_indices)\n\n    def _prepare_inputs(self, scheduler_output: \"SchedulerOutput\"):\n        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        assert total_num_scheduled_tokens > 0\n        num_reqs = self.input_batch.num_reqs\n        assert num_reqs > 0\n\n        # OPTIMIZATION: Start copying the block table first.\n        # This way, we can overlap the copy with the following CPU operations.\n        self.input_batch.block_table.commit(num_reqs)\n\n        # Get the number of scheduled tokens for each request.\n        # TODO: The Python loop can be slow. Optimize.\n        num_scheduled_tokens = []\n        max_num_scheduled_tokens = 0\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_tokens = scheduler_output.num_scheduled_tokens[req_id]\n            num_scheduled_tokens.append(num_tokens)\n            max_num_scheduled_tokens = max(max_num_scheduled_tokens,\n                                           num_tokens)\n        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)\n        assert max_num_scheduled_tokens > 0\n\n        # Get request indices.\n        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]\n        req_indices = np.repeat(self.arange_np[:num_reqs],\n                                num_scheduled_tokens)\n\n        # Get batched arange.\n        # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        arange = np.concatenate(\n            [self.arange_np[:n] for n in num_scheduled_tokens])\n\n        # Get positions.\n        positions_np = self.positions_np[:total_num_scheduled_tokens]\n        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],\n               arange,\n               out=positions_np)\n\n        # Calculate M-RoPE positions.\n        # Only relevant for models using M-RoPE (e.g, Qwen2-VL)\n        if self.model_config.uses_mrope:\n            self._calc_mrope_positions(scheduler_output)\n\n        # Get token indices.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]\n        # where M is the max_model_len.\n        token_indices = (positions_np +\n                         req_indices * self.input_batch.token_ids_cpu.shape[1])\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),\n                           0,\n                           torch.from_numpy(token_indices),\n                           out=self.input_ids_cpu[:total_num_scheduled_tokens])\n\n        # Calculate the slot mapping.\n        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]\n        # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]\n        # where K is the max_num_blocks_per_req and the block size is 2.\n        # NOTE(woosuk): We can't simply use `token_indices // block_size` here\n        # because M (max_model_len) is not necessarily divisible by block_size.\n        block_table_indices = (req_indices * self.max_num_blocks_per_req +\n                               positions_np // self.block_size)\n        # NOTE(woosuk): We use torch.index_select instead of np.take here\n        # because torch.index_select is much faster than np.take for large\n        # tensors.\n        block_table_cpu = self.input_batch.block_table.get_cpu_tensor()\n        block_numbers = block_table_cpu.flatten()[block_table_indices].numpy()\n        block_offsets = positions_np % self.block_size\n        np.add(block_numbers * self.block_size,\n               block_offsets,\n               out=self.slot_mapping_np[:total_num_scheduled_tokens])\n\n        # Prepare the attention metadata.\n        self.query_start_loc_np[0] = 0\n        np.cumsum(num_scheduled_tokens,\n                  out=self.query_start_loc_np[1:num_reqs + 1])\n\n        self.seq_lens_np[:num_reqs] = (\n            self.input_batch.num_computed_tokens_cpu[:num_reqs] +\n            num_scheduled_tokens)\n        max_seq_len = self.seq_lens_np[:num_reqs].max()\n\n        # Copy the tensors to the GPU.\n        self.input_ids[:total_num_scheduled_tokens].copy_(\n            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)\n        if self.model_config.uses_mrope:\n            # Only relevant for models using M-RoPE (e.g, Qwen2-VL)\n            self.mrope_positions[:, :total_num_scheduled_tokens].copy_(\n                self.mrope_positions_cpu[:, :total_num_scheduled_tokens],\n                non_blocking=True)\n        else:\n            # Common case (1D positions)\n            self.positions[:total_num_scheduled_tokens].copy_(\n                self.positions_cpu[:total_num_scheduled_tokens],\n                non_blocking=True)\n        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(\n            self.device, non_blocking=True)\n        seq_lens = self.seq_lens_cpu[:num_reqs].to(self.device,\n                                                   non_blocking=True)\n        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(\n            self.device, non_blocking=True).long()\n\n        # Prepare for cascade attention if needed.\n        common_prefix_len = (scheduler_output.num_common_prefix_blocks *\n                             self.block_size)\n        if common_prefix_len == 0:\n            # Common case.\n            use_cascade = False\n        else:\n            # NOTE(woosuk): Cascade attention uses two attention kernels: one\n            # for the common prefix and the other for the rest. For the first\n            # kernel, we concatenate all the query tokens (possibly from\n            # different requests) and treat them as if they are from the same\n            # request. Then, we use bi-directional attention to process the\n            # common prefix in the KV cache. Importantly, this means that the\n            # first kernel does not do any masking.\n\n            # Consider the following example:\n            # Request 1's input query: [D, E, X]\n            # Request 1's kv cache: [A, B, C, D, E, X]\n            # Request 1's num_computed_tokens: 3 (i.e., [A, B, C])\n            # Request 2's input query: [E, Y]\n            # Request 2's kv cache: [A, B, C, D, E, Y]\n            # Request 2's num_computed_tokens: 4 (i.e., [A, B, C, D])\n\n            # If we use [A, B, C, D, E] as the common prefix, then the\n            # first kernel will compute the bi-directional attention between\n            # input query [D, E, X, E, Y] and common prefix [A, B, C, D, E].\n            # However, this is wrong because D in Request 1 should not attend to\n            # E in the common prefix (i.e., we need masking).\n            # To avoid this, [A, B, C, D] should be the common prefix.\n            # That is, the common prefix should be capped by the minimum\n            # num_computed_tokens among the requests, and plus one to include\n            # the first token of the query.\n\n            # In practice, we use [A, B, C] as the common prefix, instead of\n            # [A, B, C, D] (i.e., the common prefix is capped by the minimum\n            # num_computed_tokens, without plus one).\n            # This is because of an implementation detail: We want to always\n            # use two kernels for cascade attention. Let's imagine:\n            # Request 3's input query: [D]\n            # Request 3's kv cache: [A, B, C, D]\n            # Request 3's num_computed_tokens: 4 (i.e., [A, B, C, D])\n            # If we use [A, B, C, D] as the common prefix for Request 1-3,\n            # then Request 3 will be processed only by the first kernel,\n            # and the second kernel will get an empty input. While this is not\n            # a fundamental problem, our current implementation does not support\n            # this case.\n            common_prefix_len = min(\n                common_prefix_len,\n                self.input_batch.num_computed_tokens_cpu[:num_reqs].min())\n            # common_prefix_len should be a multiple of the block size.\n            common_prefix_len = (common_prefix_len // self.block_size *\n                                 self.block_size)\n            use_cascade = FlashAttentionBackend.use_cascade_attention(\n                common_prefix_len=common_prefix_len,\n                query_lens=num_scheduled_tokens,\n                num_query_heads=self.num_query_heads,\n                num_kv_heads=self.num_kv_heads,\n                use_alibi=False,  # FIXME\n                use_sliding_window=self.sliding_window is not None,\n                num_sms=self.num_sms,\n            )\n\n        if use_cascade:\n            # TODO: Optimize.\n            cu_prefix_query_lens = torch.tensor(\n                [0, total_num_scheduled_tokens],\n                dtype=torch.int32,\n                device=self.device)\n            prefix_kv_lens = torch.tensor([common_prefix_len],\n                                          dtype=torch.int32,\n                                          device=self.device)\n            suffix_kv_lens = (self.seq_lens_np[:num_reqs] - common_prefix_len)\n            suffix_kv_lens = torch.from_numpy(suffix_kv_lens).to(self.device)\n        else:\n            cu_prefix_query_lens = None\n            prefix_kv_lens = None\n            suffix_kv_lens = None\n\n        attn_metadata = FlashAttentionMetadata(\n            num_actual_tokens=total_num_scheduled_tokens,\n            max_query_len=max_num_scheduled_tokens,\n            query_start_loc=query_start_loc,\n            max_seq_len=max_seq_len,\n            seq_lens=seq_lens,\n            block_table=(\n                self.input_batch.block_table.get_device_tensor()[:num_reqs]),\n            slot_mapping=slot_mapping,\n            use_cascade=use_cascade,\n            common_prefix_len=common_prefix_len,\n            cu_prefix_query_lens=cu_prefix_query_lens,\n            prefix_kv_lens=prefix_kv_lens,\n            suffix_kv_lens=suffix_kv_lens,\n        )\n        # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial\n        # request in the batch. While we should not sample any token from this\n        # partial request, we do so for simplicity. We will ignore the sampled\n        # token from the partial request.\n        # TODO: Support prompt logprobs.\n        logits_indices = query_start_loc[1:] - 1\n        return attn_metadata, logits_indices\n\n    def _calc_mrope_positions(self, scheduler_output: \"SchedulerOutput\"):\n        mrope_pos_ptr = 0\n        num_reqs = self.input_batch.num_reqs\n        for index, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n            assert req_id is not None\n\n            req = self.requests[req_id]\n            assert req.mrope_positions is not None\n\n            num_computed_tokens = \\\n                self.input_batch.num_computed_tokens_cpu[index]\n            num_scheduled_tokens = \\\n                scheduler_output.num_scheduled_tokens[req_id]\n            num_prompt_tokens = len(req.prompt_token_ids)\n\n            if num_computed_tokens + num_scheduled_tokens > num_prompt_tokens:\n                prompt_part_len = max(0,\n                                      num_prompt_tokens - num_computed_tokens)\n                completion_part_len = max(\n                    0, num_scheduled_tokens - prompt_part_len)\n            else:\n                prompt_part_len = num_scheduled_tokens\n                completion_part_len = 0\n\n            assert num_scheduled_tokens == prompt_part_len + completion_part_len\n\n            if prompt_part_len > 0:\n                # prompt's mrope_positions are pre-computed\n                dst_start = mrope_pos_ptr\n                dst_end = mrope_pos_ptr + prompt_part_len\n                src_start = num_computed_tokens\n                src_end = num_computed_tokens + prompt_part_len\n\n                self.mrope_positions_cpu[:, dst_start:dst_end] = \\\n                    req.mrope_positions[:,src_start:src_end]\n\n                mrope_pos_ptr += prompt_part_len\n\n            if completion_part_len > 0:\n                # compute completion's mrope_positions on-the-fly\n                dst_start = mrope_pos_ptr\n                dst_end = mrope_pos_ptr + completion_part_len\n\n                self.mrope_positions_cpu[:, dst_start:dst_end] = \\\n                    MRotaryEmbedding.get_next_input_positions_tensor(\n                        req.mrope_position_delta,\n                        context_len=num_computed_tokens +\n                        prompt_part_len,\n                        seq_len=num_computed_tokens +\n                        prompt_part_len +\n                        completion_part_len,\n                    )\n\n                mrope_pos_ptr += completion_part_len\n\n    def _prepare_sampling(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> SamplingMetadata:\n        skip_copy = True\n        if (scheduler_output.finished_req_ids\n                or scheduler_output.preempted_req_ids):\n            skip_copy = False\n        if (scheduler_output.scheduled_new_reqs\n                or scheduler_output.scheduled_resumed_reqs):\n            skip_copy = False\n        # Create the sampling metadata.\n        req_id_output_token_ids: Dict[str, List[int]] = \\\n            {req_id: req.output_token_ids \\\n                for req_id, req in self.requests.items()}\n\n        sampling_metadata = self.input_batch.make_sampling_metadata(\n            req_id_output_token_ids, skip_copy)\n        return sampling_metadata\n\n    def _execute_encoder(self, scheduler_output: \"SchedulerOutput\"):\n        scheduled_encoder_inputs = scheduler_output.scheduled_encoder_inputs\n        if not scheduled_encoder_inputs:\n            return\n\n        # Batch the multi-modal inputs.\n        mm_inputs: List[MultiModalKwargs] = []\n        req_input_ids: List[Tuple[str, int]] = []\n        for req_id, encoder_input_ids in scheduled_encoder_inputs.items():\n            req_state = self.requests[req_id]\n            for input_id in encoder_input_ids:\n                mm_inputs.append(req_state.mm_inputs[input_id])\n                req_input_ids.append((req_id, input_id))\n\n        # Batch mm inputs as much as we can: if a request in the batch has\n        # multiple modalities or a different modality than the previous one,\n        # we process it separately to preserve item order.\n        # FIXME(ywang96): This is a hacky way to deal with multiple modalities\n        # in the same batch while still being able to benefit from batching\n        # multimodal inputs. The proper solution should be reordering the\n        # encoder outputs.\n        grouped_mm_inputs_list = group_mm_inputs_by_modality(mm_inputs)\n\n        encoder_outputs = []\n        for grouped_mm_inputs in grouped_mm_inputs_list:\n            batched_mm_inputs = MultiModalKwargs.batch(grouped_mm_inputs)\n            batched_mm_inputs = MultiModalKwargs.as_kwargs(batched_mm_inputs,\n                                                           device=self.device)\n\n            # Run the encoder.\n            # `curr_group_outputs` is either of the following:\n            # 1. A tensor of shape (num_items, feature_size, hidden_size)\n            # in case feature_size is fixed across all multimodal items.\n            # 2. A list or tuple (length: num_items) of tensors, each of shape\n            # (feature_size, hidden_size) in case the feature size is dynamic\n            # depending on the input multimodal items.\n            curr_group_outputs = self.model.get_multimodal_embeddings(\n                **batched_mm_inputs)\n\n            for output in curr_group_outputs:\n                encoder_outputs.append(output)\n\n        # Cache the encoder outputs.\n        for (req_id, input_id), output in zip(req_input_ids, encoder_outputs):\n            if req_id not in self.encoder_cache:\n                self.encoder_cache[req_id] = {}\n            self.encoder_cache[req_id][input_id] = output\n\n    def _gather_encoder_outputs(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> List[torch.Tensor]:\n        encoder_outputs: List[torch.Tensor] = []\n        num_reqs = self.input_batch.num_reqs\n        for req_id in self.input_batch.req_ids[:num_reqs]:\n            assert req_id is not None\n            num_scheduled_tokens = scheduler_output.num_scheduled_tokens[\n                req_id]\n            req_state = self.requests[req_id]\n            num_computed_tokens = req_state.num_computed_tokens\n            mm_positions = req_state.mm_positions\n            for i, pos_info in enumerate(mm_positions):\n                start_pos = pos_info[\"offset\"]\n                num_encoder_tokens = pos_info[\"length\"]\n\n                # The encoder output is needed if the two ranges overlap:\n                # [num_computed_tokens,\n                #  num_computed_tokens + num_scheduled_tokens) and\n                # [start_pos, start_pos + num_encoder_tokens)\n                if start_pos >= num_computed_tokens + num_scheduled_tokens:\n                    # The encoder output is not needed in this step.\n                    break\n                if start_pos + num_encoder_tokens <= num_computed_tokens:\n                    # The encoder output is already processed and stored\n                    # in the decoder's KV cache.\n                    continue\n\n                start_idx = max(num_computed_tokens - start_pos, 0)\n                end_idx = min(\n                    num_computed_tokens - start_pos + num_scheduled_tokens,\n                    num_encoder_tokens)\n                assert start_idx < end_idx\n                assert req_id in self.encoder_cache\n                assert i in self.encoder_cache[req_id]\n                encoder_output = self.encoder_cache[req_id][i]\n                encoder_outputs.append(encoder_output[start_idx:end_idx])\n        return encoder_outputs\n\n    def get_model(self) -> nn.Module:\n        return self.model\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        scheduler_output: \"SchedulerOutput\",\n    ) -> ModelRunnerOutput:\n        self._update_states(scheduler_output)\n\n        if self.is_multimodal_model:\n            # Run the multimodal encoder if any.\n            self._execute_encoder(scheduler_output)\n            encoder_outputs = self._gather_encoder_outputs(scheduler_output)\n        else:\n            encoder_outputs = []\n\n        # Prepare the decoder inputs.\n        attn_metadata, logits_indices = self._prepare_inputs(scheduler_output)\n        num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens\n        if (self.use_cuda_graph\n                and num_scheduled_tokens <= self.cudagraph_batch_sizes[-1]):\n            # Use piecewise CUDA graphs.\n            # Add padding to the batch size.\n            num_input_tokens = self.vllm_config.pad_for_cudagraph(\n                num_scheduled_tokens)\n        else:\n            # Eager mode.\n            num_input_tokens = num_scheduled_tokens\n        attn_metadata.num_input_tokens = num_input_tokens\n\n        if self.is_multimodal_model:\n            # NOTE(woosuk): To unify token ids and soft tokens (vision\n            # embeddings), we always use embeddings (rather than token ids)\n            # as input to the multimodal model, even when the input is text.\n            input_ids = self.input_ids[:num_scheduled_tokens]\n            if encoder_outputs:\n                inputs_embeds = self.model.get_input_embeddings(\n                    input_ids, encoder_outputs)\n            else:\n                inputs_embeds = self.model.get_input_embeddings(input_ids)\n            # TODO(woosuk): Avoid the copy. Optimize.\n            self.inputs_embeds[:num_scheduled_tokens].copy_(inputs_embeds)\n            inputs_embeds = self.inputs_embeds[:num_input_tokens]\n            input_ids = None\n        else:\n            # For text-only models, we use token ids as input.\n            # While it is possible to use embeddings as input just like the\n            # multimodal models, it is not desirable for performance since\n            # then the embedding layer is not included in the CUDA graph.\n            input_ids = self.input_ids[:num_input_tokens]\n            inputs_embeds = None\n\n        # Run the decoder.\n        # Use persistent buffers for CUDA graphs.\n        with set_forward_context(attn_metadata, self.vllm_config):\n            positions = self.mrope_positions[:, :num_input_tokens] \\\n                if self.model_config.uses_mrope \\\n                else self.positions[:num_input_tokens]\n            hidden_states = self.model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=self.kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        hidden_states = hidden_states[:num_scheduled_tokens]\n        hidden_states = hidden_states[logits_indices]\n        logits = self.model.compute_logits(hidden_states, None)\n\n        # Sample the next token and get logprobs if needed.\n        sampling_metadata = self._prepare_sampling(scheduler_output)\n        sampler_output = self.model.sample(\n            logits=logits,\n            sampling_metadata=sampling_metadata,\n        )\n\n        sampled_token_ids = sampler_output.sampled_token_ids\n        # TODO(woosuk): The following loop can be slow since it iterates over\n        # the requests one by one. Optimize.\n        num_reqs = self.input_batch.num_reqs\n        for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n            assert req_id is not None\n            req_state = self.requests[req_id]\n            seq_len = (req_state.num_computed_tokens +\n                       scheduler_output.num_scheduled_tokens[req_id])\n            assert seq_len <= req_state.num_tokens\n            if seq_len == req_state.num_tokens:\n                # Append the sampled token to the output token ids.\n                token_id = sampled_token_ids[i]\n                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                self.input_batch.num_tokens[i] += 1\n                req_state.output_token_ids.append(token_id)\n            else:\n                # Ignore the sampled token from the partial request.\n                # Rewind the generator state as if the token was not sampled.\n                generator = self.input_batch.generators.get(i)\n                if generator is not None:\n                    # This relies on cuda-specific torch-internal impl details\n                    generator.set_offset(generator.get_offset() - 4)\n\n        if sampler_output.logprob_token_ids is None:\n            logprob_token_ids = None\n        else:\n            logprob_token_ids = sampler_output.logprob_token_ids.cpu()\n        if sampler_output.logprobs is None:\n            logprobs = None\n        else:\n            logprobs = sampler_output.logprobs.cpu()\n\n        # num_reqs entries should be non-None\n        assert all(\n            req_id is not None for req_id in\n            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n\n        model_runner_output = ModelRunnerOutput(\n            req_ids=req_ids,\n            req_id_to_index=self.input_batch.req_id_to_index,\n            sampled_token_ids=sampled_token_ids,\n            logprob_token_ids_cpu=logprob_token_ids,\n            logprobs_cpu=logprobs,\n        )\n        return model_runner_output\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with DeviceMemoryProfiler() as m:  # noqa: SIM117\n            self.model = get_model(vllm_config=self.vllm_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n    @torch.inference_mode()\n    def _dummy_run(\n        self,\n        num_tokens: int,\n        kv_caches: Optional[List[torch.Tensor]] = None,\n    ) -> torch.Tensor:\n        model = self.model\n        if kv_caches is None:\n            kv_caches = self.kv_caches\n        if self.is_multimodal_model:\n            input_ids = None\n            inputs_embeds = self.inputs_embeds[:num_tokens]\n        else:\n            input_ids = self.input_ids[:num_tokens]\n            inputs_embeds = None\n        with set_forward_context(None, self.vllm_config):\n            positions = self.mrope_positions[:, :num_tokens] \\\n                if self.model_config.uses_mrope \\\n                else self.positions[:num_tokens]\n            hidden_states = model(\n                input_ids=input_ids,\n                positions=positions,\n                kv_caches=kv_caches,\n                attn_metadata=None,\n                inputs_embeds=inputs_embeds,\n            )\n        return hidden_states\n\n    def profile_run(self) -> None:\n        # use an empty tensor instead of `None`` to force Dynamo to pass\n        # it by reference, rather by specializing on the value `None`.\n        # the `dtype` argument does not matter, and we use `float32` as\n        # a placeholder (it has wide hardware support).\n        # it is important to create tensors inside the loop, rather than\n        # multiplying the list, to avoid Dynamo from treating them as\n        # tensor aliasing.\n        dummy_kv_caches = [\n            torch.tensor([], dtype=torch.float32, device=self.device)\n            for _ in range(self.num_attn_layers)\n        ]\n\n        # Profile with multimodal encoder & encoder cache.\n        # TODO: handle encoder-decoder models once we support them.\n        if (self.is_multimodal_model and self.max_num_encoder_input_tokens > 0\n                and self.encoder_cache_size > 0):\n\n            # NOTE: Currently model is profiled with a single non-text\n            # modality with the max possible input tokens even when\n            # it supports multiple.\n            max_tokens_by_modality_dict = MULTIMODAL_REGISTRY.get_max_tokens_per_item_by_nonzero_modality(  # noqa: E501\n                self.model_config)\n            dummy_data_modality, max_tokens_per_mm_item = max(\n                max_tokens_by_modality_dict.items(), key=lambda item: item[1])\n\n            # Check how many items of this modality can be supported by\n            # the encoder budget.\n            encoder_budget = min(self.max_num_encoder_input_tokens,\n                                 self.encoder_cache_size)\n\n            max_num_mm_items_encoder_budget = cdiv(encoder_budget,\n                                                   max_tokens_per_mm_item)\n\n            # Check how many items of this modality can be supported by\n            # the decoder budget.\n            max_mm_items_per_req = self.mm_registry.get_mm_limits_per_prompt(\n                self.model_config)[dummy_data_modality]\n\n            # NOTE: We do not consider max_num_batched_tokens on purpose\n            # because the multimodal embeddings can be generated in advance\n            # and chunked prefilled.\n            max_num_mm_items_decoder_budget = self.max_num_reqs * \\\n                max_mm_items_per_req\n\n            max_num_mm_items = min(max_num_mm_items_encoder_budget,\n                                   max_num_mm_items_decoder_budget)\n\n            logger.info(\n                \"Encoder cache will be initialized with a budget of %s tokens,\"\n                \" and profiled with %s %s items of the maximum feature size.\",\n                encoder_budget, max_num_mm_items, dummy_data_modality)\n\n            # Create dummy batch of multimodal inputs.\n            dummy_request_data = self.input_registry.dummy_data_for_profiling(\n                model_config=self.model_config,\n                seq_len=self.max_num_tokens,\n                mm_registry=self.mm_registry,\n            )\n            dummy_mm_data = dummy_request_data.multi_modal_data\n\n            # Dummy data definition in V0 may contain multiple multimodal items\n            # (e.g, multiple images) for a single request, therefore here we\n            # always replicate first item by max_num_mm_items times since in V1\n            # they are scheduled to be processed separately.\n\n            # Case when models have a merged processor, their dummy data is\n            # already batched `MultiModalKwargs`, therefore we take the first\n            # `MultiModalKwargsItem` from the desired modality to profile on.\n            if isinstance(dummy_mm_data, MultiModalKwargs):\n                dummy_mm_item = dummy_mm_data.get_item(\n                    modality=dummy_data_modality, item_index=0)\n                dummy_mm_kwargs = MultiModalKwargs.from_items([dummy_mm_item])\n\n            # Case when models have dummy data explicitly defined as\n            # `MultiModalDataDict`, so they need to be processed through input\n            # mapper.\n            # TODO (ywang96): deprecate this path once merged processor is\n            # supported on all models.\n            else:\n                mm_kwargs_list = self.mm_input_mapper_profiling.process_inputs(\n                    mm_data=dummy_mm_data,\n                    mm_hashes=None,\n                    mm_processor_kwargs=None,\n                    precomputed_mm_inputs=None)\n                dummy_mm_kwargs = mm_kwargs_list[0]\n\n            batched_dummy_mm_inputs = MultiModalKwargs.batch(\n                [dummy_mm_kwargs] * max_num_mm_items)\n            batched_dummy_mm_inputs = MultiModalKwargs.as_kwargs(\n                batched_dummy_mm_inputs, device=self.device)\n\n            # Run multimodal encoder.\n            dummy_encoder_outputs = self.model.get_multimodal_embeddings(\n                **batched_dummy_mm_inputs)\n            assert len(dummy_encoder_outputs) == max_num_mm_items, (\n                \"Expected dimension 0 of encoder outputs to match the number \"\n                f\"of multimodal data items: {max_num_mm_items}, got \"\n                f\"{len(dummy_encoder_outputs)=} instead. This is most likely \"\n                \"due to the 'get_multimodal_embeddings' method of the model \"\n                \"not implemented correctly.\")\n\n            # Cache the dummy encoder outputs.\n            self.encoder_cache[\"tmp\"] = dict(enumerate(dummy_encoder_outputs))\n\n        # Trigger compilation for general shape.\n        hidden_states = self._dummy_run(self.max_num_tokens, dummy_kv_caches)\n        logits = self.model.compute_logits(hidden_states, None)\n        logits = logits[:self.max_num_tokens]\n        # TODO(woosuk): Consider the memory usage of the sampler.\n        torch.cuda.synchronize()\n        del hidden_states, logits\n        self.encoder_cache.clear()\n        gc.collect()\n\n    def capture_model(self) -> None:\n        if not self.use_cuda_graph:\n            logger.warning(\n                \"Skipping CUDA graph capture. Please add \"\n                \"-O %s to use CUDA graphs.\", CompilationLevel.PIECEWISE)\n            return\n\n        start_time = time.perf_counter()\n        start_free_gpu_memory = torch.cuda.mem_get_info()[0]\n\n        # Trigger CUDA graph capture for specific shapes.\n        # Capture the large shapes first so that the smaller shapes\n        # can reuse the memory pool allocated for the large shapes.\n        with graph_capture(device=self.device):\n            for num_tokens in reversed(self.cudagraph_batch_sizes):\n                for _ in range(self.vllm_config.compilation_config.\n                               cudagraph_num_of_warmups):\n                    self._dummy_run(num_tokens)\n                self._dummy_run(num_tokens)\n\n        end_time = time.perf_counter()\n        end_free_gpu_memory = torch.cuda.mem_get_info()[0]\n        elapsed_time = end_time - start_time\n        cuda_graph_size = start_free_gpu_memory - end_free_gpu_memory\n        # This usually takes 5~20 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs, took %.2f GiB\",\n                    elapsed_time, cuda_graph_size / (1 << 30))\n\n    def initialize_kv_cache(self, kv_cache_config: KVCacheConfig) -> None:\n        \"\"\"\n        Initialize KV cache based on `kv_cache_config`.\n        Args:\n            kv_cache_config: Configuration for the KV cache, including the KV \n            cache size of each layer\n        \"\"\"\n        if len(kv_cache_config.groups) > 1:\n            raise NotImplementedError(\n                \"Hybrid models with more than one KV cache type are not \"\n                \"supported yet.\")\n\n        kv_caches: Dict[str, torch.Tensor] = {}\n\n        for layer_name, layer_spec in kv_cache_config.kv_cache_spec.items():\n            tensor_config = kv_cache_config.tensors[layer_name]\n            assert tensor_config.size % layer_spec.page_size_bytes == 0\n            num_blocks = tensor_config.size // layer_spec.page_size_bytes\n            if isinstance(layer_spec, FullAttentionSpec):\n                kv_cache_shape = FlashAttentionBackend.get_kv_cache_shape(\n                    num_blocks, layer_spec.block_size, layer_spec.num_kv_heads,\n                    layer_spec.head_size)\n                dtype = layer_spec.dtype\n                kv_caches[layer_name] = torch.zeros(kv_cache_shape,\n                                                    dtype=dtype,\n                                                    device=self.device)\n            else:\n                raise NotImplementedError\n\n        bind_kv_cache(\n            kv_caches,\n            self.vllm_config.compilation_config.static_forward_context,\n            self.kv_caches)\n\n    def get_kv_cache_spec(self) -> KVCacheSpec:\n        \"\"\"\n        Generates the KVCacheSpec by parsing the kv cache format from each \n        Attention module in the static forward context.\n        Returns:\n            KVCacheSpec: A dictionary mapping layer names to their KV cache \n            format. Layers that do not need KV cache are not included.\n        \"\"\"\n\n        forward_ctx = self.vllm_config.compilation_config.static_forward_context\n        block_size = self.vllm_config.cache_config.block_size\n        kv_cache_spec: KVCacheSpec = {}\n        for layer_name, attn_module in forward_ctx.items():\n            # TODO: Support other attention modules, e.g., sliding window,\n            # cross-attention, MLA.\n            assert isinstance(attn_module, Attention)\n            if attn_module.attn_type == AttentionType.DECODER:\n                kv_cache_spec[layer_name] = FullAttentionSpec(\n                    block_size=block_size,\n                    num_kv_heads=attn_module.num_kv_heads,\n                    head_size=attn_module.head_size,\n                    dtype=attn_module.dtype,\n                )\n            elif attn_module.attn_type in (AttentionType.ENCODER,\n                                           AttentionType.ENCODER_ONLY):\n                # encoder-only attention does not need KV cache.\n                continue\n            elif attn_module.attn_type == AttentionType.ENCODER_DECODER:\n                raise NotImplementedError\n            else:\n                raise ValueError(\n                    f\"Unknown attention type: {attn_module.attn_type}\")\n\n        return kv_cache_spec\n",
      "diff": "diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py\nindex 4b3c325de..6339f1f03 100644\n--- a/vllm/v1/worker/gpu_model_runner.py\n+++ b/vllm/v1/worker/gpu_model_runner.py\n@@ -775,10 +775,10 @@ class GPUModelRunner:\n             sampling_metadata=sampling_metadata,\n         )\n \n-        sampled_token_ids = sampler_output.sampled_token_ids\n         # TODO(woosuk): The following loop can be slow since it iterates over\n         # the requests one by one. Optimize.\n         num_reqs = self.input_batch.num_reqs\n+        request_seq_lens: List[Tuple[int, CachedRequestState, int]] = []\n         for i, req_id in enumerate(self.input_batch.req_ids[:num_reqs]):\n             assert req_id is not None\n             req_state = self.requests[req_id]\n@@ -787,10 +787,10 @@ class GPUModelRunner:\n             assert seq_len <= req_state.num_tokens\n             if seq_len == req_state.num_tokens:\n                 # Append the sampled token to the output token ids.\n-                token_id = sampled_token_ids[i]\n-                self.input_batch.token_ids_cpu[i, seq_len] = token_id\n                 self.input_batch.num_tokens[i] += 1\n-                req_state.output_token_ids.append(token_id)\n+                # OPTIMIZATION: Priming the state updates for later updates.\n+                req_state.output_token_ids.append(0)\n+                request_seq_lens.append((i, req_state, seq_len))\n             else:\n                 # Ignore the sampled token from the partial request.\n                 # Rewind the generator state as if the token was not sampled.\n@@ -799,6 +799,21 @@ class GPUModelRunner:\n                     # This relies on cuda-specific torch-internal impl details\n                     generator.set_offset(generator.get_offset() - 4)\n \n+        # num_reqs entries should be non-None\n+        assert all(\n+            req_id is not None for req_id in\n+            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n+        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n+\n+        # NOTE: GPU -> CPU Sync happens here.\n+        # Move as many CPU operations as possible before this sync point.\n+        sampled_token_ids = sampler_output.sampled_token_ids.tolist()\n+        # Update with the actual token ids\n+        for i, req_state, seq_len in request_seq_lens:\n+            token_id = sampled_token_ids[i]\n+            self.input_batch.token_ids_cpu[i, seq_len] = token_id\n+            req_state.output_token_ids[-1] = token_id\n+\n         if sampler_output.logprob_token_ids is None:\n             logprob_token_ids = None\n         else:\n@@ -808,12 +823,6 @@ class GPUModelRunner:\n         else:\n             logprobs = sampler_output.logprobs.cpu()\n \n-        # num_reqs entries should be non-None\n-        assert all(\n-            req_id is not None for req_id in\n-            self.input_batch.req_ids[:num_reqs]), \"req_ids contains None\"\n-        req_ids = cast(List[str], self.input_batch.req_ids[:num_reqs])\n-\n         model_runner_output = ModelRunnerOutput(\n             req_ids=req_ids,\n             req_id_to_index=self.input_batch.req_id_to_index,",
      "change_type": "modified",
      "lines_added": 20,
      "lines_removed": 11
    }
  ],
  "affected_apis": [
    "vllm.v1.outputs.SamplerOutput.sampled_token_ids",
    "vllm.v1.sample.sampler.Sampler.forward",
    "vllm.v1.worker.gpu_model_runner.GPUModelRunner.run"
  ],
  "summary": {
    "total_files": 3,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 3
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "YES (test_sampler, test_model_runner)",
    "is_benchmark_actually_there": "",
    "sample_clues": "execute, forward, gpu_model_runner"
  }
}