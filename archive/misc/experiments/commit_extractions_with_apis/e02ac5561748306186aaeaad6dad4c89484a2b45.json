{
  "commit_hash": "e02ac5561748306186aaeaad6dad4c89484a2b45",
  "parent_hash": "73388c07a42233a1010595edaab73a9e7ab8d9a4",
  "message": "[Performance] Optimize e2e overheads: Reduce python allocations (#7162)",
  "author": "Alexander Matveev <59768536+alexm-neuralmagic@users.noreply.github.com>",
  "date": "2024-08-08 21:34:28 -0700",
  "files_changed": [
    {
      "file_path": "vllm/attention/backends/flash_attn.py",
      "old_content": "\"\"\"Attention layer with FlashAttention.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Type\n\nimport torch\nfrom vllm_flash_attn import flash_attn_varlen_func, flash_attn_with_kvcache\n\nfrom vllm import _custom_ops as ops\nfrom vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,\n                                              AttentionMetadata,\n                                              AttentionMetadataBuilder,\n                                              AttentionType)\nfrom vllm.attention.backends.utils import (PAD_SLOT_ID, compute_slot_mapping,\n                                           compute_slot_mapping_start_idx,\n                                           is_block_tables_empty)\nfrom vllm.utils import async_tensor_h2d, make_tensor_with_pad\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import ModelInputForGPUBuilder\n\n\nclass FlashAttentionBackend(AttentionBackend):\n\n    @staticmethod\n    def get_supported_head_sizes() -> List[int]:\n        return [32, 64, 96, 128, 160, 192, 224, 256]\n\n    @staticmethod\n    def get_name() -> str:\n        return \"flash-attn\"\n\n    @staticmethod\n    def get_impl_cls() -> Type[\"FlashAttentionImpl\"]:\n        return FlashAttentionImpl\n\n    @staticmethod\n    def get_metadata_cls() -> Type[\"AttentionMetadata\"]:\n        return FlashAttentionMetadata\n\n    @staticmethod\n    def get_builder_cls() -> Type[\"FlashAttentionMetadataBuilder\"]:\n        return FlashAttentionMetadataBuilder\n\n    @staticmethod\n    def get_kv_cache_shape(\n        num_blocks: int,\n        block_size: int,\n        num_kv_heads: int,\n        head_size: int,\n    ) -> Tuple[int, ...]:\n        if block_size % 16 != 0:\n            raise ValueError(\"Block size must be a multiple of 16.\")\n        return (2, num_blocks, block_size, num_kv_heads, head_size)\n\n    @staticmethod\n    def swap_blocks(\n        src_kv_cache: torch.Tensor,\n        dst_kv_cache: torch.Tensor,\n        src_to_dst: torch.Tensor,\n    ) -> None:\n        src_key_cache = src_kv_cache[0]\n        dst_key_cache = dst_kv_cache[0]\n        ops.swap_blocks(src_key_cache, dst_key_cache, src_to_dst)\n\n        src_value_cache = src_kv_cache[1]\n        dst_value_cache = dst_kv_cache[1]\n        ops.swap_blocks(src_value_cache, dst_value_cache, src_to_dst)\n\n    @staticmethod\n    def copy_blocks(\n        kv_caches: List[torch.Tensor],\n        src_to_dists: torch.Tensor,\n    ) -> None:\n        key_caches = [kv_cache[0] for kv_cache in kv_caches]\n        value_caches = [kv_cache[1] for kv_cache in kv_caches]\n        ops.copy_blocks(key_caches, value_caches, src_to_dists)\n\n\n@dataclass\nclass FlashAttentionMetadata(AttentionMetadata):\n    \"\"\"Metadata for FlashAttentionBackend.\n\n    NOTE: Any python object stored here is not updated when it is\n    cuda-graph replayed. If you have values that need to be changed\n    dynamically, it should be stored in tensor. The tensor has to be\n    updated from `CUDAGraphRunner.forward` API.\n    \"\"\"\n    # (batch_size,). The sequence length per sequence. Sequence length means\n    # the computed tokens + new tokens None if it is a decoding.\n    seq_lens: Optional[List[int]]\n    # seq_lens stored as a tensor.\n    seq_lens_tensor: Optional[torch.Tensor]\n\n    # NOTE(sang): Definition of context_len, query_len, and seq_len.\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ---------------------|\n    #                                   |-- query_len ---|\n\n    # Maximum query length in the batch. None for decoding.\n    max_query_len: Optional[int]\n    # Maximum sequence length among prefill batch. 0 if there are decoding\n    # requests only.\n    max_prefill_seq_len: int\n    # Maximum sequence length among decode batch. 0 if there are prefill\n    # requests only.\n    max_decode_seq_len: int\n    # (batch_size + 1,). The cumulative subquery lengths of the sequences in\n    # the batch, used to index into subquery. E.g., if the subquery length\n    # is [4, 6], it is [0, 4, 10].\n    query_start_loc: Optional[torch.Tensor]\n    # (batch_size + 1,). The cumulative sequence lengths of the sequences in\n    # the batch, used to index into sequence. E.g., if the sequence length is\n    # [4, 6], it is [0, 4, 10].\n    seq_start_loc: Optional[torch.Tensor]\n    # (batch_size,) A tensor of context lengths (tokens that are computed\n    # so far).\n    context_lens_tensor: Optional[torch.Tensor]\n\n    # (batch_size, max_blocks_per_seq).\n    # Block addresses per sequence. (Seq id -> list of physical block)\n    # E.g., [0, 1, 2] means tokens are stored in 0th, 1st, and 2nd blocks\n    # in the kv cache. Each block can contain up to block_size tokens.\n    # 2nd dimensions are padded up to max_blocks_per_seq if it is cuda-graph\n    # captured.\n    block_tables: Optional[torch.Tensor]\n\n    # Whether or not if cuda graph is enabled.\n    # Cuda-graph is currently enabled for decoding only.\n    # TODO(woosuk): Move `use_cuda_graph` out since it's unrelated to attention.\n    use_cuda_graph: bool\n\n    _cached_prefill_metadata: Optional[\"FlashAttentionMetadata\"] = None\n    _cached_decode_metadata: Optional[\"FlashAttentionMetadata\"] = None\n\n    @property\n    def prefill_metadata(self) -> Optional[\"FlashAttentionMetadata\"]:\n        if self.num_prefills == 0:\n            return None\n\n        if self._cached_prefill_metadata is not None:\n            return self._cached_prefill_metadata\n\n        assert self.seq_lens is not None\n        assert self.seq_lens_tensor is not None\n        assert self.query_start_loc is not None\n        assert self.context_lens_tensor is not None\n        assert self.block_tables is not None\n        assert self.seq_start_loc is not None\n\n        self._cached_prefill_metadata = FlashAttentionMetadata(\n            num_prefills=self.num_prefills,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=0,\n            slot_mapping=self.slot_mapping[:self.num_prefill_tokens],\n            seq_lens=self.seq_lens[:self.num_prefills],\n            seq_lens_tensor=self.seq_lens_tensor[:self.num_prefills],\n            max_query_len=self.max_query_len,\n            max_prefill_seq_len=self.max_prefill_seq_len,\n            max_decode_seq_len=0,\n            query_start_loc=self.query_start_loc[:self.num_prefills + 1],\n            seq_start_loc=self.seq_start_loc[:self.num_prefills + 1],\n            context_lens_tensor=self.context_lens_tensor[:self.num_prefills],\n            block_tables=self.block_tables[:self.num_prefills],\n            use_cuda_graph=False,\n        )\n        return self._cached_prefill_metadata\n\n    @property\n    def decode_metadata(self) -> Optional[\"FlashAttentionMetadata\"]:\n        if self.num_decode_tokens == 0:\n            return None\n\n        if self._cached_decode_metadata is not None:\n            return self._cached_decode_metadata\n        assert self.block_tables is not None\n        assert self.seq_lens_tensor is not None\n\n        self._cached_decode_metadata = FlashAttentionMetadata(\n            num_prefills=0,\n            num_prefill_tokens=0,\n            num_decode_tokens=self.num_decode_tokens,\n            slot_mapping=self.slot_mapping[self.num_prefill_tokens:],\n            seq_lens=None,\n            seq_lens_tensor=self.seq_lens_tensor[self.num_prefills:],\n            max_query_len=None,\n            max_prefill_seq_len=0,\n            max_decode_seq_len=self.max_decode_seq_len,\n            query_start_loc=None,\n            seq_start_loc=None,\n            context_lens_tensor=None,\n            block_tables=self.block_tables[self.num_prefills:],\n            use_cuda_graph=self.use_cuda_graph,\n        )\n        return self._cached_decode_metadata\n\n\nclass FlashAttentionMetadataBuilder(\n        AttentionMetadataBuilder[FlashAttentionMetadata]):\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n        self.has_prefix_cache_hit = False\n\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n        self.use_v2_block_manager = (\n            input_builder.scheduler_config.use_v2_block_manager)\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool, prefix_cache_hit: bool):\n        \"\"\"Add a sequence group to the metadata. Specifically update/append\n        1. context length.\n        2. block table.\n        3. slot mapping.\n        \"\"\"\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks):\n            self.context_lens.append(context_len)\n\n            if is_prompt:\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                assert query_len == 1, (\n                    \"seq_len: {}, context_len: {}, query_len: {}\".format(\n                        seq_len, context_len, query_len))\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if prefix_cache_hit:\n                # NOTE(woosuk): For flash-attn, the block table should\n                # include the entries for the incoming prefill tokens.\n                block_table = block_tables[seq_id]\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                block_table = block_tables[seq_id][-curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            # Compute slot mapping.\n            is_profile_run = is_block_tables_empty(block_tables)\n            start_idx = compute_slot_mapping_start_idx(\n                is_prompt, query_len, context_len, self.sliding_window,\n                self.use_v2_block_manager)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        prefix_cache_hit = any([\n            inter_data.prefix_cache_hit\n            for inter_data in self.input_builder.inter_data_list\n        ])\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled,\n                                prefix_cache_hit)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_query_len = max(query_lens)\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        max_decode_seq_len = max(self.curr_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size\n\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.runner.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(self.block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.from_numpy(input_block_tables).to(\n                device=device, non_blocking=True)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n        assert max_query_len > 0, (\"query_lens: {}\".format(query_lens))\n\n        assert device is not None\n        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,\n                                               device, self.runner.pin_memory)\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,\n                                             self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=device)\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        return FlashAttentionMetadata(\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            seq_lens=seq_lens,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=max_query_len,\n            max_prefill_seq_len=max_prefill_seq_len,\n            max_decode_seq_len=max_decode_seq_len,\n            query_start_loc=query_start_loc,\n            seq_start_loc=seq_start_loc,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n        )\n\n\nclass FlashAttentionImpl(AttentionImpl):\n    \"\"\"\n    If the input tensors contain prompt tokens, the layout is as follows:\n    |<--------------- num_prefill_tokens ----------------->|\t\n    |<--prefill_0-->|<--prefill_1-->|...|<--prefill_N-1--->|\n\n    Otherwise, the layout is as follows:\t\n    |<----------------- num_decode_tokens ------------------>|\t\n    |<--decode_0-->|..........|<--decode_M-1-->|<--padding-->|\n\n    Generation tokens can contain padding when cuda-graph is used.\n    Currently, prompt tokens don't contain any padding.\n\n    The prompts might have different lengths, while the generation tokens\n    always have length 1.\n\n    If chunked prefill is enabled, prefill tokens and decode tokens can be\n    batched together in a flattened 1D query.\n\n    |<----- num_prefill_tokens ---->|<------- num_decode_tokens --------->|\n    |<-prefill_0->|...|<-prefill_N-1->|<--decode_0-->|...|<--decode_M-1-->|\n\n    Currently, cuda graph is disabled for chunked prefill, meaning there's no\n    padding between prefill and decode tokens.\n    \"\"\"\n\n    def __init__(\n        self,\n        num_heads: int,\n        head_size: int,\n        scale: float,\n        num_kv_heads: int,\n        alibi_slopes: Optional[List[float]],\n        sliding_window: Optional[int],\n        kv_cache_dtype: str,\n        blocksparse_params: Optional[Dict[str, Any]] = None,\n        logits_soft_cap: Optional[float] = None,\n    ) -> None:\n        if blocksparse_params is not None:\n            raise ValueError(\n                \"FlashAttention does not support block-sparse attention.\")\n        self.num_heads = num_heads\n        self.head_size = head_size\n        self.scale = float(scale)\n        self.num_kv_heads = num_kv_heads\n        if alibi_slopes is not None:\n            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)\n        self.alibi_slopes = alibi_slopes\n        self.sliding_window = ((sliding_window, sliding_window)\n                               if sliding_window is not None else (-1, -1))\n        self.kv_cache_dtype = kv_cache_dtype\n        if logits_soft_cap is None:\n            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.\n            logits_soft_cap = 0\n        self.logits_soft_cap = logits_soft_cap\n\n        assert self.num_heads % self.num_kv_heads == 0\n        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n\n        if sliding_window is not None:\n            # NOTE(woosuk): flash-attn's sliding window does not work with\n            # paged KV cache.\n            raise ValueError(\n                \"Sliding window is not supported in FlashAttention.\")\n\n        support_head_sizes = FlashAttentionBackend.get_supported_head_sizes()\n        if head_size not in support_head_sizes:\n            raise ValueError(\n                f\"Head size {head_size} is not supported by FlashAttention. \"\n                f\"Supported head sizes are: {support_head_sizes}.\")\n\n    def forward(\n        self,\n        query: torch.Tensor,\n        key: torch.Tensor,\n        value: torch.Tensor,\n        kv_cache: torch.Tensor,\n        attn_metadata: FlashAttentionMetadata,\n        k_scale: float = 1.0,\n        v_scale: float = 1.0,\n        attn_type: AttentionType = AttentionType.DECODER,\n    ) -> torch.Tensor:\n        \"\"\"Forward pass with FlashAttention.\n\n        Args:\n            query: shape = [num_tokens, num_heads * head_size]\n            key: shape = [num_tokens, num_kv_heads * head_size]\n            value: shape = [num_tokens, num_kv_heads * head_size]\n            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]\n            attn_metadata: Metadata for attention.\n        Returns:\n            shape = [num_tokens, num_heads * head_size]\n        \"\"\"\n        if attn_type != AttentionType.DECODER:\n            raise NotImplementedError(\"Encoder self-attention and \"\n                                      \"encoder/decoder cross-attention \"\n                                      \"are not implemented for \"\n                                      \"FlashAttentionImpl\")\n\n        # NOTE(woosuk): FlashAttention does not support FP8 KV cache.\n        assert k_scale == 1.0 and v_scale == 1.0, (\n            \"key/v_scale is not supported in FlashAttention.\")\n\n        num_tokens, hidden_size = query.shape\n        # Reshape the query, key, and value tensors.\n        query = query.view(-1, self.num_heads, self.head_size)\n        key = key.view(-1, self.num_kv_heads, self.head_size)\n        value = value.view(-1, self.num_kv_heads, self.head_size)\n\n        if kv_cache is not None:\n            key_cache = kv_cache[0]\n            value_cache = kv_cache[1]\n\n            # Reshape the input keys and values and store them in the cache.\n            # If kv_cache is not provided, the new key and value tensors are\n            # not cached. This happens during the initial memory profiling run.\n            ops.reshape_and_cache_flash(\n                key,\n                value,\n                key_cache,\n                value_cache,\n                attn_metadata.slot_mapping.flatten(),\n                self.kv_cache_dtype,\n                k_scale,\n                v_scale,\n            )\n\n        num_prefill_tokens = attn_metadata.num_prefill_tokens\n        num_decode_tokens = attn_metadata.num_decode_tokens\n        assert key.shape[0] == num_prefill_tokens + num_decode_tokens\n        assert value.shape[0] == num_prefill_tokens + num_decode_tokens\n\n        output = torch.empty_like(query)\n        # Query for decode. KV is not needed because it is already cached.\n        decode_query = query[num_prefill_tokens:]\n        # QKV for prefill.\n        query = query[:num_prefill_tokens]\n        key = key[:num_prefill_tokens]\n        value = value[:num_prefill_tokens]\n\n        assert query.shape[0] == num_prefill_tokens\n        assert decode_query.shape[0] == num_decode_tokens\n\n        if prefill_meta := attn_metadata.prefill_metadata:\n            # Prompt run.\n            if (kv_cache is None or prefill_meta.block_tables is None\n                    or prefill_meta.block_tables.numel() == 0):\n                # normal attention\n                # When block_tables are not filled, it means q and k are the\n                # prompt, and they have the same length.\n                out = flash_attn_varlen_func(\n                    q=query,\n                    k=key,\n                    v=value,\n                    cu_seqlens_q=prefill_meta.seq_start_loc,\n                    cu_seqlens_k=prefill_meta.seq_start_loc,\n                    max_seqlen_q=prefill_meta.max_prefill_seq_len,\n                    max_seqlen_k=prefill_meta.max_prefill_seq_len,\n                    softmax_scale=self.scale,\n                    causal=True,\n                    window_size=self.sliding_window,\n                    alibi_slopes=self.alibi_slopes,\n                    softcap=self.logits_soft_cap,\n                )\n                assert output[:num_prefill_tokens].shape == out.shape\n                output[:num_prefill_tokens] = out\n            else:\n                # prefix-enabled attention\n                assert prefill_meta.seq_lens is not None\n                max_seq_len = max(prefill_meta.seq_lens)\n                output[:num_prefill_tokens] = flash_attn_varlen_func(\n                    q=query,\n                    k=key_cache,\n                    v=value_cache,\n                    cu_seqlens_q=prefill_meta.query_start_loc,\n                    max_seqlen_q=prefill_meta.max_query_len,\n                    cu_seqlens_k=prefill_meta.seq_start_loc,\n                    max_seqlen_k=max_seq_len,\n                    softmax_scale=self.scale,\n                    causal=True,\n                    alibi_slopes=self.alibi_slopes,\n                    block_table=prefill_meta.block_tables,\n                    softcap=self.logits_soft_cap,\n                )\n\n        if decode_meta := attn_metadata.decode_metadata:\n            # Decoding run.\n            output[num_prefill_tokens:] = flash_attn_with_kvcache(\n                decode_query.unsqueeze(1),\n                key_cache,\n                value_cache,\n                block_table=decode_meta.block_tables,\n                cache_seqlens=decode_meta.seq_lens_tensor,\n                softmax_scale=self.scale,\n                causal=True,\n                alibi_slopes=self.alibi_slopes,\n            ).squeeze(1)\n\n        # Reshape the output tensor.\n        return output.view(num_tokens, hidden_size)\n",
      "diff": "diff --git a/vllm/attention/backends/flash_attn.py b/vllm/attention/backends/flash_attn.py\nindex 8a895bbdc..5710aa193 100644\n--- a/vllm/attention/backends/flash_attn.py\n+++ b/vllm/attention/backends/flash_attn.py\n@@ -259,7 +259,11 @@ class FlashAttentionMetadataBuilder(\n                 block_table = block_tables[seq_id]\n             elif ((chunked_prefill_enabled or not is_prompt)\n                   and block_tables is not None):\n-                block_table = block_tables[seq_id][-curr_sliding_window_block:]\n+                if curr_sliding_window_block == 0:\n+                    block_table = block_tables[seq_id]\n+                else:\n+                    block_table = block_tables[seq_id][\n+                        -curr_sliding_window_block:]\n             self.block_tables.append(block_table)\n \n             # Compute slot mapping.",
      "change_type": "modified",
      "lines_added": 6,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/attention/backends/utils.py",
      "old_content": "\"\"\"Attention backend utils\"\"\"\nfrom typing import TYPE_CHECKING, Dict, List, Type, TypeVar, Union\n\nimport torch\n\nfrom vllm.attention import AttentionMetadata, AttentionMetadataBuilder\nfrom vllm.utils import async_tensor_h2d, make_tensor_with_pad\n\n# Error string(s) for encoder/decoder\n# unsupported attention scenarios\nSTR_NOT_IMPL_ENC_DEC_ROCM_HIP = (\"ROCm/HIP is not currently supported \"\n                                 \"with encoder/decoder models.\")\n\nPAD_SLOT_ID = -1\n\nif TYPE_CHECKING:\n    from vllm.worker.model_runner import ModelInputForGPUBuilder\n\n\ndef is_block_tables_empty(block_tables: Union[None, Dict]):\n    \"\"\"\n    Check if block_tables is None or a dictionary with all None values.\n    \"\"\"\n    if block_tables is None:\n        return True\n    if isinstance(block_tables, dict) and all(\n            value is None for value in block_tables.values()):\n        return True\n    return False\n\n\ndef compute_slot_mapping_start_idx(is_prompt: bool, query_len: int,\n                                   context_len: int, sliding_window: int,\n                                   use_v2_block_manager: bool):\n    \"\"\"\n    Compute the start index of slot mapping.\n    \"\"\"\n    start_idx = 0\n    if is_prompt and sliding_window is not None:\n        assert use_v2_block_manager or context_len == 0, (\n            \"Prefix caching is currently not supported with \"\n            \"sliding window attention in V1 block manager\")\n        # When prefill, we use it to not write slots to kv cache\n        # to save memory.\n        start_idx = max(0, query_len - sliding_window)\n    return start_idx\n\n\ndef compute_slot_mapping(is_profile_run: bool, slot_mapping: List[int],\n                         seq_id: int, seq_len: int, context_len: int,\n                         start_idx: int, block_size: int,\n                         block_tables: Dict[int, List[int]]):\n    \"\"\"\n    Compute slot mapping.\n    \"\"\"\n    if is_profile_run:\n        # During memory profiling, the block tables are not\n        # initialized yet. In this case, we just use a dummy\n        # slot mapping.\n        # In embeddings, the block tables are {seq_id: None}.\n        slot_mapping.extend([PAD_SLOT_ID] * seq_len)\n        return\n\n    # Mask the [0, start_idx) tokens of the prompt with\n    # PAD_SLOT_ID, where start_idx is max(0, seq_len -\n    # sliding_window). For example, if the prompt len is 10,\n    # sliding window is 8, and block size is 4, the first two\n    # tokens are masked and the slot mapping will be\n    # [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n    block_table = block_tables[seq_id]\n    slot_mapping.extend([PAD_SLOT_ID] * max(0, start_idx - context_len))\n    for i in range(max(start_idx, context_len), seq_len):\n        block_number = block_table[i // block_size]\n        block_offset = i % block_size\n        slot = block_number * block_size + block_offset\n        slot_mapping.append(slot)\n\n\nTAttentionMetadata = TypeVar(\"TAttentionMetadata\", bound='AttentionMetadata')\n\n\nclass CommonMetadataBuilder(AttentionMetadataBuilder[TAttentionMetadata]):\n\n    _metadata_cls: Type[TAttentionMetadata]\n\n    def __init__(self, input_builder: \"ModelInputForGPUBuilder\"):\n        self.slot_mapping: List[int] = []\n        self.prefill_seq_lens: List[int] = []\n        self.context_lens: List[int] = []\n        self.block_tables: List[List[int]] = []\n        self.curr_seq_lens: List[int] = []\n        self.num_prefills = 0\n        self.num_prefill_tokens = 0\n        self.num_decode_tokens = 0\n\n        self.input_builder = input_builder\n        self.runner = input_builder.runner\n\n        self.sliding_window = input_builder.sliding_window\n        self.block_size = input_builder.block_size\n        self.use_v2_block_manager = (\n            input_builder.scheduler_config.use_v2_block_manager)\n\n    def _add_seq_group(\n            self, inter_data: \"ModelInputForGPUBuilder.InterDataForSeqGroup\",\n            chunked_prefill_enabled: bool):\n        is_prompt = inter_data.is_prompt\n        block_tables = inter_data.block_tables\n        computed_block_nums = inter_data.computed_block_nums\n\n        for (seq_id, token_len, seq_len, curr_seq_len, query_len, context_len,\n             curr_sliding_window_block) in zip(\n                 inter_data.seq_ids, [len(t) for t in inter_data.input_tokens],\n                 inter_data.orig_seq_lens, inter_data.seq_lens,\n                 inter_data.query_lens, inter_data.context_lens,\n                 inter_data.curr_sliding_window_blocks):\n            self.context_lens.append(context_len)\n            if is_prompt:\n                self.num_prefills += 1\n                self.num_prefill_tokens += token_len\n                self.prefill_seq_lens.append(seq_len)\n            else:\n                assert query_len == 1, (\n                    \"seq_len: {}, context_len: {}, query_len: {}\".format(\n                        seq_len, context_len, query_len))\n                self.num_decode_tokens += query_len\n                self.curr_seq_lens.append(curr_seq_len)\n\n            # Compute block table.\n            # TODO(sang): Combine chunked prefill and prefix caching by\n            # only allowing multiple of block_size chunk size.\n            # NOTE: This only works for oooooooxxx style attention.\n            block_table = []\n            if inter_data.prefix_cache_hit:\n                block_table = computed_block_nums\n            elif ((chunked_prefill_enabled or not is_prompt)\n                  and block_tables is not None):\n                block_table = block_tables[seq_id][-curr_sliding_window_block:]\n            self.block_tables.append(block_table)\n\n            # Compute slot mapping.\n            is_profile_run = is_block_tables_empty(block_tables)\n            start_idx = compute_slot_mapping_start_idx(\n                is_prompt, query_len, context_len, self.sliding_window,\n                self.use_v2_block_manager)\n            compute_slot_mapping(is_profile_run, self.slot_mapping, seq_id,\n                                 seq_len, context_len, start_idx,\n                                 self.block_size, inter_data.block_tables)\n\n    def build(self, seq_lens: List[int], query_lens: List[int],\n              cuda_graph_pad_size: int, batch_size: int):\n        \"\"\"Build attention metadata with on-device tensors.\n\n        Args:\n            seq_lens: The maybe padded sequence lengths of the input sequences.\n            query_lens: The query lengths of the input sequences.\n            cuda_graph_pad_size: The padding size for cuda graph.\n                                 -1 if cuda graph is not used.\n            batch_size: The maybe padded batch size.\n        \"\"\"\n        for inter_data in self.input_builder.inter_data_list:\n            self._add_seq_group(inter_data,\n                                self.input_builder.chunked_prefill_enabled)\n\n        device = self.runner.device\n        use_captured_graph = cuda_graph_pad_size != -1\n\n        max_query_len = max(query_lens)\n        max_prefill_seq_len = max(self.prefill_seq_lens, default=0)\n        max_decode_seq_len = max(self.curr_seq_lens, default=0)\n        num_decode_tokens = self.num_decode_tokens\n\n        if use_captured_graph:\n            self.slot_mapping.extend([PAD_SLOT_ID] * cuda_graph_pad_size)\n            self.block_tables.extend([] * cuda_graph_pad_size)\n            num_decode_tokens = batch_size\n\n            # The shape of graph_block_tables is\n            # [max batch size, max context len // block size].\n            input_block_tables = self.runner.graph_block_tables[:batch_size]\n            for i, block_table in enumerate(self.block_tables):\n                if block_table:\n                    input_block_tables[i, :len(block_table)] = block_table\n            block_tables = torch.from_numpy(input_block_tables).to(\n                device, non_blocking=True)\n        else:\n            block_tables = make_tensor_with_pad(\n                self.block_tables,\n                pad=0,\n                dtype=torch.int,\n                device=device,\n            )\n        assert max_query_len > 0, \"query_lens: {}\".format(query_lens)\n\n        assert device is not None\n        context_lens_tensor = async_tensor_h2d(self.context_lens, torch.int,\n                                               device, self.runner.pin_memory)\n        seq_lens_tensor = async_tensor_h2d(seq_lens, torch.int, device,\n                                           self.runner.pin_memory)\n        query_lens_tensor = async_tensor_h2d(query_lens, torch.long, device,\n                                             self.runner.pin_memory)\n        slot_mapping_tensor = async_tensor_h2d(self.slot_mapping, torch.long,\n                                               device, self.runner.pin_memory)\n        query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,\n                                      dtype=torch.int32,\n                                      device=device)\n        seq_start_loc = torch.zeros(seq_lens_tensor.shape[0] + 1,\n                                    dtype=torch.int32,\n                                    device=device)\n        torch.cumsum(seq_lens_tensor,\n                     dim=0,\n                     dtype=seq_start_loc.dtype,\n                     out=seq_start_loc[1:])\n        torch.cumsum(query_lens_tensor,\n                     dim=0,\n                     dtype=query_start_loc.dtype,\n                     out=query_start_loc[1:])\n\n        return self._metadata_cls(  # type: ignore\n            num_prefills=self.num_prefills,\n            slot_mapping=slot_mapping_tensor,\n            num_prefill_tokens=self.num_prefill_tokens,\n            num_decode_tokens=num_decode_tokens,\n            seq_lens=seq_lens,\n            seq_lens_tensor=seq_lens_tensor,\n            max_query_len=max_query_len,\n            max_prefill_seq_len=max_prefill_seq_len,\n            max_decode_seq_len=max_decode_seq_len,\n            query_start_loc=query_start_loc,\n            seq_start_loc=seq_start_loc,\n            context_lens_tensor=context_lens_tensor,\n            block_tables=block_tables,\n            use_cuda_graph=use_captured_graph,\n        )\n",
      "diff": "diff --git a/vllm/attention/backends/utils.py b/vllm/attention/backends/utils.py\nindex f7cb2ee99..3ca668cb4 100644\n--- a/vllm/attention/backends/utils.py\n+++ b/vllm/attention/backends/utils.py\n@@ -68,13 +68,21 @@ def compute_slot_mapping(is_profile_run: bool, slot_mapping: List[int],\n     # tokens are masked and the slot mapping will be\n     # [-1, -1, 2, 3, 4, 5, 6, 7, 0, 1].\n     block_table = block_tables[seq_id]\n-    slot_mapping.extend([PAD_SLOT_ID] * max(0, start_idx - context_len))\n-    for i in range(max(start_idx, context_len), seq_len):\n+\n+    def add_slot(i):\n         block_number = block_table[i // block_size]\n         block_offset = i % block_size\n         slot = block_number * block_size + block_offset\n         slot_mapping.append(slot)\n \n+    if start_idx == 0 and (seq_len - context_len) == 1:\n+        # Optimization for common-case of decoding next token\n+        add_slot(seq_len - 1)\n+    else:\n+        slot_mapping.extend([PAD_SLOT_ID] * max(0, start_idx - context_len))\n+        for i in range(max(start_idx, context_len), seq_len):\n+            add_slot(i)\n+\n \n TAttentionMetadata = TypeVar(\"TAttentionMetadata\", bound='AttentionMetadata')",
      "change_type": "modified",
      "lines_added": 11,
      "lines_removed": 3
    },
    {
      "file_path": "vllm/block.py",
      "old_content": "\"\"\"Token blocks.\"\"\"\nfrom typing import List\n\nfrom vllm.utils import Device\n\nDEFAULT_LAST_ACCESSED_TIME = -1\n\n\nclass PhysicalTokenBlock:\n    \"\"\"Represents the state of a block in the KV cache.\"\"\"\n\n    def __init__(\n        self,\n        device: Device,\n        block_number: int,\n        block_size: int,\n        block_hash: int,\n        num_hashed_tokens: int,\n    ) -> None:\n        self.device = device\n        self.block_number = block_number\n        self.block_size = block_size\n        self.block_hash = block_hash\n        self.num_hashed_tokens = num_hashed_tokens\n\n        self.ref_count = 0\n        self.last_accessed = DEFAULT_LAST_ACCESSED_TIME\n\n        self.computed = False\n\n    def __repr__(self) -> str:\n        return (f'PhysicalTokenBlock(device={self.device}, '\n                f'block_number={self.block_number}, '\n                f'num_hashed_tokens={self.num_hashed_tokens}, '\n                f'ref_count={self.ref_count}, '\n                f'last_accessed={self.last_accessed}, '\n                f'computed={self.computed})')\n\n\n# Mapping: logical block number -> physical block.\nBlockTable = List[PhysicalTokenBlock]\n",
      "diff": "diff --git a/vllm/block.py b/vllm/block.py\nindex 0b8ef7d4b..95286048d 100644\n--- a/vllm/block.py\n+++ b/vllm/block.py\n@@ -1,5 +1,5 @@\n \"\"\"Token blocks.\"\"\"\n-from typing import List\n+from typing import List, Optional\n \n from vllm.utils import Device\n \n@@ -37,5 +37,47 @@ class PhysicalTokenBlock:\n                 f'computed={self.computed})')\n \n \n-# Mapping: logical block number -> physical block.\n-BlockTable = List[PhysicalTokenBlock]\n+class BlockTable:\n+    \"\"\"Holds a list of blocks with caching of their associated block_ids \n+    \"\"\"\n+\n+    def __init__(self, blocks: Optional[List[PhysicalTokenBlock]] = None):\n+        self._blocks: List[PhysicalTokenBlock] = []\n+        self._block_ids: List[int] = []\n+\n+        if blocks is not None:\n+            for block in blocks:\n+                self.append(block)\n+\n+    def append(self, block: PhysicalTokenBlock):\n+        self._blocks.append(block)\n+        self._block_ids.append(block.block_number)\n+\n+    def __len__(self) -> int:\n+        return len(self._blocks)\n+\n+    def __getitem__(self, key):\n+        return self._blocks[key]\n+\n+    def __setitem__(self, key, value):\n+        if isinstance(key, slice):\n+            blocks = value\n+            self._blocks[key] = blocks\n+            self._block_ids[key] = [b.block_number for b in blocks]\n+        else:\n+            block = value\n+            self._blocks[key] = block\n+            self._block_ids[key] = block.block_number\n+\n+    def reset(self):\n+        self._blocks = []\n+        self._block_ids = []\n+\n+    def copy(self) -> \"BlockTable\":\n+        return BlockTable(self._blocks)\n+\n+    def list(self) -> List[PhysicalTokenBlock]:\n+        return self._blocks\n+\n+    def ids(self) -> List[int]:\n+        return self._block_ids",
      "change_type": "modified",
      "lines_added": 46,
      "lines_removed": 4
    },
    {
      "file_path": "vllm/core/block_manager_v1.py",
      "old_content": "\"\"\"A block manager that manages token blocks.\"\"\"\nimport math\nfrom abc import ABC, abstractmethod\nfrom itertools import count, takewhile\nfrom os.path import commonprefix\nfrom typing import Dict, List, Optional\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Tuple\n\nfrom vllm.block import BlockTable, PhysicalTokenBlock\nfrom vllm.core.block.utils import check_no_caching_or_swa_for_blockmgr_encdec\nfrom vllm.core.evictor_v1 import EvictionPolicy, Evictor, make_evictor\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.sequence import Sequence, SequenceGroup, SequenceStatus\nfrom vllm.utils import Device\n\nlogger = init_logger(__name__)\n\n\nclass BlockAllocatorBase(ABC):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU):\n        pass\n\n    @abstractmethod\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        pass\n\n    @abstractmethod\n    def free(self, block: PhysicalTokenBlock) -> None:\n        pass\n\n    @abstractmethod\n    def get_num_free_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def get_num_total_blocks(self) -> int:\n        pass\n\n    @abstractmethod\n    def contains_block(self, block_hash: int) -> bool:\n        pass\n\n    @abstractmethod\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        pass\n\n\nclass CachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(self,\n                 device: Device,\n                 block_size: int,\n                 num_blocks: int,\n                 eviction_policy: EvictionPolicy = EvictionPolicy.LRU) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        self.current_num_blocks = 0\n        self.cached_blocks: Dict[int, PhysicalTokenBlock] = {}\n\n        self.evictor: Evictor = make_evictor(eviction_policy)\n\n        self.default_hash_ctr = count()\n\n    def allocate_block(self, block_hash: int,\n                       num_hashed_tokens: int) -> PhysicalTokenBlock:\n        if self.current_num_blocks == self.num_blocks:\n            block = self.evictor.evict()\n            block.block_hash = block_hash\n            block.num_hashed_tokens = num_hashed_tokens\n            return block\n        block = PhysicalTokenBlock(device=self.device,\n                                   block_number=self.current_num_blocks,\n                                   block_size=self.block_size,\n                                   block_hash=block_hash,\n                                   num_hashed_tokens=num_hashed_tokens)\n        self.current_num_blocks += 1\n        return block\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if block_hash is None:\n            block_hash = next(self.default_hash_ctr)\n        if block_hash in self.evictor:\n            assert block_hash not in self.cached_blocks\n            block = self.evictor.remove(block_hash)\n            assert block.ref_count == 0\n            self.cached_blocks[block_hash] = block\n            block.ref_count += 1\n            assert block.block_hash == block_hash\n            return block\n        if block_hash not in self.cached_blocks:\n            self.cached_blocks[block_hash] = self.allocate_block(\n                block_hash, num_hashed_tokens)\n        block = self.cached_blocks[block_hash]\n        assert block.block_hash == block_hash\n        block.ref_count += 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            assert block.block_hash not in self.evictor\n            self.evictor.add(block)\n\n            # Remove the block from the cached_blocks\n            del self.cached_blocks[block.block_hash]\n\n    def get_num_free_blocks(self) -> int:\n        return (self.num_blocks - self.current_num_blocks +\n                self.evictor.num_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        return block_hash in self.cached_blocks or block_hash in self.evictor\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        # Update the hash of block and the cached_blocks dictionary.\n        assert not self.contains_block(block_hash)\n        old_hash = block.block_hash\n        block.block_hash = block_hash\n        del self.cached_blocks[old_hash]\n        self.cached_blocks[block_hash] = block\n\n\nclass UncachedBlockAllocator(BlockAllocatorBase):\n    \"\"\"Manages free physical token blocks for a device.\n\n    The allocator maintains a list of free blocks and allocates a block when\n    requested. When a block is freed, its reference count is decremented. If\n    the reference count becomes zero, the block is added back to the free list.\n    \"\"\"\n\n    def __init__(\n        self,\n        device: Device,\n        block_size: int,\n        num_blocks: int,\n    ) -> None:\n        self.device = device\n        self.block_size = block_size\n        self.num_blocks = num_blocks\n\n        # Initialize the free blocks.\n        self.free_blocks: BlockTable = []\n        for i in range(num_blocks):\n            block = PhysicalTokenBlock(device=device,\n                                       block_number=i,\n                                       block_size=block_size,\n                                       block_hash=-1,\n                                       num_hashed_tokens=0)\n            self.free_blocks.append(block)\n\n    def allocate(self,\n                 block_hash: Optional[int] = None,\n                 num_hashed_tokens: int = 0) -> PhysicalTokenBlock:\n        if not self.free_blocks:\n            raise ValueError(\"Out of memory! No free blocks are available.\")\n        block = self.free_blocks.pop()\n        block.ref_count = 1\n        return block\n\n    def free(self, block: PhysicalTokenBlock) -> None:\n        if block.ref_count == 0:\n            raise ValueError(f\"Double free! {block} is already freed.\")\n        block.ref_count -= 1\n        if block.ref_count == 0:\n            self.free_blocks.append(block)\n\n    def get_num_free_blocks(self) -> int:\n        return len(self.free_blocks)\n\n    def get_num_total_blocks(self) -> int:\n        return self.num_blocks\n\n    def contains_block(self, block_hash: int) -> bool:\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n    def update_hash(self, block_hash: int, block: PhysicalTokenBlock):\n        raise NotImplementedError(\n            \"Invalid codepath for uncached block allocator.\")\n\n\nclass BlockSpaceManagerV1(BlockSpaceManager):\n    \"\"\"Manages the mapping between logical and physical token blocks.\"\"\"\n\n    def __init__(\n        self,\n        block_size: int,\n        num_gpu_blocks: int,\n        num_cpu_blocks: int,\n        watermark: float = 0.01,\n        sliding_window: Optional[int] = None,\n        enable_caching: bool = False,\n    ) -> None:\n        self.block_size = block_size\n        self.num_total_gpu_blocks = num_gpu_blocks\n        self.num_total_cpu_blocks = num_cpu_blocks\n\n        if enable_caching and sliding_window is not None:\n            raise NotImplementedError(\n                \"Sliding window is not allowed with prefix caching enabled!\")\n\n        self.block_sliding_window = None\n        if sliding_window is not None:\n            # Round up to nearest block size to regularize sliding window\n            # allocation sizes.\n            self.block_sliding_window = math.ceil(sliding_window / block_size)\n\n        self.watermark = watermark\n        assert watermark >= 0.0\n\n        self.enable_caching = enable_caching\n\n        self.watermark_blocks = int(watermark * num_gpu_blocks)\n\n        if self.enable_caching:\n            logger.info(\"Automatic prefix caching is enabled.\")\n            self.gpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator: BlockAllocatorBase = CachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        else:\n            self.gpu_allocator = UncachedBlockAllocator(\n                Device.GPU, block_size, num_gpu_blocks)\n            self.cpu_allocator = UncachedBlockAllocator(\n                Device.CPU, block_size, num_cpu_blocks)\n        # Mapping: seq_id -> BlockTable.\n        self.block_tables: Dict[int, BlockTable] = {}\n        # Mapping: req_id -> BlockTable\n        # Note that each SequenceGroup has a unique\n        # request ID\n        self.cross_block_tables: Dict[str, BlockTable] = {}\n\n    def _get_seq_num_required_blocks(self, seq: Sequence) -> int:\n        return 0 if seq is None else seq.n_blocks\n\n    def can_allocate(self, seq_group: SequenceGroup) -> AllocStatus:\n        # FIXME(woosuk): Here we assume that all sequences in the group share\n        # the same prompt. This may not be true for preempted sequences.\n\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        self_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_seqs(status=SequenceStatus.WAITING)[0])\n        cross_num_required_blocks = self._get_seq_num_required_blocks(\n            seq_group.get_encoder_seq())\n        num_required_blocks = self_num_required_blocks + \\\n                              cross_num_required_blocks\n\n        if self.block_sliding_window is not None:\n\n            num_required_blocks = min(num_required_blocks,\n                                      self.block_sliding_window)\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n\n        # Use watermark to avoid frequent cache eviction.\n        if (self.num_total_gpu_blocks - num_required_blocks <\n                self.watermark_blocks):\n            return AllocStatus.NEVER\n        if num_free_gpu_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _allocate_sequence(self, \\\n                           seq: Sequence, \\\n                           ref_count: int, \\\n                           is_encoder_decoder: bool = True) -> BlockTable:\n        # Allocate new physical token blocks that will store the prompt tokens.\n        num_prompt_blocks = seq.n_blocks\n\n        block_table: BlockTable = []\n        for logical_idx in range(num_prompt_blocks):\n            if (self.block_sliding_window is not None\n                    and logical_idx >= self.block_sliding_window):\n                block = block_table[logical_idx % self.block_sliding_window]\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            elif not is_encoder_decoder and self.enable_caching:\n                block = self.gpu_allocator.allocate(\n                    seq.hash_of_block(logical_idx),\n                    seq.num_hashed_tokens_of_block(logical_idx))\n            else:\n                block = self.gpu_allocator.allocate()\n                # Set the reference counts of the token blocks.\n                block.ref_count = ref_count\n            block_table.append(block)\n\n        return block_table\n\n    def allocate(self, seq_group: SequenceGroup) -> None:\n        is_encoder_decoder = seq_group.is_encoder_decoder()\n        check_no_caching_or_swa_for_blockmgr_encdec(self, seq_group)\n\n        # Allocate decoder sequences\n        #\n        # NOTE: Here we assume that all sequences in the group have the same\n        # decoder prompt.\n        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n        block_table: BlockTable = \\\n            self._allocate_sequence(seq,\n                                    seq_group.num_seqs(),\n                                    is_encoder_decoder)\n\n        # Assign the self-attention block tables for each sequence.\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            self.block_tables[seq.seq_id] = block_table.copy()\n\n        # Allocate encoder sequence\n        if is_encoder_decoder:\n            # A SequenceGroup has only a single encoder sequence (at most),\n            # thus allocate with a ref count of 1\n            block_table = self._allocate_sequence(seq_group.get_encoder_seq(),\n                                                  1, is_encoder_decoder)\n            # Assign the cross-attention block table for the SequenceGroup.\n            self.cross_block_tables[seq_group.request_id] = block_table\n\n    def can_append_slots(self,\n                         seq_group: SequenceGroup,\n                         num_lookahead_slots: int = 0) -> bool:\n        assert (num_lookahead_slots == 0\n                ), \"lookahead allocation not supported in BlockSpaceManagerV1\"\n\n        # Simple heuristic: If there is at least one free block\n        # for each sequence, we can append.\n        num_free_gpu_blocks = self.gpu_allocator.get_num_free_blocks()\n        num_seqs = seq_group.num_seqs(status=SequenceStatus.RUNNING)\n        return num_seqs <= num_free_gpu_blocks\n\n    def _promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        assert self.enable_caching\n\n        # Compute a new hash for the block so that it can be shared by other\n        # Sequences\n        new_hash = seq.hash_of_block(seq.n_blocks - 1)\n\n        # if new_hash is already in the cached table, then free last_block\n        # and return the cached version\n        if self.gpu_allocator.contains_block(new_hash):\n            self.gpu_allocator.free(last_block)\n            return self.gpu_allocator.allocate(new_hash)\n        else:\n            self.gpu_allocator.update_hash(new_hash, last_block)\n            return last_block\n\n    def _is_last_block_full(\n        self,\n        seq: Sequence,\n    ) -> bool:\n        token_ids_len = seq.data.get_len()\n        return token_ids_len > 0 and token_ids_len % seq.block_size == 0\n\n    def _maybe_promote_last_block(\n        self,\n        seq: Sequence,\n        last_block: PhysicalTokenBlock,\n    ) -> PhysicalTokenBlock:\n        if self._is_last_block_full(seq):\n            return self._promote_last_block(seq, last_block)\n        else:\n            return last_block\n\n    def _allocate_last_physical_block(\n        self,\n        seq: Sequence,\n    ) -> PhysicalTokenBlock:\n        # Called before a new block is appended.\n        # This is in charge of allocating a new physical block (to be appended).\n\n        # None if the last block is not full. Otherwise, we set it to the\n        # content hash.\n        if not self.enable_caching:\n            return self.gpu_allocator.allocate()\n        block_hash: Optional[int] = None\n        n_blocks = seq.n_blocks\n        if (self._is_last_block_full(seq)):\n            block_hash = seq.hash_of_block(n_blocks - 1)\n        num_hashed_tokens = seq.num_hashed_tokens_of_block(n_blocks - 1)\n\n        # num_hashed_tokens is used to compute future hashes\n        # (e.g. in the hashing function, it is used to ask the sequence for\n        # prefix tokens)\n        new_block = self.gpu_allocator.allocate(block_hash, num_hashed_tokens)\n\n        # If the block has is None, then the block is not full.\n        # If the block is not full, then we expect it to have a refcount of 1.\n        if block_hash is None:\n            assert new_block.ref_count == 1\n        return new_block\n\n    def append_slots(\n        self,\n        seq: Sequence,\n        num_lookahead_slots: int = 0,\n    ) -> List[Tuple[int, int]]:\n        \"\"\"Allocate a physical slot for a new token.\"\"\"\n        n_blocks = seq.n_blocks\n        block_table = self.block_tables[seq.seq_id]\n        # If we need to allocate a new physical block\n        if len(block_table) < n_blocks:\n            # Currently this code only supports adding one physical block\n            assert len(block_table) == n_blocks - 1\n\n            if (self.block_sliding_window\n                    and len(block_table) >= self.block_sliding_window):\n                # reuse a block\n                block_table.append(block_table[len(block_table) %\n                                               self.block_sliding_window])\n            else:\n                # The sequence hash a new logical block.\n                # Allocate a new physical block.\n                new_block = self._allocate_last_physical_block(seq)\n                block_table.append(new_block)\n                return []\n\n        # We want to append the token to the last physical block.\n        last_block = block_table[-1]\n        assert last_block.device == Device.GPU\n        if last_block.ref_count == 1:\n            # Not shared with other sequences. Appendable.\n            if self.enable_caching:\n                # If the last block is now complete, we may reuse an old block\n                # to save memory.\n                maybe_new_block = self._maybe_promote_last_block(\n                    seq, last_block)\n                block_table[-1] = maybe_new_block\n            return []\n        else:\n            # The last block is shared with other sequences.\n            # Copy on Write: Allocate a new block and copy the tokens.\n            new_block = self._allocate_last_physical_block(seq)\n\n            block_table[-1] = new_block\n            self.gpu_allocator.free(last_block)\n            return [(last_block.block_number, new_block.block_number)]\n\n    def fork(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        # NOTE: fork does not allocate a new physical block.\n        # Thus, it is always safe from OOM.\n        if parent_seq.seq_id not in self.block_tables:\n            # Parent sequence has either been freed or never existed.\n            return\n        src_block_table = self.block_tables[parent_seq.seq_id]\n        self.block_tables[child_seq.seq_id] = src_block_table.copy()\n        # When using a sliding window, blocks will be eventually reused.\n        # In this case the block tables will contain repeated blocks.\n        # When forking, we must make sure that each block's `ref_count`\n        # is only incremented by one, so we deduplicate them by wrapping\n        # them in a set.\n        for block in set(src_block_table):\n            block.ref_count += 1\n\n    def _get_physical_blocks(\n            self, seq_group: SequenceGroup) -> List[PhysicalTokenBlock]:\n\n        # NOTE: Here, we assume that the physical blocks are only shared by\n        # the sequences in the same group.\n        request_id = seq_group.request_id\n        blocks: Set[PhysicalTokenBlock] = set()\n        for seq in seq_group.get_seqs():\n            if seq.is_finished():\n                continue\n            blocks.update(self.block_tables[seq.seq_id])\n        # Cross-attention blocks\n        if seq_group.is_encoder_decoder():\n            blocks.update(self.cross_block_tables[request_id])\n        return list(blocks)\n\n    def can_swap_in(self,\n                    seq_group: SequenceGroup,\n                    num_lookahead_slots: int = 0) -> AllocStatus:\n        assert (num_lookahead_slots == 0\n                ), \"BlockSpaceManagerV1 does not support lookahead allocation\"\n\n        blocks = self._get_physical_blocks(seq_group)\n        num_swapped_seqs = seq_group.num_seqs(status=SequenceStatus.SWAPPED)\n        if seq_group.is_encoder_decoder():\n            num_swapped_seqs += 1\n        num_free_blocks = self.gpu_allocator.get_num_free_blocks()\n        # NOTE: Conservatively, we assume that every sequence will allocate\n        # at least one free block right after the swap-in.\n        # NOTE: This should match the logic in can_append_slot().\n        num_required_blocks = len(blocks) + num_swapped_seqs\n        if self.gpu_allocator.get_num_total_blocks() < num_required_blocks:\n            return AllocStatus.NEVER\n        elif num_free_blocks - num_required_blocks >= self.watermark_blocks:\n            return AllocStatus.OK\n        else:\n            return AllocStatus.LATER\n\n    def _swap_block_table(\n            self, block_table: BlockTable, src_allocator: BlockAllocatorBase,\n            dest_allocator: BlockAllocatorBase,\n            mapping: Dict[PhysicalTokenBlock,\n                          PhysicalTokenBlock]) -> BlockTable:\n        new_block_table = []\n\n        for from_block in block_table:\n            if from_block in mapping:\n                to_block = mapping[from_block]\n                to_block.ref_count += 1\n            else:\n                to_block = dest_allocator.allocate(\n                    from_block.block_hash, from_block.num_hashed_tokens)\n                mapping[from_block] = to_block\n            new_block_table.append(to_block)\n            # Free the source block swapped in to destination.\n            src_allocator.free(from_block)\n\n        return new_block_table\n\n    def swap_in(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n\n        request_id = seq_group.request_id\n\n        # CPU block -> GPU block.\n        # dict is efficient in lookup `if cpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.cpu_allocator,\n                                       self.gpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.cpu_allocator,\n                                       self.gpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def can_swap_out(self, seq_group: SequenceGroup) -> bool:\n        blocks = self._get_physical_blocks(seq_group)\n        return len(blocks) <= self.cpu_allocator.get_num_free_blocks()\n\n    def swap_out(self, seq_group: SequenceGroup) -> List[Tuple[int, int]]:\n        request_id = seq_group.request_id\n\n        # GPU block -> CPU block.\n        # dict is efficient in lookup `if gpu_block in mapping`\n        mapping: Dict[PhysicalTokenBlock, PhysicalTokenBlock] = {}\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            self.block_tables[seq.seq_id] = \\\n                self._swap_block_table(self.block_tables[seq.seq_id],\n                                       self.gpu_allocator,\n                                       self.cpu_allocator,\n                                       mapping)\n\n        if seq_group.is_encoder_decoder():\n            self.cross_block_tables[request_id] = \\\n                self._swap_block_table(self.cross_block_tables[request_id],\n                                       self.gpu_allocator,\n                                       self.cpu_allocator,\n                                       mapping)\n\n        return [(cpu_block.block_number, gpu_block.block_number)\n                for cpu_block, gpu_block in mapping.items()]\n\n    def _free_block_table(self, block_table: BlockTable) -> None:\n        # when using a sliding window, each seq will only use up\n        # to `self.block_sliding_window` blocks. When freeing\n        # the block table, we must make sure to not free blocks more\n        # than once. If no sliding window is used, there is no block\n        # reuse in the block table, so we must free all blocks.\n        blocks_to_free = (block_table[-self.block_sliding_window:]\n                          if self.block_sliding_window is not None else\n                          block_table)\n        for block in set(blocks_to_free):\n            if block.device == Device.GPU:\n                self.gpu_allocator.free(block)\n            else:\n                self.cpu_allocator.free(block)\n\n    def free(self, seq: Sequence) -> None:\n        if seq.seq_id not in self.block_tables:\n            # Already freed or haven't been scheduled yet.\n            return\n        block_table = self.block_tables[seq.seq_id]\n        self._free_block_table(block_table)\n        del self.block_tables[seq.seq_id]\n\n    def free_cross(self, seq_group: SequenceGroup) -> None:\n        if seq_group.request_id not in self.cross_block_tables:\n            # Already freed or hasn't ben scheduled yet.\n            return\n        block_table = self.cross_block_tables[seq_group.request_id]\n        self._free_block_table(block_table)\n        del self.cross_block_tables[seq_group.request_id]\n\n    def reset(self) -> None:\n        # Free decoder block tables\n        for block_table in self.block_tables.values():\n            self._free_block_table(block_table)\n        self.block_tables.clear()\n        # Free cross-attention block tables\n        for block_table in self.cross_block_tables.values():\n            self._free_block_table(block_table)\n        self.cross_block_tables.clear()\n\n    def get_block_table(self, seq: Sequence) -> List[int]:\n        block_table = self.block_tables[seq.seq_id]\n        return [block.block_number for block in block_table]\n\n    def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n        block_table = self.cross_block_tables[seq_group.request_id]\n        return [block.block_number for block in block_table]\n\n    def get_num_free_gpu_blocks(self) -> int:\n        return self.gpu_allocator.get_num_free_blocks()\n\n    def get_num_free_cpu_blocks(self) -> int:\n        return self.cpu_allocator.get_num_free_blocks()\n\n    def access_all_blocks_in_seq(\n        self,\n        seq: Sequence,\n        access_time: float,\n    ) -> None:\n        if self.enable_caching:\n            # Update the last accessed time of all the blocks accessed\n            # in this step.\n            block_table = self.block_tables[seq.seq_id]\n            for block in block_table:\n                block.last_accessed = access_time\n\n    def compute_full_blocks_in_seq(self, seq: Sequence):\n        if seq.seq_id not in self.block_tables:\n            return\n        max_full_block = seq.get_len() // self.block_size - 1\n        block_table = self.block_tables[seq.seq_id]\n        if max_full_block == -1:\n            return\n        for i in reversed(range(max_full_block)):\n            if block_table[i].computed:\n                break\n            block_table[i].computed = True\n\n    def get_all_computed_blocks(self, seq: Sequence) -> List[int]:\n        if seq.seq_id not in self.block_tables:\n            return []\n        block_table = self.block_tables[seq.seq_id]\n        # NOTE We exclude the last block to avoid the case where the entire\n        # prompt is cached. This would cause erroneous behavior in model\n        # runner.\n        return [\n            b.block_number\n            for b in takewhile(lambda b: b.computed, block_table[:-1])\n        ]\n\n    def get_common_computed_block_ids(\n            self, seqs: List[Sequence]) -> GenericSequence[int]:\n        \"\"\"Return the block ids that are common for a given sequence group.\n\n        Used in prefill (can skip prefill of some blocks).\n        \"\"\"\n        # Can return non-empty result only with prefix caching enabled.\n        if not self.enable_caching:\n            return []\n\n        ids_list = [self.get_all_computed_blocks(seq) for seq in seqs]\n        return commonprefix([ids for ids in ids_list if ids != []])\n\n    def mark_blocks_as_computed(self, seq_group: SequenceGroup):\n        if self.enable_caching:\n            for seq in seq_group.get_seqs():\n                self.compute_full_blocks_in_seq(seq)\n",
      "diff": "diff --git a/vllm/core/block_manager_v1.py b/vllm/core/block_manager_v1.py\nindex d81648caa..622aca66a 100644\n--- a/vllm/core/block_manager_v1.py\n+++ b/vllm/core/block_manager_v1.py\n@@ -170,7 +170,7 @@ class UncachedBlockAllocator(BlockAllocatorBase):\n         self.num_blocks = num_blocks\n \n         # Initialize the free blocks.\n-        self.free_blocks: BlockTable = []\n+        self.free_blocks: List[PhysicalTokenBlock] = []\n         for i in range(num_blocks):\n             block = PhysicalTokenBlock(device=device,\n                                        block_number=i,\n@@ -256,6 +256,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n                 Device.CPU, block_size, num_cpu_blocks)\n         # Mapping: seq_id -> BlockTable.\n         self.block_tables: Dict[int, BlockTable] = {}\n+\n         # Mapping: req_id -> BlockTable\n         # Note that each SequenceGroup has a unique\n         # request ID\n@@ -299,7 +300,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         # Allocate new physical token blocks that will store the prompt tokens.\n         num_prompt_blocks = seq.n_blocks\n \n-        block_table: BlockTable = []\n+        block_table: BlockTable = BlockTable()\n         for logical_idx in range(num_prompt_blocks):\n             if (self.block_sliding_window is not None\n                     and logical_idx >= self.block_sliding_window):\n@@ -326,15 +327,19 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         #\n         # NOTE: Here we assume that all sequences in the group have the same\n         # decoder prompt.\n-        seq = seq_group.get_seqs(status=SequenceStatus.WAITING)[0]\n+        wait_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n+        seq = wait_seqs[0]\n         block_table: BlockTable = \\\n             self._allocate_sequence(seq,\n                                     seq_group.num_seqs(),\n                                     is_encoder_decoder)\n \n         # Assign the self-attention block tables for each sequence.\n-        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n-            self.block_tables[seq.seq_id] = block_table.copy()\n+        if len(wait_seqs) == 1:\n+            self.block_tables[wait_seqs[0].seq_id] = block_table\n+        else:\n+            for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n+                self.block_tables[seq.seq_id] = block_table.copy()\n \n         # Allocate encoder sequence\n         if is_encoder_decoder:\n@@ -476,6 +481,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n             return\n         src_block_table = self.block_tables[parent_seq.seq_id]\n         self.block_tables[child_seq.seq_id] = src_block_table.copy()\n+\n         # When using a sliding window, blocks will be eventually reused.\n         # In this case the block tables will contain repeated blocks.\n         # When forking, we must make sure that each block's `ref_count`\n@@ -527,7 +533,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n             dest_allocator: BlockAllocatorBase,\n             mapping: Dict[PhysicalTokenBlock,\n                           PhysicalTokenBlock]) -> BlockTable:\n-        new_block_table = []\n+        new_block_table: BlockTable = BlockTable()\n \n         for from_block in block_table:\n             if from_block in mapping:\n@@ -553,8 +559,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n             self.block_tables[seq.seq_id] = \\\n                 self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.cpu_allocator,\n-                                       self.gpu_allocator,\n+                                       self.cpu_allocator, self.gpu_allocator,\n                                        mapping)\n \n         if seq_group.is_encoder_decoder():\n@@ -580,8 +585,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n             self.block_tables[seq.seq_id] = \\\n                 self._swap_block_table(self.block_tables[seq.seq_id],\n-                                       self.gpu_allocator,\n-                                       self.cpu_allocator,\n+                                       self.gpu_allocator, self.cpu_allocator,\n                                        mapping)\n \n         if seq_group.is_encoder_decoder():\n@@ -636,8 +640,7 @@ class BlockSpaceManagerV1(BlockSpaceManager):\n         self.cross_block_tables.clear()\n \n     def get_block_table(self, seq: Sequence) -> List[int]:\n-        block_table = self.block_tables[seq.seq_id]\n-        return [block.block_number for block in block_table]\n+        return self.block_tables[seq.seq_id].ids()\n \n     def get_cross_block_table(self, seq_group: SequenceGroup) -> List[int]:\n         block_table = self.cross_block_tables[seq_group.request_id]",
      "change_type": "modified",
      "lines_added": 16,
      "lines_removed": 13
    },
    {
      "file_path": "vllm/core/scheduler.py",
      "old_content": "import enum\nimport os\nimport random\nimport time\nfrom collections import deque\nfrom dataclasses import dataclass, field\nfrom typing import Deque, Dict, Iterable, List, Optional, Set, Tuple, Union\n\nfrom vllm.config import CacheConfig, LoRAConfig, SchedulerConfig\nfrom vllm.core.interfaces import AllocStatus, BlockSpaceManager\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                           SequenceGroupMetadata, SequenceStatus)\n\nlogger = init_logger(__name__)\n\n# Test-only. If configured, decode is preempted with\n# ARTIFICIAL_PREEMPTION_PROB% probability.\nENABLE_ARTIFICIAL_PREEMPT = bool(\n    os.getenv(\"VLLM_TEST_ENABLE_ARTIFICIAL_PREEMPT\", False))  # noqa\nARTIFICIAL_PREEMPTION_PROB = 0.5\nARTIFICIAL_PREEMPTION_MAX_CNT = 500\n\n\nclass PreemptionMode(enum.Enum):\n    \"\"\"Preemption modes.\n\n    1. Swapping: Swap out the blocks of the preempted sequences to CPU memory\n    and swap them back in when the sequences are resumed.\n    2. Recomputation: Discard the blocks of the preempted sequences and\n    recompute them when the sequences are resumed, treating the sequences as\n    new prompts.\n    \"\"\"\n    SWAP = enum.auto()\n    RECOMPUTE = enum.auto()\n\n\n@dataclass\nclass SchedulingBudget:\n    \"\"\"The available slots for scheduling.\n\n    TODO(sang): Right now, the budget is request_id-aware meaning it can ignore\n    budget update from the same request_id. It is because in normal scheduling\n    path, we update RUNNING num_seqs ahead of time, meaning it could be\n    updated more than once when scheduling RUNNING requests. Since this won't\n    happen if we only have chunked prefill scheduling, we can remove this\n    feature from the API when chunked prefill is enabled by default.\n    \"\"\"\n    token_budget: int\n    max_num_seqs: int\n    _request_ids_num_batched_tokens: Set[str] = field(default_factory=set)\n    _request_ids_num_curr_seqs: Set[str] = field(default_factory=set)\n    _num_batched_tokens: int = 0\n    _num_curr_seqs: int = 0\n\n    def can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\n        assert num_new_tokens != 0\n        assert num_new_seqs != 0\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\n\n    def remaining_token_budget(self):\n        return self.token_budget - self.num_batched_tokens\n\n    def add_num_batched_tokens(self, req_id: str, num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            return\n\n        self._request_ids_num_batched_tokens.add(req_id)\n        self._num_batched_tokens += num_batched_tokens\n\n    def subtract_num_batched_tokens(self, req_id: str,\n                                    num_batched_tokens: int):\n        if req_id in self._request_ids_num_batched_tokens:\n            self._request_ids_num_batched_tokens.remove(req_id)\n            self._num_batched_tokens -= num_batched_tokens\n\n    def add_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            return\n\n        self._request_ids_num_curr_seqs.add(req_id)\n        self._num_curr_seqs += num_curr_seqs\n\n    def subtract_num_seqs(self, req_id: str, num_curr_seqs: int):\n        if req_id in self._request_ids_num_curr_seqs:\n            self._request_ids_num_curr_seqs.remove(req_id)\n            self._num_curr_seqs -= num_curr_seqs\n\n    @property\n    def num_batched_tokens(self):\n        return self._num_batched_tokens\n\n    @property\n    def num_curr_seqs(self):\n        return self._num_curr_seqs\n\n\n@dataclass\nclass ScheduledSequenceGroup:\n    # A sequence group that's scheduled.\n    seq_group: SequenceGroup\n    # The total chunk size (number of tokens) to process for next iteration.\n    # 1 for decoding. Same as prompt tokens for prefill, but if prefill is\n    # chunked, it can be smaller than that.\n    token_chunk_size: int\n\n\n@dataclass\nclass SchedulerOutputs:\n    \"\"\"The scheduling decision made from a scheduler.\"\"\"\n    # Scheduled sequence groups.\n    scheduled_seq_groups: Iterable[ScheduledSequenceGroup]\n    # Number of prefill groups scheduled.\n    num_prefill_groups: int\n    # Total number of batched tokens.\n    num_batched_tokens: int\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]]\n    # Sequence groups that are going to be ignored.\n    ignored_seq_groups: List[SequenceGroup]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # The number of requests in the running queue\n    running_queue_size: int\n    preempted: int\n\n    def __post_init__(self):\n        # Swap in and swap out should never happen at the same time.\n        assert not (self.blocks_to_swap_in and self.blocks_to_swap_out)\n\n        self.num_loras: int = len(self.lora_requests)\n        if self.num_loras > 0:\n            self._sort_by_lora_ids()\n\n        self.num_prompt_adapters: int = len(self.prompt_adapter_requests)\n\n    def is_empty(self) -> bool:\n        # NOTE: We do not consider the ignored sequence groups.\n        return (not self.scheduled_seq_groups and not self.blocks_to_swap_in\n                and not self.blocks_to_swap_out and not self.blocks_to_copy)\n\n    def _sort_by_lora_ids(self):\n        self.scheduled_seq_groups = sorted(\n            self.scheduled_seq_groups,\n            key=lambda g: (g.seq_group.lora_int_id, g.seq_group.request_id))\n\n    @property\n    def lora_requests(self) -> Set[LoRARequest]:\n        return {\n            g.seq_group.lora_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.lora_request is not None\n        }\n\n    @property\n    def prompt_adapter_requests(self) -> Set[PromptAdapterRequest]:\n        return {\n            g.seq_group.prompt_adapter_request\n            for g in self.scheduled_seq_groups\n            if g.seq_group.prompt_adapter_request is not None\n        }\n\n\n@dataclass\nclass SchedulerRunningOutputs:\n    \"\"\"The requests that are scheduled from a running queue.\n\n    Could contain prefill (prefill that's chunked) or decodes. If there's not\n    enough memory, it can be preempted (for recompute) or swapped out.\n    \"\"\"\n    # Selected sequences that are running and in a decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are running and in a prefill phase.\n    # I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The preempted sequences.\n    preempted: List[SequenceGroup]\n    # Sequences that are swapped out.\n    swapped_out: List[SequenceGroup]\n    # The blocks to swap out.\n    blocks_to_swap_out: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerRunningOutputs\":\n        return SchedulerRunningOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            preempted=[],\n            swapped_out=[],\n            blocks_to_swap_out=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n        )\n\n\n@dataclass\nclass SchedulerSwappedInOutputs:\n    \"\"\"The requests that are scheduled from a swap queue.\n\n    Could contain prefill (prefill that's chunked) or decodes.\n    \"\"\"\n    # Selected sequences that are going to be swapped in and is in a\n    # decoding phase.\n    decode_seq_groups: List[SequenceGroup]\n    # Selected sequences that are going to be swapped in and in a prefill\n    # phase. I.e., it means the prefill has been chunked.\n    prefill_seq_groups: List[SequenceGroup]\n    # The blocks to swap in.\n    blocks_to_swap_in: List[Tuple[int, int]]\n    # The blocks to copy.\n    blocks_to_copy: List[Tuple[int, int]]\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int\n    # Infeasible sequence groups.\n    infeasible_seq_groups: List[SequenceGroup]\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerSwappedInOutputs\":\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=[],\n            prefill_seq_groups=[],\n            blocks_to_swap_in=[],\n            blocks_to_copy=[],\n            num_lookahead_slots=0,\n            infeasible_seq_groups=[],\n        )\n\n\n@dataclass\nclass SchedulerPrefillOutputs:\n    \"\"\"The requests that are scheduled from a waiting queue.\n\n    Could contain a fresh prefill requests or preempted requests that need\n    to be recomputed from scratch.\n    \"\"\"\n    # Selected sequences for prefill.\n    seq_groups: List[SequenceGroup]\n    # Ignored sequence groups.\n    ignored_seq_groups: List[SequenceGroup]\n    num_lookahead_slots: int\n\n    @classmethod\n    def create_empty(cls) -> \"SchedulerPrefillOutputs\":\n        return SchedulerPrefillOutputs(\n            seq_groups=[],\n            ignored_seq_groups=[],\n            num_lookahead_slots=0,\n        )\n\n\nclass Scheduler:\n\n    def __init__(\n        self,\n        scheduler_config: SchedulerConfig,\n        cache_config: CacheConfig,\n        lora_config: Optional[LoRAConfig],\n        pipeline_parallel_size: int = 1,\n    ) -> None:\n        self.scheduler_config = scheduler_config\n        self.cache_config = cache_config\n        # Note for LoRA scheduling: the current policy is extremely\n        # simple and NOT fair. It can lead to starvation of some\n        # LoRAs. This should be improved in the future.\n        self.lora_config = lora_config\n\n        version = \"v1\"\n        if self.scheduler_config.use_v2_block_manager:\n            version = \"v2\"\n        if self.scheduler_config.embedding_mode:\n            version = \"embedding\"\n\n        BlockSpaceManagerImpl = BlockSpaceManager.get_block_space_manager_class(\n            version)\n\n        num_gpu_blocks = cache_config.num_gpu_blocks\n        if num_gpu_blocks:\n            num_gpu_blocks //= pipeline_parallel_size\n\n        num_cpu_blocks = cache_config.num_cpu_blocks\n        if num_cpu_blocks:\n            num_cpu_blocks //= pipeline_parallel_size\n\n        # Create the block space manager.\n        self.block_manager = BlockSpaceManagerImpl(\n            block_size=self.cache_config.block_size,\n            num_gpu_blocks=num_gpu_blocks,\n            num_cpu_blocks=num_cpu_blocks,\n            sliding_window=self.cache_config.sliding_window,\n            enable_caching=self.cache_config.enable_prefix_caching)\n\n        # Sequence groups in the WAITING state.\n        # Contain new prefill or preempted requests.\n        self.waiting: Deque[SequenceGroup] = deque()\n        # Sequence groups in the RUNNING state.\n        # Contain decode requests.\n        self.running: Deque[SequenceGroup] = deque()\n        # Sequence groups in the SWAPPED state.\n        # Contain decode requests that are swapped out.\n        self.swapped: Deque[SequenceGroup] = deque()\n        # Sequence groups finished requests ids since last step iteration.\n        # It lets the model know that any state associated with these requests\n        # can and must be released after the current step.\n        # This is used to evict the finished requests from the Mamba cache.\n        self._finished_requests_ids: List[str] = list()\n        # Time at previous scheduling step\n        self.prev_time = 0.0\n        # Did we schedule a prompt at previous step?\n        self.prev_prompt = False\n        # Latency of the last prompt step\n        self.last_prompt_latency = 0.0\n        # preemption mode, RECOMPUTE or SWAP\n        self.user_specified_preemption_mode = scheduler_config.preemption_mode\n\n        # The following field is test-only. It is used to inject artificial\n        # preemption.\n        self.enable_artificial_preemption = ENABLE_ARTIFICIAL_PREEMPT\n        self.artificial_preempt_cnt = (ARTIFICIAL_PREEMPTION_MAX_CNT\n                                       if self.enable_artificial_preemption\n                                       else 0)\n        self.num_cumulative_preemption: int = 0\n\n    @property\n    def lora_enabled(self) -> bool:\n        return bool(self.lora_config)\n\n    @property\n    def num_decoding_tokens_per_seq(self) -> int:\n        \"\"\"The number of new tokens.\"\"\"\n        return 1\n\n    def add_seq_group(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the waiting queue.\n        self.waiting.append(seq_group)\n\n    def _add_seq_group_to_running(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the running queue.\n        # Only for testing purposes.\n        self.running.append(seq_group)\n\n    def _add_seq_group_to_swapped(self, seq_group: SequenceGroup) -> None:\n        # Add sequence groups to the swapped queue.\n        # Only for testing purposes.\n        self.swapped.append(seq_group)\n\n    def abort_seq_group(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a sequence group with the given ID.\n\n        Check if the sequence group with the given ID\n            is present in any of the state queue.\n        If present, remove the sequence group from the state queue.\n            Also, if any of the sequences in the sequence group is not finished,\n                free the sequence with status `FINISHED_ABORTED`.\n        Otherwise, do nothing.\n\n        Args:\n            request_id: The ID(s) of the sequence group to abort.\n        \"\"\"\n        if isinstance(request_id, str):\n            request_id = (request_id, )\n        request_ids = set(request_id)\n        for state_queue in [self.waiting, self.running, self.swapped]:\n            aborted_groups: List[SequenceGroup] = []\n            for seq_group in state_queue:\n                if not request_ids:\n                    # Using 'break' here may add two extra iterations,\n                    # but is acceptable to reduce complexity.\n                    break\n                if seq_group.request_id in request_ids:\n                    # Appending aborted group into pending list.\n                    aborted_groups.append(seq_group)\n                    request_ids.remove(seq_group.request_id)\n            for aborted_group in aborted_groups:\n                # Remove the sequence group from the state queue.\n                state_queue.remove(aborted_group)\n                # Remove the aborted request from the Mamba cache.\n                self._finished_requests_ids.append(aborted_group.request_id)\n                for seq in aborted_group.get_seqs():\n                    if seq.is_finished():\n                        continue\n                    seq.status = SequenceStatus.FINISHED_ABORTED\n                    self.free_seq(seq)\n\n                self._free_seq_group_cross_attn_blocks(aborted_group)\n\n    def _free_seq_group_cross_attn_blocks(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        \"\"\"\n        Free a sequence group from a cross-attention block table.\n        Has no effect on decoder-only models.\n        \"\"\"\n        if seq_group.is_encoder_decoder():\n            self.block_manager.free_cross(seq_group)\n\n    def has_unfinished_seqs(self) -> bool:\n        return len(self.waiting) != 0 or len(self.running) != 0 or len(\n            self.swapped) != 0\n\n    def get_num_unfinished_seq_groups(self) -> int:\n        return len(self.waiting) + len(self.running) + len(self.swapped)\n\n    def get_and_reset_finished_requests_ids(self) -> List[str]:\n        \"\"\"Flushes the list of request ids of previously finished seq_groups.\"\"\"\n        finished_requests_ids = self._finished_requests_ids\n        self._finished_requests_ids = list()\n        return finished_requests_ids\n\n    def _schedule_running(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerRunningOutputs:\n        \"\"\"Schedule sequence groups that are running.\n\n        Running queue should include decode and chunked prefill requests.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any decodes are preempted.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any decodes are preempted.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n    \n        Returns:\n            SchedulerRunningOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_out: List[Tuple[int, int]] = []\n        blocks_to_copy: List[Tuple[int, int]] = []\n\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        preempted: List[SequenceGroup] = []\n        swapped_out: List[SequenceGroup] = []\n\n        # NOTE(woosuk): Preemption happens only when there is no available slot\n        # to keep all the sequence groups in the RUNNING state.\n\n        running_queue = self.running\n\n        while running_queue:\n            seq_group = running_queue[0]\n            num_running_tokens = self._get_num_new_tokens(\n                seq_group, SequenceStatus.RUNNING, enable_chunking, budget)\n\n            if num_running_tokens == 0:\n                break\n\n            running_queue.popleft()\n            while not self._can_append_slots(seq_group):\n                budget.subtract_num_batched_tokens(seq_group.request_id,\n                                                   num_running_tokens)\n                num_running_seqs = seq_group.get_max_num_running_seqs()\n                budget.subtract_num_seqs(seq_group.request_id,\n                                         num_running_seqs)\n\n                if (curr_loras is not None and seq_group.lora_int_id > 0\n                        and seq_group.lora_int_id in curr_loras):\n                    curr_loras.remove(seq_group.lora_int_id)\n\n                if running_queue:\n                    # Preempt the lowest-priority sequence groups.\n                    victim_seq_group = running_queue.pop()\n                    preempted_mode = self._preempt(victim_seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(victim_seq_group)\n                    else:\n                        swapped_out.append(victim_seq_group)\n                else:\n                    # No other sequence groups can be preempted.\n                    # Preempt the current sequence group.\n                    preempted_mode = self._preempt(seq_group,\n                                                   blocks_to_swap_out)\n                    if preempted_mode == PreemptionMode.RECOMPUTE:\n                        preempted.append(seq_group)\n                    else:\n                        swapped_out.append(seq_group)\n                    break\n            else:\n                self._append_slots(seq_group, blocks_to_copy)\n                is_prefill = seq_group.is_prefill()\n                if is_prefill:\n                    prefill_seq_groups.append(\n                        ScheduledSequenceGroup(\n                            seq_group=seq_group,\n                            token_chunk_size=num_running_tokens))\n                else:\n                    decode_seq_groups.append(\n                        ScheduledSequenceGroup(seq_group=seq_group,\n                                               token_chunk_size=1))\n                budget.add_num_batched_tokens(seq_group.request_id,\n                                              num_running_tokens)\n                # OPTIMIZATION:  Note that get_max_num_running_seqs is\n                # expensive. For the default scheduling chase where\n                # enable_chunking is False, num_seqs are updated before running\n                # this method, so we don't have to update it again here.\n                if enable_chunking:\n                    num_running_seqs = seq_group.get_max_num_running_seqs()\n                    budget.add_num_seqs(seq_group.request_id, num_running_seqs)\n                if curr_loras is not None and seq_group.lora_int_id > 0:\n                    curr_loras.add(seq_group.lora_int_id)\n\n        return SchedulerRunningOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            preempted=preempted,\n            swapped_out=swapped_out,\n            blocks_to_swap_out=blocks_to_swap_out,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False))\n\n    def _schedule_swapped(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerSwappedInOutputs:\n        \"\"\"Schedule sequence groups that are swapped out.\n\n        It schedules swapped requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are swapped in.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are swapped in.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerSwappedInOutputs.\n        \"\"\"\n        # Blocks that need to be swapped or copied before model execution.\n        blocks_to_swap_in: List[Tuple[int, int]] = []\n        blocks_to_copy: List[Tuple[int, int]] = []\n        decode_seq_groups: List[ScheduledSequenceGroup] = []\n        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n        infeasible_seq_groups: List[SequenceGroup] = []\n\n        swapped_queue = self.swapped\n\n        leftover_swapped: Deque[SequenceGroup] = deque()\n        while swapped_queue:\n            seq_group = swapped_queue[0]\n\n            # If the sequence group cannot be swapped in, stop.\n            is_prefill = seq_group.is_prefill()\n            alloc_status = self.block_manager.can_swap_in(\n                seq_group, self._get_num_lookahead_slots(is_prefill))\n            if alloc_status == AllocStatus.LATER:\n                break\n            elif alloc_status == AllocStatus.NEVER:\n                logger.warning(\n                    \"Failing the request %s because there's not enough kv \"\n                    \"cache blocks to run the entire sequence.\",\n                    seq_group.request_id)\n                for seq in seq_group.get_seqs():\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                infeasible_seq_groups.append(seq_group)\n                swapped_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (lora_int_id > 0 and (lora_int_id not in curr_loras)\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_swapped.appendleft(seq_group)\n                    swapped_queue.popleft()\n                    continue\n\n            # The total number of sequences in the RUNNING state should not\n            # exceed the maximum number of sequences.\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.SWAPPED,\n                                                      enable_chunking, budget)\n\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            if lora_int_id > 0 and curr_loras is not None:\n                curr_loras.add(lora_int_id)\n            swapped_queue.popleft()\n            self._swap_in(seq_group, blocks_to_swap_in)\n            self._append_slots(seq_group, blocks_to_copy)\n            is_prefill = seq_group.is_prefill()\n            if is_prefill:\n                prefill_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group,\n                                           token_chunk_size=num_new_tokens))\n            else:\n                decode_seq_groups.append(\n                    ScheduledSequenceGroup(seq_group, token_chunk_size=1))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        swapped_queue.extendleft(leftover_swapped)\n\n        return SchedulerSwappedInOutputs(\n            decode_seq_groups=decode_seq_groups,\n            prefill_seq_groups=prefill_seq_groups,\n            blocks_to_swap_in=blocks_to_swap_in,\n            blocks_to_copy=blocks_to_copy,\n            num_lookahead_slots=self._get_num_lookahead_slots(\n                is_prefill=False),\n            infeasible_seq_groups=infeasible_seq_groups,\n        )\n\n    def _get_prompt_limit(self, seq_group: SequenceGroup) -> int:\n        if self.scheduler_config.chunked_prefill_enabled:\n            prompt_limit = self.scheduler_config.max_model_len\n        else:\n            prompt_limit = min(self.scheduler_config.max_model_len,\n                               self.scheduler_config.max_num_batched_tokens)\n\n        # Model is fine tuned with long context. Return the fine tuned max_len.\n        if (seq_group.lora_request\n                and seq_group.lora_request.long_lora_max_len):\n            assert prompt_limit <= seq_group.lora_request.long_lora_max_len\n            return seq_group.lora_request.long_lora_max_len\n        else:\n            return prompt_limit\n\n    def _schedule_prefills(\n        self,\n        budget: SchedulingBudget,\n        curr_loras: Optional[Set[int]],\n        enable_chunking: bool = False,\n    ) -> SchedulerPrefillOutputs:\n        \"\"\"Schedule sequence groups that are in prefill stage.\n\n        Note that the current scheduler treats PREEMPTED_FOR_RECOMPUTE\n        as a new prefill (that starts from beginning -> most recently generated\n        tokens).\n\n        It schedules waiting requests as long as it fits `budget` and\n        curr_loras <= max_lora from the scheduling config. The input arguments\n        `budget` and `curr_loras` are updated based on scheduled seq_groups.\n\n        Args:\n            budget: The scheduling budget. The argument is in-place updated\n                when any requests are scheduled.\n            curr_loras: Currently batched lora request ids. The argument is\n                in-place updated when any requests are scheduled.\n            enable_chunking: If True, seq group can be chunked and only a\n                chunked number of tokens are scheduled  if\n                `budget.num_batched_tokens` has not enough capacity to schedule\n                all tokens.\n\n        Returns:\n            SchedulerPrefillOutputs.\n        \"\"\"\n        ignored_seq_groups: List[SequenceGroup] = []\n        seq_groups: List[SequenceGroup] = []\n\n        waiting_queue = self.waiting\n\n        leftover_waiting_sequences: Deque[SequenceGroup] = deque()\n        while self._passed_delay(time.time()) and waiting_queue:\n            seq_group = waiting_queue[0]\n\n            waiting_seqs = seq_group.get_seqs(status=SequenceStatus.WAITING)\n            assert len(waiting_seqs) == 1, (\n                \"Waiting sequence group should have only one prompt \"\n                \"sequence.\")\n            num_new_tokens = self._get_num_new_tokens(seq_group,\n                                                      SequenceStatus.WAITING,\n                                                      enable_chunking, budget)\n            if not enable_chunking:\n                num_prompt_tokens = waiting_seqs[0].get_len()\n                assert num_new_tokens == num_prompt_tokens\n\n            prompt_limit = self._get_prompt_limit(seq_group)\n            if num_new_tokens > prompt_limit:\n                logger.warning(\n                    \"Input prompt (%d tokens) is too long\"\n                    \" and exceeds limit of %d\", num_new_tokens, prompt_limit)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            # If the sequence group cannot be allocated, stop.\n            can_allocate = self.block_manager.can_allocate(seq_group)\n            if can_allocate == AllocStatus.LATER:\n                break\n            elif can_allocate == AllocStatus.NEVER:\n                logger.warning(\n                    \"Input prompt (%d tokens) is too long\"\n                    \" and exceeds the capacity of block_manager\",\n                    num_new_tokens)\n                for seq in waiting_seqs:\n                    seq.status = SequenceStatus.FINISHED_IGNORED\n                ignored_seq_groups.append(seq_group)\n                waiting_queue.popleft()\n                continue\n\n            lora_int_id = 0\n            if self.lora_enabled:\n                lora_int_id = seq_group.lora_int_id\n                assert curr_loras is not None\n                assert self.lora_config is not None\n                if (self.lora_enabled and lora_int_id > 0\n                        and lora_int_id not in curr_loras\n                        and len(curr_loras) >= self.lora_config.max_loras):\n                    # We don't have a space for another LoRA, so\n                    # we ignore this request for now.\n                    leftover_waiting_sequences.appendleft(seq_group)\n                    waiting_queue.popleft()\n                    continue\n\n            num_new_seqs = seq_group.get_max_num_running_seqs()\n            if (num_new_tokens == 0\n                    or not budget.can_schedule(num_new_tokens=num_new_tokens,\n                                               num_new_seqs=num_new_seqs)):\n                break\n\n            # Can schedule this request.\n            if curr_loras is not None and lora_int_id > 0:\n                curr_loras.add(lora_int_id)\n            waiting_queue.popleft()\n            self._allocate_and_set_running(seq_group)\n            seq_groups.append(\n                ScheduledSequenceGroup(seq_group=seq_group,\n                                       token_chunk_size=num_new_tokens))\n            budget.add_num_batched_tokens(seq_group.request_id, num_new_tokens)\n            budget.add_num_seqs(seq_group.request_id, num_new_seqs)\n\n        # Queue requests that couldn't be scheduled.\n        waiting_queue.extendleft(leftover_waiting_sequences)\n        if len(seq_groups) > 0:\n            self.prev_prompt = True\n\n        return SchedulerPrefillOutputs(\n            seq_groups=seq_groups,\n            ignored_seq_groups=ignored_seq_groups,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill=True))\n\n    def _schedule_default(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        The current policy is designed to optimize the throughput. First,\n        it batches as many prefill requests as possible. And it schedules\n        decodes. If there's a pressure on GPU memory, decode requests can\n        be swapped or preempted.\n        \"\"\"\n        # Include running requests to the budget.\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        # Make sure we include num running seqs before scheduling prefill,\n        # so that we don't schedule beyond max_num_seqs for prefill.\n        for seq_group in self.running:\n            budget.add_num_seqs(seq_group.request_id,\n                                seq_group.get_max_num_running_seqs())\n        curr_loras = set(\n            seq_group.lora_int_id for seq_group in self.running\n            if seq_group.lora_int_id > 0) if self.lora_enabled else None\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        running_scheduled = SchedulerRunningOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # If any requests are swapped, prioritized swapped requests.\n        if not self.swapped:\n            prefills = self._schedule_prefills(budget,\n                                               curr_loras,\n                                               enable_chunking=False)\n\n        # Don't schedule decodes if prefills are scheduled.\n        # NOTE: If `_schedule_prefills` doesn't enable chunking, self.running\n        # only contains decode requests, not chunked prefills.\n        if len(prefills.seq_groups) == 0:\n            running_scheduled = self._schedule_running(budget,\n                                                       curr_loras,\n                                                       enable_chunking=False)\n\n            # If any sequence group is preempted, do not swap in any sequence\n            # group. because it means there's no slot for new running requests.\n            if len(running_scheduled.preempted) + len(\n                    running_scheduled.swapped_out) == 0:\n                swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        preempted = (len(running_scheduled.preempted) +\n                     len(running_scheduled.swapped_out))\n\n        # There should be no prefill from running queue because this policy\n        # doesn't allow chunked prefills.\n        assert len(running_scheduled.prefill_seq_groups) == 0\n        assert len(swapped_in.prefill_seq_groups) == 0\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=len(prefills.seq_groups),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=running_scheduled.blocks_to_copy +\n            swapped_in.blocks_to_copy,\n            ignored_seq_groups=prefills.ignored_seq_groups +\n            swapped_in.infeasible_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=preempted,\n        )\n\n    def _schedule_chunked_prefill(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\n        \n        Chunked prefill allows to chunk prefill requests, batch them together\n        with decode requests. This policy 1. schedule as many decoding requests\n        as possible. 2. schedule chunked prefill requests that are not\n        finished. 3. schedule swapped request. 4. schedule new prefill\n        requests.\n\n        The policy can sustain the high GPU utilization because it can put\n        prefill and decodes requests to the same batch, while it improves\n        inter token latency because decodes requests don't need to be blocked\n        by prefill requests.\n        \"\"\"\n        budget = SchedulingBudget(\n            token_budget=self.scheduler_config.max_num_batched_tokens,\n            max_num_seqs=self.scheduler_config.max_num_seqs,\n        )\n        curr_loras: Set[int] = set()\n\n        prefills = SchedulerPrefillOutputs.create_empty()\n        swapped_in = SchedulerSwappedInOutputs.create_empty()\n\n        # Decoding should be always scheduled first by fcfs.\n        running_scheduled = self._schedule_running(budget,\n                                                   curr_loras,\n                                                   enable_chunking=True)\n\n        # Schedule swapped out requests.\n        # If preemption happens, it means we don't have space for swap-in.\n        if len(running_scheduled.preempted) + len(\n                running_scheduled.swapped_out) == 0:\n            swapped_in = self._schedule_swapped(budget, curr_loras)\n\n        # Schedule new prefills.\n        prefills = self._schedule_prefills(budget,\n                                           curr_loras,\n                                           enable_chunking=True)\n\n        assert (budget.num_batched_tokens <=\n                self.scheduler_config.max_num_batched_tokens)\n        assert budget.num_curr_seqs <= self.scheduler_config.max_num_seqs\n\n        # Update waiting requests.\n        self.waiting.extendleft(running_scheduled.preempted)\n        # Update new running requests.\n        self.running.extend([s.seq_group for s in prefills.seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in running_scheduled.prefill_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.decode_seq_groups])\n        self.running.extend(\n            [s.seq_group for s in swapped_in.prefill_seq_groups])\n        # Update swapped requests.\n        self.swapped.extend(running_scheduled.swapped_out)\n        return SchedulerOutputs(\n            scheduled_seq_groups=(prefills.seq_groups +\n                                  running_scheduled.prefill_seq_groups +\n                                  swapped_in.prefill_seq_groups +\n                                  running_scheduled.decode_seq_groups +\n                                  swapped_in.decode_seq_groups),\n            num_prefill_groups=(len(prefills.seq_groups) +\n                                len(swapped_in.prefill_seq_groups) +\n                                len(running_scheduled.prefill_seq_groups)),\n            num_batched_tokens=budget.num_batched_tokens,\n            blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n            blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n            blocks_to_copy=running_scheduled.blocks_to_copy +\n            swapped_in.blocks_to_copy,\n            ignored_seq_groups=prefills.ignored_seq_groups +\n            swapped_in.infeasible_seq_groups,\n            num_lookahead_slots=running_scheduled.num_lookahead_slots,\n            running_queue_size=len(self.running),\n            preempted=(len(running_scheduled.preempted) +\n                       len(running_scheduled.swapped_out)),\n        )\n\n    def _schedule(self) -> SchedulerOutputs:\n        \"\"\"Schedule queued requests.\"\"\"\n        if self.scheduler_config.chunked_prefill_enabled:\n            return self._schedule_chunked_prefill()\n        else:\n            return self._schedule_default()\n\n    def _can_append_slots(self, seq_group: SequenceGroup) -> bool:\n        \"\"\"Determine whether or not we have enough space in the KV cache to\n        continue generation of the sequence group.\n        \"\"\"\n        # It is True only for testing case to trigger artificial preemption.\n        if (self.enable_artificial_preemption\n                and random.uniform(0, 1) < ARTIFICIAL_PREEMPTION_PROB\n                and self.artificial_preempt_cnt > 0):\n            self.artificial_preempt_cnt -= 1\n            return False\n\n        # Appending slots only occurs in decoding.\n        is_prefill = False\n\n        return self.block_manager.can_append_slots(\n            seq_group=seq_group,\n            num_lookahead_slots=self._get_num_lookahead_slots(is_prefill),\n        )\n\n    def schedule(self) -> Tuple[List[SequenceGroupMetadata], SchedulerOutputs]:\n        # Schedule sequence groups.\n        # This function call changes the internal states of the scheduler\n        # such as self.running, self.swapped, and self.waiting.\n        scheduler_outputs = self._schedule()\n        now = time.time()\n\n        # Create input data structures.\n        seq_group_metadata_list: List[SequenceGroupMetadata] = []\n        for i, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n            token_chunk_size = scheduled_seq_group.token_chunk_size\n            seq_group.maybe_set_first_scheduled_time(now)\n\n            # seq_id -> SequenceData\n            seq_data: Dict[int, SequenceData] = {}\n            # seq_id -> physical block numbers\n            block_tables: Dict[int, List[int]] = {}\n\n            if seq_group.is_encoder_decoder():\n                # Encoder associated with SequenceGroup\n                encoder_seq_data = seq_group.get_encoder_seq().data\n                # Block table for cross-attention\n                # Also managed at SequenceGroup level\n                cross_block_table = self.block_manager.get_cross_block_table(\n                    seq_group)\n            else:\n                encoder_seq_data = None\n                cross_block_table = None\n\n            for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n                seq_id = seq.seq_id\n                seq_data[seq_id] = seq.data\n                block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                self.block_manager.access_all_blocks_in_seq(seq, now)\n\n            common_computed_block_nums = (\n                self.block_manager.get_common_computed_block_ids(\n                    seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n\n            do_sample = True\n            if seq_group.is_prefill():\n                seqs = seq_group.get_seqs()\n                # Prefill has only 1 sequence.\n                assert len(seqs) == 1\n                # In the next iteration, all prompt tokens are not computed.\n                # It means the prefill is chunked, and we don't need sampling.\n                # NOTE: We use get_len instead of get_prompt_len because when\n                # a sequence is preempted, prefill includes previous generated\n                # output tokens.\n                if (token_chunk_size + seqs[0].data.get_num_computed_tokens() <\n                        seqs[0].data.get_len()):\n                    do_sample = False\n\n            # It assumes the scheduled_seq_groups is ordered by\n            # prefill < decoding.\n            is_prompt = seq_group.is_prefill()\n            seq_group_metadata = SequenceGroupMetadata(\n                request_id=seq_group.request_id,\n                is_prompt=is_prompt,\n                seq_data=seq_data,\n                sampling_params=seq_group.sampling_params,\n                block_tables=block_tables,\n                do_sample=do_sample,\n                pooling_params=seq_group.pooling_params,\n                token_chunk_size=token_chunk_size,\n                lora_request=seq_group.lora_request,\n                computed_block_nums=common_computed_block_nums,\n                encoder_seq_data=encoder_seq_data,\n                cross_block_table=cross_block_table,\n                # `multi_modal_data` will only be present for the 1st comm\n                # between engine and worker.\n                # the subsequent comms can still use delta, but\n                # `multi_modal_data` will be None.\n                multi_modal_data=seq_group.multi_modal_data\n                if scheduler_outputs.num_prefill_groups > 0 else None,\n                prompt_adapter_request=seq_group.prompt_adapter_request,\n            )\n            seq_group_metadata_list.append(seq_group_metadata)\n\n        # Now that the batch has been created, we can assume all blocks in the\n        # batch will have been computed before the next scheduling invocation.\n        # This is because the engine assumes that a failure in model execution\n        # will crash the vLLM instance / will not retry.\n        for scheduled_seq_group in scheduler_outputs.scheduled_seq_groups:\n            self.block_manager.mark_blocks_as_computed(\n                scheduled_seq_group.seq_group)\n\n        return seq_group_metadata_list, scheduler_outputs\n\n    def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n        self.block_manager.fork(parent_seq, child_seq)\n\n    def free_seq(self, seq: Sequence) -> None:\n        \"\"\"Free a sequence from a block table.\"\"\"\n        self.block_manager.free(seq)\n\n    def free_finished_seq_groups(self) -> None:\n        remaining: Deque[SequenceGroup] = deque()\n        for seq_group in self.running:\n            if seq_group.is_finished():\n                # Free cross-attention block table, if it exists\n                self._free_seq_group_cross_attn_blocks(seq_group)\n                # Add the finished requests to the finished requests list.\n                # This list will be used to update the Mamba cache in the\n                # next step.\n                self._finished_requests_ids.append(seq_group.request_id)\n            else:\n                remaining.append(seq_group)\n        self.running = remaining\n\n    def _allocate_and_set_running(self, seq_group: SequenceGroup) -> None:\n        self.block_manager.allocate(seq_group)\n        for seq in seq_group.get_seqs(status=SequenceStatus.WAITING):\n            seq.status = SequenceStatus.RUNNING\n\n    def _append_slots(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_copy: List[Tuple[int, int]],\n    ) -> None:\n        \"\"\"Appends new slots to the sequences in the given sequence group.\n\n        Args:\n            seq_group (SequenceGroup): The sequence group containing the\n                sequences to append slots to.\n            blocks_to_copy (List[Tuple[int, int]]): A list of tuple of two\n                ints, the first int is the source block index, and the second\n                int is the destination block index. This list is updated with\n                the new source and destination block indices for the appended\n                slots.\n        \"\"\"\n        num_lookahead_slots = self._get_num_lookahead_slots(is_prefill=False)\n\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n            blocks_to_copy.extend(cows)\n\n    def _preempt(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n        preemption_mode: Optional[PreemptionMode] = None,\n    ) -> PreemptionMode:\n        # If preemption mode is not specified, we determine the mode as follows:\n        # We use recomputation by default since it incurs lower overhead than\n        # swapping. However, when the sequence group has multiple sequences\n        # (e.g., beam search), recomputation is not currently supported. In\n        # such a case, we use swapping instead.\n        # FIXME(woosuk): This makes our scheduling policy a bit bizarre.\n        # As swapped sequences are prioritized over waiting sequences,\n        # sequence groups with multiple sequences are implicitly prioritized\n        # over sequence groups with a single sequence.\n        # TODO(woosuk): Support recomputation for sequence groups with multiple\n        # sequences. This may require a more sophisticated CUDA kernel.\n        if self.user_specified_preemption_mode is None:\n            if seq_group.get_max_num_running_seqs() == 1:\n                preemption_mode = PreemptionMode.RECOMPUTE\n            else:\n                preemption_mode = PreemptionMode.SWAP\n\n        elif self.user_specified_preemption_mode == \"swap\":\n            preemption_mode = PreemptionMode.SWAP\n        else:\n            preemption_mode = PreemptionMode.RECOMPUTE\n\n        if self.num_cumulative_preemption % 50 == 0:\n            logger.warning(\n                \"Sequence group %s is preempted by %s mode because there is \"\n                \"not enough KV cache space. This can affect the end-to-end \"\n                \"performance. Increase gpu_memory_utilization or \"\n                \"tensor_parallel_size to provide more KV cache memory. \"\n                \"total_num_cumulative_preemption=%d\", seq_group.request_id,\n                preemption_mode, self.num_cumulative_preemption + 1)\n        self.num_cumulative_preemption += 1\n\n        if preemption_mode == PreemptionMode.RECOMPUTE:\n            self._preempt_by_recompute(seq_group)\n        elif preemption_mode == PreemptionMode.SWAP:\n            self._preempt_by_swap(seq_group, blocks_to_swap_out)\n        else:\n            raise AssertionError(\"Invalid preemption mode.\")\n        return preemption_mode\n\n    def _preempt_by_recompute(\n        self,\n        seq_group: SequenceGroup,\n    ) -> None:\n        seqs = seq_group.get_seqs(status=SequenceStatus.RUNNING)\n        assert len(seqs) == 1\n        for seq in seqs:\n            seq.status = SequenceStatus.WAITING\n            self.free_seq(seq)\n            seq.reset_state_for_recompute()\n\n    def _preempt_by_swap(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        self._swap_out(seq_group, blocks_to_swap_out)\n\n    def _swap_in(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_in: List[Tuple[int, int]],\n    ) -> None:\n        mapping = self.block_manager.swap_in(seq_group)\n        blocks_to_swap_in.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.SWAPPED):\n            seq.status = SequenceStatus.RUNNING\n\n    def _swap_out(\n        self,\n        seq_group: SequenceGroup,\n        blocks_to_swap_out: List[Tuple[int, int]],\n    ) -> None:\n        if not self.block_manager.can_swap_out(seq_group):\n            # FIXME(woosuk): Abort the sequence group instead of aborting the\n            # entire engine.\n            raise RuntimeError(\n                \"Aborted due to the lack of CPU swap space. Please increase \"\n                \"the swap space to avoid this error.\")\n        mapping = self.block_manager.swap_out(seq_group)\n        blocks_to_swap_out.extend(mapping)\n        for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n            seq.status = SequenceStatus.SWAPPED\n\n    def _passed_delay(self, now: float) -> bool:\n        if self.prev_prompt:\n            self.last_prompt_latency = now - self.prev_time\n        self.prev_time, self.prev_prompt = now, False\n        # Delay scheduling prompts to let waiting queue fill up\n        if self.scheduler_config.delay_factor > 0 and self.waiting:\n            earliest_arrival_time = min(\n                [e.metrics.arrival_time for e in self.waiting])\n            passed_delay = (\n                (now - earliest_arrival_time) >\n                (self.scheduler_config.delay_factor * self.last_prompt_latency)\n                or not self.running)\n        else:\n            passed_delay = True\n        return passed_delay\n\n    def _get_num_lookahead_slots(self, is_prefill: bool) -> int:\n        \"\"\"The number of slots to allocate per sequence per step, beyond known\n        token ids. Speculative decoding uses these slots to store KV activations\n        of tokens which may or may not be accepted.\n\n        Speculative decoding does not yet support prefill, so we do not perform\n        lookahead allocation for prefill.\n        \"\"\"\n        if is_prefill:\n            return 0\n\n        return self.scheduler_config.num_lookahead_slots\n\n    def _get_num_new_tokens(self, seq_group: SequenceGroup,\n                            status: SequenceStatus, enable_chunking: bool,\n                            budget: SchedulingBudget) -> int:\n        \"\"\"Get the next new tokens to compute for a given sequence group\n            that's in a given `status`.\n\n        The API could chunk the number of tokens to compute based on `budget`\n        if `enable_chunking` is True. If a sequence group has multiple\n        sequences (e.g., running beam search), it means it is in decoding\n        phase, so chunking doesn't happen.\n\n        Returns 0 if the new token cannot be computed due to token budget.\n        \"\"\"\n        num_new_tokens = 0\n        seqs = seq_group.get_seqs(status=status)\n        for seq in seqs:\n            num_new_tokens += seq.get_num_new_tokens()\n        assert num_new_tokens > 0\n        # Chunk if a running request cannot fit in.\n        # If number of seq > 1, it means it is doing beam search in a\n        # decode phase. Do not chunk in that case.\n        if enable_chunking and len(seqs) == 1:\n            num_new_tokens = min(num_new_tokens,\n                                 budget.remaining_token_budget())\n        return num_new_tokens\n",
      "diff": "diff --git a/vllm/core/scheduler.py b/vllm/core/scheduler.py\nindex 950abfccb..a40f6e2e2 100644\n--- a/vllm/core/scheduler.py\n+++ b/vllm/core/scheduler.py\n@@ -13,6 +13,7 @@ from vllm.lora.request import LoRARequest\n from vllm.prompt_adapter.request import PromptAdapterRequest\n from vllm.sequence import (Sequence, SequenceData, SequenceGroup,\n                            SequenceGroupMetadata, SequenceStatus)\n+from vllm.utils import PyObjectCache\n \n logger = init_logger(__name__)\n \n@@ -176,10 +177,10 @@ class SchedulerRunningOutputs:\n     enough memory, it can be preempted (for recompute) or swapped out.\n     \"\"\"\n     # Selected sequences that are running and in a decoding phase.\n-    decode_seq_groups: List[SequenceGroup]\n+    decode_seq_groups: List[ScheduledSequenceGroup]\n     # Selected sequences that are running and in a prefill phase.\n     # I.e., it means the prefill has been chunked.\n-    prefill_seq_groups: List[SequenceGroup]\n+    prefill_seq_groups: List[ScheduledSequenceGroup]\n     # The preempted sequences.\n     preempted: List[SequenceGroup]\n     # Sequences that are swapped out.\n@@ -191,6 +192,10 @@ class SchedulerRunningOutputs:\n     # The number of slots for lookahead decoding.\n     num_lookahead_slots: int\n \n+    # Optimization for fast-access to seq_group lists\n+    decode_seq_groups_list: List[SequenceGroup]\n+    prefill_seq_groups_list: List[SequenceGroup]\n+\n     @classmethod\n     def create_empty(cls) -> \"SchedulerRunningOutputs\":\n         return SchedulerRunningOutputs(\n@@ -201,6 +206,8 @@ class SchedulerRunningOutputs:\n             blocks_to_swap_out=[],\n             blocks_to_copy=[],\n             num_lookahead_slots=0,\n+            decode_seq_groups_list=[],\n+            prefill_seq_groups_list=[],\n         )\n \n \n@@ -259,6 +266,30 @@ class SchedulerPrefillOutputs:\n         )\n \n \n+def seq_group_metadata_builder():\n+    return SequenceGroupMetadata(request_id=\"\",\n+                                 is_prompt=False,\n+                                 seq_data={},\n+                                 sampling_params=None,\n+                                 block_tables={})\n+\n+\n+def scheduler_running_outputs_builder():\n+    return SchedulerRunningOutputs(decode_seq_groups=[],\n+                                   prefill_seq_groups=[],\n+                                   preempted=[],\n+                                   swapped_out=[],\n+                                   blocks_to_swap_out=[],\n+                                   blocks_to_copy=[],\n+                                   num_lookahead_slots=0,\n+                                   prefill_seq_groups_list=[],\n+                                   decode_seq_groups_list=[])\n+\n+\n+def scheduled_seq_group_builder():\n+    return ScheduledSequenceGroup(seq_group=None, token_chunk_size=0)\n+\n+\n class Scheduler:\n \n     def __init__(\n@@ -331,6 +362,14 @@ class Scheduler:\n                                        else 0)\n         self.num_cumulative_preemption: int = 0\n \n+        # Used to cache python objects\n+        self._seq_group_metadata_cache: PyObjectCache = PyObjectCache(\n+            seq_group_metadata_builder)\n+        self._scheduler_running_outputs_cache: PyObjectCache = PyObjectCache(\n+            scheduler_running_outputs_builder)\n+        self._scheduled_seq_group_cache: PyObjectCache = PyObjectCache(\n+            scheduled_seq_group_builder)\n+\n     @property\n     def lora_enabled(self) -> bool:\n         return bool(self.lora_config)\n@@ -441,14 +480,30 @@ class Scheduler:\n         Returns:\n             SchedulerRunningOutputs.\n         \"\"\"\n+        ret: SchedulerRunningOutputs = \\\n+            self._scheduler_running_outputs_cache.get_object()\n+        ret.blocks_to_swap_out.clear()\n+        ret.blocks_to_copy.clear()\n+        ret.decode_seq_groups.clear()\n+        ret.prefill_seq_groups.clear()\n+        ret.preempted.clear()\n+        ret.swapped_out.clear()\n+\n+        ret.num_lookahead_slots = self._get_num_lookahead_slots(\n+            is_prefill=False)\n+\n+        ret.decode_seq_groups_list.clear()\n+        ret.prefill_seq_groups_list.clear()\n+\n         # Blocks that need to be swapped or copied before model execution.\n-        blocks_to_swap_out: List[Tuple[int, int]] = []\n-        blocks_to_copy: List[Tuple[int, int]] = []\n+        blocks_to_swap_out: List[Tuple[int, int]] = ret.blocks_to_swap_out\n+        blocks_to_copy: List[Tuple[int, int]] = ret.blocks_to_copy\n \n-        decode_seq_groups: List[ScheduledSequenceGroup] = []\n-        prefill_seq_groups: List[ScheduledSequenceGroup] = []\n-        preempted: List[SequenceGroup] = []\n-        swapped_out: List[SequenceGroup] = []\n+        decode_seq_groups: List[ScheduledSequenceGroup] = ret.decode_seq_groups\n+        prefill_seq_groups: List[\n+            ScheduledSequenceGroup] = ret.prefill_seq_groups\n+        preempted: List[SequenceGroup] = ret.preempted\n+        swapped_out: List[SequenceGroup] = ret.swapped_out\n \n         # NOTE(woosuk): Preemption happens only when there is no available slot\n         # to keep all the sequence groups in the RUNNING state.\n@@ -497,15 +552,19 @@ class Scheduler:\n             else:\n                 self._append_slots(seq_group, blocks_to_copy)\n                 is_prefill = seq_group.is_prefill()\n+\n+                scheduled_seq_group: ScheduledSequenceGroup = \\\n+                    self._scheduled_seq_group_cache.get_object()\n+                scheduled_seq_group.seq_group = seq_group\n                 if is_prefill:\n-                    prefill_seq_groups.append(\n-                        ScheduledSequenceGroup(\n-                            seq_group=seq_group,\n-                            token_chunk_size=num_running_tokens))\n+                    scheduled_seq_group.token_chunk_size = num_running_tokens\n+                    prefill_seq_groups.append(scheduled_seq_group)\n+                    ret.prefill_seq_groups_list.append(seq_group)\n                 else:\n-                    decode_seq_groups.append(\n-                        ScheduledSequenceGroup(seq_group=seq_group,\n-                                               token_chunk_size=1))\n+                    scheduled_seq_group.token_chunk_size = 1\n+                    decode_seq_groups.append(scheduled_seq_group)\n+                    ret.decode_seq_groups_list.append(seq_group)\n+\n                 budget.add_num_batched_tokens(seq_group.request_id,\n                                               num_running_tokens)\n                 # OPTIMIZATION:  Note that get_max_num_running_seqs is\n@@ -518,15 +577,10 @@ class Scheduler:\n                 if curr_loras is not None and seq_group.lora_int_id > 0:\n                     curr_loras.add(seq_group.lora_int_id)\n \n-        return SchedulerRunningOutputs(\n-            decode_seq_groups=decode_seq_groups,\n-            prefill_seq_groups=prefill_seq_groups,\n-            preempted=preempted,\n-            swapped_out=swapped_out,\n-            blocks_to_swap_out=blocks_to_swap_out,\n-            blocks_to_copy=blocks_to_copy,\n-            num_lookahead_slots=self._get_num_lookahead_slots(\n-                is_prefill=False))\n+        self._scheduler_running_outputs_cache.reset()\n+        self._scheduled_seq_group_cache.reset()\n+\n+        return ret\n \n     def _schedule_swapped(\n         self,\n@@ -820,11 +874,15 @@ class Scheduler:\n         # Update waiting requests.\n         self.waiting.extendleft(running_scheduled.preempted)\n         # Update new running requests.\n-        self.running.extend([s.seq_group for s in prefills.seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in running_scheduled.decode_seq_groups])\n-        self.running.extend(\n-            [s.seq_group for s in swapped_in.decode_seq_groups])\n+        if len(prefills.seq_groups) > 0:\n+            self.running.extend([s.seq_group for s in prefills.seq_groups])\n+\n+        self.running.extend(running_scheduled.decode_seq_groups_list)\n+\n+        if len(swapped_in.decode_seq_groups) > 0:\n+            self.running.extend(\n+                [s.seq_group for s in swapped_in.decode_seq_groups])\n+\n         # Update swapped requests.\n         self.swapped.extend(running_scheduled.swapped_out)\n         preempted = (len(running_scheduled.preempted) +\n@@ -834,18 +892,30 @@ class Scheduler:\n         # doesn't allow chunked prefills.\n         assert len(running_scheduled.prefill_seq_groups) == 0\n         assert len(swapped_in.prefill_seq_groups) == 0\n+\n+        # Merge lists\n+        num_prefill_groups = len(prefills.seq_groups)\n+        if num_prefill_groups > 0:\n+            scheduled_seq_groups = prefills.seq_groups\n+            scheduled_seq_groups.extend(running_scheduled.decode_seq_groups)\n+        else:\n+            scheduled_seq_groups = running_scheduled.decode_seq_groups\n+        scheduled_seq_groups.extend(swapped_in.decode_seq_groups)\n+\n+        blocks_to_copy = running_scheduled.blocks_to_copy\n+        blocks_to_copy.extend(swapped_in.blocks_to_copy)\n+\n+        ignored_seq_groups = prefills.ignored_seq_groups\n+        ignored_seq_groups.extend(swapped_in.infeasible_seq_groups)\n+\n         return SchedulerOutputs(\n-            scheduled_seq_groups=(prefills.seq_groups +\n-                                  running_scheduled.decode_seq_groups +\n-                                  swapped_in.decode_seq_groups),\n-            num_prefill_groups=len(prefills.seq_groups),\n+            scheduled_seq_groups=scheduled_seq_groups,\n+            num_prefill_groups=num_prefill_groups,\n             num_batched_tokens=budget.num_batched_tokens,\n             blocks_to_swap_in=swapped_in.blocks_to_swap_in,\n             blocks_to_swap_out=running_scheduled.blocks_to_swap_out,\n-            blocks_to_copy=running_scheduled.blocks_to_copy +\n-            swapped_in.blocks_to_copy,\n-            ignored_seq_groups=prefills.ignored_seq_groups +\n-            swapped_in.infeasible_seq_groups,\n+            blocks_to_copy=blocks_to_copy,\n+            ignored_seq_groups=ignored_seq_groups,\n             num_lookahead_slots=running_scheduled.num_lookahead_slots,\n             running_queue_size=len(self.running),\n             preempted=preempted,\n@@ -963,6 +1033,9 @@ class Scheduler:\n         scheduler_outputs = self._schedule()\n         now = time.time()\n \n+        if not self.cache_config.enable_prefix_caching:\n+            common_computed_block_nums = []\n+\n         # Create input data structures.\n         seq_group_metadata_list: List[SequenceGroupMetadata] = []\n         for i, scheduled_seq_group in enumerate(\n@@ -971,10 +1044,15 @@ class Scheduler:\n             token_chunk_size = scheduled_seq_group.token_chunk_size\n             seq_group.maybe_set_first_scheduled_time(now)\n \n+            seq_group_metadata = self._seq_group_metadata_cache.get_object()\n+            seq_group_metadata.seq_data.clear()\n+            seq_group_metadata.block_tables.clear()\n+\n             # seq_id -> SequenceData\n-            seq_data: Dict[int, SequenceData] = {}\n+            seq_data: Dict[int, SequenceData] = seq_group_metadata.seq_data\n             # seq_id -> physical block numbers\n-            block_tables: Dict[int, List[int]] = {}\n+            block_tables: Dict[int,\n+                               List[int]] = seq_group_metadata.block_tables\n \n             if seq_group.is_encoder_decoder():\n                 # Encoder associated with SequenceGroup\n@@ -993,9 +1071,10 @@ class Scheduler:\n                 block_tables[seq_id] = self.block_manager.get_block_table(seq)\n                 self.block_manager.access_all_blocks_in_seq(seq, now)\n \n-            common_computed_block_nums = (\n-                self.block_manager.get_common_computed_block_ids(\n-                    seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n+            if self.cache_config.enable_prefix_caching:\n+                common_computed_block_nums = (\n+                    self.block_manager.get_common_computed_block_ids(\n+                        seq_group.get_seqs(status=SequenceStatus.RUNNING)))\n \n             do_sample = True\n             if seq_group.is_prefill():\n@@ -1014,7 +1093,8 @@ class Scheduler:\n             # It assumes the scheduled_seq_groups is ordered by\n             # prefill < decoding.\n             is_prompt = seq_group.is_prefill()\n-            seq_group_metadata = SequenceGroupMetadata(\n+\n+            seq_group_metadata.__init__(\n                 request_id=seq_group.request_id,\n                 is_prompt=is_prompt,\n                 seq_data=seq_data,\n@@ -1045,6 +1125,8 @@ class Scheduler:\n             self.block_manager.mark_blocks_as_computed(\n                 scheduled_seq_group.seq_group)\n \n+        self._seq_group_metadata_cache.reset()\n+\n         return seq_group_metadata_list, scheduler_outputs\n \n     def fork_seq(self, parent_seq: Sequence, child_seq: Sequence) -> None:\n@@ -1093,7 +1175,8 @@ class Scheduler:\n \n         for seq in seq_group.get_seqs(status=SequenceStatus.RUNNING):\n             cows = self.block_manager.append_slots(seq, num_lookahead_slots)\n-            blocks_to_copy.extend(cows)\n+            if len(cows) > 0:\n+                blocks_to_copy.extend(cows)\n \n     def _preempt(\n         self,",
      "change_type": "modified",
      "lines_added": 128,
      "lines_removed": 45
    },
    {
      "file_path": "vllm/model_executor/__init__.py",
      "old_content": "from vllm.model_executor.parameter import (BasevLLMParameter,\n                                           PackedvLLMParameter)\nfrom vllm.model_executor.sampling_metadata import SamplingMetadata\nfrom vllm.model_executor.utils import set_random_seed\n\n__all__ = [\n    \"SamplingMetadata\",\n    \"set_random_seed\",\n    \"BasevLLMParameter\",\n    \"PackedvLLMParameter\",\n]\n",
      "diff": "diff --git a/vllm/model_executor/__init__.py b/vllm/model_executor/__init__.py\nindex 5c767e22d..7278c7fbe 100644\n--- a/vllm/model_executor/__init__.py\n+++ b/vllm/model_executor/__init__.py\n@@ -1,10 +1,12 @@\n from vllm.model_executor.parameter import (BasevLLMParameter,\n                                            PackedvLLMParameter)\n-from vllm.model_executor.sampling_metadata import SamplingMetadata\n+from vllm.model_executor.sampling_metadata import (SamplingMetadata,\n+                                                   SamplingMetadataCache)\n from vllm.model_executor.utils import set_random_seed\n \n __all__ = [\n     \"SamplingMetadata\",\n+    \"SamplingMetadataCache\",\n     \"set_random_seed\",\n     \"BasevLLMParameter\",\n     \"PackedvLLMParameter\",",
      "change_type": "modified",
      "lines_added": 4,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/model_executor/sampling_metadata.py",
      "old_content": "import random\nfrom array import array\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\n\nfrom vllm.sampling_params import SamplingParams, SamplingType\nfrom vllm.sequence import SequenceData, SequenceGroupMetadata\nfrom vllm.triton_utils.sample import get_num_triton_sampler_splits\nfrom vllm.utils import (async_tensor_h2d, is_pin_memory_available,\n                        make_tensor_with_pad, maybe_expand_dim)\n\n_SAMPLING_EPS = 1e-5\n_SEED_0_REPLACEMENT = 3403598558\n# Some triton sampler related code is guarded before it is ready.\n_USE_TRITON_SAMPLER = False\n\n\n@dataclass\nclass SequenceGroupToSample:\n    # |---------- N-1 iteration --------|\n    # |---------------- N iteration ---------------------|\n    # |- tokenA -|......................|-- newTokens ---|\n    # |---------- context_len ----------|\n    # |-------------------- seq_len ----------------------|\n    #                                   |-- query_len ---|\n\n    # Sequence ids for the sequence group in a previous step.\n    seq_ids: List[int]\n    sampling_params: SamplingParams\n    # seq_id -> sequence data.\n    seq_data: Dict[int, SequenceData]\n    # The length of the sequence (all tokens seen in the past + new token to\n    # compute attention) of the sequence group. None if it is in a decode\n    # stage.\n    seq_len: Optional[int]\n    # The length of new query tokens to compute in the current step. None if it\n    # is in a decode stage. The length of query_len <= seq_len if chunked\n    # prefill is enabled.\n    query_len: Optional[int]\n    # A random number generator for sampling.\n    generator: Optional[torch.Generator]\n    # True if the sequence group is in prefill stage. False if it is in a\n    # decode stage.\n    is_prompt: bool\n    # Query token indices from logits. to compute prompt logprob. Empty if\n    # prompt logprob is not required.\n    prompt_logprob_indices: List[int]\n    # Sample token indices from logits. Empty if sampling is not required.\n    sample_indices: List[int]\n\n    @property\n    def do_sample(self):\n        return len(self.sample_indices) > 0\n\n    def __post_init__(self):\n        if len(self.prompt_logprob_indices) > 0:\n            assert self.sampling_params.prompt_logprobs is not None\n        if self.is_prompt:\n            assert self.seq_len is not None\n            assert self.query_len is not None\n\n\nclass SamplingMetadata:\n    \"\"\"Metadata for input sequences. Used in sampler.\n\n    The usage is as follow;\n    ```\n    hidden_states = execute_model(...)\n    logits = hidden_states[sampling_metadata.selected_token_indices]\n    sample(logits)\n\n    def sample(logits):\n        # Use categorized_sample_indices for sampling....\n    ```\n\n    Args:\n        seq_groups: List of batched sequence groups.\n        selected_token_indices: (num_query_tokens_to_logprob). Indices to find\n            logits from the initial model output hidden states.\n        categorized_sample_indices: SamplingType -> token indices to sample.\n            Each token indices is 2D tensor of (num_indices, num_indices) where\n            the first item means the sample index within the returned logit\n            (before pruning padding), and the second item means the sample\n            index after pruning using selected_token_indices.\n            For example, if the returned logit is [1, 2, 3], and we select\n            [1, 2] for sampling, the pruned logit will be [2, 3]. In this case,\n            The first tuple is [1, 2] (sampled index within original logit),\n            and the second tuple is [0, 1] (sampled index within pruned logit).\n        num_prompts: Number of prompt sequence groups in seq_groups.\n        skip_sampler_cpu_output: Indicates if we want to skip the GPU=>CPU \n            serialization of token outputs.\n        reuse_sampling_tensors: Indicates if we want to reuse sampling \n            tensors that are part of the sampler forward pass. Currently,\n            it is mainly used for multi-step decode.\n            \n    \"\"\"\n\n    def __init__(\n        self,\n        seq_groups: List[SequenceGroupToSample],\n        selected_token_indices: torch.Tensor,\n        categorized_sample_indices: Dict[SamplingType, torch.Tensor],\n        num_prompts: int,\n        skip_sampler_cpu_output: bool = False,\n        reuse_sampling_tensors: bool = False,\n    ) -> None:\n        self.seq_groups = seq_groups\n        self.selected_token_indices = selected_token_indices\n        self.categorized_sample_indices = categorized_sample_indices\n        self.num_prompts = num_prompts\n        self.skip_sampler_cpu_output = skip_sampler_cpu_output\n        self.reuse_sampling_tensors = reuse_sampling_tensors\n\n    @staticmethod\n    def prepare(\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        seq_lens: List[int],\n        query_lens: Optional[List[int]],\n        device: str,\n        pin_memory: bool,\n        generators: Optional[Dict[str, torch.Generator]] = None,\n    ) -> \"SamplingMetadata\":\n        (\n            seq_groups,\n            selected_token_indices,\n            categorized_sample_indices,\n            num_prompts,\n        ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\n                                device, generators)\n        selected_token_indices = async_tensor_h2d(selected_token_indices,\n                                                  dtype=torch.long,\n                                                  target_device=device,\n                                                  pin_memory=pin_memory)\n        categorized_sample_indices = {\n            t: maybe_expand_dim(\n                async_tensor_h2d(seq_ids,\n                                 dtype=torch.int,\n                                 target_device=device,\n                                 pin_memory=pin_memory), 2, 2)\n            for t, seq_ids in categorized_sample_indices.items()\n        }\n\n        sampling_metadata = SamplingMetadata(\n            seq_groups=seq_groups,\n            selected_token_indices=selected_token_indices,\n            categorized_sample_indices=categorized_sample_indices,\n            num_prompts=num_prompts,\n        )\n        return sampling_metadata\n\n    def __repr__(self) -> str:\n        return (\n            \"SamplingMetadata(\"\n            f\"seq_groups={self.seq_groups}, \"\n            f\"selected_token_indices={self.selected_token_indices}, \"\n            f\"categorized_sample_indices={self.categorized_sample_indices}), \")\n\n\ndef _prepare_seq_groups(\n    seq_group_metadata_list: List[SequenceGroupMetadata],\n    seq_lens: List[int],\n    query_lens: Optional[List[int]],\n    device: str,\n    generators: Optional[Dict[str, torch.Generator]] = None,\n) -> Tuple[List[SequenceGroupToSample], List[int], Dict[\n        SamplingType, List[Tuple[int, int]]], int]:\n    \"\"\"Prepare sequence groups and indices for sampling.\n\n    Args:\n        seq_group_metadata_list: A list of sequence group to batch.\n        seq_lens: A list of sequence lens per sequence group.\n            Index of prompt len should match with seq_group_metadata_list.\n        query_lens: A list of query lengths. Prompt lens include the length\n            of entire prompt tokens, and it could be shorter.\n        device: A device to use for random number generators,\n            `SequenceGroupToSample.generator`.\n        generators: A store of per-request random number generators used\n            for seeded requests.\n\n    Returns:\n        seq_groups: A list of sequence group to sample.\n        selected_token_indices: See the definition from `SamplingMetadata`.\n        categorized_sample_indices: See the definition from `SamplingMetadata`.\n        num_prompts: Total number of prompts from `seq_group_metadata_list`.\n    \"\"\"\n    # Batched sequence groups for the current model forward stsep.\n    seq_groups: List[SequenceGroupToSample] = []\n    # A list of token indices to sample/compute logprob. It is used to\n    # prune the outcome logits from the model for the performance.\n    selected_token_indices: List[int] = []\n    # Used for selected_token_indices.\n    model_output_idx = 0\n\n    # Sampling type -> (\n    # indices to sample/prompt logprob within pruned output logits,\n    # indices to sample within pruned logits)\n    categorized_sample_indices: Dict[SamplingType, List[Tuple[int, int]]] = {\n        t: []\n        for t in SamplingType\n    }\n    # Index of logits to compute logprob. Logits include both prompt logprob\n    # and sample logprob indices.\n    logit_idx = 0\n    # Index to sample from a sample tensor. It is used by triton sample kernel.\n    # See `_sample_with_triton_kernel` for more details.\n    sample_idx = 0\n    # Total number of prompts from given sequence groups.\n    num_prompts = 0\n\n    for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n        seq_ids = list(seq_group_metadata.seq_data.keys())\n        sampling_params = seq_group_metadata.sampling_params\n        is_prompt = seq_group_metadata.is_prompt\n        generator: Optional[torch.Generator] = None\n        # If the current seq group is in decode stage, it is None.\n        seq_len: Optional[int] = None\n        query_len: Optional[int] = None\n        prompt_logprob_indices: List[int] = []\n        sample_indices: List[int] = []\n        do_sample = seq_group_metadata.do_sample\n\n        if seq_group_metadata.is_prompt:\n            if sampling_params.seed is not None:\n                generator = torch.Generator(device=device).manual_seed(\n                    sampling_params.seed)\n                if generators is not None:\n                    generators[seq_group_metadata.request_id] = generator\n\n            num_prompts += 1\n            num_prefill_sample = len(seq_ids)\n            assert num_prefill_sample == 1\n            assert query_lens is not None and seq_lens is not None\n            query_len, seq_len = query_lens[i], seq_lens[i]\n            # If we need sampling, exclude num_prefill_sample tokens from\n            # prompt logprob.\n            prompt_logprob_len = (query_len - num_prefill_sample\n                                  if do_sample else query_len)\n            sample_len = num_prefill_sample if do_sample else 0\n        else:\n            # Decode\n            prompt_logprob_len = 0\n            sample_len = len(seq_ids) if do_sample else 0\n\n            if sampling_params.seed is not None and generators is not None:\n                generator = generators.get(seq_group_metadata.request_id)\n\n        # Update indices to select from the model output.\n        \"\"\"\n        This blocks computes selected_token_indices which is used in the\n        following way.\n\n        hidden_states = model(...)\n        logits = hidden_states[selected_token_indices]\n        \"\"\"\n\n        if sampling_params.prompt_logprobs is not None:\n            selected_token_indices.extend(\n                range(model_output_idx, model_output_idx + prompt_logprob_len))\n        model_output_idx += prompt_logprob_len\n        if do_sample:\n            selected_token_indices.extend(\n                range(model_output_idx, model_output_idx + sample_len))\n        model_output_idx += sample_len\n\n        # We now find indices for logprob computation and sampling.\n        \"\"\"\n        This block computes categorized_sample_indices which is used in the\n        following way.\n\n        hidden_states = model(...)\n        logits = hidden_states[selected_token_indices]\n        def sample(logits):\n           # Use categorized_sample_indices for sampling.\n           # prompt_logprob_indices to find prompt logprob indices.\n           # sample_indices to find sample indices.\n        \"\"\"\n\n        if sampling_params.prompt_logprobs is not None:\n            prompt_logprob_indices.extend(\n                range(logit_idx, logit_idx + prompt_logprob_len))\n            logit_idx += prompt_logprob_len\n        if do_sample:\n            sample_indices.extend(range(logit_idx, logit_idx + sample_len))\n            categorized_sample_indices[sampling_params.sampling_type].extend(\n                list(\n                    zip(range(logit_idx, logit_idx + sample_len),\n                        range(sample_idx, sample_idx + sample_len))))\n            logit_idx += sample_len\n            sample_idx += sample_len\n\n        seq_groups.append(\n            SequenceGroupToSample(\n                seq_ids=seq_ids,\n                sampling_params=sampling_params,\n                seq_data=seq_group_metadata.seq_data,\n                seq_len=seq_len,\n                query_len=query_len,\n                generator=generator,\n                is_prompt=is_prompt,\n                prompt_logprob_indices=list(prompt_logprob_indices),\n                sample_indices=list(sample_indices)))\n    return (seq_groups, selected_token_indices, categorized_sample_indices,\n            num_prompts)\n\n\n@dataclass\nclass SamplingTensors:\n    \"\"\"Tensors for sampling.\"\"\"\n\n    temperatures: torch.Tensor\n    top_ps: torch.Tensor\n    top_ks: torch.Tensor\n    min_ps: torch.Tensor\n    presence_penalties: torch.Tensor\n    frequency_penalties: torch.Tensor\n    repetition_penalties: torch.Tensor\n    sampling_seeds: torch.Tensor\n    sample_indices: torch.Tensor\n    extra_seeds: Optional[torch.Tensor]\n    prompt_tokens: torch.Tensor\n    output_tokens: torch.Tensor\n\n    @classmethod\n    def from_sampling_metadata(\n        cls,\n        sampling_metadata: \"SamplingMetadata\",\n        vocab_size: int,\n        device: torch.device,\n        dtype: torch.dtype,\n        *,\n        extra_seeds_to_generate: int = 0,\n        extra_entropy: Optional[Tuple[int, ...]] = None\n    ) -> Tuple[\"SamplingTensors\", bool, bool, bool]:\n        \"\"\"\n        extra_seeds_to_generate: extra seeds to generate using the\n            user-defined seed for each sequence.\n        extra_entropy: extra entropy to use when generating seeds.\n        \"\"\"\n        prompt_tokens: List[array] = []\n        output_tokens: List[array] = []\n        top_ks: List[int] = []\n        temperatures: List[float] = []\n        top_ps: List[float] = []\n        min_ps: List[float] = []\n        presence_penalties: List[float] = []\n        frequency_penalties: List[float] = []\n        repetition_penalties: List[float] = []\n        sampling_seeds: List[int] = []\n        sample_indices: List[int] = []\n        do_penalties = False\n        do_top_p_top_k = False\n        do_min_p = False\n\n        if _USE_TRITON_SAMPLER:\n            prompt_best_of: List[int] = []\n\n            # We need one base seed per Triton slice.\n            seeds_to_generate = (extra_seeds_to_generate +\n                                 get_num_triton_sampler_splits(vocab_size))\n\n        assert sampling_metadata.seq_groups is not None\n        for seq_group in sampling_metadata.seq_groups:\n            seq_ids = seq_group.seq_ids\n            sampling_params = seq_group.sampling_params\n            temperature = sampling_params.temperature\n            p = sampling_params.presence_penalty\n            f = sampling_params.frequency_penalty\n            r = sampling_params.repetition_penalty\n            top_p = sampling_params.top_p\n            min_p = sampling_params.min_p\n\n            # k should not be greater than the vocab size.\n            top_k = min(sampling_params.top_k, vocab_size)\n            top_k = vocab_size if top_k == -1 else top_k\n            if temperature < _SAMPLING_EPS:\n                # NOTE: Zero temperature means deterministic sampling\n                # (i.e., greedy sampling or beam search).\n                # Set the temperature to 1 to avoid division by zero.\n                temperature = 1.0\n            if not do_top_p_top_k and (top_p < 1.0 - _SAMPLING_EPS\n                                       or top_k != vocab_size):\n                do_top_p_top_k = True\n            if not do_min_p and min_p > _SAMPLING_EPS:\n                do_min_p = True\n            if not do_penalties and (abs(p) >= _SAMPLING_EPS\n                                     or abs(f) >= _SAMPLING_EPS\n                                     or abs(r - 1.0) >= _SAMPLING_EPS):\n                do_penalties = True\n\n            is_prompt = seq_group.is_prompt\n            if (is_prompt and sampling_params.prompt_logprobs is not None):\n                # For tokens in the prompt that we only need to get\n                # their logprobs\n                query_len = seq_group.query_len\n                assert query_len is not None\n                prefill_len = len(seq_group.prompt_logprob_indices)\n                temperatures += [temperature] * prefill_len\n                top_ps += [top_p] * prefill_len\n                top_ks += [top_k] * prefill_len\n                min_ps += [min_p] * prefill_len\n                presence_penalties += [0] * prefill_len\n                frequency_penalties += [0] * prefill_len\n                repetition_penalties += [1] * prefill_len\n\n            if seq_group.do_sample:\n                sample_lens = len(seq_group.sample_indices)\n                assert sample_lens == len(seq_ids)\n                temperatures += [temperature] * len(seq_ids)\n                top_ps += [top_p] * len(seq_ids)\n                top_ks += [top_k] * len(seq_ids)\n                min_ps += [min_p] * len(seq_ids)\n                presence_penalties += [p] * len(seq_ids)\n                frequency_penalties += [f] * len(seq_ids)\n                repetition_penalties += [r] * len(seq_ids)\n\n            if _USE_TRITON_SAMPLER:\n                if is_prompt:\n                    prompt_best_of.append(sampling_params.best_of)\n                    query_len = seq_group.query_len\n                    assert query_len is not None\n\n                seed = sampling_params.seed\n                is_greedy = sampling_params.sampling_type == SamplingType.GREEDY\n\n                for seq_id in seq_ids:\n                    seq_data = seq_group.seq_data[seq_id]\n                    extra_entropy = extra_entropy or ()\n                    seq_seeds = cls._get_sequence_seeds(\n                        seed,\n                        seq_data.get_len(),\n                        *extra_entropy,\n                        seq_id,\n                        seeds_to_generate=seeds_to_generate,\n                        is_greedy=is_greedy)\n                    sampling_seeds.append(seq_seeds)\n                sample_indices.extend(seq_group.sample_indices)\n\n        if do_penalties:\n            for seq_group in sampling_metadata.seq_groups:\n                seq_ids = seq_group.seq_ids\n                if (seq_group.is_prompt\n                        and sampling_params.prompt_logprobs is not None):\n                    prefill_len = len(seq_group.prompt_logprob_indices)\n                    prompt_tokens.extend(\n                        array('l') for _ in range(prefill_len))\n                    output_tokens.extend(\n                        array('l') for _ in range(prefill_len))\n                if seq_group.do_sample:\n                    for seq_id in seq_ids:\n                        seq_data = seq_group.seq_data[seq_id]\n                        prompt_tokens.append(seq_data.prompt_token_ids_array)\n                        output_tokens.append(seq_data.output_token_ids_array)\n\n        sampling_tensors = SamplingTensors.from_lists(\n            temperatures, top_ps, top_ks, min_ps, presence_penalties,\n            frequency_penalties, repetition_penalties, sampling_seeds,\n            sample_indices, prompt_tokens, output_tokens, vocab_size,\n            extra_seeds_to_generate, device, dtype)\n        return (sampling_tensors, do_penalties, do_top_p_top_k, do_min_p)\n\n    @classmethod\n    def from_lists(cls, temperatures: List[float], top_ps: List[float],\n                   top_ks: List[int], min_ps: List[float],\n                   presence_penalties: List[float],\n                   frequency_penalties: List[float],\n                   repetition_penalties: List[float],\n                   sampling_seeds: List[int], sample_indices: List[int],\n                   prompt_tokens: List[array], output_tokens: List[array],\n                   vocab_size: int, extra_seeds_to_generate: int,\n                   device: torch.device,\n                   dtype: torch.dtype) -> \"SamplingTensors\":\n        # Note that the performance will be very bad without\n        # pinned memory.\n        pin_memory = is_pin_memory_available()\n\n        do_penalties = prompt_tokens or output_tokens\n\n        if do_penalties:\n            prompt_t = make_tensor_with_pad(\n                prompt_tokens,\n                vocab_size,\n                device=\"cpu\",\n                dtype=torch.int64,\n                pin_memory=pin_memory,\n            )\n            output_t = make_tensor_with_pad(\n                output_tokens,\n                vocab_size,\n                device=\"cpu\",\n                dtype=torch.int64,\n                pin_memory=pin_memory,\n            )\n        else:\n            empty_tensor = torch.empty(0, device=device, dtype=torch.long)\n            prompt_t = empty_tensor\n            output_t = empty_tensor\n\n        temperatures_t = torch.tensor(\n            temperatures,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ps_t = torch.tensor(\n            top_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        min_ps_t = torch.tensor(\n            min_ps,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        presence_penalties_t = torch.tensor(\n            presence_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        frequency_penalties_t = torch.tensor(\n            frequency_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        repetition_penalties_t = torch.tensor(\n            repetition_penalties,\n            device=\"cpu\",\n            dtype=dtype,\n            pin_memory=pin_memory,\n        )\n        top_ks_t = torch.tensor(\n            top_ks,\n            device=\"cpu\",\n            dtype=torch.int,\n            pin_memory=pin_memory,\n        )\n        sample_indices_t = torch.tensor(\n            sample_indices,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        )\n        # need to transpose and make contiguous to\n        # copy the tensor correctly.\n        # [batch_size, n_seeds] -> [n_seeds, batch_size]\n        sampling_seeds_t = torch.tensor(\n            sampling_seeds,\n            device=\"cpu\",\n            dtype=torch.long,\n            pin_memory=pin_memory,\n        ).t().contiguous()\n\n        # Because the memory is pinned, we can do non-blocking\n        # transfer to device.\n\n        # How many seeds the sample operation itself will need.\n        num_base_seeds = sampling_seeds_t.shape[0] - extra_seeds_to_generate\n        sampling_seeds_gpu = sampling_seeds_t.to(device=device,\n                                                 non_blocking=True)\n        extra_seeds_gpu = sampling_seeds_gpu[num_base_seeds:]\n        if not extra_seeds_gpu.numel():\n            extra_seeds_gpu = None\n        sampling_seeds_gpu = sampling_seeds_gpu[:num_base_seeds]\n\n        return cls(\n            temperatures=temperatures_t.to(device=device, non_blocking=True),\n            top_ps=top_ps_t.to(device=device, non_blocking=True),\n            top_ks=top_ks_t.to(device=device, non_blocking=True),\n            min_ps=min_ps_t.to(device=device, non_blocking=True),\n            presence_penalties=presence_penalties_t.to(device=device,\n                                                       non_blocking=True),\n            frequency_penalties=frequency_penalties_t.to(device=device,\n                                                         non_blocking=True),\n            repetition_penalties=repetition_penalties_t.to(device=device,\n                                                           non_blocking=True),\n            prompt_tokens=prompt_t.to(device=device, non_blocking=True),\n            output_tokens=output_t.to(device=device, non_blocking=True),\n            sampling_seeds=sampling_seeds_gpu,\n            sample_indices=sample_indices_t.to(device=device,\n                                               non_blocking=True),\n            extra_seeds=extra_seeds_gpu,\n        )\n\n    @staticmethod\n    def _get_sequence_seeds(\n        seed: int,\n        *extra_entropy: int,\n        seeds_to_generate: int,\n        is_greedy: bool,\n    ):\n        \"\"\"Get `seeds_to_generate` child seeds from `seed` and extra entropy.\"\"\"\n        if not is_greedy:\n            if seed is None:\n                randint_fn = random.randint\n            else:\n                generator = random.Random(str((seed, ) + extra_entropy))\n                randint_fn = generator.randint\n            lo, hi = torch.iinfo(torch.long).min, torch.iinfo(torch.long).max\n            # If the user/random sets seed = 0 but request should\n            # have sampling, we need to change it to something\n            # else. We use a constant in that case.\n            # This way we don't need to create and load a bool\n            # matrix in the sampling kernel, which reduces CPU\n            # overhead and latency.\n            seq_seeds = [\n                randint_fn(lo, hi) or _SEED_0_REPLACEMENT\n                for _ in range(seeds_to_generate)\n            ]\n        else:\n            # For the kernel, seed == 0 means greedy decoding.\n            seq_seeds = [0] * seeds_to_generate\n        return seq_seeds\n",
      "diff": "diff --git a/vllm/model_executor/sampling_metadata.py b/vllm/model_executor/sampling_metadata.py\nindex 015e85b4c..94b4b1441 100644\n--- a/vllm/model_executor/sampling_metadata.py\n+++ b/vllm/model_executor/sampling_metadata.py\n@@ -8,8 +8,9 @@ import torch\n from vllm.sampling_params import SamplingParams, SamplingType\n from vllm.sequence import SequenceData, SequenceGroupMetadata\n from vllm.triton_utils.sample import get_num_triton_sampler_splits\n-from vllm.utils import (async_tensor_h2d, is_pin_memory_available,\n-                        make_tensor_with_pad, maybe_expand_dim)\n+from vllm.utils import (PyObjectCache, async_tensor_h2d,\n+                        is_pin_memory_available, make_tensor_with_pad,\n+                        maybe_expand_dim)\n \n _SAMPLING_EPS = 1e-5\n _SEED_0_REPLACEMENT = 3403598558\n@@ -62,6 +63,39 @@ class SequenceGroupToSample:\n             assert self.query_len is not None\n \n \n+def gen_seq_group_to_sample_builder(num_seqs: int):\n+    return lambda: SequenceGroupToSample(\n+        seq_ids=[0] * num_seqs,\n+        sampling_params=None,\n+        seq_data=None,  # type: ignore\n+        seq_len=0,\n+        query_len=0,\n+        generator=None,\n+        is_prompt=True,\n+        prompt_logprob_indices=[],\n+        sample_indices=[])\n+\n+\n+class SamplingMetadataCache:\n+    \"\"\"Used to cache SamplingMetadata objects between scheduler iterations\n+    \"\"\"\n+\n+    def __init__(self):\n+        self._seq_group_to_sample_cache: Dict[int, PyObjectCache] = {}\n+\n+    def get_cached_seq_group_to_sample(self, num_seqs):\n+        if num_seqs not in self._seq_group_to_sample_cache:\n+            self._seq_group_to_sample_cache[num_seqs] = PyObjectCache(\n+                gen_seq_group_to_sample_builder(num_seqs))\n+\n+        obj = self._seq_group_to_sample_cache[num_seqs].get_object()\n+        return obj\n+\n+    def reset(self):\n+        for cache in self._seq_group_to_sample_cache.values():\n+            cache.reset()\n+\n+\n class SamplingMetadata:\n     \"\"\"Metadata for input sequences. Used in sampler.\n \n@@ -121,6 +155,7 @@ class SamplingMetadata:\n         device: str,\n         pin_memory: bool,\n         generators: Optional[Dict[str, torch.Generator]] = None,\n+        cache: Optional[SamplingMetadataCache] = None,\n     ) -> \"SamplingMetadata\":\n         (\n             seq_groups,\n@@ -128,7 +163,7 @@ class SamplingMetadata:\n             categorized_sample_indices,\n             num_prompts,\n         ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\n-                                device, generators)\n+                                device, generators, cache)\n         selected_token_indices = async_tensor_h2d(selected_token_indices,\n                                                   dtype=torch.long,\n                                                   target_device=device,\n@@ -164,6 +199,7 @@ def _prepare_seq_groups(\n     query_lens: Optional[List[int]],\n     device: str,\n     generators: Optional[Dict[str, torch.Generator]] = None,\n+    cache: Optional[SamplingMetadataCache] = None,\n ) -> Tuple[List[SequenceGroupToSample], List[int], Dict[\n         SamplingType, List[Tuple[int, int]]], int]:\n     \"\"\"Prepare sequence groups and indices for sampling.\n@@ -210,15 +246,27 @@ def _prepare_seq_groups(\n     num_prompts = 0\n \n     for i, seq_group_metadata in enumerate(seq_group_metadata_list):\n-        seq_ids = list(seq_group_metadata.seq_data.keys())\n+        seq_ids = seq_group_metadata.seq_data.keys()\n+\n+        if cache is not None:\n+            sample_obj = cache.get_cached_seq_group_to_sample(len(seq_ids))\n+\n+            for j, seq_id in enumerate(seq_ids):\n+                sample_obj.seq_ids[j] = seq_id\n+\n+            sample_obj.prompt_logprob_indices.clear()\n+            sample_obj.sample_indices.clear()\n+\n         sampling_params = seq_group_metadata.sampling_params\n         is_prompt = seq_group_metadata.is_prompt\n         generator: Optional[torch.Generator] = None\n         # If the current seq group is in decode stage, it is None.\n         seq_len: Optional[int] = None\n         query_len: Optional[int] = None\n-        prompt_logprob_indices: List[int] = []\n-        sample_indices: List[int] = []\n+        prompt_logprob_indices: List[int] = \\\n+            sample_obj.prompt_logprob_indices if cache is not None else []\n+        sample_indices: List[int] = \\\n+            sample_obj.sample_indices if cache is not None else []\n         do_sample = seq_group_metadata.do_sample\n \n         if seq_group_metadata.is_prompt:\n@@ -290,9 +338,16 @@ def _prepare_seq_groups(\n             logit_idx += sample_len\n             sample_idx += sample_len\n \n-        seq_groups.append(\n-            SequenceGroupToSample(\n-                seq_ids=seq_ids,\n+        if cache is not None:\n+            sample_obj.sampling_params = sampling_params\n+            sample_obj.seq_data = seq_group_metadata.seq_data\n+            sample_obj.seq_len = seq_len\n+            sample_obj.query_len = query_len\n+            sample_obj.generator = generator\n+            sample_obj.is_prompt = is_prompt\n+        else:\n+            sample_obj = SequenceGroupToSample(\n+                seq_ids=list(seq_ids),\n                 sampling_params=sampling_params,\n                 seq_data=seq_group_metadata.seq_data,\n                 seq_len=seq_len,\n@@ -300,7 +355,13 @@ def _prepare_seq_groups(\n                 generator=generator,\n                 is_prompt=is_prompt,\n                 prompt_logprob_indices=list(prompt_logprob_indices),\n-                sample_indices=list(sample_indices)))\n+                sample_indices=list(sample_indices))\n+\n+        seq_groups.append(sample_obj)\n+\n+    if cache is not None:\n+        cache.reset()\n+\n     return (seq_groups, selected_token_indices, categorized_sample_indices,\n             num_prompts)",
      "change_type": "modified",
      "lines_added": 72,
      "lines_removed": 11
    },
    {
      "file_path": "vllm/outputs.py",
      "old_content": "import time\nfrom dataclasses import dataclass\nfrom typing import List, Optional, Tuple, Union\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.sequence import (PromptLogprobs, RequestMetrics, SampleLogprobs,\n                           SequenceGroup, SequenceStatus)\n\n\n@dataclass\nclass CompletionOutput:\n    \"\"\"The output data of one completion output of a request.\n\n    Args:\n        index: The index of the output in the request.\n        text: The generated output text.\n        token_ids: The token IDs of the generated output text.\n        cumulative_logprob: The cumulative log probability of the generated\n            output text.\n        logprobs: The log probabilities of the top probability words at each\n            position if the logprobs are requested.\n        finish_reason: The reason why the sequence is finished.\n        stop_reason: The stop string or token id that caused the completion\n            to stop, None if the completion finished for some other reason\n            including encountering the EOS token.\n        lora_request: The LoRA request that was used to generate the output.\n    \"\"\"\n\n    index: int\n    text: str\n    token_ids: Tuple[int, ...]\n    cumulative_logprob: Optional[float]\n    logprobs: Optional[SampleLogprobs]\n    finish_reason: Optional[str] = None\n    stop_reason: Union[int, str, None] = None\n    lora_request: Optional[LoRARequest] = None\n\n    def finished(self) -> bool:\n        return self.finish_reason is not None\n\n    def __repr__(self) -> str:\n        return (f\"CompletionOutput(index={self.index}, \"\n                f\"text={self.text!r}, \"\n                f\"token_ids={self.token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob}, \"\n                f\"logprobs={self.logprobs}, \"\n                f\"finish_reason={self.finish_reason}, \"\n                f\"stop_reason={self.stop_reason})\")\n\n\n@dataclass\nclass EmbeddingOutput:\n    \"\"\"The output data of one completion output of a request.\n\n    Args:\n        embedding: The embedding vector, which is a list of floats. The\n        length of vector depends on the model as listed in the embedding guide.\n    \"\"\"\n\n    embedding: List[float]\n\n    def __repr__(self) -> str:\n        return (f\"EmbeddingOutput(\"\n                f\"embedding={len(self.embedding)})\")\n\n\nclass RequestOutput:\n    \"\"\"The output data of a completion request to the LLM.\n\n    Args:\n        request_id: The unique ID of the request.\n        prompt: The prompt string of the request.\n                For encoder/decoder models, this is the\n                decoder input prompt.\n        prompt_token_ids: The token IDs of the prompt.\n                          For encoder/decoder models, this is the\n                          decoder input prompt token ids.\n        prompt_logprobs: The log probabilities to return per prompt token.\n        outputs: The output sequences of the request.\n        finished: Whether the whole request is finished.\n        metrics: Metrics associated with the request.\n        lora_request: The LoRA request that was used to generate the output.\n        encoder_prompt: The encoder prompt string of the request; \n                        None if decoder-only\n        encoder_prompt_token_ids: The token IDs of the encoder prompt;\n                                  None if decoder-only\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        prompt_token_ids: List[int],\n        prompt_logprobs: Optional[PromptLogprobs],\n        outputs: List[CompletionOutput],\n        finished: bool,\n        metrics: Optional[RequestMetrics] = None,\n        lora_request: Optional[LoRARequest] = None,\n        encoder_prompt: Optional[str] = None,\n        encoder_prompt_token_ids: Optional[List[int]] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.prompt_logprobs = prompt_logprobs\n        self.outputs = outputs\n        self.finished = finished\n        self.metrics = metrics\n        self.lora_request = lora_request\n        self.encoder_prompt = encoder_prompt\n        self.encoder_prompt_token_ids = encoder_prompt_token_ids\n\n    @classmethod\n    def from_seq_group(cls, seq_group: SequenceGroup) -> \"RequestOutput\":\n        if seq_group.sampling_params is None:\n            raise ValueError(\n                \"Sampling parameters are missing for a CompletionRequest.\")\n        seqs = seq_group.get_seqs()\n        if len(seqs) == 1:\n            top_n_seqs = seqs\n        else:\n            # Get the top-n sequences.\n            n = seq_group.sampling_params.n\n            if seq_group.sampling_params.use_beam_search:\n                sorting_key = lambda seq: seq.get_beam_search_score(\n                    seq_group.sampling_params.length_penalty)\n            else:\n                sorting_key = lambda seq: seq.get_cumulative_logprob()\n            sorted_seqs = sorted(seqs, key=sorting_key, reverse=True)\n            top_n_seqs = sorted_seqs[:n]\n\n        # Create the outputs.\n        # NOTE: We need omit logprobs here explicitly because the sequence\n        # always has the logprobs of the sampled tokens even if the\n        # logprobs are not requested.\n        include_logprobs = seq_group.sampling_params.logprobs is not None\n        text_buffer_length = seq_group.sampling_params.output_text_buffer_length\n        outputs = [\n            CompletionOutput(\n                seqs.index(seq),\n                seq.get_output_text_to_return(text_buffer_length),\n                seq.get_output_token_ids(),\n                seq.get_cumulative_logprob() if include_logprobs else None,\n                seq.output_logprobs if include_logprobs else None,\n                SequenceStatus.get_finished_reason(seq.status),\n                seq.stop_reason) for seq in top_n_seqs\n        ]\n\n        # Every sequence in the sequence group should have the same prompt.\n        prompt = seq_group.prompt\n        prompt_token_ids = seq_group.prompt_token_ids\n        encoder_prompt = seq_group.encoder_prompt\n        encoder_prompt_token_ids = seq_group.encoder_prompt_token_ids\n        prompt_logprobs = seq_group.prompt_logprobs\n        finished = seq_group.is_finished()\n        finished_time = time.time() if finished else None\n        seq_group.set_finished_time(finished_time)\n        return cls(seq_group.request_id,\n                   prompt,\n                   prompt_token_ids,\n                   prompt_logprobs,\n                   outputs,\n                   finished,\n                   seq_group.metrics,\n                   lora_request=seq_group.lora_request,\n                   encoder_prompt=encoder_prompt,\n                   encoder_prompt_token_ids=encoder_prompt_token_ids)\n\n    def __repr__(self) -> str:\n        return (f\"RequestOutput(request_id={self.request_id}, \"\n                f\"prompt={self.prompt!r}, \"\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\n                f\"encoder_prompt={self.encoder_prompt!r}, \"\n                f\"encoder_prompt_token_ids={self.encoder_prompt_token_ids}, \"\n                f\"prompt_logprobs={self.prompt_logprobs}, \"\n                f\"outputs={self.outputs}, \"\n                f\"finished={self.finished}, \"\n                f\"metrics={self.metrics}, \"\n                f\"lora_request={self.lora_request})\")\n\n\nclass EmbeddingRequestOutput:\n    \"\"\"\n    The output data of an embedding request to the LLM.\n\n    Args:\n        request_id (str): A unique identifier for the embedding request.\n        outputs (EmbeddingOutput): The embedding results for the given input.\n        prompt_token_ids (List[int]): A list of token IDs used in the prompt.\n        finished (bool): A flag indicating whether the embedding is completed.\n    \"\"\"\n\n    def __init__(self, request_id: str, outputs: \"EmbeddingOutput\",\n                 prompt_token_ids: List[int], finished: bool):\n        self.request_id = request_id\n        self.prompt_token_ids = prompt_token_ids\n        self.finished = finished\n        self.outputs = outputs\n\n    @classmethod\n    def from_seq_group(cls,\n                       seq_group: 'SequenceGroup') -> \"EmbeddingRequestOutput\":\n        if seq_group.embeddings is None:\n            raise ValueError(\n                \"Embeddings are missing in seq_group for EmbeddingRequest.\")\n        output = EmbeddingOutput(seq_group.embeddings)\n        prompt_token_ids = seq_group.prompt_token_ids\n        finished = seq_group.is_finished()\n\n        return cls(seq_group.request_id, output, prompt_token_ids, finished)\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of an EmbeddingRequestOutput instance.\n\n        The representation includes the request_id and the number of outputs,\n        providing a quick overview of the embedding request's results.\n\n        Returns:\n            str: A string representation of the EmbeddingRequestOutput instance.\n        \"\"\"\n        return (f\"EmbeddingRequestOutput(request_id='{self.request_id}', \"\n                f\"outputs={repr(self.outputs)}, \"\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\n                f\"finished={self.finished})\")\n\n\nclass RequestOutputFactory:\n\n    @staticmethod\n    def create(seq_group):\n        # Determine the type based on a condition, for example:\n        if hasattr(seq_group,\n                   'embeddings') and seq_group.embeddings is not None:\n            return EmbeddingRequestOutput.from_seq_group(seq_group)\n        else:\n            return RequestOutput.from_seq_group(seq_group)\n",
      "diff": "diff --git a/vllm/outputs.py b/vllm/outputs.py\nindex 040f77081..6e11ff841 100644\n--- a/vllm/outputs.py\n+++ b/vllm/outputs.py\n@@ -139,7 +139,7 @@ class RequestOutput:\n             CompletionOutput(\n                 seqs.index(seq),\n                 seq.get_output_text_to_return(text_buffer_length),\n-                seq.get_output_token_ids(),\n+                seq.data._output_token_ids,  # type: ignore\n                 seq.get_cumulative_logprob() if include_logprobs else None,\n                 seq.output_logprobs if include_logprobs else None,\n                 SequenceStatus.get_finished_reason(seq.status),",
      "change_type": "modified",
      "lines_added": 2,
      "lines_removed": 2
    },
    {
      "file_path": "vllm/sequence.py",
      "old_content": "\"\"\"Sequence and its related classes.\"\"\"\nimport copy\nimport enum\nimport math\nfrom abc import ABC, abstractmethod\nfrom array import array\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import (TYPE_CHECKING, Dict, List, Mapping, Optional, Set, Tuple,\n                    Union, cast)\n\nimport torch\n\nfrom vllm.inputs.parse import is_valid_encoder_decoder_llm_inputs\nfrom vllm.lora.request import LoRARequest\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\n\nif TYPE_CHECKING:\n    from vllm.inputs import LLMInputs\n    from vllm.multimodal import MultiModalDataDict\n    from vllm.spec_decode.metrics import SpecDecodeWorkerMetrics\n\n\n@dataclass\nclass Logprob:\n    \"\"\"Infos for supporting OpenAI compatible logprobs and token ranks.\n\n    Attributes:\n        logprob: The logprob of chosen token\n        rank: The vocab rank of chosen token (>=1)\n        decoded_token: The decoded chosen token index\n    \"\"\"\n    logprob: float\n    rank: Optional[int] = None\n    decoded_token: Optional[str] = None\n\n\n# {token_id -> logprob} per each sequence group. None if the corresponding\n# sequence group doesn't require prompt logprob.\nPromptLogprobs = List[Optional[Dict[int, Logprob]]]\n# {token_id -> logprob} for each sequence group.\nSampleLogprobs = List[Dict[int, Logprob]]\n\n\nclass SequenceStatus(enum.IntEnum):\n    \"\"\"Status of a sequence.\"\"\"\n    WAITING = 0\n    RUNNING = 1\n    SWAPPED = 2\n    # Note: anything after SWAPPED (2) will be considered\n    # as a finished status.\n    FINISHED_STOPPED = 3\n    FINISHED_LENGTH_CAPPED = 4\n    FINISHED_ABORTED = 5\n    FINISHED_IGNORED = 6\n\n    @staticmethod\n    def is_finished(status: \"SequenceStatus\") -> bool:\n        return status > SequenceStatus.SWAPPED\n\n    @staticmethod\n    def get_finished_reason(status: \"SequenceStatus\") -> Union[str, None]:\n        if status == SequenceStatus.FINISHED_STOPPED:\n            finish_reason = \"stop\"\n        elif status == SequenceStatus.FINISHED_LENGTH_CAPPED:\n            finish_reason = \"length\"\n        elif status == SequenceStatus.FINISHED_ABORTED:\n            finish_reason = \"abort\"\n        elif status == SequenceStatus.FINISHED_IGNORED:\n            # The ignored sequences are the sequences whose prompt lengths\n            # are longer than the model's length cap. Therefore, the stop\n            # reason should also be \"length\" as in OpenAI API.\n            finish_reason = \"length\"\n        else:\n            finish_reason = None\n        return finish_reason\n\n\nclass SequenceStage(enum.Enum):\n    PREFILL = enum.auto()\n    DECODE = enum.auto()\n\n\n@dataclass\nclass RequestMetrics:\n    \"\"\"Metrics associated with a request.\n\n    Attributes:\n        arrival_time: The time when the request arrived.\n        first_scheduled_time: The time when the request was first scheduled.\n        first_token_time: The time when the first token was generated.\n        time_in_queue: The time the request spent in the queue.\n        finished_time: The time when the request was finished.\n    \"\"\"\n    arrival_time: float\n    last_token_time: float\n    first_scheduled_time: Optional[float]\n    first_token_time: Optional[float]\n    time_in_queue: Optional[float]\n    finished_time: Optional[float] = None\n\n\nclass SequenceData:\n    \"\"\"Data associated with a sequence.\n\n    Args:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output. Set to an empty list if\n            None.\n\n    Attributes:\n        prompt_token_ids: The token IDs of the prompt.\n        output_token_ids: The token IDs of the output.\n        cumulative_logprob: The cumulative log probability of the output.\n    \"\"\"\n\n    def __init__(\n        self,\n        prompt_token_ids: List[int],\n        output_token_ids: Optional[List[int]] = None,\n    ) -> None:\n        self._prompt_token_ids = array('l', prompt_token_ids)\n        self._prompt_token_ids_tuple: Tuple[int, ...] = tuple(prompt_token_ids)\n        self._output_token_ids = array(\n            'l', output_token_ids if output_token_ids is not None else [])\n\n        self.cumulative_logprob = 0.0\n        # The number of tokens that are computed (that run against the model).\n        self._num_computed_tokens = 0\n        self._stage: SequenceStage = SequenceStage.PREFILL\n\n        self._update_cached_all_tokens()\n\n    def _update_cached_all_tokens(self):\n        self._cached_all_token_ids: List[int] = list(self._prompt_token_ids +\n                                                     self._output_token_ids)\n\n    @property\n    def prompt_token_ids(self) -> Tuple[int, ...]:\n        return self._prompt_token_ids_tuple\n\n    @prompt_token_ids.setter\n    def prompt_token_ids(self, new_prompt_token_ids) -> None:\n        self._prompt_token_ids = array('l', new_prompt_token_ids)\n        self._prompt_token_ids_tuple = tuple(new_prompt_token_ids)\n        self._update_cached_all_tokens()\n\n    @property\n    def prompt_token_ids_array(self) -> array:\n        return self._prompt_token_ids\n\n    @property\n    def output_token_ids(self) -> Tuple[int, ...]:\n        return tuple(self._output_token_ids)\n\n    @output_token_ids.setter\n    def output_token_ids(self, new_output_token_ids) -> None:\n        self._output_token_ids = array('l', new_output_token_ids)\n        self._update_cached_all_tokens()\n\n    @property\n    def output_token_ids_array(self) -> array:\n        return self._output_token_ids\n\n    def append_token_id(self, token_id: int, logprob: float) -> None:\n        self._output_token_ids.append(token_id)\n        self._cached_all_token_ids.append(token_id)\n        self.cumulative_logprob += logprob\n\n    def get_len(self) -> int:\n        return len(self._output_token_ids) + len(self._prompt_token_ids)\n\n    def get_prompt_len(self) -> int:\n        return len(self._prompt_token_ids)\n\n    def get_output_len(self) -> int:\n        return len(self._output_token_ids)\n\n    def get_token_ids(self) -> List[int]:\n        return self._cached_all_token_ids\n\n    def get_prefix_token_ids(\n            self, num_tokens: int\n    ) -> Tuple[Tuple[int, ...], Optional[Tuple[int, ...]]]:\n        \"\"\"Get prefix tokens, and make the return value hashable\"\"\"\n        prompt_length = self.get_prompt_len()\n        if num_tokens > prompt_length:\n            return (self._prompt_token_ids_tuple,\n                    tuple(self._output_token_ids[:num_tokens - prompt_length]))\n        else:\n            return (self._prompt_token_ids_tuple[:num_tokens], None)\n\n    def get_num_computed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are already computed.\"\"\"\n        return self._num_computed_tokens\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        self._num_computed_tokens += num_new_computed_tokens\n        assert self._num_computed_tokens <= self.get_len(), (\n            self._num_computed_tokens, self.get_len())\n        # If all tokens are computed, it means it is in decoding phase.\n        if self.get_num_uncomputed_tokens() == 0:\n            self._stage = SequenceStage.DECODE\n\n    def reset_state_for_recompute(self) -> None:\n        \"\"\"Reset the number of computed tokens from this sequence. It is\n        supposed to be called when a sequence needs to be started from\n        the beginning again (e.g., sequence is preempted).\n        \"\"\"\n        self._num_computed_tokens = 0\n        self._stage = SequenceStage.PREFILL\n\n    def get_num_uncomputed_tokens(self) -> int:\n        \"\"\"Return the number of prefill tokens that are not computed.\"\"\"\n        # we use `get_len()` which includes prompt_len + output_len instead\n        # of prompt_len here. This is because during recompute we need to\n        # prefill for both prompt and output.\n        return self.get_len() - self.get_num_computed_tokens()\n\n    def get_last_token_id(self) -> int:\n        if not self._output_token_ids:\n            return self._prompt_token_ids[-1]\n        return self._output_token_ids[-1]\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.prompt_token_ids\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.output_token_ids\n\n    @property\n    def stage(self) -> SequenceStage:\n        return self._stage\n\n    def __repr__(self) -> str:\n        return (f\"SequenceData(\"\n                f\"prompt_token_ids={self._prompt_token_ids}, \"\n                f\"output_token_ids={self._output_token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob})\")\n\n\nclass Sequence:\n    \"\"\"Stores the data, status, and block information of a sequence.\n\n    The sequence is constructed from the LLMInputs instance passed\n    in through the `inputs` constructor argument.\n\n    For encoder/decoder models, LLMInputs encapsulates both a\n    decoder and encoder prompt, creating an ambiguity about which\n    prompt to construct the sequence from. The `from_decoder_prompt`\n    constructor argument signals whether to construct the Sequence\n    from the LLMInputs decoder prompt, or encoder prompt.\n\n    Args:\n        seq_id: The ID of the sequence.\n        inputs: The inputs of the sequence.\n        block_size: The block size of the sequence. Should be the same as the\n            block size used by the block manager and cache engine.\n        eos_token_id: The end-of-sequence (EOS) token id recognized by this LLM.\n        lora_request: LoRA request.\n        prompt_adapter_request: Prompt Adapter request.\n        from_decoder_prompt: Construct Sequence from LLMInputs decoder prompt\n                             (True) or encoder prompt (False.) Must be True\n                             for decoder-only model.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        seq_id: int,\n        inputs: \"LLMInputs\",\n        block_size: int,\n        eos_token_id: Optional[int] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        from_decoder_prompt: bool = True,\n    ) -> None:\n        self.seq_id = seq_id\n        self.inputs = inputs\n        self.block_size = block_size\n        self.eos_token_id = eos_token_id\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n        self.from_decoder_prompt = from_decoder_prompt\n        self._prompt: Optional[str] = None\n        self._prompt_token_ids: Optional[List[int]] = None\n\n        # For decoder-only models, a Sequence is constructed\n        # from an LLMInputs instance (the `inputs` arg.)\n        #\n        # For encoder/decoder models the same `inputs`\n        # instance could be utilized to construct either an\n        # encoder sequence or a decoder sequence, because\n        # `LLMInputs` has both decoder- and encoder-oriented\n        # member variables (i.e. it encapsulates both an encoder\n        # and a decoder prompt.) The decision of which type of sequence\n        # to generate is determined by the `from_decoder_prompt` argument.\n        #\n        # When constructing a encoder sequence\n        # (`from_decoder_prompt` False) it matters that\n        # the `LLMInputs` instance stored in `inputs` is valid\n        # in the sense that its encoder-related member variables are\n        # populated; below, an exception is raised if this is\n        # not the case.\n        #\n        # When constructing a decoder sequence (`from_decoder_prompt` True)\n        # it does not matter whether `inputs` has its encoder-related\n        # member variables populated.\n        if not (from_decoder_prompt\n                or is_valid_encoder_decoder_llm_inputs(inputs)):\n            raise ValueError(\"Cannot extract encoder input prompt from \"\n                             f\"invalid input {inputs}; did you forget the \"\n                             \"encoder input prompt fields?\")\n\n        self.data = SequenceData(self.prompt_token_ids)\n        self.output_logprobs: SampleLogprobs = []\n        self.output_text = \"\"\n\n        self.status = SequenceStatus.WAITING\n        self.stop_reason: Union[int, str, None] = None\n\n        # Used for incremental detokenization\n        self.prefix_offset = 0\n        self.read_offset = 0\n        # Input + output tokens\n        self.tokens: Optional[List[str]] = None\n\n    @property\n    def n_blocks(self) -> int:\n        return math.ceil(self.get_len() / self.block_size)\n\n    @property\n    def prompt(self) -> Optional[str]:\n        if self._prompt is not None:\n            # Reuse precomputed prompt string\n            return self._prompt\n\n        # Select decoder or encoder input prompt str,\n        # as appropriate\n        prompt_key: str = (\"prompt\"\n                           if self.from_decoder_prompt else \"encoder_prompt\")\n\n        # Cache prompt\n        self._prompt = cast(Optional[str], self.inputs.get(prompt_key))\n        return self._prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        if self._prompt_token_ids is not None:\n            # Reuse precomputed prompt token ids\n            return self._prompt_token_ids\n\n        # Select decoder or encoder input prompt\n        # token ids, as appropriate\n        prompt_token_ids_key: str = (\"prompt_token_ids\"\n                                     if self.from_decoder_prompt else\n                                     \"encoder_prompt_token_ids\")\n\n        # Cache computed prompt token ids\n        self._prompt_token_ids = cast(List[int],\n                                      self.inputs.get(prompt_token_ids_key))\n        return self._prompt_token_ids\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        return self.inputs.get(\"multi_modal_data\") or {}\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    def get_output_text_to_return(self, buffer_length: int):\n        # We return the full output text if the sequence is finished.\n        truncate = buffer_length and not self.is_finished()\n        return self.output_text[:-buffer_length] if truncate else (\n            self.output_text)\n\n    def hash_of_block(self, logical_idx: int) -> int:\n        # TODO This can produce incorrect hash when block size > prompt size\n\n        # Compute the number of tokens in the sequence\n        # TODO: The current hashing function is O(L^2). We should optimize\n        # this in the future.\n        num_tokens = self.num_hashed_tokens_of_block(logical_idx)\n        hashed_tokens = self.data.get_prefix_token_ids(num_tokens)\n        return hash((hashed_tokens, self.lora_int_id))\n\n    def num_hashed_tokens_of_block(self, logical_idx: int):\n        return logical_idx * self.block_size + self.block_size\n\n    def reset_state_for_recompute(self):\n        \"\"\"Reset the sequence states for recomputation.\"\"\"\n        self.data.reset_state_for_recompute()\n\n    def append_token_id(\n        self,\n        token_id: int,\n        logprobs: Dict[int, Logprob],\n    ) -> None:\n        assert token_id in logprobs\n        self.output_logprobs.append(logprobs)\n        self.data.append_token_id(token_id, logprobs[token_id].logprob)\n\n    def get_len(self) -> int:\n        return self.data.get_len()\n\n    def get_prompt_len(self) -> int:\n        return self.data.get_prompt_len()\n\n    def get_output_len(self) -> int:\n        return self.data.get_output_len()\n\n    def get_token_ids(self) -> List[int]:\n        return self.data.get_token_ids()\n\n    def get_prompt_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_prompt_token_ids()\n\n    def get_last_token_id(self) -> int:\n        return self.data.get_last_token_id()\n\n    def get_output_token_ids(self) -> Tuple[int, ...]:\n        return self.data.get_output_token_ids()\n\n    def get_cumulative_logprob(self) -> float:\n        return self.data.cumulative_logprob\n\n    def get_beam_search_score(self,\n                              length_penalty: float = 1.0,\n                              seq_len: Optional[int] = None,\n                              eos_token_id: Optional[int] = None) -> float:\n        \"\"\"Calculate the beam search score with length penalty.\n\n        Adapted from\n\n        https://github.com/huggingface/transformers/blob/ccb92be23def445f2afdea94c31286f84b89eb5b/src/transformers/generation/beam_search.py#L938\n        \"\"\"\n        if seq_len is None:\n            seq_len = self.get_len()\n            # NOTE: HF implementation does not count the EOS token\n            # towards the length, we align with that here for testing.\n            if (eos_token_id is not None\n                    and self.get_last_token_id() == eos_token_id):\n                seq_len -= 1\n        return self.get_cumulative_logprob() / (seq_len**length_penalty)\n\n    def is_finished(self) -> bool:\n        return SequenceStatus.is_finished(self.status)\n\n    def fork(self, new_seq_id: int) -> \"Sequence\":\n        new_seq = copy.deepcopy(self)\n        new_seq.seq_id = new_seq_id\n        return new_seq\n\n    def get_num_new_tokens(self) -> int:\n        \"\"\"Get the number of new tokens to be computed.\n\n        Returns:\n            The new number of tokens to be computed. I.e., 1 for decode, or\n            the remaining prompt size for prefill.\n        \"\"\"\n        if self.data.stage == SequenceStage.DECODE:\n            return 1\n        return self.data.get_num_uncomputed_tokens()\n\n    def is_prefill(self) -> bool:\n        return self.data.stage == SequenceStage.PREFILL\n\n    def __repr__(self) -> str:\n        return (f\"Sequence(seq_id={self.seq_id}, \"\n                f\"status={self.status.name}, \"\n                f\"num_blocks={self.n_blocks}, \")\n\n\nclass SequenceGroup:\n    \"\"\"A group of sequences that are generated from the same prompt.\n\n    Args:\n        request_id: The ID of the request.\n        seqs: The list of sequences.\n        sampling_params: The sampling parameters used to generate the outputs.\n        arrival_time: The arrival time of the request.\n        lora_request: LoRA request.\n        embeddings: The embeddings vectors of the prompt of the sequence group\n            for an embedding model.\n        pooling_params: The pooling parameters used to generate the pooling\n            for an embedding model.\n        encoder_seq: Optional, the single encoder sequence. Should be None\n                     unless you are working with an encoder/decoder model.\n        trace_headers: OpenTelemetry trace headers.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        seqs: List[Sequence],\n        arrival_time: float,\n        sampling_params: Optional[SamplingParams] = None,\n        lora_request: Optional[LoRARequest] = None,\n        embeddings: Optional[List[float]] = None,\n        pooling_params: Optional[PoolingParams] = None,\n        encoder_seq: Optional[Sequence] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.seqs = seqs\n        self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n        self.sampling_params = sampling_params\n        self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                      last_token_time=arrival_time,\n                                      first_scheduled_time=None,\n                                      first_token_time=None,\n                                      time_in_queue=None)\n        self.lora_request = lora_request\n        self.prompt_logprobs: Optional[PromptLogprobs] = None\n        self.embeddings = embeddings\n        self.pooling_params = pooling_params\n        self.prompt_adapter_request = prompt_adapter_request\n        self.encoder_seq = encoder_seq\n        self.trace_headers = trace_headers\n\n    @property\n    def prompt(self) -> Optional[str]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return self.seqs[0].prompt\n\n    @property\n    def prompt_token_ids(self) -> List[int]:\n        # All sequences in the group should have the same prompt.\n        # We use the prompt of an arbitrary sequence.\n        return self.seqs[0].prompt_token_ids\n\n    @property\n    def encoder_prompt(self) -> Optional[str]:\n        # There are either 0 or 1 encoder sequences\n        # If one is present, its prompt is distinct\n        # from the decoder's.\n        return (self.encoder_seq.prompt\n                if self.encoder_seq is not None else None)\n\n    @property\n    def encoder_prompt_token_ids(self) -> Optional[List[int]]:\n        # There are either 0 or 1 encoder sequences\n        # If one is present, its prompt token ids are\n        # distinct from the decoder's.\n        return (self.encoder_seq.prompt_token_ids\n                if self.encoder_seq is not None else None)\n\n    @property\n    def multi_modal_data(self) -> \"MultiModalDataDict\":\n        # All sequences in the group should have the same multi-modal data.\n        # We use the multi-modal data of an arbitrary sequence.\n        return self.seqs[0].multi_modal_data\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens\\\n                         if self.prompt_adapter_request else 0\n\n    def get_last_latency(self, now: float) -> Optional[float]:\n        \"\"\"Sets the last token time for Request level timings.\"\"\"\n        # If still in prefill phase, raise Error.\n        if self.is_prefill():\n            raise ValueError(\n                \"seq_group.get_last_latency() should not be called \"\n                \"if the seq_group is in prefill phase.\")\n\n        # Otherwise return token latency.\n        latency = now - self.metrics.last_token_time\n        self.metrics.last_token_time = now\n        return latency\n\n    def maybe_set_first_token_time(self, time: float) -> None:\n        \"\"\"Sets the first token time for Request level timings.\"\"\"\n        # Note: in a case where a sequence_group is swapped and\n        #   recomputed, the time between iterations is counted\n        #   in TPOT, rather than recalculating TTFT (since from the )\n        #   POV of the user, there is simply a long generation delay.\n        if (self.metrics.first_token_time is None\n                and self.seqs[0].get_output_len() == 1):\n            self.metrics.first_token_time = time\n\n    def maybe_set_first_scheduled_time(self, time: float) -> None:\n        \"\"\"Sets the first scheduled time and time in queue for Request\n        level timings.\"\"\"\n        if self.metrics.first_scheduled_time is None:\n            self.metrics.first_scheduled_time = time\n            self.metrics.time_in_queue = time - self.metrics.arrival_time\n\n    def set_finished_time(self, time: Optional[float]) -> None:\n        \"\"\"Sets the finished time for Request level timings.\"\"\"\n        self.metrics.finished_time = time\n\n    def get_max_num_running_seqs(self) -> int:\n        \"\"\"The maximum number of sequences running in parallel in the remaining\n        lifetime of the request.\"\"\"\n        if self.sampling_params and self.sampling_params.use_beam_search:\n            # For beam search, maximally there will always be `best_of` beam\n            # candidates running in the future.\n            return self.sampling_params.best_of\n        else:\n            if (self.sampling_params\n                    and self.sampling_params.best_of > self.num_seqs()):\n                # At prompt stage, the sequence group is not yet filled up\n                # and only have one sequence running. However, in the\n                # generation stage, we will have `best_of` sequences running.\n                return self.sampling_params.best_of\n            # At sampling stages, return the number of actual sequences\n            # that are not finished yet.\n            return self.num_unfinished_seqs()\n\n    def get_seqs(\n        self,\n        status: Optional[SequenceStatus] = None,\n    ) -> List[Sequence]:\n        if status is None:\n            return self.seqs\n        return [seq for seq in self.seqs if seq.status == status]\n\n    def is_encoder_decoder(self) -> bool:\n        return self.encoder_seq is not None\n\n    def get_encoder_seq(self) -> Optional[Sequence]:\n        return self.encoder_seq\n\n    def get_unfinished_seqs(self) -> List[Sequence]:\n        return [seq for seq in self.seqs if not seq.is_finished()]\n\n    def get_finished_seqs(self) -> List[Sequence]:\n        return [seq for seq in self.seqs if seq.is_finished()]\n\n    def update_num_computed_tokens(self, num_new_computed_tokens: int):\n        \"\"\"Update number of tokens computed so far.\"\"\"\n        for seq in self.seqs:\n            if not seq.is_finished():\n                seq.data.update_num_computed_tokens(num_new_computed_tokens)\n\n    def get_num_uncomputed_tokens(self) -> int:\n        num_uncomputed_tokens = 0\n        for seq in self.seqs:\n            if not seq.is_finished():\n                num_uncomputed_tokens += seq.data.get_num_uncomputed_tokens()\n        return num_uncomputed_tokens\n\n    def num_seqs(self, status: Optional[SequenceStatus] = None) -> int:\n        # Optimization. We don't need to call get_seqs if we don't need to\n        # filter by states.\n        if status is None:\n            return len(self.seqs)\n\n        return len(self.get_seqs(status))\n\n    def num_unfinished_seqs(self) -> int:\n        return len(self.get_unfinished_seqs())\n\n    def num_finished_seqs(self) -> int:\n        return len(self.get_finished_seqs())\n\n    def find(self, seq_id: int) -> Sequence:\n        if seq_id not in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        return self.seqs_dict[seq_id]\n\n    def add(self, seq: Sequence) -> None:\n        if seq.seq_id in self.seqs_dict:\n            raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n        self.seqs_dict[seq.seq_id] = seq\n        self.seqs.append(seq)\n\n    def remove(self, seq_id: int) -> None:\n        seq = self.seqs_dict.pop(seq_id, None)\n        if seq is None:\n            raise ValueError(f\"Sequence {seq_id} not found.\")\n        self.seqs.remove(seq)\n\n    def is_finished(self) -> bool:\n        return all(seq.is_finished() for seq in self.seqs)\n\n    def is_prefill(self) -> bool:\n        # Every sequence should be in the same stage.\n        return self.seqs[0].is_prefill()\n\n    def __repr__(self) -> str:\n        return (f\"SequenceGroup(request_id={self.request_id}, \"\n                f\"sampling_params={self.sampling_params}, \"\n                f\"num_seqs={len(self.seqs)})\")\n\n\nclass SequenceGroupMetadata:\n    \"\"\"Metadata for a sequence group. Used to create `AttentionMetadata`.\n\n    Args:\n        request_id: The ID of the request.\n        is_prompt: Whether the request is at prompt stage.\n        seq_data: The sequence data. (Seq id -> sequence data)\n        sampling_params: The sampling parameters used to generate the outputs.\n        block_tables: The block tables. (Seq id -> list of physical block\n            numbers)\n        do_sample: True if sampling is required. Sampling is not required when\n            e.g., prefill is chunked, and the current iteration only computes\n            query tokens for prefill, we don't need sampling.\n        token_chunk_size: The number of tokens to be processed (per sequence).\n            None if chunking is not required.\n        lora_request: LoRA request.\n        computed_block_nums: The block numbers that are already computed,\n            used in prefix caching.\n        multi_modal_data: Multi modal data.\n        encoder_seq_data: Optional sequence data for encoder prompt\n                          (SequenceGroup.encoder_seq). Should be None \n                          unless you are working with an encoder/decoder\n                          model.\n        cross_block_table: Optional cross-attention block table associated\n                           with the encoder prompt\n                           (SequenceGroup.encoder_seq). Should be None\n                           unless you are working with an encoder/decoder\n                           model.\n        prompt_adapter_request: Prompt Adapter request.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        is_prompt: bool,\n        seq_data: Dict[int, SequenceData],\n        sampling_params: SamplingParams,\n        block_tables: Dict[int, List[int]],\n        do_sample: bool = True,\n        pooling_params: Optional[PoolingParams] = None,\n        token_chunk_size: Optional[int] = None,\n        lora_request: Optional[LoRARequest] = None,\n        computed_block_nums: Optional[List[int]] = None,\n        multi_modal_data: Optional[\"MultiModalDataDict\"] = None,\n        encoder_seq_data: Optional[SequenceData] = None,\n        cross_block_table: Optional[List[int]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.is_prompt = is_prompt\n        self.seq_data = seq_data\n        self.sampling_params = sampling_params\n        self.block_tables = block_tables\n        self.pooling_params = pooling_params\n        self.lora_request = lora_request\n        self.prompt_adapter_request = prompt_adapter_request\n        self.computed_block_nums = computed_block_nums\n        self.multi_modal_data = multi_modal_data\n        self.encoder_seq_data = encoder_seq_data\n        self.cross_block_table = cross_block_table\n        self._token_chunk_size = token_chunk_size\n        self.do_sample = do_sample\n\n        # The number of speculative tokens adopted in this request.\n        # None means specuative decoding is not used.\n        # Zero means speculative decoding is disabled for some reasons.\n        # TODO: We should maintain this states out of the sequence group.\n        self.num_speculative_tokens = None\n\n        if self._token_chunk_size is None:\n            if is_prompt:\n                self._token_chunk_size = list(seq_data.values())[0].get_len()\n            else:\n                self._token_chunk_size = 1\n\n    @property\n    def lora_int_id(self) -> int:\n        return self.lora_request.lora_int_id if self.lora_request else 0\n\n    @property\n    def prompt_adapter_id(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_id \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def prompt_adapter_num_virtual_tokens(self) -> int:\n        return self.prompt_adapter_request.prompt_adapter_num_virtual_tokens \\\n                        if self.prompt_adapter_request else 0\n\n    @property\n    def token_chunk_size(self) -> int:\n        \"\"\"Return the number of tokens to be processed (chunk size).\"\"\"\n        assert self._token_chunk_size is not None\n        return self._token_chunk_size\n\n\nclass SequenceOutput:\n    \"\"\"The model output associated with a sequence.\n\n    Args:\n        parent_seq_id: The ID of the parent sequence (for forking in beam\n            search).\n        output_token: The output token ID.\n        logprobs: The logprobs of the output token.\n            (Token id -> logP(x_i+1 | x_0, ..., x_i))\n    \"\"\"\n\n    def __init__(\n        self,\n        parent_seq_id: int,\n        output_token: int,\n        logprobs: Dict[int, Logprob],\n    ) -> None:\n        self.parent_seq_id = parent_seq_id\n        self.output_token = output_token\n        self.logprobs = logprobs\n\n    def __repr__(self) -> str:\n        return (f\"SequenceOutput(parent_seq_id={self.parent_seq_id}, \"\n                f\"output_token={self.output_token}, \"\n                f\"logprobs={self.logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, SequenceOutput):\n            raise NotImplementedError()\n        equal = (self.parent_seq_id == other.parent_seq_id\n                 and self.output_token == other.output_token)\n        log_probs_equal = other.logprobs == self.logprobs\n        return equal and log_probs_equal\n\n\nclass SequenceGroupOutput(ABC):\n    \"\"\"The base class for model outputs associated with a sequence group.\"\"\"\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        pass\n\n    @abstractmethod\n    def __eq__(self, other: object) -> bool:\n        pass\n\n\nclass CompletionSequenceGroupOutput(SequenceGroupOutput):\n    \"\"\"The model output associated with a completion sequence group.\"\"\"\n\n    def __init__(\n        self,\n        samples: List[SequenceOutput],\n        prompt_logprobs: Optional[PromptLogprobs],\n    ) -> None:\n        self.samples = samples\n        # Prompt logprob for each prompt query token.\n        self.prompt_logprobs = prompt_logprobs\n\n    def __repr__(self) -> str:\n        return (f\"CompletionSequenceGroupOutput(samples={self.samples}, \"\n                f\"prompt_logprobs={self.prompt_logprobs})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, CompletionSequenceGroupOutput):\n            raise NotImplementedError()\n        return (self.samples == other.samples\n                and self.prompt_logprobs == other.prompt_logprobs)\n\n\nclass EmbeddingSequenceGroupOutput(SequenceGroupOutput):\n    \"\"\"The model output associated with an embedding sequence group.\"\"\"\n\n    def __init__(\n        self,\n        embeddings: List[float],\n    ) -> None:\n        self.embeddings = embeddings\n\n    def __repr__(self) -> str:\n        return (f\"EmbeddingSequenceGroupOutput(\"\n                f\"embeddings_shape={len(self.embeddings)})\")\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, EmbeddingSequenceGroupOutput):\n            raise NotImplementedError()\n        return self.embeddings == other.embeddings\n\n\n@dataclass\nclass IntermediateTensors:\n    \"\"\"For all pipeline stages except the last, we need to return the hidden\n    states and residuals to be sent to the next stage. This data structure\n    contains the hidden states and residuals for a request.\n    \"\"\"\n\n    tensors: Dict[str, torch.Tensor]\n\n    def __getitem__(self, key: Union[str, slice]):\n        if isinstance(key, str):\n            return self.tensors[key]\n        elif isinstance(key, slice):\n            return self.__class__({k: v[key] for k, v in self.tensors.items()})\n\n    def __setitem__(self, key: str, value):\n        self.tensors[key] = value\n\n    def __len__(self):\n        return len(self.tensors)\n\n    def __eq__(self, other: object):\n        return isinstance(other, self.__class__) and self\n\n    def __repr__(self) -> str:\n        return f\"IntermediateTensors(tensors={self.tensors})\"\n\n\n@dataclass\nclass SamplerOutput:\n    \"\"\"For each sequence group, we generate a list of SequenceOutput object,\n    each of which contains one possible candidate for the next token.\n\n    This data structure implements methods, so it can be used like a list, but\n    also has optional fields for device tensors.\n    \"\"\"\n\n    outputs: List[CompletionSequenceGroupOutput]\n\n    # On-device tensor containing probabilities of each token.\n    sampled_token_probs: Optional[torch.Tensor] = None\n\n    # On-device tensor containing the logprobs of each token.\n    logprobs: Optional[\"torch.Tensor\"] = None\n\n    # On-device tensor containing the sampled token ids.\n    sampled_token_ids: Optional[torch.Tensor] = None\n\n    # Spec decode metrics populated by workers.\n    spec_decode_worker_metrics: Optional[\"SpecDecodeWorkerMetrics\"] = None\n\n    # Optional last hidden states from the model.\n    hidden_states: Optional[torch.Tensor] = None\n\n    def __getitem__(self, idx: int):\n        return self.outputs[idx]\n\n    def __setitem__(self, idx: int, value):\n        self.outputs[idx] = value\n\n    def __len__(self):\n        return len(self.outputs)\n\n    def __eq__(self, other: object):\n        return isinstance(other,\n                          self.__class__) and self.outputs == other.outputs\n\n    def __repr__(self) -> str:\n        \"\"\"Show the shape of a tensor instead of its values to reduce noise.\n        \"\"\"\n        sampled_token_probs_repr = (\"None\" if self.sampled_token_probs is None\n                                    else self.sampled_token_probs.shape)\n        sampled_token_ids_repr = (\"None\" if self.sampled_token_ids is None else\n                                  self.sampled_token_ids.shape)\n        return (\n            f\"SamplerOutput(outputs={self.outputs}, \"\n            f\"sampled_token_probs={sampled_token_probs_repr}, \"\n            f\"sampled_token_ids={sampled_token_ids_repr}, \"\n            f\"spec_decode_worker_metrics={self.spec_decode_worker_metrics})\")\n\n\n@dataclass\nclass PoolerOutput:\n    \"\"\"The output from a pooling operation in the embedding model.\"\"\"\n    outputs: List[EmbeddingSequenceGroupOutput]\n\n    spec_decode_worker_metrics: Optional[\"SpecDecodeWorkerMetrics\"] = None\n\n    def __getitem__(self, idx: int):\n        return self.outputs[idx]\n\n    def __setitem__(self, idx: int, value):\n        self.outputs[idx] = value\n\n    def __len__(self):\n        return len(self.outputs)\n\n    def __eq__(self, other: object):\n        return isinstance(other,\n                          self.__class__) and self.outputs == other.outputs\n\n\ndef get_all_seq_ids(\n        seq_group_metadata_list: List[SequenceGroupMetadata]) -> List[int]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    return [seq_id for sg in seq_group_metadata_list for seq_id in sg.seq_data]\n\n\ndef get_all_seq_ids_and_request_ids(\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n) -> Tuple[List[int], Dict[str, Set[int]]]:\n    \"\"\"Given a list of SequenceGroupMetadata, create a list of all\n    sequence ids.\n    \"\"\"\n    seq_ids: List[int] = []\n    request_id_seq_ids_mapping: Dict[str, Set[int]] = defaultdict(set)\n    for sg in seq_group_metadata_list:\n        for seq_id in sg.seq_data:\n            seq_ids.append(seq_id)\n            request_id_seq_ids_mapping[sg.request_id].add(seq_id)\n    return seq_ids, request_id_seq_ids_mapping\n\n\nclass HiddenStates:\n    \"\"\"Hidden states corresponding to in-progress sequences.\n    Used in speculative decoding to pass hidden states from\n    the target model to the proposer model in the subsequent step.\n\n    seq_ids are the sequence ids of each entry of the batch\n    dimension of the hidden_states tensor\"\"\"\n\n    def __init__(self, seq_group_metadata_list: List[SequenceGroupMetadata],\n                 hidden_states: torch.Tensor):\n        assert len(seq_group_metadata_list) == len(hidden_states)\n        self.seq_ids: List[int] = get_all_seq_ids(seq_group_metadata_list)\n        self.hidden_states: torch.Tensor = hidden_states\n\n    def update(self, seq_group_metadata_list: List[SequenceGroupMetadata],\n               hidden_states: torch.Tensor) -> None:\n        \"\"\"Update hidden states from target model invocation.\"\"\"\n        assert len(seq_group_metadata_list) == len(hidden_states)\n        self.seq_ids.extend(get_all_seq_ids(seq_group_metadata_list))\n        self.hidden_states = torch.cat([self.hidden_states, hidden_states])\n\n    def prune(self,\n              seq_group_metadata_list: List[SequenceGroupMetadata]) -> None:\n        \"\"\"Prune to provided list of sequence ids.\"\"\"\n        seq_ids = get_all_seq_ids(seq_group_metadata_list)\n        if seq_ids != self.seq_ids:\n            # Batch contents changed - prune removed sequences.\n            index = [self.seq_ids.index(seq_id) for seq_id in seq_ids]\n            self.hidden_states = self.hidden_states[index]\n            self.seq_ids = seq_ids\n\n\n@dataclass\nclass ExecuteModelRequest:\n    \"\"\"The model execution request, containing CPU metadata only. The LLM\n    engine should create an instance of this class for each request batch.\"\"\"\n    # The sequence group metadata list.\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n    # Blocks to swap in. List of CPU -> GPU block number.\n    blocks_to_swap_in: List[Tuple[int, int]] = field(default_factory=list)\n    # Blocks to swap out. List of GPU -> CPU block number.\n    blocks_to_swap_out: List[Tuple[int, int]] = field(default_factory=list)\n    # Blocks to copy. Source to dest block.\n    blocks_to_copy: List[Tuple[int, int]] = field(default_factory=list)\n    # Virtual engine ID for pipeline parallel.\n    virtual_engine: int = 0\n    # The number of slots for lookahead decoding.\n    num_lookahead_slots: int = 0\n    # The number of requests in the running queue.\n    running_queue_size: int = 0\n    # Optional hidden states from prior step.\n    previous_hidden_states: Optional[HiddenStates] = None\n    # The number of forward steps to run.\n    num_steps: int = 1\n    # Finished request ids since last step.\n    finished_requests_ids: List[str] = field(default_factory=list)\n\n    def clone(\n        self, seq_group_metadata_list: List[SequenceGroupMetadata]\n    ) -> \"ExecuteModelRequest\":\n        \"\"\"Clone the request with a new sequence group metadata list.\"\"\"\n        return ExecuteModelRequest(\n            seq_group_metadata_list=seq_group_metadata_list,\n            blocks_to_swap_in=self.blocks_to_swap_in.copy(),\n            blocks_to_swap_out=self.blocks_to_swap_out.copy(),\n            blocks_to_copy=self.blocks_to_copy.copy(),\n            virtual_engine=self.virtual_engine,\n            num_lookahead_slots=self.num_lookahead_slots,\n            running_queue_size=self.running_queue_size,\n            previous_hidden_states=self.previous_hidden_states,\n            num_steps=self.num_steps,\n            finished_requests_ids=self.finished_requests_ids)\n",
      "diff": "diff --git a/vllm/sequence.py b/vllm/sequence.py\nindex fbd148001..ba477efc5 100644\n--- a/vllm/sequence.py\n+++ b/vllm/sequence.py\n@@ -1,7 +1,6 @@\n \"\"\"Sequence and its related classes.\"\"\"\n import copy\n import enum\n-import math\n from abc import ABC, abstractmethod\n from array import array\n from collections import defaultdict\n@@ -330,7 +329,7 @@ class Sequence:\n \n     @property\n     def n_blocks(self) -> int:\n-        return math.ceil(self.get_len() / self.block_size)\n+        return (self.get_len() + self.block_size - 1) // self.block_size\n \n     @property\n     def prompt(self) -> Optional[str]:\n@@ -514,7 +513,9 @@ class SequenceGroup:\n     ) -> None:\n         self.request_id = request_id\n         self.seqs = seqs\n+        self.is_single_seq = len(seqs) == 1\n         self.seqs_dict = {seq.seq_id: seq for seq in seqs}\n+\n         self.sampling_params = sampling_params\n         self.metrics = RequestMetrics(arrival_time=arrival_time,\n                                       last_token_time=arrival_time,\n@@ -635,6 +636,10 @@ class SequenceGroup:\n     ) -> List[Sequence]:\n         if status is None:\n             return self.seqs\n+\n+        if self.is_single_seq:\n+            return self.seqs if self.seqs[0].status == status else []\n+\n         return [seq for seq in self.seqs if seq.status == status]\n \n     def is_encoder_decoder(self) -> bool:\n@@ -644,6 +649,9 @@ class SequenceGroup:\n         return self.encoder_seq\n \n     def get_unfinished_seqs(self) -> List[Sequence]:\n+        if self.is_single_seq:\n+            return self.seqs if not self.seqs[0].is_finished() else []\n+\n         return [seq for seq in self.seqs if not seq.is_finished()]\n \n     def get_finished_seqs(self) -> List[Sequence]:\n@@ -668,12 +676,21 @@ class SequenceGroup:\n         if status is None:\n             return len(self.seqs)\n \n+        if self.is_single_seq:\n+            return 1 if self.seqs[0].status == status else 0\n+\n         return len(self.get_seqs(status))\n \n     def num_unfinished_seqs(self) -> int:\n+        if self.is_single_seq:\n+            return 1 if not self.seqs[0].is_finished() else 0\n+\n         return len(self.get_unfinished_seqs())\n \n     def num_finished_seqs(self) -> int:\n+        if self.is_single_seq:\n+            return 1 if self.seqs[0].is_finished() else 0\n+\n         return len(self.get_finished_seqs())\n \n     def find(self, seq_id: int) -> Sequence:\n@@ -686,12 +703,14 @@ class SequenceGroup:\n             raise ValueError(f\"Sequence {seq.seq_id} already exists.\")\n         self.seqs_dict[seq.seq_id] = seq\n         self.seqs.append(seq)\n+        self.is_single_seq = len(self.seqs) == 1\n \n     def remove(self, seq_id: int) -> None:\n         seq = self.seqs_dict.pop(seq_id, None)\n         if seq is None:\n             raise ValueError(f\"Sequence {seq_id} not found.\")\n         self.seqs.remove(seq)\n+        self.is_single_seq = len(self.seqs) == 1\n \n     def is_finished(self) -> bool:\n         return all(seq.is_finished() for seq in self.seqs)\n@@ -775,9 +794,10 @@ class SequenceGroupMetadata:\n         # TODO: We should maintain this states out of the sequence group.\n         self.num_speculative_tokens = None\n \n-        if self._token_chunk_size is None:\n+        if seq_data is not None and self._token_chunk_size is None:\n             if is_prompt:\n-                self._token_chunk_size = list(seq_data.values())[0].get_len()\n+                self._token_chunk_size = next(iter(\n+                    seq_data.values())).get_len()\n             else:\n                 self._token_chunk_size = 1",
      "change_type": "modified",
      "lines_added": 25,
      "lines_removed": 5
    },
    {
      "file_path": "vllm/utils.py",
      "old_content": "import argparse\nimport asyncio\nimport contextlib\nimport datetime\nimport enum\nimport gc\nimport os\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nimport uuid\nimport warnings\nfrom asyncio import FIRST_COMPLETED, ensure_future\nfrom collections import defaultdict\nfrom functools import lru_cache, partial, wraps\nfrom platform import uname\nfrom typing import (Any, AsyncGenerator, Awaitable, Callable, Dict, Generic,\n                    Hashable, List, Literal, Optional, OrderedDict, Set, Tuple,\n                    Type, TypeVar, Union, overload)\nfrom uuid import uuid4\n\nimport numpy as np\nimport numpy.typing as npt\nimport psutil\nimport torch\nimport torch.types\nfrom typing_extensions import ParamSpec, TypeIs, assert_never\n\nimport vllm.envs as envs\nfrom vllm import _custom_ops as ops\nfrom vllm.logger import enable_trace_function_call, init_logger\n\nlogger = init_logger(__name__)\n\n# Exception strings for non-implemented encoder/decoder scenarios\n\nSTR_NOT_IMPL_ENC_DEC_SWA = \\\n    \"Sliding window attention for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_PREFIX_CACHE = \\\n    \"Prefix caching for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL = \\\n    \"Chunked prefill for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP = (\n    \"Models with logits_soft_cap \"\n    \"require FlashInfer backend, which is \"\n    \"currently not supported for encoder/decoder \"\n    \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_LORA = (\"LoRA is currently not currently \"\n                             \"supported with encoder/decoder \"\n                             \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PP = (\"Pipeline parallelism is not \"\n                           \"currently supported with \"\n                           \"encoder/decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_MM = (\"Multimodal is not currently \"\n                           \"supported with encoder/decoder \"\n                           \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_SPEC_DEC = (\"Speculative decoding is not \"\n                                 \"currently supported with encoder/\"\n                                 \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_CUDAGRAPH = (\"CUDAGraph is not \"\n                                  \"currently supported with encoder/\"\n                                  \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_BACKEND = (\"XFormers is the only backend \"\n                                \"currently supported with encoder/\"\n                                \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER = (\"Prompt adapters are not \"\n                                       \"currently supported with encoder/\"\n                                       \"decoder models.\")\n\n# Efficiently import all enc/dec error strings\n# rather than having to import all of the above\nSTR_NOT_IMPL_ENC_DEC_ERR_STRS = {\n    \"STR_NOT_IMPL_ENC_DEC_SWA\": STR_NOT_IMPL_ENC_DEC_SWA,\n    \"STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\": STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n    \"STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL\":\n    STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL,\n    \"STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP\": STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP,\n    \"STR_NOT_IMPL_ENC_DEC_LORA\": STR_NOT_IMPL_ENC_DEC_LORA,\n    \"STR_NOT_IMPL_ENC_DEC_PP\": STR_NOT_IMPL_ENC_DEC_PP,\n    \"STR_NOT_IMPL_ENC_DEC_MM\": STR_NOT_IMPL_ENC_DEC_MM,\n    \"STR_NOT_IMPL_ENC_DEC_SPEC_DEC\": STR_NOT_IMPL_ENC_DEC_SPEC_DEC,\n    \"STR_NOT_IMPL_ENC_DEC_CUDA_GRAPH\": STR_NOT_IMPL_ENC_DEC_CUDAGRAPH,\n    \"STR_NOT_IMPL_ENC_DEC_BACKEND\": STR_NOT_IMPL_ENC_DEC_BACKEND,\n    \"STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER\": STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER,\n}\n\n# Constants related to forcing the attention backend selection\n\n# String name of register which may be set in order to\n# force auto-selection of attention backend by Attention\n# wrapper\nSTR_BACKEND_ENV_VAR: str = \"VLLM_ATTENTION_BACKEND\"\n\n# Possible string values of STR_BACKEND_ENV_VAR\n# register, corresponding to possible backends\nSTR_FLASHINFER_ATTN_VAL: str = \"FLASHINFER\"\nSTR_TORCH_SDPA_ATTN_VAL: str = \"TORCH_SDPA\"\nSTR_ROCM_FLASH_ATTN_VAL: str = \"ROCM_FLASH\"\nSTR_XFORMERS_ATTN_VAL: str = \"XFORMERS\"\nSTR_FLASH_ATTN_VAL: str = \"FLASH_ATTN\"\nSTR_INVALID_VAL: str = \"INVALID\"\n\nSTR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.half,\n    \"bfloat16\": torch.bfloat16,\n    \"float\": torch.float,\n    \"fp8\": torch.uint8,\n    \"fp8_e4m3\": torch.uint8,\n    \"fp8_e5m2\": torch.uint8,\n}\n\nTORCH_DTYPE_TO_NUMPY_DTYPE = {\n    torch.float16: np.float16,\n    torch.float32: np.float32,\n    torch.float64: np.float64,\n    torch.uint8: np.uint8,\n    torch.int32: np.int32,\n    torch.int64: np.int64,\n}\n\nP = ParamSpec('P')\nK = TypeVar(\"K\")\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n\nclass _Sentinel:\n    ...\n\n\nALL_PINNED_SENTINEL = _Sentinel()\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\nclass LRUCache(Generic[T]):\n\n    def __init__(self, capacity: int):\n        self.cache: OrderedDict[Hashable, T] = OrderedDict()\n        self.pinned_items: Set[Hashable] = set()\n        self.capacity = capacity\n\n    def __contains__(self, key: Hashable) -> bool:\n        return key in self.cache\n\n    def __len__(self) -> int:\n        return len(self.cache)\n\n    def __getitem__(self, key: Hashable) -> T:\n        value = self.cache[key]  # Raise KeyError if not exists\n        self.cache.move_to_end(key)\n        return value\n\n    def __setitem__(self, key: Hashable, value: T) -> None:\n        self.put(key, value)\n\n    def __delitem__(self, key: Hashable) -> None:\n        self.pop(key)\n\n    def touch(self, key: Hashable) -> None:\n        self.cache.move_to_end(key)\n\n    def get(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        value: Optional[T]\n        if key in self.cache:\n            value = self.cache[key]\n            self.cache.move_to_end(key)\n        else:\n            value = default_value\n        return value\n\n    def put(self, key: Hashable, value: T) -> None:\n        self.cache[key] = value\n        self.cache.move_to_end(key)\n        self._remove_old_if_needed()\n\n    def pin(self, key: Hashable) -> None:\n        \"\"\"\n        Pins a key in the cache preventing it from being\n        evicted in the LRU order.\n        \"\"\"\n        if key not in self.cache:\n            raise ValueError(f\"Cannot pin key: {key} not in cache.\")\n        self.pinned_items.add(key)\n\n    def _unpin(self, key: Hashable) -> None:\n        self.pinned_items.remove(key)\n\n    def _on_remove(self, key: Hashable, value: Optional[T]):\n        pass\n\n    def remove_oldest(self, remove_pinned=False):\n        if not self.cache:\n            return\n\n        if not remove_pinned:\n            # pop the oldest item in the cache that is not pinned\n            lru_key = next(\n                (key for key in self.cache if key not in self.pinned_items),\n                ALL_PINNED_SENTINEL)\n            if lru_key is ALL_PINNED_SENTINEL:\n                raise RuntimeError(\"All items are pinned, \"\n                                   \"cannot remove oldest from the cache.\")\n        else:\n            lru_key = next(iter(self.cache))\n        self.pop(lru_key)\n\n    def _remove_old_if_needed(self) -> None:\n        while len(self.cache) > self.capacity:\n            self.remove_oldest()\n\n    def pop(self,\n            key: Hashable,\n            default_value: Optional[T] = None) -> Optional[T]:\n        run_on_remove = key in self.cache\n        value: Optional[T] = self.cache.pop(key, default_value)\n        # remove from pinned items\n        if key in self.pinned_items:\n            self._unpin(key)\n        if run_on_remove:\n            self._on_remove(key, value)\n        return value\n\n    def clear(self):\n        while len(self.cache) > 0:\n            self.remove_oldest(remove_pinned=True)\n        self.cache.clear()\n\n\ndef is_hip() -> bool:\n    return torch.version.hip is not None\n\n\n@lru_cache(maxsize=None)\ndef is_cpu() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"cpu\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_openvino() -> bool:\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        return \"openvino\" in version(\"vllm\")\n    except PackageNotFoundError:\n        return False\n\n\n@lru_cache(maxsize=None)\ndef is_neuron() -> bool:\n    try:\n        import transformers_neuronx\n    except ImportError:\n        transformers_neuronx = None\n    return transformers_neuronx is not None\n\n\n@lru_cache(maxsize=None)\ndef is_tpu() -> bool:\n    try:\n        import libtpu\n    except ImportError:\n        libtpu = None\n    return libtpu is not None\n\n\n@lru_cache(maxsize=None)\ndef is_xpu() -> bool:\n    from importlib.metadata import version\n    is_xpu_flag = \"xpu\" in version(\"vllm\")\n    # vllm is not build with xpu\n    if not is_xpu_flag:\n        return False\n    try:\n        import intel_extension_for_pytorch as ipex  # noqa: F401\n        _import_ipex = True\n    except ImportError as e:\n        logger.warning(\"Import Error for IPEX: %s\", e.msg)\n        _import_ipex = False\n    # ipex dependency is not ready\n    if not _import_ipex:\n        logger.warning(\"not found ipex lib\")\n        return False\n    return hasattr(torch, \"xpu\") and torch.xpu.is_available()\n\n\n@lru_cache(maxsize=None)\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    max_shared_mem = (\n        ops.get_max_shared_memory_per_block_device_attribute(gpu))\n    # value 0 will cause MAX_SEQ_LEN become negative and test_attention.py\n    # will fail\n    assert max_shared_mem > 0, \"max_shared_mem can not be zero\"\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\n@lru_cache(maxsize=None)\ndef get_vllm_instance_id() -> str:\n    \"\"\"\n    If the environment variable VLLM_INSTANCE_ID is set, return it.\n    Otherwise, return a random UUID.\n    Instance id represents an instance of the VLLM. All processes in the same\n    instance should have the same instance id.\n    \"\"\"\n    return envs.VLLM_INSTANCE_ID or f\"vllm-instance-{random_uuid()}\"\n\n\n@lru_cache(maxsize=None)\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\ndef make_async(func: Callable[P, T]) -> Callable[P, Awaitable[T]]:\n    \"\"\"Take a blocking function, and run it on in an executor thread.\n\n    This function prevents the blocking function from blocking the\n    asyncio event loop.\n    The code in this function needs to be thread safe.\n    \"\"\"\n\n    def _async_wrapper(*args: P.args, **kwargs: P.kwargs) -> asyncio.Future:\n        loop = asyncio.get_event_loop()\n        p_func = partial(func, *args, **kwargs)\n        return loop.run_in_executor(executor=None, func=p_func)\n\n    return _async_wrapper\n\n\nasync def iterate_with_cancellation(\n    iterator: AsyncGenerator[T, None],\n    is_cancelled: Callable[[], Awaitable[bool]],\n) -> AsyncGenerator[T, None]:\n    \"\"\"Convert async iterator into one that polls the provided function\n    at least once per second to check for client cancellation.\n    \"\"\"\n\n    # Can use anext() in python >= 3.10\n    awaits = [ensure_future(iterator.__anext__())]\n    while True:\n        done, pending = await asyncio.wait(awaits, timeout=1)\n        if await is_cancelled():\n            with contextlib.suppress(BaseException):\n                awaits[0].cancel()\n                await iterator.aclose()\n            raise asyncio.CancelledError(\"client cancelled\")\n        if done:\n            try:\n                item = await awaits[0]\n                awaits[0] = ensure_future(iterator.__anext__())\n                yield item\n            except StopAsyncIteration:\n                # we are done\n                return\n\n\nasync def merge_async_iterators(\n    *iterators: AsyncGenerator[T, None],\n    is_cancelled: Optional[Callable[[], Awaitable[bool]]] = None,\n) -> AsyncGenerator[Tuple[int, T], None]:\n    \"\"\"Merge multiple asynchronous iterators into a single iterator.\n\n    This method handle the case where some iterators finish before others.\n    When it yields, it yields a tuple (i, item) where i is the index of the\n    iterator that yields the item.\n\n    It also optionally polls a provided function at least once per second\n    to check for client cancellation.\n    \"\"\"\n\n    # Can use anext() in python >= 3.10\n    awaits = {\n        ensure_future(pair[1].__anext__()): pair\n        for pair in enumerate(iterators)\n    }\n    timeout = None if is_cancelled is None else 1\n    try:\n        while awaits:\n            done, pending = await asyncio.wait(awaits.keys(),\n                                               return_when=FIRST_COMPLETED,\n                                               timeout=timeout)\n            if is_cancelled is not None and await is_cancelled():\n                raise asyncio.CancelledError(\"client cancelled\")\n            for d in done:\n                pair = awaits.pop(d)\n                try:\n                    item = await d\n                    i, it = pair\n                    awaits[ensure_future(it.__anext__())] = pair\n                    yield i, item\n                except StopAsyncIteration:\n                    pass\n    finally:\n        # Cancel any remaining iterators\n        for f, (_, it) in awaits.items():\n            with contextlib.suppress(BaseException):\n                f.cancel()\n                await it.aclose()\n\n\ndef get_ip() -> str:\n    host_ip = envs.VLLM_HOST_IP\n    if host_ip:\n        return host_ip\n\n    # IP is not set, try to get it from the network interface\n\n    # try ipv4\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        s.connect((\"8.8.8.8\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    # try ipv6\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        # Google's public DNS server, see\n        # https://developers.google.com/speed/public-dns/docs/using#addresses\n        s.connect((\"2001:4860:4860::8888\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    warnings.warn(\n        \"Failed to get the IP address, using 0.0.0.0 by default.\"\n        \"The value can be set by the environment variable\"\n        \" VLLM_HOST_IP or HOST_IP.\",\n        stacklevel=2)\n    return \"0.0.0.0\"\n\n\ndef get_distributed_init_method(ip: str, port: int) -> str:\n    # Brackets are not permitted in ipv4 addresses,\n    # see https://github.com/python/cpython/issues/103848\n    return f\"tcp://[{ip}]:{port}\" if \":\" in ip else f\"tcp://{ip}:{port}\"\n\n\ndef get_open_zmq_ipc_path() -> str:\n    base_rpc_path = envs.VLLM_RPC_BASE_PATH\n    return f\"ipc://{base_rpc_path}/{uuid4()}\"\n\n\ndef get_open_port() -> int:\n    port = envs.VLLM_PORT\n    if port is not None:\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\",\n                            port - 1, port)\n    # try ipv4\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n    except OSError:\n        # try ipv6\n        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n\n\ndef update_environment_variables(envs: Dict[str, str]):\n    for k, v in envs.items():\n        if k in os.environ and os.environ[k] != v:\n            logger.warning(\n                \"Overwriting environment variable %s \"\n                \"from '%s' to '%s'\", k, os.environ[k], v)\n        os.environ[k] = v\n\n\ndef chunk_list(lst: List[T], chunk_size: int):\n    \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i:i + chunk_size]\n\n\ndef cdiv(a: int, b: int) -> int:\n    \"\"\"Ceiling division.\"\"\"\n    return -(a // -b)\n\n\ndef _generate_random_fp8(\n    tensor: torch.Tensor,\n    low: float,\n    high: float,\n) -> None:\n    # NOTE(zhaoyang): Due to NaN and Inf representation for fp8 data type,\n    # it may occur Inf or NaN if we directly use torch.randint\n    # to generate random data for fp8 data.\n    # For example, s.11111.00 in fp8e5m2 format represents Inf.\n    #     | E4M3        | E5M2\n    #-----|-------------|-------------------\n    # Inf | N/A         | s.11111.00\n    # NaN | s.1111.111  | s.11111.{01,10,11}\n    from vllm import _custom_ops as ops\n    tensor_tmp = torch.empty_like(tensor, dtype=torch.float16)\n    tensor_tmp.uniform_(low, high)\n    ops.convert_fp8(tensor, tensor_tmp)\n    del tensor_tmp\n\n\ndef get_kv_cache_torch_dtype(\n        cache_dtype: Optional[Union[str, torch.dtype]],\n        model_dtype: Optional[Union[str, torch.dtype]] = None) -> torch.dtype:\n    if isinstance(cache_dtype, str):\n        if cache_dtype == \"auto\":\n            if isinstance(model_dtype, str):\n                torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[model_dtype]\n            elif isinstance(model_dtype, torch.dtype):\n                torch_dtype = model_dtype\n            else:\n                raise ValueError(f\"Invalid model dtype: {model_dtype}\")\n        elif cache_dtype in [\"half\", \"bfloat16\", \"float\"]:\n            torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_dtype]\n        elif cache_dtype == \"fp8\":\n            torch_dtype = torch.uint8\n        else:\n            raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    elif isinstance(cache_dtype, torch.dtype):\n        torch_dtype = cache_dtype\n    else:\n        raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    return torch_dtype\n\n\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: int = 0,\n    device: Optional[str] = \"cuda\",\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n    key_value_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n    scale = head_size**-0.5\n\n    key_caches: List[torch.Tensor] = []\n    value_caches: List[torch.Tensor] = []\n\n    for _ in range(num_layers):\n        key_value_cache = torch.empty(size=key_value_cache_shape,\n                                      dtype=torch_dtype,\n                                      device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_value_cache[:, 0])\n        value_caches.append(key_value_cache[:, 1])\n    return key_caches, value_caches\n\n\ndef create_kv_caches_with_random(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: int = 0,\n    device: Optional[str] = \"cuda\",\n) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n\n    if cache_dtype == \"fp8\" and head_size % 16:\n        raise ValueError(\n            f\"Does not support key cache of type fp8 with head_size {head_size}\"\n        )\n\n    torch.random.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n\n    scale = head_size**-0.5\n    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_caches: List[torch.Tensor] = []\n    for _ in range(num_layers):\n        key_cache = torch.empty(size=key_cache_shape,\n                                dtype=torch_dtype,\n                                device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_cache)\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_caches: List[torch.Tensor] = []\n    for _ in range(num_layers):\n        value_cache = torch.empty(size=value_cache_shape,\n                                  dtype=torch_dtype,\n                                  device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support value cache of type {cache_dtype}\")\n        value_caches.append(value_cache)\n    return key_caches, value_caches\n\n\n@lru_cache\ndef print_warning_once(msg: str) -> None:\n    logger.warning(msg)\n\n\n@lru_cache(maxsize=None)\ndef is_pin_memory_available() -> bool:\n\n    if in_wsl():\n        # Pinning memory in WSL is not supported.\n        # https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications\n        print_warning_once(\"Using 'pin_memory=False' as WSL is detected. \"\n                           \"This may slow down the performance.\")\n        return False\n    elif is_xpu():\n        print_warning_once(\"Pin memory is not supported on XPU.\")\n        return False\n    elif is_neuron():\n        print_warning_once(\"Pin memory is not supported on Neuron.\")\n        return False\n    elif is_cpu() or is_openvino():\n        return False\n    return True\n\n\nclass CudaMemoryProfiler:\n\n    def __init__(self, device: Optional[torch.types.Device] = None):\n        self.device = device\n\n    def current_memory_usage(self) -> float:\n        # Return the memory usage in bytes.\n        if torch.cuda.is_available():\n            torch.cuda.reset_peak_memory_stats(self.device)\n            mem = torch.cuda.max_memory_allocated(self.device)\n        elif is_xpu():\n            torch.xpu.reset_peak_memory_stats(self.device)  # type: ignore\n            mem = torch.xpu.max_memory_allocated(self.device)  # type: ignore\n        return mem\n\n    def __enter__(self):\n        self.initial_memory = self.current_memory_usage()\n        # This allows us to call methods of the context manager if needed\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.final_memory = self.current_memory_usage()\n        self.consumed_memory = self.final_memory - self.initial_memory\n\n        # Force garbage collection\n        gc.collect()\n\n\ndef str_to_int_tuple(s: str) -> Tuple[int, ...]:\n    \"\"\"Convert a string to a tuple of integers.\"\"\"\n    try:\n        return tuple(map(int, s.split(\",\")))\n    except ValueError as e:\n        raise ValueError(\n            \"String must be a series of integers separated by commas \"\n            f\"(e.g., 1, 2, 3). Given input: {s}\") from e\n\n\ndef make_ndarray_with_pad(\n    x: List[List[T]],\n    pad: T,\n    dtype: npt.DTypeLike,\n    *,\n    max_len: Optional[int] = None,\n) -> npt.NDArray:\n    \"\"\"\n    Make a padded array from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    if max_len is None:\n        # Unlike for most functions, map is faster than a genexpr over `len`\n        max_len = max(map(len, x), default=0)\n\n    padded_x = np.full((len(x), max_len), pad, dtype=dtype)\n    for ind, blocktb in enumerate(x):\n        assert len(blocktb) <= max_len\n        padded_x[ind, :len(blocktb)] = blocktb\n\n    return padded_x\n\n\ndef make_tensor_with_pad(\n    x: List[List[T]],\n    pad: T,\n    dtype: torch.dtype,\n    *,\n    max_len: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    pin_memory: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Make a padded tensor from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE[dtype]\n    padded_x = make_ndarray_with_pad(x, pad, np_dtype, max_len=max_len)\n\n    tensor = torch.from_numpy(padded_x).to(device)\n    if pin_memory:\n        tensor = tensor.pin_memory()\n\n    return tensor\n\n\ndef async_tensor_h2d(\n    data: list,\n    dtype: torch.dtype,\n    target_device: Union[str, torch.device],\n    pin_memory: bool,\n) -> torch.Tensor:\n    \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n    return t.to(device=target_device, non_blocking=True)\n\n\ndef maybe_expand_dim(tensor: torch.Tensor,\n                     target_dims: int,\n                     size: int = 1) -> torch.Tensor:\n    \"\"\"Expand the tensor to the target_dims.\"\"\"\n    if tensor.ndim < target_dims:\n        tensor = tensor.view(-1, *([size] * (target_dims - tensor.ndim)))\n    return tensor\n\n\ndef get_dtype_size(dtype: torch.dtype) -> int:\n    \"\"\"Get the size of the data type in bytes.\"\"\"\n    return torch.tensor([], dtype=dtype).element_size()\n\n\n# `collections` helpers\ndef is_list_of(\n    value: object,\n    typ: Type[T],\n    *,\n    check: Literal[\"first\", \"all\"] = \"first\",\n) -> TypeIs[List[T]]:\n    if not isinstance(value, list):\n        return False\n\n    if check == \"first\":\n        return len(value) == 0 or isinstance(value[0], typ)\n    elif check == \"all\":\n        return all(isinstance(v, typ) for v in value)\n\n    assert_never(check)\n\n\ndef merge_dicts(dict1: Dict[K, List[T]],\n                dict2: Dict[K, List[T]]) -> Dict[K, List[T]]:\n    \"\"\"Merge 2 dicts that have key -> List of items.\n\n    When a key conflicts, the values in dict1 is prioritized.\n    \"\"\"\n    merged_dict: Dict[K, List[T]] = defaultdict(list)\n\n    for key, value in dict1.items():\n        merged_dict[key].extend(value)\n\n    for key, value in dict2.items():\n        merged_dict[key].extend(value)\n\n    return dict(merged_dict)\n\n\nJSONTree = Union[Dict[str, \"JSONTree[T]\"], List[\"JSONTree[T]\"],\n                 Tuple[\"JSONTree[T]\", ...], T]\n\"\"\"A nested JSON structure where the leaves need not be JSON-serializable.\"\"\"\n\n\n@overload\ndef json_map_leaves(\n    func: Callable[[T], U],\n    value: Dict[str, JSONTree[T]],\n) -> Dict[str, JSONTree[U]]:\n    ...\n\n\n@overload\ndef json_map_leaves(\n    func: Callable[[T], U],\n    value: List[JSONTree[T]],\n) -> List[JSONTree[U]]:\n    ...\n\n\n@overload\ndef json_map_leaves(\n    func: Callable[[T], U],\n    value: Tuple[JSONTree[T], ...],\n) -> Tuple[JSONTree[U], ...]:\n    ...\n\n\n@overload\ndef json_map_leaves(\n    func: Callable[[T], U],\n    value: JSONTree[T],\n) -> JSONTree[U]:\n    ...\n\n\ndef json_map_leaves(func: Callable[[T], U], value: JSONTree[T]) -> JSONTree[U]:\n    if isinstance(value, dict):\n        return {k: json_map_leaves(func, v) for k, v in value.items()}\n    elif isinstance(value, list):\n        return [json_map_leaves(func, v) for v in value]\n    elif isinstance(value, tuple):\n        return tuple(json_map_leaves(func, v) for v in value)\n    else:\n        return func(value)\n\n\ndef flatten_2d_lists(lists: List[List[T]]) -> List[T]:\n    \"\"\"Flatten a list of lists to a single list.\"\"\"\n    return [item for sublist in lists for item in sublist]\n\n\ndef init_cached_hf_modules() -> None:\n    \"\"\"\n    Lazy initialization of the Hugging Face modules.\n    \"\"\"\n    from transformers.dynamic_module_utils import init_hf_modules\n    init_hf_modules()\n\n\n@lru_cache(maxsize=None)\ndef find_library(lib_name: str) -> str:\n    \"\"\"\n    Find the library file in the system.\n    `lib_name` is full filename, with both prefix and suffix.\n    This function resolves `lib_name` to the full path of the library.\n    \"\"\"\n    # Adapted from https://github.com/openai/triton/blob/main/third_party/nvidia/backend/driver.py#L19 # noqa\n    # According to https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard\n    # `/sbin/ldconfig` should exist in all Linux systems.\n    # `/sbin/ldconfig` searches the library in the system\n    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n    # each line looks like the following:\n    # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n    locs = [line.split()[-1] for line in libs.splitlines() if lib_name in line]\n    # `LD_LIBRARY_PATH` searches the library in the user-defined paths\n    env_ld_library_path = envs.LD_LIBRARY_PATH\n    if not locs and env_ld_library_path:\n        locs = [\n            os.path.join(dir, lib_name)\n            for dir in env_ld_library_path.split(\":\")\n            if os.path.exists(os.path.join(dir, lib_name))\n        ]\n    if not locs:\n        raise ValueError(f\"Cannot find {lib_name} in the system.\")\n    return locs[0]\n\n\ndef find_nccl_library() -> str:\n    \"\"\"\n    We either use the library file specified by the `VLLM_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.VLLM_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable VLLM_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return so_file\n\n\ndef enable_trace_function_call_for_thread() -> None:\n    \"\"\"Set up function tracing for the current thread,\n    if enabled via the VLLM_TRACE_FUNCTION environment variable\n    \"\"\"\n\n    if envs.VLLM_TRACE_FUNCTION:\n        tmp_dir = tempfile.gettempdir()\n        filename = (f\"VLLM_TRACE_FUNCTION_for_process_{os.getpid()}\"\n                    f\"_thread_{threading.get_ident()}_\"\n                    f\"at_{datetime.datetime.now()}.log\").replace(\" \", \"_\")\n        log_path = os.path.join(tmp_dir, \"vllm\", get_vllm_instance_id(),\n                                filename)\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n        enable_trace_function_call(log_path)\n\n\n# `functools` helpers\ndef identity(value: T) -> T:\n    return value\n\n\nF = TypeVar('F', bound=Callable[..., Any])\n\n\ndef deprecate_kwargs(\n        *kws: str,\n        is_deprecated: Union[bool, Callable[[], bool]] = True,\n        additional_message: Optional[str] = None) -> Callable[[F], F]:\n    deprecated_kws = set(kws)\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_kwargs = kwargs.keys() & deprecated_kws\n                if deprecated_kwargs:\n                    msg = (\n                        f\"The keyword arguments {deprecated_kwargs} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\n@lru_cache(maxsize=8)\ndef _cuda_device_count_stateless(\n        cuda_visible_devices: Optional[str] = None) -> int:\n    # Note: cuda_visible_devices is not used, but we keep it as an argument for\n    # LRU Cache purposes.\n\n    # Code below is based on\n    # https://github.com/pytorch/pytorch/blob/\n    # c1cd946818442aca8c7f812b16d187ce1586c3bc/\n    # torch/cuda/__init__.py#L831C1-L831C17\n    import torch.cuda\n    import torch.version\n\n    if not torch.cuda._is_compiled():\n        return 0\n    if is_hip():\n        # ROCm uses amdsmi instead of nvml for stateless device count\n        # This requires a sufficiently modern version of Torch 2.4.0\n        raw_count = torch.cuda._device_count_amdsmi() if (hasattr(\n            torch.cuda, \"_device_count_amdsmi\")) else -1\n    else:\n        raw_count = torch.cuda._device_count_nvml()\n    r = torch._C._cuda_getDeviceCount() if raw_count < 0 else raw_count\n    return r\n\n\ndef cuda_device_count_stateless() -> int:\n    \"\"\"Get number of CUDA devices, caching based on the value of\n    CUDA_VISIBLE_DEVICES at the time of call.\n    \n    This should be used instead of torch.cuda.device_count()\n    unless CUDA_VISIBLE_DEVICES has already been set to the desired\n    value.\"\"\"\n\n    # This can be removed and simply replaced with torch.cuda.get_device_count\n    # after https://github.com/pytorch/pytorch/pull/122815 is released.\n    return _cuda_device_count_stateless(envs.CUDA_VISIBLE_DEVICES)\n\n\n#From: https://stackoverflow.com/a/4104188/2749989\ndef run_once(f):\n\n    def wrapper(*args, **kwargs) -> Any:\n        if not wrapper.has_run:  # type: ignore[attr-defined]\n            wrapper.has_run = True  # type: ignore[attr-defined]\n            return f(*args, **kwargs)\n\n    wrapper.has_run = False  # type: ignore[attr-defined]\n    return wrapper\n\n\nclass FlexibleArgumentParser(argparse.ArgumentParser):\n    \"\"\"ArgumentParser that allows both underscore and dash in names.\"\"\"\n\n    def parse_args(self, args=None, namespace=None):\n        if args is None:\n            args = sys.argv[1:]\n\n        # Convert underscores to dashes and vice versa in argument names\n        processed_args = []\n        for arg in args:\n            if arg.startswith('--'):\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    key = '--' + key[len('--'):].replace('_', '-')\n                    processed_args.append(f'{key}={value}')\n                else:\n                    processed_args.append('--' +\n                                          arg[len('--'):].replace('_', '-'))\n            else:\n                processed_args.append(arg)\n\n        return super().parse_args(processed_args, namespace)\n\n\nasync def _run_task_with_lock(task: Callable, lock: asyncio.Lock, *args,\n                              **kwargs):\n    \"\"\"Utility function to run async task in a lock\"\"\"\n    async with lock:\n        return await task(*args, **kwargs)\n",
      "diff": "diff --git a/vllm/utils.py b/vllm/utils.py\nindex f8251284a..ecdd4760e 100644\n--- a/vllm/utils.py\n+++ b/vllm/utils.py\n@@ -261,6 +261,44 @@ class LRUCache(Generic[T]):\n         self.cache.clear()\n \n \n+class PyObjectCache:\n+    \"\"\"Used to cache python objects to avoid object allocations \n+    across scheduler iterations.\n+    \"\"\"\n+\n+    def __init__(self, obj_builder):\n+        self._obj_builder = obj_builder\n+        self._index = 0\n+\n+        self._obj_cache = []\n+        for _ in range(128):\n+            self._obj_cache.append(self._obj_builder())\n+\n+    def _grow_cache(self):\n+        # Double the size of the cache\n+        num_objs = len(self._obj_cache)\n+        for _ in range(num_objs):\n+            self._obj_cache.append(self._obj_builder())\n+\n+    def get_object(self):\n+        \"\"\"Returns a pre-allocated cached object. If there is not enough \n+        objects, then the cache size will double.\n+        \"\"\"\n+        if self._index >= len(self._obj_cache):\n+            self._grow_cache()\n+            assert self._index < len(self._obj_cache)\n+\n+        obj = self._obj_cache[self._index]\n+        self._index += 1\n+\n+        return obj\n+\n+    def reset(self):\n+        \"\"\"Makes all cached-objects available for the next scheduler iteration.\n+        \"\"\"\n+        self._index = 0\n+\n+\n def is_hip() -> bool:\n     return torch.version.hip is not None",
      "change_type": "modified",
      "lines_added": 39,
      "lines_removed": 1
    },
    {
      "file_path": "vllm/worker/model_runner.py",
      "old_content": "import dataclasses\nimport gc\nimport time\nimport warnings\nimport weakref\nfrom dataclasses import dataclass\nfrom typing import (TYPE_CHECKING, Any, Dict, List, Optional, Set, Tuple, Type,\n                    TypeVar, Union)\n\nimport numpy as np\nimport torch\nimport torch.distributed\nimport torch.nn as nn\n\ntry:\n    from flashinfer import BatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.decode import CUDAGraphBatchDecodeWithPagedKVCacheWrapper\n    from flashinfer.prefill import BatchPrefillWithPagedKVCacheWrapper\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 256 * 1024 * 1024\nexcept ImportError:\n    BatchDecodeWithPagedKVCacheWrapper = None\n    CUDAGraphBatchDecodeWithPagedKVCacheWrapper = None\n    BatchPrefillWithPagedKVCacheWrapper = None\n    FLASHINFER_WORKSPACE_BUFFER_SIZE = 0\n\nimport vllm.envs as envs\nfrom vllm.attention import AttentionMetadata, get_attn_backend\nfrom vllm.config import (CacheConfig, DeviceConfig, LoadConfig, LoRAConfig,\n                         ModelConfig, MultiModalConfig, ParallelConfig,\n                         PromptAdapterConfig, SchedulerConfig)\nfrom vllm.distributed import get_pp_group\nfrom vllm.distributed.parallel_state import graph_capture\nfrom vllm.inputs import INPUT_REGISTRY\nfrom vllm.logger import init_logger\nfrom vllm.lora.layers import LoRAMapping\nfrom vllm.lora.request import LoRARequest\nfrom vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\nfrom vllm.model_executor import SamplingMetadata\nfrom vllm.model_executor.model_loader import get_model\nfrom vllm.model_executor.model_loader.tensorizer import TensorizerConfig\nfrom vllm.model_executor.models.interfaces import (supports_lora,\n                                                   supports_vision)\nfrom vllm.model_executor.models.utils import set_cpu_offload_max_bytes\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,\n                             MultiModalInputs)\nfrom vllm.prompt_adapter.layers import PromptAdapterMapping\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.prompt_adapter.worker_manager import (\n    LRUCacheWorkerPromptAdapterManager)\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.sequence import (IntermediateTensors, SamplerOutput,\n                           SequenceGroupMetadata)\nfrom vllm.utils import (CudaMemoryProfiler, async_tensor_h2d, flatten_2d_lists,\n                        get_kv_cache_torch_dtype, is_hip,\n                        is_pin_memory_available)\nfrom vllm.worker.model_runner_base import (\n    ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n    _add_attn_metadata_broadcastable_dict,\n    _add_sampling_metadata_broadcastable_dict,\n    _init_attn_metadata_from_tensor_dict,\n    _init_sampling_metadata_from_tensor_dict)\n\nif TYPE_CHECKING:\n    from vllm.attention.backends.abstract import AttentionBackend\n\nlogger = init_logger(__name__)\n\n_PAD_SLOT_ID = -1\nLORA_WARMUP_RANK = 8\n_BATCH_SIZE_ALIGNMENT = 8\n# Capture graphs for token size 1, 2, 4, 8, 16, 24, 32, 40, ..., 256.\n# NOTE: _get_graph_batch_size needs to be updated if this list is changed.\n_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [\n    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)\n]\n_NUM_WARMUP_ITERS = 2\n\nTModelInputForGPU = TypeVar('TModelInputForGPU', bound=\"ModelInputForGPU\")\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPU(ModelRunnerInputBase):\n    \"\"\"\n    This base class contains metadata needed for the base model forward pass\n    but not metadata for possible additional steps, e.g., sampling. Model\n    runners that run additional steps should subclass this method to add\n    additional fields.\n    \"\"\"\n    input_tokens: Optional[torch.Tensor] = None\n    input_positions: Optional[torch.Tensor] = None\n    seq_lens: Optional[List[int]] = None\n    query_lens: Optional[List[int]] = None\n    lora_mapping: Optional[\"LoRAMapping\"] = None\n    lora_requests: Optional[Set[LoRARequest]] = None\n    attn_metadata: Optional[\"AttentionMetadata\"] = None\n    prompt_adapter_mapping: Optional[PromptAdapterMapping] = None\n    prompt_adapter_requests: Optional[Set[PromptAdapterRequest]] = None\n    multi_modal_kwargs: Optional[BatchedTensorInputs] = None\n    request_ids_to_seq_ids: Optional[Dict[str, List[int]]] = None\n    finished_requests_ids: Optional[List[str]] = None\n    virtual_engine: int = 0\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls: Type[TModelInputForGPU],\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> TModelInputForGPU:\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\n@dataclass(frozen=True)\nclass ModelInputForGPUWithSamplingMetadata(ModelInputForGPU):\n    \"\"\"\n    Used by the ModelRunner.\n    \"\"\"\n    sampling_metadata: Optional[\"SamplingMetadata\"] = None\n    # Used for speculative decoding. We do not broadcast it because it is only\n    # used by the driver worker.\n    is_prompt: Optional[bool] = None\n\n    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:\n        tensor_dict = {\n            \"input_tokens\": self.input_tokens,\n            \"input_positions\": self.input_positions,\n            \"lora_requests\": self.lora_requests,\n            \"lora_mapping\": self.lora_mapping,\n            \"multi_modal_kwargs\": self.multi_modal_kwargs,\n            \"prompt_adapter_mapping\": self.prompt_adapter_mapping,\n            \"prompt_adapter_requests\": self.prompt_adapter_requests,\n            \"virtual_engine\": self.virtual_engine,\n            \"request_ids_to_seq_ids\": self.request_ids_to_seq_ids,\n            \"finished_requests_ids\": self.finished_requests_ids,\n        }\n        _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)\n        _add_sampling_metadata_broadcastable_dict(tensor_dict,\n                                                  self.sampling_metadata)\n        return tensor_dict\n\n    @classmethod\n    def from_broadcasted_tensor_dict(\n        cls,\n        tensor_dict: Dict[str, Any],\n        attn_backend: Optional[\"AttentionBackend\"] = None,\n    ) -> \"ModelInputForGPUWithSamplingMetadata\":\n        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)\n        if attn_backend is not None:\n            tensor_dict = _init_attn_metadata_from_tensor_dict(\n                attn_backend, tensor_dict)\n        return cls(**tensor_dict)\n\n\nclass ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n    \"\"\"Build ModelInputForGPU from SequenceGroupMetadata.\"\"\"\n\n    # Note: ideally we would be using a dataclass(kw_only=True)\n    # here, so that this can be subclassed easily,\n    # but kw_only is not supported in python<3.10.\n    class InterDataForSeqGroup:\n        \"\"\"Intermediate data for the current sequence group.\"\"\"\n\n        def __init__(\n            self,\n            *,\n            # From sequence group metadata.\n            request_id: str,\n            seq_ids: List[int],\n            is_prompt: bool,\n            block_tables: Optional[Dict[int, List[int]]],\n            computed_block_nums: List[int],\n            n_seqs: int = 0,\n\n            # Input tokens and positions.\n            input_tokens: Optional[List[List[int]]] = None,\n            input_positions: Optional[List[List[int]]] = None,\n\n            # The sequence length (may be capped to the sliding window).\n            seq_lens: Optional[List[int]] = None,\n            # The original sequence length (before applying sliding window).\n            # This is used to compute slot mapping.\n            orig_seq_lens: Optional[List[int]] = None,\n            # The query length.\n            query_lens: Optional[List[int]] = None,\n            # The number of tokens that are already computed.\n            context_lens: Optional[List[int]] = None,\n            # The current sliding window block.\n            curr_sliding_window_blocks: Optional[List[int]] = None,\n\n            # LoRA inputs.\n            lora_index_mapping: Optional[List[List[int]]] = None,\n            lora_prompt_mapping: Optional[List[List[int]]] = None,\n            lora_requests: Optional[Set[LoRARequest]] = None,\n\n            # Prompt adapter inputs.\n            prompt_adapter_index_mapping: Optional[List[int]] = None,\n            prompt_adapter_prompt_mapping: Optional[List[int]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n\n            # Multi-modal inputs.\n            multi_modal_inputs: Optional[MultiModalInputs] = None,\n\n            # Whether the prefix cache is hit (prefill only).\n            prefix_cache_hit: bool = False,\n        ):\n            self.request_id = request_id\n            self.seq_ids = seq_ids\n            self.is_prompt = is_prompt\n            self.block_tables = block_tables\n            self.computed_block_nums = computed_block_nums\n            self.n_seqs = n_seqs\n            self.input_tokens = input_tokens or []\n            self.input_positions = input_positions or []\n            self.seq_lens = seq_lens or []\n            self.orig_seq_lens = orig_seq_lens or []\n            self.query_lens = query_lens or []\n            self.context_lens = context_lens or []\n            self.curr_sliding_window_blocks = curr_sliding_window_blocks or []\n\n            self.lora_index_mapping = lora_index_mapping or []\n            self.lora_prompt_mapping = lora_prompt_mapping or []\n            self.lora_requests = lora_requests or set()\n\n            self.prompt_adapter_index_mapping = (prompt_adapter_index_mapping\n                                                 or [])\n            self.prompt_adapter_prompt_mapping = (prompt_adapter_prompt_mapping\n                                                  or [])\n            self.prompt_adapter_request = prompt_adapter_request\n\n            self.multi_modal_inputs = multi_modal_inputs\n            self.prefix_cache_hit = prefix_cache_hit\n\n            self.__post_init__()\n\n        def __post_init__(self):\n            self.n_seqs = len(self.seq_ids)\n\n            self.input_tokens = [[] for _ in range(self.n_seqs)]\n            self.input_positions = [[] for _ in range(self.n_seqs)]\n            self.seq_lens = [0] * self.n_seqs\n            self.orig_seq_lens = [0] * self.n_seqs\n            self.query_lens = [0] * self.n_seqs\n            self.context_lens = [0] * self.n_seqs\n            self.curr_sliding_window_blocks = [0] * self.n_seqs\n\n            self.lora_index_mapping = [[] for _ in range(self.n_seqs)]\n            self.lora_prompt_mapping = [[] for _ in range(self.n_seqs)]\n\n    def __init__(self,\n                 runner: \"GPUModelRunnerBase\",\n                 finished_requests_ids: Optional[List[str]] = None):\n        super().__init__()\n        # Compute functions for each sequence in a sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_compute_fns = [\n            self._compute_lens,\n            self._compute_for_prefix_cache_hit,\n            self._compute_for_sliding_window,\n            self._compute_lora_input,\n        ]\n        # Compute functions for each sequence group.\n        # WARNING: The order of the functions matters!\n        self.per_seq_group_compute_fns = [\n            self._compute_prompt_adapter_input,\n            self._compute_multi_modal_input,\n        ]\n\n        self.runner = runner\n        self.model_input_cls = self.runner._model_input_cls\n        self.attn_backend = self.runner.attn_backend\n        self.scheduler_config = self.runner.scheduler_config\n        self.sliding_window = self.runner.sliding_window\n        self.block_size = self.runner.block_size\n        self.enable_lora = self.runner.lora_config is not None\n        self.enable_prompt_adapter = (self.runner.prompt_adapter_config\n                                      is not None)\n        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper\n        self.finished_requests_ids = finished_requests_ids\n        self.decode_only = True\n\n        # Intermediate data (data in CPU before going to GPU) for\n        # the current sequence group.\n        self.inter_data_list: List[\n            ModelInputForGPUBuilder.InterDataForSeqGroup] = []\n\n        # Attention metadata inputs.\n        self.attn_metadata_builder = self.attn_backend.make_metadata_builder(\n            weakref.proxy(self))\n\n        # Engine/Model configurations.\n        self.chunked_prefill_enabled = (\n            self.scheduler_config is not None\n            and self.scheduler_config.chunked_prefill_enabled)\n        if self.sliding_window is not None:\n            self.sliding_window_blocks = (\n                self.sliding_window + self.block_size - 1) // self.block_size\n            self.block_aligned_sliding_window = \\\n                self.sliding_window_blocks * self.block_size\n\n    def _compute_lens(self, inter_data: InterDataForSeqGroup, seq_idx: int,\n                      seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Compute context length, sequence length and tokens\n        for the given sequence data.\n        \"\"\"\n        seq_data = seq_group_metadata.seq_data[inter_data.seq_ids[seq_idx]]\n        token_chunk_size = seq_group_metadata.token_chunk_size\n\n        # Compute context length (the number of tokens that are\n        # already computed) and sequence length (total number of tokens).\n        seq_len = seq_data.get_len()\n        if inter_data.is_prompt:\n            context_len = seq_data.get_num_computed_tokens()\n        else:\n            # get_num_computed_tokens is incorrect for spec decoding.\n            # So, we should have a special logic here.\n            # TODO(sang): Fix it.\n            context_len = seq_len - 1\n        seq_len = min(seq_len, context_len + token_chunk_size)\n\n        # Compute tokens.\n        if inter_data.is_prompt:\n            tokens = seq_data.get_token_ids()[context_len:seq_len]\n        else:\n            # Optimization. get_token_ids requires the entire copy of\n            # tokens.\n            tokens = [seq_data.get_last_token_id()]\n\n        inter_data.seq_lens[seq_idx] = seq_len\n        inter_data.orig_seq_lens[seq_idx] = seq_len\n        inter_data.context_lens[seq_idx] = context_len\n        inter_data.input_tokens[seq_idx] = tokens\n        inter_data.input_positions[seq_idx] = list(range(context_len, seq_len))\n        inter_data.query_lens[\n            seq_idx] = seq_len - context_len if inter_data.is_prompt else 1\n\n    def _compute_for_prefix_cache_hit(\n            self, inter_data: InterDataForSeqGroup, seq_idx: int,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Check if hit prefix cache (i.e., some blocks are already computed).\n        If hit, update input tokens and positions to only compute the\n        remaining blocks.\n        \"\"\"\n        computed_block_nums = inter_data.computed_block_nums\n\n        # Note that prefix caching does not support sliding window.\n        prefix_cache_hit = (computed_block_nums is not None\n                            and len(computed_block_nums) > 0\n                            and self.sliding_window is None\n                            and inter_data.is_prompt)\n        inter_data.prefix_cache_hit = prefix_cache_hit\n        if self.chunked_prefill_enabled and prefix_cache_hit:\n            raise RuntimeError(\n                \"chunked prefill cannot be used with prefix caching now.\")\n\n        # If prefix cache is hit, advance context length to bypass\n        # hit blocks. Accordingly, input tokens, position and query length\n        # have to be updated.\n        if prefix_cache_hit:\n            assert computed_block_nums is not None\n            context_len = len(computed_block_nums) * self.block_size\n            inter_data.input_tokens[seq_idx] = inter_data.input_tokens[\n                seq_idx][context_len:]\n            inter_data.input_positions[seq_idx] = inter_data.input_positions[\n                seq_idx][context_len:]\n            inter_data.context_lens[seq_idx] = context_len\n            inter_data.query_lens[\n                seq_idx] = inter_data.seq_lens[seq_idx] - context_len\n\n    def _compute_for_sliding_window(self, inter_data: InterDataForSeqGroup,\n                                    seq_idx: int,\n                                    seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Update seq_len and curr_sliding_window_block for the given\n        sequence data (only required by decoding) if sliding window is enabled.\n        \"\"\"\n        curr_sliding_window_block = 0\n        sliding_seq_len = inter_data.seq_lens[seq_idx]\n        if not inter_data.is_prompt and self.sliding_window is not None:\n            # TODO(sang): This is a hack to make sliding window work with\n            # paged attn. We can remove it if we make paged attn kernel\n            # to properly handle slinding window attn.\n            curr_sliding_window_block = self.sliding_window_blocks\n            if self.scheduler_config.use_v2_block_manager:\n                # number of elements in last block\n                suff_len = inter_data.seq_lens[seq_idx] % self.block_size\n                sliding_seq_len = min(\n                    inter_data.seq_lens[seq_idx],\n                    self.block_aligned_sliding_window + suff_len)\n                if suff_len > 0:\n                    curr_sliding_window_block += 1\n            else:\n                sliding_seq_len = min(inter_data.seq_lens[seq_idx],\n                                      self.sliding_window)\n\n        inter_data.curr_sliding_window_blocks[\n            seq_idx] = curr_sliding_window_block\n        inter_data.seq_lens[seq_idx] = sliding_seq_len\n\n    def _compute_lora_input(self, inter_data: InterDataForSeqGroup,\n                            seq_idx: int,\n                            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If LoRA is enabled, compute LoRA index and prompt mapping.\"\"\"\n        if not self.enable_lora:\n            return\n\n        lora_id = seq_group_metadata.lora_int_id\n        if lora_id > 0:\n            inter_data.lora_requests.add(seq_group_metadata.lora_request)\n        query_len = inter_data.query_lens[seq_idx]\n        inter_data.lora_index_mapping.append([lora_id] * query_len)\n        inter_data.lora_prompt_mapping.append(\n            [lora_id] *\n            (query_len if seq_group_metadata.sampling_params\n             and seq_group_metadata.sampling_params.prompt_logprobs is not None\n             else 1))\n\n    def _compute_prompt_adapter_input(\n            self, inter_data: InterDataForSeqGroup,\n            seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If prompt adapter is enabled, compute index and prompt mapping.\n        \"\"\"\n        # Note that when is_prompt=True, we expect only one sequence\n        # in the group.\n        if not self.enable_prompt_adapter:\n            return\n\n        prompt_adapter_id = seq_group_metadata.prompt_adapter_id\n        if prompt_adapter_id <= 0 or not inter_data.is_prompt:\n            return\n\n        # We expect only one sequence in the group when is_prompt=True.\n        assert inter_data.n_seqs == 1\n        query_len = inter_data.query_lens[0]\n        inter_data.prompt_adapter_request = (\n            seq_group_metadata.prompt_adapter_request)\n\n        num_tokens = seq_group_metadata.prompt_adapter_num_virtual_tokens\n        inter_data.prompt_adapter_index_mapping = [\n            prompt_adapter_id\n        ] * num_tokens + [0] * (query_len - num_tokens)\n        inter_data.prompt_adapter_prompt_mapping = [prompt_adapter_id] * (\n            query_len if seq_group_metadata.sampling_params\n            and seq_group_metadata.sampling_params.prompt_logprobs else 1)\n\n    def _compute_multi_modal_input(self, inter_data: InterDataForSeqGroup,\n                                   seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"If multi-modal data is given, add it to the input.\"\"\"\n        mm_data = seq_group_metadata.multi_modal_data\n        if not mm_data:\n            return\n\n        mm_kwargs = self.multi_modal_input_mapper(mm_data)\n        inter_data.multi_modal_inputs = mm_kwargs\n\n    def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):\n        \"\"\"Add a sequence group to the builder.\"\"\"\n        seq_ids = list(seq_group_metadata.seq_data.keys())\n        n_seqs = len(seq_ids)\n        is_prompt = seq_group_metadata.is_prompt\n\n        if is_prompt:\n            assert n_seqs == 1\n            self.decode_only = False\n\n        inter_data = self.InterDataForSeqGroup(\n            request_id=seq_group_metadata.request_id,\n            seq_ids=seq_ids,\n            is_prompt=is_prompt,\n            block_tables=seq_group_metadata.block_tables,\n            computed_block_nums=seq_group_metadata.computed_block_nums)\n        self.inter_data_list.append(inter_data)\n\n        for seq_idx in range(n_seqs):\n            for per_seq_fn in self.per_seq_compute_fns:\n                per_seq_fn(inter_data, seq_idx, seq_group_metadata)\n        for per_seq_group_fn in self.per_seq_group_compute_fns:\n            per_seq_group_fn(inter_data, seq_group_metadata)\n\n    def _use_captured_graph(self, batch_size: int,\n                            max_decode_seq_len: int) -> bool:\n        return (self.decode_only and not self.runner.model_config.enforce_eager\n                and batch_size <= _BATCH_SIZES_TO_CAPTURE[-1]\n                and max_decode_seq_len <= self.runner.max_seq_len_to_capture)\n\n    def build(self) -> ModelInputForGPU:\n        \"\"\"Finalize the builder intermediate data and\n        create on-device tensors.\n        \"\"\"\n        # Combine and flatten intermediate data.\n        input_tokens = flatten_2d_lists([\n            flatten_2d_lists(inter_data.input_tokens)\n            for inter_data in self.inter_data_list\n        ])\n        if not input_tokens:\n            # This may happen when all prefill requests hit\n            # prefix caching and there is no decode request.\n            return self.model_input_cls()\n        input_positions = flatten_2d_lists([\n            flatten_2d_lists(inter_data.input_positions)\n            for inter_data in self.inter_data_list\n        ])\n        seq_lens = []\n        max_decode_seq_len = 0\n        for inter_data in self.inter_data_list:\n            seq_lens.extend(inter_data.seq_lens)\n            if not inter_data.is_prompt:\n                max_decode_seq_len = max(max_decode_seq_len,\n                                         max(inter_data.seq_lens))\n        query_lens = flatten_2d_lists(\n            [inter_data.query_lens for inter_data in self.inter_data_list])\n        # Mapping from request IDs to sequence IDs. Used for Jamba models\n        # that manages the cache by itself.\n        request_ids_to_seq_ids = {\n            data.request_id: data.seq_ids\n            for data in self.inter_data_list\n        }\n\n        batch_size = len(input_tokens)\n        use_captured_graph = self._use_captured_graph(batch_size,\n                                                      max_decode_seq_len)\n\n        # If cuda graph can be used, pad tensors accordingly.\n        # See `capture_model` API for more details.\n        # vLLM uses cuda graph only for decoding requests.\n        cuda_graph_pad_size = -1\n        if use_captured_graph:\n            graph_batch_size = _get_graph_batch_size(batch_size)\n            assert graph_batch_size >= batch_size\n            cuda_graph_pad_size = graph_batch_size - batch_size\n            batch_size = graph_batch_size\n\n        # Tokens and positions.\n        input_tokens.extend([0] * cuda_graph_pad_size)\n        input_positions.extend([0] * cuda_graph_pad_size)\n        assert self.runner.device is not None\n        input_tokens_tensor = async_tensor_h2d(input_tokens, torch.long,\n                                               self.runner.device,\n                                               self.runner.pin_memory)\n        input_positions_tensor = async_tensor_h2d(input_positions, torch.long,\n                                                  self.runner.device,\n                                                  self.runner.pin_memory)\n\n        # Sequence and query lengths.\n        seq_lens.extend([1] * cuda_graph_pad_size)\n\n        # Attention metadata.\n        attn_metadata = self.attn_metadata_builder.build(\n            seq_lens, query_lens, cuda_graph_pad_size, batch_size)\n\n        # LoRA data.\n        lora_requests = set()\n        lora_mapping = None\n        if self.enable_lora:\n            lora_requests = set(r for data in self.inter_data_list\n                                for r in data.lora_requests)\n            lora_index_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_index_mapping)\n                for inter_data in self.inter_data_list\n            ])\n            lora_index_mapping.extend([0] * cuda_graph_pad_size)\n            lora_prompt_mapping = flatten_2d_lists([\n                flatten_2d_lists(inter_data.lora_prompt_mapping)\n                for inter_data in self.inter_data_list\n            ])\n            lora_mapping = LoRAMapping(\n                **dict(index_mapping=lora_index_mapping,\n                       prompt_mapping=lora_prompt_mapping,\n                       is_prefill=not self.decode_only))\n\n        # Prompt adapter data.\n        prompt_adapter_requests: Set[PromptAdapterRequest] = set()\n        prompt_adapter_mapping = None\n        if self.enable_prompt_adapter:\n            prompt_adapter_requests = set(\n                data.prompt_adapter_request for data in self.inter_data_list\n                if data.prompt_adapter_request is not None)\n            prompt_adapter_index_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_index_mapping\n                for inter_data in self.inter_data_list\n            ])\n            prompt_adapter_index_mapping.extend([0] * cuda_graph_pad_size)\n            prompt_adapter_prompt_mapping = flatten_2d_lists([\n                inter_data.prompt_adapter_prompt_mapping\n                for inter_data in self.inter_data_list\n            ])\n            prompt_adapter_mapping = PromptAdapterMapping(\n                prompt_adapter_index_mapping,\n                prompt_adapter_prompt_mapping,\n            )\n\n        # Multi-modal data.\n        multi_modal_inputs_list = [\n            data.multi_modal_inputs for data in self.inter_data_list\n            if data.multi_modal_inputs is not None\n        ]\n        multi_modal_kwargs = MultiModalInputs.batch(multi_modal_inputs_list)\n\n        return self.model_input_cls(\n            input_tokens=input_tokens_tensor,\n            input_positions=input_positions_tensor,\n            attn_metadata=attn_metadata,\n            seq_lens=seq_lens,\n            query_lens=query_lens,\n            lora_mapping=lora_mapping,\n            lora_requests=lora_requests,\n            multi_modal_kwargs=multi_modal_kwargs,\n            request_ids_to_seq_ids=request_ids_to_seq_ids,\n            finished_requests_ids=self.finished_requests_ids,\n            prompt_adapter_mapping=prompt_adapter_mapping,\n            prompt_adapter_requests=prompt_adapter_requests)\n\n\nclass GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n    \"\"\"\n    Helper class for shared methods between GPU model runners.\n    \"\"\"\n    _model_input_cls: Type[TModelInputForGPU]\n    _builder_cls: Type[ModelInputForGPUBuilder]\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        parallel_config: ParallelConfig,\n        scheduler_config: SchedulerConfig,\n        device_config: DeviceConfig,\n        cache_config: CacheConfig,\n        load_config: LoadConfig,\n        lora_config: Optional[LoRAConfig],\n        kv_cache_dtype: Optional[str] = \"auto\",\n        is_driver_worker: bool = False,\n        prompt_adapter_config: Optional[PromptAdapterConfig] = None,\n        multimodal_config: Optional[MultiModalConfig] = None,\n        return_hidden_states: bool = False,\n    ):\n        self.model_config = model_config\n        self.parallel_config = parallel_config\n        self.scheduler_config = scheduler_config\n        self.device_config = device_config\n        self.cache_config = cache_config\n        self.lora_config = lora_config\n        self.load_config = load_config\n        self.is_driver_worker = is_driver_worker\n        self.prompt_adapter_config = prompt_adapter_config\n        self.multimodal_config = multimodal_config\n        self.return_hidden_states = return_hidden_states\n\n        self.device = self.device_config.device\n        self.pin_memory = is_pin_memory_available()\n\n        self.kv_cache_dtype = kv_cache_dtype\n        self.sliding_window = model_config.get_sliding_window()\n        self.block_size = cache_config.block_size\n        self.max_seq_len_to_capture = self.model_config.max_seq_len_to_capture\n\n        self.graph_runners: List[Dict[int, CUDAGraphRunner]] = [\n            {} for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n        self.graph_memory_pool: Optional[Tuple[\n            int, int]] = None  # Set during graph capture.\n\n        self.has_seqlen_agnostic = model_config.contains_seqlen_agnostic_layers(\n            parallel_config)\n\n        # When using CUDA graph, the input block tables must be padded to\n        # max_seq_len_to_capture. However, creating the block table in\n        # Python can be expensive. To optimize this, we cache the block table\n        # in numpy and only copy the actual input content at every iteration.\n        # The shape of the cached block table will be\n        # (max batch size to capture, max context len to capture / block size).\n        self.graph_block_tables = np.zeros(\n            (max(_BATCH_SIZES_TO_CAPTURE), self.get_max_block_per_batch()),\n            dtype=np.int32)\n        num_attn_heads = self.model_config.get_num_attention_heads(\n            self.parallel_config)\n        self.attn_backend = get_attn_backend(\n            num_attn_heads,\n            self.model_config.get_head_size(),\n            self.model_config.get_num_kv_heads(self.parallel_config),\n            self.model_config.get_sliding_window(),\n            self.model_config.dtype,\n            self.kv_cache_dtype,\n            self.block_size,\n        ) if num_attn_heads else None\n\n        # Multi-modal data support\n        self.multi_modal_input_mapper = MULTIMODAL_REGISTRY \\\n            .create_input_mapper(self.model_config)\n\n        # Lazy initialization\n        self.model: nn.Module  # Set after load_model\n        # Set after load_model.\n        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None\n        self.prompt_adapter_manager: LRUCacheWorkerPromptAdapterManager = None\n\n        self.flashinfer_decode_workspace_buffer = None\n        self.flashinfer_decode_wrapper = None\n        self.flashinfer_prefill_workspace_buffer = None\n        self.flashinfer_prefill_wrapper = None\n\n        set_cpu_offload_max_bytes(\n            int(self.cache_config.cpu_offload_gb * 1024**3))\n\n    def load_model(self) -> None:\n        logger.info(\"Starting to load model %s...\", self.model_config.model)\n        with CudaMemoryProfiler() as m:\n            self.model = get_model(model_config=self.model_config,\n                                   device_config=self.device_config,\n                                   load_config=self.load_config,\n                                   lora_config=self.lora_config,\n                                   multimodal_config=self.multimodal_config,\n                                   parallel_config=self.parallel_config,\n                                   scheduler_config=self.scheduler_config,\n                                   cache_config=self.cache_config)\n\n        self.model_memory_usage = m.consumed_memory\n        logger.info(\"Loading model weights took %.4f GB\",\n                    self.model_memory_usage / float(2**30))\n\n        if self.lora_config:\n            assert supports_lora(self.model), \"Model does not support LoRA\"\n            assert not supports_vision(\n                self.model\n            ), \"To be tested: vision language model with LoRA settings.\"\n\n            self.lora_manager = LRUCacheWorkerLoRAManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens,\n                self.vocab_size,\n                self.lora_config,\n                self.device,\n                self.model.embedding_modules,\n                self.model.embedding_padding_modules,\n                max_position_embeddings=self.model.config.\n                max_position_embeddings,\n            )\n            self.model = self.lora_manager.create_lora_manager(self.model)\n\n        if self.prompt_adapter_config:\n            self.prompt_adapter_manager = LRUCacheWorkerPromptAdapterManager(\n                self.scheduler_config.max_num_seqs,\n                self.scheduler_config.max_num_batched_tokens, self.device,\n                self.prompt_adapter_config)\n            self.model = (\n                self.prompt_adapter_manager.create_prompt_adapter_manager(\n                    self.model))\n\n        if self.kv_cache_dtype == \"fp8\" and is_hip():\n            # Currently only ROCm accepts kv-cache scaling factors\n            # via quantization_param_path and this will be deprecated\n            # in the future.\n            if self.model_config.quantization_param_path is not None:\n                if callable(getattr(self.model, \"load_kv_cache_scales\", None)):\n                    warnings.warn(\n                        \"Loading kv cache scaling factor from JSON is \"\n                        \"deprecated and will be removed. Please include \"\n                        \"kv cache scaling factors in the model checkpoint.\",\n                        FutureWarning,\n                        stacklevel=2)\n                    self.model.load_kv_cache_scales(\n                        self.model_config.quantization_param_path)\n                    logger.info(\"Loaded KV cache scaling factors from %s\",\n                                self.model_config.quantization_param_path)\n                else:\n                    raise RuntimeError(\n                        \"Using FP8 KV cache and scaling factors provided but \"\n                        \"model %s does not support loading scaling factors.\",\n                        self.model.__class__)\n            else:\n                logger.warning(\n                    \"Using FP8 KV cache but no scaling factors \"\n                    \"provided. Defaulting to scaling factors of 1.0. \"\n                    \"This may lead to less accurate results!\")\n\n        if envs.VLLM_TEST_DYNAMO_GRAPH_CAPTURE:\n            self.model = torch.compile(self.model,\n                                       fullgraph=True,\n                                       backend=\"eager\")\n\n    def save_sharded_state(\n        self,\n        path: str,\n        pattern: Optional[str] = None,\n        max_size: Optional[int] = None,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import ShardedStateLoader\n        ShardedStateLoader.save_model(\n            self.model,\n            path,\n            pattern=pattern,\n            max_size=max_size,\n        )\n\n    def save_tensorized_model(\n        self,\n        tensorizer_config: TensorizerConfig,\n    ) -> None:\n        from vllm.model_executor.model_loader.loader import TensorizerLoader\n        TensorizerLoader.save_model(\n            self.model,\n            tensorizer_config=tensorizer_config,\n        )\n\n    def get_max_block_per_batch(self) -> int:\n        block_size = self.block_size\n        return (self.max_seq_len_to_capture + block_size - 1) // block_size\n\n    def _prepare_model_input_tensors(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> TModelInputForGPU:\n        \"\"\"Helper method to prepare the model input based on a given sequence\n        group. Prepares metadata needed for the base model forward pass but not\n        metadata for possible additional steps, e.g., sampling.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)\n        for seq_group_metadata in seq_group_metadata_list:\n            builder.add_seq_group(seq_group_metadata)\n        return builder.build()  # type: ignore\n\n    @torch.inference_mode()\n    def profile_run(self) -> None:\n        # Enable top-k sampling to reflect the accurate memory usage.\n        sampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\n        max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens\n        max_num_seqs = self.scheduler_config.max_num_seqs\n        # This represents the maximum number of different requests\n        # that will have unique loras, an therefore the max amount of memory\n        # consumption create dummy lora request copies from the lora request\n        # passed in, which contains a lora from the lora warmup path.\n        dummy_lora_requests: List[LoRARequest] = []\n        dummy_lora_requests_per_seq: List[LoRARequest] = []\n        if self.lora_config:\n            assert self.lora_manager is not None\n            with self.lora_manager.dummy_lora_cache():\n                for idx in range(self.lora_config.max_loras):\n                    lora_id = idx + 1\n                    dummy_lora_request = LoRARequest(\n                        lora_name=f\"warmup_{lora_id}\",\n                        lora_int_id=lora_id,\n                        lora_path=\"/not/a/real/path\",\n                    )\n                    self.lora_manager.add_dummy_lora(dummy_lora_request,\n                                                     rank=LORA_WARMUP_RANK)\n                    dummy_lora_requests.append(dummy_lora_request)\n                dummy_lora_requests_per_seq = [\n                    dummy_lora_requests[idx % len(dummy_lora_requests)]\n                    for idx in range(max_num_seqs)\n                ]\n\n        # Profile memory usage with max_num_sequences sequences and the total\n        # number of tokens equal to max_num_batched_tokens.\n        seqs: List[SequenceGroupMetadata] = []\n        # Additional GPU memory may be needed for vision encoding, which needs\n        # to be accounted for when calculating the GPU blocks for\n        # vLLM blocker manager.\n        # To exercise the worst scenario for GPU memory consumption,\n        # the number of seqs (batch_size) is chosen to maximize the number\n        # of images processed.\n        model_config = self.model_config\n\n        if supports_vision(self.model):\n            max_mm_tokens = MULTIMODAL_REGISTRY \\\n                .get_max_multimodal_tokens(model_config)\n            max_num_seqs_orig = max_num_seqs\n            max_num_seqs = min(max_num_seqs,\n                               max_num_batched_tokens // max_mm_tokens)\n            if max_num_seqs < 1:\n                expr = (f\"min({max_num_seqs_orig}, \"\n                        f\"{max_num_batched_tokens} // {max_mm_tokens})\")\n                logger.warning(\n                    \"Computed max_num_seqs (%s) to be less than 1. \"\n                    \"Setting it to the minimum value of 1.\", expr)\n                max_num_seqs = 1\n\n        batch_size = 0\n        for group_id in range(max_num_seqs):\n            seq_len = (max_num_batched_tokens // max_num_seqs +\n                       (group_id < max_num_batched_tokens % max_num_seqs))\n            batch_size += seq_len\n\n            seq_data, dummy_multi_modal_data = INPUT_REGISTRY \\\n                .dummy_data_for_profiling(model_config, seq_len)\n\n            # Having more tokens is over-conservative but otherwise fine\n            assert len(seq_data.prompt_token_ids) >= seq_len, (\n                f\"Expected at least {seq_len} dummy tokens for profiling, \"\n                f\"but got: {len(seq_data.prompt_token_ids)}\")\n\n            seq = SequenceGroupMetadata(\n                request_id=str(group_id),\n                is_prompt=True,\n                seq_data={group_id: seq_data},\n                sampling_params=sampling_params,\n                block_tables=None,\n                lora_request=dummy_lora_requests_per_seq[group_id]\n                if dummy_lora_requests_per_seq else None,\n                multi_modal_data=dummy_multi_modal_data,\n            )\n            seqs.append(seq)\n\n        # Run the model with the dummy inputs.\n        num_layers = self.model_config.get_num_layers(self.parallel_config)\n        kv_caches = [None] * num_layers\n        finished_requests_ids = [seq.request_id for seq in seqs]\n        model_input = self.prepare_model_input(\n            seqs, finished_requests_ids=finished_requests_ids)\n        intermediate_tensors = None\n        if not get_pp_group().is_first_rank:\n            intermediate_tensors = self.model.make_empty_intermediate_tensors(\n                batch_size=batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n        self.execute_model(model_input, kv_caches, intermediate_tensors)\n        torch.cuda.synchronize()\n        return\n\n    def remove_all_loras(self):\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.remove_all_adapters()\n\n    def set_active_loras(self, lora_requests: Set[LoRARequest],\n                         lora_mapping: LoRAMapping) -> None:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.add_adapter(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.remove_adapter(lora_id)\n\n    def pin_lora(self, lora_id: int) -> bool:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.pin_adapter(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        if not self.lora_manager:\n            raise RuntimeError(\"LoRA is not enabled.\")\n        return self.lora_manager.list_adapters()\n\n    def remove_all_prompt_adapters(self):\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.remove_all_adapters()\n\n    def set_active_prompt_adapters(\n            self, prompt_adapter_requests: Set[PromptAdapterRequest],\n            prompt_adapter_mapping: PromptAdapterMapping) -> None:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        self.prompt_adapter_manager.set_active_adapters(\n            prompt_adapter_requests, prompt_adapter_mapping)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.add_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.remove_adapter(prompt_adapter_id)\n\n    def pin_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.pin_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> Set[int]:\n        if not self.prompt_adapter_manager:\n            raise RuntimeError(\"PromptAdapter is not enabled.\")\n        return self.prompt_adapter_manager.list_adapters()\n\n    @torch.inference_mode()\n    def capture_model(self, kv_caches: List[List[torch.Tensor]]) -> None:\n        \"\"\"Cuda graph capture a model.\n\n        Note that CUDA graph's performance gain is negligible if number\n        of batched tokens are larger than 200. And since CUDA graph\n        requires fixed sized tensors, supporting large/variable batch\n        size requires high GPU memory overhead. Thus, vLLM only captures\n        decoding requests. Mixed batch (chunked prefill + decoding) or\n        prefill requests are not captured.\n\n        Since it is used for decoding-only, it assumes there's only 1 token\n        per sequence in the batch.\n        \"\"\"\n        assert not self.model_config.enforce_eager\n        logger.info(\"Capturing the model for CUDA graphs. This may lead to \"\n                    \"unexpected consequences if the model is not static. To \"\n                    \"run the model in eager mode, set 'enforce_eager=True' or \"\n                    \"use '--enforce-eager' in the CLI.\")\n        logger.info(\"CUDA graphs can take additional 1~3 GiB memory per GPU. \"\n                    \"If you are running out of memory, consider decreasing \"\n                    \"`gpu_memory_utilization` or enforcing eager mode. \"\n                    \"You can also reduce the `max_num_seqs` as needed \"\n                    \"to decrease memory usage.\")\n        start_time = time.perf_counter()\n\n        # Prepare dummy inputs. These will be reused for all batch sizes.\n        max_batch_size = max(_BATCH_SIZES_TO_CAPTURE)\n        input_tokens = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        input_positions = torch.zeros(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping = torch.empty(max_batch_size, dtype=torch.long).cuda()\n        slot_mapping.fill_(_PAD_SLOT_ID)\n        seq_lens = torch.ones(max_batch_size, dtype=torch.int32).cuda()\n        block_tables = torch.from_numpy(self.graph_block_tables).cuda()\n        intermediate_inputs = None\n        if not get_pp_group().is_first_rank:\n            intermediate_inputs = self.model.make_empty_intermediate_tensors(\n                batch_size=max_batch_size,\n                dtype=self.model_config.dtype,\n                device=self.device)\n\n        # Prepare buffer for outputs. These will be reused for all batch sizes.\n        # It will be filled after the first graph capture.\n        hidden_or_intermediate_states: List[Optional[torch.Tensor]] = [\n            None\n        ] * self.parallel_config.pipeline_parallel_size\n\n        graph_batch_size = _get_graph_batch_size(\n            self.scheduler_config.max_num_seqs)\n        batch_size_capture_list = [\n            bs for bs in _BATCH_SIZES_TO_CAPTURE if bs <= graph_batch_size\n        ]\n\n        if self.attn_backend.get_name() == \"flashinfer\":\n            # For flashinfer, different batch sizes will share the\n            # same workspace buffer.\n            decode_workspace_buffer = \\\n            torch.empty(FLASHINFER_WORKSPACE_BUFFER_SIZE,\n                                                dtype=torch.uint8,\n                                              device=self.device)\n            indices_buffer = torch.empty(max_batch_size *\n                                         self.cache_config.num_gpu_blocks,\n                                         dtype=torch.int32,\n                                         device=self.device)\n            indptr_buffer = torch.empty(max_batch_size + 1,\n                                        dtype=torch.int32,\n                                        device=self.device)\n            last_page_len_buffer = torch.empty(max_batch_size,\n                                               dtype=torch.int32,\n                                               device=self.device)\n\n        with graph_capture() as graph_capture_context:\n            # NOTE: Capturing the largest batch size first may help reduce the\n            # memory usage of CUDA graph.\n            for virtual_engine in range(\n                    self.parallel_config.pipeline_parallel_size):\n                for batch_size in reversed(batch_size_capture_list):\n                    if self.attn_backend.get_name() == \"flashinfer\":\n                        _indptr_buffer = indptr_buffer[:batch_size + 1]\n                        _last_page_len_buffer = last_page_len_buffer[:\n                                                                     batch_size]\n\n                        num_qo_heads = (\n                            self.model_config.get_num_attention_heads(\n                                self.parallel_config))\n                        num_kv_heads = self.model_config.get_num_kv_heads(\n                            self.parallel_config)\n                        if num_qo_heads // num_kv_heads >= 4:\n                            use_tensor_cores = True\n                        else:\n                            use_tensor_cores = False\n                        decode_wrapper = \\\n                            CUDAGraphBatchDecodeWithPagedKVCacheWrapper(\n                            decode_workspace_buffer, _indptr_buffer,\n                            indices_buffer, _last_page_len_buffer, \"NHD\",\n                            use_tensor_cores)\n                        kv_cache_dtype = get_kv_cache_torch_dtype(\n                            self.kv_cache_dtype, self.model_config.dtype)\n\n                        paged_kv_indptr_tensor_host = torch.arange(\n                            0, batch_size + 1, dtype=torch.int32)\n                        paged_kv_indices_tensor_host = torch.arange(\n                            0, batch_size, dtype=torch.int32)\n                        paged_kv_last_page_len_tensor_host = torch.full(\n                            (batch_size, ), self.block_size, dtype=torch.int32)\n                        query_start_loc_host = torch.arange(0,\n                                                            batch_size + 1,\n                                                            dtype=torch.int32)\n\n                        attn_metadata = self.attn_backend.make_metadata(\n                            num_prefills=0,\n                            slot_mapping=slot_mapping[:batch_size],\n                            num_prefill_tokens=0,\n                            num_decode_tokens=batch_size,\n                            max_prefill_seq_len=0,\n                            block_tables=block_tables,\n                            paged_kv_indptr=paged_kv_indptr_tensor_host,\n                            paged_kv_indices=paged_kv_indices_tensor_host,\n                            paged_kv_last_page_len=\n                            paged_kv_last_page_len_tensor_host,\n                            num_qo_heads=num_qo_heads,\n                            num_kv_heads=num_kv_heads,\n                            head_dim=self.model_config.get_head_size(),\n                            page_size=self.block_size,\n                            seq_start_loc=None,\n                            query_start_loc=query_start_loc_host,\n                            device=self.device,\n                            data_type=kv_cache_dtype,\n                            use_cuda_graph=True,\n                            decode_wrapper=decode_wrapper,\n                            prefill_wrapper=None)\n                        attn_metadata.begin_forward()\n                    else:\n                        attn_metadata = self.attn_backend.make_metadata(\n                            num_prefills=0,\n                            num_prefill_tokens=0,\n                            num_decode_tokens=batch_size,\n                            slot_mapping=slot_mapping[:batch_size],\n                            seq_lens=None,\n                            seq_lens_tensor=seq_lens[:batch_size],\n                            max_query_len=None,\n                            max_prefill_seq_len=0,\n                            max_decode_seq_len=self.max_seq_len_to_capture,\n                            query_start_loc=None,\n                            seq_start_loc=None,\n                            context_lens_tensor=None,\n                            block_tables=block_tables[:batch_size],\n                            use_cuda_graph=True,\n                        )\n\n                    if self.lora_config:\n                        lora_mapping = LoRAMapping(\n                            **dict(index_mapping=[0] * batch_size,\n                                   prompt_mapping=[0] * batch_size,\n                                   is_prefill=False))\n                        self.set_active_loras(set(), lora_mapping)\n\n                    if self.prompt_adapter_config:\n                        prompt_adapter_mapping = PromptAdapterMapping(\n                            [-1] * batch_size,\n                            [-1] * batch_size,\n                        )\n                        self.set_active_prompt_adapters(\n                            set(), prompt_adapter_mapping)\n\n                    graph_runner = CUDAGraphRunner(\n                        self.model, self.attn_backend.get_name())\n\n                    if self.attn_backend.get_name() == \"flashinfer\":\n                        graph_runner.flashinfer_indptr_buffer = _indptr_buffer\n                        graph_runner.flashinfer_indices_buffer = indices_buffer\n                        graph_runner.flashinfer_last_page_len_buffer = \\\n                            _last_page_len_buffer\n                        graph_runner.flashinfer_decode_workspace_buffer = \\\n                                decode_workspace_buffer\n                        graph_runner.flashinfer_decode_wrapper = \\\n                            decode_wrapper\n\n                    capture_inputs = {\n                        \"input_ids\":\n                        input_tokens[:batch_size],\n                        \"positions\":\n                        input_positions[:batch_size],\n                        \"hidden_or_intermediate_states\":\n                        hidden_or_intermediate_states[\n                            virtual_engine]  # type: ignore\n                        [:batch_size]\n                        if hidden_or_intermediate_states[virtual_engine]\n                        is not None else None,\n                        \"intermediate_inputs\":\n                        intermediate_inputs[:batch_size]\n                        if intermediate_inputs is not None else None,\n                        \"kv_caches\":\n                        kv_caches[virtual_engine],\n                        \"attn_metadata\":\n                        attn_metadata,\n                        \"memory_pool\":\n                        self.graph_memory_pool,\n                        \"stream\":\n                        graph_capture_context.stream\n                    }\n                    if self.has_seqlen_agnostic:\n                        # Only used by Mamba-based models CUDA graph atm (Jamba)\n                        capture_inputs.update({\n                            \"seqlen_agnostic_capture_inputs\":\n                            self.model.get_seqlen_agnostic_capture_inputs(\n                                batch_size)\n                        })\n                    graph_runner.capture(**capture_inputs)\n                    self.graph_memory_pool = graph_runner.graph.pool()\n                    self.graph_runners[virtual_engine][batch_size] = (\n                        graph_runner)\n\n        end_time = time.perf_counter()\n        elapsed_time = end_time - start_time\n        # This usually takes < 10 seconds.\n        logger.info(\"Graph capturing finished in %.0f secs.\", elapsed_time)\n\n    @property\n    def vocab_size(self) -> int:\n        return self.model_config.get_vocab_size()\n\n\nclass ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n    \"\"\"\n    GPU model runner with sampling step.\n    \"\"\"\n    _model_input_cls: Type[ModelInputForGPUWithSamplingMetadata] = (\n        ModelInputForGPUWithSamplingMetadata)\n    _builder_cls: Type[ModelInputForGPUBuilder] = ModelInputForGPUBuilder\n\n    def make_model_input_from_broadcasted_tensor_dict(\n        self,\n        tensor_dict: Dict[str, Any],\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        model_input = \\\n            ModelInputForGPUWithSamplingMetadata.from_broadcasted_tensor_dict(\n                tensor_dict,\n                attn_backend=self.attn_backend,\n            )\n        return model_input\n\n    def prepare_model_input(\n        self,\n        seq_group_metadata_list: List[SequenceGroupMetadata],\n        virtual_engine: int = 0,\n        finished_requests_ids: Optional[List[str]] = None\n    ) -> ModelInputForGPUWithSamplingMetadata:\n        \"\"\"Prepare the model input based on a given sequence group, including\n        metadata for the sampling step.\n\n        The API assumes seq_group_metadata_list is sorted by prefill -> decode.\n\n        The result tensors and data structure also batches input in prefill\n        -> decode order. For example,\n\n        - input_tokens[:num_prefill_tokens] contains prefill tokens.\n        - input_tokens[num_prefill_tokens:] contains decode tokens.\n\n        If cuda graph is required, this API automatically pads inputs.\n        \"\"\"\n        model_input = self._prepare_model_input_tensors(\n            seq_group_metadata_list, finished_requests_ids)\n        if get_pp_group().is_last_rank:\n            # Sampling metadata is only required for the final pp group\n            generators = self.get_generators(finished_requests_ids)\n            sampling_metadata = SamplingMetadata.prepare(\n                seq_group_metadata_list, model_input.seq_lens,\n                model_input.query_lens, self.device, self.pin_memory,\n                generators)\n        else:\n            sampling_metadata = None\n        is_prompt = (seq_group_metadata_list[0].is_prompt\n                     if seq_group_metadata_list else None)\n        return dataclasses.replace(model_input,\n                                   sampling_metadata=sampling_metadata,\n                                   is_prompt=is_prompt,\n                                   virtual_engine=virtual_engine)\n\n    @torch.inference_mode()\n    def execute_model(\n        self,\n        model_input: ModelInputForGPUWithSamplingMetadata,\n        kv_caches: List[torch.Tensor],\n        intermediate_tensors: Optional[IntermediateTensors] = None,\n        num_steps: int = 1,\n    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:\n        if num_steps > 1:\n            raise ValueError(\"num_steps > 1 is not supported in ModelRunner\")\n\n        if self.lora_config:\n            assert model_input.lora_requests is not None\n            assert model_input.lora_mapping is not None\n            self.set_active_loras(model_input.lora_requests,\n                                  model_input.lora_mapping)\n\n        if self.prompt_adapter_config:\n            assert model_input.prompt_adapter_requests is not None\n            assert model_input.prompt_adapter_mapping is not None\n            self.set_active_prompt_adapters(\n                model_input.prompt_adapter_requests,\n                model_input.prompt_adapter_mapping)\n\n        if self.attn_backend.get_name() == \"flashinfer\":\n            assert model_input.attn_metadata is not None\n            assert model_input.input_tokens is not None\n            if self.flashinfer_decode_workspace_buffer is None:\n                self.flashinfer_decode_workspace_buffer = torch.empty(\n                    FLASHINFER_WORKSPACE_BUFFER_SIZE,\n                    dtype=torch.uint8,\n                    device=self.device)\n                self.flashinfer_decode_wrapper = \\\n                    BatchDecodeWithPagedKVCacheWrapper(\n                    self.flashinfer_decode_workspace_buffer, \"NHD\")\n                self.flashinfer_prefill_workspace_buffer = torch.empty(\n                    FLASHINFER_WORKSPACE_BUFFER_SIZE,\n                    dtype=torch.uint8,\n                    device=self.device)\n                self.flashinfer_prefill_wrapper = \\\n                    BatchPrefillWithPagedKVCacheWrapper(\n                    self.flashinfer_prefill_workspace_buffer, \"NHD\")\n\n            model_input.attn_metadata.prefill_wrapper = \\\n                self.flashinfer_prefill_wrapper\n            if model_input.attn_metadata.use_cuda_graph:\n                batch_size = model_input.input_tokens.shape[0]\n                model_input.attn_metadata.decode_wrapper = self.graph_runners[\n                    model_input.\n                    virtual_engine][batch_size].flashinfer_decode_wrapper\n            else:\n                model_input.attn_metadata.decode_wrapper = \\\n                    self.flashinfer_decode_wrapper\n            model_input.attn_metadata.begin_forward()\n\n        # Currently cuda graph is only supported by the decode phase.\n        assert model_input.attn_metadata is not None\n        prefill_meta = model_input.attn_metadata.prefill_metadata\n        decode_meta = model_input.attn_metadata.decode_metadata\n        # TODO(andoorve): We can remove this once all\n        # virtual engines share the same kv cache.\n        virtual_engine = model_input.virtual_engine\n        if prefill_meta is None and decode_meta.use_cuda_graph:\n            assert model_input.input_tokens is not None\n            graph_batch_size = model_input.input_tokens.shape[0]\n            model_executable = self.graph_runners[virtual_engine][\n                graph_batch_size]\n        else:\n            model_executable = self.model\n\n        multi_modal_kwargs = model_input.multi_modal_kwargs or {}\n        seqlen_agnostic_kwargs = {\n            \"finished_requests_ids\": model_input.finished_requests_ids,\n            \"request_ids_to_seq_ids\": model_input.request_ids_to_seq_ids,\n        } if self.has_seqlen_agnostic else {}\n        hidden_or_intermediate_states = model_executable(\n            input_ids=model_input.input_tokens,\n            positions=model_input.input_positions,\n            kv_caches=kv_caches,\n            attn_metadata=model_input.attn_metadata,\n            intermediate_tensors=intermediate_tensors,\n            **MultiModalInputs.as_kwargs(multi_modal_kwargs,\n                                         device=self.device),\n            **seqlen_agnostic_kwargs)\n\n        # Compute the logits in the last pipeline stage.\n        if not get_pp_group().is_last_rank:\n            return hidden_or_intermediate_states\n\n        logits = self.model.compute_logits(hidden_or_intermediate_states,\n                                           model_input.sampling_metadata)\n\n        if not self.is_driver_worker:\n            return []\n\n        # Sample the next token.\n        output: SamplerOutput = self.model.sample(\n            logits=logits,\n            sampling_metadata=model_input.sampling_metadata,\n        )\n\n        if self.return_hidden_states:\n            # we only need to pass hidden states of most recent token\n            assert model_input.sampling_metadata is not None\n            indices = model_input.sampling_metadata.selected_token_indices\n            if model_input.is_prompt:\n                hidden_states = hidden_or_intermediate_states.index_select(\n                    0, indices)\n            elif decode_meta.use_cuda_graph:\n                hidden_states = hidden_or_intermediate_states[:len(indices)]\n            else:\n                hidden_states = hidden_or_intermediate_states\n\n            output.hidden_states = hidden_states\n\n        return [output]\n\n\nclass CUDAGraphRunner:\n\n    def __init__(self, model: nn.Module, backend_name: str):\n        self.model = model\n        self.backend_name = backend_name\n\n        self.input_buffers: Dict[str, torch.Tensor] = {}\n        self.output_buffers: Dict[str, torch.Tensor] = {}\n\n        self._graph: Optional[torch.cuda.CUDAGraph] = None\n\n        self.flashinfer_decode_workspace_buffer: Optional[torch.Tensor] = None\n        self.flashinfer_indptr_buffer: Optional[torch.Tensor] = None\n        self.flashinfer_indices_buffer: Optional[torch.Tensor] = None\n        self.flashinfer_last_page_len_buffer: Optional[torch.Tensor] = None\n        self.flashinfer_decode_wrapper: Optional[\n            CUDAGraphBatchDecodeWithPagedKVCacheWrapper] = None\n\n    @property\n    def graph(self):\n        assert self._graph is not None\n        return self._graph\n\n    def capture(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        hidden_or_intermediate_states: Optional[Union[IntermediateTensors,\n                                                      torch.Tensor]],\n        intermediate_inputs: Optional[IntermediateTensors],\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        memory_pool: Optional[Tuple[int, int]],\n        stream: torch.cuda.Stream,\n        **kwargs,\n    ) -> Union[torch.Tensor, IntermediateTensors]:\n        assert self._graph is None\n        # Run the model a few times without capturing the graph.\n        # This is to make sure that the captured graph does not include the\n        # kernel launches for initial benchmarking (e.g., Triton autotune).\n        # Note one iteration is not enough for torch.jit.script\n        for _ in range(_NUM_WARMUP_ITERS):\n            self.model(\n                input_ids,\n                positions,\n                kv_caches,\n                attn_metadata,\n                intermediate_inputs,\n                **kwargs,\n            )\n        torch.cuda.synchronize()\n\n        # Capture the graph.\n        self._graph = torch.cuda.CUDAGraph()\n        with torch.cuda.graph(self._graph, pool=memory_pool, stream=stream):\n            output_hidden_or_intermediate_states = self.model(\n                input_ids,\n                positions,\n                kv_caches,\n                attn_metadata,\n                intermediate_inputs,\n                **kwargs,\n            )\n            if hidden_or_intermediate_states is not None:\n                if get_pp_group().is_last_rank:\n                    hidden_or_intermediate_states.copy_(\n                        output_hidden_or_intermediate_states)\n                else:\n                    for key in hidden_or_intermediate_states.tensors:\n                        hidden_or_intermediate_states[key].copy_(\n                            output_hidden_or_intermediate_states[key])\n            else:\n                hidden_or_intermediate_states = (\n                    output_hidden_or_intermediate_states)\n\n            del output_hidden_or_intermediate_states\n            # make sure `output_hidden_states` is deleted\n            # in the graph's memory pool\n            gc.collect()\n        torch.cuda.synchronize()\n\n        # Save the input and output buffers.\n        if self.backend_name == \"flashinfer\":\n            self.input_buffers = {\n                \"input_ids\": input_ids,\n                \"positions\": positions,\n                \"kv_caches\": kv_caches,\n                \"slot_mapping\": attn_metadata.slot_mapping,\n                **kwargs,\n            }\n        else:\n            self.input_buffers = {\n                \"input_ids\": input_ids,\n                \"positions\": positions,\n                \"kv_caches\": kv_caches,\n                \"slot_mapping\": attn_metadata.slot_mapping,\n                \"seq_lens_tensor\":\n                attn_metadata.decode_metadata.seq_lens_tensor,\n                \"block_tables\": attn_metadata.decode_metadata.block_tables,\n                **kwargs,\n            }\n        if intermediate_inputs is not None:\n            self.input_buffers.update(intermediate_inputs.tensors)\n        if get_pp_group().is_last_rank:\n            self.output_buffers = {\n                \"hidden_states\": hidden_or_intermediate_states\n            }\n        else:\n            self.output_buffers = hidden_or_intermediate_states\n        return hidden_or_intermediate_states\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        positions: torch.Tensor,\n        kv_caches: List[torch.Tensor],\n        attn_metadata: AttentionMetadata,\n        intermediate_tensors: Optional[IntermediateTensors],\n        **kwargs,\n    ) -> torch.Tensor:\n        # KV caches are fixed tensors, so we don't need to copy them.\n        del kv_caches\n\n        # Copy the input tensors to the input buffers.\n        self.input_buffers[\"input_ids\"].copy_(input_ids, non_blocking=True)\n        self.input_buffers[\"positions\"].copy_(positions, non_blocking=True)\n        self.input_buffers[\"slot_mapping\"].copy_(attn_metadata.slot_mapping,\n                                                 non_blocking=True)\n        if self.backend_name != \"flashinfer\":\n            self.input_buffers[\"seq_lens_tensor\"].copy_(\n                attn_metadata.decode_metadata.seq_lens_tensor,\n                non_blocking=True)\n            self.input_buffers[\"block_tables\"].copy_(\n                attn_metadata.decode_metadata.block_tables, non_blocking=True)\n        if \"seqlen_agnostic_capture_inputs\" in self.input_buffers:\n            self.model.copy_inputs_before_cuda_graphs(self.input_buffers,\n                                                      **kwargs)\n        if intermediate_tensors is not None:\n            for key in intermediate_tensors.tensors:\n                self.input_buffers[key].copy_(intermediate_tensors[key],\n                                              non_blocking=True)\n        # Run the graph.\n        self.graph.replay()\n        if \"seqlen_agnostic_capture_inputs\" in self.input_buffers:\n            self.model.copy_outputs_after_cuda_graphs(self.input_buffers,\n                                                      **kwargs)\n        # Return the output tensor.\n        if get_pp_group().is_last_rank:\n            return self.output_buffers[\"hidden_states\"]\n\n        return self.output_buffers\n\n    def __call__(self, *args, **kwargs):\n        return self.forward(*args, **kwargs)\n\n\ndef _get_graph_batch_size(batch_size: int) -> int:\n    \"\"\"Returns the padded batch size given actual batch size.\n\n    Batch sizes are 1, 2, 4, _BATCH_SIZE_ALIGNMENT,\n    2*_BATCH_SIZE_ALIGNMENT, 3*_BATCH_SIZE_ALIGNMENT...\n    \"\"\"\n    if batch_size <= 2:\n        return batch_size\n    elif batch_size <= 4:\n        return 4\n    else:\n        return ((batch_size + _BATCH_SIZE_ALIGNMENT - 1) //\n                _BATCH_SIZE_ALIGNMENT * _BATCH_SIZE_ALIGNMENT)\n",
      "diff": "diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py\nindex 8b744a438..913a08ce9 100644\n--- a/vllm/worker/model_runner.py\n+++ b/vllm/worker/model_runner.py\n@@ -1,5 +1,6 @@\n import dataclasses\n import gc\n+import itertools\n import time\n import warnings\n import weakref\n@@ -35,7 +36,7 @@ from vllm.logger import init_logger\n from vllm.lora.layers import LoRAMapping\n from vllm.lora.request import LoRARequest\n from vllm.lora.worker_manager import LRUCacheWorkerLoRAManager\n-from vllm.model_executor import SamplingMetadata\n+from vllm.model_executor import SamplingMetadata, SamplingMetadataCache\n from vllm.model_executor.model_loader import get_model\n from vllm.model_executor.model_loader.tensorizer import TensorizerConfig\n from vllm.model_executor.models.interfaces import (supports_lora,\n@@ -50,8 +51,8 @@ from vllm.prompt_adapter.worker_manager import (\n from vllm.sampling_params import SamplingParams\n from vllm.sequence import (IntermediateTensors, SamplerOutput,\n                            SequenceGroupMetadata)\n-from vllm.utils import (CudaMemoryProfiler, async_tensor_h2d, flatten_2d_lists,\n-                        get_kv_cache_torch_dtype, is_hip,\n+from vllm.utils import (CudaMemoryProfiler, PyObjectCache, async_tensor_h2d,\n+                        flatten_2d_lists, get_kv_cache_torch_dtype, is_hip,\n                         is_pin_memory_available)\n from vllm.worker.model_runner_base import (\n     ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,\n@@ -178,6 +179,20 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n     class InterDataForSeqGroup:\n         \"\"\"Intermediate data for the current sequence group.\"\"\"\n \n+        def simple_reinit(self):\n+            self.input_tokens[0].clear()  # type: ignore\n+            self.input_positions[0].clear()  # type: ignore\n+            self.seq_lens[0] = 0  # type: ignore\n+            self.orig_seq_lens[0] = 0  # type: ignore\n+            self.query_lens[0] = 0  # type: ignore\n+            self.context_lens[0] = 0  # type: ignore\n+            self.curr_sliding_window_blocks[0] = 0  # type: ignore\n+            self.lora_index_mapping.clear()  # type: ignore\n+            self.lora_prompt_mapping.clear()  # type: ignore\n+            self.lora_requests.clear()  # type: ignore\n+            self.prompt_adapter_index_mapping.clear()  # type: ignore\n+            self.prompt_adapter_prompt_mapping.clear()  # type: ignore\n+\n         def __init__(\n             self,\n             *,\n@@ -220,35 +235,121 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n \n             # Whether the prefix cache is hit (prefill only).\n             prefix_cache_hit: bool = False,\n+            reinit: bool = False,\n+            reinit_use_defaults: bool = False,\n         ):\n+            if reinit:\n+                assert len(self.seq_ids) == len(seq_ids)  # type: ignore\n+                for i, seq_id in enumerate(seq_ids):\n+                    self.seq_ids[i] = seq_id  # type: ignore\n+            else:\n+                self.seq_ids = seq_ids\n+\n             self.request_id = request_id\n-            self.seq_ids = seq_ids\n             self.is_prompt = is_prompt\n             self.block_tables = block_tables\n             self.computed_block_nums = computed_block_nums\n             self.n_seqs = n_seqs\n-            self.input_tokens = input_tokens or []\n-            self.input_positions = input_positions or []\n-            self.seq_lens = seq_lens or []\n-            self.orig_seq_lens = orig_seq_lens or []\n-            self.query_lens = query_lens or []\n-            self.context_lens = context_lens or []\n-            self.curr_sliding_window_blocks = curr_sliding_window_blocks or []\n-\n-            self.lora_index_mapping = lora_index_mapping or []\n-            self.lora_prompt_mapping = lora_prompt_mapping or []\n-            self.lora_requests = lora_requests or set()\n-\n-            self.prompt_adapter_index_mapping = (prompt_adapter_index_mapping\n-                                                 or [])\n-            self.prompt_adapter_prompt_mapping = (prompt_adapter_prompt_mapping\n-                                                  or [])\n-            self.prompt_adapter_request = prompt_adapter_request\n \n+            if reinit:\n+                if len(self.seq_ids) == 1 and reinit_use_defaults:\n+                    self.simple_reinit()\n+                else:\n+                    if input_tokens:\n+                        self.input_tokens = input_tokens\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.input_tokens[seq_id].clear()\n+\n+                    if input_positions:\n+                        self.input_positions = input_positions\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.input_positions[seq_id].clear()\n+\n+                    if seq_lens:\n+                        self.seq_lens = seq_lens\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.seq_lens[seq_id] = 0\n+\n+                    if orig_seq_lens:\n+                        self.orig_seq_lens = orig_seq_lens\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.orig_seq_lens[seq_id] = 0\n+\n+                    if query_lens:\n+                        self.query_lens = query_lens\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.query_lens[seq_id] = 0\n+\n+                    if context_lens:\n+                        self.context_lens = context_lens\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.context_lens[seq_id] = 0\n+\n+                    if curr_sliding_window_blocks:\n+                        self.curr_sliding_window_blocks = \\\n+                            curr_sliding_window_blocks\n+                    else:\n+                        for seq_id in range(len(self.seq_ids)):\n+                            self.curr_sliding_window_blocks[seq_id] = 0\n+\n+                    if lora_index_mapping:\n+                        self.lora_index_mapping = lora_index_mapping\n+                    else:\n+                        self.lora_index_mapping.clear()\n+\n+                    if lora_prompt_mapping:\n+                        self.lora_prompt_mapping = lora_prompt_mapping\n+                    else:\n+                        self.lora_prompt_mapping.clear()\n+\n+                    if lora_requests:\n+                        self.lora_requests = lora_requests\n+                    else:\n+                        self.lora_requests.clear()\n+\n+                    if prompt_adapter_index_mapping:\n+                        self.prompt_adapter_index_mapping = \\\n+                            prompt_adapter_index_mapping\n+                    else:\n+                        self.prompt_adapter_index_mapping.clear()\n+\n+                    if prompt_adapter_prompt_mapping:\n+                        self.prompt_adapter_prompt_mapping = \\\n+                            prompt_adapter_prompt_mapping\n+                    else:\n+                        self.prompt_adapter_prompt_mapping.clear()\n+\n+            else:\n+                self.input_tokens = input_tokens or []\n+                self.input_positions = input_positions or []\n+                self.seq_lens = seq_lens or []\n+                self.orig_seq_lens = orig_seq_lens or []\n+                self.query_lens = query_lens or []\n+                self.context_lens = context_lens or []\n+                self.curr_sliding_window_blocks = \\\n+                    curr_sliding_window_blocks or []\n+\n+                self.lora_index_mapping = lora_index_mapping or []\n+                self.lora_prompt_mapping = lora_prompt_mapping or []\n+                self.lora_requests = lora_requests or set()\n+\n+                self.prompt_adapter_index_mapping = (\n+                    prompt_adapter_index_mapping or [])\n+                self.prompt_adapter_prompt_mapping = (\n+                    prompt_adapter_prompt_mapping or [])\n+\n+            self.prompt_adapter_request = prompt_adapter_request\n             self.multi_modal_inputs = multi_modal_inputs\n             self.prefix_cache_hit = prefix_cache_hit\n \n-            self.__post_init__()\n+            if not reinit:\n+                self.__post_init__()\n \n         def __post_init__(self):\n             self.n_seqs = len(self.seq_ids)\n@@ -261,8 +362,36 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             self.context_lens = [0] * self.n_seqs\n             self.curr_sliding_window_blocks = [0] * self.n_seqs\n \n-            self.lora_index_mapping = [[] for _ in range(self.n_seqs)]\n-            self.lora_prompt_mapping = [[] for _ in range(self.n_seqs)]\n+            self.lora_index_mapping = []\n+            self.lora_prompt_mapping = []\n+\n+    def gen_inter_data_builder(self, num_seqs: int):\n+        return lambda: ModelInputForGPUBuilder.InterDataForSeqGroup(\n+            request_id=\"\",\n+            seq_ids=[0] * num_seqs,\n+            is_prompt=True,\n+            block_tables=None,\n+            computed_block_nums=[])\n+\n+    def init_cached_inter_data(self, *args, **kwargs):\n+        assert len(args) == 0\n+        assert \"seq_ids\" in kwargs\n+        seq_ids = kwargs[\"seq_ids\"]\n+        num_seqs = len(seq_ids)\n+\n+        # The inter-data cache is per model_runner\n+        inter_data_cache = self.runner.inter_data_cache\n+        if num_seqs not in inter_data_cache:\n+            inter_data_cache[num_seqs] = PyObjectCache(\n+                self.gen_inter_data_builder(num_seqs))\n+\n+        obj = inter_data_cache[num_seqs].get_object()\n+        obj.__init__(*args, **kwargs)\n+        return obj\n+\n+    def reset_cached_inter_data(self):\n+        for cache in self.runner.inter_data_cache.values():\n+            cache.reset()\n \n     def __init__(self,\n                  runner: \"GPUModelRunnerBase\",\n@@ -337,17 +466,29 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n \n         # Compute tokens.\n         if inter_data.is_prompt:\n-            tokens = seq_data.get_token_ids()[context_len:seq_len]\n+            tokens = seq_data.get_token_ids()\n+            if context_len != 0 or seq_len < len(tokens):\n+                tokens = tokens[context_len:seq_len]\n         else:\n             # Optimization. get_token_ids requires the entire copy of\n             # tokens.\n-            tokens = [seq_data.get_last_token_id()]\n+            tokens = seq_data.get_last_token_id()\n \n         inter_data.seq_lens[seq_idx] = seq_len\n         inter_data.orig_seq_lens[seq_idx] = seq_len\n         inter_data.context_lens[seq_idx] = context_len\n-        inter_data.input_tokens[seq_idx] = tokens\n-        inter_data.input_positions[seq_idx] = list(range(context_len, seq_len))\n+\n+        if isinstance(tokens, list):\n+            inter_data.input_tokens[seq_idx].extend(tokens)\n+        else:\n+            inter_data.input_tokens[seq_idx].append(tokens)\n+\n+        if (seq_len - context_len) == 1:\n+            inter_data.input_positions[seq_idx].append(seq_len - 1)\n+        else:\n+            inter_data.input_positions[seq_idx].extend(\n+                range(context_len, seq_len))\n+\n         inter_data.query_lens[\n             seq_idx] = seq_len - context_len if inter_data.is_prompt else 1\n \n@@ -471,7 +612,7 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n \n     def add_seq_group(self, seq_group_metadata: SequenceGroupMetadata):\n         \"\"\"Add a sequence group to the builder.\"\"\"\n-        seq_ids = list(seq_group_metadata.seq_data.keys())\n+        seq_ids = seq_group_metadata.seq_data.keys()\n         n_seqs = len(seq_ids)\n         is_prompt = seq_group_metadata.is_prompt\n \n@@ -479,12 +620,15 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             assert n_seqs == 1\n             self.decode_only = False\n \n-        inter_data = self.InterDataForSeqGroup(\n+        inter_data = self.init_cached_inter_data(\n             request_id=seq_group_metadata.request_id,\n             seq_ids=seq_ids,\n             is_prompt=is_prompt,\n             block_tables=seq_group_metadata.block_tables,\n-            computed_block_nums=seq_group_metadata.computed_block_nums)\n+            computed_block_nums=seq_group_metadata.computed_block_nums,\n+            reinit=True,\n+            reinit_use_defaults=True)\n+\n         self.inter_data_list.append(inter_data)\n \n         for seq_idx in range(n_seqs):\n@@ -504,18 +648,21 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n         create on-device tensors.\n         \"\"\"\n         # Combine and flatten intermediate data.\n-        input_tokens = flatten_2d_lists([\n-            flatten_2d_lists(inter_data.input_tokens)\n-            for inter_data in self.inter_data_list\n-        ])\n+        input_tokens = []\n+        for inter_data in self.inter_data_list:\n+            for cur_input_tokens in inter_data.input_tokens:\n+                input_tokens.extend(cur_input_tokens)\n+\n         if not input_tokens:\n             # This may happen when all prefill requests hit\n             # prefix caching and there is no decode request.\n             return self.model_input_cls()\n-        input_positions = flatten_2d_lists([\n-            flatten_2d_lists(inter_data.input_positions)\n-            for inter_data in self.inter_data_list\n-        ])\n+\n+        input_positions = []\n+        for inter_data in self.inter_data_list:\n+            for cur_input_positions in inter_data.input_positions:\n+                input_positions.extend(cur_input_positions)\n+\n         seq_lens = []\n         max_decode_seq_len = 0\n         for inter_data in self.inter_data_list:\n@@ -523,8 +670,10 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             if not inter_data.is_prompt:\n                 max_decode_seq_len = max(max_decode_seq_len,\n                                          max(inter_data.seq_lens))\n-        query_lens = flatten_2d_lists(\n-            [inter_data.query_lens for inter_data in self.inter_data_list])\n+        query_lens = []\n+        for inter_data in self.inter_data_list:\n+            query_lens.extend(inter_data.query_lens)\n+\n         # Mapping from request IDs to sequence IDs. Used for Jamba models\n         # that manages the cache by itself.\n         request_ids_to_seq_ids = {\n@@ -547,8 +696,9 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n             batch_size = graph_batch_size\n \n         # Tokens and positions.\n-        input_tokens.extend([0] * cuda_graph_pad_size)\n-        input_positions.extend([0] * cuda_graph_pad_size)\n+        if cuda_graph_pad_size:\n+            input_tokens.extend(itertools.repeat(0, cuda_graph_pad_size))\n+            input_positions.extend(itertools.repeat(0, cuda_graph_pad_size))\n         assert self.runner.device is not None\n         input_tokens_tensor = async_tensor_h2d(input_tokens, torch.long,\n                                                self.runner.device,\n@@ -558,7 +708,8 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                                                   self.runner.pin_memory)\n \n         # Sequence and query lengths.\n-        seq_lens.extend([1] * cuda_graph_pad_size)\n+        if cuda_graph_pad_size:\n+            seq_lens.extend(itertools.repeat(1, cuda_graph_pad_size))\n \n         # Attention metadata.\n         attn_metadata = self.attn_metadata_builder.build(\n@@ -574,11 +725,14 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 flatten_2d_lists(inter_data.lora_index_mapping)\n                 for inter_data in self.inter_data_list\n             ])\n-            lora_index_mapping.extend([0] * cuda_graph_pad_size)\n+            if cuda_graph_pad_size:\n+                lora_index_mapping.extend(\n+                    itertools.repeat(0, cuda_graph_pad_size))\n             lora_prompt_mapping = flatten_2d_lists([\n                 flatten_2d_lists(inter_data.lora_prompt_mapping)\n                 for inter_data in self.inter_data_list\n             ])\n+\n             lora_mapping = LoRAMapping(\n                 **dict(index_mapping=lora_index_mapping,\n                        prompt_mapping=lora_prompt_mapping,\n@@ -595,7 +749,9 @@ class ModelInputForGPUBuilder(ModelRunnerInputBuilderBase[ModelInputForGPU]):\n                 inter_data.prompt_adapter_index_mapping\n                 for inter_data in self.inter_data_list\n             ])\n-            prompt_adapter_index_mapping.extend([0] * cuda_graph_pad_size)\n+            if cuda_graph_pad_size:\n+                prompt_adapter_index_mapping.extend(\n+                    itertools.repeat(0, cuda_graph_pad_size))\n             prompt_adapter_prompt_mapping = flatten_2d_lists([\n                 inter_data.prompt_adapter_prompt_mapping\n                 for inter_data in self.inter_data_list\n@@ -717,6 +873,11 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         set_cpu_offload_max_bytes(\n             int(self.cache_config.cpu_offload_gb * 1024**3))\n \n+        # Used to cache python objects\n+        self.inter_data_cache: Dict[int, PyObjectCache] = {}\n+        self.sampling_metadata_cache: SamplingMetadataCache = \\\n+            SamplingMetadataCache()\n+\n     def load_model(self) -> None:\n         logger.info(\"Starting to load model %s...\", self.model_config.model)\n         with CudaMemoryProfiler() as m:\n@@ -843,6 +1004,9 @@ class GPUModelRunnerBase(ModelRunnerBase[TModelInputForGPU]):\n         builder = self._builder_cls(weakref.proxy(self), finished_requests_ids)\n         for seq_group_metadata in seq_group_metadata_list:\n             builder.add_seq_group(seq_group_metadata)\n+\n+        builder.reset_cached_inter_data()\n+\n         return builder.build()  # type: ignore\n \n     @torch.inference_mode()\n@@ -1276,7 +1440,7 @@ class ModelRunner(GPUModelRunnerBase[ModelInputForGPUWithSamplingMetadata]):\n             sampling_metadata = SamplingMetadata.prepare(\n                 seq_group_metadata_list, model_input.seq_lens,\n                 model_input.query_lens, self.device, self.pin_memory,\n-                generators)\n+                generators, self.sampling_metadata_cache)\n         else:\n             sampling_metadata = None\n         is_prompt = (seq_group_metadata_list[0].is_prompt",
      "change_type": "modified",
      "lines_added": 212,
      "lines_removed": 48
    }
  ],
  "affected_apis": [
    "compute_slot_mapping",
    "BlockTable.ids",
    "BlockSpaceManagerV1.get_block_table",
    "Scheduler._schedule",
    "FlashAttentionMetadataBuilder"
  ],
  "summary": {
    "total_files": 11,
    "files_added": 0,
    "files_deleted": 0,
    "files_modified": 11
  },
  "csv_metadata": {
    "category": "miscellaneous",
    "json_has_tests": "FALSE",
    "json_has_benchmarks": "FALSE",
    "is_test_actually_there": "Yes (too much BS)",
    "is_benchmark_actually_there": "",
    "sample_clues": "__init__, attention, attn"
  }
}